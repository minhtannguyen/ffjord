{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.5, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.2677(27.2459) | Bit/dim 8.6908(8.9524) | Xent 2.2807(2.3001) | Loss 25.7810(26.5016) | Error 0.8044(0.8606) Steps 0(0.00) | Grad Norm 22.8226(29.5441) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.6329(23.2880) | Bit/dim 8.4862(8.8634) | Xent 2.2264(2.2874) | Loss 25.7202(26.2829) | Error 0.7233(0.8329) Steps 0(0.00) | Grad Norm 9.6132(25.5790) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 11.5679(20.3076) | Bit/dim 8.3925(8.7516) | Xent 2.1742(2.2634) | Loss 24.5535(26.0097) | Error 0.7556(0.8095) Steps 0(0.00) | Grad Norm 9.3554(21.0048) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.4467(18.1624) | Bit/dim 8.1932(8.6235) | Xent 2.1138(2.2346) | Loss 25.1822(25.7421) | Error 0.7289(0.7912) Steps 0(0.00) | Grad Norm 5.9474(17.2387) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 11.9962(16.5846) | Bit/dim 7.9746(8.4740) | Xent 2.1022(2.2021) | Loss 24.2295(25.3892) | Error 0.7033(0.7752) Steps 0(0.00) | Grad Norm 5.7759(14.3163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 73.4180, Epoch Time 778.5472(778.5472), Bit/dim 7.7700(best: inf), Xent 2.0786, Loss 8.8093, Error 0.7016(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.4428(15.4932) | Bit/dim 7.6553(8.2924) | Xent 2.0893(2.1719) | Loss 23.6120(27.9129) | Error 0.7056(0.7587) Steps 0(0.00) | Grad Norm 5.3236(12.0653) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 12.9539(14.7758) | Bit/dim 7.3662(8.0809) | Xent 2.0765(2.1454) | Loss 22.8365(26.6829) | Error 0.7078(0.7420) Steps 0(0.00) | Grad Norm 4.5573(10.1984) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.1247(14.2792) | Bit/dim 7.1813(7.8642) | Xent 2.0842(2.1250) | Loss 22.7765(25.6916) | Error 0.6778(0.7259) Steps 0(0.00) | Grad Norm 3.3245(8.5217) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 12.7687(13.9121) | Bit/dim 7.0764(7.6684) | Xent 2.0855(2.1141) | Loss 22.7057(24.8683) | Error 0.6900(0.7187) Steps 0(0.00) | Grad Norm 2.5552(6.9992) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.7595(13.6994) | Bit/dim 7.0196(7.5058) | Xent 2.0789(2.1033) | Loss 22.3235(24.2317) | Error 0.7011(0.7148) Steps 0(0.00) | Grad Norm 2.7125(5.7673) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.1666(13.5967) | Bit/dim 6.9997(7.3757) | Xent 2.0655(2.0946) | Loss 22.5731(23.7874) | Error 0.7167(0.7142) Steps 0(0.00) | Grad Norm 6.7968(5.2248) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 72.5118, Epoch Time 801.8130(779.2451), Bit/dim 6.9918(best: 7.7700), Xent 2.0575, Loss 8.0206, Error 0.6948(best: 0.7016)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.3725(13.5402) | Bit/dim 6.9377(7.2706) | Xent 2.0503(2.0853) | Loss 22.4185(26.0707) | Error 0.6900(0.7120) Steps 0(0.00) | Grad Norm 3.8606(5.1183) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.7765(13.6340) | Bit/dim 6.9078(7.1832) | Xent 2.0658(2.0760) | Loss 22.3142(25.0859) | Error 0.7100(0.7087) Steps 0(0.00) | Grad Norm 5.2583(4.7259) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 14.3379(13.7126) | Bit/dim 6.8744(7.1068) | Xent 2.0520(2.0701) | Loss 22.1469(24.3026) | Error 0.6900(0.7053) Steps 0(0.00) | Grad Norm 15.3659(5.4573) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 14.0014(13.7233) | Bit/dim 6.8472(7.0376) | Xent 1.9906(2.0584) | Loss 22.4559(23.7101) | Error 0.6689(0.7000) Steps 0(0.00) | Grad Norm 2.4205(5.6664) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 13.7730(13.7565) | Bit/dim 6.7217(6.9634) | Xent 1.9755(2.0475) | Loss 21.9246(23.2131) | Error 0.6811(0.6952) Steps 0(0.00) | Grad Norm 17.0057(6.6386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 74.9481, Epoch Time 849.8088(781.3621), Bit/dim 6.6407(best: 6.9918), Xent 2.0278, Loss 7.6546, Error 0.6930(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 13.8236(13.7565) | Bit/dim 6.5821(6.8816) | Xent 2.0157(2.0451) | Loss 21.3452(25.8314) | Error 0.6711(0.6970) Steps 0(0.00) | Grad Norm 12.2169(13.2581) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.5752(13.7439) | Bit/dim 6.4602(6.7797) | Xent 2.2099(2.0467) | Loss 21.6480(24.6727) | Error 0.7878(0.7012) Steps 0(0.00) | Grad Norm 119.0344(23.9657) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.6492(13.8039) | Bit/dim 6.2608(6.6615) | Xent 2.0378(2.0680) | Loss 21.0518(23.7847) | Error 0.7033(0.7173) Steps 0(0.00) | Grad Norm 26.0877(35.9470) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 12.9739(13.7209) | Bit/dim 6.1051(6.5331) | Xent 2.0569(2.0632) | Loss 20.2259(22.9590) | Error 0.6967(0.7156) Steps 0(0.00) | Grad Norm 49.9786(38.9663) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.6585(13.7436) | Bit/dim 6.0484(6.4338) | Xent 2.0795(2.0909) | Loss 20.1390(22.3733) | Error 0.7344(0.7277) Steps 0(0.00) | Grad Norm 51.7290(57.6487) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.4266(13.6484) | Bit/dim 5.9114(6.3128) | Xent 2.0454(2.0813) | Loss 20.1252(21.8174) | Error 0.6889(0.7211) Steps 0(0.00) | Grad Norm 22.6569(51.5984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 72.8966, Epoch Time 842.8342(783.2062), Bit/dim 5.8965(best: 6.6407), Xent 2.0614, Loss 6.9271, Error 0.7215(best: 0.6930)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.6193(13.6002) | Bit/dim 5.8314(6.1884) | Xent 2.0098(2.0703) | Loss 19.4950(23.9833) | Error 0.6722(0.7176) Steps 0(0.00) | Grad Norm 35.5744(44.6164) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.1750(13.5679) | Bit/dim 5.7255(6.0777) | Xent 1.9846(2.0548) | Loss 19.8081(22.9143) | Error 0.6700(0.7106) Steps 0(0.00) | Grad Norm 31.3100(39.0878) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 14.0204(13.5240) | Bit/dim 5.6659(5.9808) | Xent 1.9474(2.0379) | Loss 19.4343(22.0017) | Error 0.6444(0.7009) Steps 0(0.00) | Grad Norm 30.6405(34.3102) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.1264(13.5087) | Bit/dim 5.6908(5.8993) | Xent 2.0401(2.0256) | Loss 19.2114(21.3511) | Error 0.7033(0.6968) Steps 0(0.00) | Grad Norm 39.4571(35.2666) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.1299(13.4706) | Bit/dim 5.6416(5.8345) | Xent 1.9864(2.0176) | Loss 19.6449(20.8723) | Error 0.6900(0.6920) Steps 0(0.00) | Grad Norm 43.3233(35.3766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 73.9103, Epoch Time 829.8489(784.6055), Bit/dim 5.6227(best: 5.8965), Xent 1.9568, Loss 6.6011, Error 0.6686(best: 0.6930)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.4903(13.4118) | Bit/dim 5.6322(5.7811) | Xent 1.9739(2.0082) | Loss 19.4814(23.5912) | Error 0.6878(0.6916) Steps 0(0.00) | Grad Norm 22.9572(34.0479) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.4990(13.3856) | Bit/dim 5.5860(5.7330) | Xent 1.9424(2.0010) | Loss 19.3079(22.4533) | Error 0.6789(0.6892) Steps 0(0.00) | Grad Norm 53.0464(35.1990) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.9662(13.3405) | Bit/dim 5.6130(5.6946) | Xent 2.0039(2.0026) | Loss 18.8332(21.6264) | Error 0.7189(0.6954) Steps 0(0.00) | Grad Norm 58.4765(43.8395) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.0270(13.3336) | Bit/dim 5.5688(5.6603) | Xent 1.9721(1.9916) | Loss 19.3769(21.0104) | Error 0.6900(0.6893) Steps 0(0.00) | Grad Norm 20.0720(38.3204) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.1425(13.3211) | Bit/dim 5.5490(5.6299) | Xent 1.9805(1.9829) | Loss 19.0381(20.5097) | Error 0.6867(0.6842) Steps 0(0.00) | Grad Norm 43.3388(34.5193) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.5573(13.3710) | Bit/dim 5.5342(5.6014) | Xent 1.9445(1.9840) | Loss 19.5065(20.1963) | Error 0.6722(0.6852) Steps 0(0.00) | Grad Norm 29.8543(35.0107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 74.1997, Epoch Time 824.4009(785.7994), Bit/dim 5.4896(best: 5.6227), Xent 1.9339, Loss 6.4566, Error 0.6432(best: 0.6686)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.4370(13.4100) | Bit/dim 5.4603(5.5695) | Xent 1.9144(1.9743) | Loss 19.1012(22.5281) | Error 0.6500(0.6796) Steps 0(0.00) | Grad Norm 16.1589(30.9711) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 12.9864(13.4087) | Bit/dim 5.3963(5.5399) | Xent 1.9229(1.9613) | Loss 19.0652(21.6059) | Error 0.6556(0.6761) Steps 0(0.00) | Grad Norm 13.3503(28.3152) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.5313(13.4590) | Bit/dim 5.3715(5.5012) | Xent 1.9679(1.9521) | Loss 19.2408(20.9090) | Error 0.6778(0.6743) Steps 0(0.00) | Grad Norm 58.3376(31.0313) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.3818(13.6132) | Bit/dim 5.3461(5.4679) | Xent 1.9178(1.9404) | Loss 18.9573(20.4412) | Error 0.6544(0.6702) Steps 0(0.00) | Grad Norm 19.9781(29.2143) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 14.4670(13.7668) | Bit/dim 5.3616(5.4307) | Xent 1.8399(1.9289) | Loss 19.0853(20.0891) | Error 0.6289(0.6659) Steps 0(0.00) | Grad Norm 19.2313(28.3168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 77.1598, Epoch Time 854.5758(787.8627), Bit/dim 5.2844(best: 5.4896), Xent 1.8689, Loss 6.2189, Error 0.6458(best: 0.6432)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.0435(13.8378) | Bit/dim 5.3224(5.3937) | Xent 1.9726(1.9172) | Loss 18.9000(23.1164) | Error 0.6889(0.6632) Steps 0(0.00) | Grad Norm 59.7382(29.6393) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.7199(13.8267) | Bit/dim 5.4722(5.4023) | Xent 2.0927(1.9857) | Loss 19.2009(22.1539) | Error 0.7444(0.6899) Steps 0(0.00) | Grad Norm 45.5691(43.9714) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.5836(13.8113) | Bit/dim 5.2666(5.3797) | Xent 2.0111(1.9957) | Loss 18.6828(21.3357) | Error 0.6989(0.6938) Steps 0(0.00) | Grad Norm 7.0265(38.4959) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 14.0556(13.8214) | Bit/dim 5.2041(5.3449) | Xent 1.9713(1.9859) | Loss 18.7907(20.6410) | Error 0.6744(0.6894) Steps 0(0.00) | Grad Norm 7.5250(31.0501) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.8621(13.8812) | Bit/dim 5.1681(5.3050) | Xent 1.9694(1.9823) | Loss 18.6715(20.1229) | Error 0.7067(0.6897) Steps 0(0.00) | Grad Norm 10.1935(25.0446) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.7503(13.8660) | Bit/dim 5.1454(5.2664) | Xent 1.9160(1.9702) | Loss 18.5396(19.6828) | Error 0.6744(0.6866) Steps 0(0.00) | Grad Norm 6.8064(20.3565) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 76.1441, Epoch Time 855.7679(789.8998), Bit/dim 5.1365(best: 5.2844), Xent 1.8994, Loss 6.0862, Error 0.6435(best: 0.6432)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 13.6486(13.8908) | Bit/dim 5.1077(5.2318) | Xent 1.9093(1.9543) | Loss 18.4758(22.0291) | Error 0.6511(0.6797) Steps 0(0.00) | Grad Norm 4.6030(16.6962) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.2563(14.0389) | Bit/dim 5.1007(5.2005) | Xent 1.8589(1.9424) | Loss 18.2344(21.1060) | Error 0.6356(0.6770) Steps 0(0.00) | Grad Norm 7.6847(16.1847) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.5958(14.1859) | Bit/dim 5.1049(5.1684) | Xent 1.9355(1.9281) | Loss 18.3665(20.3484) | Error 0.6778(0.6711) Steps 0(0.00) | Grad Norm 13.8782(14.7632) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 14.5511(14.2532) | Bit/dim 5.0905(5.1388) | Xent 1.8815(1.9125) | Loss 18.3247(19.7739) | Error 0.6556(0.6660) Steps 0(0.00) | Grad Norm 13.5196(14.2000) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 14.0569(14.3373) | Bit/dim 5.0262(5.1102) | Xent 1.9382(1.9063) | Loss 18.1906(19.3610) | Error 0.6678(0.6647) Steps 0(0.00) | Grad Norm 50.8137(18.3172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 75.9101, Epoch Time 886.5684(792.7999), Bit/dim 4.9994(best: 5.1365), Xent 1.8856, Loss 5.9422, Error 0.6507(best: 0.6432)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.7798(14.4111) | Bit/dim 4.9615(5.0812) | Xent 1.8470(1.9050) | Loss 17.9729(22.1165) | Error 0.6367(0.6652) Steps 0(0.00) | Grad Norm 30.0309(22.1501) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.8670(14.4638) | Bit/dim 4.9690(5.0520) | Xent 1.8499(1.8925) | Loss 17.7662(21.0341) | Error 0.6211(0.6607) Steps 0(0.00) | Grad Norm 40.5322(22.8617) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.6053(14.4980) | Bit/dim 4.9169(5.0224) | Xent 1.8559(1.8838) | Loss 17.9374(20.2247) | Error 0.6422(0.6566) Steps 0(0.00) | Grad Norm 24.1251(22.9631) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 13.7028(14.5916) | Bit/dim 4.9865(5.0080) | Xent 2.0606(1.8972) | Loss 17.9469(19.6922) | Error 0.7389(0.6630) Steps 0(0.00) | Grad Norm 66.4678(31.2425) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.0996(14.7266) | Bit/dim 4.9037(4.9831) | Xent 1.9967(1.9117) | Loss 18.0739(19.2697) | Error 0.6989(0.6696) Steps 0(0.00) | Grad Norm 17.4891(29.2125) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 14.8670(14.8005) | Bit/dim 4.8275(4.9579) | Xent 1.8523(1.9109) | Loss 17.6802(18.9215) | Error 0.6556(0.6696) Steps 0(0.00) | Grad Norm 4.2284(27.3721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 79.2479, Epoch Time 912.8290(796.4007), Bit/dim 4.8541(best: 4.9994), Xent 1.8592, Loss 5.7837, Error 0.6416(best: 0.6432)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 15.6463(14.8899) | Bit/dim 4.8717(4.9315) | Xent 1.8269(1.8980) | Loss 17.9027(21.3557) | Error 0.6700(0.6657) Steps 0(0.00) | Grad Norm 22.3422(25.2297) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.0566(14.9803) | Bit/dim 4.8948(4.9150) | Xent 1.9519(1.8949) | Loss 18.1273(20.4844) | Error 0.6933(0.6661) Steps 0(0.00) | Grad Norm 61.2863(29.5242) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.7989(15.0514) | Bit/dim 4.8346(4.8982) | Xent 1.8376(1.8848) | Loss 17.6672(19.7882) | Error 0.6389(0.6618) Steps 0(0.00) | Grad Norm 17.0522(27.9573) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.7847(15.1774) | Bit/dim 4.7413(4.8702) | Xent 1.8441(1.8691) | Loss 17.5760(19.2459) | Error 0.6667(0.6578) Steps 0(0.00) | Grad Norm 20.5640(25.4045) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 15.2165(15.1795) | Bit/dim 4.7776(4.8501) | Xent 1.8380(1.8535) | Loss 17.8857(18.8413) | Error 0.6300(0.6521) Steps 0(0.00) | Grad Norm 45.8360(26.7083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Iter 0610 | Time 15.3102(15.2758) | Bit/dim 4.7870(4.8252) | Xent 1.7592(1.8346) | Loss 18.0710(21.7134) | Error 0.6200(0.6459) Steps 0(0.00) | Grad Norm 23.5791(25.4307) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.4796(15.3290) | Bit/dim 4.8126(4.8250) | Xent 2.2542(1.8532) | Loss 18.6276(20.7569) | Error 0.7700(0.6534) Steps 0(0.00) | Grad Norm 95.6786(34.4659) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 15.3402(15.2605) | Bit/dim 4.8011(4.8245) | Xent 1.8838(1.8702) | Loss 17.9806(20.0263) | Error 0.6778(0.6620) Steps 0(0.00) | Grad Norm 20.4632(32.3857) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 15.5025(15.2220) | Bit/dim 4.7486(4.8046) | Xent 1.8897(1.8721) | Loss 17.9821(19.4077) | Error 0.6789(0.6630) Steps 0(0.00) | Grad Norm 17.1682(26.8824) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.9038(15.2179) | Bit/dim 4.6974(4.7800) | Xent 1.8265(1.8560) | Loss 17.4768(18.9102) | Error 0.6422(0.6561) Steps 0(0.00) | Grad Norm 24.8244(23.4905) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 14.9629(15.2865) | Bit/dim 4.7441(4.7676) | Xent 1.8650(1.8508) | Loss 17.7942(18.5813) | Error 0.6700(0.6543) Steps 0(0.00) | Grad Norm 33.7147(27.2732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 79.2204, Epoch Time 936.6673(804.8351), Bit/dim 4.7762(best: 4.7498), Xent 1.7742, Loss 5.6633, Error 0.6173(best: 0.6106)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.3953(15.2979) | Bit/dim 4.7114(4.7571) | Xent 1.7207(1.8310) | Loss 17.3046(21.0195) | Error 0.6100(0.6465) Steps 0(0.00) | Grad Norm 10.0971(26.9438) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.3396(15.3386) | Bit/dim 4.6803(4.7355) | Xent 1.7156(1.8055) | Loss 17.0716(20.0604) | Error 0.6000(0.6380) Steps 0(0.00) | Grad Norm 15.6701(24.2382) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.3951(15.3972) | Bit/dim 4.6248(4.7156) | Xent 1.7684(1.7989) | Loss 17.2180(19.3999) | Error 0.6289(0.6368) Steps 0(0.00) | Grad Norm 16.2246(26.0801) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.7504(15.4294) | Bit/dim 4.6533(4.6964) | Xent 1.7631(1.7801) | Loss 17.8500(18.9183) | Error 0.6400(0.6316) Steps 0(0.00) | Grad Norm 26.8878(24.0124) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.7715(15.5136) | Bit/dim 4.5967(4.6777) | Xent 1.7530(1.7737) | Loss 17.6255(18.5266) | Error 0.6244(0.6280) Steps 0(0.00) | Grad Norm 30.8988(25.9427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 83.5036, Epoch Time 954.0564(809.3117), Bit/dim 4.6537(best: 4.7498), Xent 1.7346, Loss 5.5210, Error 0.6280(best: 0.6106)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 15.9449(15.5489) | Bit/dim 4.7490(4.6823) | Xent 2.0050(1.8090) | Loss 18.5332(21.7037) | Error 0.7089(0.6380) Steps 0(0.00) | Grad Norm 48.3480(33.5708) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 15.7942(15.5549) | Bit/dim 4.6657(4.6847) | Xent 1.8211(1.8240) | Loss 17.4746(20.6209) | Error 0.6578(0.6467) Steps 0(0.00) | Grad Norm 13.0241(29.5453) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.6835(15.4642) | Bit/dim 4.6143(4.6703) | Xent 1.8002(1.8192) | Loss 17.2099(19.7120) | Error 0.6300(0.6450) Steps 0(0.00) | Grad Norm 14.0675(25.1481) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.9160(15.3896) | Bit/dim 4.5889(4.6536) | Xent 1.8287(1.7996) | Loss 17.2448(19.0651) | Error 0.6467(0.6384) Steps 0(0.00) | Grad Norm 17.3520(22.8955) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 15.0343(15.3594) | Bit/dim 4.5610(4.6311) | Xent 1.7422(1.7801) | Loss 17.6151(18.6167) | Error 0.6178(0.6329) Steps 0(0.00) | Grad Norm 25.5952(20.6772) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.5509(15.4286) | Bit/dim 4.6367(4.6234) | Xent 1.7317(1.7851) | Loss 17.6189(18.3229) | Error 0.6100(0.6389) Steps 0(0.00) | Grad Norm 27.1429(24.0102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 81.8169, Epoch Time 947.9719(813.4715), Bit/dim 4.5809(best: 4.6537), Xent 1.6605, Loss 5.4112, Error 0.5812(best: 0.6106)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 15.7402(15.4553) | Bit/dim 4.5224(4.6062) | Xent 1.6625(1.7593) | Loss 16.7982(20.9305) | Error 0.5944(0.6283) Steps 0(0.00) | Grad Norm 9.2862(21.5591) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.1588(15.5213) | Bit/dim 4.5995(4.5878) | Xent 1.7615(1.7471) | Loss 17.3867(19.9707) | Error 0.6256(0.6236) Steps 0(0.00) | Grad Norm 67.9204(21.8941) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 15.3737(15.5975) | Bit/dim 4.5595(4.5876) | Xent 1.7275(1.7449) | Loss 17.2525(19.3010) | Error 0.6389(0.6220) Steps 0(0.00) | Grad Norm 23.9834(24.5152) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.2657(15.7074) | Bit/dim 4.5449(4.5763) | Xent 1.6679(1.7331) | Loss 17.2840(18.7815) | Error 0.5900(0.6191) Steps 0(0.00) | Grad Norm 32.3117(23.0076) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 16.0138(15.7526) | Bit/dim 4.5172(4.5637) | Xent 1.7567(1.7217) | Loss 17.2350(18.3966) | Error 0.6322(0.6167) Steps 0(0.00) | Grad Norm 39.2409(23.0883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 82.8049, Epoch Time 968.6879(818.1280), Bit/dim 4.4931(best: 4.5809), Xent 1.5875, Loss 5.2869, Error 0.5648(best: 0.5812)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 15.1297(15.7750) | Bit/dim 4.5189(4.5459) | Xent 1.7287(1.7064) | Loss 16.9044(21.4568) | Error 0.6211(0.6120) Steps 0(0.00) | Grad Norm 46.7454(22.6822) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 15.9890(15.8173) | Bit/dim 4.4892(4.5324) | Xent 1.6652(1.6934) | Loss 17.5399(20.3539) | Error 0.6189(0.6069) Steps 0(0.00) | Grad Norm 28.9967(23.2926) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.9644(15.8441) | Bit/dim 4.4425(4.5145) | Xent 1.6486(1.6836) | Loss 17.3050(19.4971) | Error 0.5756(0.6030) Steps 0(0.00) | Grad Norm 19.1975(21.6331) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.5065(15.9123) | Bit/dim 4.4778(4.5037) | Xent 1.6602(1.6776) | Loss 17.3064(18.8710) | Error 0.5822(0.5996) Steps 0(0.00) | Grad Norm 42.3027(23.9729) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 15.6594(15.9444) | Bit/dim 4.4327(4.4976) | Xent 1.6843(1.6930) | Loss 17.1251(18.4681) | Error 0.6189(0.6070) Steps 0(0.00) | Grad Norm 14.2086(25.6844) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.9739(15.9995) | Bit/dim 4.4184(4.4832) | Xent 1.6836(1.6932) | Loss 16.8929(18.0896) | Error 0.6178(0.6086) Steps 0(0.00) | Grad Norm 13.0425(22.5803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 84.6402, Epoch Time 983.1294(823.0781), Bit/dim 4.4434(best: 4.4931), Xent 1.5970, Loss 5.2419, Error 0.5686(best: 0.5648)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.6118(16.1160) | Bit/dim 4.4567(4.4710) | Xent 1.6681(1.6780) | Loss 17.4136(20.6731) | Error 0.5867(0.6030) Steps 0(0.00) | Grad Norm 19.9034(22.2032) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 16.6019(16.1984) | Bit/dim 4.4395(4.4604) | Xent 1.6761(1.6750) | Loss 16.8413(19.7516) | Error 0.5844(0.6006) Steps 0(0.00) | Grad Norm 41.0369(23.9018) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 15.9082(16.2559) | Bit/dim 4.3957(4.4466) | Xent 1.5767(1.6644) | Loss 16.7848(19.0435) | Error 0.5800(0.5977) Steps 0(0.00) | Grad Norm 10.2642(22.5457) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 15.6732(16.2520) | Bit/dim 4.4012(4.4404) | Xent 1.6007(1.6579) | Loss 16.9068(18.5170) | Error 0.5711(0.5962) Steps 0(0.00) | Grad Norm 11.0856(22.1761) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.1666(16.2078) | Bit/dim 4.4156(4.4349) | Xent 1.6291(1.6572) | Loss 17.1407(18.1505) | Error 0.5967(0.5940) Steps 0(0.00) | Grad Norm 7.2770(22.1253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 84.3627, Epoch Time 999.9163(828.3832), Bit/dim 4.4257(best: 4.4434), Xent 1.5627, Loss 5.2071, Error 0.5558(best: 0.5648)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.1521(16.2036) | Bit/dim 4.4061(4.4214) | Xent 1.5454(1.6427) | Loss 16.7746(21.1983) | Error 0.5367(0.5874) Steps 0(0.00) | Grad Norm 19.6524(21.8724) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.6328(16.2199) | Bit/dim 4.3543(4.4061) | Xent 1.5772(1.6329) | Loss 16.9050(20.0619) | Error 0.5689(0.5859) Steps 0(0.00) | Grad Norm 8.5664(20.1009) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.3523(16.2564) | Bit/dim 4.3789(4.3907) | Xent 1.6154(1.6236) | Loss 16.8728(19.2319) | Error 0.5756(0.5828) Steps 0(0.00) | Grad Norm 18.0879(20.0110) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.9693(16.3262) | Bit/dim 4.3456(4.3745) | Xent 1.6488(1.6182) | Loss 16.6211(18.6269) | Error 0.6022(0.5835) Steps 0(0.00) | Grad Norm 27.0709(18.9915) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 15.8032(16.3207) | Bit/dim 4.4309(4.3710) | Xent 1.6260(1.6240) | Loss 17.1603(18.1792) | Error 0.5878(0.5849) Steps 0(0.00) | Grad Norm 25.9123(21.6573) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 17.4653(16.3717) | Bit/dim 4.3372(4.3628) | Xent 1.6248(1.6329) | Loss 16.6008(17.7966) | Error 0.6111(0.5900) Steps 0(0.00) | Grad Norm 19.3244(21.9134) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 83.8588, Epoch Time 1001.7352(833.5838), Bit/dim 4.3293(best: 4.4257), Xent 1.5089, Loss 5.0838, Error 0.5458(best: 0.5558)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.6003(16.4731) | Bit/dim 4.3145(4.3523) | Xent 1.6734(1.6210) | Loss 16.7897(20.3262) | Error 0.6167(0.5856) Steps 0(0.00) | Grad Norm 24.9204(20.4811) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.3086(16.4031) | Bit/dim 4.2984(4.3360) | Xent 1.5608(1.6052) | Loss 16.5992(19.3433) | Error 0.5578(0.5806) Steps 0(0.00) | Grad Norm 20.2379(18.3134) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 16.2721(16.3666) | Bit/dim 4.2565(4.3242) | Xent 1.5412(1.5904) | Loss 16.2811(18.6248) | Error 0.5733(0.5753) Steps 0(0.00) | Grad Norm 14.2366(17.2266) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.6562(16.4121) | Bit/dim 4.3160(4.3233) | Xent 1.9612(1.6056) | Loss 16.7503(18.1501) | Error 0.6822(0.5791) Steps 0(0.00) | Grad Norm 47.7822(19.7697) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.8628(16.5005) | Bit/dim 4.3077(4.3200) | Xent 1.6589(1.6294) | Loss 16.5970(17.7955) | Error 0.5978(0.5867) Steps 0(0.00) | Grad Norm 23.8536(19.7864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 81.7174, Epoch Time 1004.2297(838.7032), Bit/dim 4.2845(best: 4.3293), Xent 1.4969, Loss 5.0329, Error 0.5399(best: 0.5458)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 16.6896(16.4445) | Bit/dim 4.2510(4.3104) | Xent 1.5917(1.6158) | Loss 16.2674(20.5457) | Error 0.5889(0.5834) Steps 0(0.00) | Grad Norm 25.9057(18.9241) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 16.0026(16.3631) | Bit/dim 4.2503(4.2980) | Xent 1.5282(1.5942) | Loss 16.4155(19.4478) | Error 0.5722(0.5774) Steps 0(0.00) | Grad Norm 7.0513(17.6441) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.3763(16.3358) | Bit/dim 4.2062(4.2804) | Xent 1.5374(1.5781) | Loss 16.2803(18.6393) | Error 0.5700(0.5714) Steps 0(0.00) | Grad Norm 14.1311(15.9372) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 15.5647(16.3280) | Bit/dim 4.2421(4.2674) | Xent 1.5326(1.5605) | Loss 16.3359(18.0147) | Error 0.5633(0.5653) Steps 0(0.00) | Grad Norm 10.8582(14.6680) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 16.1220(16.3548) | Bit/dim 4.2134(4.2653) | Xent 1.5858(1.5658) | Loss 16.6703(17.6686) | Error 0.5833(0.5672) Steps 0(0.00) | Grad Norm 12.3659(17.1915) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 16.7513(16.4112) | Bit/dim 4.2111(4.2581) | Xent 1.5163(1.5663) | Loss 16.2169(17.3420) | Error 0.5522(0.5673) Steps 0(0.00) | Grad Norm 13.6410(17.5317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 84.3247, Epoch Time 1001.6481(843.5915), Bit/dim 4.2278(best: 4.2845), Xent 1.4669, Loss 4.9612, Error 0.5302(best: 0.5399)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.4506(16.3462) | Bit/dim 4.1959(4.2445) | Xent 1.5005(1.5527) | Loss 16.3812(19.8984) | Error 0.5500(0.5611) Steps 0(0.00) | Grad Norm 9.0074(15.8356) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 16.5262(16.3172) | Bit/dim 4.2085(4.2336) | Xent 1.4897(1.5402) | Loss 15.9524(18.9157) | Error 0.5389(0.5585) Steps 0(0.00) | Grad Norm 17.3394(15.1556) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 16.1231(16.2237) | Bit/dim 4.1436(4.2210) | Xent 1.4074(1.5155) | Loss 16.2078(18.2161) | Error 0.5144(0.5509) Steps 0(0.00) | Grad Norm 8.3183(15.0356) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 15.4567(16.2473) | Bit/dim 4.3543(4.2429) | Xent 1.5585(1.5283) | Loss 16.4381(17.7951) | Error 0.5422(0.5527) Steps 0(0.00) | Grad Norm 27.1864(18.5141) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 15.6777(16.1942) | Bit/dim 4.2341(4.2491) | Xent 1.4773(1.5320) | Loss 16.4116(17.4708) | Error 0.5444(0.5534) Steps 0(0.00) | Grad Norm 13.1225(17.7383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 79.5930, Epoch Time 984.9943(847.8336), Bit/dim 4.2070(best: 4.2278), Xent 1.4193, Loss 4.9167, Error 0.5146(best: 0.5302)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 15.6807(16.0865) | Bit/dim 4.1958(4.2380) | Xent 1.3810(1.5162) | Loss 15.8169(20.2521) | Error 0.4944(0.5484) Steps 0(0.00) | Grad Norm 10.1796(15.6184) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.5571(15.9512) | Bit/dim 4.1620(4.2249) | Xent 1.4431(1.5014) | Loss 15.7495(19.1512) | Error 0.5233(0.5418) Steps 0(0.00) | Grad Norm 11.5394(14.4387) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.6812(15.8468) | Bit/dim 4.1677(4.2085) | Xent 1.4121(1.4877) | Loss 16.4066(18.3717) | Error 0.5033(0.5377) Steps 0(0.00) | Grad Norm 5.8151(14.0406) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 16.1407(15.8291) | Bit/dim 4.1469(4.1923) | Xent 1.5188(1.4871) | Loss 16.2587(17.7773) | Error 0.5389(0.5377) Steps 0(0.00) | Grad Norm 14.7369(14.2237) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 15.5760(15.7625) | Bit/dim 4.1194(4.1743) | Xent 1.4985(1.4807) | Loss 15.8647(17.2873) | Error 0.5444(0.5361) Steps 0(0.00) | Grad Norm 28.1539(14.7577) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.0163(15.6875) | Bit/dim 4.1720(4.1648) | Xent 1.3953(1.4743) | Loss 16.1540(16.9355) | Error 0.5322(0.5351) Steps 0(0.00) | Grad Norm 10.4872(15.0086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 81.8304, Epoch Time 957.2481(851.1160), Bit/dim 4.1326(best: 4.2070), Xent 1.3763, Loss 4.8208, Error 0.4988(best: 0.5146)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 16.4118(15.6871) | Bit/dim 4.1560(4.1561) | Xent 1.4240(1.4618) | Loss 16.0765(19.4087) | Error 0.5167(0.5300) Steps 0(0.00) | Grad Norm 8.1726(14.7466) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 15.3650(15.6087) | Bit/dim 4.1477(4.1474) | Xent 1.3838(1.4527) | Loss 15.8251(18.4829) | Error 0.5044(0.5270) Steps 0(0.00) | Grad Norm 19.8455(15.2575) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.3629(15.5812) | Bit/dim 4.1070(4.1385) | Xent 1.4413(1.4513) | Loss 16.2310(17.7997) | Error 0.5256(0.5257) Steps 0(0.00) | Grad Norm 20.7419(15.3289) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.5208(15.6164) | Bit/dim 4.1597(4.1324) | Xent 1.4122(1.4474) | Loss 15.7431(17.3372) | Error 0.5211(0.5257) Steps 0(0.00) | Grad Norm 20.2773(15.7077) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.7801(15.6427) | Bit/dim 4.1208(4.1250) | Xent 1.4375(1.4385) | Loss 16.1276(16.9952) | Error 0.5378(0.5228) Steps 0(0.00) | Grad Norm 17.2590(15.1522) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 81.6673, Epoch Time 958.6523(854.3421), Bit/dim 4.1690(best: 4.1326), Xent 1.5458, Loss 4.9419, Error 0.5677(best: 0.4988)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 16.1127(15.6890) | Bit/dim 4.1604(4.1285) | Xent 1.5784(1.4519) | Loss 16.3841(20.0544) | Error 0.5833(0.5274) Steps 0(0.00) | Grad Norm 20.7722(17.9142) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.6792(15.6689) | Bit/dim 4.1059(4.1271) | Xent 1.4001(1.4510) | Loss 15.6401(18.9919) | Error 0.5267(0.5303) Steps 0(0.00) | Grad Norm 8.8764(16.5997) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 16.1380(15.7367) | Bit/dim 4.1237(4.1233) | Xent 1.3821(1.4378) | Loss 15.9563(18.1918) | Error 0.4800(0.5247) Steps 0(0.00) | Grad Norm 11.2334(15.2028) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.7341(15.7020) | Bit/dim 4.0914(4.1133) | Xent 1.3588(1.4271) | Loss 15.9366(17.5329) | Error 0.5122(0.5216) Steps 0(0.00) | Grad Norm 17.7237(14.5870) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.2027(15.7536) | Bit/dim 4.0911(4.1023) | Xent 1.4905(1.4123) | Loss 16.0877(17.0747) | Error 0.5344(0.5147) Steps 0(0.00) | Grad Norm 8.9275(12.9445) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 15.5854(15.7245) | Bit/dim 4.0847(4.0944) | Xent 1.3341(1.3989) | Loss 15.7789(16.7416) | Error 0.4922(0.5096) Steps 0(0.00) | Grad Norm 10.2731(12.7396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 81.3822, Epoch Time 964.4638(857.6458), Bit/dim 4.0708(best: 4.1326), Xent 1.3837, Loss 4.7626, Error 0.5081(best: 0.4988)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.3353(15.6934) | Bit/dim 4.0643(4.0867) | Xent 1.3924(1.3929) | Loss 15.5271(19.2095) | Error 0.4867(0.5046) Steps 0(0.00) | Grad Norm 17.3178(14.3179) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 15.8839(15.6432) | Bit/dim 4.0501(4.0773) | Xent 1.4663(1.3911) | Loss 16.1221(18.2915) | Error 0.5411(0.5030) Steps 0(0.00) | Grad Norm 20.5098(14.4430) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 15.4422(15.6582) | Bit/dim 4.0714(4.0767) | Xent 1.4394(1.3855) | Loss 15.8037(17.6345) | Error 0.5167(0.4989) Steps 0(0.00) | Grad Norm 18.5677(14.7739) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.7052(15.6395) | Bit/dim 4.0398(4.0676) | Xent 1.3256(1.3775) | Loss 15.5126(17.0994) | Error 0.4767(0.4955) Steps 0(0.00) | Grad Norm 5.9965(13.4248) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 15.6523(15.5786) | Bit/dim 4.0271(4.0634) | Xent 1.3935(1.3704) | Loss 15.7641(16.7014) | Error 0.5111(0.4950) Steps 0(0.00) | Grad Norm 10.7020(12.6910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 80.1134, Epoch Time 954.2143(860.5428), Bit/dim 4.0354(best: 4.0708), Xent 1.2612, Loss 4.6660, Error 0.4597(best: 0.4988)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.7237(15.5730) | Bit/dim 4.0165(4.0529) | Xent 1.2613(1.3588) | Loss 15.3245(19.5894) | Error 0.4444(0.4905) Steps 0(0.00) | Grad Norm 13.3433(12.2840) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 14.4508(15.4685) | Bit/dim 4.0458(4.0507) | Xent 1.3217(1.3544) | Loss 15.3199(18.5081) | Error 0.4711(0.4899) Steps 0(0.00) | Grad Norm 17.0777(14.0434) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 15.9514(15.5397) | Bit/dim 4.0403(4.0490) | Xent 1.4155(1.3593) | Loss 15.6892(17.7663) | Error 0.5144(0.4924) Steps 0(0.00) | Grad Norm 11.5328(15.2285) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 15.3900(15.5575) | Bit/dim 4.0672(4.0454) | Xent 1.3602(1.3579) | Loss 15.6688(17.1864) | Error 0.5078(0.4928) Steps 0(0.00) | Grad Norm 19.6183(15.0771) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.7446(15.5009) | Bit/dim 3.9923(4.0416) | Xent 1.4293(1.3627) | Loss 15.4631(16.7323) | Error 0.4956(0.4938) Steps 0(0.00) | Grad Norm 21.1209(15.7288) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 15.2783(15.4754) | Bit/dim 4.0196(4.0355) | Xent 1.3286(1.3612) | Loss 15.2154(16.3926) | Error 0.4756(0.4921) Steps 0(0.00) | Grad Norm 6.3783(15.2637) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 82.4076, Epoch Time 950.7672(863.2495), Bit/dim 4.0209(best: 4.0354), Xent 1.2596, Loss 4.6507, Error 0.4569(best: 0.4597)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 15.3668(15.5135) | Bit/dim 4.0174(4.0303) | Xent 1.2695(1.3450) | Loss 15.7892(18.9023) | Error 0.4322(0.4848) Steps 0(0.00) | Grad Norm 7.7044(13.3495) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 15.5805(15.4981) | Bit/dim 4.0336(4.0241) | Xent 1.3226(1.3316) | Loss 15.7423(18.0166) | Error 0.4789(0.4800) Steps 0(0.00) | Grad Norm 10.4444(12.4819) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 14.9808(15.4319) | Bit/dim 3.9728(4.0182) | Xent 1.2736(1.3273) | Loss 15.0069(17.2885) | Error 0.4600(0.4789) Steps 0(0.00) | Grad Norm 5.0863(12.9617) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 15.5873(15.4346) | Bit/dim 3.9963(4.0146) | Xent 1.3206(1.3231) | Loss 15.5205(16.7807) | Error 0.4800(0.4780) Steps 0(0.00) | Grad Norm 9.2543(11.7600) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 15.9679(15.4759) | Bit/dim 4.0157(4.0106) | Xent 1.3761(1.3222) | Loss 15.4812(16.4253) | Error 0.5144(0.4790) Steps 0(0.00) | Grad Norm 12.7014(11.8699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 80.4172, Epoch Time 950.9570(865.8808), Bit/dim 3.9905(best: 4.0209), Xent 1.2225, Loss 4.6018, Error 0.4454(best: 0.4569)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.1612(15.4826) | Bit/dim 3.9573(4.0076) | Xent 1.2195(1.3083) | Loss 15.1415(19.3107) | Error 0.4256(0.4742) Steps 0(0.00) | Grad Norm 17.5823(12.2437) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 15.6201(15.4523) | Bit/dim 3.9951(4.0027) | Xent 1.3306(1.3066) | Loss 15.5973(18.2746) | Error 0.4700(0.4730) Steps 0(0.00) | Grad Norm 24.5363(12.5780) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.7194(15.4538) | Bit/dim 4.0299(4.0008) | Xent 1.2382(1.3039) | Loss 15.3822(17.5194) | Error 0.4478(0.4717) Steps 0(0.00) | Grad Norm 14.0091(13.5420) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 16.2771(15.5014) | Bit/dim 3.9851(3.9981) | Xent 1.3122(1.2957) | Loss 15.3774(16.9477) | Error 0.4600(0.4707) Steps 0(0.00) | Grad Norm 6.0568(13.2503) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.2285(15.4890) | Bit/dim 3.9770(3.9936) | Xent 1.2864(1.2984) | Loss 15.6426(16.5137) | Error 0.4878(0.4727) Steps 0(0.00) | Grad Norm 10.4325(13.2533) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.4087(15.5441) | Bit/dim 3.9498(3.9864) | Xent 1.2561(1.2957) | Loss 14.8441(16.1732) | Error 0.4800(0.4728) Steps 0(0.00) | Grad Norm 7.7926(12.4288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 79.7992, Epoch Time 949.5748(868.3916), Bit/dim 3.9659(best: 3.9905), Xent 1.2206, Loss 4.5762, Error 0.4432(best: 0.4454)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.0045(15.5150) | Bit/dim 3.9752(3.9842) | Xent 1.2895(1.2942) | Loss 15.3893(18.6304) | Error 0.4800(0.4697) Steps 0(0.00) | Grad Norm 16.7229(13.3186) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.3205(15.5288) | Bit/dim 3.9545(3.9815) | Xent 1.3201(1.2907) | Loss 15.1009(17.7712) | Error 0.4644(0.4664) Steps 0(0.00) | Grad Norm 17.0821(13.3567) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.1254(15.5196) | Bit/dim 3.9752(3.9762) | Xent 1.2657(1.2831) | Loss 14.8117(17.0928) | Error 0.4622(0.4621) Steps 0(0.00) | Grad Norm 15.4045(12.6872) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 16.3562(15.5266) | Bit/dim 3.9583(3.9728) | Xent 1.2905(1.2821) | Loss 15.5657(16.6483) | Error 0.4800(0.4638) Steps 0(0.00) | Grad Norm 16.3175(12.6153) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 14.6951(15.5039) | Bit/dim 3.9749(3.9698) | Xent 1.2629(1.2814) | Loss 14.8629(16.2896) | Error 0.4578(0.4645) Steps 0(0.00) | Grad Norm 16.2309(12.2035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 79.9980, Epoch Time 950.2505(870.8474), Bit/dim 3.9543(best: 3.9659), Xent 1.1826, Loss 4.5456, Error 0.4283(best: 0.4432)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.4876(15.4006) | Bit/dim 3.9693(3.9668) | Xent 1.2199(1.2710) | Loss 15.2004(19.0478) | Error 0.4389(0.4601) Steps 0(0.00) | Grad Norm 8.0602(11.1143) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 14.7556(15.3489) | Bit/dim 3.9492(3.9612) | Xent 1.2715(1.2661) | Loss 15.2172(18.0238) | Error 0.4367(0.4582) Steps 0(0.00) | Grad Norm 10.6344(11.2261) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.5472(15.3170) | Bit/dim 3.9554(3.9572) | Xent 1.1849(1.2644) | Loss 14.9956(17.2843) | Error 0.4344(0.4580) Steps 0(0.00) | Grad Norm 8.0930(11.3919) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.0459(15.3106) | Bit/dim 3.9263(3.9527) | Xent 1.3618(1.2698) | Loss 15.3026(16.7361) | Error 0.4800(0.4595) Steps 0(0.00) | Grad Norm 26.5444(12.6858) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.2111(15.3259) | Bit/dim 3.9641(3.9569) | Xent 1.2556(1.2789) | Loss 15.4334(16.3331) | Error 0.4711(0.4617) Steps 0(0.00) | Grad Norm 9.5659(13.8054) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.1123(15.2692) | Bit/dim 3.9506(3.9517) | Xent 1.3061(1.2829) | Loss 15.3101(16.0280) | Error 0.4767(0.4649) Steps 0(0.00) | Grad Norm 10.5162(13.2439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 79.8000, Epoch Time 934.7420(872.7642), Bit/dim 3.9403(best: 3.9543), Xent 1.2168, Loss 4.5487, Error 0.4346(best: 0.4283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 15.3839(15.2820) | Bit/dim 3.9481(3.9497) | Xent 1.1933(1.2701) | Loss 15.0354(18.3770) | Error 0.4433(0.4619) Steps 0(0.00) | Grad Norm 8.6249(12.5618) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 15.3846(15.3317) | Bit/dim 3.9485(3.9466) | Xent 1.2092(1.2529) | Loss 15.0195(17.4794) | Error 0.4344(0.4549) Steps 0(0.00) | Grad Norm 11.7857(11.6801) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.2020(15.3384) | Bit/dim 3.9206(3.9435) | Xent 1.1873(1.2452) | Loss 14.9070(16.8048) | Error 0.4267(0.4516) Steps 0(0.00) | Grad Norm 9.5667(12.2709) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.2444(15.3732) | Bit/dim 3.9281(3.9406) | Xent 1.2301(1.2473) | Loss 14.6505(16.3382) | Error 0.4433(0.4524) Steps 0(0.00) | Grad Norm 6.2484(12.1916) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 15.5953(15.4250) | Bit/dim 3.9664(3.9360) | Xent 1.3081(1.2434) | Loss 15.2919(15.9874) | Error 0.4656(0.4516) Steps 0(0.00) | Grad Norm 15.9001(11.9250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 80.3965, Epoch Time 945.1241(874.9350), Bit/dim 3.9310(best: 3.9403), Xent 1.1514, Loss 4.5067, Error 0.4192(best: 0.4283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.1182(15.3750) | Bit/dim 3.9358(3.9324) | Xent 1.2388(1.2385) | Loss 15.3231(18.9569) | Error 0.4489(0.4496) Steps 0(0.00) | Grad Norm 19.1390(11.4146) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 15.3682(15.3545) | Bit/dim 3.9469(3.9300) | Xent 1.2000(1.2309) | Loss 14.9428(17.9186) | Error 0.4378(0.4467) Steps 0(0.00) | Grad Norm 14.8698(11.5582) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 15.4500(15.3356) | Bit/dim 3.9247(3.9264) | Xent 1.1886(1.2244) | Loss 14.9169(17.1584) | Error 0.4456(0.4461) Steps 0(0.00) | Grad Norm 23.3934(12.4056) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.7741(15.4065) | Bit/dim 3.8997(3.9254) | Xent 1.2552(1.2355) | Loss 15.1004(16.6244) | Error 0.4567(0.4475) Steps 0(0.00) | Grad Norm 15.4479(13.6377) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.3804(15.3792) | Bit/dim 3.9137(3.9244) | Xent 1.1908(1.2272) | Loss 14.9325(16.2052) | Error 0.4311(0.4449) Steps 0(0.00) | Grad Norm 8.7319(13.4331) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.1389(15.3714) | Bit/dim 3.9168(3.9225) | Xent 1.1639(1.2193) | Loss 14.6269(15.8549) | Error 0.4244(0.4426) Steps 0(0.00) | Grad Norm 5.4945(12.6201) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 79.2207, Epoch Time 942.6943(876.9678), Bit/dim 3.9099(best: 3.9310), Xent 1.1364, Loss 4.4781, Error 0.4092(best: 0.4192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 14.6899(15.3226) | Bit/dim 3.9413(3.9239) | Xent 1.2030(1.2073) | Loss 14.8376(18.2194) | Error 0.4378(0.4381) Steps 0(0.00) | Grad Norm 13.2280(12.0987) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.9643(15.2593) | Bit/dim 3.8948(3.9170) | Xent 1.1841(1.2001) | Loss 14.8766(17.3492) | Error 0.4189(0.4340) Steps 0(0.00) | Grad Norm 11.6660(11.6271) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.3260(15.2380) | Bit/dim 3.8870(3.9124) | Xent 1.2113(1.1997) | Loss 14.9171(16.7030) | Error 0.4522(0.4328) Steps 0(0.00) | Grad Norm 16.9640(12.0472) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.1768(15.2004) | Bit/dim 3.8774(3.9111) | Xent 1.2480(1.2005) | Loss 14.9552(16.2317) | Error 0.4544(0.4350) Steps 0(0.00) | Grad Norm 13.8820(12.7652) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.7494(15.2780) | Bit/dim 3.9141(3.9109) | Xent 1.2317(1.2063) | Loss 14.7081(15.8839) | Error 0.4233(0.4361) Steps 0(0.00) | Grad Norm 14.8835(13.4685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 79.1289, Epoch Time 933.6175(878.6673), Bit/dim 3.9095(best: 3.9099), Xent 1.1329, Loss 4.4759, Error 0.4075(best: 0.4092)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.3166(15.2792) | Bit/dim 3.9198(3.9120) | Xent 1.1084(1.2025) | Loss 14.5638(18.6871) | Error 0.3956(0.4329) Steps 0(0.00) | Grad Norm 15.5868(13.2312) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 16.3160(15.2934) | Bit/dim 3.8747(3.9104) | Xent 1.1307(1.1923) | Loss 15.0886(17.7102) | Error 0.3989(0.4299) Steps 0(0.00) | Grad Norm 5.7192(12.2159) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.5913(15.2615) | Bit/dim 3.8885(3.9059) | Xent 1.1759(1.1891) | Loss 14.7974(16.9488) | Error 0.4167(0.4276) Steps 0(0.00) | Grad Norm 16.4851(12.0388) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 14.6184(15.2637) | Bit/dim 3.9043(3.9018) | Xent 1.1707(1.1821) | Loss 14.6028(16.3861) | Error 0.4078(0.4265) Steps 0(0.00) | Grad Norm 12.9300(12.2675) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.7923(15.3782) | Bit/dim 3.8676(3.8969) | Xent 1.1696(1.1763) | Loss 14.9921(15.9882) | Error 0.4233(0.4244) Steps 0(0.00) | Grad Norm 9.8273(11.5868) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.6778(15.3774) | Bit/dim 3.8806(3.8951) | Xent 1.2031(1.1720) | Loss 14.8579(15.6712) | Error 0.4233(0.4216) Steps 0(0.00) | Grad Norm 12.0698(12.3694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 78.7868, Epoch Time 941.8642(880.5632), Bit/dim 3.8884(best: 3.9095), Xent 1.1034, Loss 4.4401, Error 0.3963(best: 0.4075)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 15.2005(15.3083) | Bit/dim 3.9057(3.8952) | Xent 1.1699(1.1664) | Loss 15.4801(18.0918) | Error 0.4278(0.4202) Steps 0(0.00) | Grad Norm 17.9859(12.8651) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.6719(15.3337) | Bit/dim 3.8497(3.8896) | Xent 1.0971(1.1571) | Loss 14.6762(17.2054) | Error 0.4167(0.4176) Steps 0(0.00) | Grad Norm 8.3732(12.2412) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 15.5415(15.3620) | Bit/dim 3.8867(3.8891) | Xent 1.1505(1.1499) | Loss 14.5068(16.5718) | Error 0.4244(0.4157) Steps 0(0.00) | Grad Norm 13.0165(11.7193) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.3084(15.3583) | Bit/dim 3.8844(3.8856) | Xent 1.1108(1.1475) | Loss 14.5960(16.0911) | Error 0.4011(0.4153) Steps 0(0.00) | Grad Norm 9.8453(11.7256) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 14.6062(15.3447) | Bit/dim 3.8937(3.8872) | Xent 1.2296(1.1504) | Loss 14.6735(15.7478) | Error 0.4322(0.4140) Steps 0(0.00) | Grad Norm 9.7049(11.6413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 77.9670, Epoch Time 938.5835(882.3038), Bit/dim 3.8868(best: 3.8884), Xent 1.0983, Loss 4.4359, Error 0.3988(best: 0.3963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.7208(15.3236) | Bit/dim 3.8695(3.8876) | Xent 1.1786(1.1547) | Loss 14.7464(18.5054) | Error 0.4178(0.4156) Steps 0(0.00) | Grad Norm 21.9188(12.7805) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.5085(15.2597) | Bit/dim 3.8719(3.8876) | Xent 1.1558(1.1468) | Loss 14.7681(17.5122) | Error 0.4178(0.4136) Steps 0(0.00) | Grad Norm 11.1635(12.9956) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.2401(15.2386) | Bit/dim 3.8982(3.8863) | Xent 1.1495(1.1425) | Loss 14.9983(16.7906) | Error 0.4278(0.4110) Steps 0(0.00) | Grad Norm 19.0640(13.3437) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.5447(15.2608) | Bit/dim 3.8715(3.8856) | Xent 1.1728(1.1563) | Loss 15.0108(16.3188) | Error 0.4222(0.4169) Steps 0(0.00) | Grad Norm 7.6133(13.5573) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.7619(15.3327) | Bit/dim 3.8305(3.8814) | Xent 1.1381(1.1434) | Loss 14.6939(15.9404) | Error 0.4078(0.4120) Steps 0(0.00) | Grad Norm 7.8089(12.7829) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.0824(15.2871) | Bit/dim 3.8364(3.8782) | Xent 1.1581(1.1394) | Loss 14.9800(15.6323) | Error 0.4144(0.4103) Steps 0(0.00) | Grad Norm 10.4108(12.6174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 78.6124, Epoch Time 935.5815(883.9021), Bit/dim 3.8769(best: 3.8868), Xent 1.0626, Loss 4.4082, Error 0.3819(best: 0.3963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 14.8289(15.2676) | Bit/dim 3.8703(3.8750) | Xent 1.1072(1.1260) | Loss 15.0622(18.1173) | Error 0.3711(0.4059) Steps 0(0.00) | Grad Norm 12.7465(11.9061) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 15.8140(15.2956) | Bit/dim 3.8801(3.8714) | Xent 1.0616(1.1218) | Loss 15.0389(17.2461) | Error 0.3911(0.4028) Steps 0(0.00) | Grad Norm 8.4992(12.0546) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.1781(15.2798) | Bit/dim 3.9171(3.8723) | Xent 1.0714(1.1128) | Loss 14.7241(16.6008) | Error 0.3756(0.3983) Steps 0(0.00) | Grad Norm 6.3375(11.3236) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.9198(15.3048) | Bit/dim 3.8468(3.8693) | Xent 1.1046(1.1024) | Loss 14.7646(16.1130) | Error 0.3989(0.3954) Steps 0(0.00) | Grad Norm 7.7328(10.6031) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 14.8053(15.2873) | Bit/dim 3.8853(3.8703) | Xent 1.1102(1.1046) | Loss 14.9781(15.7801) | Error 0.3967(0.3948) Steps 0(0.00) | Grad Norm 9.6251(10.7647) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 79.9277, Epoch Time 938.6663(885.5450), Bit/dim 3.8704(best: 3.8769), Xent 1.0550, Loss 4.3979, Error 0.3812(best: 0.3819)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.5146(15.2700) | Bit/dim 3.8645(3.8708) | Xent 1.1018(1.1052) | Loss 14.9576(18.7571) | Error 0.3933(0.3960) Steps 0(0.00) | Grad Norm 8.2024(11.6456) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.2275(15.2520) | Bit/dim 3.8717(3.8679) | Xent 1.0606(1.1018) | Loss 15.3315(17.7377) | Error 0.3800(0.3960) Steps 0(0.00) | Grad Norm 10.6956(11.6733) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 14.7318(15.1636) | Bit/dim 3.8739(3.8677) | Xent 1.0863(1.0903) | Loss 15.0253(16.9567) | Error 0.3811(0.3922) Steps 0(0.00) | Grad Norm 7.6625(11.1879) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.8284(15.2275) | Bit/dim 3.8504(3.8637) | Xent 1.0579(1.0921) | Loss 14.7619(16.3977) | Error 0.3833(0.3929) Steps 0(0.00) | Grad Norm 16.2247(11.9054) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 15.7559(15.2623) | Bit/dim 3.8712(3.8616) | Xent 1.0829(1.0962) | Loss 14.8474(15.9497) | Error 0.3933(0.3934) Steps 0(0.00) | Grad Norm 15.1505(12.0411) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.3461(15.2793) | Bit/dim 3.8522(3.8576) | Xent 1.0935(1.0934) | Loss 14.7000(15.6482) | Error 0.3922(0.3922) Steps 0(0.00) | Grad Norm 6.9545(11.3552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 79.6438, Epoch Time 934.6150(887.0171), Bit/dim 3.8595(best: 3.8704), Xent 1.0167, Loss 4.3678, Error 0.3645(best: 0.3812)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.2555(15.3163) | Bit/dim 3.8719(3.8598) | Xent 1.0506(1.0841) | Loss 14.6623(18.2277) | Error 0.3933(0.3905) Steps 0(0.00) | Grad Norm 8.4459(11.5567) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 16.1492(15.2595) | Bit/dim 3.8751(3.8565) | Xent 1.1172(1.0860) | Loss 14.9975(17.3245) | Error 0.4044(0.3910) Steps 0(0.00) | Grad Norm 10.1182(11.5926) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 14.9834(15.3462) | Bit/dim 3.8413(3.8542) | Xent 1.0460(1.0799) | Loss 14.7574(16.6548) | Error 0.3733(0.3885) Steps 0(0.00) | Grad Norm 6.8400(10.9821) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.6981(15.3851) | Bit/dim 3.8333(3.8506) | Xent 1.0334(1.0779) | Loss 14.4644(16.1745) | Error 0.3456(0.3870) Steps 0(0.00) | Grad Norm 12.7829(10.7609) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 16.3846(15.3513) | Bit/dim 3.8359(3.8479) | Xent 1.0679(1.0691) | Loss 14.6128(15.7739) | Error 0.3956(0.3848) Steps 0(0.00) | Grad Norm 11.0429(10.6247) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 78.9535, Epoch Time 941.2390(888.6438), Bit/dim 3.8470(best: 3.8595), Xent 1.0765, Loss 4.3852, Error 0.3854(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.1341(15.3179) | Bit/dim 3.8420(3.8477) | Xent 1.1360(1.0731) | Loss 14.9261(18.5021) | Error 0.4144(0.3859) Steps 0(0.00) | Grad Norm 17.9331(11.4447) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 14.5776(15.2517) | Bit/dim 3.8716(3.8520) | Xent 1.0581(1.0726) | Loss 14.3561(17.5078) | Error 0.3756(0.3847) Steps 0(0.00) | Grad Norm 8.1952(11.6238) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.0429(15.2236) | Bit/dim 3.8582(3.8505) | Xent 1.0458(1.0684) | Loss 14.9780(16.7999) | Error 0.3844(0.3837) Steps 0(0.00) | Grad Norm 16.1425(12.1938) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 14.5820(15.3042) | Bit/dim 3.8491(3.8489) | Xent 1.0074(1.0587) | Loss 14.5607(16.2775) | Error 0.3600(0.3804) Steps 0(0.00) | Grad Norm 7.2041(11.2329) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.8586(15.3425) | Bit/dim 3.8345(3.8458) | Xent 1.0273(1.0506) | Loss 15.0199(15.8638) | Error 0.3778(0.3775) Steps 0(0.00) | Grad Norm 9.0981(10.8985) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 14.9358(15.3145) | Bit/dim 3.8092(3.8403) | Xent 1.1179(1.0497) | Loss 14.9548(15.5638) | Error 0.3978(0.3773) Steps 0(0.00) | Grad Norm 15.3975(10.5503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 79.8194, Epoch Time 938.5480(890.1409), Bit/dim 3.8375(best: 3.8470), Xent 1.0140, Loss 4.3444, Error 0.3685(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.5314(15.3529) | Bit/dim 3.8154(3.8401) | Xent 1.0199(1.0544) | Loss 14.8263(18.0740) | Error 0.3600(0.3776) Steps 0(0.00) | Grad Norm 8.5950(11.1373) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.3339(15.3095) | Bit/dim 3.8310(3.8388) | Xent 1.0808(1.0571) | Loss 15.0627(17.1943) | Error 0.4011(0.3787) Steps 0(0.00) | Grad Norm 11.6541(12.1909) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 14.8379(15.3259) | Bit/dim 3.8482(3.8388) | Xent 1.0270(1.0604) | Loss 14.8927(16.5761) | Error 0.3644(0.3806) Steps 0(0.00) | Grad Norm 5.9541(12.1338) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 16.1682(15.3154) | Bit/dim 3.7943(3.8375) | Xent 1.0496(1.0549) | Loss 14.5024(16.0717) | Error 0.3689(0.3784) Steps 0(0.00) | Grad Norm 6.8315(11.6409) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.4976(15.3194) | Bit/dim 3.8242(3.8331) | Xent 1.0780(1.0528) | Loss 14.8561(15.7263) | Error 0.3700(0.3773) Steps 0(0.00) | Grad Norm 15.1857(11.6646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 80.9269, Epoch Time 940.2678(891.6447), Bit/dim 3.8243(best: 3.8375), Xent 0.9936, Loss 4.3211, Error 0.3522(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.3106(15.2733) | Bit/dim 3.8583(3.8327) | Xent 1.0078(1.0463) | Loss 14.7528(18.5221) | Error 0.3511(0.3745) Steps 0(0.00) | Grad Norm 13.7830(11.6585) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.2407(15.2983) | Bit/dim 3.8481(3.8317) | Xent 1.0406(1.0428) | Loss 14.8551(17.5052) | Error 0.3578(0.3734) Steps 0(0.00) | Grad Norm 10.3031(11.3164) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.6202(15.2952) | Bit/dim 3.8498(3.8280) | Xent 1.0533(1.0475) | Loss 14.9282(16.7773) | Error 0.3789(0.3744) Steps 0(0.00) | Grad Norm 9.0147(11.6463) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.7138(15.3072) | Bit/dim 3.8303(3.8267) | Xent 1.0553(1.0405) | Loss 15.1352(16.2075) | Error 0.3800(0.3718) Steps 0(0.00) | Grad Norm 11.3724(11.7799) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 14.5557(15.3600) | Bit/dim 3.8069(3.8260) | Xent 1.0275(1.0468) | Loss 14.4846(15.8281) | Error 0.3589(0.3739) Steps 0(0.00) | Grad Norm 9.5625(12.1701) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.3538(15.2850) | Bit/dim 3.8459(3.8288) | Xent 1.0712(1.0407) | Loss 14.6513(15.5344) | Error 0.3722(0.3714) Steps 0(0.00) | Grad Norm 15.5732(12.2653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 78.9543, Epoch Time 939.3807(893.0768), Bit/dim 3.8248(best: 3.8243), Xent 0.9891, Loss 4.3193, Error 0.3539(best: 0.3522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.8462(15.3162) | Bit/dim 3.8160(3.8241) | Xent 1.0198(1.0303) | Loss 14.4592(17.9341) | Error 0.3611(0.3681) Steps 0(0.00) | Grad Norm 12.1497(11.4215) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 15.6184(15.3394) | Bit/dim 3.8148(3.8240) | Xent 1.0346(1.0205) | Loss 14.5542(17.0679) | Error 0.3711(0.3642) Steps 0(0.00) | Grad Norm 9.4856(10.8963) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.5008(15.3545) | Bit/dim 3.7878(3.8226) | Xent 0.9564(1.0106) | Loss 14.8479(16.4494) | Error 0.3156(0.3597) Steps 0(0.00) | Grad Norm 5.2571(10.0851) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 15.0588(15.3534) | Bit/dim 3.8323(3.8222) | Xent 1.1116(1.0147) | Loss 14.8463(16.0002) | Error 0.3689(0.3601) Steps 0(0.00) | Grad Norm 16.6254(10.0061) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.1715(15.3168) | Bit/dim 3.8239(3.8182) | Xent 1.1802(1.0263) | Loss 14.6417(15.6442) | Error 0.4200(0.3640) Steps 0(0.00) | Grad Norm 19.2834(11.2007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 79.0703, Epoch Time 938.7377(894.4466), Bit/dim 3.8313(best: 3.8243), Xent 1.0161, Loss 4.3393, Error 0.3612(best: 0.3522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.7797(15.3145) | Bit/dim 3.8098(3.8215) | Xent 1.0406(1.0326) | Loss 14.5926(18.5646) | Error 0.3711(0.3658) Steps 0(0.00) | Grad Norm 8.9864(11.6547) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.2497(15.3491) | Bit/dim 3.8175(3.8184) | Xent 1.0244(1.0228) | Loss 14.6159(17.5146) | Error 0.3656(0.3628) Steps 0(0.00) | Grad Norm 9.0829(11.1543) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 14.8877(15.2768) | Bit/dim 3.8312(3.8172) | Xent 0.9864(1.0164) | Loss 14.8830(16.7649) | Error 0.3489(0.3607) Steps 0(0.00) | Grad Norm 15.6807(10.8088) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.6023(15.3239) | Bit/dim 3.8345(3.8193) | Xent 1.0734(1.0138) | Loss 14.6146(16.2072) | Error 0.3722(0.3606) Steps 0(0.00) | Grad Norm 10.7232(10.7709) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.0478(15.3098) | Bit/dim 3.7919(3.8127) | Xent 0.9670(1.0089) | Loss 14.6332(15.7923) | Error 0.3489(0.3604) Steps 0(0.00) | Grad Norm 6.2514(10.3663) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 14.7165(15.2782) | Bit/dim 3.8293(3.8133) | Xent 1.0226(1.0087) | Loss 14.7898(15.5284) | Error 0.3700(0.3610) Steps 0(0.00) | Grad Norm 11.3875(11.0589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 79.0803, Epoch Time 939.6345(895.8023), Bit/dim 3.8125(best: 3.8243), Xent 0.9794, Loss 4.3022, Error 0.3498(best: 0.3522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.6371(15.3352) | Bit/dim 3.8077(3.8144) | Xent 1.0019(1.0049) | Loss 14.5977(17.9800) | Error 0.3522(0.3613) Steps 0(0.00) | Grad Norm 9.2056(11.5205) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 14.9763(15.3450) | Bit/dim 3.8360(3.8140) | Xent 0.9681(0.9959) | Loss 14.7021(17.1103) | Error 0.3422(0.3577) Steps 0(0.00) | Grad Norm 8.6431(10.8979) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.6774(15.3552) | Bit/dim 3.8153(3.8103) | Xent 0.9762(0.9921) | Loss 14.4741(16.4738) | Error 0.3556(0.3562) Steps 0(0.00) | Grad Norm 5.5090(10.5495) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.6362(15.3714) | Bit/dim 3.8308(3.8067) | Xent 1.0738(0.9930) | Loss 15.0049(15.9888) | Error 0.4100(0.3569) Steps 0(0.00) | Grad Norm 9.2759(10.5724) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 15.8007(15.3600) | Bit/dim 3.8032(3.8083) | Xent 1.1406(1.0148) | Loss 14.7244(15.6321) | Error 0.4244(0.3630) Steps 0(0.00) | Grad Norm 16.6502(11.9835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 79.2467, Epoch Time 943.4344(897.2312), Bit/dim 3.8105(best: 3.8125), Xent 0.9925, Loss 4.3068, Error 0.3524(best: 0.3498)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.9314(15.3600) | Bit/dim 3.7933(3.8085) | Xent 0.9572(1.0184) | Loss 14.5009(18.6793) | Error 0.3344(0.3636) Steps 0(0.00) | Grad Norm 15.0987(11.6596) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.2097(15.3518) | Bit/dim 3.8230(3.8083) | Xent 0.9632(1.0055) | Loss 14.5504(17.6000) | Error 0.3500(0.3596) Steps 0(0.00) | Grad Norm 3.6658(11.5703) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.2479(15.3084) | Bit/dim 3.7977(3.8081) | Xent 0.9787(0.9980) | Loss 14.5374(16.8057) | Error 0.3433(0.3583) Steps 0(0.00) | Grad Norm 6.7664(11.1484) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 14.9644(15.2994) | Bit/dim 3.7795(3.8042) | Xent 0.9923(0.9967) | Loss 14.7057(16.2097) | Error 0.3856(0.3581) Steps 0(0.00) | Grad Norm 11.1254(10.8847) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 14.9394(15.2647) | Bit/dim 3.7244(3.8033) | Xent 1.0511(1.0026) | Loss 14.2656(15.7810) | Error 0.3611(0.3597) Steps 0(0.00) | Grad Norm 10.3365(11.5437) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 15.0340(15.3004) | Bit/dim 3.8047(3.8043) | Xent 0.9618(0.9999) | Loss 14.2764(15.4633) | Error 0.3256(0.3581) Steps 0(0.00) | Grad Norm 8.4714(11.2270) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 79.3677, Epoch Time 937.4524(898.4379), Bit/dim 3.7978(best: 3.8105), Xent 0.9525, Loss 4.2741, Error 0.3426(best: 0.3498)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.2768(15.3293) | Bit/dim 3.8166(3.8022) | Xent 0.9205(0.9878) | Loss 14.1911(17.8928) | Error 0.3289(0.3536) Steps 0(0.00) | Grad Norm 10.2925(10.4458) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.7155(15.2977) | Bit/dim 3.7682(3.8005) | Xent 0.9113(0.9798) | Loss 14.2663(17.0175) | Error 0.3167(0.3513) Steps 0(0.00) | Grad Norm 7.4697(10.2609) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 14.9195(15.2857) | Bit/dim 3.7907(3.7985) | Xent 0.9097(0.9754) | Loss 14.5076(16.3884) | Error 0.3256(0.3491) Steps 0(0.00) | Grad Norm 4.8105(10.1735) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 15.7715(15.3536) | Bit/dim 3.7854(3.7966) | Xent 0.8643(0.9733) | Loss 14.8450(15.9170) | Error 0.3144(0.3470) Steps 0(0.00) | Grad Norm 8.8763(10.2199) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.4905(15.3732) | Bit/dim 3.7734(3.7941) | Xent 0.8196(0.9661) | Loss 14.4311(15.5447) | Error 0.2922(0.3442) Steps 0(0.00) | Grad Norm 6.4707(9.8833) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 79.6144, Epoch Time 941.3349(899.7248), Bit/dim 3.7923(best: 3.7978), Xent 0.9220, Loss 4.2533, Error 0.3325(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.5435(15.3418) | Bit/dim 3.7693(3.7919) | Xent 0.9845(0.9614) | Loss 14.5126(18.4557) | Error 0.3678(0.3427) Steps 0(0.00) | Grad Norm 13.8586(9.5260) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 16.0591(15.4089) | Bit/dim 3.8190(3.7915) | Xent 0.9633(0.9647) | Loss 14.6140(17.4333) | Error 0.3556(0.3440) Steps 0(0.00) | Grad Norm 14.6698(10.1957) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.9641(15.4090) | Bit/dim 3.7733(3.7897) | Xent 0.9589(0.9571) | Loss 14.2266(16.6775) | Error 0.3500(0.3416) Steps 0(0.00) | Grad Norm 13.9708(10.1385) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 14.8965(15.3945) | Bit/dim 3.8190(3.7888) | Xent 0.9463(0.9565) | Loss 14.6678(16.1369) | Error 0.3456(0.3419) Steps 0(0.00) | Grad Norm 10.1430(9.9821) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.3839(15.4384) | Bit/dim 3.7942(3.7911) | Xent 1.1002(0.9674) | Loss 14.7357(15.7488) | Error 0.3833(0.3462) Steps 0(0.00) | Grad Norm 17.3553(11.0884) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.9737(15.4407) | Bit/dim 3.7642(3.7910) | Xent 0.9836(0.9746) | Loss 14.4991(15.4532) | Error 0.3389(0.3491) Steps 0(0.00) | Grad Norm 8.4870(11.6747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 79.8465, Epoch Time 948.1133(901.1764), Bit/dim 3.7863(best: 3.7923), Xent 0.9495, Loss 4.2610, Error 0.3365(best: 0.3325)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.9764(15.4796) | Bit/dim 3.7523(3.7891) | Xent 0.9123(0.9678) | Loss 14.6072(17.9890) | Error 0.3378(0.3467) Steps 0(0.00) | Grad Norm 9.4852(11.9799) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.6063(15.4690) | Bit/dim 3.7837(3.7891) | Xent 0.9176(0.9593) | Loss 14.4620(17.0861) | Error 0.3189(0.3412) Steps 0(0.00) | Grad Norm 7.2197(11.0200) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.3519(15.4127) | Bit/dim 3.7948(3.7860) | Xent 0.9370(0.9541) | Loss 14.8504(16.4559) | Error 0.3511(0.3401) Steps 0(0.00) | Grad Norm 11.5944(10.2939) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 15.4968(15.4632) | Bit/dim 3.7741(3.7882) | Xent 0.9181(0.9483) | Loss 14.6381(16.0153) | Error 0.3256(0.3388) Steps 0(0.00) | Grad Norm 9.7085(10.3799) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.0616(15.4909) | Bit/dim 3.8346(3.7909) | Xent 0.9924(0.9558) | Loss 14.7387(15.6762) | Error 0.3489(0.3409) Steps 0(0.00) | Grad Norm 17.1482(11.7015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 80.5892, Epoch Time 950.3144(902.6506), Bit/dim 3.7898(best: 3.7863), Xent 0.9797, Loss 4.2797, Error 0.3489(best: 0.3325)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.1533(15.3940) | Bit/dim 3.8126(3.7881) | Xent 0.9583(0.9606) | Loss 14.8683(18.5625) | Error 0.3333(0.3432) Steps 0(0.00) | Grad Norm 11.6803(11.4563) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.3433(15.3701) | Bit/dim 3.7840(3.7885) | Xent 0.9586(0.9609) | Loss 14.9211(17.5494) | Error 0.3333(0.3433) Steps 0(0.00) | Grad Norm 8.4206(11.8450) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 16.2418(15.4097) | Bit/dim 3.7699(3.7860) | Xent 0.9433(0.9566) | Loss 14.8273(16.7734) | Error 0.3311(0.3424) Steps 0(0.00) | Grad Norm 8.3056(11.7932) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.3178(15.3927) | Bit/dim 3.7972(3.7885) | Xent 1.0183(0.9670) | Loss 14.6977(16.2288) | Error 0.3589(0.3452) Steps 0(0.00) | Grad Norm 8.7888(11.8836) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.8490(15.4483) | Bit/dim 3.7547(3.7863) | Xent 0.9181(0.9663) | Loss 14.5966(15.7930) | Error 0.3356(0.3456) Steps 0(0.00) | Grad Norm 8.7833(11.6402) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.1776(15.4667) | Bit/dim 3.7543(3.7833) | Xent 0.9411(0.9639) | Loss 14.8241(15.5000) | Error 0.3544(0.3457) Steps 0(0.00) | Grad Norm 11.9801(11.4923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 79.7578, Epoch Time 944.5409(903.9073), Bit/dim 3.7792(best: 3.7863), Xent 0.9001, Loss 4.2292, Error 0.3212(best: 0.3325)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 14.8147(15.3829) | Bit/dim 3.7697(3.7811) | Xent 0.9337(0.9490) | Loss 14.4264(18.0444) | Error 0.3422(0.3400) Steps 0(0.00) | Grad Norm 10.0072(11.0316) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.3390(15.4229) | Bit/dim 3.7847(3.7772) | Xent 0.9305(0.9471) | Loss 14.3678(17.1384) | Error 0.3356(0.3387) Steps 0(0.00) | Grad Norm 9.0634(10.8051) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.8621(15.4184) | Bit/dim 3.7824(3.7771) | Xent 0.8743(0.9451) | Loss 14.4443(16.4563) | Error 0.3089(0.3374) Steps 0(0.00) | Grad Norm 6.0552(10.6723) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.9286(15.4657) | Bit/dim 3.7651(3.7785) | Xent 0.9362(0.9424) | Loss 14.7766(15.9781) | Error 0.3589(0.3378) Steps 0(0.00) | Grad Norm 8.7484(10.6981) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.9356(15.5057) | Bit/dim 3.7502(3.7772) | Xent 0.9340(0.9382) | Loss 14.9228(15.6086) | Error 0.3567(0.3349) Steps 0(0.00) | Grad Norm 13.4651(10.2524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 79.7885, Epoch Time 947.7763(905.2234), Bit/dim 3.7700(best: 3.7792), Xent 0.9154, Loss 4.2277, Error 0.3245(best: 0.3212)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.3514(15.5088) | Bit/dim 3.7563(3.7763) | Xent 0.8827(0.9320) | Loss 14.3898(18.4916) | Error 0.3044(0.3328) Steps 0(0.00) | Grad Norm 7.9179(10.2110) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.3429(15.4852) | Bit/dim 3.7716(3.7748) | Xent 0.8890(0.9297) | Loss 14.0642(17.4372) | Error 0.3122(0.3338) Steps 0(0.00) | Grad Norm 13.2149(9.8642) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.8536(15.4583) | Bit/dim 3.7536(3.7734) | Xent 0.9255(0.9313) | Loss 14.3328(16.6801) | Error 0.3233(0.3341) Steps 0(0.00) | Grad Norm 6.4519(10.5279) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.1142(15.4098) | Bit/dim 3.7810(3.7751) | Xent 0.9059(0.9244) | Loss 14.5982(16.1468) | Error 0.3378(0.3313) Steps 0(0.00) | Grad Norm 7.5059(9.8729) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.3736(15.4250) | Bit/dim 3.7531(3.7741) | Xent 0.9191(0.9345) | Loss 14.4025(15.7340) | Error 0.3244(0.3355) Steps 0(0.00) | Grad Norm 8.9440(10.8809) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.1189(15.4059) | Bit/dim 3.7552(3.7708) | Xent 0.8872(0.9345) | Loss 14.1765(15.4246) | Error 0.3411(0.3349) Steps 0(0.00) | Grad Norm 7.8962(10.5085) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 81.0771, Epoch Time 945.6605(906.4365), Bit/dim 3.7702(best: 3.7700), Xent 0.8840, Loss 4.2122, Error 0.3170(best: 0.3212)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.0947(15.4699) | Bit/dim 3.7596(3.7701) | Xent 0.9609(0.9284) | Loss 14.4222(17.9171) | Error 0.3244(0.3302) Steps 0(0.00) | Grad Norm 12.0667(10.6111) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.9080(15.4533) | Bit/dim 3.7794(3.7713) | Xent 0.9283(0.9277) | Loss 14.0789(17.0151) | Error 0.3344(0.3300) Steps 0(0.00) | Grad Norm 7.2164(10.6020) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.0628(15.5137) | Bit/dim 3.7481(3.7699) | Xent 0.9587(0.9265) | Loss 14.6127(16.3466) | Error 0.3333(0.3290) Steps 0(0.00) | Grad Norm 11.8242(10.7540) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.5074(15.5228) | Bit/dim 3.7645(3.7703) | Xent 0.8366(0.9197) | Loss 14.4962(15.8880) | Error 0.3011(0.3277) Steps 0(0.00) | Grad Norm 15.2473(11.5691) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.2664(15.5469) | Bit/dim 3.7854(3.7708) | Xent 0.9420(0.9259) | Loss 14.5903(15.5652) | Error 0.3444(0.3300) Steps 0(0.00) | Grad Norm 14.7111(11.1685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 81.6430, Epoch Time 957.5887(907.9710), Bit/dim 3.7765(best: 3.7700), Xent 0.8988, Loss 4.2259, Error 0.3178(best: 0.3170)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.1429(15.5298) | Bit/dim 3.7398(3.7695) | Xent 0.8856(0.9151) | Loss 14.2122(18.5674) | Error 0.3289(0.3264) Steps 0(0.00) | Grad Norm 11.4725(10.8972) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 16.2096(15.5406) | Bit/dim 3.7172(3.7676) | Xent 0.8643(0.9090) | Loss 14.4649(17.4970) | Error 0.2944(0.3244) Steps 0(0.00) | Grad Norm 9.1577(10.4337) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.0736(15.5506) | Bit/dim 3.7263(3.7673) | Xent 0.8568(0.9051) | Loss 14.9624(16.7298) | Error 0.3000(0.3230) Steps 0(0.00) | Grad Norm 9.3133(10.4602) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.4168(15.5046) | Bit/dim 3.7561(3.7674) | Xent 0.9272(0.9080) | Loss 14.5498(16.1651) | Error 0.3400(0.3249) Steps 0(0.00) | Grad Norm 10.5658(10.7103) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.7163(15.5195) | Bit/dim 3.7949(3.7681) | Xent 0.9165(0.9119) | Loss 14.6595(15.7996) | Error 0.3367(0.3250) Steps 0(0.00) | Grad Norm 16.4253(11.4677) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.2340(15.4777) | Bit/dim 3.7436(3.7678) | Xent 0.9513(0.9126) | Loss 14.2970(15.5069) | Error 0.3389(0.3257) Steps 0(0.00) | Grad Norm 9.3882(11.0781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 78.7635, Epoch Time 946.9919(909.1417), Bit/dim 3.7675(best: 3.7700), Xent 0.8790, Loss 4.2069, Error 0.3121(best: 0.3170)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.0604(15.5290) | Bit/dim 3.7926(3.7672) | Xent 0.8739(0.9093) | Loss 14.8541(17.9547) | Error 0.3067(0.3271) Steps 0(0.00) | Grad Norm 6.8372(11.0898) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.3534(15.4928) | Bit/dim 3.7703(3.7666) | Xent 0.9466(0.9134) | Loss 14.4499(17.0788) | Error 0.3244(0.3278) Steps 0(0.00) | Grad Norm 11.2329(11.1573) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.8942(15.4355) | Bit/dim 3.8051(3.7651) | Xent 0.9145(0.9157) | Loss 14.5988(16.4260) | Error 0.3256(0.3283) Steps 0(0.00) | Grad Norm 12.3777(10.7698) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.9435(15.4438) | Bit/dim 3.7645(3.7637) | Xent 0.8584(0.9082) | Loss 14.4200(15.9524) | Error 0.3167(0.3235) Steps 0(0.00) | Grad Norm 7.7593(10.1823) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.1157(15.4332) | Bit/dim 3.7573(3.7586) | Xent 0.9812(0.9054) | Loss 14.8309(15.5668) | Error 0.3411(0.3224) Steps 0(0.00) | Grad Norm 9.8381(9.8948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 81.5961, Epoch Time 948.6382(910.3266), Bit/dim 3.7559(best: 3.7675), Xent 0.8756, Loss 4.1938, Error 0.3140(best: 0.3121)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.0547(15.4356) | Bit/dim 3.7498(3.7604) | Xent 0.8602(0.8960) | Loss 14.3731(18.5431) | Error 0.2933(0.3185) Steps 0(0.00) | Grad Norm 15.2081(9.4249) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.0863(15.4230) | Bit/dim 3.7672(3.7614) | Xent 0.8829(0.8886) | Loss 14.8668(17.5014) | Error 0.3056(0.3157) Steps 0(0.00) | Grad Norm 9.2399(9.4782) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.5879(15.3984) | Bit/dim 3.7351(3.7577) | Xent 0.9715(0.8939) | Loss 14.5808(16.7285) | Error 0.3400(0.3188) Steps 0(0.00) | Grad Norm 9.1455(10.1722) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.2325(15.3741) | Bit/dim 3.7602(3.7590) | Xent 0.9811(0.9018) | Loss 14.7755(16.1714) | Error 0.3411(0.3207) Steps 0(0.00) | Grad Norm 13.1185(11.1057) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 15.2370(15.3680) | Bit/dim 3.7554(3.7570) | Xent 0.9075(0.8986) | Loss 14.5066(15.7459) | Error 0.3333(0.3205) Steps 0(0.00) | Grad Norm 6.6950(10.5823) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 15.7182(15.3468) | Bit/dim 3.7491(3.7536) | Xent 0.8544(0.8871) | Loss 14.5159(15.4048) | Error 0.3000(0.3153) Steps 0(0.00) | Grad Norm 6.1414(9.5972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 81.4944, Epoch Time 942.5425(911.2930), Bit/dim 3.7495(best: 3.7559), Xent 0.8456, Loss 4.1722, Error 0.3000(best: 0.3121)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 15.5215(15.4181) | Bit/dim 3.7473(3.7558) | Xent 0.7774(0.8705) | Loss 14.0063(17.8848) | Error 0.2833(0.3107) Steps 0(0.00) | Grad Norm 7.3619(9.1904) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 16.6418(15.4781) | Bit/dim 3.7223(3.7547) | Xent 0.8481(0.8761) | Loss 14.7712(17.0160) | Error 0.2978(0.3124) Steps 0(0.00) | Grad Norm 10.3043(9.5642) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.6264(15.4057) | Bit/dim 3.7721(3.7529) | Xent 0.8713(0.8763) | Loss 14.3858(16.3174) | Error 0.3122(0.3127) Steps 0(0.00) | Grad Norm 7.7865(9.4738) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 14.9041(15.3779) | Bit/dim 3.7147(3.7487) | Xent 0.8670(0.8670) | Loss 14.1288(15.8200) | Error 0.3267(0.3120) Steps 0(0.00) | Grad Norm 9.9345(9.4946) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.1480(15.3618) | Bit/dim 3.7677(3.7483) | Xent 0.9242(0.8771) | Loss 14.8985(15.5162) | Error 0.3244(0.3154) Steps 0(0.00) | Grad Norm 17.6557(10.2855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 81.1706, Epoch Time 948.1763(912.3995), Bit/dim 3.7551(best: 3.7495), Xent 0.8749, Loss 4.1926, Error 0.3093(best: 0.3000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.7479(15.4242) | Bit/dim 3.7549(3.7491) | Xent 0.8787(0.8798) | Loss 14.2459(18.4900) | Error 0.3078(0.3157) Steps 0(0.00) | Grad Norm 8.7392(11.0064) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.3863(15.3791) | Bit/dim 3.7590(3.7484) | Xent 0.7592(0.8707) | Loss 14.4911(17.4417) | Error 0.2778(0.3122) Steps 0(0.00) | Grad Norm 7.2049(9.9876) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.3582(15.3924) | Bit/dim 3.7527(3.7487) | Xent 0.9357(0.8748) | Loss 14.5162(16.7004) | Error 0.3133(0.3125) Steps 0(0.00) | Grad Norm 8.8810(9.7401) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.1696(15.4043) | Bit/dim 3.7364(3.7466) | Xent 0.8301(0.8691) | Loss 14.3486(16.1255) | Error 0.2878(0.3101) Steps 0(0.00) | Grad Norm 5.5942(9.3748) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.5254(15.3061) | Bit/dim 3.7757(3.7477) | Xent 0.8442(0.8645) | Loss 14.4078(15.6906) | Error 0.3056(0.3089) Steps 0(0.00) | Grad Norm 9.5537(9.1175) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.9198(15.2981) | Bit/dim 3.7271(3.7442) | Xent 0.8845(0.8697) | Loss 14.6708(15.3842) | Error 0.3133(0.3104) Steps 0(0.00) | Grad Norm 10.6537(9.9524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 82.1094, Epoch Time 942.4021(913.2996), Bit/dim 3.7461(best: 3.7495), Xent 0.8441, Loss 4.1682, Error 0.2983(best: 0.3000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.3448(15.3342) | Bit/dim 3.7400(3.7448) | Xent 0.8043(0.8581) | Loss 14.3455(17.9387) | Error 0.2922(0.3065) Steps 0(0.00) | Grad Norm 7.6502(9.6387) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.8406(15.3530) | Bit/dim 3.7546(3.7442) | Xent 0.8276(0.8619) | Loss 14.7993(17.0431) | Error 0.3000(0.3083) Steps 0(0.00) | Grad Norm 14.6757(10.3169) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.6764(15.3122) | Bit/dim 3.7347(3.7426) | Xent 0.8580(0.8596) | Loss 14.9026(16.3535) | Error 0.2956(0.3069) Steps 0(0.00) | Grad Norm 7.1716(10.1772) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.4972(15.3340) | Bit/dim 3.7512(3.7432) | Xent 0.8895(0.8579) | Loss 14.7293(15.8901) | Error 0.3033(0.3054) Steps 0(0.00) | Grad Norm 14.0561(10.1933) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.1955(15.3483) | Bit/dim 3.7542(3.7460) | Xent 0.8749(0.8624) | Loss 14.4783(15.5409) | Error 0.3111(0.3076) Steps 0(0.00) | Grad Norm 10.5215(10.2779) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 81.2343, Epoch Time 944.5478(914.2371), Bit/dim 3.7520(best: 3.7461), Xent 0.8903, Loss 4.1972, Error 0.3215(best: 0.2983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.2059(15.3348) | Bit/dim 3.7419(3.7464) | Xent 0.9279(0.8769) | Loss 14.4172(18.4844) | Error 0.3344(0.3126) Steps 0(0.00) | Grad Norm 10.7072(11.4262) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.5823(15.3188) | Bit/dim 3.7441(3.7539) | Xent 0.8104(0.8691) | Loss 14.3689(17.4528) | Error 0.3078(0.3096) Steps 0(0.00) | Grad Norm 10.5126(11.5203) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.2052(15.3188) | Bit/dim 3.7325(3.7529) | Xent 0.8631(0.8674) | Loss 14.2726(16.6981) | Error 0.3011(0.3087) Steps 0(0.00) | Grad Norm 11.0530(10.9144) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.4319(15.3166) | Bit/dim 3.7636(3.7501) | Xent 0.8328(0.8584) | Loss 14.6012(16.1106) | Error 0.2933(0.3055) Steps 0(0.00) | Grad Norm 7.3103(10.2214) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.7032(15.3333) | Bit/dim 3.6938(3.7436) | Xent 0.8994(0.8566) | Loss 14.5418(15.6943) | Error 0.3089(0.3045) Steps 0(0.00) | Grad Norm 10.8137(10.3140) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 16.1841(15.4135) | Bit/dim 3.7543(3.7431) | Xent 0.9163(0.8697) | Loss 14.4765(15.4019) | Error 0.3311(0.3094) Steps 0(0.00) | Grad Norm 16.7130(11.8942) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 82.0871, Epoch Time 944.9442(915.1583), Bit/dim 3.7541(best: 3.7461), Xent 0.8895, Loss 4.1989, Error 0.3113(best: 0.2983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.5716(15.4567) | Bit/dim 3.7394(3.7444) | Xent 0.8976(0.8721) | Loss 14.8683(17.9932) | Error 0.3267(0.3098) Steps 0(0.00) | Grad Norm 12.0111(12.1502) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.6443(15.4930) | Bit/dim 3.7416(3.7427) | Xent 0.8631(0.8701) | Loss 14.6715(17.1288) | Error 0.2933(0.3076) Steps 0(0.00) | Grad Norm 12.3786(12.0561) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.6271(15.4629) | Bit/dim 3.7798(3.7478) | Xent 0.8221(0.8636) | Loss 14.6915(16.4550) | Error 0.2911(0.3063) Steps 0(0.00) | Grad Norm 5.1854(11.4222) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.3172(15.5101) | Bit/dim 3.7418(3.7448) | Xent 0.8456(0.8540) | Loss 14.5696(15.9649) | Error 0.3156(0.3042) Steps 0(0.00) | Grad Norm 6.2616(10.1785) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.7323(15.5568) | Bit/dim 3.7220(3.7423) | Xent 0.8541(0.8445) | Loss 14.5952(15.5922) | Error 0.3022(0.3005) Steps 0(0.00) | Grad Norm 11.8004(9.8486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 81.9142, Epoch Time 954.6548(916.3432), Bit/dim 3.7320(best: 3.7461), Xent 0.8340, Loss 4.1490, Error 0.2944(best: 0.2983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.1075(15.4909) | Bit/dim 3.7269(3.7384) | Xent 0.7886(0.8361) | Loss 14.1872(18.5468) | Error 0.2822(0.2979) Steps 0(0.00) | Grad Norm 5.7808(9.4270) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.3700(15.4196) | Bit/dim 3.7131(3.7380) | Xent 0.8730(0.8321) | Loss 14.6400(17.5131) | Error 0.3144(0.2960) Steps 0(0.00) | Grad Norm 9.3323(9.5428) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.4006(15.4291) | Bit/dim 3.7189(3.7361) | Xent 0.8639(0.8314) | Loss 14.8748(16.7508) | Error 0.3100(0.2970) Steps 0(0.00) | Grad Norm 10.2635(9.6812) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.2817(15.4630) | Bit/dim 3.7396(3.7341) | Xent 0.8707(0.8386) | Loss 14.7259(16.1777) | Error 0.3100(0.2997) Steps 0(0.00) | Grad Norm 9.4965(9.8735) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.3884(15.4627) | Bit/dim 3.6989(3.7339) | Xent 0.9370(0.8465) | Loss 14.6440(15.7650) | Error 0.3167(0.3015) Steps 0(0.00) | Grad Norm 8.2043(9.7876) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 14.9958(15.4335) | Bit/dim 3.7827(3.7352) | Xent 0.8020(0.8441) | Loss 14.5633(15.4639) | Error 0.2833(0.3002) Steps 0(0.00) | Grad Norm 6.5389(9.5154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.9050, Epoch Time 946.3861(917.2445), Bit/dim 3.7393(best: 3.7320), Xent 0.8436, Loss 4.1611, Error 0.3013(best: 0.2944)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.1392(15.4205) | Bit/dim 3.7089(3.7338) | Xent 0.8345(0.8439) | Loss 14.4589(18.0732) | Error 0.3056(0.3012) Steps 0(0.00) | Grad Norm 9.9890(9.6116) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.1970(15.4198) | Bit/dim 3.7409(3.7337) | Xent 0.8107(0.8414) | Loss 14.6412(17.1514) | Error 0.3000(0.3007) Steps 0(0.00) | Grad Norm 7.9815(10.2820) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.6068(15.4999) | Bit/dim 3.7491(3.7330) | Xent 0.8762(0.8443) | Loss 14.1112(16.4209) | Error 0.3133(0.3006) Steps 0(0.00) | Grad Norm 14.7903(10.4819) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.3524(15.5409) | Bit/dim 3.6887(3.7326) | Xent 0.8941(0.8410) | Loss 14.5393(15.9127) | Error 0.3244(0.3001) Steps 0(0.00) | Grad Norm 9.6417(10.1621) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 16.1031(15.5883) | Bit/dim 3.7502(3.7317) | Xent 0.9110(0.8431) | Loss 14.4155(15.5549) | Error 0.3311(0.3021) Steps 0(0.00) | Grad Norm 9.8974(9.9274) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 82.6081, Epoch Time 957.3051(918.4463), Bit/dim 3.7274(best: 3.7320), Xent 0.8221, Loss 4.1384, Error 0.2931(best: 0.2944)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.1285(15.5361) | Bit/dim 3.7475(3.7345) | Xent 0.7798(0.8358) | Loss 14.2273(18.5446) | Error 0.2800(0.2993) Steps 0(0.00) | Grad Norm 5.6610(9.5898) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 16.4776(15.5531) | Bit/dim 3.7116(3.7331) | Xent 0.7847(0.8285) | Loss 14.4265(17.4552) | Error 0.2844(0.2965) Steps 0(0.00) | Grad Norm 9.2881(9.1459) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.6260(15.6229) | Bit/dim 3.7019(3.7294) | Xent 0.7818(0.8164) | Loss 13.8736(16.6413) | Error 0.2833(0.2916) Steps 0(0.00) | Grad Norm 6.9011(8.3812) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.6363(15.5845) | Bit/dim 3.7589(3.7273) | Xent 0.8855(0.8158) | Loss 14.5827(16.0471) | Error 0.3067(0.2919) Steps 0(0.00) | Grad Norm 13.1609(8.7576) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.3257(15.5775) | Bit/dim 3.7432(3.7301) | Xent 0.8180(0.8169) | Loss 14.5962(15.6315) | Error 0.2878(0.2908) Steps 0(0.00) | Grad Norm 7.6360(9.1193) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.8369(15.5956) | Bit/dim 3.7463(3.7312) | Xent 0.8435(0.8228) | Loss 14.8725(15.3441) | Error 0.3078(0.2942) Steps 0(0.00) | Grad Norm 14.7864(10.4919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 81.5420, Epoch Time 955.9689(919.5720), Bit/dim 3.7316(best: 3.7274), Xent 0.8165, Loss 4.1398, Error 0.2896(best: 0.2931)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.0676(15.5976) | Bit/dim 3.7639(3.7316) | Xent 0.8643(0.8183) | Loss 14.3768(17.8673) | Error 0.3178(0.2920) Steps 0(0.00) | Grad Norm 12.2876(10.5636) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.4330(15.5729) | Bit/dim 3.7592(3.7302) | Xent 0.7872(0.8202) | Loss 14.4468(16.9886) | Error 0.2756(0.2925) Steps 0(0.00) | Grad Norm 9.5896(10.6957) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.1937(15.6164) | Bit/dim 3.7520(3.7292) | Xent 0.8492(0.8206) | Loss 14.2449(16.3269) | Error 0.3156(0.2937) Steps 0(0.00) | Grad Norm 9.6629(10.2396) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.2653(15.5948) | Bit/dim 3.7541(3.7284) | Xent 0.8573(0.8175) | Loss 14.6488(15.8436) | Error 0.3000(0.2923) Steps 0(0.00) | Grad Norm 7.0504(9.5653) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.7981(15.6113) | Bit/dim 3.7251(3.7255) | Xent 0.7681(0.8135) | Loss 14.3359(15.4734) | Error 0.2833(0.2910) Steps 0(0.00) | Grad Norm 8.6085(9.8040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 81.5669, Epoch Time 958.5625(920.7417), Bit/dim 3.7176(best: 3.7274), Xent 0.7905, Loss 4.1129, Error 0.2832(best: 0.2896)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 16.0619(15.6143) | Bit/dim 3.6714(3.7255) | Xent 0.8289(0.8070) | Loss 14.4578(18.3501) | Error 0.2844(0.2882) Steps 0(0.00) | Grad Norm 18.4556(9.8779) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.0517(15.6341) | Bit/dim 3.7105(3.7253) | Xent 0.8182(0.8057) | Loss 14.1789(17.3337) | Error 0.3156(0.2890) Steps 0(0.00) | Grad Norm 5.5587(9.7684) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.3910(15.6419) | Bit/dim 3.7363(3.7247) | Xent 0.8080(0.8075) | Loss 14.3940(16.5686) | Error 0.2833(0.2902) Steps 0(0.00) | Grad Norm 4.6049(10.3836) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.2457(15.6100) | Bit/dim 3.7137(3.7237) | Xent 0.7729(0.8085) | Loss 14.4361(16.0101) | Error 0.2700(0.2897) Steps 0(0.00) | Grad Norm 7.1557(9.5950) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.4809(15.6503) | Bit/dim 3.7391(3.7260) | Xent 0.7455(0.7977) | Loss 14.2879(15.5889) | Error 0.2733(0.2856) Steps 0(0.00) | Grad Norm 6.8696(9.2641) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.9711(15.6966) | Bit/dim 3.7294(3.7232) | Xent 0.7751(0.7992) | Loss 14.4692(15.3113) | Error 0.2800(0.2861) Steps 0(0.00) | Grad Norm 14.4038(9.4066) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 82.6517, Epoch Time 963.4485(922.0229), Bit/dim 3.7176(best: 3.7176), Xent 0.8135, Loss 4.1244, Error 0.2853(best: 0.2832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.1562(15.6310) | Bit/dim 3.7282(3.7196) | Xent 0.7400(0.7944) | Loss 14.4950(17.7321) | Error 0.2756(0.2849) Steps 0(0.00) | Grad Norm 10.2553(9.6048) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.0417(15.6312) | Bit/dim 3.7243(3.7212) | Xent 0.7562(0.7902) | Loss 14.5996(16.8749) | Error 0.2667(0.2824) Steps 0(0.00) | Grad Norm 7.3702(9.3351) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.7105(15.5799) | Bit/dim 3.7094(3.7189) | Xent 0.9081(0.7990) | Loss 14.5573(16.1993) | Error 0.3200(0.2839) Steps 0(0.00) | Grad Norm 16.0111(9.9999) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.0650(15.6289) | Bit/dim 3.7039(3.7204) | Xent 0.8612(0.7984) | Loss 14.7122(15.7632) | Error 0.3144(0.2853) Steps 0(0.00) | Grad Norm 10.5260(10.4186) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.0699(15.7524) | Bit/dim 3.7331(3.7227) | Xent 0.8020(0.7942) | Loss 14.5439(15.4477) | Error 0.2878(0.2830) Steps 0(0.00) | Grad Norm 13.2972(9.7678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 83.6678, Epoch Time 965.3833(923.3237), Bit/dim 3.7240(best: 3.7176), Xent 0.8122, Loss 4.1300, Error 0.2848(best: 0.2832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.1850(15.7797) | Bit/dim 3.6849(3.7237) | Xent 0.7101(0.7896) | Loss 14.3563(18.5103) | Error 0.2578(0.2824) Steps 0(0.00) | Grad Norm 8.2855(9.7086) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.7941(15.7609) | Bit/dim 3.7254(3.7230) | Xent 0.7966(0.7775) | Loss 14.6464(17.4369) | Error 0.2867(0.2774) Steps 0(0.00) | Grad Norm 8.3504(9.2446) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.2504(15.8273) | Bit/dim 3.7439(3.7202) | Xent 0.7632(0.7810) | Loss 14.4249(16.6659) | Error 0.2789(0.2787) Steps 0(0.00) | Grad Norm 6.9970(9.1002) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 16.5017(15.8280) | Bit/dim 3.7165(3.7190) | Xent 0.7897(0.7807) | Loss 14.8093(16.0876) | Error 0.2778(0.2797) Steps 0(0.00) | Grad Norm 13.6054(8.9142) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 15.7167(15.8295) | Bit/dim 3.7176(3.7200) | Xent 0.8224(0.7829) | Loss 14.7476(15.6662) | Error 0.2878(0.2801) Steps 0(0.00) | Grad Norm 6.6812(9.5915) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.9962(15.8322) | Bit/dim 3.7252(3.7211) | Xent 0.8346(0.7898) | Loss 14.4180(15.3414) | Error 0.2922(0.2831) Steps 0(0.00) | Grad Norm 6.5334(9.9368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 82.7858, Epoch Time 971.3392(924.7642), Bit/dim 3.7204(best: 3.7176), Xent 0.7958, Loss 4.1183, Error 0.2853(best: 0.2832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.0720(15.8488) | Bit/dim 3.7302(3.7201) | Xent 0.7242(0.7790) | Loss 14.3221(17.7994) | Error 0.2600(0.2789) Steps 0(0.00) | Grad Norm 9.6097(9.6588) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 15.4252(15.8062) | Bit/dim 3.7262(3.7179) | Xent 0.8943(0.7796) | Loss 14.4956(16.9316) | Error 0.3133(0.2795) Steps 0(0.00) | Grad Norm 16.4433(10.0215) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 14.8235(15.7890) | Bit/dim 3.7285(3.7191) | Xent 0.8173(0.7882) | Loss 14.2641(16.2964) | Error 0.2800(0.2803) Steps 0(0.00) | Grad Norm 7.1939(10.3733) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 15.6761(15.6918) | Bit/dim 3.6884(3.7177) | Xent 0.7868(0.7890) | Loss 14.5371(15.8342) | Error 0.2767(0.2793) Steps 0(0.00) | Grad Norm 6.2956(9.9147) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.6277(15.7636) | Bit/dim 3.7106(3.7157) | Xent 0.7483(0.7852) | Loss 14.7490(15.4780) | Error 0.2544(0.2773) Steps 0(0.00) | Grad Norm 10.9644(9.8476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 82.9889, Epoch Time 966.7455(926.0236), Bit/dim 3.7161(best: 3.7176), Xent 0.8190, Loss 4.1256, Error 0.2914(best: 0.2832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.4977(15.7969) | Bit/dim 3.7427(3.7171) | Xent 0.7649(0.7834) | Loss 14.5059(18.5960) | Error 0.2711(0.2770) Steps 0(0.00) | Grad Norm 5.4838(9.6990) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 14.9535(15.7766) | Bit/dim 3.7029(3.7164) | Xent 0.7592(0.7822) | Loss 14.5584(17.5578) | Error 0.2689(0.2782) Steps 0(0.00) | Grad Norm 6.9417(9.5225) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.7319(15.7774) | Bit/dim 3.7229(3.7144) | Xent 0.7625(0.7767) | Loss 14.6963(16.7723) | Error 0.2722(0.2759) Steps 0(0.00) | Grad Norm 10.0208(9.3591) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.6203(15.8178) | Bit/dim 3.7239(3.7131) | Xent 0.7770(0.7749) | Loss 14.7306(16.1578) | Error 0.2811(0.2749) Steps 0(0.00) | Grad Norm 7.3027(8.8467) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.6847(15.8317) | Bit/dim 3.7083(3.7124) | Xent 0.7808(0.7757) | Loss 14.5882(15.7359) | Error 0.3022(0.2760) Steps 0(0.00) | Grad Norm 7.7922(9.1345) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.5058(15.8644) | Bit/dim 3.7345(3.7126) | Xent 0.8080(0.7792) | Loss 14.7174(15.4667) | Error 0.2911(0.2780) Steps 0(0.00) | Grad Norm 20.0278(10.4806) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 85.0552, Epoch Time 976.2565(927.5306), Bit/dim 3.7119(best: 3.7161), Xent 0.7778, Loss 4.1008, Error 0.2765(best: 0.2832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.8471(15.8457) | Bit/dim 3.6560(3.7135) | Xent 0.7002(0.7698) | Loss 14.6541(17.9728) | Error 0.2500(0.2740) Steps 0(0.00) | Grad Norm 5.2795(9.8061) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.5738(15.7656) | Bit/dim 3.6709(3.7122) | Xent 0.7557(0.7591) | Loss 14.7372(17.0468) | Error 0.2489(0.2702) Steps 0(0.00) | Grad Norm 6.8916(9.0605) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.5352(15.8501) | Bit/dim 3.7366(3.7121) | Xent 0.8802(0.7628) | Loss 14.6318(16.4179) | Error 0.3033(0.2722) Steps 0(0.00) | Grad Norm 15.3813(9.3756) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.7214(15.8585) | Bit/dim 3.6954(3.7131) | Xent 0.8264(0.7725) | Loss 14.8046(15.9299) | Error 0.3044(0.2767) Steps 0(0.00) | Grad Norm 11.2592(9.8482) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.2738(15.8215) | Bit/dim 3.7180(3.7093) | Xent 0.7326(0.7695) | Loss 14.6256(15.5354) | Error 0.2611(0.2735) Steps 0(0.00) | Grad Norm 8.9723(9.2963) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 84.5070, Epoch Time 973.1229(928.8983), Bit/dim 3.7121(best: 3.7119), Xent 0.7849, Loss 4.1046, Error 0.2805(best: 0.2765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.2652(15.8546) | Bit/dim 3.7424(3.7106) | Xent 0.7844(0.7687) | Loss 14.5851(18.5999) | Error 0.3000(0.2743) Steps 0(0.00) | Grad Norm 12.4505(9.5917) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.6899(15.8060) | Bit/dim 3.7069(3.7093) | Xent 0.7959(0.7667) | Loss 14.9164(17.5270) | Error 0.3078(0.2740) Steps 0(0.00) | Grad Norm 10.8312(9.7005) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 16.2179(15.8591) | Bit/dim 3.6957(3.7085) | Xent 0.7256(0.7604) | Loss 14.5314(16.7272) | Error 0.2689(0.2726) Steps 0(0.00) | Grad Norm 4.4632(9.1410) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 16.1510(15.8504) | Bit/dim 3.7136(3.7104) | Xent 0.7709(0.7585) | Loss 14.4923(16.1320) | Error 0.2733(0.2715) Steps 0(0.00) | Grad Norm 8.2879(8.7206) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.9653(15.8176) | Bit/dim 3.7086(3.7082) | Xent 0.7739(0.7600) | Loss 14.7349(15.6964) | Error 0.2656(0.2708) Steps 0(0.00) | Grad Norm 5.7960(8.9570) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 14.4385(15.7186) | Bit/dim 3.6941(3.7033) | Xent 0.7665(0.7591) | Loss 14.0997(15.3418) | Error 0.2600(0.2703) Steps 0(0.00) | Grad Norm 9.8482(9.2609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 83.4128, Epoch Time 967.0028(930.0415), Bit/dim 3.7107(best: 3.7119), Xent 0.8190, Loss 4.1202, Error 0.2882(best: 0.2765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.5002(15.7208) | Bit/dim 3.6992(3.7070) | Xent 0.7821(0.7541) | Loss 14.6239(17.9769) | Error 0.2656(0.2696) Steps 0(0.00) | Grad Norm 10.6648(9.4316) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 16.5744(15.8016) | Bit/dim 3.6951(3.7046) | Xent 0.6867(0.7473) | Loss 14.1483(17.0562) | Error 0.2433(0.2661) Steps 0(0.00) | Grad Norm 4.9927(9.2781) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 15.4226(15.7864) | Bit/dim 3.7227(3.7040) | Xent 0.6703(0.7440) | Loss 14.6584(16.3852) | Error 0.2356(0.2662) Steps 0(0.00) | Grad Norm 7.5998(8.5287) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.7301(15.7669) | Bit/dim 3.6729(3.7009) | Xent 0.7529(0.7478) | Loss 14.5721(15.8760) | Error 0.2611(0.2674) Steps 0(0.00) | Grad Norm 10.5789(9.2995) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.6775(15.7435) | Bit/dim 3.7251(3.7025) | Xent 0.8303(0.7610) | Loss 14.6099(15.5233) | Error 0.2911(0.2714) Steps 0(0.00) | Grad Norm 14.7730(10.4347) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 83.7289, Epoch Time 971.3282(931.2801), Bit/dim 3.7044(best: 3.7107), Xent 0.8178, Loss 4.1133, Error 0.2840(best: 0.2765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.3087(15.8295) | Bit/dim 3.7063(3.7067) | Xent 0.6461(0.7582) | Loss 14.7804(18.5193) | Error 0.2211(0.2701) Steps 0(0.00) | Grad Norm 5.7798(10.2438) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.4923(15.8225) | Bit/dim 3.7295(3.7056) | Xent 0.6945(0.7531) | Loss 14.7692(17.4776) | Error 0.2544(0.2695) Steps 0(0.00) | Grad Norm 6.0612(9.6221) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.0794(15.7916) | Bit/dim 3.6745(3.7016) | Xent 0.7403(0.7518) | Loss 14.6774(16.7105) | Error 0.2622(0.2681) Steps 0(0.00) | Grad Norm 11.2190(9.2194) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.2355(15.8106) | Bit/dim 3.7043(3.7017) | Xent 0.7296(0.7465) | Loss 14.7175(16.1352) | Error 0.2511(0.2676) Steps 0(0.00) | Grad Norm 8.6164(9.2038) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.4867(15.9145) | Bit/dim 3.6849(3.7011) | Xent 0.7485(0.7466) | Loss 14.5564(15.6944) | Error 0.2656(0.2678) Steps 0(0.00) | Grad Norm 9.3661(8.9250) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 15.6672(15.8931) | Bit/dim 3.7013(3.7013) | Xent 0.7519(0.7440) | Loss 14.3935(15.3675) | Error 0.2567(0.2665) Steps 0(0.00) | Grad Norm 7.9986(8.2160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 84.5021, Epoch Time 976.5029(932.6368), Bit/dim 3.6978(best: 3.7044), Xent 0.7562, Loss 4.0759, Error 0.2702(best: 0.2765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.7643(15.9523) | Bit/dim 3.7273(3.7001) | Xent 0.7524(0.7371) | Loss 14.9139(17.9997) | Error 0.2800(0.2651) Steps 0(0.00) | Grad Norm 13.1418(8.3217) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.6589(15.9340) | Bit/dim 3.7230(3.7027) | Xent 0.7480(0.7423) | Loss 14.3439(17.0952) | Error 0.2756(0.2661) Steps 0(0.00) | Grad Norm 7.7220(8.7179) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.5027(15.8835) | Bit/dim 3.7212(3.7046) | Xent 0.6933(0.7370) | Loss 14.5651(16.3966) | Error 0.2533(0.2648) Steps 0(0.00) | Grad Norm 8.9848(8.9828) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 16.3024(15.9481) | Bit/dim 3.6539(3.7026) | Xent 0.7310(0.7387) | Loss 14.5930(15.8785) | Error 0.2711(0.2652) Steps 0(0.00) | Grad Norm 6.0343(8.8034) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.6470(16.0555) | Bit/dim 3.6951(3.7018) | Xent 0.7140(0.7312) | Loss 14.6582(15.5057) | Error 0.2656(0.2626) Steps 0(0.00) | Grad Norm 5.2031(8.6022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 84.7123, Epoch Time 985.1449(934.2120), Bit/dim 3.7005(best: 3.6978), Xent 0.7798, Loss 4.0904, Error 0.2717(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.3860(16.0832) | Bit/dim 3.6857(3.7015) | Xent 0.6821(0.7322) | Loss 14.3140(18.5888) | Error 0.2544(0.2630) Steps 0(0.00) | Grad Norm 9.1793(8.7619) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.1424(16.1579) | Bit/dim 3.7198(3.7027) | Xent 0.7188(0.7297) | Loss 14.4847(17.5028) | Error 0.2556(0.2616) Steps 0(0.00) | Grad Norm 9.8459(8.9808) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 15.8170(16.0789) | Bit/dim 3.6968(3.6999) | Xent 0.8085(0.7277) | Loss 14.4147(16.6973) | Error 0.3000(0.2621) Steps 0(0.00) | Grad Norm 8.6144(9.3267) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.9105(16.0184) | Bit/dim 3.7251(3.6982) | Xent 0.7327(0.7315) | Loss 14.4255(16.1011) | Error 0.2678(0.2631) Steps 0(0.00) | Grad Norm 11.4740(9.4051) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.6942(15.9542) | Bit/dim 3.7299(3.6991) | Xent 0.7578(0.7336) | Loss 14.3765(15.6783) | Error 0.2589(0.2636) Steps 0(0.00) | Grad Norm 8.3642(9.5153) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 16.2581(15.9564) | Bit/dim 3.7619(3.7015) | Xent 0.8286(0.7416) | Loss 14.8169(15.3503) | Error 0.2989(0.2659) Steps 0(0.00) | Grad Norm 14.6495(10.0224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 85.0555, Epoch Time 981.7336(935.6377), Bit/dim 3.7013(best: 3.6978), Xent 0.7603, Loss 4.0814, Error 0.2672(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.8265(15.9607) | Bit/dim 3.6639(3.6991) | Xent 0.7105(0.7369) | Loss 14.3692(17.9735) | Error 0.2478(0.2631) Steps 0(0.00) | Grad Norm 7.1385(9.9451) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.5562(15.9406) | Bit/dim 3.6724(3.6968) | Xent 0.7203(0.7303) | Loss 14.2103(17.0295) | Error 0.2478(0.2589) Steps 0(0.00) | Grad Norm 8.1792(9.3703) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 16.0138(15.9850) | Bit/dim 3.7273(3.6980) | Xent 0.7644(0.7352) | Loss 14.3744(16.3656) | Error 0.2611(0.2604) Steps 0(0.00) | Grad Norm 6.7360(10.1867) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 15.3146(15.9382) | Bit/dim 3.7100(3.6980) | Xent 0.7680(0.7388) | Loss 14.5913(15.8794) | Error 0.2811(0.2640) Steps 0(0.00) | Grad Norm 11.8748(9.9067) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.7029(15.9514) | Bit/dim 3.7046(3.6984) | Xent 0.6976(0.7378) | Loss 14.2535(15.4906) | Error 0.2400(0.2626) Steps 0(0.00) | Grad Norm 7.4701(9.2441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 84.9386, Epoch Time 980.0031(936.9686), Bit/dim 3.6977(best: 3.6978), Xent 0.7667, Loss 4.0811, Error 0.2674(best: 0.2672)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 16.0818(15.9489) | Bit/dim 3.7161(3.6969) | Xent 0.7916(0.7316) | Loss 14.6874(18.5181) | Error 0.2856(0.2612) Steps 0(0.00) | Grad Norm 12.9181(9.0435) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 15.6062(15.9689) | Bit/dim 3.7013(3.6954) | Xent 0.6829(0.7258) | Loss 14.2096(17.4343) | Error 0.2389(0.2590) Steps 0(0.00) | Grad Norm 6.0704(9.1209) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 16.1902(15.9961) | Bit/dim 3.6890(3.6956) | Xent 0.7609(0.7280) | Loss 14.5241(16.6535) | Error 0.2600(0.2596) Steps 0(0.00) | Grad Norm 10.7589(8.9668) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 15.2220(16.0256) | Bit/dim 3.6758(3.6962) | Xent 0.7311(0.7236) | Loss 14.4073(16.0830) | Error 0.2578(0.2582) Steps 0(0.00) | Grad Norm 8.0824(8.6909) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.8727(16.0093) | Bit/dim 3.6716(3.6960) | Xent 0.7159(0.7225) | Loss 14.5709(15.6796) | Error 0.2567(0.2575) Steps 0(0.00) | Grad Norm 10.2549(8.6154) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 15.9229(16.0200) | Bit/dim 3.7234(3.6963) | Xent 0.7253(0.7183) | Loss 14.7488(15.3691) | Error 0.2644(0.2558) Steps 0(0.00) | Grad Norm 7.8004(8.0994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 85.6359, Epoch Time 985.8167(938.4341), Bit/dim 3.6877(best: 3.6977), Xent 0.7556, Loss 4.0655, Error 0.2681(best: 0.2672)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 17.3462(16.0201) | Bit/dim 3.7114(3.6967) | Xent 0.7458(0.7142) | Loss 14.6981(17.9322) | Error 0.2722(0.2554) Steps 0(0.00) | Grad Norm 6.8948(8.7506) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 15.6018(15.9845) | Bit/dim 3.6742(3.6953) | Xent 0.6308(0.7210) | Loss 13.8516(16.9893) | Error 0.2333(0.2581) Steps 0(0.00) | Grad Norm 7.1875(8.8984) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.2406(15.9745) | Bit/dim 3.7407(3.6965) | Xent 0.6980(0.7197) | Loss 14.2589(16.2942) | Error 0.2433(0.2567) Steps 0(0.00) | Grad Norm 8.8589(8.7728) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.5206(15.9255) | Bit/dim 3.6916(3.6933) | Xent 0.6721(0.7192) | Loss 14.4747(15.7821) | Error 0.2300(0.2568) Steps 0(0.00) | Grad Norm 5.1938(8.5400) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 15.4648(15.8634) | Bit/dim 3.7029(3.6938) | Xent 0.7154(0.7137) | Loss 14.5877(15.4172) | Error 0.2511(0.2548) Steps 0(0.00) | Grad Norm 14.9388(8.7408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 86.6227, Epoch Time 978.1681(939.6261), Bit/dim 3.6933(best: 3.6877), Xent 0.7586, Loss 4.0726, Error 0.2738(best: 0.2672)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 15.6426(15.8999) | Bit/dim 3.6452(3.6911) | Xent 0.6673(0.7095) | Loss 14.3313(18.5796) | Error 0.2211(0.2522) Steps 0(0.00) | Grad Norm 7.2390(8.8153) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.4243(15.8854) | Bit/dim 3.6902(3.6925) | Xent 0.7504(0.6996) | Loss 14.7506(17.5033) | Error 0.2656(0.2488) Steps 0(0.00) | Grad Norm 10.4887(8.2559) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.1535(15.8747) | Bit/dim 3.7286(3.6935) | Xent 0.7128(0.6915) | Loss 14.3250(16.6747) | Error 0.2778(0.2471) Steps 0(0.00) | Grad Norm 5.8312(8.0263) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 15.7276(15.8900) | Bit/dim 3.6874(3.6921) | Xent 0.7085(0.6985) | Loss 14.4207(16.0682) | Error 0.2456(0.2491) Steps 0(0.00) | Grad Norm 12.0072(8.5492) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 16.2391(15.9203) | Bit/dim 3.7124(3.6920) | Xent 0.6691(0.7040) | Loss 14.6395(15.6753) | Error 0.2211(0.2504) Steps 0(0.00) | Grad Norm 9.6937(9.2569) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 16.0193(15.9263) | Bit/dim 3.6893(3.6907) | Xent 0.6415(0.7064) | Loss 14.3314(15.3211) | Error 0.2267(0.2517) Steps 0(0.00) | Grad Norm 4.2691(8.7129) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 85.4696, Epoch Time 978.8744(940.8035), Bit/dim 3.6855(best: 3.6877), Xent 0.7359, Loss 4.0535, Error 0.2606(best: 0.2672)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
