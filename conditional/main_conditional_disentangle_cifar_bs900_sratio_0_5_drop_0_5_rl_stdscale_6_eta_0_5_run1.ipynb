{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.5, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4350 | Time 16.8347(18.7105) | Bit/dim 3.6548(3.6910) | Xent 0.6859(0.7160) | Loss 13.5788(18.2957) | Error 0.2311(0.2555) Steps 0(0.00) | Grad Norm 6.5517(9.0275) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 17.7096(18.7331) | Bit/dim 3.6785(3.6893) | Xent 0.6607(0.7119) | Loss 13.9756(17.2104) | Error 0.2322(0.2528) Steps 0(0.00) | Grad Norm 6.7178(8.5065) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 19.1800(18.6956) | Bit/dim 3.6636(3.6904) | Xent 0.7640(0.7088) | Loss 13.9314(16.4177) | Error 0.2667(0.2521) Steps 0(0.00) | Grad Norm 9.7381(8.5029) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 17.1247(18.5779) | Bit/dim 3.6932(3.6895) | Xent 0.7369(0.7034) | Loss 13.9914(15.8493) | Error 0.2700(0.2509) Steps 0(0.00) | Grad Norm 9.8571(8.3153) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 18.6029(18.5957) | Bit/dim 3.6539(3.6884) | Xent 0.6566(0.7020) | Loss 14.5535(15.4247) | Error 0.2444(0.2513) Steps 0(0.00) | Grad Norm 7.9872(8.9636) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 18.2187(18.4415) | Bit/dim 3.6867(3.6912) | Xent 0.7551(0.7051) | Loss 14.0999(15.0825) | Error 0.2667(0.2525) Steps 0(0.00) | Grad Norm 8.4541(9.0015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 98.1775, Epoch Time 1154.5585(1051.5817), Bit/dim 3.6918(best: inf), Xent 0.7441, Loss 4.0639, Error 0.2628(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 18.1054(18.3131) | Bit/dim 3.6487(3.6861) | Xent 0.6772(0.7052) | Loss 13.7510(17.7419) | Error 0.2489(0.2527) Steps 0(0.00) | Grad Norm 6.1616(8.2792) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.1085(18.2954) | Bit/dim 3.6919(3.6881) | Xent 0.7792(0.7060) | Loss 14.1016(16.8083) | Error 0.2800(0.2522) Steps 0(0.00) | Grad Norm 10.2038(8.7633) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 17.4718(18.2761) | Bit/dim 3.6953(3.6891) | Xent 0.6828(0.7031) | Loss 14.0918(16.1199) | Error 0.2467(0.2507) Steps 0(0.00) | Grad Norm 8.8007(8.6465) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.1109(18.1328) | Bit/dim 3.6870(3.6891) | Xent 0.7394(0.6975) | Loss 14.2960(15.5647) | Error 0.2722(0.2500) Steps 0(0.00) | Grad Norm 19.0511(8.8963) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.8748(18.0674) | Bit/dim 3.6977(3.6903) | Xent 0.7939(0.7001) | Loss 14.3851(15.1883) | Error 0.2756(0.2498) Steps 0(0.00) | Grad Norm 9.8006(8.9052) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 94.6527, Epoch Time 1103.1287(1053.1281), Bit/dim 3.6884(best: 3.6918), Xent 0.7911, Loss 4.0839, Error 0.2762(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 17.3558(18.0190) | Bit/dim 3.6799(3.6905) | Xent 0.6523(0.7089) | Loss 14.0756(18.4822) | Error 0.2344(0.2530) Steps 0(0.00) | Grad Norm 9.7290(9.7292) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.1662(18.0912) | Bit/dim 3.7151(3.6946) | Xent 0.6789(0.7139) | Loss 13.6835(17.4141) | Error 0.2456(0.2551) Steps 0(0.00) | Grad Norm 4.4467(9.5442) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 18.2898(18.1236) | Bit/dim 3.6959(3.6932) | Xent 0.6932(0.7086) | Loss 14.2046(16.5629) | Error 0.2433(0.2521) Steps 0(0.00) | Grad Norm 4.5835(8.5439) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 19.5199(18.2500) | Bit/dim 3.6961(3.6897) | Xent 0.7609(0.7061) | Loss 14.3868(15.9263) | Error 0.2811(0.2510) Steps 0(0.00) | Grad Norm 12.8623(9.0005) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 18.0379(18.4336) | Bit/dim 3.7181(3.6907) | Xent 0.6677(0.7047) | Loss 14.1515(15.4792) | Error 0.2478(0.2515) Steps 0(0.00) | Grad Norm 10.9625(8.9627) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 19.1043(18.4906) | Bit/dim 3.6737(3.6877) | Xent 0.7445(0.7069) | Loss 14.4530(15.1845) | Error 0.2622(0.2513) Steps 0(0.00) | Grad Norm 8.0236(8.8701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 95.2645, Epoch Time 1129.3748(1055.4155), Bit/dim 3.6814(best: 3.6884), Xent 0.7344, Loss 4.0486, Error 0.2592(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 18.2081(18.4522) | Bit/dim 3.6896(3.6891) | Xent 0.6633(0.7006) | Loss 14.0504(17.9736) | Error 0.2411(0.2498) Steps 0(0.00) | Grad Norm 10.4239(8.9006) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 19.4061(18.4251) | Bit/dim 3.6487(3.6876) | Xent 0.7157(0.7039) | Loss 14.2321(17.0083) | Error 0.2611(0.2521) Steps 0(0.00) | Grad Norm 9.4414(9.4705) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 19.5868(18.3839) | Bit/dim 3.7045(3.6891) | Xent 0.7679(0.7006) | Loss 14.6222(16.2922) | Error 0.2722(0.2502) Steps 0(0.00) | Grad Norm 10.0388(9.1327) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 17.7207(18.3580) | Bit/dim 3.6590(3.6886) | Xent 0.6804(0.7008) | Loss 13.7838(15.7491) | Error 0.2544(0.2513) Steps 0(0.00) | Grad Norm 5.1085(8.9613) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 18.2676(18.4151) | Bit/dim 3.6949(3.6858) | Xent 0.7281(0.6960) | Loss 14.4220(15.3979) | Error 0.2556(0.2500) Steps 0(0.00) | Grad Norm 8.7848(8.7157) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 92.5362, Epoch Time 1119.4943(1057.3379), Bit/dim 3.6867(best: 3.6814), Xent 0.7246, Loss 4.0490, Error 0.2530(best: 0.2592)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 19.8779(18.5245) | Bit/dim 3.6513(3.6778) | Xent 0.6634(0.6889) | Loss 14.5633(18.4799) | Error 0.2389(0.2473) Steps 0(0.00) | Grad Norm 8.9411(8.1721) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 19.4717(18.4743) | Bit/dim 3.7164(3.6814) | Xent 0.6822(0.6857) | Loss 14.0646(17.3356) | Error 0.2411(0.2453) Steps 0(0.00) | Grad Norm 7.8819(8.5669) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.5612(18.4650) | Bit/dim 3.7078(3.6832) | Xent 0.6814(0.6818) | Loss 14.4519(16.5054) | Error 0.2300(0.2434) Steps 0(0.00) | Grad Norm 7.4374(8.6225) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 18.1917(18.4808) | Bit/dim 3.7065(3.6829) | Xent 0.7251(0.6868) | Loss 14.0239(15.9018) | Error 0.2544(0.2449) Steps 0(0.00) | Grad Norm 11.8773(9.6131) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 24.0764(18.5513) | Bit/dim 3.7200(3.6847) | Xent 0.7655(0.6900) | Loss 15.1992(15.4972) | Error 0.2778(0.2466) Steps 0(0.00) | Grad Norm 9.7072(9.4804) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.4223(18.4684) | Bit/dim 3.6840(3.6839) | Xent 0.7161(0.6948) | Loss 14.1434(15.1596) | Error 0.2611(0.2469) Steps 0(0.00) | Grad Norm 8.8493(9.4081) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 95.6347, Epoch Time 1132.9731(1059.6069), Bit/dim 3.6830(best: 3.6814), Xent 0.7358, Loss 4.0509, Error 0.2569(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 18.3727(18.5057) | Bit/dim 3.6605(3.6825) | Xent 0.6439(0.6873) | Loss 14.1033(17.7654) | Error 0.2344(0.2441) Steps 0(0.00) | Grad Norm 6.2799(8.8849) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.3598(18.4768) | Bit/dim 3.6724(3.6822) | Xent 0.7605(0.6828) | Loss 14.1370(16.8399) | Error 0.2756(0.2436) Steps 0(0.00) | Grad Norm 11.7266(8.8220) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.7132(18.4176) | Bit/dim 3.6766(3.6809) | Xent 0.5955(0.6797) | Loss 14.2714(16.1694) | Error 0.2167(0.2432) Steps 0(0.00) | Grad Norm 6.9784(8.6626) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 19.8219(18.5550) | Bit/dim 3.7079(3.6815) | Xent 0.6927(0.6853) | Loss 14.5381(15.6615) | Error 0.2322(0.2446) Steps 0(0.00) | Grad Norm 8.5145(9.0490) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 17.7293(18.7121) | Bit/dim 3.6498(3.6794) | Xent 0.6878(0.6859) | Loss 14.0339(15.2745) | Error 0.2400(0.2451) Steps 0(0.00) | Grad Norm 11.2563(8.7851) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 94.3112, Epoch Time 1136.9375(1061.9269), Bit/dim 3.6819(best: 3.6814), Xent 0.7644, Loss 4.0640, Error 0.2696(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.3623(18.5912) | Bit/dim 3.6873(3.6833) | Xent 0.5967(0.6816) | Loss 13.1901(18.4820) | Error 0.2067(0.2439) Steps 0(0.00) | Grad Norm 7.7761(8.8061) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 20.2837(18.5159) | Bit/dim 3.6609(3.6834) | Xent 0.6950(0.6779) | Loss 14.2315(17.3909) | Error 0.2456(0.2423) Steps 0(0.00) | Grad Norm 11.2231(9.1221) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 21.5793(18.5301) | Bit/dim 3.6522(3.6801) | Xent 0.6801(0.6762) | Loss 14.8474(16.5779) | Error 0.2633(0.2419) Steps 0(0.00) | Grad Norm 7.3204(9.1053) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 17.8807(18.3729) | Bit/dim 3.7119(3.6811) | Xent 0.6307(0.6783) | Loss 13.9332(15.9361) | Error 0.2178(0.2428) Steps 0(0.00) | Grad Norm 6.5186(8.5308) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 18.3432(18.5445) | Bit/dim 3.6670(3.6802) | Xent 0.6597(0.6834) | Loss 13.8402(15.5116) | Error 0.2344(0.2447) Steps 0(0.00) | Grad Norm 8.0358(8.9973) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 17.7657(18.5016) | Bit/dim 3.7102(3.6800) | Xent 0.7031(0.6873) | Loss 14.0273(15.1313) | Error 0.2611(0.2464) Steps 0(0.00) | Grad Norm 6.8806(8.8256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 96.0022, Epoch Time 1127.6392(1063.8982), Bit/dim 3.6809(best: 3.6814), Xent 0.7372, Loss 4.0495, Error 0.2577(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 18.3591(18.5487) | Bit/dim 3.6839(3.6805) | Xent 0.6337(0.6793) | Loss 14.1069(17.7259) | Error 0.2322(0.2435) Steps 0(0.00) | Grad Norm 7.7732(8.5220) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 18.0970(18.4532) | Bit/dim 3.6772(3.6793) | Xent 0.6816(0.6744) | Loss 14.4407(16.8214) | Error 0.2333(0.2420) Steps 0(0.00) | Grad Norm 5.5323(8.3298) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 19.9245(18.3895) | Bit/dim 3.6600(3.6744) | Xent 0.6709(0.6769) | Loss 14.6650(16.1532) | Error 0.2389(0.2434) Steps 0(0.00) | Grad Norm 12.1206(8.2774) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.4330(18.4317) | Bit/dim 3.6611(3.6759) | Xent 0.7230(0.6803) | Loss 14.2431(15.6598) | Error 0.2422(0.2444) Steps 0(0.00) | Grad Norm 8.0734(8.6728) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 20.2911(18.5883) | Bit/dim 3.6548(3.6755) | Xent 0.6389(0.6779) | Loss 14.4401(15.2946) | Error 0.2289(0.2425) Steps 0(0.00) | Grad Norm 7.6709(8.6497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 95.0207, Epoch Time 1129.5601(1065.8681), Bit/dim 3.6717(best: 3.6809), Xent 0.7220, Loss 4.0327, Error 0.2533(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 17.3520(18.5025) | Bit/dim 3.6619(3.6760) | Xent 0.7934(0.6796) | Loss 14.2083(18.5848) | Error 0.2600(0.2430) Steps 0(0.00) | Grad Norm 17.6294(8.9281) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 18.2222(18.4636) | Bit/dim 3.6836(3.6784) | Xent 0.7322(0.6801) | Loss 14.0980(17.4211) | Error 0.2500(0.2428) Steps 0(0.00) | Grad Norm 6.8924(8.9352) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.7313(18.4423) | Bit/dim 3.6520(3.6733) | Xent 0.6643(0.6725) | Loss 14.7601(16.6404) | Error 0.2467(0.2405) Steps 0(0.00) | Grad Norm 4.0733(8.1810) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 18.0376(18.4554) | Bit/dim 3.6790(3.6736) | Xent 0.7008(0.6668) | Loss 14.2039(15.9903) | Error 0.2433(0.2375) Steps 0(0.00) | Grad Norm 9.0977(8.0526) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 18.1459(18.5005) | Bit/dim 3.6548(3.6710) | Xent 0.6612(0.6602) | Loss 14.2525(15.5079) | Error 0.2411(0.2363) Steps 0(0.00) | Grad Norm 11.2670(8.3148) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 19.9169(18.4658) | Bit/dim 3.7015(3.6756) | Xent 0.6876(0.6613) | Loss 14.6155(15.1733) | Error 0.2544(0.2370) Steps 0(0.00) | Grad Norm 10.5044(8.4305) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 94.5526, Epoch Time 1126.7768(1067.6953), Bit/dim 3.6740(best: 3.6717), Xent 0.7298, Loss 4.0388, Error 0.2572(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 19.3720(18.4373) | Bit/dim 3.6918(3.6711) | Xent 0.5642(0.6565) | Loss 13.9156(17.6640) | Error 0.1833(0.2346) Steps 0(0.00) | Grad Norm 6.2565(8.4290) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 19.5112(18.4837) | Bit/dim 3.6740(3.6737) | Xent 0.6253(0.6524) | Loss 14.1891(16.7691) | Error 0.2200(0.2325) Steps 0(0.00) | Grad Norm 10.4312(8.6442) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 18.3398(18.5191) | Bit/dim 3.6504(3.6719) | Xent 0.7037(0.6526) | Loss 14.3670(16.1392) | Error 0.2444(0.2328) Steps 0(0.00) | Grad Norm 5.7979(8.4005) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 19.7297(18.6642) | Bit/dim 3.6881(3.6724) | Xent 0.6391(0.6554) | Loss 14.5587(15.6428) | Error 0.2389(0.2341) Steps 0(0.00) | Grad Norm 9.2292(8.1216) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 24.9862(18.7112) | Bit/dim 3.7094(3.6756) | Xent 0.6410(0.6583) | Loss 15.1929(15.2845) | Error 0.2333(0.2353) Steps 0(0.00) | Grad Norm 8.4435(8.3129) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 94.7887, Epoch Time 1137.8250(1069.7992), Bit/dim 3.6734(best: 3.6717), Xent 0.7283, Loss 4.0376, Error 0.2544(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 18.7986(18.5641) | Bit/dim 3.6616(3.6734) | Xent 0.6139(0.6464) | Loss 14.4191(18.5626) | Error 0.2278(0.2315) Steps 0(0.00) | Grad Norm 5.4377(8.1497) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 20.4978(18.5474) | Bit/dim 3.6389(3.6710) | Xent 0.6348(0.6435) | Loss 14.4738(17.4330) | Error 0.2411(0.2308) Steps 0(0.00) | Grad Norm 8.5056(7.8292) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 19.8969(18.5641) | Bit/dim 3.6649(3.6698) | Xent 0.6707(0.6426) | Loss 14.4859(16.5543) | Error 0.2244(0.2299) Steps 0(0.00) | Grad Norm 7.9033(7.7358) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 19.1622(18.4761) | Bit/dim 3.6726(3.6705) | Xent 0.7235(0.6505) | Loss 14.4548(15.9812) | Error 0.2756(0.2338) Steps 0(0.00) | Grad Norm 18.6512(8.3406) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 19.3593(18.4582) | Bit/dim 3.6547(3.6710) | Xent 0.6445(0.6492) | Loss 14.4314(15.4984) | Error 0.2300(0.2332) Steps 0(0.00) | Grad Norm 10.1200(8.6357) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.7759(18.3640) | Bit/dim 3.6397(3.6712) | Xent 0.7240(0.6570) | Loss 14.3088(15.1478) | Error 0.2533(0.2352) Steps 0(0.00) | Grad Norm 6.6531(8.5497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 96.6210, Epoch Time 1123.3524(1071.4058), Bit/dim 3.6721(best: 3.6717), Xent 0.7402, Loss 4.0422, Error 0.2616(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 18.6938(18.3594) | Bit/dim 3.6764(3.6721) | Xent 0.5651(0.6500) | Loss 14.3425(18.0095) | Error 0.1922(0.2328) Steps 0(0.00) | Grad Norm 4.4582(8.1243) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.7922(18.3294) | Bit/dim 3.6550(3.6705) | Xent 0.6021(0.6426) | Loss 13.5671(17.0045) | Error 0.2044(0.2304) Steps 0(0.00) | Grad Norm 6.9352(7.8107) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.8614(18.2748) | Bit/dim 3.6561(3.6724) | Xent 0.6505(0.6359) | Loss 13.7089(16.2483) | Error 0.2422(0.2285) Steps 0(0.00) | Grad Norm 7.5724(7.7924) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 16.8729(18.2690) | Bit/dim 3.6649(3.6682) | Xent 0.6166(0.6397) | Loss 14.2289(15.7463) | Error 0.2244(0.2293) Steps 0(0.00) | Grad Norm 9.0698(7.7374) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.7943(18.1991) | Bit/dim 3.6516(3.6706) | Xent 0.6191(0.6403) | Loss 14.1129(15.3303) | Error 0.2089(0.2283) Steps 0(0.00) | Grad Norm 8.7439(7.7818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 96.4438, Epoch Time 1114.6009(1072.7017), Bit/dim 3.6696(best: 3.6717), Xent 0.7093, Loss 4.0243, Error 0.2495(best: 0.2530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 18.6623(18.2109) | Bit/dim 3.7001(3.6699) | Xent 0.6441(0.6409) | Loss 14.3361(18.3672) | Error 0.2344(0.2294) Steps 0(0.00) | Grad Norm 6.9915(7.6788) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 19.6543(18.3531) | Bit/dim 3.6187(3.6685) | Xent 0.5991(0.6463) | Loss 14.3155(17.2829) | Error 0.2122(0.2315) Steps 0(0.00) | Grad Norm 8.9295(8.5021) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 18.9993(18.3803) | Bit/dim 3.6829(3.6689) | Xent 0.6177(0.6484) | Loss 13.9851(16.4901) | Error 0.2244(0.2317) Steps 0(0.00) | Grad Norm 10.0946(8.7706) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 18.9636(18.2846) | Bit/dim 3.6496(3.6677) | Xent 0.6221(0.6454) | Loss 14.1364(15.8666) | Error 0.2089(0.2301) Steps 0(0.00) | Grad Norm 10.8824(8.6014) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 19.1212(18.2960) | Bit/dim 3.6297(3.6654) | Xent 0.6396(0.6459) | Loss 13.8558(15.4202) | Error 0.2178(0.2306) Steps 0(0.00) | Grad Norm 5.5178(8.3889) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 18.3655(18.3439) | Bit/dim 3.6488(3.6662) | Xent 0.6027(0.6447) | Loss 14.1039(15.1570) | Error 0.2044(0.2306) Steps 0(0.00) | Grad Norm 6.4671(8.3534) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 95.7309, Epoch Time 1125.8371(1074.2957), Bit/dim 3.6680(best: 3.6696), Xent 0.7034, Loss 4.0197, Error 0.2435(best: 0.2495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 17.5928(18.2897) | Bit/dim 3.6741(3.6680) | Xent 0.6197(0.6420) | Loss 13.8686(17.9395) | Error 0.2300(0.2300) Steps 0(0.00) | Grad Norm 6.5822(8.3289) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 18.4148(18.2721) | Bit/dim 3.6791(3.6685) | Xent 0.6396(0.6422) | Loss 14.4586(16.9234) | Error 0.2278(0.2284) Steps 0(0.00) | Grad Norm 9.0265(8.5849) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 16.9488(18.3522) | Bit/dim 3.6602(3.6666) | Xent 0.6184(0.6341) | Loss 13.7369(16.2058) | Error 0.2222(0.2261) Steps 0(0.00) | Grad Norm 8.5017(8.2597) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 18.4245(18.5906) | Bit/dim 3.6743(3.6668) | Xent 0.5921(0.6288) | Loss 14.3672(15.6649) | Error 0.2233(0.2240) Steps 0(0.00) | Grad Norm 6.2276(7.8981) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 17.5211(18.5618) | Bit/dim 3.6640(3.6656) | Xent 0.6007(0.6293) | Loss 13.7224(15.2856) | Error 0.2078(0.2236) Steps 0(0.00) | Grad Norm 6.2401(7.5758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 96.8682, Epoch Time 1137.5297(1076.1928), Bit/dim 3.6622(best: 3.6680), Xent 0.6890, Loss 4.0067, Error 0.2403(best: 0.2435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.1328(18.5435) | Bit/dim 3.6697(3.6653) | Xent 0.6546(0.6280) | Loss 14.3015(18.5465) | Error 0.2267(0.2229) Steps 0(0.00) | Grad Norm 11.2278(7.5067) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.6265(18.5709) | Bit/dim 3.6594(3.6652) | Xent 0.5860(0.6183) | Loss 14.0961(17.4230) | Error 0.2044(0.2197) Steps 0(0.00) | Grad Norm 11.0067(7.4495) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 19.5007(18.4607) | Bit/dim 3.6733(3.6663) | Xent 0.5981(0.6217) | Loss 14.3631(16.5731) | Error 0.2100(0.2214) Steps 0(0.00) | Grad Norm 11.3751(8.1320) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 19.7015(18.6115) | Bit/dim 3.6477(3.6629) | Xent 0.6354(0.6225) | Loss 14.5142(15.9464) | Error 0.2433(0.2215) Steps 0(0.00) | Grad Norm 8.6477(8.1422) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 19.3369(18.4164) | Bit/dim 3.6715(3.6608) | Xent 0.6777(0.6342) | Loss 14.2682(15.4883) | Error 0.2522(0.2259) Steps 0(0.00) | Grad Norm 10.5852(8.6909) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 18.0141(18.4860) | Bit/dim 3.6626(3.6637) | Xent 0.6134(0.6328) | Loss 14.2329(15.1618) | Error 0.2233(0.2264) Steps 0(0.00) | Grad Norm 6.2622(8.2899) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 95.5817, Epoch Time 1127.4924(1077.7317), Bit/dim 3.6687(best: 3.6622), Xent 0.7019, Loss 4.0197, Error 0.2449(best: 0.2403)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 18.1815(18.5162) | Bit/dim 3.6762(3.6635) | Xent 0.5928(0.6256) | Loss 14.2883(17.9751) | Error 0.1989(0.2238) Steps 0(0.00) | Grad Norm 6.0295(7.8747) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 18.0759(18.4329) | Bit/dim 3.6711(3.6627) | Xent 0.5839(0.6236) | Loss 13.9732(16.9654) | Error 0.2167(0.2234) Steps 0(0.00) | Grad Norm 6.3318(7.7513) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 18.5221(18.3211) | Bit/dim 3.6841(3.6647) | Xent 0.5836(0.6255) | Loss 14.2777(16.2687) | Error 0.2078(0.2238) Steps 0(0.00) | Grad Norm 9.3751(8.0405) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 18.6662(18.3128) | Bit/dim 3.6436(3.6644) | Xent 0.6510(0.6353) | Loss 14.6026(15.7565) | Error 0.2333(0.2273) Steps 0(0.00) | Grad Norm 7.0784(9.2086) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 20.0086(18.3713) | Bit/dim 3.6865(3.6660) | Xent 0.6141(0.6354) | Loss 14.7395(15.3816) | Error 0.2378(0.2277) Steps 0(0.00) | Grad Norm 10.3793(8.9872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 95.6915, Epoch Time 1129.0617(1079.2716), Bit/dim 3.6660(best: 3.6622), Xent 0.7020, Loss 4.0170, Error 0.2418(best: 0.2403)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 19.8186(18.6284) | Bit/dim 3.6710(3.6649) | Xent 0.6807(0.6433) | Loss 14.0794(18.6464) | Error 0.2344(0.2293) Steps 0(0.00) | Grad Norm 9.6101(9.6100) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 20.2392(18.5616) | Bit/dim 3.6516(3.6654) | Xent 0.6506(0.6450) | Loss 14.5356(17.4939) | Error 0.2356(0.2302) Steps 0(0.00) | Grad Norm 7.0800(8.9349) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 18.6581(18.6064) | Bit/dim 3.6561(3.6625) | Xent 0.6706(0.6382) | Loss 13.9438(16.6271) | Error 0.2400(0.2271) Steps 0(0.00) | Grad Norm 9.0773(8.5414) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.2340(18.6406) | Bit/dim 3.6569(3.6625) | Xent 0.5707(0.6347) | Loss 13.8587(16.0137) | Error 0.2100(0.2254) Steps 0(0.00) | Grad Norm 6.2657(8.5978) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 18.0410(18.4485) | Bit/dim 3.6214(3.6607) | Xent 0.6387(0.6286) | Loss 13.4260(15.4454) | Error 0.2289(0.2237) Steps 0(0.00) | Grad Norm 7.5804(8.0343) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 18.2036(18.4159) | Bit/dim 3.6474(3.6592) | Xent 0.5557(0.6210) | Loss 14.0473(15.1406) | Error 0.2133(0.2209) Steps 0(0.00) | Grad Norm 5.0366(7.6151) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 96.0394, Epoch Time 1128.1813(1080.7389), Bit/dim 3.6601(best: 3.6622), Xent 0.6896, Loss 4.0049, Error 0.2387(best: 0.2403)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 18.3248(18.4923) | Bit/dim 3.6463(3.6588) | Xent 0.5727(0.6096) | Loss 13.8416(17.8439) | Error 0.2089(0.2166) Steps 0(0.00) | Grad Norm 9.8449(7.6803) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 19.1153(18.6348) | Bit/dim 3.6226(3.6557) | Xent 0.5795(0.6125) | Loss 13.8886(16.9471) | Error 0.2178(0.2173) Steps 0(0.00) | Grad Norm 8.5087(7.8938) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 16.9739(18.5591) | Bit/dim 3.6741(3.6573) | Xent 0.6202(0.6150) | Loss 14.0169(16.1836) | Error 0.2178(0.2196) Steps 0(0.00) | Grad Norm 5.0076(8.3083) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 16.3192(18.5256) | Bit/dim 3.6686(3.6596) | Xent 0.6269(0.6267) | Loss 14.1823(15.7045) | Error 0.2300(0.2227) Steps 0(0.00) | Grad Norm 8.1695(8.8631) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 19.5335(18.5160) | Bit/dim 3.6644(3.6615) | Xent 0.6879(0.6301) | Loss 14.8891(15.2994) | Error 0.2489(0.2228) Steps 0(0.00) | Grad Norm 13.0415(8.9404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 97.1550, Epoch Time 1139.8333(1082.5118), Bit/dim 3.6645(best: 3.6601), Xent 0.7285, Loss 4.0288, Error 0.2524(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 19.6382(18.5124) | Bit/dim 3.6421(3.6618) | Xent 0.5839(0.6231) | Loss 14.6189(18.4621) | Error 0.2078(0.2220) Steps 0(0.00) | Grad Norm 8.0280(8.5669) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 17.3178(18.6324) | Bit/dim 3.6776(3.6611) | Xent 0.6269(0.6166) | Loss 14.3407(17.4117) | Error 0.2378(0.2203) Steps 0(0.00) | Grad Norm 6.7348(8.0672) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.6196(18.6220) | Bit/dim 3.6122(3.6620) | Xent 0.5609(0.6085) | Loss 14.1114(16.5717) | Error 0.2044(0.2171) Steps 0(0.00) | Grad Norm 4.5601(7.4855) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 18.2270(18.6535) | Bit/dim 3.6308(3.6593) | Xent 0.5400(0.6031) | Loss 13.9477(15.9430) | Error 0.1878(0.2160) Steps 0(0.00) | Grad Norm 6.3448(7.1325) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 17.5250(18.5375) | Bit/dim 3.6964(3.6586) | Xent 0.6710(0.6053) | Loss 14.4475(15.4937) | Error 0.2400(0.2165) Steps 0(0.00) | Grad Norm 8.0207(7.2706) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.6497(18.5559) | Bit/dim 3.6649(3.6588) | Xent 0.6207(0.6088) | Loss 14.0185(15.1303) | Error 0.2256(0.2169) Steps 0(0.00) | Grad Norm 8.2001(7.4691) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 96.7041, Epoch Time 1137.4553(1084.1601), Bit/dim 3.6577(best: 3.6601), Xent 0.7335, Loss 4.0245, Error 0.2554(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 18.3748(18.8726) | Bit/dim 3.6980(3.6601) | Xent 0.5847(0.6161) | Loss 14.0057(17.9443) | Error 0.2056(0.2190) Steps 0(0.00) | Grad Norm 10.3202(8.5592) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 18.6662(18.8905) | Bit/dim 3.6652(3.6587) | Xent 0.6249(0.6188) | Loss 14.4094(17.0444) | Error 0.2344(0.2209) Steps 0(0.00) | Grad Norm 10.6998(9.0968) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 18.3973(18.8736) | Bit/dim 3.6628(3.6608) | Xent 0.5969(0.6187) | Loss 14.3099(16.3173) | Error 0.2222(0.2211) Steps 0(0.00) | Grad Norm 6.5002(8.4963) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 19.6828(19.0410) | Bit/dim 3.6869(3.6612) | Xent 0.6101(0.6108) | Loss 14.3821(15.7725) | Error 0.2278(0.2208) Steps 0(0.00) | Grad Norm 8.0730(7.6286) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 17.5395(19.1283) | Bit/dim 3.6523(3.6613) | Xent 0.5771(0.6097) | Loss 13.7262(15.3665) | Error 0.2000(0.2192) Steps 0(0.00) | Grad Norm 5.8030(7.4657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 96.5073, Epoch Time 1173.4319(1086.8382), Bit/dim 3.6587(best: 3.6577), Xent 0.7030, Loss 4.0102, Error 0.2461(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 18.0554(19.2115) | Bit/dim 3.6671(3.6597) | Xent 0.5615(0.6037) | Loss 14.1218(18.5759) | Error 0.2033(0.2173) Steps 0(0.00) | Grad Norm 8.1100(7.3580) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 18.5608(19.0935) | Bit/dim 3.6184(3.6599) | Xent 0.6179(0.6051) | Loss 14.2959(17.4493) | Error 0.2133(0.2170) Steps 0(0.00) | Grad Norm 8.9814(7.8545) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 17.6774(18.9752) | Bit/dim 3.6372(3.6590) | Xent 0.6370(0.5985) | Loss 14.1409(16.5864) | Error 0.2367(0.2138) Steps 0(0.00) | Grad Norm 8.5902(7.7792) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 18.1698(19.0092) | Bit/dim 3.6579(3.6600) | Xent 0.6439(0.5988) | Loss 13.6522(15.9550) | Error 0.2267(0.2141) Steps 0(0.00) | Grad Norm 8.7180(7.9743) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 19.7623(18.9988) | Bit/dim 3.6381(3.6588) | Xent 0.5468(0.6027) | Loss 14.3021(15.5331) | Error 0.1967(0.2155) Steps 0(0.00) | Grad Norm 5.3415(8.3052) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 17.9650(19.0091) | Bit/dim 3.6503(3.6569) | Xent 0.6266(0.6066) | Loss 13.5244(15.1498) | Error 0.2289(0.2171) Steps 0(0.00) | Grad Norm 6.6160(8.0732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 97.4700, Epoch Time 1159.9081(1089.0303), Bit/dim 3.6683(best: 3.6577), Xent 0.7146, Loss 4.0256, Error 0.2449(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.9749(19.0584) | Bit/dim 3.6679(3.6624) | Xent 0.6210(0.6133) | Loss 14.3066(18.0874) | Error 0.2278(0.2200) Steps 0(0.00) | Grad Norm 6.8431(8.8558) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 20.3322(19.2139) | Bit/dim 3.7015(3.6622) | Xent 0.6306(0.6123) | Loss 14.5718(17.0827) | Error 0.2233(0.2181) Steps 0(0.00) | Grad Norm 8.2975(9.1135) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 19.3479(19.2425) | Bit/dim 3.6495(3.6599) | Xent 0.6563(0.6133) | Loss 14.7375(16.3409) | Error 0.2333(0.2197) Steps 0(0.00) | Grad Norm 6.1987(8.5316) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 18.9429(19.1025) | Bit/dim 3.6288(3.6568) | Xent 0.5931(0.6125) | Loss 14.4115(15.7707) | Error 0.2089(0.2187) Steps 0(0.00) | Grad Norm 6.8856(8.3909) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 17.8179(18.8158) | Bit/dim 3.6458(3.6565) | Xent 0.5992(0.6149) | Loss 14.3266(15.3587) | Error 0.2100(0.2195) Steps 0(0.00) | Grad Norm 6.6837(8.3593) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 96.2941, Epoch Time 1159.5146(1091.1449), Bit/dim 3.6672(best: 3.6577), Xent 0.6989, Loss 4.0166, Error 0.2431(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 20.5435(18.9336) | Bit/dim 3.6664(3.6563) | Xent 0.5798(0.6007) | Loss 14.0883(18.6374) | Error 0.2122(0.2146) Steps 0(0.00) | Grad Norm 9.0354(8.1509) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 21.4053(18.9251) | Bit/dim 3.6277(3.6533) | Xent 0.6297(0.6009) | Loss 14.5342(17.4817) | Error 0.2300(0.2141) Steps 0(0.00) | Grad Norm 8.3654(8.1943) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.7141(18.8523) | Bit/dim 3.6513(3.6541) | Xent 0.6495(0.6046) | Loss 13.1185(16.5592) | Error 0.2422(0.2160) Steps 0(0.00) | Grad Norm 9.8078(8.5355) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 18.3242(18.8460) | Bit/dim 3.6369(3.6569) | Xent 0.5463(0.6017) | Loss 13.9005(15.9605) | Error 0.1989(0.2147) Steps 0(0.00) | Grad Norm 6.6600(8.1854) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 19.3642(19.0977) | Bit/dim 3.6720(3.6575) | Xent 0.6502(0.6013) | Loss 14.6896(15.5685) | Error 0.2189(0.2146) Steps 0(0.00) | Grad Norm 10.7493(8.1068) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 20.6569(18.9994) | Bit/dim 3.6717(3.6565) | Xent 0.6262(0.6075) | Loss 14.3035(15.2029) | Error 0.2244(0.2176) Steps 0(0.00) | Grad Norm 5.5883(8.4672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 95.3406, Epoch Time 1157.6110(1093.1388), Bit/dim 3.6640(best: 3.6577), Xent 0.7326, Loss 4.0303, Error 0.2510(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 18.4505(18.9927) | Bit/dim 3.5988(3.6547) | Xent 0.5416(0.6035) | Loss 14.3957(17.9997) | Error 0.1800(0.2146) Steps 0(0.00) | Grad Norm 7.3372(8.4025) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 18.2036(18.7718) | Bit/dim 3.6311(3.6563) | Xent 0.5780(0.5962) | Loss 14.0210(17.0111) | Error 0.1944(0.2110) Steps 0(0.00) | Grad Norm 8.1604(8.4569) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 18.6604(18.8931) | Bit/dim 3.6325(3.6540) | Xent 0.5624(0.5959) | Loss 14.5120(16.2969) | Error 0.2078(0.2116) Steps 0(0.00) | Grad Norm 5.8555(8.2012) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 17.2655(18.8177) | Bit/dim 3.6821(3.6545) | Xent 0.5928(0.5919) | Loss 14.2189(15.7185) | Error 0.2044(0.2097) Steps 0(0.00) | Grad Norm 6.7316(8.1407) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.8111(18.9783) | Bit/dim 3.6352(3.6549) | Xent 0.6140(0.5956) | Loss 13.9093(15.3048) | Error 0.2033(0.2118) Steps 0(0.00) | Grad Norm 6.1692(8.0466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 96.3888, Epoch Time 1149.4353(1094.8277), Bit/dim 3.6501(best: 3.6577), Xent 0.6848, Loss 3.9925, Error 0.2350(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 19.7344(18.8560) | Bit/dim 3.6643(3.6537) | Xent 0.5509(0.5868) | Loss 14.2669(18.5821) | Error 0.2011(0.2078) Steps 0(0.00) | Grad Norm 6.5112(7.6313) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 18.1147(18.8451) | Bit/dim 3.6227(3.6528) | Xent 0.5879(0.5887) | Loss 14.3794(17.4699) | Error 0.2200(0.2086) Steps 0(0.00) | Grad Norm 7.8445(7.7697) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 19.0825(18.8538) | Bit/dim 3.6683(3.6508) | Xent 0.5696(0.5830) | Loss 14.6024(16.6002) | Error 0.2089(0.2057) Steps 0(0.00) | Grad Norm 7.7541(7.6517) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 18.7908(18.8527) | Bit/dim 3.6496(3.6512) | Xent 0.5463(0.5815) | Loss 14.1117(15.9736) | Error 0.1956(0.2054) Steps 0(0.00) | Grad Norm 5.0678(7.3691) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 19.4031(19.0598) | Bit/dim 3.6363(3.6519) | Xent 0.6111(0.5777) | Loss 14.3769(15.5451) | Error 0.2189(0.2052) Steps 0(0.00) | Grad Norm 9.0358(7.2479) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 19.0739(19.1627) | Bit/dim 3.6293(3.6488) | Xent 0.5869(0.5835) | Loss 14.7534(15.2522) | Error 0.2056(0.2076) Steps 0(0.00) | Grad Norm 8.2655(7.3644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 96.6274, Epoch Time 1165.9906(1096.9626), Bit/dim 3.6655(best: 3.6501), Xent 0.6909, Loss 4.0110, Error 0.2394(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 23.7455(19.2295) | Bit/dim 3.6440(3.6484) | Xent 0.5834(0.5848) | Loss 15.3710(17.9137) | Error 0.2156(0.2078) Steps 0(0.00) | Grad Norm 9.0973(8.0814) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 19.1703(19.1212) | Bit/dim 3.6559(3.6479) | Xent 0.5612(0.5814) | Loss 14.2334(16.9574) | Error 0.1889(0.2061) Steps 0(0.00) | Grad Norm 5.3943(7.9360) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 20.1692(18.9546) | Bit/dim 3.6504(3.6506) | Xent 0.6247(0.5813) | Loss 14.7398(16.2502) | Error 0.2256(0.2062) Steps 0(0.00) | Grad Norm 18.2414(7.8949) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 21.0053(19.1357) | Bit/dim 3.6630(3.6522) | Xent 0.5557(0.5814) | Loss 14.7633(15.7819) | Error 0.2011(0.2078) Steps 0(0.00) | Grad Norm 8.0251(7.9715) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 18.2178(19.0854) | Bit/dim 3.6455(3.6538) | Xent 0.5934(0.5875) | Loss 14.3118(15.4083) | Error 0.2133(0.2103) Steps 0(0.00) | Grad Norm 9.6938(8.2589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 96.4322, Epoch Time 1163.2314(1098.9507), Bit/dim 3.6563(best: 3.6501), Xent 0.7206, Loss 4.0166, Error 0.2502(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 17.4517(19.0057) | Bit/dim 3.6091(3.6549) | Xent 0.6170(0.5912) | Loss 13.8161(18.6148) | Error 0.2256(0.2117) Steps 0(0.00) | Grad Norm 9.1990(8.4239) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 17.2922(18.9718) | Bit/dim 3.6437(3.6553) | Xent 0.5543(0.5870) | Loss 13.6599(17.5105) | Error 0.2033(0.2093) Steps 0(0.00) | Grad Norm 6.9562(8.4974) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 17.6289(18.9657) | Bit/dim 3.6540(3.6538) | Xent 0.5875(0.5916) | Loss 14.0219(16.6899) | Error 0.2111(0.2119) Steps 0(0.00) | Grad Norm 8.3090(8.3202) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.7939(18.9873) | Bit/dim 3.6173(3.6509) | Xent 0.5543(0.5829) | Loss 15.0007(16.0564) | Error 0.1911(0.2078) Steps 0(0.00) | Grad Norm 7.0905(7.8882) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 18.4258(18.8923) | Bit/dim 3.6463(3.6531) | Xent 0.5918(0.5827) | Loss 14.0813(15.5987) | Error 0.2200(0.2075) Steps 0(0.00) | Grad Norm 5.7129(8.2253) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 17.3883(18.8943) | Bit/dim 3.6432(3.6532) | Xent 0.5816(0.5827) | Loss 14.2893(15.2291) | Error 0.2033(0.2070) Steps 0(0.00) | Grad Norm 6.9637(8.3472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 96.5795, Epoch Time 1148.6646(1100.4421), Bit/dim 3.6548(best: 3.6501), Xent 0.6906, Loss 4.0001, Error 0.2344(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 17.4840(18.8449) | Bit/dim 3.6484(3.6519) | Xent 0.5309(0.5771) | Loss 14.0174(18.0695) | Error 0.1933(0.2054) Steps 0(0.00) | Grad Norm 6.0532(8.0150) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 20.8385(19.0332) | Bit/dim 3.6418(3.6504) | Xent 0.5730(0.5711) | Loss 15.0310(17.0850) | Error 0.1944(0.2024) Steps 0(0.00) | Grad Norm 9.8950(8.0491) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 17.6590(19.1201) | Bit/dim 3.6567(3.6465) | Xent 0.5748(0.5684) | Loss 14.4202(16.3337) | Error 0.1989(0.2023) Steps 0(0.00) | Grad Norm 9.8734(7.8730) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 17.6381(19.2812) | Bit/dim 3.6372(3.6467) | Xent 0.5903(0.5715) | Loss 13.5622(15.8072) | Error 0.2033(0.2027) Steps 0(0.00) | Grad Norm 11.8682(8.0683) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 19.8357(19.2728) | Bit/dim 3.6565(3.6470) | Xent 0.6169(0.5770) | Loss 13.8547(15.3902) | Error 0.2244(0.2048) Steps 0(0.00) | Grad Norm 9.1544(8.0218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 96.7864, Epoch Time 1178.7543(1102.7915), Bit/dim 3.6494(best: 3.6501), Xent 0.6976, Loss 3.9982, Error 0.2356(best: 0.2344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 19.5103(19.3447) | Bit/dim 3.6375(3.6477) | Xent 0.5908(0.5778) | Loss 14.7814(18.7898) | Error 0.2122(0.2057) Steps 0(0.00) | Grad Norm 6.7682(7.7361) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 17.9768(19.2297) | Bit/dim 3.6636(3.6497) | Xent 0.5453(0.5717) | Loss 14.3489(17.6242) | Error 0.1911(0.2051) Steps 0(0.00) | Grad Norm 6.4365(7.7229) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 18.0090(19.1866) | Bit/dim 3.6069(3.6472) | Xent 0.5546(0.5710) | Loss 14.3036(16.7404) | Error 0.1789(0.2037) Steps 0(0.00) | Grad Norm 6.6912(7.5819) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 19.4236(19.1662) | Bit/dim 3.6661(3.6468) | Xent 0.5845(0.5693) | Loss 14.1745(16.1247) | Error 0.2156(0.2038) Steps 0(0.00) | Grad Norm 11.4767(7.3811) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 20.8010(19.2457) | Bit/dim 3.6131(3.6458) | Xent 0.5311(0.5717) | Loss 14.0660(15.6452) | Error 0.1733(0.2037) Steps 0(0.00) | Grad Norm 4.4110(7.5855) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.2380(19.3035) | Bit/dim 3.6383(3.6466) | Xent 0.5533(0.5768) | Loss 14.4331(15.2675) | Error 0.1967(0.2047) Steps 0(0.00) | Grad Norm 4.9229(7.7271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 96.1234, Epoch Time 1173.6813(1104.9182), Bit/dim 3.6498(best: 3.6494), Xent 0.6745, Loss 3.9871, Error 0.2295(best: 0.2344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 19.0103(19.2674) | Bit/dim 3.6398(3.6462) | Xent 0.5556(0.5665) | Loss 14.5496(18.1955) | Error 0.1922(0.2013) Steps 0(0.00) | Grad Norm 6.2561(7.2027) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 17.0392(19.1500) | Bit/dim 3.6338(3.6452) | Xent 0.5309(0.5607) | Loss 14.1507(17.1107) | Error 0.1889(0.2000) Steps 0(0.00) | Grad Norm 8.0890(7.3237) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 18.6046(19.1385) | Bit/dim 3.6629(3.6455) | Xent 0.5985(0.5621) | Loss 14.2786(16.4174) | Error 0.2278(0.1997) Steps 0(0.00) | Grad Norm 8.7941(7.4409) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 19.5075(19.1216) | Bit/dim 3.6475(3.6440) | Xent 0.5703(0.5726) | Loss 14.4913(15.8366) | Error 0.1900(0.2034) Steps 0(0.00) | Grad Norm 7.0922(8.3762) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 19.9342(19.1240) | Bit/dim 3.6436(3.6438) | Xent 0.5618(0.5766) | Loss 14.3592(15.4597) | Error 0.2044(0.2052) Steps 0(0.00) | Grad Norm 5.6848(7.8720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 97.0669, Epoch Time 1160.5398(1106.5868), Bit/dim 3.6522(best: 3.6494), Xent 0.6843, Loss 3.9943, Error 0.2361(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 19.2028(19.0209) | Bit/dim 3.6591(3.6454) | Xent 0.5844(0.5725) | Loss 14.0296(18.7713) | Error 0.2044(0.2041) Steps 0(0.00) | Grad Norm 9.0645(7.7201) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 18.4229(19.0130) | Bit/dim 3.6596(3.6439) | Xent 0.5070(0.5637) | Loss 14.4780(17.5980) | Error 0.1711(0.1996) Steps 0(0.00) | Grad Norm 7.9411(7.5945) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 17.9998(18.8698) | Bit/dim 3.6610(3.6466) | Xent 0.5517(0.5692) | Loss 13.8653(16.6828) | Error 0.2033(0.2018) Steps 0(0.00) | Grad Norm 7.3410(8.2911) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 19.3620(19.0190) | Bit/dim 3.6606(3.6470) | Xent 0.5522(0.5671) | Loss 14.5419(16.0960) | Error 0.2089(0.2019) Steps 0(0.00) | Grad Norm 9.1037(8.0875) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 19.4799(19.0165) | Bit/dim 3.6339(3.6503) | Xent 0.5674(0.5634) | Loss 14.1317(15.6415) | Error 0.1978(0.2006) Steps 0(0.00) | Grad Norm 5.3424(7.8344) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 17.7708(18.8877) | Bit/dim 3.6093(3.6439) | Xent 0.4790(0.5594) | Loss 14.0029(15.2576) | Error 0.1744(0.1987) Steps 0(0.00) | Grad Norm 6.5771(7.3987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 97.9180, Epoch Time 1154.9130(1108.0366), Bit/dim 3.6477(best: 3.6494), Xent 0.7108, Loss 4.0031, Error 0.2399(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 19.3421(18.7877) | Bit/dim 3.6345(3.6443) | Xent 0.5391(0.5563) | Loss 14.2271(18.0559) | Error 0.1967(0.1979) Steps 0(0.00) | Grad Norm 7.3935(7.4800) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 18.6820(18.9006) | Bit/dim 3.6614(3.6461) | Xent 0.5692(0.5526) | Loss 14.0575(17.0861) | Error 0.1956(0.1956) Steps 0(0.00) | Grad Norm 8.0892(7.5317) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.0000(18.8552) | Bit/dim 3.6408(3.6437) | Xent 0.5456(0.5493) | Loss 13.8769(16.3398) | Error 0.1922(0.1945) Steps 0(0.00) | Grad Norm 8.8397(7.3993) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 20.5152(19.0595) | Bit/dim 3.6630(3.6430) | Xent 0.5183(0.5527) | Loss 14.6787(15.8431) | Error 0.1767(0.1955) Steps 0(0.00) | Grad Norm 6.1325(7.4577) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 19.7347(19.2245) | Bit/dim 3.6394(3.6423) | Xent 0.5479(0.5551) | Loss 14.4808(15.4927) | Error 0.1856(0.1968) Steps 0(0.00) | Grad Norm 12.8492(7.6022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 97.9924, Epoch Time 1169.4182(1109.8780), Bit/dim 3.6421(best: 3.6477), Xent 0.7046, Loss 3.9944, Error 0.2410(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 19.3866(19.1851) | Bit/dim 3.6659(3.6418) | Xent 0.5358(0.5597) | Loss 14.7058(18.9056) | Error 0.2000(0.1991) Steps 0(0.00) | Grad Norm 6.8895(8.2341) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 19.4356(19.0843) | Bit/dim 3.6472(3.6459) | Xent 0.5779(0.5624) | Loss 14.5053(17.7034) | Error 0.2078(0.2009) Steps 0(0.00) | Grad Norm 6.0866(8.3995) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 21.2473(19.0535) | Bit/dim 3.6522(3.6439) | Xent 0.5253(0.5581) | Loss 14.5064(16.7431) | Error 0.1800(0.2003) Steps 0(0.00) | Grad Norm 5.0773(7.7332) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 18.6189(19.2230) | Bit/dim 3.6274(3.6440) | Xent 0.5293(0.5485) | Loss 14.5927(16.1303) | Error 0.1867(0.1956) Steps 0(0.00) | Grad Norm 7.9265(7.0481) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 19.0387(19.2511) | Bit/dim 3.6557(3.6408) | Xent 0.5258(0.5454) | Loss 14.2873(15.6565) | Error 0.1800(0.1930) Steps 0(0.00) | Grad Norm 4.9192(6.8653) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 19.0564(19.1978) | Bit/dim 3.6838(3.6422) | Xent 0.5194(0.5428) | Loss 14.0538(15.2553) | Error 0.1900(0.1917) Steps 0(0.00) | Grad Norm 5.2862(6.7839) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 96.5673, Epoch Time 1168.7780(1111.6450), Bit/dim 3.6402(best: 3.6421), Xent 0.6720, Loss 3.9762, Error 0.2323(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 18.3936(19.1373) | Bit/dim 3.6465(3.6422) | Xent 0.5943(0.5431) | Loss 14.4378(18.1374) | Error 0.2022(0.1920) Steps 0(0.00) | Grad Norm 9.4393(6.8753) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 20.1908(19.2700) | Bit/dim 3.6304(3.6394) | Xent 0.5942(0.5416) | Loss 14.1616(17.1563) | Error 0.2044(0.1918) Steps 0(0.00) | Grad Norm 6.7812(7.1682) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 17.8190(19.1469) | Bit/dim 3.6308(3.6411) | Xent 0.5845(0.5514) | Loss 14.4369(16.3735) | Error 0.2211(0.1951) Steps 0(0.00) | Grad Norm 9.1677(7.9026) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 18.3531(19.0637) | Bit/dim 3.6339(3.6411) | Xent 0.5434(0.5620) | Loss 14.4872(15.8200) | Error 0.1867(0.1983) Steps 0(0.00) | Grad Norm 4.6578(8.3212) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 17.9172(18.9806) | Bit/dim 3.6115(3.6414) | Xent 0.5498(0.5624) | Loss 13.9256(15.4244) | Error 0.1900(0.1985) Steps 0(0.00) | Grad Norm 4.1376(8.5380) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 96.6027, Epoch Time 1161.3862(1113.1373), Bit/dim 3.6458(best: 3.6402), Xent 0.7156, Loss 4.0036, Error 0.2411(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 18.5453(19.0854) | Bit/dim 3.6702(3.6453) | Xent 0.5547(0.5541) | Loss 14.4019(18.8444) | Error 0.1967(0.1961) Steps 0(0.00) | Grad Norm 7.4812(8.2847) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 19.1722(19.0574) | Bit/dim 3.6629(3.6482) | Xent 0.5406(0.5523) | Loss 14.4410(17.6163) | Error 0.1844(0.1954) Steps 0(0.00) | Grad Norm 8.0098(8.5805) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 19.2981(19.1313) | Bit/dim 3.6355(3.6447) | Xent 0.5306(0.5477) | Loss 14.5576(16.7775) | Error 0.1878(0.1941) Steps 0(0.00) | Grad Norm 6.5433(8.0972) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 18.1011(19.1365) | Bit/dim 3.6389(3.6427) | Xent 0.5263(0.5444) | Loss 14.1699(16.1518) | Error 0.1822(0.1934) Steps 0(0.00) | Grad Norm 10.5803(8.2502) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.9133(19.2044) | Bit/dim 3.6042(3.6389) | Xent 0.5318(0.5373) | Loss 14.1909(15.6580) | Error 0.1933(0.1905) Steps 0(0.00) | Grad Norm 5.2530(7.7524) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 19.5063(19.3082) | Bit/dim 3.6049(3.6388) | Xent 0.5078(0.5398) | Loss 14.1217(15.3048) | Error 0.1622(0.1916) Steps 0(0.00) | Grad Norm 7.2521(7.7413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 97.5609, Epoch Time 1178.8072(1115.1074), Bit/dim 3.6387(best: 3.6402), Xent 0.7164, Loss 3.9970, Error 0.2455(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.4089(19.3300) | Bit/dim 3.6156(3.6386) | Xent 0.5387(0.5395) | Loss 14.2176(18.2369) | Error 0.1878(0.1916) Steps 0(0.00) | Grad Norm 5.8968(7.7418) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 17.9249(19.2013) | Bit/dim 3.6292(3.6379) | Xent 0.5322(0.5364) | Loss 14.3299(17.1852) | Error 0.1878(0.1914) Steps 0(0.00) | Grad Norm 9.1029(7.7634) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 19.0107(19.3736) | Bit/dim 3.6491(3.6424) | Xent 0.5208(0.5375) | Loss 13.6136(16.4393) | Error 0.1822(0.1909) Steps 0(0.00) | Grad Norm 7.8574(7.9254) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 18.5484(19.3934) | Bit/dim 3.6582(3.6412) | Xent 0.5800(0.5507) | Loss 13.7543(15.8744) | Error 0.2089(0.1964) Steps 0(0.00) | Grad Norm 11.5382(8.8790) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 18.4955(19.2855) | Bit/dim 3.6299(3.6414) | Xent 0.5495(0.5495) | Loss 13.4030(15.4280) | Error 0.1822(0.1951) Steps 0(0.00) | Grad Norm 7.5183(8.5035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 97.4678, Epoch Time 1182.8409(1117.1394), Bit/dim 3.6404(best: 3.6387), Xent 0.6892, Loss 3.9850, Error 0.2332(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 18.3006(19.3478) | Bit/dim 3.6455(3.6419) | Xent 0.5264(0.5441) | Loss 14.3488(18.7400) | Error 0.1878(0.1920) Steps 0(0.00) | Grad Norm 8.4373(8.1745) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 18.9062(19.3380) | Bit/dim 3.6548(3.6451) | Xent 0.5459(0.5403) | Loss 14.1035(17.5921) | Error 0.1911(0.1911) Steps 0(0.00) | Grad Norm 12.6208(8.4429) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 18.2217(19.1999) | Bit/dim 3.6707(3.6460) | Xent 0.5741(0.5396) | Loss 14.5465(16.7090) | Error 0.2189(0.1913) Steps 0(0.00) | Grad Norm 11.1827(8.7166) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 17.9151(19.1816) | Bit/dim 3.6055(3.6447) | Xent 0.4966(0.5415) | Loss 13.8046(16.0473) | Error 0.1689(0.1915) Steps 0(0.00) | Grad Norm 4.6992(8.5352) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 19.5146(19.2142) | Bit/dim 3.6104(3.6405) | Xent 0.5426(0.5434) | Loss 14.1237(15.5688) | Error 0.1922(0.1913) Steps 0(0.00) | Grad Norm 4.7132(8.0075) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 21.1246(19.5854) | Bit/dim 3.6435(3.6421) | Xent 0.5250(0.5420) | Loss 14.6093(15.2325) | Error 0.1867(0.1912) Steps 0(0.00) | Grad Norm 5.4929(7.7761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 97.0049, Epoch Time 1182.6866(1119.1058), Bit/dim 3.6449(best: 3.6387), Xent 0.6874, Loss 3.9886, Error 0.2351(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 18.4929(19.4916) | Bit/dim 3.6191(3.6428) | Xent 0.5925(0.5400) | Loss 14.7288(18.1294) | Error 0.2133(0.1911) Steps 0(0.00) | Grad Norm 14.2373(7.8419) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.8594(19.4878) | Bit/dim 3.6571(3.6418) | Xent 0.5105(0.5449) | Loss 14.6294(17.1242) | Error 0.1989(0.1930) Steps 0(0.00) | Grad Norm 5.5472(7.5020) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 18.6297(19.3488) | Bit/dim 3.6220(3.6406) | Xent 0.5062(0.5361) | Loss 13.5192(16.3917) | Error 0.1833(0.1905) Steps 0(0.00) | Grad Norm 6.0456(7.2639) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 20.2429(19.4134) | Bit/dim 3.6574(3.6426) | Xent 0.5124(0.5332) | Loss 14.8923(15.8562) | Error 0.1811(0.1903) Steps 0(0.00) | Grad Norm 5.8848(7.3086) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 19.5016(19.3905) | Bit/dim 3.6376(3.6423) | Xent 0.5025(0.5291) | Loss 14.1294(15.4532) | Error 0.1756(0.1880) Steps 0(0.00) | Grad Norm 6.2581(7.3090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 96.8782, Epoch Time 1177.5731(1120.8598), Bit/dim 3.6448(best: 3.6387), Xent 0.6651, Loss 3.9774, Error 0.2283(best: 0.2295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 18.8395(19.3777) | Bit/dim 3.6805(3.6394) | Xent 0.4952(0.5301) | Loss 14.4835(18.8226) | Error 0.1733(0.1889) Steps 0(0.00) | Grad Norm 8.9540(7.7825) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 19.9918(19.4040) | Bit/dim 3.6422(3.6397) | Xent 0.5323(0.5313) | Loss 13.8880(17.6116) | Error 0.1978(0.1892) Steps 0(0.00) | Grad Norm 12.3324(7.8092) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 18.9216(19.4432) | Bit/dim 3.6191(3.6398) | Xent 0.5155(0.5295) | Loss 13.9519(16.7088) | Error 0.1967(0.1887) Steps 0(0.00) | Grad Norm 8.6167(7.9494) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 18.8477(19.2871) | Bit/dim 3.6725(3.6385) | Xent 0.4664(0.5279) | Loss 14.2180(16.0723) | Error 0.1711(0.1882) Steps 0(0.00) | Grad Norm 4.2317(7.6330) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 19.1148(19.2167) | Bit/dim 3.6701(3.6391) | Xent 0.5980(0.5304) | Loss 14.8500(15.6028) | Error 0.2056(0.1888) Steps 0(0.00) | Grad Norm 11.0264(7.4684) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 20.2669(19.2743) | Bit/dim 3.6268(3.6391) | Xent 0.5350(0.5376) | Loss 13.9183(15.2066) | Error 0.1933(0.1907) Steps 0(0.00) | Grad Norm 8.7158(7.9581) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 97.8439, Epoch Time 1176.5288(1122.5299), Bit/dim 3.6454(best: 3.6387), Xent 0.7093, Loss 4.0001, Error 0.2413(best: 0.2283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.0861(19.2285) | Bit/dim 3.6562(3.6385) | Xent 0.4835(0.5339) | Loss 14.8697(18.1652) | Error 0.1778(0.1896) Steps 0(0.00) | Grad Norm 6.0073(7.9477) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 19.0330(19.3050) | Bit/dim 3.6294(3.6413) | Xent 0.4609(0.5274) | Loss 14.2926(17.1686) | Error 0.1689(0.1878) Steps 0(0.00) | Grad Norm 3.5233(7.8993) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 20.3110(19.2973) | Bit/dim 3.6372(3.6431) | Xent 0.5196(0.5310) | Loss 14.6515(16.4137) | Error 0.1856(0.1899) Steps 0(0.00) | Grad Norm 7.8308(8.1066) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 20.3103(19.3370) | Bit/dim 3.6488(3.6416) | Xent 0.4832(0.5273) | Loss 14.8277(15.8705) | Error 0.1578(0.1884) Steps 0(0.00) | Grad Norm 5.8881(7.6130) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.5879(19.3365) | Bit/dim 3.6270(3.6404) | Xent 0.5231(0.5227) | Loss 14.3020(15.4843) | Error 0.1867(0.1860) Steps 0(0.00) | Grad Norm 7.8777(7.2803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 97.7553, Epoch Time 1179.7246(1124.2457), Bit/dim 3.6371(best: 3.6387), Xent 0.6987, Loss 3.9864, Error 0.2354(best: 0.2283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 19.5169(19.3207) | Bit/dim 3.6361(3.6370) | Xent 0.5186(0.5113) | Loss 14.1780(18.5968) | Error 0.1956(0.1820) Steps 0(0.00) | Grad Norm 5.7163(6.8721) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 25.9879(19.6308) | Bit/dim 3.6041(3.6326) | Xent 0.5217(0.5076) | Loss 15.1603(17.4633) | Error 0.1867(0.1798) Steps 0(0.00) | Grad Norm 9.6373(7.0157) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 18.9476(19.6084) | Bit/dim 3.6330(3.6325) | Xent 0.5101(0.5038) | Loss 14.3638(16.6771) | Error 0.1856(0.1781) Steps 0(0.00) | Grad Norm 6.8264(6.7760) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 18.5428(19.4110) | Bit/dim 3.6509(3.6326) | Xent 0.5036(0.5135) | Loss 14.1439(16.0350) | Error 0.1944(0.1819) Steps 0(0.00) | Grad Norm 5.1202(7.1443) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 20.2055(19.3917) | Bit/dim 3.6628(3.6341) | Xent 0.5322(0.5167) | Loss 14.8746(15.5472) | Error 0.1767(0.1842) Steps 0(0.00) | Grad Norm 11.4249(7.3036) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 19.1744(19.2660) | Bit/dim 3.6513(3.6384) | Xent 0.6210(0.5325) | Loss 14.6255(15.2692) | Error 0.2122(0.1881) Steps 0(0.00) | Grad Norm 11.8008(8.1415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 98.2205, Epoch Time 1181.7716(1125.9715), Bit/dim 3.6490(best: 3.6371), Xent 0.7289, Loss 4.0134, Error 0.2435(best: 0.2283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 20.6627(19.2735) | Bit/dim 3.6550(3.6393) | Xent 0.5195(0.5278) | Loss 14.3936(18.0808) | Error 0.1933(0.1867) Steps 0(0.00) | Grad Norm 9.3781(8.1538) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 19.3158(19.4624) | Bit/dim 3.6213(3.6394) | Xent 0.5143(0.5274) | Loss 14.1188(17.0793) | Error 0.1822(0.1876) Steps 0(0.00) | Grad Norm 9.7495(8.2515) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 20.6644(19.5793) | Bit/dim 3.6862(3.6426) | Xent 0.4992(0.5229) | Loss 14.9229(16.3434) | Error 0.1733(0.1864) Steps 0(0.00) | Grad Norm 5.5928(8.0843) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 18.8744(19.4769) | Bit/dim 3.6320(3.6368) | Xent 0.5847(0.5224) | Loss 14.8831(15.8315) | Error 0.2089(0.1867) Steps 0(0.00) | Grad Norm 6.6064(7.6556) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 18.8526(19.6448) | Bit/dim 3.6243(3.6347) | Xent 0.5223(0.5213) | Loss 14.4703(15.4651) | Error 0.1733(0.1861) Steps 0(0.00) | Grad Norm 5.5480(7.4615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 97.2797, Epoch Time 1199.2895(1128.1710), Bit/dim 3.6335(best: 3.6371), Xent 0.6664, Loss 3.9667, Error 0.2256(best: 0.2283)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 23.3094(19.8188) | Bit/dim 3.6180(3.6345) | Xent 0.5044(0.5153) | Loss 14.6632(18.8534) | Error 0.1767(0.1830) Steps 0(0.00) | Grad Norm 7.9034(7.1395) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 19.7483(19.7088) | Bit/dim 3.6656(3.6348) | Xent 0.4604(0.5145) | Loss 14.2382(17.6635) | Error 0.1700(0.1824) Steps 0(0.00) | Grad Norm 8.2772(7.1614) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 21.7631(19.8450) | Bit/dim 3.6398(3.6343) | Xent 0.5110(0.5145) | Loss 14.8038(16.8751) | Error 0.1811(0.1824) Steps 0(0.00) | Grad Norm 10.3155(7.3958) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 19.8300(19.7358) | Bit/dim 3.6110(3.6333) | Xent 0.4782(0.5170) | Loss 14.3813(16.2210) | Error 0.1767(0.1849) Steps 0(0.00) | Grad Norm 5.3895(7.4114) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 19.7633(19.8780) | Bit/dim 3.6360(3.6346) | Xent 0.5612(0.5230) | Loss 14.1569(15.7592) | Error 0.1967(0.1870) Steps 0(0.00) | Grad Norm 6.3116(7.5220) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 19.0149(19.7779) | Bit/dim 3.6205(3.6324) | Xent 0.4741(0.5194) | Loss 14.5227(15.4069) | Error 0.1689(0.1861) Steps 0(0.00) | Grad Norm 4.6204(7.2456) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 99.7064, Epoch Time 1208.1568(1130.5706), Bit/dim 3.6367(best: 3.6335), Xent 0.6875, Loss 3.9804, Error 0.2325(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 19.1335(19.6070) | Bit/dim 3.6459(3.6317) | Xent 0.5316(0.5156) | Loss 14.3518(18.3268) | Error 0.1967(0.1847) Steps 0(0.00) | Grad Norm 9.3830(7.8616) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 19.9924(19.5548) | Bit/dim 3.6396(3.6342) | Xent 0.5081(0.5138) | Loss 14.8658(17.3016) | Error 0.1856(0.1841) Steps 0(0.00) | Grad Norm 4.9353(7.5287) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 19.1471(19.4559) | Bit/dim 3.5957(3.6326) | Xent 0.5075(0.5178) | Loss 13.8850(16.4986) | Error 0.1911(0.1861) Steps 0(0.00) | Grad Norm 6.9939(7.6588) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 17.7260(19.4198) | Bit/dim 3.6136(3.6312) | Xent 0.5034(0.5161) | Loss 13.9746(15.9255) | Error 0.1722(0.1855) Steps 0(0.00) | Grad Norm 4.7905(7.5451) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 18.8351(19.5713) | Bit/dim 3.6150(3.6282) | Xent 0.5182(0.5057) | Loss 14.1750(15.4933) | Error 0.1878(0.1820) Steps 0(0.00) | Grad Norm 4.9624(6.9159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 98.5878, Epoch Time 1187.2430(1132.2708), Bit/dim 3.6299(best: 3.6335), Xent 0.6684, Loss 3.9641, Error 0.2282(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 18.3844(19.6146) | Bit/dim 3.6160(3.6276) | Xent 0.5076(0.5013) | Loss 14.2694(19.0017) | Error 0.1533(0.1793) Steps 0(0.00) | Grad Norm 7.9229(6.8579) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.3757(19.5922) | Bit/dim 3.6569(3.6289) | Xent 0.5458(0.4973) | Loss 14.2339(17.7850) | Error 0.1911(0.1784) Steps 0(0.00) | Grad Norm 11.7314(6.9016) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 18.6648(19.5507) | Bit/dim 3.6341(3.6311) | Xent 0.5450(0.5197) | Loss 14.2870(16.9323) | Error 0.2022(0.1862) Steps 0(0.00) | Grad Norm 12.9707(8.4837) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.7131(19.5587) | Bit/dim 3.6500(3.6304) | Xent 0.5222(0.5247) | Loss 14.7108(16.2792) | Error 0.1900(0.1884) Steps 0(0.00) | Grad Norm 9.2486(8.5664) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 20.8729(19.5707) | Bit/dim 3.6518(3.6346) | Xent 0.5576(0.5239) | Loss 14.8403(15.7783) | Error 0.1978(0.1880) Steps 0(0.00) | Grad Norm 12.5159(8.5334) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 19.9856(19.5372) | Bit/dim 3.6398(3.6351) | Xent 0.4900(0.5292) | Loss 14.1654(15.3848) | Error 0.1700(0.1890) Steps 0(0.00) | Grad Norm 8.2692(8.7677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 97.8663, Epoch Time 1190.0867(1134.0053), Bit/dim 3.6384(best: 3.6299), Xent 0.7290, Loss 4.0029, Error 0.2463(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 21.4816(19.5275) | Bit/dim 3.6180(3.6336) | Xent 0.4788(0.5232) | Loss 14.2829(18.3089) | Error 0.1722(0.1870) Steps 0(0.00) | Grad Norm 5.2090(8.4955) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 22.8324(19.5150) | Bit/dim 3.6225(3.6328) | Xent 0.4934(0.5167) | Loss 15.2501(17.2630) | Error 0.1767(0.1848) Steps 0(0.00) | Grad Norm 6.3733(7.7943) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 19.7342(19.5873) | Bit/dim 3.6533(3.6338) | Xent 0.4728(0.5065) | Loss 14.6117(16.5145) | Error 0.1767(0.1812) Steps 0(0.00) | Grad Norm 5.3080(7.3072) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 19.9143(19.6881) | Bit/dim 3.6101(3.6332) | Xent 0.4658(0.5072) | Loss 14.3157(15.9414) | Error 0.1589(0.1816) Steps 0(0.00) | Grad Norm 7.4046(7.2931) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.6659(19.7378) | Bit/dim 3.6035(3.6323) | Xent 0.5197(0.5095) | Loss 14.2736(15.4908) | Error 0.1867(0.1832) Steps 0(0.00) | Grad Norm 8.1750(7.7929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 96.5022, Epoch Time 1197.1484(1135.8996), Bit/dim 3.6287(best: 3.6299), Xent 0.6788, Loss 3.9681, Error 0.2293(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 19.4688(19.6080) | Bit/dim 3.6667(3.6305) | Xent 0.4942(0.5105) | Loss 14.0867(18.8760) | Error 0.1744(0.1844) Steps 0(0.00) | Grad Norm 9.3720(7.6818) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 20.6806(19.6425) | Bit/dim 3.6426(3.6336) | Xent 0.5499(0.5143) | Loss 14.0415(17.6648) | Error 0.1922(0.1856) Steps 0(0.00) | Grad Norm 8.5688(8.0957) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 21.6634(19.7056) | Bit/dim 3.6381(3.6352) | Xent 0.5518(0.5149) | Loss 14.3763(16.7836) | Error 0.2089(0.1857) Steps 0(0.00) | Grad Norm 7.4518(8.2257) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 19.3491(19.6812) | Bit/dim 3.6385(3.6349) | Xent 0.4638(0.5097) | Loss 14.4486(16.1460) | Error 0.1622(0.1833) Steps 0(0.00) | Grad Norm 5.3950(7.6237) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 18.0749(19.8887) | Bit/dim 3.6225(3.6347) | Xent 0.4733(0.5075) | Loss 14.3407(15.6830) | Error 0.1822(0.1833) Steps 0(0.00) | Grad Norm 7.4913(7.7125) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 21.7600(19.9904) | Bit/dim 3.6196(3.6309) | Xent 0.5029(0.5020) | Loss 14.9144(15.3602) | Error 0.1856(0.1815) Steps 0(0.00) | Grad Norm 6.8010(7.4010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 96.7500, Epoch Time 1208.1797(1138.0680), Bit/dim 3.6351(best: 3.6287), Xent 0.6698, Loss 3.9700, Error 0.2298(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 20.9940(19.9015) | Bit/dim 3.6502(3.6273) | Xent 0.4842(0.4936) | Loss 14.2213(18.3332) | Error 0.1756(0.1771) Steps 0(0.00) | Grad Norm 6.4983(6.9333) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 23.1961(20.0713) | Bit/dim 3.6364(3.6274) | Xent 0.4807(0.4868) | Loss 14.5114(17.2816) | Error 0.1689(0.1744) Steps 0(0.00) | Grad Norm 9.3694(7.0034) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 19.2199(19.9532) | Bit/dim 3.5946(3.6279) | Xent 0.5194(0.4885) | Loss 14.3934(16.4918) | Error 0.1800(0.1742) Steps 0(0.00) | Grad Norm 10.0842(7.2807) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 18.6961(19.8714) | Bit/dim 3.6602(3.6275) | Xent 0.4711(0.4901) | Loss 13.9523(15.9207) | Error 0.1744(0.1740) Steps 0(0.00) | Grad Norm 10.6165(7.3070) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 21.1780(19.9826) | Bit/dim 3.6268(3.6270) | Xent 0.4493(0.4865) | Loss 14.3859(15.4242) | Error 0.1678(0.1738) Steps 0(0.00) | Grad Norm 5.6899(7.1495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 96.6335, Epoch Time 1205.8282(1140.1008), Bit/dim 3.6371(best: 3.6287), Xent 0.6952, Loss 3.9847, Error 0.2353(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 18.9595(19.8043) | Bit/dim 3.5976(3.6271) | Xent 0.5624(0.4949) | Loss 14.1366(18.7407) | Error 0.2189(0.1778) Steps 0(0.00) | Grad Norm 8.4303(7.3074) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 18.7354(19.6761) | Bit/dim 3.6737(3.6315) | Xent 0.5294(0.4945) | Loss 14.7814(17.5634) | Error 0.1900(0.1777) Steps 0(0.00) | Grad Norm 6.8714(7.0709) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 21.4979(19.6983) | Bit/dim 3.6067(3.6291) | Xent 0.4934(0.4935) | Loss 14.3597(16.7030) | Error 0.1756(0.1768) Steps 0(0.00) | Grad Norm 7.7220(7.0367) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 20.9080(19.6431) | Bit/dim 3.6387(3.6314) | Xent 0.4147(0.4886) | Loss 14.0660(16.0705) | Error 0.1456(0.1734) Steps 0(0.00) | Grad Norm 6.1957(6.8807) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 20.6972(19.7219) | Bit/dim 3.6371(3.6304) | Xent 0.5222(0.4904) | Loss 14.9693(15.6555) | Error 0.1833(0.1740) Steps 0(0.00) | Grad Norm 6.1088(6.9842) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 18.8381(19.7268) | Bit/dim 3.6540(3.6276) | Xent 0.4799(0.4961) | Loss 14.1743(15.3364) | Error 0.1689(0.1756) Steps 0(0.00) | Grad Norm 6.2369(7.2801) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 98.8603, Epoch Time 1196.2214(1141.7844), Bit/dim 3.6315(best: 3.6287), Xent 0.6775, Loss 3.9702, Error 0.2262(best: 0.2256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 21.6474(19.7914) | Bit/dim 3.6177(3.6304) | Xent 0.4871(0.4905) | Loss 14.3156(18.3495) | Error 0.1822(0.1738) Steps 0(0.00) | Grad Norm 7.6719(7.2821) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 19.2758(19.8974) | Bit/dim 3.6160(3.6309) | Xent 0.4505(0.4880) | Loss 14.5733(17.2788) | Error 0.1544(0.1731) Steps 0(0.00) | Grad Norm 4.5791(7.4068) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 18.5420(19.8349) | Bit/dim 3.6198(3.6288) | Xent 0.4833(0.4931) | Loss 14.2327(16.4898) | Error 0.1756(0.1742) Steps 0(0.00) | Grad Norm 6.2768(7.7013) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 19.0314(19.8461) | Bit/dim 3.6052(3.6254) | Xent 0.4219(0.4838) | Loss 14.2755(15.9233) | Error 0.1711(0.1729) Steps 0(0.00) | Grad Norm 4.6792(7.1226) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_0_5_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
