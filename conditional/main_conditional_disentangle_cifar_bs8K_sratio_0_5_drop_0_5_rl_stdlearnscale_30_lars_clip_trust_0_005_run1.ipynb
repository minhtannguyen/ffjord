{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_005_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.005, val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 104.1842(104.1842) | Bit/dim 8.8973(8.8973) | Xent 2.3026(2.3026) | Loss 20.9453(20.9453) | Error 0.8982(0.8982) Steps 430(430.00) | Grad Norm 27.4559(27.4559) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 39.7239(102.2504) | Bit/dim 8.8291(8.8953) | Xent 2.2924(2.3023) | Loss 20.7805(20.9403) | Error 0.7824(0.8948) Steps 478(431.44) | Grad Norm 24.9291(27.3801) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 38.0156(100.3233) | Bit/dim 8.7457(8.8908) | Xent 2.2820(2.3017) | Loss 20.6518(20.9317) | Error 0.7708(0.8911) Steps 490(433.20) | Grad Norm 20.9295(27.1866) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 37.8190(98.4482) | Bit/dim 8.6340(8.8831) | Xent 2.2699(2.3007) | Loss 19.9288(20.9016) | Error 0.7618(0.8872) Steps 466(434.18) | Grad Norm 16.2135(26.8574) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 36.3029(96.5838) | Bit/dim 8.5956(8.8745) | Xent 2.2573(2.2994) | Loss 20.0845(20.8771) | Error 0.7628(0.8834) Steps 442(434.42) | Grad Norm 11.0344(26.3827) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 37.9520(94.8249) | Bit/dim 8.4956(8.8631) | Xent 2.2391(2.2976) | Loss 19.7997(20.8448) | Error 0.7430(0.8792) Steps 454(435.00) | Grad Norm 8.1369(25.8353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 37.1299, Epoch Time 350.5508(350.5508), Bit/dim 8.4569(best: inf), Xent 2.2199, Loss 9.5669, Error 0.7383(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 45.3789(93.3415) | Bit/dim 8.4643(8.8511) | Xent 2.2216(2.2953) | Loss 22.3968(20.8913) | Error 0.7470(0.8753) Steps 460(435.75) | Grad Norm 8.0496(25.3018) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 36.5289(91.6371) | Bit/dim 8.4307(8.8385) | Xent 2.2073(2.2927) | Loss 19.8806(20.8610) | Error 0.7492(0.8715) Steps 454(436.30) | Grad Norm 11.0048(24.8729) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 36.1997(89.9740) | Bit/dim 8.4160(8.8259) | Xent 2.1876(2.2895) | Loss 19.3304(20.8151) | Error 0.7552(0.8680) Steps 448(436.65) | Grad Norm 13.6100(24.5350) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 35.7821(88.3482) | Bit/dim 8.3763(8.8124) | Xent 2.1762(2.2861) | Loss 19.5639(20.7775) | Error 0.7700(0.8651) Steps 454(437.17) | Grad Norm 15.1107(24.2522) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 33.8151(86.7122) | Bit/dim 8.3423(8.7983) | Xent 2.1650(2.2825) | Loss 19.5125(20.7396) | Error 0.7664(0.8621) Steps 430(436.96) | Grad Norm 15.5660(23.9917) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 35.5831(85.1784) | Bit/dim 8.2496(8.7818) | Xent 2.1491(2.2785) | Loss 19.4452(20.7008) | Error 0.7564(0.8589) Steps 466(437.83) | Grad Norm 13.9814(23.6914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 19.5951, Epoch Time 261.8185(347.8888), Bit/dim 8.1855(best: 8.4569), Xent 2.1194, Loss 9.2452, Error 0.7308(best: 0.7383)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 40.6182(83.8416) | Bit/dim 8.2247(8.7651) | Xent 2.1203(2.2738) | Loss 21.6317(20.7287) | Error 0.7390(0.8553) Steps 490(439.39) | Grad Norm 11.3856(23.3222) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 34.3717(82.3575) | Bit/dim 8.1140(8.7456) | Xent 2.1091(2.2688) | Loss 18.8131(20.6712) | Error 0.7220(0.8513) Steps 430(439.11) | Grad Norm 7.7094(22.8538) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 36.8158(80.9912) | Bit/dim 8.0778(8.7255) | Xent 2.0921(2.2635) | Loss 18.4629(20.6050) | Error 0.7205(0.8474) Steps 454(439.56) | Grad Norm 5.8997(22.3452) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 37.5708(79.6886) | Bit/dim 7.9747(8.7030) | Xent 2.0738(2.2578) | Loss 18.8419(20.5521) | Error 0.7245(0.8437) Steps 418(438.91) | Grad Norm 7.8739(21.9110) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 36.4699(78.3921) | Bit/dim 7.9262(8.6797) | Xent 2.0759(2.2524) | Loss 18.4279(20.4884) | Error 0.7245(0.8401) Steps 460(439.54) | Grad Norm 10.6528(21.5733) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 38.6190(77.1989) | Bit/dim 7.8680(8.6553) | Xent 2.0623(2.2467) | Loss 18.7768(20.4370) | Error 0.7241(0.8367) Steps 466(440.34) | Grad Norm 11.3638(21.2670) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 18.5838, Epoch Time 261.5975(345.3001), Bit/dim 7.7785(best: 8.1855), Xent 2.0365, Loss 8.7968, Error 0.7027(best: 0.7308)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 42.3751(76.1541) | Bit/dim 7.7844(8.6292) | Xent 2.0499(2.2408) | Loss 21.1370(20.4580) | Error 0.7132(0.8330) Steps 490(441.83) | Grad Norm 10.3059(20.9382) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 38.7549(75.0322) | Bit/dim 7.6925(8.6011) | Xent 2.0314(2.2345) | Loss 18.1446(20.3886) | Error 0.6889(0.8286) Steps 442(441.83) | Grad Norm 7.1585(20.5248) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 40.1537(73.9858) | Bit/dim 7.6166(8.5716) | Xent 2.0255(2.2282) | Loss 17.4757(20.3012) | Error 0.6829(0.8243) Steps 436(441.66) | Grad Norm 4.6248(20.0478) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 37.7514(72.8988) | Bit/dim 7.5335(8.5404) | Xent 2.0216(2.2220) | Loss 17.8885(20.2288) | Error 0.6925(0.8203) Steps 454(442.03) | Grad Norm 6.5288(19.6422) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 36.9116(71.8192) | Bit/dim 7.4568(8.5079) | Xent 2.0305(2.2163) | Loss 17.1494(20.1365) | Error 0.7127(0.8171) Steps 424(441.49) | Grad Norm 8.9713(19.3221) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 40.3751(70.8758) | Bit/dim 7.3954(8.4746) | Xent 2.0315(2.2107) | Loss 17.6800(20.0628) | Error 0.7094(0.8138) Steps 442(441.50) | Grad Norm 10.6638(19.0623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 20.4860, Epoch Time 275.0297(343.1920), Bit/dim 7.3301(best: 7.7785), Xent 2.0032, Loss 8.3317, Error 0.6825(best: 0.7027)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 43.6296(70.0585) | Bit/dim 7.3321(8.4403) | Xent 2.0021(2.2045) | Loss 20.3345(20.0709) | Error 0.6854(0.8100) Steps 424(440.98) | Grad Norm 7.3805(18.7119) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 41.7970(69.2106) | Bit/dim 7.2710(8.4052) | Xent 2.0017(2.1984) | Loss 17.3659(19.9898) | Error 0.6790(0.8061) Steps 484(442.27) | Grad Norm 4.2036(18.2766) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 38.3322(68.2843) | Bit/dim 7.2176(8.3696) | Xent 2.0058(2.1926) | Loss 17.3995(19.9121) | Error 0.6759(0.8022) Steps 466(442.98) | Grad Norm 6.0513(17.9099) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 37.9658(67.3747) | Bit/dim 7.1829(8.3340) | Xent 2.0136(2.1872) | Loss 17.1787(19.8301) | Error 0.6799(0.7985) Steps 478(444.03) | Grad Norm 7.2026(17.5887) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 43.3197(66.6531) | Bit/dim 7.1517(8.2985) | Xent 2.0126(2.1820) | Loss 17.3940(19.7570) | Error 0.6825(0.7950) Steps 478(445.05) | Grad Norm 5.9226(17.2387) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 35.5001(65.7185) | Bit/dim 7.1028(8.2626) | Xent 2.0122(2.1769) | Loss 16.9091(19.6715) | Error 0.6861(0.7917) Steps 454(445.32) | Grad Norm 2.8990(16.8085) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 20.0120, Epoch Time 279.0328(341.2672), Bit/dim 7.0863(best: 7.3301), Xent 2.0135, Loss 8.0930, Error 0.6946(best: 0.6825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 36.9228(64.8546) | Bit/dim 7.0862(8.2273) | Xent 2.0295(2.1725) | Loss 19.4917(19.6661) | Error 0.7100(0.7893) Steps 436(445.04) | Grad Norm 6.2308(16.4912) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 39.5577(64.0957) | Bit/dim 7.0686(8.1926) | Xent 2.0429(2.1686) | Loss 16.9152(19.5836) | Error 0.7219(0.7873) Steps 466(445.67) | Grad Norm 6.2007(16.1824) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 37.9728(63.3120) | Bit/dim 7.0496(8.1583) | Xent 2.0232(2.1642) | Loss 16.6855(19.4967) | Error 0.7105(0.7850) Steps 448(445.74) | Grad Norm 4.4090(15.8292) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 38.7902(62.5763) | Bit/dim 7.0355(8.1246) | Xent 2.0331(2.1603) | Loss 16.7494(19.4143) | Error 0.7110(0.7827) Steps 460(446.17) | Grad Norm 3.9341(15.4724) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 38.0910(61.8418) | Bit/dim 7.0192(8.0914) | Xent 2.0329(2.1565) | Loss 16.9709(19.3410) | Error 0.7180(0.7808) Steps 496(447.66) | Grad Norm 3.7730(15.1214) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 40.2219(61.1932) | Bit/dim 7.0157(8.0592) | Xent 2.0404(2.1530) | Loss 16.7692(19.2638) | Error 0.7189(0.7789) Steps 454(447.85) | Grad Norm 3.3656(14.7687) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 20.7680, Epoch Time 271.4157(339.1716), Bit/dim 7.0080(best: 7.0863), Xent 2.0300, Loss 8.0230, Error 0.7034(best: 0.6825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 38.9406(60.5256) | Bit/dim 7.0048(8.0275) | Xent 2.0342(2.1494) | Loss 18.4548(19.2395) | Error 0.7101(0.7769) Steps 490(449.12) | Grad Norm 4.3498(14.4562) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 39.5963(59.8977) | Bit/dim 6.9994(7.9967) | Xent 2.0317(2.1459) | Loss 16.6171(19.1609) | Error 0.7144(0.7750) Steps 478(449.98) | Grad Norm 3.9005(14.1395) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 39.4925(59.2856) | Bit/dim 6.9961(7.9667) | Xent 2.0346(2.1426) | Loss 16.3123(19.0754) | Error 0.7164(0.7732) Steps 496(451.36) | Grad Norm 3.3744(13.8165) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 42.0856(58.7696) | Bit/dim 6.9967(7.9376) | Xent 2.0308(2.1392) | Loss 16.6900(19.0038) | Error 0.7112(0.7714) Steps 484(452.34) | Grad Norm 3.4005(13.5041) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 43.4537(58.3101) | Bit/dim 6.9844(7.9090) | Xent 2.0254(2.1358) | Loss 16.9690(18.9428) | Error 0.6925(0.7690) Steps 484(453.29) | Grad Norm 2.6347(13.1780) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 44.5246(57.8965) | Bit/dim 6.9795(7.8811) | Xent 2.0162(2.1322) | Loss 16.5298(18.8704) | Error 0.6906(0.7667) Steps 514(455.11) | Grad Norm 5.5782(12.9500) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 21.3728, Epoch Time 288.5526(337.6531), Bit/dim 6.9795(best: 7.0080), Xent 2.0066, Loss 7.9828, Error 0.6958(best: 0.6825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 43.3067(57.4588) | Bit/dim 6.9695(7.8538) | Xent 2.0143(2.1287) | Loss 19.2535(18.8819) | Error 0.7105(0.7650) Steps 466(455.44) | Grad Norm 12.8524(12.9471) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 46.6672(57.1351) | Bit/dim 6.9820(7.8276) | Xent 2.0990(2.1278) | Loss 16.6826(18.8159) | Error 0.7524(0.7646) Steps 514(457.20) | Grad Norm 38.0995(13.7016) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 45.8157(56.7955) | Bit/dim 6.9757(7.8020) | Xent 2.0180(2.1245) | Loss 16.9932(18.7612) | Error 0.7100(0.7630) Steps 490(458.18) | Grad Norm 20.0513(13.8921) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 45.4715(56.4558) | Bit/dim 6.9599(7.7768) | Xent 2.0270(2.1216) | Loss 16.5094(18.6937) | Error 0.7104(0.7614) Steps 538(460.57) | Grad Norm 24.2724(14.2035) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 46.7556(56.1648) | Bit/dim 6.9678(7.7525) | Xent 2.0353(2.1190) | Loss 16.6909(18.6336) | Error 0.7135(0.7600) Steps 502(461.82) | Grad Norm 29.4774(14.6617) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 44.6799(55.8202) | Bit/dim 6.9448(7.7283) | Xent 1.9955(2.1153) | Loss 16.7945(18.5784) | Error 0.6809(0.7576) Steps 490(462.66) | Grad Norm 13.1674(14.6169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 20.9946, Epoch Time 312.3502(336.8940), Bit/dim 6.9371(best: 6.9795), Xent 1.9658, Loss 7.9200, Error 0.6720(best: 0.6825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 40.6566(55.3653) | Bit/dim 6.9228(7.7041) | Xent 1.9797(2.1112) | Loss 18.8890(18.5877) | Error 0.6799(0.7552) Steps 472(462.94) | Grad Norm 7.9419(14.4167) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 47.3676(55.1254) | Bit/dim 6.9376(7.6811) | Xent 1.9821(2.1073) | Loss 16.6897(18.5308) | Error 0.6656(0.7526) Steps 484(463.57) | Grad Norm 10.5652(14.3011) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 41.6593(54.7214) | Bit/dim 6.9276(7.6585) | Xent 2.0056(2.1043) | Loss 16.4808(18.4693) | Error 0.7000(0.7510) Steps 466(463.65) | Grad Norm 23.3437(14.5724) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 42.4906(54.3545) | Bit/dim 6.9198(7.6364) | Xent 2.0286(2.1020) | Loss 15.8254(18.3900) | Error 0.7090(0.7497) Steps 496(464.62) | Grad Norm 31.5218(15.0809) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 48.1015(54.1669) | Bit/dim 6.9068(7.6145) | Xent 1.9774(2.0983) | Loss 16.5883(18.3359) | Error 0.6843(0.7478) Steps 550(467.18) | Grad Norm 6.8542(14.8341) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 43.0131(53.8323) | Bit/dim 6.8981(7.5930) | Xent 1.9955(2.0952) | Loss 16.4057(18.2780) | Error 0.6994(0.7463) Steps 496(468.04) | Grad Norm 21.3956(15.0309) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 20.5866, Epoch Time 302.7163(335.8687), Bit/dim 6.8894(best: 6.9371), Xent 2.0372, Loss 7.9079, Error 0.7149(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 44.7471(53.5597) | Bit/dim 6.8783(7.5715) | Xent 2.0520(2.0939) | Loss 19.2270(18.3065) | Error 0.7283(0.7458) Steps 502(469.06) | Grad Norm 38.0597(15.7218) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 46.1463(53.3373) | Bit/dim 6.8910(7.5511) | Xent 1.9727(2.0903) | Loss 16.5614(18.2541) | Error 0.6787(0.7438) Steps 502(470.05) | Grad Norm 12.6973(15.6311) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 42.5091(53.0125) | Bit/dim 6.8774(7.5309) | Xent 2.0567(2.0892) | Loss 16.2023(18.1926) | Error 0.7335(0.7434) Steps 502(471.01) | Grad Norm 32.6648(16.1421) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 40.6814(52.6426) | Bit/dim 6.8747(7.5112) | Xent 1.9760(2.0858) | Loss 16.3836(18.1383) | Error 0.6895(0.7418) Steps 478(471.22) | Grad Norm 14.7723(16.1010) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 43.3817(52.3647) | Bit/dim 6.8636(7.4918) | Xent 2.1538(2.0879) | Loss 16.6312(18.0931) | Error 0.7466(0.7420) Steps 490(471.78) | Grad Norm 28.2680(16.4660) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 44.0153(52.1142) | Bit/dim 6.8779(7.4734) | Xent 2.0055(2.0854) | Loss 16.4794(18.0447) | Error 0.6861(0.7403) Steps 478(471.97) | Grad Norm 18.9353(16.5401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.1473, Epoch Time 301.2714(334.8307), Bit/dim 6.8267(best: 6.8894), Xent 2.1902, Loss 7.9218, Error 0.7888(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 42.1700(51.8159) | Bit/dim 6.8167(7.4537) | Xent 2.1856(2.0884) | Loss 19.0757(18.0756) | Error 0.7802(0.7415) Steps 478(472.15) | Grad Norm 36.9099(17.1512) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 45.0853(51.6140) | Bit/dim 6.9231(7.4378) | Xent 2.1304(2.0897) | Loss 16.4672(18.0274) | Error 0.7729(0.7424) Steps 478(472.33) | Grad Norm 34.2888(17.6653) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 40.4812(51.2800) | Bit/dim 6.8080(7.4189) | Xent 1.9675(2.0860) | Loss 15.7997(17.9605) | Error 0.6753(0.7404) Steps 466(472.14) | Grad Norm 12.2231(17.5020) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 43.3311(51.0415) | Bit/dim 6.8184(7.4009) | Xent 2.1090(2.0867) | Loss 16.5949(17.9196) | Error 0.7682(0.7413) Steps 460(471.77) | Grad Norm 40.7413(18.1992) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 42.4497(50.7838) | Bit/dim 6.9313(7.3868) | Xent 2.0263(2.0849) | Loss 16.6164(17.8805) | Error 0.7116(0.7404) Steps 466(471.60) | Grad Norm 19.7125(18.2446) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 45.5101(50.6256) | Bit/dim 6.7421(7.3674) | Xent 1.9925(2.0821) | Loss 16.2754(17.8323) | Error 0.7147(0.7396) Steps 478(471.79) | Grad Norm 24.8473(18.4427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 20.4674, Epoch Time 297.9109(333.7231), Bit/dim 6.7705(best: 6.8267), Xent 2.1886, Loss 7.8648, Error 0.7750(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 40.7909(50.3305) | Bit/dim 6.7683(7.3495) | Xent 2.1875(2.0853) | Loss 18.7907(17.8611) | Error 0.7761(0.7407) Steps 454(471.26) | Grad Norm 32.5299(18.8653) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 42.6008(50.0986) | Bit/dim 6.8454(7.3343) | Xent 2.0222(2.0834) | Loss 16.7200(17.8268) | Error 0.7096(0.7398) Steps 496(472.00) | Grad Norm 16.8338(18.8044) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 42.6385(49.8748) | Bit/dim 6.6975(7.3152) | Xent 2.0446(2.0822) | Loss 16.2319(17.7790) | Error 0.7521(0.7401) Steps 448(471.28) | Grad Norm 29.2421(19.1175) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 43.4334(49.6816) | Bit/dim 6.6768(7.2961) | Xent 2.0767(2.0821) | Loss 16.3869(17.7372) | Error 0.7550(0.7406) Steps 478(471.48) | Grad Norm 39.4560(19.7276) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 42.0858(49.4537) | Bit/dim 6.6975(7.2781) | Xent 2.0607(2.0814) | Loss 16.1301(17.6890) | Error 0.7548(0.7410) Steps 484(471.86) | Grad Norm 31.4947(20.0807) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 42.4185(49.2427) | Bit/dim 6.7011(7.2608) | Xent 2.1406(2.0832) | Loss 16.3378(17.6485) | Error 0.7897(0.7425) Steps 478(472.04) | Grad Norm 39.0351(20.6493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 20.6828, Epoch Time 292.5427(332.4877), Bit/dim 6.6417(best: 6.7705), Xent 2.0124, Loss 7.6479, Error 0.7333(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 46.5705(49.1625) | Bit/dim 6.6366(7.2421) | Xent 2.0340(2.0817) | Loss 17.9973(17.6589) | Error 0.7418(0.7424) Steps 484(472.40) | Grad Norm 21.6123(20.6782) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 44.4292(49.0205) | Bit/dim 6.6470(7.2242) | Xent 2.0790(2.0816) | Loss 16.2645(17.6171) | Error 0.7305(0.7421) Steps 454(471.85) | Grad Norm 38.9033(21.2249) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 41.1686(48.7849) | Bit/dim 6.6008(7.2055) | Xent 2.0630(2.0811) | Loss 15.4662(17.5526) | Error 0.7171(0.7413) Steps 454(471.31) | Grad Norm 32.3721(21.5593) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 40.3672(48.5324) | Bit/dim 6.5104(7.1847) | Xent 2.0404(2.0799) | Loss 15.9066(17.5032) | Error 0.7092(0.7404) Steps 478(471.51) | Grad Norm 14.8547(21.3582) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 43.7597(48.3892) | Bit/dim 6.4875(7.1638) | Xent 2.0322(2.0784) | Loss 15.6431(17.4474) | Error 0.7087(0.7394) Steps 508(472.61) | Grad Norm 25.2998(21.4765) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 43.7180(48.2491) | Bit/dim 6.3937(7.1407) | Xent 2.0110(2.0764) | Loss 15.6265(17.3928) | Error 0.6880(0.7379) Steps 430(471.33) | Grad Norm 7.7754(21.0654) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 20.3207, Epoch Time 298.7955(331.4770), Bit/dim 6.3764(best: 6.6417), Xent 2.0298, Loss 7.3913, Error 0.7084(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 43.2962(48.1005) | Bit/dim 6.3753(7.1177) | Xent 2.0276(2.0749) | Loss 18.3576(17.4217) | Error 0.7086(0.7370) Steps 478(471.53) | Grad Norm 20.7202(21.0551) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 41.9910(47.9172) | Bit/dim 6.3299(7.0941) | Xent 2.0274(2.0735) | Loss 15.6804(17.3695) | Error 0.7040(0.7360) Steps 484(471.90) | Grad Norm 20.7900(21.0471) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 42.4299(47.7526) | Bit/dim 6.2292(7.0681) | Xent 2.0094(2.0716) | Loss 15.3429(17.3087) | Error 0.6887(0.7346) Steps 502(472.81) | Grad Norm 4.7225(20.5574) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 42.0500(47.5815) | Bit/dim 6.1601(7.0409) | Xent 2.0247(2.0702) | Loss 15.0521(17.2410) | Error 0.7054(0.7337) Steps 466(472.60) | Grad Norm 15.0479(20.3921) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 40.6484(47.3735) | Bit/dim 6.1225(7.0133) | Xent 2.0484(2.0695) | Loss 14.9948(17.1736) | Error 0.7243(0.7334) Steps 478(472.76) | Grad Norm 42.6257(21.0591) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 43.7205(47.2639) | Bit/dim 6.0189(6.9835) | Xent 2.0092(2.0677) | Loss 14.8765(17.1047) | Error 0.6870(0.7320) Steps 508(473.82) | Grad Norm 13.0577(20.8191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 21.6071, Epoch Time 294.5028(330.3677), Bit/dim 6.1512(best: 6.3764), Xent 2.1187, Loss 7.2105, Error 0.7690(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 42.5049(47.1212) | Bit/dim 6.1509(6.9585) | Xent 2.1471(2.0701) | Loss 18.3866(17.1431) | Error 0.7797(0.7335) Steps 496(474.49) | Grad Norm 98.3146(23.1439) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 43.0754(46.9998) | Bit/dim 6.1285(6.9336) | Xent 2.1444(2.0723) | Loss 15.2424(17.0861) | Error 0.7736(0.7347) Steps 526(476.03) | Grad Norm 110.6217(25.7683) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 43.8540(46.9054) | Bit/dim 5.9221(6.9033) | Xent 2.0113(2.0705) | Loss 15.0070(17.0237) | Error 0.7026(0.7337) Steps 532(477.71) | Grad Norm 29.7555(25.8879) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 46.2579(46.8860) | Bit/dim 5.9464(6.8746) | Xent 1.9802(2.0678) | Loss 14.7949(16.9569) | Error 0.6846(0.7322) Steps 526(479.16) | Grad Norm 35.1739(26.1665) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 45.6393(46.8486) | Bit/dim 6.0446(6.8497) | Xent 2.1462(2.0701) | Loss 15.0273(16.8990) | Error 0.7624(0.7331) Steps 526(480.56) | Grad Norm 107.2022(28.5975) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 44.3387(46.7733) | Bit/dim 6.0024(6.8242) | Xent 2.0691(2.0701) | Loss 15.0165(16.8425) | Error 0.7335(0.7332) Steps 538(482.29) | Grad Norm 85.3358(30.2997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 23.0394, Epoch Time 307.4547(329.6803), Bit/dim 5.8924(best: 6.1512), Xent 2.0017, Loss 6.8933, Error 0.6998(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 45.3345(46.7301) | Bit/dim 5.9067(6.7967) | Xent 2.0289(2.0689) | Loss 17.6606(16.8671) | Error 0.7169(0.7327) Steps 538(483.96) | Grad Norm 49.3685(30.8717) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 46.9751(46.7375) | Bit/dim 5.8664(6.7688) | Xent 1.9994(2.0668) | Loss 14.4434(16.7944) | Error 0.6939(0.7315) Steps 532(485.40) | Grad Norm 43.0935(31.2384) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 42.8450(46.6207) | Bit/dim 5.8698(6.7418) | Xent 2.0130(2.0652) | Loss 14.7138(16.7319) | Error 0.6863(0.7302) Steps 508(486.08) | Grad Norm 43.7321(31.6132) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 44.7741(46.5653) | Bit/dim 5.8006(6.7136) | Xent 1.9949(2.0631) | Loss 14.5427(16.6663) | Error 0.6740(0.7285) Steps 544(487.82) | Grad Norm 27.7475(31.4972) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 42.5811(46.4458) | Bit/dim 5.8517(6.6877) | Xent 2.0011(2.0612) | Loss 14.6713(16.6064) | Error 0.6937(0.7274) Steps 520(488.78) | Grad Norm 56.7790(32.2557) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 47.1589(46.4672) | Bit/dim 5.8189(6.6617) | Xent 2.0056(2.0595) | Loss 14.4847(16.5428) | Error 0.7010(0.7266) Steps 550(490.62) | Grad Norm 51.0187(32.8186) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 21.5156, Epoch Time 310.1245(329.0937), Bit/dim 5.7343(best: 5.8924), Xent 1.9724, Loss 6.7205, Error 0.6740(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 44.6485(46.4126) | Bit/dim 5.7253(6.6336) | Xent 1.9837(2.0573) | Loss 17.7095(16.5778) | Error 0.6845(0.7254) Steps 520(491.50) | Grad Norm 16.9914(32.3438) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 44.4299(46.3531) | Bit/dim 5.8307(6.6095) | Xent 2.3689(2.0666) | Loss 14.7728(16.5236) | Error 0.8143(0.7280) Steps 496(491.63) | Grad Norm 79.6172(33.7620) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 43.7006(46.2736) | Bit/dim 5.7899(6.5849) | Xent 2.1181(2.0682) | Loss 14.5954(16.4658) | Error 0.7632(0.7291) Steps 502(491.95) | Grad Norm 44.3836(34.0806) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 41.1409(46.1196) | Bit/dim 5.7920(6.5611) | Xent 2.1424(2.0704) | Loss 14.6207(16.4104) | Error 0.7660(0.7302) Steps 508(492.43) | Grad Norm 53.2524(34.6558) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 41.0066(45.9662) | Bit/dim 5.7372(6.5364) | Xent 2.1248(2.0720) | Loss 14.6735(16.3583) | Error 0.7625(0.7312) Steps 490(492.35) | Grad Norm 38.5238(34.7718) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 42.0116(45.8476) | Bit/dim 5.7848(6.5139) | Xent 2.0365(2.0710) | Loss 14.1339(16.2916) | Error 0.7252(0.7310) Steps 508(492.82) | Grad Norm 33.2174(34.7252) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 21.2986, Epoch Time 296.2754(328.1091), Bit/dim 5.7551(best: 5.7343), Xent 2.0199, Loss 6.7650, Error 0.7137(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 41.7301(45.7240) | Bit/dim 5.7618(6.4913) | Xent 2.0544(2.0705) | Loss 17.4916(16.3276) | Error 0.7284(0.7309) Steps 496(492.92) | Grad Norm 25.1056(34.4366) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 40.5559(45.5690) | Bit/dim 5.6803(6.4670) | Xent 2.0429(2.0696) | Loss 14.0224(16.2584) | Error 0.7288(0.7308) Steps 484(492.65) | Grad Norm 14.4493(33.8370) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 42.2814(45.4704) | Bit/dim 5.7028(6.4440) | Xent 2.0290(2.0684) | Loss 14.2173(16.1972) | Error 0.7072(0.7301) Steps 508(493.11) | Grad Norm 19.7170(33.4134) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 40.2786(45.3146) | Bit/dim 5.6508(6.4202) | Xent 2.0270(2.0672) | Loss 14.0810(16.1337) | Error 0.7110(0.7296) Steps 490(493.02) | Grad Norm 11.4255(32.7537) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 39.6767(45.1455) | Bit/dim 5.6182(6.3962) | Xent 2.0393(2.0663) | Loss 14.2008(16.0757) | Error 0.7129(0.7291) Steps 490(492.93) | Grad Norm 16.4797(32.2655) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 41.7831(45.0446) | Bit/dim 5.6110(6.3726) | Xent 2.0324(2.0653) | Loss 14.2281(16.0203) | Error 0.7063(0.7284) Steps 496(493.02) | Grad Norm 7.9302(31.5355) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 20.9154, Epoch Time 285.6277(326.8347), Bit/dim 5.5899(best: 5.7343), Xent 2.0353, Loss 6.6075, Error 0.7150(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 44.6343(45.0323) | Bit/dim 5.5923(6.3492) | Xent 2.0457(2.0647) | Loss 17.4531(16.0633) | Error 0.7201(0.7281) Steps 496(493.11) | Grad Norm 13.2186(30.9860) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 40.5085(44.8966) | Bit/dim 5.5536(6.3253) | Xent 2.0181(2.0633) | Loss 13.4886(15.9860) | Error 0.7025(0.7274) Steps 460(492.12) | Grad Norm 5.4211(30.2190) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 39.1738(44.7249) | Bit/dim 5.5606(6.3024) | Xent 2.0028(2.0615) | Loss 14.0019(15.9265) | Error 0.6864(0.7261) Steps 454(490.97) | Grad Norm 10.3940(29.6243) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 39.5226(44.5688) | Bit/dim 5.5301(6.2792) | Xent 2.0432(2.0610) | Loss 14.1171(15.8722) | Error 0.7129(0.7257) Steps 478(490.58) | Grad Norm 9.4276(29.0184) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 42.0399(44.4930) | Bit/dim 5.5037(6.2560) | Xent 2.0025(2.0592) | Loss 13.7414(15.8083) | Error 0.6870(0.7246) Steps 448(489.31) | Grad Norm 6.7438(28.3501) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 39.0474(44.3296) | Bit/dim 5.5010(6.2333) | Xent 1.9998(2.0574) | Loss 13.7651(15.7470) | Error 0.6972(0.7238) Steps 460(488.43) | Grad Norm 9.2897(27.7783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 20.4481, Epoch Time 284.4088(325.5619), Bit/dim 5.4642(best: 5.5899), Xent 1.9650, Loss 6.4467, Error 0.6662(best: 0.6720)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 41.6626(44.2496) | Bit/dim 5.4636(6.2102) | Xent 1.9848(2.0552) | Loss 16.6156(15.7731) | Error 0.6826(0.7225) Steps 442(487.03) | Grad Norm 7.1104(27.1583) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 41.7376(44.1742) | Bit/dim 5.4693(6.1880) | Xent 1.9638(2.0525) | Loss 13.4512(15.7034) | Error 0.6825(0.7213) Steps 466(486.40) | Grad Norm 16.6818(26.8440) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 41.2903(44.0877) | Bit/dim 5.4243(6.1651) | Xent 1.9801(2.0503) | Loss 13.4394(15.6355) | Error 0.6869(0.7203) Steps 466(485.79) | Grad Norm 12.7716(26.4218) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 42.9997(44.0551) | Bit/dim 5.4087(6.1424) | Xent 1.9465(2.0472) | Loss 13.3688(15.5675) | Error 0.6707(0.7188) Steps 484(485.74) | Grad Norm 13.4340(26.0322) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 44.7008(44.0744) | Bit/dim 5.3797(6.1195) | Xent 1.9888(2.0455) | Loss 13.5777(15.5078) | Error 0.6951(0.7181) Steps 460(484.97) | Grad Norm 26.4993(26.0462) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 38.7061(43.9134) | Bit/dim 5.3642(6.0969) | Xent 1.9597(2.0429) | Loss 13.5348(15.4486) | Error 0.6754(0.7168) Steps 466(484.40) | Grad Norm 20.3713(25.8759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 20.0685, Epoch Time 290.0285(324.4959), Bit/dim 5.3726(best: 5.4642), Xent 1.9468, Loss 6.3460, Error 0.6786(best: 0.6662)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 40.2570(43.8037) | Bit/dim 5.3673(6.0750) | Xent 1.9772(2.0409) | Loss 16.1581(15.4699) | Error 0.6947(0.7161) Steps 460(483.66) | Grad Norm 32.7289(26.0815) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 41.6085(43.7378) | Bit/dim 5.3919(6.0545) | Xent 1.9301(2.0376) | Loss 13.5979(15.4137) | Error 0.6624(0.7145) Steps 454(482.77) | Grad Norm 22.4617(25.9729) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 42.9258(43.7135) | Bit/dim 5.3263(6.0326) | Xent 1.9611(2.0353) | Loss 13.5366(15.3574) | Error 0.6806(0.7135) Steps 466(482.27) | Grad Norm 26.7778(25.9971) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 45.2212(43.7587) | Bit/dim 5.3313(6.0116) | Xent 1.9961(2.0341) | Loss 13.3341(15.2967) | Error 0.7086(0.7134) Steps 436(480.88) | Grad Norm 27.6240(26.0459) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 41.1775(43.6813) | Bit/dim 5.2716(5.9894) | Xent 1.9396(2.0313) | Loss 12.7204(15.2194) | Error 0.6687(0.7120) Steps 466(480.44) | Grad Norm 11.5294(25.6104) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 40.3520(43.5814) | Bit/dim 5.3956(5.9716) | Xent 2.0029(2.0304) | Loss 13.5405(15.1691) | Error 0.7097(0.7120) Steps 466(480.00) | Grad Norm 49.6599(26.3319) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 19.6734, Epoch Time 290.0942(323.4639), Bit/dim 5.2549(best: 5.3726), Xent 1.9328, Loss 6.2213, Error 0.6650(best: 0.6662)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 39.4570(43.4577) | Bit/dim 5.2604(5.9502) | Xent 1.9578(2.0283) | Loss 15.8418(15.1892) | Error 0.6793(0.7110) Steps 448(479.04) | Grad Norm 12.1878(25.9076) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 43.2160(43.4504) | Bit/dim 5.2971(5.9306) | Xent 1.9448(2.0258) | Loss 13.2640(15.1315) | Error 0.6811(0.7101) Steps 454(478.29) | Grad Norm 26.0559(25.9120) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 41.7961(43.4008) | Bit/dim 5.2276(5.9096) | Xent 1.9586(2.0237) | Loss 13.1195(15.0711) | Error 0.6841(0.7093) Steps 472(478.10) | Grad Norm 5.9280(25.3125) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 40.7231(43.3205) | Bit/dim 5.2256(5.8890) | Xent 1.9349(2.0211) | Loss 12.8879(15.0056) | Error 0.6678(0.7081) Steps 448(477.20) | Grad Norm 17.1614(25.0680) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 42.2715(43.2890) | Bit/dim 5.2495(5.8698) | Xent 1.9110(2.0178) | Loss 13.3019(14.9545) | Error 0.6574(0.7065) Steps 490(477.58) | Grad Norm 17.2635(24.8338) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 45.3957(43.3522) | Bit/dim 5.2061(5.8499) | Xent 1.9203(2.0149) | Loss 13.1737(14.9011) | Error 0.6585(0.7051) Steps 478(477.60) | Grad Norm 13.9037(24.5059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 20.2709, Epoch Time 291.8113(322.5143), Bit/dim 5.2426(best: 5.2549), Xent 1.9261, Loss 6.2057, Error 0.6664(best: 0.6650)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 45.7647(43.4246) | Bit/dim 5.2336(5.8314) | Xent 1.9658(2.0134) | Loss 15.7219(14.9257) | Error 0.6843(0.7045) Steps 466(477.25) | Grad Norm 33.4032(24.7728) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 44.9320(43.4698) | Bit/dim 5.1778(5.8118) | Xent 1.9359(2.0111) | Loss 13.0144(14.8684) | Error 0.6711(0.7035) Steps 472(477.09) | Grad Norm 17.8185(24.5642) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 42.3774(43.4370) | Bit/dim 5.2355(5.7945) | Xent 1.9236(2.0084) | Loss 13.1298(14.8162) | Error 0.6760(0.7026) Steps 466(476.76) | Grad Norm 23.3860(24.5289) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 41.0915(43.3666) | Bit/dim 5.2152(5.7772) | Xent 1.8822(2.0046) | Loss 13.0361(14.7628) | Error 0.6541(0.7012) Steps 460(476.26) | Grad Norm 9.8602(24.0888) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 46.0125(43.4460) | Bit/dim 5.2068(5.7601) | Xent 1.9808(2.0039) | Loss 13.0396(14.7111) | Error 0.6851(0.7007) Steps 496(476.85) | Grad Norm 32.2739(24.3343) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 42.4161(43.4151) | Bit/dim 5.1499(5.7417) | Xent 1.9114(2.0012) | Loss 12.9487(14.6582) | Error 0.6574(0.6994) Steps 466(476.52) | Grad Norm 14.5641(24.0412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 20.2863, Epoch Time 301.5124(321.8842), Bit/dim 5.2020(best: 5.2426), Xent 1.9109, Loss 6.1575, Error 0.6599(best: 0.6650)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 44.1464(43.4371) | Bit/dim 5.2014(5.7255) | Xent 1.9125(1.9985) | Loss 15.7780(14.6918) | Error 0.6763(0.6987) Steps 484(476.75) | Grad Norm 21.8287(23.9749) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 46.3351(43.5240) | Bit/dim 5.1909(5.7095) | Xent 1.8930(1.9953) | Loss 13.2150(14.6475) | Error 0.6498(0.6972) Steps 466(476.42) | Grad Norm 11.5919(23.6034) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 42.5728(43.4955) | Bit/dim 5.1451(5.6926) | Xent 1.9937(1.9953) | Loss 12.8055(14.5923) | Error 0.7087(0.6976) Steps 472(476.29) | Grad Norm 19.0745(23.4675) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 42.5750(43.4679) | Bit/dim 5.1105(5.6751) | Xent 1.9614(1.9943) | Loss 12.4650(14.5285) | Error 0.6881(0.6973) Steps 478(476.34) | Grad Norm 19.4489(23.3470) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 45.2187(43.5204) | Bit/dim 5.1010(5.6579) | Xent 1.9300(1.9923) | Loss 13.0711(14.4847) | Error 0.6915(0.6971) Steps 472(476.21) | Grad Norm 15.4879(23.1112) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 43.4190(43.5173) | Bit/dim 5.0917(5.6409) | Xent 1.9218(1.9902) | Loss 12.7811(14.4336) | Error 0.6778(0.6966) Steps 466(475.91) | Grad Norm 11.1954(22.7537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 20.6171, Epoch Time 302.7806(321.3111), Bit/dim 5.0620(best: 5.2020), Xent 1.9015, Loss 6.0128, Error 0.6518(best: 0.6599)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 43.5099(43.5171) | Bit/dim 5.0732(5.6239) | Xent 1.9283(1.9884) | Loss 15.2301(14.4575) | Error 0.6741(0.6959) Steps 466(475.61) | Grad Norm 14.2734(22.4993) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 41.1685(43.4467) | Bit/dim 5.0577(5.6069) | Xent 1.9070(1.9859) | Loss 12.4663(14.3978) | Error 0.6669(0.6950) Steps 466(475.32) | Grad Norm 8.9540(22.0929) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 45.4597(43.5070) | Bit/dim 5.0643(5.5906) | Xent 1.9110(1.9837) | Loss 12.6442(14.3452) | Error 0.6594(0.6939) Steps 472(475.22) | Grad Norm 6.3997(21.6221) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 44.4973(43.5368) | Bit/dim 5.0323(5.5739) | Xent 1.9161(1.9816) | Loss 12.9011(14.3019) | Error 0.6627(0.6930) Steps 484(475.48) | Grad Norm 5.4120(21.1358) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 44.3693(43.5617) | Bit/dim 5.0048(5.5568) | Xent 1.9166(1.9797) | Loss 12.7055(14.2540) | Error 0.6647(0.6922) Steps 478(475.56) | Grad Norm 11.2887(20.8404) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 44.0993(43.5779) | Bit/dim 5.0180(5.5406) | Xent 1.9361(1.9784) | Loss 12.6508(14.2059) | Error 0.6838(0.6919) Steps 460(475.09) | Grad Norm 15.8905(20.6919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.9790, Epoch Time 302.4543(320.7454), Bit/dim 5.0100(best: 5.0620), Xent 1.9014, Loss 5.9607, Error 0.6532(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 44.6368(43.6096) | Bit/dim 5.0163(5.5249) | Xent 1.9162(1.9765) | Loss 15.6474(14.2491) | Error 0.6641(0.6911) Steps 484(475.36) | Grad Norm 13.6506(20.4807) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 44.6368(43.6404) | Bit/dim 4.9887(5.5088) | Xent 1.9061(1.9744) | Loss 12.3678(14.1927) | Error 0.6665(0.6903) Steps 478(475.44) | Grad Norm 9.4027(20.1483) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 41.6636(43.5811) | Bit/dim 4.9717(5.4927) | Xent 1.8854(1.9717) | Loss 12.4569(14.1406) | Error 0.6536(0.6892) Steps 466(475.16) | Grad Norm 4.6142(19.6823) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 45.2018(43.6298) | Bit/dim 4.9587(5.4767) | Xent 1.8741(1.9688) | Loss 12.2342(14.0834) | Error 0.6625(0.6884) Steps 478(475.24) | Grad Norm 6.3993(19.2838) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 43.4885(43.6255) | Bit/dim 4.9779(5.4617) | Xent 1.9272(1.9676) | Loss 12.2684(14.0290) | Error 0.6785(0.6881) Steps 478(475.32) | Grad Norm 17.2030(19.2214) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 44.0128(43.6371) | Bit/dim 4.9663(5.4468) | Xent 1.9497(1.9670) | Loss 12.5752(13.9853) | Error 0.6951(0.6883) Steps 478(475.40) | Grad Norm 19.9841(19.2443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 19.3755, Epoch Time 301.9669(320.1821), Bit/dim 4.9346(best: 5.0100), Xent 1.8646, Loss 5.8669, Error 0.6603(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 42.6819(43.6085) | Bit/dim 4.9415(5.4317) | Xent 1.8819(1.9645) | Loss 14.8969(14.0127) | Error 0.6656(0.6877) Steps 466(475.12) | Grad Norm 11.1215(19.0006) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 42.3016(43.5693) | Bit/dim 4.9474(5.4172) | Xent 1.8658(1.9615) | Loss 12.4948(13.9672) | Error 0.6549(0.6867) Steps 454(474.49) | Grad Norm 7.2382(18.6477) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 41.5326(43.5082) | Bit/dim 4.9445(5.4030) | Xent 1.9284(1.9605) | Loss 12.5737(13.9254) | Error 0.6894(0.6868) Steps 448(473.69) | Grad Norm 17.2235(18.6050) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 42.5275(43.4788) | Bit/dim 4.9050(5.3880) | Xent 1.8739(1.9579) | Loss 12.1689(13.8727) | Error 0.6583(0.6859) Steps 466(473.46) | Grad Norm 5.3203(18.2065) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 47.7015(43.6054) | Bit/dim 4.9021(5.3735) | Xent 1.9051(1.9563) | Loss 12.6839(13.8370) | Error 0.6740(0.6855) Steps 466(473.24) | Grad Norm 15.5840(18.1278) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 46.7502(43.6998) | Bit/dim 4.8758(5.3585) | Xent 1.9154(1.9551) | Loss 12.0275(13.7827) | Error 0.6811(0.6854) Steps 508(474.28) | Grad Norm 19.2093(18.1602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.4797, Epoch Time 302.3232(319.6463), Bit/dim 4.8957(best: 4.9346), Xent 1.8638, Loss 5.8276, Error 0.6538(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 42.6791(43.6692) | Bit/dim 4.8843(5.3443) | Xent 1.8941(1.9533) | Loss 15.1160(13.8227) | Error 0.6750(0.6851) Steps 478(474.39) | Grad Norm 23.2845(18.3140) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 47.0505(43.7706) | Bit/dim 4.9875(5.3336) | Xent 2.0875(1.9573) | Loss 12.8588(13.7938) | Error 0.7198(0.6861) Steps 496(475.04) | Grad Norm 54.0460(19.3859) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 43.1526(43.7521) | Bit/dim 4.9131(5.3210) | Xent 1.9674(1.9576) | Loss 12.4230(13.7527) | Error 0.6952(0.6864) Steps 472(474.95) | Grad Norm 17.1917(19.3201) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 37.5643(43.5664) | Bit/dim 5.0400(5.3126) | Xent 2.2064(1.9651) | Loss 12.8852(13.7266) | Error 0.7659(0.6888) Steps 454(474.32) | Grad Norm 57.6335(20.4695) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 40.9198(43.4870) | Bit/dim 5.0655(5.3051) | Xent 2.0065(1.9663) | Loss 12.9565(13.7035) | Error 0.7298(0.6900) Steps 448(473.53) | Grad Norm 38.6641(21.0153) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 42.8375(43.4675) | Bit/dim 4.9544(5.2946) | Xent 2.3029(1.9764) | Loss 12.7772(13.6758) | Error 0.7817(0.6928) Steps 448(472.77) | Grad Norm 53.1847(21.9804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 20.8586, Epoch Time 294.4104(318.8892), Bit/dim 4.9508(best: 4.8957), Xent 2.1033, Loss 6.0024, Error 0.7424(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 43.9207(43.4811) | Bit/dim 4.9549(5.2844) | Xent 2.1517(1.9817) | Loss 15.4038(13.7276) | Error 0.7415(0.6942) Steps 460(472.38) | Grad Norm 41.5258(22.5668) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 46.0477(43.5581) | Bit/dim 4.9646(5.2748) | Xent 1.8853(1.9788) | Loss 12.4825(13.6902) | Error 0.6639(0.6933) Steps 478(472.55) | Grad Norm 11.2326(22.2267) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 40.8354(43.4765) | Bit/dim 4.8854(5.2632) | Xent 1.9931(1.9792) | Loss 12.4676(13.6536) | Error 0.7063(0.6937) Steps 460(472.18) | Grad Norm 9.8161(21.8544) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 43.0040(43.4623) | Bit/dim 4.9256(5.2530) | Xent 2.0147(1.9803) | Loss 12.7815(13.6274) | Error 0.7220(0.6946) Steps 466(471.99) | Grad Norm 8.2651(21.4467) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 42.1639(43.4233) | Bit/dim 4.8866(5.2420) | Xent 1.9996(1.9809) | Loss 12.2492(13.5861) | Error 0.7096(0.6950) Steps 466(471.81) | Grad Norm 9.4685(21.0874) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 42.9088(43.4079) | Bit/dim 4.8601(5.2306) | Xent 1.9581(1.9802) | Loss 12.2948(13.5473) | Error 0.6980(0.6951) Steps 478(472.00) | Grad Norm 5.0061(20.6050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 20.9826, Epoch Time 298.3364(318.2726), Bit/dim 4.8720(best: 4.8957), Xent 1.9207, Loss 5.8323, Error 0.6737(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 43.8738(43.4219) | Bit/dim 4.8833(5.2202) | Xent 1.9495(1.9793) | Loss 15.0189(13.5915) | Error 0.6990(0.6952) Steps 478(472.18) | Grad Norm 6.0226(20.1675) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 45.0867(43.4718) | Bit/dim 4.8686(5.2096) | Xent 1.9344(1.9779) | Loss 12.4683(13.5578) | Error 0.6944(0.6952) Steps 484(472.53) | Grad Norm 7.3963(19.7844) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 43.8151(43.4821) | Bit/dim 4.8234(5.1980) | Xent 1.9241(1.9763) | Loss 12.3368(13.5211) | Error 0.6815(0.6948) Steps 448(471.80) | Grad Norm 4.4269(19.3236) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 47.3243(43.5974) | Bit/dim 4.8319(5.1870) | Xent 1.9242(1.9747) | Loss 12.1092(13.4788) | Error 0.6784(0.6943) Steps 454(471.26) | Grad Norm 6.3792(18.9353) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 42.5859(43.5670) | Bit/dim 4.8063(5.1756) | Xent 1.9049(1.9726) | Loss 12.0117(13.4348) | Error 0.6669(0.6935) Steps 460(470.92) | Grad Norm 5.8152(18.5417) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 48.1140(43.7034) | Bit/dim 4.8063(5.1645) | Xent 1.9054(1.9706) | Loss 12.0399(13.3929) | Error 0.6726(0.6928) Steps 466(470.78) | Grad Norm 7.9200(18.2230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 19.9430, Epoch Time 308.9124(317.9918), Bit/dim 4.7920(best: 4.8720), Xent 1.8377, Loss 5.7108, Error 0.6391(best: 0.6518)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 43.6206(43.7010) | Bit/dim 4.7911(5.1533) | Xent 1.8500(1.9670) | Loss 14.8385(13.4363) | Error 0.6452(0.6914) Steps 460(470.45) | Grad Norm 3.5548(17.7830) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 43.1686(43.6850) | Bit/dim 4.7819(5.1422) | Xent 1.8623(1.9639) | Loss 12.3164(13.4027) | Error 0.6579(0.6904) Steps 472(470.50) | Grad Norm 7.3897(17.4712) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 43.8119(43.6888) | Bit/dim 4.7791(5.1313) | Xent 1.8726(1.9611) | Loss 12.0324(13.3616) | Error 0.6545(0.6893) Steps 478(470.72) | Grad Norm 10.3311(17.2570) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 43.5650(43.6851) | Bit/dim 4.7617(5.1202) | Xent 1.8585(1.9580) | Loss 12.2023(13.3268) | Error 0.6650(0.6886) Steps 454(470.22) | Grad Norm 11.8379(17.0944) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 42.8970(43.6614) | Bit/dim 4.7646(5.1095) | Xent 1.8674(1.9553) | Loss 12.2386(13.2942) | Error 0.6609(0.6878) Steps 472(470.28) | Grad Norm 14.4538(17.0152) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 47.8132(43.7860) | Bit/dim 4.7648(5.0992) | Xent 1.8448(1.9520) | Loss 12.1490(13.2598) | Error 0.6500(0.6866) Steps 454(469.79) | Grad Norm 12.6157(16.8832) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 20.2252, Epoch Time 303.7282(317.5639), Bit/dim 4.7384(best: 4.7920), Xent 1.7984, Loss 5.6376, Error 0.6284(best: 0.6391)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 44.3659(43.8034) | Bit/dim 4.7515(5.0888) | Xent 1.8573(1.9492) | Loss 14.4742(13.2962) | Error 0.6531(0.6856) Steps 466(469.67) | Grad Norm 14.8193(16.8213) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 47.0811(43.9017) | Bit/dim 4.7266(5.0779) | Xent 1.8461(1.9461) | Loss 12.0770(13.2597) | Error 0.6512(0.6846) Steps 484(470.10) | Grad Norm 18.0737(16.8589) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 42.6556(43.8643) | Bit/dim 4.7116(5.0669) | Xent 1.8024(1.9418) | Loss 12.1271(13.2257) | Error 0.6329(0.6831) Steps 460(469.80) | Grad Norm 14.4909(16.7878) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 48.0431(43.9897) | Bit/dim 4.7071(5.0561) | Xent 1.8154(1.9380) | Loss 11.7027(13.1800) | Error 0.6375(0.6817) Steps 484(470.23) | Grad Norm 13.2012(16.6802) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 42.0224(43.9307) | Bit/dim 4.7325(5.0464) | Xent 1.8489(1.9353) | Loss 11.8242(13.1393) | Error 0.6535(0.6808) Steps 454(469.74) | Grad Norm 15.4184(16.6424) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 43.3905(43.9145) | Bit/dim 4.7044(5.0362) | Xent 1.8397(1.9324) | Loss 12.1204(13.1088) | Error 0.6406(0.6796) Steps 460(469.45) | Grad Norm 24.6448(16.8825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 20.0838, Epoch Time 306.5825(317.2345), Bit/dim 4.6905(best: 4.7384), Xent 1.7677, Loss 5.5743, Error 0.6190(best: 0.6284)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 41.5615(43.8439) | Bit/dim 4.6847(5.0256) | Xent 1.8161(1.9289) | Loss 14.6563(13.1552) | Error 0.6312(0.6782) Steps 472(469.52) | Grad Norm 18.3679(16.9270) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 47.3757(43.9498) | Bit/dim 4.7014(5.0159) | Xent 1.8312(1.9260) | Loss 12.0985(13.1235) | Error 0.6434(0.6771) Steps 490(470.14) | Grad Norm 14.4148(16.8516) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 42.5516(43.9079) | Bit/dim 4.6790(5.0058) | Xent 1.8159(1.9227) | Loss 12.0442(13.0911) | Error 0.6430(0.6761) Steps 478(470.37) | Grad Norm 20.6154(16.9646) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 46.4458(43.9840) | Bit/dim 4.7814(4.9990) | Xent 2.3396(1.9352) | Loss 12.7055(13.0795) | Error 0.7725(0.6790) Steps 478(470.60) | Grad Norm 62.6762(18.3359) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 43.6744(43.9747) | Bit/dim 4.7728(4.9923) | Xent 1.8505(1.9327) | Loss 12.0015(13.0472) | Error 0.6500(0.6781) Steps 490(471.18) | Grad Norm 20.8180(18.4104) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 48.7667(44.1185) | Bit/dim 4.8856(4.9891) | Xent 2.2187(1.9413) | Loss 12.8242(13.0405) | Error 0.7240(0.6795) Steps 496(471.93) | Grad Norm 53.6233(19.4668) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 21.7601, Epoch Time 311.0729(317.0496), Bit/dim 4.8910(best: 4.6905), Xent 1.9834, Loss 5.8827, Error 0.6916(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 48.4192(44.2475) | Bit/dim 4.8852(4.9859) | Xent 2.0383(1.9442) | Loss 15.3828(13.1108) | Error 0.6990(0.6801) Steps 484(472.29) | Grad Norm 34.6162(19.9212) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 47.5532(44.3467) | Bit/dim 4.7053(4.9775) | Xent 1.8868(1.9424) | Loss 11.9461(13.0758) | Error 0.6665(0.6797) Steps 472(472.28) | Grad Norm 13.3737(19.7248) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 48.6000(44.4743) | Bit/dim 4.7535(4.9708) | Xent 1.9503(1.9427) | Loss 12.2420(13.0508) | Error 0.7000(0.6803) Steps 496(472.99) | Grad Norm 17.6786(19.6634) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 50.0768(44.6424) | Bit/dim 4.7066(4.9629) | Xent 1.8828(1.9409) | Loss 12.2775(13.0276) | Error 0.6704(0.6800) Steps 502(473.86) | Grad Norm 6.3575(19.2643) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 43.8748(44.6193) | Bit/dim 4.7323(4.9560) | Xent 1.9664(1.9417) | Loss 12.0941(12.9996) | Error 0.6877(0.6802) Steps 496(474.53) | Grad Norm 27.2511(19.5039) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 46.0072(44.6610) | Bit/dim 4.7335(4.9493) | Xent 1.9186(1.9410) | Loss 12.2266(12.9764) | Error 0.6810(0.6803) Steps 502(475.35) | Grad Norm 22.5822(19.5962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 22.0688, Epoch Time 324.5353(317.2742), Bit/dim 4.7069(best: 4.6905), Xent 1.8293, Loss 5.6216, Error 0.6489(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 44.2420(44.6484) | Bit/dim 4.6987(4.9418) | Xent 1.8568(1.9384) | Loss 14.8899(13.0338) | Error 0.6644(0.6798) Steps 502(476.15) | Grad Norm 7.8976(19.2453) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 47.9072(44.7462) | Bit/dim 4.6798(4.9339) | Xent 1.8836(1.9368) | Loss 12.1755(13.0081) | Error 0.6664(0.6794) Steps 502(476.93) | Grad Norm 6.0726(18.8501) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 45.3565(44.7645) | Bit/dim 4.6605(4.9257) | Xent 1.9383(1.9368) | Loss 12.1294(12.9817) | Error 0.6934(0.6798) Steps 526(478.40) | Grad Norm 24.2184(19.0111) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 47.0770(44.8339) | Bit/dim 4.6832(4.9184) | Xent 1.8679(1.9348) | Loss 12.0370(12.9534) | Error 0.6639(0.6793) Steps 520(479.65) | Grad Norm 10.1498(18.7453) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 46.8652(44.8948) | Bit/dim 4.6482(4.9103) | Xent 1.8496(1.9322) | Loss 11.7564(12.9175) | Error 0.6581(0.6787) Steps 520(480.86) | Grad Norm 5.6285(18.3518) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 51.4277(45.0908) | Bit/dim 4.6397(4.9022) | Xent 1.8943(1.9311) | Loss 11.9128(12.8873) | Error 0.6789(0.6787) Steps 556(483.11) | Grad Norm 20.8941(18.4280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 21.7380, Epoch Time 322.4638(317.4299), Bit/dim 4.6723(best: 4.6905), Xent 1.8103, Loss 5.5774, Error 0.6380(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 47.3408(45.1583) | Bit/dim 4.6657(4.8951) | Xent 1.8425(1.9284) | Loss 15.1687(12.9558) | Error 0.6519(0.6779) Steps 508(483.86) | Grad Norm 9.5894(18.1629) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 46.0080(45.1838) | Bit/dim 4.6476(4.8877) | Xent 1.8517(1.9261) | Loss 11.8698(12.9232) | Error 0.6655(0.6775) Steps 484(483.86) | Grad Norm 9.0517(17.8896) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 44.1274(45.1521) | Bit/dim 4.6500(4.8806) | Xent 1.8257(1.9231) | Loss 11.7673(12.8885) | Error 0.6524(0.6768) Steps 490(484.05) | Grad Norm 8.1336(17.5969) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 46.7585(45.2003) | Bit/dim 4.6358(4.8732) | Xent 1.8160(1.9199) | Loss 11.9067(12.8591) | Error 0.6326(0.6754) Steps 466(483.51) | Grad Norm 10.4271(17.3818) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 48.2978(45.2932) | Bit/dim 4.6208(4.8656) | Xent 1.8489(1.9178) | Loss 11.8832(12.8298) | Error 0.6609(0.6750) Steps 478(483.34) | Grad Norm 18.4512(17.4139) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 40.8052(45.1586) | Bit/dim 4.6615(4.8595) | Xent 1.8616(1.9161) | Loss 11.9647(12.8038) | Error 0.6651(0.6747) Steps 478(483.18) | Grad Norm 22.5092(17.5667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 21.1786, Epoch Time 313.2627(317.3049), Bit/dim 4.6235(best: 4.6723), Xent 1.7744, Loss 5.5107, Error 0.6276(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 44.7532(45.1464) | Bit/dim 4.6280(4.8526) | Xent 1.8189(1.9132) | Loss 14.3915(12.8515) | Error 0.6411(0.6737) Steps 478(483.03) | Grad Norm 14.0682(17.4618) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 49.5990(45.2800) | Bit/dim 4.5844(4.8445) | Xent 1.7826(1.9093) | Loss 11.5552(12.8126) | Error 0.6296(0.6724) Steps 496(483.41) | Grad Norm 6.6427(17.1372) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 46.3045(45.3107) | Bit/dim 4.6164(4.8377) | Xent 1.7780(1.9053) | Loss 11.9272(12.7860) | Error 0.6339(0.6712) Steps 502(483.97) | Grad Norm 7.9375(16.8612) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 43.6541(45.2610) | Bit/dim 4.5852(4.8301) | Xent 1.7721(1.9013) | Loss 11.7903(12.7561) | Error 0.6269(0.6699) Steps 490(484.15) | Grad Norm 7.5001(16.5804) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 46.3415(45.2934) | Bit/dim 4.5768(4.8225) | Xent 1.7482(1.8967) | Loss 11.8085(12.7277) | Error 0.6116(0.6681) Steps 502(484.69) | Grad Norm 5.9805(16.2624) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 46.9809(45.3441) | Bit/dim 4.5739(4.8151) | Xent 1.7760(1.8931) | Loss 11.4689(12.6899) | Error 0.6294(0.6670) Steps 484(484.67) | Grad Norm 9.9521(16.0731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 20.1803, Epoch Time 316.2865(317.2743), Bit/dim 4.5984(best: 4.6235), Xent 1.8187, Loss 5.5078, Error 0.6318(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 45.7900(45.3574) | Bit/dim 4.6064(4.8088) | Xent 1.8717(1.8925) | Loss 14.3276(12.7391) | Error 0.6414(0.6662) Steps 496(485.01) | Grad Norm 34.5097(16.6262) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 47.3224(45.4164) | Bit/dim 4.5678(4.8016) | Xent 1.7345(1.8877) | Loss 11.5732(12.7041) | Error 0.6153(0.6647) Steps 520(486.06) | Grad Norm 14.9252(16.5751) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 46.8435(45.4592) | Bit/dim 4.5464(4.7939) | Xent 1.7662(1.8841) | Loss 11.3719(12.6641) | Error 0.6256(0.6635) Steps 484(486.00) | Grad Norm 6.6826(16.2784) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 44.1553(45.4201) | Bit/dim 4.5359(4.7862) | Xent 1.7452(1.8799) | Loss 11.6548(12.6338) | Error 0.6229(0.6623) Steps 484(485.94) | Grad Norm 15.0714(16.2422) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 43.5717(45.3646) | Bit/dim 4.5814(4.7800) | Xent 1.8479(1.8789) | Loss 11.8006(12.6088) | Error 0.6387(0.6616) Steps 466(485.34) | Grad Norm 34.3523(16.7855) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 45.8341(45.3787) | Bit/dim 4.5431(4.7729) | Xent 1.7262(1.8744) | Loss 11.5798(12.5780) | Error 0.6042(0.6599) Steps 490(485.48) | Grad Norm 6.7526(16.4845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 21.2297, Epoch Time 313.9661(317.1751), Bit/dim 4.5561(best: 4.5984), Xent 1.8308, Loss 5.4714, Error 0.6591(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 46.5373(45.4135) | Bit/dim 4.5586(4.7665) | Xent 1.8875(1.8748) | Loss 14.5148(12.6361) | Error 0.6655(0.6600) Steps 472(485.07) | Grad Norm 38.3970(17.1418) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 44.9754(45.4003) | Bit/dim 4.5846(4.7610) | Xent 1.8719(1.8747) | Loss 11.7703(12.6101) | Error 0.6649(0.6602) Steps 502(485.58) | Grad Norm 29.7418(17.5198) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 45.8363(45.4134) | Bit/dim 4.5401(4.7544) | Xent 1.7931(1.8722) | Loss 11.7707(12.5849) | Error 0.6459(0.6597) Steps 490(485.71) | Grad Norm 11.2174(17.3308) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 47.7747(45.4842) | Bit/dim 4.5371(4.7479) | Xent 1.7576(1.8688) | Loss 11.6801(12.5578) | Error 0.6229(0.6586) Steps 472(485.30) | Grad Norm 13.5918(17.2186) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 43.7599(45.4325) | Bit/dim 4.5104(4.7408) | Xent 1.7934(1.8665) | Loss 11.6624(12.5309) | Error 0.6421(0.6581) Steps 484(485.26) | Grad Norm 17.0977(17.2150) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 42.2476(45.3370) | Bit/dim 4.5693(4.7356) | Xent 1.7693(1.8636) | Loss 11.6567(12.5047) | Error 0.6200(0.6570) Steps 466(484.69) | Grad Norm 14.8903(17.1452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 21.0742, Epoch Time 311.1291(316.9937), Bit/dim 4.5528(best: 4.5561), Xent 1.7051, Loss 5.4053, Error 0.6091(best: 0.6190)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 43.8724(45.2930) | Bit/dim 4.5506(4.7301) | Xent 1.7657(1.8607) | Loss 14.6011(12.5676) | Error 0.6222(0.6560) Steps 496(485.02) | Grad Norm 16.7270(17.1327) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 43.0543(45.2259) | Bit/dim 4.6815(4.7286) | Xent 1.7264(1.8566) | Loss 12.0090(12.5508) | Error 0.6085(0.6545) Steps 490(485.17) | Grad Norm 12.0335(16.9797) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 46.6428(45.2684) | Bit/dim 4.5810(4.7242) | Xent 1.7960(1.8548) | Loss 11.8916(12.5311) | Error 0.6274(0.6537) Steps 484(485.14) | Grad Norm 17.8687(17.0064) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 45.7759(45.2836) | Bit/dim 4.5899(4.7202) | Xent 1.8541(1.8548) | Loss 12.0107(12.5154) | Error 0.6605(0.6539) Steps 502(485.64) | Grad Norm 26.6613(17.2960) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 44.6279(45.2639) | Bit/dim 4.5938(4.7164) | Xent 1.8181(1.8537) | Loss 11.3905(12.4817) | Error 0.6529(0.6539) Steps 490(485.78) | Grad Norm 11.4592(17.1209) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 44.5855(45.2436) | Bit/dim 4.5463(4.7113) | Xent 1.8082(1.8523) | Loss 11.3734(12.4484) | Error 0.6449(0.6536) Steps 484(485.72) | Grad Norm 21.6174(17.2558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 21.2343, Epoch Time 308.1607(316.7287), Bit/dim 4.5513(best: 4.5528), Xent 1.7535, Loss 5.4280, Error 0.6171(best: 0.6091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 45.0237(45.2370) | Bit/dim 4.5570(4.7066) | Xent 1.7935(1.8506) | Loss 14.2575(12.5027) | Error 0.6332(0.6530) Steps 502(486.21) | Grad Norm 18.5087(17.2934) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 43.5463(45.1863) | Bit/dim 4.5657(4.7024) | Xent 1.7317(1.8470) | Loss 11.5098(12.4729) | Error 0.6175(0.6519) Steps 484(486.14) | Grad Norm 13.0962(17.1675) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 46.3145(45.2201) | Bit/dim 4.5285(4.6972) | Xent 1.7404(1.8438) | Loss 11.5457(12.4451) | Error 0.6214(0.6510) Steps 514(486.98) | Grad Norm 8.2572(16.9002) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 43.8326(45.1785) | Bit/dim 4.5170(4.6918) | Xent 1.7522(1.8411) | Loss 11.3330(12.4117) | Error 0.6162(0.6500) Steps 478(486.71) | Grad Norm 12.1945(16.7590) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 45.1572(45.1778) | Bit/dim 4.5086(4.6863) | Xent 1.8143(1.8403) | Loss 11.6991(12.3904) | Error 0.6432(0.6498) Steps 472(486.27) | Grad Norm 23.1994(16.9522) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 50.9200(45.3501) | Bit/dim 4.5107(4.6810) | Xent 1.7177(1.8366) | Loss 11.5014(12.3637) | Error 0.6075(0.6485) Steps 490(486.38) | Grad Norm 10.5581(16.7604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 21.6188, Epoch Time 315.3517(316.6874), Bit/dim 4.5044(best: 4.5513), Xent 1.6722, Loss 5.3405, Error 0.5915(best: 0.6091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 47.6965(45.4205) | Bit/dim 4.5116(4.6759) | Xent 1.7011(1.8325) | Loss 14.6502(12.4323) | Error 0.6081(0.6473) Steps 502(486.85) | Grad Norm 4.9970(16.4075) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 45.0275(45.4087) | Bit/dim 4.4702(4.6698) | Xent 1.7774(1.8309) | Loss 11.5678(12.4064) | Error 0.6294(0.6468) Steps 478(486.58) | Grad Norm 16.3811(16.4067) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 47.6235(45.4751) | Bit/dim 4.4768(4.6640) | Xent 1.8219(1.8306) | Loss 11.4928(12.3790) | Error 0.6471(0.6468) Steps 472(486.15) | Grad Norm 26.8057(16.7187) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 45.7532(45.4835) | Bit/dim 4.4824(4.6585) | Xent 1.7080(1.8269) | Loss 11.3311(12.3475) | Error 0.6016(0.6454) Steps 496(486.44) | Grad Norm 5.9389(16.3953) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 43.9092(45.4363) | Bit/dim 4.4711(4.6529) | Xent 1.8429(1.8274) | Loss 11.4903(12.3218) | Error 0.6520(0.6456) Steps 496(486.73) | Grad Norm 28.9272(16.7712) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 47.3136(45.4926) | Bit/dim 4.4458(4.6467) | Xent 1.7175(1.8241) | Loss 11.2251(12.2889) | Error 0.6096(0.6445) Steps 490(486.83) | Grad Norm 6.2008(16.4541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 21.8049, Epoch Time 318.0183(316.7273), Bit/dim 4.4696(best: 4.5044), Xent 1.7546, Loss 5.3470, Error 0.6195(best: 0.5915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 45.6904(45.4985) | Bit/dim 4.4574(4.6410) | Xent 1.7976(1.8233) | Loss 14.0842(12.3428) | Error 0.6427(0.6445) Steps 502(487.28) | Grad Norm 25.0279(16.7113) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 44.6847(45.4741) | Bit/dim 4.4625(4.6357) | Xent 1.7523(1.8212) | Loss 11.3374(12.3126) | Error 0.6209(0.6438) Steps 484(487.18) | Grad Norm 12.0922(16.5728) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 43.3675(45.4109) | Bit/dim 4.4628(4.6305) | Xent 1.7413(1.8188) | Loss 11.2819(12.2817) | Error 0.6136(0.6429) Steps 496(487.45) | Grad Norm 14.3549(16.5062) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 47.3039(45.4677) | Bit/dim 4.4247(4.6243) | Xent 1.7187(1.8158) | Loss 11.1564(12.2479) | Error 0.6041(0.6417) Steps 490(487.52) | Grad Norm 5.9105(16.1884) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 46.3936(45.4955) | Bit/dim 4.4280(4.6184) | Xent 1.7658(1.8143) | Loss 11.1076(12.2137) | Error 0.6299(0.6414) Steps 514(488.32) | Grad Norm 16.4756(16.1970) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 48.7037(45.5917) | Bit/dim 4.4164(4.6124) | Xent 1.7121(1.8112) | Loss 11.4774(12.1916) | Error 0.6025(0.6402) Steps 496(488.55) | Grad Norm 8.1621(15.9559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 22.3883, Epoch Time 316.3200(316.7151), Bit/dim 4.4330(best: 4.4696), Xent 1.7058, Loss 5.2859, Error 0.6054(best: 0.5915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 51.4446(45.7673) | Bit/dim 4.4245(4.6067) | Xent 1.7442(1.8092) | Loss 14.3333(12.2559) | Error 0.6240(0.6397) Steps 484(488.41) | Grad Norm 17.9024(16.0143) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 44.4141(45.7267) | Bit/dim 4.4604(4.6023) | Xent 1.7135(1.8063) | Loss 11.4170(12.2307) | Error 0.6050(0.6387) Steps 532(489.72) | Grad Norm 9.8509(15.8294) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 44.9407(45.7031) | Bit/dim 4.5086(4.5995) | Xent 1.7976(1.8061) | Loss 11.6275(12.2126) | Error 0.6452(0.6389) Steps 514(490.45) | Grad Norm 16.9425(15.8628) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 48.8522(45.7976) | Bit/dim 4.7170(4.6030) | Xent 1.7282(1.8037) | Loss 12.1090(12.2095) | Error 0.6028(0.6378) Steps 508(490.98) | Grad Norm 17.3003(15.9059) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 49.8211(45.9183) | Bit/dim 4.4619(4.5988) | Xent 1.7926(1.8034) | Loss 11.6641(12.1931) | Error 0.6435(0.6379) Steps 550(492.75) | Grad Norm 27.2753(16.2470) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 48.8914(46.0075) | Bit/dim 4.5449(4.5972) | Xent 1.8557(1.8050) | Loss 11.6312(12.1763) | Error 0.6542(0.6384) Steps 550(494.46) | Grad Norm 32.9473(16.7480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 21.6713, Epoch Time 328.5308(317.0696), Bit/dim 4.5267(best: 4.4330), Xent 1.7641, Loss 5.4088, Error 0.6323(best: 0.5915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 43.3845(45.9288) | Bit/dim 4.5314(4.5952) | Xent 1.8174(1.8053) | Loss 14.4109(12.2433) | Error 0.6374(0.6384) Steps 490(494.33) | Grad Norm 35.9439(17.3239) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 53.0925(46.1437) | Bit/dim 4.4956(4.5922) | Xent 1.7177(1.8027) | Loss 11.8065(12.2302) | Error 0.6150(0.6377) Steps 520(495.10) | Grad Norm 13.0274(17.1950) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 45.7024(46.1305) | Bit/dim 4.4619(4.5883) | Xent 1.7065(1.7998) | Loss 11.5041(12.2084) | Error 0.6069(0.6368) Steps 544(496.57) | Grad Norm 8.8393(16.9443) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 48.9146(46.2140) | Bit/dim 4.4560(4.5844) | Xent 1.7735(1.7990) | Loss 11.5205(12.1878) | Error 0.6235(0.6364) Steps 490(496.37) | Grad Norm 15.4224(16.8987) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 50.8096(46.3519) | Bit/dim 4.4627(4.5807) | Xent 1.7245(1.7968) | Loss 11.5831(12.1697) | Error 0.6101(0.6356) Steps 538(497.62) | Grad Norm 9.2797(16.6701) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 48.7505(46.4238) | Bit/dim 4.4283(4.5761) | Xent 1.7008(1.7939) | Loss 11.3935(12.1464) | Error 0.6001(0.6345) Steps 538(498.83) | Grad Norm 6.3524(16.3606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 22.0675, Epoch Time 331.4893(317.5022), Bit/dim 4.4071(best: 4.4330), Xent 1.6555, Loss 5.2348, Error 0.5820(best: 0.5915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 51.0154(46.5616) | Bit/dim 4.4145(4.5713) | Xent 1.6970(1.7910) | Loss 14.4622(12.2158) | Error 0.6024(0.6336) Steps 526(499.65) | Grad Norm 8.1766(16.1151) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 50.5966(46.6826) | Bit/dim 4.4244(4.5669) | Xent 1.7442(1.7896) | Loss 11.5686(12.1964) | Error 0.6206(0.6332) Steps 598(502.60) | Grad Norm 19.5642(16.2185) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 44.7939(46.6260) | Bit/dim 4.4610(4.5637) | Xent 1.7101(1.7872) | Loss 11.2358(12.1676) | Error 0.6024(0.6323) Steps 508(502.76) | Grad Norm 18.9776(16.3013) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 45.7076(46.5984) | Bit/dim 4.4352(4.5598) | Xent 1.7157(1.7851) | Loss 11.4198(12.1452) | Error 0.5982(0.6312) Steps 562(504.54) | Grad Norm 10.9499(16.1408) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 47.9321(46.6384) | Bit/dim 4.3698(4.5541) | Xent 1.7161(1.7830) | Loss 11.3741(12.1220) | Error 0.6156(0.6308) Steps 532(505.36) | Grad Norm 8.5899(15.9142) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 47.7123(46.6706) | Bit/dim 4.4005(4.5495) | Xent 1.7008(1.7805) | Loss 11.4034(12.1005) | Error 0.6021(0.6299) Steps 532(506.16) | Grad Norm 15.1403(15.8910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.8400, Epoch Time 328.9596(317.8459), Bit/dim 4.4181(best: 4.4071), Xent 1.6495, Loss 5.2428, Error 0.5929(best: 0.5820)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 48.9259(46.7383) | Bit/dim 4.4044(4.5452) | Xent 1.7097(1.7784) | Loss 14.3380(12.1676) | Error 0.6056(0.6292) Steps 526(506.75) | Grad Norm 13.3596(15.8151) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 48.6081(46.7944) | Bit/dim 4.3834(4.5403) | Xent 1.6693(1.7751) | Loss 11.0972(12.1355) | Error 0.5966(0.6282) Steps 508(506.79) | Grad Norm 5.8439(15.5159) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 45.2036(46.7467) | Bit/dim 4.3972(4.5360) | Xent 1.7265(1.7737) | Loss 11.3926(12.1132) | Error 0.6205(0.6280) Steps 532(507.55) | Grad Norm 14.8112(15.4948) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 48.3845(46.7958) | Bit/dim 4.3594(4.5307) | Xent 1.6831(1.7710) | Loss 11.1588(12.0846) | Error 0.5986(0.6271) Steps 532(508.28) | Grad Norm 5.4111(15.1923) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 46.5175(46.7875) | Bit/dim 4.3636(4.5257) | Xent 1.6693(1.7679) | Loss 11.2002(12.0580) | Error 0.5934(0.6261) Steps 538(509.17) | Grad Norm 10.4661(15.0505) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 48.7322(46.8458) | Bit/dim 4.3699(4.5210) | Xent 1.6724(1.7650) | Loss 11.2916(12.0350) | Error 0.5954(0.6252) Steps 532(509.86) | Grad Norm 10.9101(14.9263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 21.5319, Epoch Time 325.9288(318.0884), Bit/dim 4.3487(best: 4.4071), Xent 1.6126, Loss 5.1551, Error 0.5766(best: 0.5820)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 48.3425(46.8907) | Bit/dim 4.3492(4.5159) | Xent 1.6744(1.7623) | Loss 13.7443(12.0863) | Error 0.5955(0.6243) Steps 550(511.06) | Grad Norm 5.6106(14.6468) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 48.8014(46.9480) | Bit/dim 4.3459(4.5108) | Xent 1.6650(1.7594) | Loss 11.0777(12.0561) | Error 0.5896(0.6232) Steps 520(511.33) | Grad Norm 7.3051(14.4266) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 45.9576(46.9183) | Bit/dim 4.3320(4.5054) | Xent 1.6472(1.7560) | Loss 11.1284(12.0282) | Error 0.5850(0.6221) Steps 502(511.05) | Grad Norm 9.1540(14.2684) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 47.3602(46.9316) | Bit/dim 4.3206(4.4999) | Xent 1.6616(1.7532) | Loss 11.0088(11.9977) | Error 0.5842(0.6209) Steps 526(511.50) | Grad Norm 7.4660(14.0643) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 48.5023(46.9787) | Bit/dim 4.3333(4.4949) | Xent 1.6464(1.7500) | Loss 11.1378(11.9719) | Error 0.5938(0.6201) Steps 526(511.93) | Grad Norm 8.5127(13.8978) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 46.7913(46.9731) | Bit/dim 4.3179(4.4896) | Xent 1.6800(1.7479) | Loss 10.7802(11.9361) | Error 0.5958(0.6194) Steps 502(511.64) | Grad Norm 14.0030(13.9009) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.2873, Epoch Time 326.3039(318.3348), Bit/dim 4.3477(best: 4.3487), Xent 1.6132, Loss 5.1543, Error 0.5715(best: 0.5766)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 47.7039(46.9950) | Bit/dim 4.3444(4.4852) | Xent 1.6694(1.7456) | Loss 14.1586(12.0028) | Error 0.5965(0.6187) Steps 538(512.43) | Grad Norm 16.9740(13.9931) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 46.8930(46.9919) | Bit/dim 4.3626(4.4815) | Xent 1.6725(1.7434) | Loss 11.3559(11.9834) | Error 0.5911(0.6179) Steps 532(513.01) | Grad Norm 13.5440(13.9796) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 51.4987(47.1271) | Bit/dim 4.4333(4.4801) | Xent 1.7320(1.7430) | Loss 11.3747(11.9651) | Error 0.6119(0.6177) Steps 544(513.94) | Grad Norm 15.3537(14.0209) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 47.8795(47.1497) | Bit/dim 4.6256(4.4845) | Xent 1.6840(1.7412) | Loss 11.6512(11.9557) | Error 0.5984(0.6171) Steps 538(514.66) | Grad Norm 14.2473(14.0277) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 46.5546(47.1318) | Bit/dim 4.3895(4.4816) | Xent 1.6558(1.7387) | Loss 11.1615(11.9319) | Error 0.5950(0.6165) Steps 526(515.00) | Grad Norm 6.8312(13.8118) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 53.8081(47.3321) | Bit/dim 4.5237(4.4829) | Xent 1.7146(1.7380) | Loss 11.4391(11.9171) | Error 0.6081(0.6162) Steps 580(516.95) | Grad Norm 17.4497(13.9209) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 22.3439, Epoch Time 334.8017(318.8288), Bit/dim 4.3872(best: 4.3477), Xent 1.6406, Loss 5.2075, Error 0.5947(best: 0.5715)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 46.2498(47.2997) | Bit/dim 4.3825(4.4799) | Xent 1.6861(1.7364) | Loss 13.9092(11.9769) | Error 0.6100(0.6160) Steps 508(516.69) | Grad Norm 18.4105(14.0556) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 49.1107(47.3540) | Bit/dim 4.4931(4.4803) | Xent 1.7007(1.7353) | Loss 11.5239(11.9633) | Error 0.6128(0.6159) Steps 550(517.69) | Grad Norm 18.4537(14.1875) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 50.8197(47.4580) | Bit/dim 4.4131(4.4782) | Xent 1.6320(1.7322) | Loss 11.3777(11.9457) | Error 0.5801(0.6149) Steps 532(518.12) | Grad Norm 7.9808(14.0013) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 48.1418(47.4785) | Bit/dim 4.4527(4.4775) | Xent 1.8274(1.7351) | Loss 11.3319(11.9273) | Error 0.6334(0.6154) Steps 532(518.53) | Grad Norm 35.3490(14.6418) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 46.5374(47.4502) | Bit/dim 4.4552(4.4768) | Xent 1.7362(1.7351) | Loss 11.3106(11.9088) | Error 0.6096(0.6152) Steps 556(519.66) | Grad Norm 22.2699(14.8706) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 49.9931(47.5265) | Bit/dim 4.4007(4.4745) | Xent 1.6759(1.7333) | Loss 11.3521(11.8921) | Error 0.5974(0.6147) Steps 574(521.29) | Grad Norm 9.9106(14.7218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 21.1390, Epoch Time 330.6968(319.1849), Bit/dim 4.4116(best: 4.3477), Xent 1.6903, Loss 5.2567, Error 0.6067(best: 0.5715)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 46.8583(47.5065) | Bit/dim 4.4133(4.4727) | Xent 1.7423(1.7336) | Loss 13.5124(11.9407) | Error 0.6244(0.6150) Steps 520(521.25) | Grad Norm 18.2379(14.8273) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 48.4443(47.5346) | Bit/dim 4.3687(4.4696) | Xent 1.6954(1.7325) | Loss 11.2100(11.9188) | Error 0.6090(0.6148) Steps 490(520.31) | Grad Norm 7.0736(14.5947) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 48.2850(47.5571) | Bit/dim 4.3710(4.4666) | Xent 1.7004(1.7315) | Loss 11.2384(11.8984) | Error 0.6089(0.6146) Steps 508(519.94) | Grad Norm 12.1762(14.5221) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 48.1377(47.5745) | Bit/dim 4.3392(4.4628) | Xent 1.6968(1.7305) | Loss 11.2009(11.8774) | Error 0.5993(0.6142) Steps 532(520.30) | Grad Norm 5.1918(14.2422) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 50.3948(47.6592) | Bit/dim 4.3605(4.4597) | Xent 1.6706(1.7287) | Loss 11.3718(11.8623) | Error 0.5901(0.6134) Steps 532(520.65) | Grad Norm 9.4804(14.0994) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 50.7909(47.7531) | Bit/dim 4.3329(4.4559) | Xent 1.6853(1.7274) | Loss 11.0626(11.8383) | Error 0.6118(0.6134) Steps 514(520.45) | Grad Norm 7.8322(13.9113) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 20.9220, Epoch Time 331.8605(319.5651), Bit/dim 4.2996(best: 4.3477), Xent 1.6228, Loss 5.1110, Error 0.5803(best: 0.5715)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 46.6685(47.7206) | Bit/dim 4.3058(4.4514) | Xent 1.6730(1.7257) | Loss 13.6163(11.8916) | Error 0.5961(0.6129) Steps 538(520.98) | Grad Norm 5.4508(13.6575) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 46.3892(47.6806) | Bit/dim 4.3095(4.4472) | Xent 1.6356(1.7230) | Loss 11.0689(11.8669) | Error 0.5816(0.6119) Steps 502(520.41) | Grad Norm 8.9775(13.5171) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 49.1365(47.7243) | Bit/dim 4.2993(4.4427) | Xent 1.6619(1.7212) | Loss 10.9504(11.8394) | Error 0.5942(0.6114) Steps 514(520.22) | Grad Norm 10.8396(13.4368) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 46.9083(47.6998) | Bit/dim 4.2898(4.4381) | Xent 1.6294(1.7184) | Loss 11.0503(11.8158) | Error 0.5805(0.6105) Steps 508(519.85) | Grad Norm 13.9829(13.4532) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 48.1130(47.7122) | Bit/dim 4.3416(4.4352) | Xent 1.6837(1.7174) | Loss 11.1466(11.7957) | Error 0.5990(0.6101) Steps 526(520.04) | Grad Norm 18.3367(13.5997) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 47.3067(47.7001) | Bit/dim 4.2764(4.4305) | Xent 1.6804(1.7163) | Loss 11.0021(11.7719) | Error 0.5939(0.6097) Steps 490(519.14) | Grad Norm 18.9436(13.7600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 21.2661, Epoch Time 323.5764(319.6855), Bit/dim 4.2784(best: 4.2996), Xent 1.5728, Loss 5.0647, Error 0.5649(best: 0.5715)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 44.1253(47.5928) | Bit/dim 4.2670(4.4256) | Xent 1.6245(1.7135) | Loss 13.5529(11.8253) | Error 0.5784(0.6087) Steps 514(518.98) | Grad Norm 7.4259(13.5700) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 49.0075(47.6353) | Bit/dim 4.2807(4.4212) | Xent 1.6498(1.7116) | Loss 10.7795(11.7939) | Error 0.5856(0.6080) Steps 508(518.65) | Grad Norm 11.5873(13.5105) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 49.4068(47.6884) | Bit/dim 4.2942(4.4174) | Xent 1.6429(1.7096) | Loss 11.1656(11.7751) | Error 0.5789(0.6071) Steps 532(519.05) | Grad Norm 13.7090(13.5165) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 49.9168(47.7553) | Bit/dim 4.2738(4.4131) | Xent 1.6301(1.7072) | Loss 11.1292(11.7557) | Error 0.5822(0.6064) Steps 544(519.80) | Grad Norm 14.3397(13.5412) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 49.7035(47.8137) | Bit/dim 4.2664(4.4087) | Xent 1.6155(1.7044) | Loss 11.1341(11.7371) | Error 0.5791(0.6056) Steps 514(519.63) | Grad Norm 5.8590(13.3107) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 45.9148(47.7567) | Bit/dim 4.2451(4.4038) | Xent 1.6010(1.7013) | Loss 10.6480(11.7044) | Error 0.5769(0.6047) Steps 520(519.64) | Grad Norm 8.1670(13.1564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.0178, Epoch Time 329.1667(319.9699), Bit/dim 4.2471(best: 4.2784), Xent 1.5807, Loss 5.0374, Error 0.5651(best: 0.5649)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 47.9426(47.7623) | Bit/dim 4.2709(4.3998) | Xent 1.6352(1.6993) | Loss 13.8495(11.7687) | Error 0.5819(0.6040) Steps 514(519.47) | Grad Norm 14.2769(13.1900) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 48.8353(47.7945) | Bit/dim 4.2327(4.3948) | Xent 1.5828(1.6958) | Loss 10.8545(11.7413) | Error 0.5742(0.6031) Steps 544(520.20) | Grad Norm 10.0789(13.0967) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 52.5274(47.9365) | Bit/dim 4.2358(4.3900) | Xent 1.6013(1.6930) | Loss 10.9974(11.7190) | Error 0.5741(0.6023) Steps 508(519.84) | Grad Norm 7.1387(12.9179) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 50.4345(48.0114) | Bit/dim 4.2324(4.3853) | Xent 1.6136(1.6906) | Loss 10.9689(11.6965) | Error 0.5787(0.6016) Steps 538(520.38) | Grad Norm 8.8712(12.7965) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 47.0053(47.9812) | Bit/dim 4.2567(4.3814) | Xent 1.6460(1.6893) | Loss 10.7926(11.6694) | Error 0.5855(0.6011) Steps 526(520.55) | Grad Norm 15.0366(12.8637) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 44.7980(47.8857) | Bit/dim 4.3192(4.3796) | Xent 1.6405(1.6878) | Loss 11.0184(11.6498) | Error 0.5861(0.6006) Steps 514(520.36) | Grad Norm 15.0055(12.9280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 21.4573, Epoch Time 331.2849(320.3094), Bit/dim 4.2505(best: 4.2471), Xent 1.5452, Loss 5.0231, Error 0.5531(best: 0.5649)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 47.1930(47.8650) | Bit/dim 4.2504(4.3757) | Xent 1.6095(1.6855) | Loss 13.7666(11.7133) | Error 0.5764(0.5999) Steps 508(519.98) | Grad Norm 13.0568(12.9318) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 49.6484(47.9185) | Bit/dim 4.2359(4.3715) | Xent 1.6143(1.6833) | Loss 10.4373(11.6751) | Error 0.5817(0.5994) Steps 520(519.99) | Grad Norm 10.9105(12.8712) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 43.7501(47.7934) | Bit/dim 4.2384(4.3675) | Xent 1.6196(1.6814) | Loss 10.7653(11.6478) | Error 0.5844(0.5989) Steps 508(519.63) | Grad Norm 14.7273(12.9269) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 45.2828(47.7181) | Bit/dim 4.2303(4.3634) | Xent 1.6026(1.6791) | Loss 10.8303(11.6232) | Error 0.5745(0.5982) Steps 520(519.64) | Grad Norm 11.4898(12.8838) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 46.9964(47.6964) | Bit/dim 4.2349(4.3595) | Xent 1.6442(1.6780) | Loss 10.9041(11.6017) | Error 0.5933(0.5980) Steps 514(519.47) | Grad Norm 16.6409(12.9965) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 45.6154(47.6340) | Bit/dim 4.2303(4.3557) | Xent 1.6060(1.6759) | Loss 10.6468(11.5730) | Error 0.5820(0.5976) Steps 520(519.48) | Grad Norm 12.1883(12.9722) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 21.9975, Epoch Time 318.0899(320.2428), Bit/dim 4.2275(best: 4.2471), Xent 1.5658, Loss 5.0103, Error 0.5578(best: 0.5531)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 48.7759(47.6683) | Bit/dim 4.2249(4.3517) | Xent 1.6231(1.6743) | Loss 13.7628(11.6387) | Error 0.5815(0.5971) Steps 526(519.68) | Grad Norm 14.7683(13.0261) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 49.9102(47.7355) | Bit/dim 4.2128(4.3476) | Xent 1.5927(1.6718) | Loss 10.9004(11.6166) | Error 0.5707(0.5963) Steps 508(519.33) | Grad Norm 11.0671(12.9674) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 45.4836(47.6680) | Bit/dim 4.1966(4.3430) | Xent 1.6195(1.6703) | Loss 10.6946(11.5889) | Error 0.5800(0.5958) Steps 526(519.53) | Grad Norm 11.5052(12.9235) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 43.7089(47.5492) | Bit/dim 4.2660(4.3407) | Xent 1.5992(1.6681) | Loss 10.7538(11.5639) | Error 0.5741(0.5951) Steps 502(519.00) | Grad Norm 12.3536(12.9064) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 46.2750(47.5110) | Bit/dim 4.1784(4.3359) | Xent 1.6258(1.6669) | Loss 10.7770(11.5403) | Error 0.5849(0.5948) Steps 520(519.03) | Grad Norm 14.8958(12.9661) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 45.3080(47.4449) | Bit/dim 4.2048(4.3319) | Xent 1.5687(1.6639) | Loss 10.6633(11.5139) | Error 0.5590(0.5938) Steps 514(518.88) | Grad Norm 9.6060(12.8653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 22.6309, Epoch Time 320.5243(320.2512), Bit/dim 4.1885(best: 4.2275), Xent 1.5177, Loss 4.9474, Error 0.5420(best: 0.5531)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 44.7405(47.3638) | Bit/dim 4.1767(4.3273) | Xent 1.5761(1.6613) | Loss 12.4680(11.5426) | Error 0.5721(0.5931) Steps 514(518.74) | Grad Norm 7.3345(12.6993) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 44.1244(47.2666) | Bit/dim 4.1988(4.3234) | Xent 1.6106(1.6598) | Loss 10.7655(11.5193) | Error 0.5751(0.5926) Steps 490(517.87) | Grad Norm 12.2425(12.6856) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 52.4834(47.4231) | Bit/dim 4.1747(4.3190) | Xent 1.5776(1.6573) | Loss 10.5605(11.4905) | Error 0.5644(0.5917) Steps 490(517.04) | Grad Norm 7.8777(12.5414) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 53.2263(47.5972) | Bit/dim 4.1770(4.3147) | Xent 1.5711(1.6547) | Loss 10.9356(11.4738) | Error 0.5656(0.5909) Steps 502(516.59) | Grad Norm 8.6813(12.4256) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 48.0982(47.6122) | Bit/dim 4.2180(4.3118) | Xent 1.5409(1.6513) | Loss 10.9839(11.4592) | Error 0.5530(0.5898) Steps 514(516.51) | Grad Norm 12.7418(12.4351) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 45.7462(47.5562) | Bit/dim 4.1748(4.3077) | Xent 1.6459(1.6511) | Loss 10.1684(11.4204) | Error 0.5835(0.5896) Steps 502(516.07) | Grad Norm 20.5048(12.6772) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 21.8878, Epoch Time 329.0733(320.5159), Bit/dim 4.2021(best: 4.1885), Xent 1.5944, Loss 4.9993, Error 0.5694(best: 0.5420)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 48.8120(47.5939) | Bit/dim 4.2090(4.3047) | Xent 1.6594(1.6514) | Loss 13.8285(11.4927) | Error 0.5968(0.5898) Steps 526(516.37) | Grad Norm 17.2043(12.8130) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 47.9716(47.6052) | Bit/dim 4.1829(4.3011) | Xent 1.5762(1.6491) | Loss 10.7195(11.4695) | Error 0.5635(0.5890) Steps 508(516.12) | Grad Norm 12.9328(12.8166) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 47.3642(47.5980) | Bit/dim 4.1805(4.2975) | Xent 1.7542(1.6523) | Loss 10.9028(11.4525) | Error 0.6155(0.5898) Steps 490(515.34) | Grad Norm 28.1894(13.2778) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 44.7852(47.5136) | Bit/dim 4.1838(4.2940) | Xent 1.5923(1.6505) | Loss 10.7919(11.4327) | Error 0.5714(0.5893) Steps 502(514.94) | Grad Norm 11.4066(13.2216) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 52.8649(47.6742) | Bit/dim 4.1511(4.2898) | Xent 1.5857(1.6485) | Loss 10.6384(11.4088) | Error 0.5687(0.5887) Steps 514(514.91) | Grad Norm 4.0504(12.9465) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 45.4800(47.6083) | Bit/dim 4.1506(4.2856) | Xent 1.5749(1.6463) | Loss 10.8512(11.3921) | Error 0.5675(0.5880) Steps 520(515.06) | Grad Norm 10.2427(12.8654) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 21.0355, Epoch Time 326.9887(320.7101), Bit/dim 4.1956(best: 4.1885), Xent 1.5301, Loss 4.9607, Error 0.5484(best: 0.5420)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 45.0403(47.5313) | Bit/dim 4.1972(4.2829) | Xent 1.5771(1.6442) | Loss 13.7325(11.4623) | Error 0.5629(0.5873) Steps 526(515.39) | Grad Norm 12.9209(12.8671) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 43.4270(47.4082) | Bit/dim 4.1551(4.2791) | Xent 1.6075(1.6431) | Loss 10.6975(11.4394) | Error 0.5853(0.5872) Steps 484(514.45) | Grad Norm 13.8722(12.8972) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 44.7465(47.3283) | Bit/dim 4.1761(4.2760) | Xent 1.5693(1.6409) | Loss 10.6705(11.4163) | Error 0.5661(0.5866) Steps 484(513.53) | Grad Norm 11.8301(12.8652) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 48.4273(47.3613) | Bit/dim 4.1838(4.2732) | Xent 1.6057(1.6399) | Loss 10.7084(11.3951) | Error 0.5839(0.5865) Steps 496(513.01) | Grad Norm 19.1885(13.0549) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 44.8587(47.2862) | Bit/dim 4.1561(4.2697) | Xent 1.5615(1.6375) | Loss 10.4925(11.3680) | Error 0.5534(0.5855) Steps 508(512.86) | Grad Norm 12.3669(13.0343) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 45.0974(47.2205) | Bit/dim 4.1358(4.2657) | Xent 1.5788(1.6358) | Loss 10.4530(11.3405) | Error 0.5710(0.5851) Steps 502(512.53) | Grad Norm 8.0367(12.8843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 21.3777, Epoch Time 312.1293(320.4527), Bit/dim 4.1540(best: 4.1885), Xent 1.4899, Loss 4.8989, Error 0.5444(best: 0.5420)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 48.0051(47.2441) | Bit/dim 4.1723(4.2629) | Xent 1.5589(1.6334) | Loss 13.8881(11.4170) | Error 0.5577(0.5842) Steps 514(512.58) | Grad Norm 8.9805(12.7672) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 47.1802(47.2422) | Bit/dim 4.1778(4.2604) | Xent 1.5392(1.6306) | Loss 10.8334(11.3995) | Error 0.5506(0.5832) Steps 502(512.26) | Grad Norm 7.6318(12.6132) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 47.0434(47.2362) | Bit/dim 4.1296(4.2564) | Xent 1.5416(1.6279) | Loss 10.4744(11.3717) | Error 0.5585(0.5825) Steps 514(512.31) | Grad Norm 8.2091(12.4810) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 45.4913(47.1839) | Bit/dim 4.1110(4.2521) | Xent 1.5535(1.6257) | Loss 10.3965(11.3424) | Error 0.5606(0.5818) Steps 490(511.64) | Grad Norm 7.9413(12.3448) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 46.7787(47.1717) | Bit/dim 4.1391(4.2487) | Xent 1.5171(1.6225) | Loss 10.3696(11.3133) | Error 0.5421(0.5807) Steps 502(511.35) | Grad Norm 8.1531(12.2191) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 47.8277(47.1914) | Bit/dim 4.1350(4.2453) | Xent 1.5159(1.6193) | Loss 10.5046(11.2890) | Error 0.5484(0.5797) Steps 508(511.25) | Grad Norm 10.6894(12.1732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 21.3269, Epoch Time 322.2905(320.5078), Bit/dim 4.1204(best: 4.1540), Xent 1.4618, Loss 4.8513, Error 0.5276(best: 0.5420)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 45.9911(47.1554) | Bit/dim 4.1213(4.2416) | Xent 1.5263(1.6165) | Loss 13.5033(11.3554) | Error 0.5566(0.5790) Steps 496(510.79) | Grad Norm 7.4537(12.0316) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 45.0771(47.0930) | Bit/dim 4.1359(4.2384) | Xent 1.5205(1.6136) | Loss 10.4829(11.3293) | Error 0.5500(0.5781) Steps 502(510.53) | Grad Norm 10.2652(11.9786) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 45.9512(47.0588) | Bit/dim 4.1321(4.2352) | Xent 1.5175(1.6107) | Loss 10.4786(11.3037) | Error 0.5414(0.5770) Steps 502(510.27) | Grad Norm 10.2852(11.9278) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 52.8088(47.2313) | Bit/dim 4.1290(4.2320) | Xent 1.5576(1.6091) | Loss 10.0718(11.2668) | Error 0.5626(0.5766) Steps 538(511.11) | Grad Norm 14.3376(12.0001) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 43.8491(47.1298) | Bit/dim 4.1344(4.2291) | Xent 1.5587(1.6076) | Loss 10.6804(11.2492) | Error 0.5727(0.5765) Steps 484(510.29) | Grad Norm 11.9348(11.9981) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 47.8552(47.1516) | Bit/dim 4.1043(4.2253) | Xent 1.5237(1.6051) | Loss 10.6583(11.2315) | Error 0.5484(0.5756) Steps 490(509.68) | Grad Norm 11.2740(11.9764) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 21.2541, Epoch Time 321.3849(320.5341), Bit/dim 4.1125(best: 4.1204), Xent 1.5277, Loss 4.8763, Error 0.5571(best: 0.5276)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 44.6314(47.0760) | Bit/dim 4.1038(4.2217) | Xent 1.5805(1.6043) | Loss 13.1882(11.2902) | Error 0.5627(0.5752) Steps 502(509.45) | Grad Norm 14.9512(12.0657) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 49.0125(47.1341) | Bit/dim 4.1340(4.2191) | Xent 1.6090(1.6045) | Loss 10.6429(11.2707) | Error 0.5831(0.5755) Steps 526(509.95) | Grad Norm 17.9599(12.2425) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 49.9000(47.2170) | Bit/dim 4.1673(4.2175) | Xent 1.5744(1.6036) | Loss 10.7593(11.2554) | Error 0.5717(0.5754) Steps 550(511.15) | Grad Norm 17.1483(12.3897) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 46.7604(47.2033) | Bit/dim 4.1184(4.2145) | Xent 1.5899(1.6032) | Loss 10.7598(11.2405) | Error 0.5724(0.5753) Steps 520(511.42) | Grad Norm 10.6707(12.3381) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 51.6870(47.3378) | Bit/dim 4.1221(4.2118) | Xent 1.6994(1.6061) | Loss 10.7287(11.2252) | Error 0.5956(0.5759) Steps 514(511.49) | Grad Norm 23.0072(12.6582) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 49.0983(47.3907) | Bit/dim 4.1773(4.2107) | Xent 1.7099(1.6092) | Loss 10.8890(11.2151) | Error 0.6196(0.5772) Steps 520(511.75) | Grad Norm 16.1001(12.7614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.1822, Epoch Time 332.8886(320.9047), Bit/dim 4.1651(best: 4.1125), Xent 1.5488, Loss 4.9395, Error 0.5584(best: 0.5276)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 46.8972(47.3759) | Bit/dim 4.1673(4.2094) | Xent 1.6370(1.6100) | Loss 13.2810(11.2771) | Error 0.5786(0.5772) Steps 514(511.82) | Grad Norm 14.4823(12.8131) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 49.3391(47.4348) | Bit/dim 4.1866(4.2087) | Xent 1.7075(1.6129) | Loss 10.9685(11.2678) | Error 0.6093(0.5782) Steps 514(511.88) | Grad Norm 12.4013(12.8007) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 49.2400(47.4889) | Bit/dim 4.1180(4.2060) | Xent 1.5437(1.6109) | Loss 10.6823(11.2503) | Error 0.5505(0.5774) Steps 526(512.31) | Grad Norm 4.6858(12.5573) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 45.8585(47.4400) | Bit/dim 4.1744(4.2051) | Xent 1.5763(1.6098) | Loss 10.6428(11.2320) | Error 0.5716(0.5772) Steps 532(512.90) | Grad Norm 9.0129(12.4509) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 47.7899(47.4505) | Bit/dim 4.1383(4.2031) | Xent 1.5673(1.6085) | Loss 10.6198(11.2137) | Error 0.5611(0.5767) Steps 508(512.75) | Grad Norm 8.9116(12.3447) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 46.8890(47.4336) | Bit/dim 4.1314(4.2009) | Xent 1.6245(1.6090) | Loss 10.7286(11.1991) | Error 0.5804(0.5768) Steps 532(513.33) | Grad Norm 18.2341(12.5214) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 21.7849, Epoch Time 325.9835(321.0571), Bit/dim 4.1704(best: 4.1125), Xent 1.5434, Loss 4.9421, Error 0.5617(best: 0.5276)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 47.5104(47.4360) | Bit/dim 4.1698(4.2000) | Xent 1.6236(1.6095) | Loss 13.8478(11.2786) | Error 0.5806(0.5769) Steps 520(513.53) | Grad Norm 18.9964(12.7157) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 51.0251(47.5436) | Bit/dim 4.1194(4.1976) | Xent 1.5216(1.6068) | Loss 10.6404(11.2594) | Error 0.5436(0.5759) Steps 520(513.72) | Grad Norm 7.0888(12.5469) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 49.6397(47.6065) | Bit/dim 4.1183(4.1952) | Xent 1.5788(1.6060) | Loss 10.3242(11.2314) | Error 0.5557(0.5753) Steps 508(513.55) | Grad Norm 15.0464(12.6219) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 45.8244(47.5530) | Bit/dim 4.1088(4.1926) | Xent 1.5478(1.6042) | Loss 10.4948(11.2093) | Error 0.5544(0.5747) Steps 532(514.10) | Grad Norm 6.0236(12.4239) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 48.2700(47.5746) | Bit/dim 4.1050(4.1900) | Xent 1.5752(1.6034) | Loss 10.5764(11.1903) | Error 0.5615(0.5743) Steps 526(514.46) | Grad Norm 10.2553(12.3589) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 45.7009(47.5183) | Bit/dim 4.1388(4.1884) | Xent 1.5532(1.6019) | Loss 10.5893(11.1723) | Error 0.5550(0.5737) Steps 514(514.45) | Grad Norm 11.8237(12.3428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 21.4862, Epoch Time 327.9774(321.2647), Bit/dim 4.1293(best: 4.1125), Xent 1.4790, Loss 4.8688, Error 0.5338(best: 0.5276)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 52.2108(47.6591) | Bit/dim 4.1311(4.1867) | Xent 1.5270(1.5996) | Loss 13.4198(11.2397) | Error 0.5528(0.5731) Steps 538(515.15) | Grad Norm 7.3562(12.1932) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 45.3798(47.5907) | Bit/dim 4.0946(4.1839) | Xent 1.5126(1.5970) | Loss 10.5136(11.2179) | Error 0.5528(0.5725) Steps 502(514.76) | Grad Norm 6.7337(12.0294) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 47.3685(47.5841) | Bit/dim 4.0829(4.1809) | Xent 1.5161(1.5946) | Loss 10.4945(11.1962) | Error 0.5448(0.5717) Steps 520(514.92) | Grad Norm 5.8449(11.8439) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 46.0441(47.5379) | Bit/dim 4.1028(4.1786) | Xent 1.5162(1.5922) | Loss 10.2790(11.1687) | Error 0.5501(0.5710) Steps 514(514.89) | Grad Norm 9.7068(11.7798) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 49.0761(47.5840) | Bit/dim 4.0922(4.1760) | Xent 1.5444(1.5908) | Loss 10.6873(11.1542) | Error 0.5493(0.5704) Steps 532(515.40) | Grad Norm 11.8267(11.7812) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 50.2248(47.6632) | Bit/dim 4.0858(4.1733) | Xent 1.5276(1.5889) | Loss 10.6009(11.1376) | Error 0.5503(0.5698) Steps 544(516.26) | Grad Norm 15.5315(11.8937) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 21.3462, Epoch Time 329.7891(321.5204), Bit/dim 4.0882(best: 4.1125), Xent 1.4480, Loss 4.8122, Error 0.5200(best: 0.5276)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 46.6101(47.6316) | Bit/dim 4.0843(4.1706) | Xent 1.4899(1.5859) | Loss 13.2053(11.1997) | Error 0.5384(0.5688) Steps 526(516.55) | Grad Norm 10.4954(11.8517) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 46.1806(47.5881) | Bit/dim 4.0669(4.1675) | Xent 1.4906(1.5831) | Loss 10.3987(11.1756) | Error 0.5341(0.5678) Steps 514(516.48) | Grad Norm 6.0592(11.6780) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 50.5968(47.6784) | Bit/dim 4.0656(4.1644) | Xent 1.4798(1.5800) | Loss 10.3658(11.1513) | Error 0.5324(0.5667) Steps 538(517.12) | Grad Norm 6.7144(11.5290) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 46.5613(47.6449) | Bit/dim 4.0625(4.1614) | Xent 1.4773(1.5769) | Loss 10.3916(11.1286) | Error 0.5310(0.5656) Steps 520(517.21) | Grad Norm 6.1753(11.3684) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 45.6724(47.5857) | Bit/dim 4.0632(4.1584) | Xent 1.4887(1.5742) | Loss 10.3953(11.1066) | Error 0.5360(0.5648) Steps 532(517.65) | Grad Norm 9.6006(11.3154) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 48.0004(47.5981) | Bit/dim 4.0729(4.1559) | Xent 1.5338(1.5730) | Loss 10.5037(11.0885) | Error 0.5647(0.5648) Steps 550(518.62) | Grad Norm 15.1987(11.4319) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 22.6919, Epoch Time 325.6727(321.6450), Bit/dim 4.1665(best: 4.0882), Xent 1.6797, Loss 5.0064, Error 0.6025(best: 0.5200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 49.6821(47.6606) | Bit/dim 4.1620(4.1561) | Xent 1.7561(1.5785) | Loss 13.8408(11.1710) | Error 0.6214(0.5665) Steps 532(519.02) | Grad Norm 24.9525(11.8375) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 48.3672(47.6818) | Bit/dim 4.0955(4.1542) | Xent 1.5245(1.5769) | Loss 10.3813(11.1473) | Error 0.5385(0.5656) Steps 550(519.95) | Grad Norm 9.9790(11.7818) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 49.3112(47.7307) | Bit/dim 4.1380(4.1538) | Xent 1.6908(1.5803) | Loss 10.9024(11.1400) | Error 0.6009(0.5667) Steps 544(520.67) | Grad Norm 28.3665(12.2793) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 45.5913(47.6665) | Bit/dim 4.1062(4.1523) | Xent 1.7310(1.5848) | Loss 10.6004(11.1238) | Error 0.6010(0.5677) Steps 526(520.83) | Grad Norm 25.3198(12.6705) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 47.5794(47.6639) | Bit/dim 4.1078(4.1510) | Xent 1.6006(1.5853) | Loss 10.5685(11.1072) | Error 0.5776(0.5680) Steps 526(520.99) | Grad Norm 12.7969(12.6743) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 49.3628(47.7149) | Bit/dim 4.1062(4.1496) | Xent 1.5442(1.5841) | Loss 10.5544(11.0906) | Error 0.5599(0.5678) Steps 520(520.96) | Grad Norm 6.1132(12.4775) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 22.6698, Epoch Time 331.7802(321.9491), Bit/dim 4.1274(best: 4.0882), Xent 1.5280, Loss 4.8914, Error 0.5520(best: 0.5200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 51.2319(47.8204) | Bit/dim 4.1332(4.1492) | Xent 1.6117(1.5849) | Loss 13.5698(11.1649) | Error 0.5742(0.5679) Steps 526(521.11) | Grad Norm 14.4843(12.5377) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 50.3227(47.8955) | Bit/dim 4.0817(4.1471) | Xent 1.5474(1.5838) | Loss 10.5445(11.1463) | Error 0.5600(0.5677) Steps 520(521.08) | Grad Norm 11.1957(12.4974) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 49.7312(47.9505) | Bit/dim 4.1315(4.1467) | Xent 1.5184(1.5818) | Loss 10.7144(11.1334) | Error 0.5414(0.5669) Steps 544(521.76) | Grad Norm 7.2211(12.3391) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 50.6102(48.0303) | Bit/dim 4.0850(4.1448) | Xent 1.5275(1.5802) | Loss 10.6523(11.1189) | Error 0.5570(0.5666) Steps 550(522.61) | Grad Norm 5.6477(12.1384) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 49.5126(48.0748) | Bit/dim 4.0669(4.1425) | Xent 1.5422(1.5791) | Loss 10.5678(11.1024) | Error 0.5550(0.5663) Steps 538(523.07) | Grad Norm 9.2285(12.0511) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 49.5170(48.1181) | Bit/dim 4.0532(4.1398) | Xent 1.5014(1.5767) | Loss 10.5523(11.0859) | Error 0.5400(0.5655) Steps 550(523.88) | Grad Norm 6.6056(11.8877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.1501, Epoch Time 342.8816(322.5770), Bit/dim 4.0668(best: 4.0882), Xent 1.4281, Loss 4.7809, Error 0.5096(best: 0.5200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 49.2697(48.1526) | Bit/dim 4.0725(4.1378) | Xent 1.4649(1.5734) | Loss 13.2093(11.1496) | Error 0.5296(0.5644) Steps 568(525.20) | Grad Norm 8.1972(11.7770) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 50.1731(48.2132) | Bit/dim 4.0369(4.1347) | Xent 1.5125(1.5715) | Loss 10.2108(11.1214) | Error 0.5501(0.5640) Steps 562(526.31) | Grad Norm 7.7331(11.6557) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 51.4807(48.3113) | Bit/dim 4.0468(4.1321) | Xent 1.4970(1.5693) | Loss 10.3982(11.0997) | Error 0.5423(0.5633) Steps 544(526.84) | Grad Norm 7.2749(11.5243) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 51.4294(48.4048) | Bit/dim 4.0484(4.1296) | Xent 1.4608(1.5660) | Loss 10.4507(11.0803) | Error 0.5262(0.5622) Steps 538(527.17) | Grad Norm 8.9540(11.4472) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 49.2682(48.4307) | Bit/dim 4.0567(4.1274) | Xent 1.5807(1.5665) | Loss 10.5559(11.0645) | Error 0.5581(0.5621) Steps 538(527.50) | Grad Norm 17.5586(11.6305) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 49.7864(48.4714) | Bit/dim 4.0600(4.1254) | Xent 1.6744(1.5697) | Loss 10.5797(11.0500) | Error 0.5894(0.5629) Steps 532(527.63) | Grad Norm 21.8476(11.9370) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 22.3126, Epoch Time 342.8259(323.1845), Bit/dim 4.0441(best: 4.0668), Xent 1.4276, Loss 4.7579, Error 0.5191(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 51.4847(48.5618) | Bit/dim 4.0521(4.1232) | Xent 1.4840(1.5672) | Loss 13.2954(11.1174) | Error 0.5388(0.5622) Steps 562(528.66) | Grad Norm 6.8716(11.7851) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 49.0128(48.5753) | Bit/dim 4.0271(4.1203) | Xent 1.5398(1.5663) | Loss 10.4376(11.0970) | Error 0.5516(0.5619) Steps 526(528.58) | Grad Norm 9.8795(11.7279) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 50.6184(48.6366) | Bit/dim 4.0526(4.1183) | Xent 1.4673(1.5634) | Loss 10.6460(11.0834) | Error 0.5315(0.5610) Steps 532(528.69) | Grad Norm 8.5265(11.6318) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 48.0634(48.6194) | Bit/dim 4.0353(4.1158) | Xent 1.5289(1.5623) | Loss 10.1456(11.0553) | Error 0.5566(0.5608) Steps 562(529.69) | Grad Norm 11.5910(11.6306) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 53.5592(48.7676) | Bit/dim 4.0477(4.1137) | Xent 1.5871(1.5631) | Loss 10.5358(11.0397) | Error 0.5797(0.5614) Steps 550(530.30) | Grad Norm 19.0319(11.8527) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 52.3129(48.8740) | Bit/dim 4.0533(4.1119) | Xent 1.6270(1.5650) | Loss 10.6488(11.0280) | Error 0.5640(0.5615) Steps 586(531.97) | Grad Norm 26.4935(12.2919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 24.1924, Epoch Time 349.2653(323.9669), Bit/dim 4.0873(best: 4.0441), Xent 1.4496, Loss 4.8120, Error 0.5183(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 50.7501(48.9302) | Bit/dim 4.0872(4.1112) | Xent 1.5028(1.5631) | Loss 13.6857(11.1077) | Error 0.5405(0.5608) Steps 544(532.33) | Grad Norm 10.9916(12.2529) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 50.1853(48.9679) | Bit/dim 4.0325(4.1088) | Xent 1.5054(1.5614) | Loss 10.4923(11.0893) | Error 0.5450(0.5604) Steps 544(532.68) | Grad Norm 8.0779(12.1276) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 53.2359(49.0959) | Bit/dim 4.0516(4.1071) | Xent 1.5345(1.5606) | Loss 10.6725(11.0768) | Error 0.5497(0.5601) Steps 550(533.20) | Grad Norm 16.2770(12.2521) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 48.2397(49.0702) | Bit/dim 4.0607(4.1057) | Xent 1.5021(1.5588) | Loss 10.4722(11.0586) | Error 0.5457(0.5596) Steps 544(533.52) | Grad Norm 10.8042(12.2087) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 47.2642(49.0161) | Bit/dim 4.0513(4.1041) | Xent 1.5016(1.5571) | Loss 10.5751(11.0441) | Error 0.5357(0.5589) Steps 532(533.48) | Grad Norm 8.1456(12.0868) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 49.9592(49.0444) | Bit/dim 4.0290(4.1018) | Xent 1.4727(1.5546) | Loss 10.4670(11.0268) | Error 0.5319(0.5581) Steps 550(533.97) | Grad Norm 7.1880(11.9398) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 22.1317, Epoch Time 340.4341(324.4609), Bit/dim 4.0259(best: 4.0441), Xent 1.4622, Loss 4.7570, Error 0.5359(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 50.7643(49.0960) | Bit/dim 4.0185(4.0993) | Xent 1.5010(1.5530) | Loss 12.9487(11.0845) | Error 0.5449(0.5577) Steps 544(534.27) | Grad Norm 12.3617(11.9525) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 49.6267(49.1119) | Bit/dim 4.0282(4.0972) | Xent 1.4475(1.5498) | Loss 10.3090(11.0612) | Error 0.5294(0.5568) Steps 562(535.10) | Grad Norm 6.3974(11.7858) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 49.2167(49.1150) | Bit/dim 4.0372(4.0954) | Xent 1.4520(1.5469) | Loss 10.4593(11.0431) | Error 0.5251(0.5559) Steps 538(535.19) | Grad Norm 5.4691(11.5963) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 53.4524(49.2451) | Bit/dim 4.0014(4.0926) | Xent 1.4391(1.5436) | Loss 10.2792(11.0202) | Error 0.5231(0.5549) Steps 514(534.56) | Grad Norm 5.7425(11.4207) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 48.0588(49.2096) | Bit/dim 4.0159(4.0903) | Xent 1.4370(1.5404) | Loss 10.2251(10.9964) | Error 0.5117(0.5536) Steps 526(534.30) | Grad Norm 8.1837(11.3236) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 51.6785(49.2836) | Bit/dim 4.0044(4.0877) | Xent 1.4546(1.5379) | Loss 10.4323(10.9794) | Error 0.5221(0.5527) Steps 544(534.59) | Grad Norm 10.0629(11.2858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 21.9908, Epoch Time 344.3559(325.0578), Bit/dim 4.0191(best: 4.0259), Xent 1.4202, Loss 4.7292, Error 0.5151(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 48.6938(49.2659) | Bit/dim 4.0183(4.0856) | Xent 1.4735(1.5359) | Loss 13.2659(11.0480) | Error 0.5349(0.5521) Steps 538(534.69) | Grad Norm 13.7652(11.3602) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 47.5825(49.2154) | Bit/dim 4.0105(4.0834) | Xent 1.4284(1.5327) | Loss 10.4265(11.0294) | Error 0.5186(0.5511) Steps 520(534.25) | Grad Norm 7.2005(11.2354) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 53.5051(49.3441) | Bit/dim 3.9886(4.0805) | Xent 1.4187(1.5293) | Loss 10.2594(11.0063) | Error 0.5135(0.5500) Steps 526(534.00) | Grad Norm 4.9080(11.0455) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 52.9159(49.4513) | Bit/dim 4.0114(4.0784) | Xent 1.4209(1.5260) | Loss 10.5504(10.9926) | Error 0.5219(0.5492) Steps 544(534.30) | Grad Norm 6.2248(10.9009) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 48.5788(49.4251) | Bit/dim 4.0149(4.0765) | Xent 1.4442(1.5236) | Loss 10.2222(10.9695) | Error 0.5259(0.5485) Steps 550(534.78) | Grad Norm 11.9499(10.9324) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 49.9399(49.4405) | Bit/dim 4.0065(4.0744) | Xent 1.4832(1.5224) | Loss 10.2461(10.9478) | Error 0.5394(0.5482) Steps 544(535.05) | Grad Norm 14.4888(11.0391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 22.7984, Epoch Time 342.2155(325.5725), Bit/dim 3.9935(best: 4.0191), Xent 1.4236, Loss 4.7054, Error 0.5141(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 47.6221(49.3860) | Bit/dim 3.9892(4.0719) | Xent 1.4784(1.5211) | Loss 12.7459(11.0017) | Error 0.5359(0.5478) Steps 526(534.78) | Grad Norm 16.2455(11.1953) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 49.2112(49.3807) | Bit/dim 4.0293(4.0706) | Xent 1.4789(1.5198) | Loss 10.3368(10.9818) | Error 0.5325(0.5474) Steps 544(535.06) | Grad Norm 14.1935(11.2852) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 48.0913(49.3421) | Bit/dim 3.9889(4.0682) | Xent 1.4108(1.5165) | Loss 10.1976(10.9583) | Error 0.5155(0.5464) Steps 532(534.97) | Grad Norm 4.4510(11.0802) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 46.5945(49.2596) | Bit/dim 3.9835(4.0656) | Xent 1.3996(1.5130) | Loss 10.1197(10.9331) | Error 0.5066(0.5452) Steps 538(535.06) | Grad Norm 7.4370(10.9709) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 47.8260(49.2166) | Bit/dim 3.9941(4.0635) | Xent 1.4343(1.5107) | Loss 10.2130(10.9115) | Error 0.5214(0.5445) Steps 562(535.86) | Grad Norm 11.5720(10.9889) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 49.0679(49.2122) | Bit/dim 3.9775(4.0609) | Xent 1.4189(1.5079) | Loss 10.0803(10.8866) | Error 0.5028(0.5432) Steps 532(535.75) | Grad Norm 7.1212(10.8729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 22.7865, Epoch Time 330.8758(325.7316), Bit/dim 4.0110(best: 3.9935), Xent 1.3692, Loss 4.6956, Error 0.4991(best: 0.5096)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 51.0834(49.2683) | Bit/dim 4.0219(4.0597) | Xent 1.4419(1.5059) | Loss 13.3238(10.9597) | Error 0.5200(0.5425) Steps 544(536.00) | Grad Norm 9.9510(10.8452) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 49.3701(49.2714) | Bit/dim 3.9805(4.0573) | Xent 1.4408(1.5040) | Loss 10.0914(10.9336) | Error 0.5235(0.5420) Steps 538(536.06) | Grad Norm 10.7882(10.8435) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 47.1304(49.2071) | Bit/dim 3.9913(4.0554) | Xent 1.4393(1.5020) | Loss 10.1994(10.9116) | Error 0.5249(0.5415) Steps 544(536.29) | Grad Norm 12.7781(10.9016) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 46.3681(49.1220) | Bit/dim 3.9688(4.0528) | Xent 1.3937(1.4988) | Loss 10.1996(10.8903) | Error 0.5014(0.5403) Steps 544(536.53) | Grad Norm 4.9524(10.7231) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 51.9778(49.2076) | Bit/dim 3.9797(4.0506) | Xent 1.4164(1.4963) | Loss 10.2594(10.8713) | Error 0.5124(0.5394) Steps 556(537.11) | Grad Norm 7.5416(10.6276) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 49.0869(49.2040) | Bit/dim 3.9515(4.0476) | Xent 1.4185(1.4940) | Loss 10.2525(10.8528) | Error 0.5109(0.5386) Steps 520(536.60) | Grad Norm 7.6996(10.5398) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.0805, Epoch Time 337.5538(326.0863), Bit/dim 3.9838(best: 3.9935), Xent 1.3628, Loss 4.6652, Error 0.4936(best: 0.4991)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 49.5889(49.2155) | Bit/dim 3.9935(4.0460) | Xent 1.4278(1.4920) | Loss 13.2504(10.9247) | Error 0.5144(0.5378) Steps 526(536.28) | Grad Norm 9.0143(10.4940) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 52.0138(49.2995) | Bit/dim 3.9772(4.0439) | Xent 1.3872(1.4888) | Loss 10.1262(10.9007) | Error 0.5002(0.5367) Steps 526(535.97) | Grad Norm 9.6926(10.4700) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 42.0406(49.0817) | Bit/dim 3.9747(4.0418) | Xent 1.4480(1.4876) | Loss 10.1036(10.8768) | Error 0.5185(0.5362) Steps 514(535.31) | Grad Norm 10.5113(10.4712) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 48.5946(49.0671) | Bit/dim 3.9576(4.0393) | Xent 1.4605(1.4868) | Loss 10.2382(10.8577) | Error 0.5242(0.5358) Steps 520(534.85) | Grad Norm 13.6501(10.5666) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 49.1108(49.0684) | Bit/dim 3.9657(4.0371) | Xent 1.3681(1.4832) | Loss 10.1512(10.8365) | Error 0.4960(0.5346) Steps 538(534.95) | Grad Norm 8.9890(10.5193) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 50.1905(49.1021) | Bit/dim 3.9509(4.0345) | Xent 1.3941(1.4806) | Loss 10.0405(10.8126) | Error 0.5054(0.5337) Steps 550(535.40) | Grad Norm 8.8091(10.4680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 22.9589, Epoch Time 334.0544(326.3253), Bit/dim 3.9592(best: 3.9838), Xent 1.3937, Loss 4.6561, Error 0.5073(best: 0.4936)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 45.6499(48.9985) | Bit/dim 3.9630(4.0324) | Xent 1.4511(1.4797) | Loss 12.0623(10.8501) | Error 0.5309(0.5337) Steps 538(535.48) | Grad Norm 11.1452(10.4883) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 51.2431(49.0659) | Bit/dim 3.9797(4.0308) | Xent 1.4135(1.4777) | Loss 10.1610(10.8294) | Error 0.5052(0.5328) Steps 544(535.73) | Grad Norm 13.7291(10.5855) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 46.9553(49.0025) | Bit/dim 3.9743(4.0291) | Xent 1.4806(1.4778) | Loss 10.4168(10.8170) | Error 0.5337(0.5328) Steps 526(535.44) | Grad Norm 13.6958(10.6788) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 49.1062(49.0057) | Bit/dim 3.9903(4.0279) | Xent 1.4587(1.4772) | Loss 10.4869(10.8071) | Error 0.5182(0.5324) Steps 538(535.52) | Grad Norm 10.4025(10.6705) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 51.1705(49.0706) | Bit/dim 3.9634(4.0260) | Xent 1.3985(1.4749) | Loss 10.0960(10.7858) | Error 0.5086(0.5317) Steps 544(535.77) | Grad Norm 8.5883(10.6081) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 52.7966(49.1824) | Bit/dim 3.9724(4.0244) | Xent 1.4438(1.4739) | Loss 10.3043(10.7714) | Error 0.5259(0.5315) Steps 544(536.02) | Grad Norm 11.1936(10.6256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 23.1173, Epoch Time 338.9394(326.7037), Bit/dim 3.9629(best: 3.9592), Xent 1.3454, Loss 4.6356, Error 0.4839(best: 0.4936)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 47.2096(49.1232) | Bit/dim 3.9590(4.0224) | Xent 1.3878(1.4713) | Loss 12.9226(10.8359) | Error 0.5039(0.5307) Steps 544(536.26) | Grad Norm 6.3749(10.4981) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 48.0049(49.0896) | Bit/dim 3.9667(4.0208) | Xent 1.3953(1.4691) | Loss 10.2544(10.8184) | Error 0.4992(0.5297) Steps 526(535.95) | Grad Norm 10.4384(10.4963) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 47.2568(49.0347) | Bit/dim 3.9588(4.0189) | Xent 1.3974(1.4669) | Loss 10.3338(10.8039) | Error 0.5075(0.5291) Steps 550(536.37) | Grad Norm 11.2973(10.5204) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 48.3003(49.0126) | Bit/dim 3.9864(4.0179) | Xent 1.4575(1.4666) | Loss 10.4447(10.7931) | Error 0.5236(0.5289) Steps 544(536.60) | Grad Norm 15.7366(10.6768) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 50.6693(49.0623) | Bit/dim 3.9955(4.0173) | Xent 1.4693(1.4667) | Loss 10.2059(10.7755) | Error 0.5326(0.5290) Steps 538(536.64) | Grad Norm 13.9343(10.7746) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 56.0496(49.2720) | Bit/dim 3.9614(4.0156) | Xent 1.4318(1.4657) | Loss 10.2446(10.7596) | Error 0.5110(0.5285) Steps 544(536.86) | Grad Norm 7.4476(10.6748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 22.5099, Epoch Time 339.8059(327.0968), Bit/dim 3.9577(best: 3.9592), Xent 1.4128, Loss 4.6641, Error 0.5098(best: 0.4839)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 51.0572(49.3255) | Bit/dim 3.9758(4.0144) | Xent 1.4405(1.4649) | Loss 13.2001(10.8328) | Error 0.5114(0.5280) Steps 562(537.62) | Grad Norm 9.6651(10.6445) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 53.1969(49.4417) | Bit/dim 3.9606(4.0128) | Xent 1.4066(1.4632) | Loss 10.2147(10.8143) | Error 0.5075(0.5273) Steps 532(537.45) | Grad Norm 7.9965(10.5650) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 54.4871(49.5930) | Bit/dim 3.9662(4.0114) | Xent 1.3636(1.4602) | Loss 10.0528(10.7914) | Error 0.4850(0.5261) Steps 538(537.46) | Grad Norm 5.8903(10.4248) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 47.6167(49.5337) | Bit/dim 3.9832(4.0105) | Xent 1.3904(1.4581) | Loss 10.2326(10.7746) | Error 0.4944(0.5251) Steps 526(537.12) | Grad Norm 7.0139(10.3225) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 47.1429(49.4620) | Bit/dim 3.9522(4.0088) | Xent 1.3764(1.4556) | Loss 10.1119(10.7548) | Error 0.4985(0.5243) Steps 532(536.97) | Grad Norm 7.2166(10.2293) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 52.2951(49.5470) | Bit/dim 3.9683(4.0076) | Xent 1.3513(1.4525) | Loss 10.1427(10.7364) | Error 0.4838(0.5231) Steps 550(537.36) | Grad Norm 7.8436(10.1577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 22.6192, Epoch Time 347.8627(327.7198), Bit/dim 3.9419(best: 3.9577), Xent 1.3099, Loss 4.5968, Error 0.4671(best: 0.4839)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 46.6546(49.4602) | Bit/dim 3.9491(4.0058) | Xent 1.3843(1.4504) | Loss 12.7418(10.7966) | Error 0.4945(0.5223) Steps 532(537.20) | Grad Norm 6.6055(10.0511) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 47.1036(49.3895) | Bit/dim 3.9610(4.0045) | Xent 1.3673(1.4480) | Loss 9.8627(10.7685) | Error 0.4931(0.5214) Steps 532(537.04) | Grad Norm 7.8346(9.9846) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 52.4689(49.4819) | Bit/dim 3.9522(4.0029) | Xent 1.3571(1.4452) | Loss 10.1214(10.7491) | Error 0.4845(0.5203) Steps 538(537.07) | Grad Norm 7.9825(9.9246) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 50.3761(49.5087) | Bit/dim 3.9490(4.0013) | Xent 1.3419(1.4421) | Loss 10.1458(10.7310) | Error 0.4801(0.5191) Steps 562(537.82) | Grad Norm 6.3758(9.8181) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 54.3765(49.6548) | Bit/dim 3.9332(3.9992) | Xent 1.3911(1.4406) | Loss 10.1990(10.7151) | Error 0.5014(0.5185) Steps 574(538.90) | Grad Norm 12.8227(9.9083) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 49.7793(49.6585) | Bit/dim 3.9423(3.9975) | Xent 1.4275(1.4402) | Loss 9.7612(10.6865) | Error 0.5091(0.5183) Steps 508(537.98) | Grad Norm 15.7997(10.0850) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 22.7416, Epoch Time 342.3228(328.1579), Bit/dim 3.9462(best: 3.9419), Xent 1.3365, Loss 4.6145, Error 0.4851(best: 0.4671)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 53.3687(49.7698) | Bit/dim 3.9331(3.9956) | Xent 1.3953(1.4389) | Loss 13.1285(10.7597) | Error 0.5036(0.5178) Steps 568(538.88) | Grad Norm 13.0047(10.1726) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 51.3599(49.8175) | Bit/dim 3.9406(3.9939) | Xent 1.3566(1.4364) | Loss 9.9669(10.7359) | Error 0.4899(0.5170) Steps 562(539.57) | Grad Norm 9.8425(10.1627) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 48.3567(49.7737) | Bit/dim 3.9379(3.9923) | Xent 1.3473(1.4337) | Loss 10.2106(10.7202) | Error 0.4859(0.5160) Steps 550(539.88) | Grad Norm 9.3793(10.1392) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 52.3455(49.8508) | Bit/dim 3.9561(3.9912) | Xent 1.3409(1.4309) | Loss 10.1071(10.7018) | Error 0.4815(0.5150) Steps 550(540.19) | Grad Norm 8.1558(10.0797) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 50.7106(49.8766) | Bit/dim 3.9320(3.9894) | Xent 1.3286(1.4279) | Loss 9.9423(10.6790) | Error 0.4804(0.5140) Steps 532(539.94) | Grad Norm 4.8324(9.9223) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 51.1340(49.9144) | Bit/dim 3.9405(3.9879) | Xent 1.3267(1.4248) | Loss 9.9489(10.6571) | Error 0.4764(0.5128) Steps 538(539.88) | Grad Norm 4.8649(9.7705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 22.3234, Epoch Time 348.7040(328.7743), Bit/dim 3.9271(best: 3.9419), Xent 1.2693, Loss 4.5618, Error 0.4538(best: 0.4671)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 48.4297(49.8698) | Bit/dim 3.9233(3.9860) | Xent 1.3083(1.4213) | Loss 12.9179(10.7249) | Error 0.4810(0.5119) Steps 544(540.01) | Grad Norm 5.4645(9.6414) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 46.7147(49.7752) | Bit/dim 3.9343(3.9845) | Xent 1.3499(1.4192) | Loss 10.0087(10.7034) | Error 0.4842(0.5111) Steps 526(539.59) | Grad Norm 11.5816(9.6996) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 51.5019(49.8270) | Bit/dim 3.9313(3.9829) | Xent 1.3934(1.4184) | Loss 9.9966(10.6822) | Error 0.4972(0.5106) Steps 532(539.36) | Grad Norm 12.7165(9.7901) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 46.9085(49.7394) | Bit/dim 3.9338(3.9814) | Xent 1.3228(1.4155) | Loss 10.1851(10.6673) | Error 0.4774(0.5096) Steps 544(539.50) | Grad Norm 8.1306(9.7403) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 51.4085(49.7895) | Bit/dim 3.9204(3.9796) | Xent 1.3331(1.4131) | Loss 9.8909(10.6440) | Error 0.4768(0.5087) Steps 568(540.35) | Grad Norm 8.4530(9.7017) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 52.3977(49.8677) | Bit/dim 3.9073(3.9774) | Xent 1.3640(1.4116) | Loss 10.1464(10.6291) | Error 0.4912(0.5081) Steps 562(541.00) | Grad Norm 13.3497(9.8111) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 23.0058, Epoch Time 340.2330(329.1180), Bit/dim 3.9303(best: 3.9271), Xent 1.2987, Loss 4.5797, Error 0.4726(best: 0.4538)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 44.7307(49.7136) | Bit/dim 3.9336(3.9761) | Xent 1.3420(1.4095) | Loss 13.0825(10.7027) | Error 0.4868(0.5075) Steps 526(540.55) | Grad Norm 7.0425(9.7281) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 47.5521(49.6488) | Bit/dim 3.9409(3.9750) | Xent 1.3317(1.4072) | Loss 10.0391(10.6828) | Error 0.4921(0.5070) Steps 520(539.94) | Grad Norm 5.4001(9.5982) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 53.7242(49.7710) | Bit/dim 3.9089(3.9730) | Xent 1.3253(1.4047) | Loss 10.0675(10.6643) | Error 0.4752(0.5061) Steps 550(540.24) | Grad Norm 8.8526(9.5758) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 53.3858(49.8795) | Bit/dim 3.9329(3.9718) | Xent 1.3057(1.4017) | Loss 9.9396(10.6426) | Error 0.4684(0.5049) Steps 538(540.17) | Grad Norm 7.4266(9.5114) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 49.2158(49.8596) | Bit/dim 3.9226(3.9704) | Xent 1.3306(1.3996) | Loss 9.9475(10.6217) | Error 0.4816(0.5042) Steps 538(540.11) | Grad Norm 11.4013(9.5681) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 50.0906(49.8665) | Bit/dim 3.9315(3.9692) | Xent 1.3579(1.3984) | Loss 10.1338(10.6071) | Error 0.4915(0.5039) Steps 562(540.76) | Grad Norm 14.2117(9.7074) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 22.7730, Epoch Time 340.9138(329.4719), Bit/dim 3.9060(best: 3.9271), Xent 1.2938, Loss 4.5529, Error 0.4700(best: 0.4538)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 45.9251(49.7483) | Bit/dim 3.9031(3.9672) | Xent 1.3165(1.3959) | Loss 12.5487(10.6653) | Error 0.4730(0.5029) Steps 544(540.86) | Grad Norm 9.8215(9.7108) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 47.2938(49.6746) | Bit/dim 3.9347(3.9662) | Xent 1.3118(1.3934) | Loss 9.9587(10.6441) | Error 0.4738(0.5021) Steps 532(540.59) | Grad Norm 6.4785(9.6138) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 50.2157(49.6909) | Bit/dim 3.9384(3.9654) | Xent 1.3324(1.3916) | Loss 10.1809(10.6302) | Error 0.4840(0.5015) Steps 544(540.70) | Grad Norm 10.3305(9.6353) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 50.2388(49.7073) | Bit/dim 3.9127(3.9638) | Xent 1.3418(1.3901) | Loss 9.8262(10.6061) | Error 0.4788(0.5008) Steps 514(539.90) | Grad Norm 9.8849(9.6428) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 47.8183(49.6506) | Bit/dim 3.9189(3.9625) | Xent 1.3495(1.3888) | Loss 9.7547(10.5806) | Error 0.4832(0.5003) Steps 508(538.94) | Grad Norm 9.3070(9.6327) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 51.7173(49.7126) | Bit/dim 3.9179(3.9611) | Xent 1.3798(1.3886) | Loss 10.1849(10.5687) | Error 0.5028(0.5004) Steps 514(538.19) | Grad Norm 8.0086(9.5840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 22.5335, Epoch Time 335.0357(329.6388), Bit/dim 3.9038(best: 3.9060), Xent 1.2826, Loss 4.5451, Error 0.4609(best: 0.4538)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 49.3879(49.7029) | Bit/dim 3.9036(3.9594) | Xent 1.3403(1.3871) | Loss 13.1698(10.6467) | Error 0.4740(0.4996) Steps 532(538.00) | Grad Norm 11.2045(9.6326) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 49.9077(49.7090) | Bit/dim 3.9285(3.9585) | Xent 1.5276(1.3913) | Loss 9.9862(10.6269) | Error 0.5448(0.5009) Steps 526(537.64) | Grad Norm 23.1399(10.0379) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 45.1521(49.5723) | Bit/dim 3.9339(3.9577) | Xent 1.3690(1.3907) | Loss 10.0612(10.6100) | Error 0.4998(0.5009) Steps 526(537.30) | Grad Norm 6.9978(9.9467) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 49.9649(49.5841) | Bit/dim 3.9048(3.9562) | Xent 1.3601(1.3897) | Loss 10.0074(10.5919) | Error 0.4955(0.5007) Steps 544(537.50) | Grad Norm 5.3436(9.8086) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 49.3394(49.5768) | Bit/dim 3.9041(3.9546) | Xent 1.3630(1.3889) | Loss 10.0834(10.5766) | Error 0.4855(0.5003) Steps 538(537.51) | Grad Norm 8.9579(9.7830) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 50.9815(49.6189) | Bit/dim 3.9136(3.9534) | Xent 1.3602(1.3881) | Loss 10.0627(10.5612) | Error 0.4864(0.4999) Steps 538(537.53) | Grad Norm 4.8069(9.6338) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 22.4645, Epoch Time 335.3983(329.8116), Bit/dim 3.9196(best: 3.9038), Xent 1.2658, Loss 4.5525, Error 0.4524(best: 0.4538)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 54.2778(49.7587) | Bit/dim 3.9113(3.9521) | Xent 1.3046(1.3856) | Loss 13.0294(10.6353) | Error 0.4673(0.4989) Steps 538(537.54) | Grad Norm 3.8072(9.4590) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 49.8571(49.7616) | Bit/dim 3.9091(3.9508) | Xent 1.2937(1.3828) | Loss 9.8417(10.6115) | Error 0.4619(0.4978) Steps 550(537.91) | Grad Norm 4.0742(9.2974) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 48.7528(49.7314) | Bit/dim 3.9345(3.9503) | Xent 1.3282(1.3812) | Loss 10.0812(10.5955) | Error 0.4761(0.4971) Steps 514(537.20) | Grad Norm 9.3414(9.2987) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 52.4693(49.8135) | Bit/dim 3.9186(3.9494) | Xent 1.3229(1.3794) | Loss 9.9608(10.5765) | Error 0.4749(0.4965) Steps 556(537.76) | Grad Norm 10.8003(9.3438) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 47.7031(49.7502) | Bit/dim 3.9163(3.9484) | Xent 1.3321(1.3780) | Loss 10.0941(10.5620) | Error 0.4764(0.4959) Steps 538(537.77) | Grad Norm 6.8206(9.2681) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 46.8987(49.6646) | Bit/dim 3.8882(3.9466) | Xent 1.3530(1.3773) | Loss 10.1319(10.5491) | Error 0.4864(0.4956) Steps 514(537.05) | Grad Norm 7.4503(9.2136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 22.2038, Epoch Time 340.7804(330.1407), Bit/dim 3.9123(best: 3.9038), Xent 1.2781, Loss 4.5514, Error 0.4553(best: 0.4524)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 47.2834(49.5932) | Bit/dim 3.9151(3.9456) | Xent 1.3397(1.3761) | Loss 13.0893(10.6253) | Error 0.4761(0.4950) Steps 538(537.08) | Grad Norm 6.9170(9.1447) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 46.8275(49.5102) | Bit/dim 3.9122(3.9446) | Xent 1.2733(1.3731) | Loss 9.7856(10.6001) | Error 0.4577(0.4939) Steps 544(537.29) | Grad Norm 7.6587(9.1001) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 50.0109(49.5252) | Bit/dim 3.9177(3.9438) | Xent 1.3318(1.3718) | Loss 9.9908(10.5819) | Error 0.4731(0.4933) Steps 532(537.13) | Grad Norm 14.0512(9.2486) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 49.9155(49.5370) | Bit/dim 3.9261(3.9433) | Xent 1.3242(1.3704) | Loss 10.0024(10.5645) | Error 0.4765(0.4928) Steps 556(537.70) | Grad Norm 12.4267(9.3440) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 49.7428(49.5431) | Bit/dim 3.9208(3.9426) | Xent 1.2943(1.3681) | Loss 10.0704(10.5497) | Error 0.4605(0.4918) Steps 532(537.53) | Grad Norm 8.6168(9.3221) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 48.5786(49.5142) | Bit/dim 3.9085(3.9416) | Xent 1.3008(1.3661) | Loss 9.7801(10.5266) | Error 0.4630(0.4909) Steps 532(537.36) | Grad Norm 6.8692(9.2486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 23.2940, Epoch Time 334.7050(330.2776), Bit/dim 3.9121(best: 3.9038), Xent 1.2768, Loss 4.5506, Error 0.4542(best: 0.4524)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_005_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0 --trust_coefficient 0.005 --clip True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
