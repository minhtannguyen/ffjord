{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run3/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2401 | Time 118.7833(63.5168) | Bit/dim 3.7250(3.7315) | Xent 0.3238(0.4000) | Loss 3.8869(3.9315) | Error 0.1176(0.1464) Steps 700(700.29) | Grad Norm 4.5819(7.9966) | Total Time 14.00(14.00)\n",
      "Iter 2402 | Time 61.7310(63.4632) | Bit/dim 3.7239(3.7313) | Xent 0.3342(0.3980) | Loss 3.8910(3.9303) | Error 0.1195(0.1456) Steps 688(699.92) | Grad Norm 4.2608(7.8845) | Total Time 14.00(14.00)\n",
      "Iter 2403 | Time 59.9781(63.3586) | Bit/dim 3.7234(3.7311) | Xent 0.3126(0.3955) | Loss 3.8797(3.9288) | Error 0.1132(0.1446) Steps 706(700.10) | Grad Norm 2.5015(7.7230) | Total Time 14.00(14.00)\n",
      "Iter 2404 | Time 61.4283(63.3007) | Bit/dim 3.7219(3.7308) | Xent 0.3135(0.3930) | Loss 3.8787(3.9273) | Error 0.1140(0.1437) Steps 700(700.10) | Grad Norm 1.5846(7.5388) | Total Time 14.00(14.00)\n",
      "Iter 2405 | Time 63.1950(63.2976) | Bit/dim 3.7217(3.7305) | Xent 0.3085(0.3905) | Loss 3.8760(3.9258) | Error 0.1166(0.1429) Steps 700(700.09) | Grad Norm 1.9497(7.3712) | Total Time 14.00(14.00)\n",
      "Iter 2406 | Time 62.0959(63.2615) | Bit/dim 3.7135(3.7300) | Xent 0.3257(0.3885) | Loss 3.8764(3.9243) | Error 0.1209(0.1422) Steps 700(700.09) | Grad Norm 2.4343(7.2231) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 45.4642, Epoch Time 483.3879(409.9507), Bit/dim 3.7329(best: inf), Xent 1.9111, Loss 4.6885, Error 0.4332(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 74.3638(63.5946) | Bit/dim 3.7175(3.7296) | Xent 0.3003(0.3859) | Loss 3.8677(3.9226) | Error 0.1111(0.1413) Steps 700(700.09) | Grad Norm 2.7943(7.0902) | Total Time 14.00(14.00)\n",
      "Iter 2408 | Time 69.2651(63.7647) | Bit/dim 3.7362(3.7298) | Xent 0.2975(0.3832) | Loss 3.8850(3.9215) | Error 0.1064(0.1402) Steps 700(700.09) | Grad Norm 2.4706(6.9516) | Total Time 14.00(14.00)\n",
      "Iter 2409 | Time 64.2420(63.7790) | Bit/dim 3.7130(3.7293) | Xent 0.2963(0.3806) | Loss 3.8611(3.9196) | Error 0.1069(0.1392) Steps 706(700.26) | Grad Norm 1.8317(6.7980) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 62.0221(63.7263) | Bit/dim 3.7275(3.7293) | Xent 0.3069(0.3784) | Loss 3.8809(3.9185) | Error 0.1148(0.1385) Steps 718(700.80) | Grad Norm 1.7379(6.6462) | Total Time 14.00(14.00)\n",
      "Iter 2411 | Time 61.9621(63.6734) | Bit/dim 3.7161(3.7289) | Xent 0.3142(0.3765) | Loss 3.8732(3.9171) | Error 0.1184(0.1379) Steps 706(700.95) | Grad Norm 1.8578(6.5026) | Total Time 14.00(14.00)\n",
      "Iter 2412 | Time 64.1732(63.6884) | Bit/dim 3.7231(3.7287) | Xent 0.3162(0.3747) | Loss 3.8812(3.9160) | Error 0.1111(0.1371) Steps 700(700.92) | Grad Norm 2.2794(6.3759) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 25.4503, Epoch Time 437.4644(410.7761), Bit/dim 3.7336(best: 3.7329), Xent 1.9041, Loss 4.6857, Error 0.4227(best: 0.4332)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 60.8425(63.6030) | Bit/dim 3.7264(3.7286) | Xent 0.2942(0.3723) | Loss 3.8735(3.9148) | Error 0.1071(0.1362) Steps 706(701.08) | Grad Norm 1.9211(6.2422) | Total Time 14.00(14.00)\n",
      "Iter 2414 | Time 62.7156(63.5764) | Bit/dim 3.7247(3.7285) | Xent 0.2990(0.3701) | Loss 3.8742(3.9136) | Error 0.1091(0.1354) Steps 700(701.04) | Grad Norm 2.1461(6.1193) | Total Time 14.00(14.00)\n",
      "Iter 2415 | Time 62.3994(63.5411) | Bit/dim 3.7220(3.7283) | Xent 0.3060(0.3681) | Loss 3.8750(3.9124) | Error 0.1120(0.1347) Steps 694(700.83) | Grad Norm 1.8428(5.9910) | Total Time 14.00(14.00)\n",
      "Iter 2416 | Time 62.1440(63.4991) | Bit/dim 3.7115(3.7278) | Xent 0.3080(0.3663) | Loss 3.8655(3.9110) | Error 0.1105(0.1340) Steps 700(700.81) | Grad Norm 0.9194(5.8389) | Total Time 14.00(14.00)\n",
      "Iter 2417 | Time 59.9795(63.3936) | Bit/dim 3.7176(3.7275) | Xent 0.2971(0.3643) | Loss 3.8661(3.9096) | Error 0.1096(0.1332) Steps 706(700.96) | Grad Norm 1.1676(5.6987) | Total Time 14.00(14.00)\n",
      "Iter 2418 | Time 61.8277(63.3466) | Bit/dim 3.7230(3.7274) | Xent 0.3130(0.3627) | Loss 3.8795(3.9087) | Error 0.1131(0.1326) Steps 694(700.75) | Grad Norm 1.1222(5.5615) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 25.3696, Epoch Time 410.7597(410.7756), Bit/dim 3.7319(best: 3.7329), Xent 1.9274, Loss 4.6956, Error 0.4236(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 59.8467(63.2416) | Bit/dim 3.7202(3.7272) | Xent 0.2958(0.3607) | Loss 3.8681(3.9075) | Error 0.1095(0.1319) Steps 700(700.73) | Grad Norm 1.2427(5.4319) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 58.7550(63.1070) | Bit/dim 3.7136(3.7268) | Xent 0.2962(0.3588) | Loss 3.8617(3.9061) | Error 0.1062(0.1312) Steps 694(700.53) | Grad Norm 1.1372(5.3030) | Total Time 14.00(14.00)\n",
      "Iter 2421 | Time 62.3305(63.0837) | Bit/dim 3.7293(3.7268) | Xent 0.2958(0.3569) | Loss 3.8772(3.9053) | Error 0.1091(0.1305) Steps 700(700.51) | Grad Norm 1.2599(5.1818) | Total Time 14.00(14.00)\n",
      "Iter 2422 | Time 62.5046(63.0663) | Bit/dim 3.7158(3.7265) | Xent 0.2972(0.3551) | Loss 3.8644(3.9041) | Error 0.1088(0.1299) Steps 700(700.50) | Grad Norm 1.2632(5.0642) | Total Time 14.00(14.00)\n",
      "Iter 2423 | Time 61.5317(63.0203) | Bit/dim 3.7249(3.7265) | Xent 0.2804(0.3529) | Loss 3.8651(3.9029) | Error 0.0991(0.1289) Steps 706(700.66) | Grad Norm 0.9195(4.9399) | Total Time 14.00(14.00)\n",
      "Iter 2424 | Time 62.8334(63.0147) | Bit/dim 3.7145(3.7261) | Xent 0.2979(0.3512) | Loss 3.8635(3.9017) | Error 0.1082(0.1283) Steps 706(700.82) | Grad Norm 1.1499(4.8262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 25.1674, Epoch Time 408.4862(410.7070), Bit/dim 3.7325(best: 3.7319), Xent 1.9599, Loss 4.7125, Error 0.4265(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 60.1715(62.9294) | Bit/dim 3.7184(3.7259) | Xent 0.2940(0.3495) | Loss 3.8654(3.9006) | Error 0.1072(0.1277) Steps 700(700.80) | Grad Norm 1.6835(4.7319) | Total Time 14.00(14.00)\n",
      "Iter 2426 | Time 60.5976(62.8594) | Bit/dim 3.7162(3.7256) | Xent 0.2850(0.3476) | Loss 3.8587(3.8994) | Error 0.1042(0.1270) Steps 706(700.95) | Grad Norm 1.0975(4.6228) | Total Time 14.00(14.00)\n",
      "Iter 2427 | Time 62.7962(62.8575) | Bit/dim 3.7121(3.7252) | Xent 0.2961(0.3460) | Loss 3.8602(3.8982) | Error 0.1108(0.1265) Steps 700(700.93) | Grad Norm 0.9685(4.5132) | Total Time 14.00(14.00)\n",
      "Iter 2428 | Time 62.7766(62.8551) | Bit/dim 3.7199(3.7250) | Xent 0.2880(0.3443) | Loss 3.8639(3.8972) | Error 0.1046(0.1258) Steps 700(700.90) | Grad Norm 1.0345(4.4089) | Total Time 14.00(14.00)\n",
      "Iter 2429 | Time 61.6790(62.8198) | Bit/dim 3.7198(3.7249) | Xent 0.3019(0.3430) | Loss 3.8708(3.8964) | Error 0.1119(0.1254) Steps 700(700.87) | Grad Norm 1.6565(4.3263) | Total Time 14.00(14.00)\n",
      "Iter 2430 | Time 62.7154(62.8167) | Bit/dim 3.7249(3.7249) | Xent 0.2961(0.3416) | Loss 3.8730(3.8957) | Error 0.1052(0.1248) Steps 706(701.03) | Grad Norm 0.7641(4.2194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 25.4858, Epoch Time 411.7950(410.7396), Bit/dim 3.7317(best: 3.7319), Xent 1.9511, Loss 4.7073, Error 0.4254(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 61.9435(62.7905) | Bit/dim 3.7168(3.7246) | Xent 0.2981(0.3403) | Loss 3.8658(3.8948) | Error 0.1089(0.1243) Steps 700(700.99) | Grad Norm 0.8309(4.1178) | Total Time 14.00(14.00)\n",
      "Iter 2432 | Time 61.5650(62.7537) | Bit/dim 3.7230(3.7246) | Xent 0.3032(0.3392) | Loss 3.8746(3.8942) | Error 0.1110(0.1239) Steps 700(700.96) | Grad Norm 0.9448(4.0226) | Total Time 14.00(14.00)\n",
      "Iter 2433 | Time 62.0591(62.7329) | Bit/dim 3.7116(3.7242) | Xent 0.2913(0.3377) | Loss 3.8573(3.8931) | Error 0.1075(0.1234) Steps 706(701.12) | Grad Norm 1.3556(3.9426) | Total Time 14.00(14.00)\n",
      "Iter 2434 | Time 59.2361(62.6280) | Bit/dim 3.7203(3.7241) | Xent 0.2911(0.3364) | Loss 3.8659(3.8922) | Error 0.1076(0.1230) Steps 694(700.90) | Grad Norm 0.8268(3.8491) | Total Time 14.00(14.00)\n",
      "Iter 2435 | Time 59.2204(62.5258) | Bit/dim 3.7127(3.7237) | Xent 0.2871(0.3349) | Loss 3.8563(3.8912) | Error 0.1019(0.1223) Steps 712(701.24) | Grad Norm 1.3632(3.7745) | Total Time 14.00(14.00)\n",
      "Iter 2436 | Time 62.8697(62.5361) | Bit/dim 3.7267(3.7238) | Xent 0.2816(0.3333) | Loss 3.8675(3.8905) | Error 0.0999(0.1217) Steps 706(701.38) | Grad Norm 0.7619(3.6841) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 25.4334, Epoch Time 407.7561(410.6501), Bit/dim 3.7314(best: 3.7317), Xent 1.9705, Loss 4.7167, Error 0.4293(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 60.5151(62.4754) | Bit/dim 3.7207(3.7237) | Xent 0.2940(0.3321) | Loss 3.8678(3.8898) | Error 0.1098(0.1213) Steps 694(701.16) | Grad Norm 0.8446(3.5989) | Total Time 14.00(14.00)\n",
      "Iter 2438 | Time 63.9591(62.5200) | Bit/dim 3.7133(3.7234) | Xent 0.3015(0.3312) | Loss 3.8640(3.8890) | Error 0.1122(0.1210) Steps 700(701.12) | Grad Norm 0.8170(3.5155) | Total Time 14.00(14.00)\n",
      "Iter 2439 | Time 61.0178(62.4749) | Bit/dim 3.7204(3.7233) | Xent 0.2760(0.3295) | Loss 3.8584(3.8881) | Error 0.1009(0.1204) Steps 688(700.73) | Grad Norm 0.8377(3.4352) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 61.2889(62.4393) | Bit/dim 3.7278(3.7235) | Xent 0.2935(0.3284) | Loss 3.8746(3.8877) | Error 0.1064(0.1200) Steps 700(700.71) | Grad Norm 1.0629(3.3640) | Total Time 14.00(14.00)\n",
      "Iter 2441 | Time 61.7696(62.4192) | Bit/dim 3.7089(3.7230) | Xent 0.2879(0.3272) | Loss 3.8529(3.8866) | Error 0.1011(0.1194) Steps 706(700.87) | Grad Norm 0.8103(3.2874) | Total Time 14.00(14.00)\n",
      "Iter 2442 | Time 59.9808(62.3461) | Bit/dim 3.7237(3.7230) | Xent 0.2967(0.3263) | Loss 3.8721(3.8862) | Error 0.1061(0.1190) Steps 700(700.84) | Grad Norm 0.7771(3.2121) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 25.0683, Epoch Time 409.0819(410.6030), Bit/dim 3.7311(best: 3.7314), Xent 1.9705, Loss 4.7164, Error 0.4258(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 62.6340(62.3547) | Bit/dim 3.7139(3.7228) | Xent 0.2861(0.3251) | Loss 3.8569(3.8853) | Error 0.1035(0.1186) Steps 700(700.81) | Grad Norm 0.8856(3.1423) | Total Time 14.00(14.00)\n",
      "Iter 2444 | Time 61.0810(62.3165) | Bit/dim 3.7245(3.7228) | Xent 0.2787(0.3237) | Loss 3.8638(3.8847) | Error 0.1001(0.1180) Steps 700(700.79) | Grad Norm 1.1455(3.0824) | Total Time 14.00(14.00)\n",
      "Iter 2445 | Time 62.1170(62.3105) | Bit/dim 3.7067(3.7223) | Xent 0.2882(0.3226) | Loss 3.8508(3.8837) | Error 0.1069(0.1177) Steps 694(700.59) | Grad Norm 1.5806(3.0373) | Total Time 14.00(14.00)\n",
      "Iter 2446 | Time 64.4459(62.3746) | Bit/dim 3.7171(3.7222) | Xent 0.2984(0.3219) | Loss 3.8664(3.8831) | Error 0.1111(0.1175) Steps 706(700.75) | Grad Norm 1.0295(2.9771) | Total Time 14.00(14.00)\n",
      "Iter 2447 | Time 61.2295(62.3402) | Bit/dim 3.7369(3.7226) | Xent 0.2845(0.3208) | Loss 3.8791(3.8830) | Error 0.1048(0.1171) Steps 706(700.91) | Grad Norm 0.9198(2.9154) | Total Time 14.00(14.00)\n",
      "Iter 2448 | Time 63.9564(62.3887) | Bit/dim 3.7136(3.7224) | Xent 0.2890(0.3198) | Loss 3.8581(3.8823) | Error 0.1055(0.1168) Steps 706(701.06) | Grad Norm 0.9405(2.8561) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 25.1721, Epoch Time 416.3623(410.7758), Bit/dim 3.7316(best: 3.7311), Xent 1.9760, Loss 4.7196, Error 0.4286(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 63.5063(62.4222) | Bit/dim 3.7210(3.7223) | Xent 0.2771(0.3186) | Loss 3.8595(3.8816) | Error 0.0996(0.1162) Steps 700(701.03) | Grad Norm 1.2147(2.8069) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 61.5653(62.3965) | Bit/dim 3.7267(3.7224) | Xent 0.2861(0.3176) | Loss 3.8697(3.8812) | Error 0.1041(0.1159) Steps 706(701.18) | Grad Norm 1.0336(2.7537) | Total Time 14.00(14.00)\n",
      "Iter 2451 | Time 60.3004(62.3336) | Bit/dim 3.7130(3.7222) | Xent 0.2877(0.3167) | Loss 3.8569(3.8805) | Error 0.1085(0.1157) Steps 694(700.96) | Grad Norm 1.0497(2.7026) | Total Time 14.00(14.00)\n",
      "Iter 2452 | Time 60.3312(62.2736) | Bit/dim 3.7224(3.7222) | Xent 0.2897(0.3159) | Loss 3.8672(3.8801) | Error 0.1015(0.1152) Steps 712(701.29) | Grad Norm 0.7920(2.6452) | Total Time 14.00(14.00)\n",
      "Iter 2453 | Time 63.5148(62.3108) | Bit/dim 3.7106(3.7218) | Xent 0.2819(0.3149) | Loss 3.8515(3.8792) | Error 0.1066(0.1150) Steps 706(701.43) | Grad Norm 0.8510(2.5914) | Total Time 14.00(14.00)\n",
      "Iter 2454 | Time 64.5007(62.3765) | Bit/dim 3.7213(3.7218) | Xent 0.2932(0.3142) | Loss 3.8680(3.8789) | Error 0.1045(0.1147) Steps 694(701.21) | Grad Norm 1.0347(2.5447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 25.2415, Epoch Time 414.2044(410.8787), Bit/dim 3.7326(best: 3.7311), Xent 1.9979, Loss 4.7315, Error 0.4285(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 64.3807(62.4366) | Bit/dim 3.7172(3.7217) | Xent 0.2886(0.3134) | Loss 3.8616(3.8784) | Error 0.1036(0.1143) Steps 718(701.71) | Grad Norm 1.0962(2.5013) | Total Time 14.00(14.00)\n",
      "Iter 2456 | Time 61.3351(62.4036) | Bit/dim 3.7194(3.7216) | Xent 0.2947(0.3129) | Loss 3.8667(3.8780) | Error 0.1046(0.1140) Steps 700(701.66) | Grad Norm 1.0757(2.4585) | Total Time 14.00(14.00)\n",
      "Iter 2457 | Time 63.9076(62.4487) | Bit/dim 3.7136(3.7214) | Xent 0.2875(0.3121) | Loss 3.8574(3.8774) | Error 0.1019(0.1137) Steps 706(701.79) | Grad Norm 0.9431(2.4130) | Total Time 14.00(14.00)\n",
      "Iter 2458 | Time 63.0848(62.4678) | Bit/dim 3.7142(3.7211) | Xent 0.2770(0.3111) | Loss 3.8527(3.8767) | Error 0.1028(0.1133) Steps 706(701.92) | Grad Norm 0.8288(2.3655) | Total Time 14.00(14.00)\n",
      "Iter 2459 | Time 62.4129(62.4661) | Bit/dim 3.7224(3.7212) | Xent 0.2813(0.3102) | Loss 3.8630(3.8763) | Error 0.1002(0.1130) Steps 700(701.86) | Grad Norm 1.4488(2.3380) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 62.0406(62.4534) | Bit/dim 3.7233(3.7212) | Xent 0.2952(0.3097) | Loss 3.8709(3.8761) | Error 0.1039(0.1127) Steps 706(701.99) | Grad Norm 1.0153(2.2983) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 25.5409, Epoch Time 418.1439(411.0966), Bit/dim 3.7310(best: 3.7311), Xent 2.0097, Loss 4.7359, Error 0.4281(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 63.4254(62.4825) | Bit/dim 3.7205(3.7212) | Xent 0.2865(0.3090) | Loss 3.8638(3.8757) | Error 0.1029(0.1124) Steps 700(701.93) | Grad Norm 0.7915(2.2531) | Total Time 14.00(14.00)\n",
      "Iter 2462 | Time 62.3043(62.4772) | Bit/dim 3.7204(3.7212) | Xent 0.2875(0.3084) | Loss 3.8641(3.8754) | Error 0.1041(0.1121) Steps 700(701.87) | Grad Norm 0.7593(2.2083) | Total Time 14.00(14.00)\n",
      "Iter 2463 | Time 61.9782(62.4622) | Bit/dim 3.7180(3.7211) | Xent 0.2873(0.3077) | Loss 3.8617(3.8750) | Error 0.1096(0.1121) Steps 700(701.81) | Grad Norm 1.0096(2.1723) | Total Time 14.00(14.00)\n",
      "Iter 2464 | Time 64.3130(62.5177) | Bit/dim 3.7086(3.7207) | Xent 0.2853(0.3071) | Loss 3.8512(3.8743) | Error 0.1009(0.1117) Steps 694(701.58) | Grad Norm 0.9357(2.1352) | Total Time 14.00(14.00)\n",
      "Iter 2465 | Time 62.9767(62.5315) | Bit/dim 3.7179(3.7206) | Xent 0.2791(0.3062) | Loss 3.8575(3.8738) | Error 0.1019(0.1114) Steps 700(701.53) | Grad Norm 1.0357(2.1023) | Total Time 14.00(14.00)\n",
      "Iter 2466 | Time 62.0630(62.5175) | Bit/dim 3.7272(3.7208) | Xent 0.2752(0.3053) | Loss 3.8648(3.8735) | Error 0.0996(0.1111) Steps 700(701.48) | Grad Norm 1.0128(2.0696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 25.5109, Epoch Time 418.3257(411.3135), Bit/dim 3.7321(best: 3.7310), Xent 1.9984, Loss 4.7314, Error 0.4275(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 60.9856(62.4715) | Bit/dim 3.7210(3.7208) | Xent 0.2884(0.3048) | Loss 3.8652(3.8732) | Error 0.1050(0.1109) Steps 694(701.26) | Grad Norm 1.1069(2.0407) | Total Time 14.00(14.00)\n",
      "Iter 2468 | Time 61.1342(62.4314) | Bit/dim 3.7117(3.7206) | Xent 0.2816(0.3041) | Loss 3.8525(3.8726) | Error 0.0998(0.1106) Steps 718(701.76) | Grad Norm 1.4808(2.0239) | Total Time 14.00(14.00)\n",
      "Iter 2469 | Time 61.4149(62.4009) | Bit/dim 3.7253(3.7207) | Xent 0.2787(0.3033) | Loss 3.8646(3.8724) | Error 0.1024(0.1103) Steps 700(701.71) | Grad Norm 0.8168(1.9877) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 61.5816(62.3763) | Bit/dim 3.7144(3.7205) | Xent 0.2845(0.3028) | Loss 3.8566(3.8719) | Error 0.1064(0.1102) Steps 700(701.66) | Grad Norm 0.9583(1.9568) | Total Time 14.00(14.00)\n",
      "Iter 2471 | Time 63.4013(62.4071) | Bit/dim 3.7149(3.7204) | Xent 0.2902(0.3024) | Loss 3.8600(3.8716) | Error 0.1081(0.1101) Steps 700(701.61) | Grad Norm 1.3122(1.9375) | Total Time 14.00(14.00)\n",
      "Iter 2472 | Time 62.8318(62.4198) | Bit/dim 3.7168(3.7202) | Xent 0.2912(0.3021) | Loss 3.8624(3.8713) | Error 0.1066(0.1100) Steps 706(701.74) | Grad Norm 1.2285(1.9162) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 25.3745, Epoch Time 412.3903(411.3458), Bit/dim 3.7313(best: 3.7310), Xent 2.0214, Loss 4.7420, Error 0.4283(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 60.9416(62.3755) | Bit/dim 3.7186(3.7202) | Xent 0.2793(0.3014) | Loss 3.8583(3.8709) | Error 0.1029(0.1098) Steps 700(701.69) | Grad Norm 1.0382(1.8899) | Total Time 14.00(14.00)\n",
      "Iter 2474 | Time 60.7900(62.3279) | Bit/dim 3.7168(3.7201) | Xent 0.2920(0.3011) | Loss 3.8628(3.8706) | Error 0.1059(0.1097) Steps 706(701.82) | Grad Norm 0.8618(1.8590) | Total Time 14.00(14.00)\n",
      "Iter 2475 | Time 63.6315(62.3670) | Bit/dim 3.7131(3.7199) | Xent 0.2797(0.3005) | Loss 3.8529(3.8701) | Error 0.0975(0.1093) Steps 706(701.94) | Grad Norm 1.2260(1.8400) | Total Time 14.00(14.00)\n",
      "Iter 2476 | Time 59.9671(62.2950) | Bit/dim 3.7204(3.7199) | Xent 0.2683(0.2995) | Loss 3.8546(3.8696) | Error 0.0982(0.1090) Steps 700(701.88) | Grad Norm 1.2488(1.8223) | Total Time 14.00(14.00)\n",
      "Iter 2477 | Time 61.5061(62.2713) | Bit/dim 3.7220(3.7200) | Xent 0.2777(0.2988) | Loss 3.8609(3.8694) | Error 0.1021(0.1088) Steps 700(701.83) | Grad Norm 0.9227(1.7953) | Total Time 14.00(14.00)\n",
      "Iter 2478 | Time 61.7630(62.2561) | Bit/dim 3.7209(3.7200) | Xent 0.2714(0.2980) | Loss 3.8566(3.8690) | Error 0.0992(0.1085) Steps 694(701.59) | Grad Norm 1.0298(1.7723) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 25.3331, Epoch Time 409.3231(411.2851), Bit/dim 3.7315(best: 3.7310), Xent 2.0128, Loss 4.7379, Error 0.4258(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2479 | Time 62.9408(62.2766) | Bit/dim 3.7130(3.7198) | Xent 0.2784(0.2974) | Loss 3.8522(3.8685) | Error 0.1014(0.1083) Steps 700(701.55) | Grad Norm 1.1336(1.7532) | Total Time 14.00(14.00)\n",
      "Iter 2480 | Time 60.3936(62.2201) | Bit/dim 3.7176(3.7197) | Xent 0.2681(0.2965) | Loss 3.8517(3.8680) | Error 0.1000(0.1080) Steps 706(701.68) | Grad Norm 1.4775(1.7449) | Total Time 14.00(14.00)\n",
      "Iter 2481 | Time 62.6495(62.2330) | Bit/dim 3.7192(3.7197) | Xent 0.2571(0.2954) | Loss 3.8478(3.8674) | Error 0.0939(0.1076) Steps 700(701.63) | Grad Norm 0.9758(1.7218) | Total Time 14.00(14.00)\n",
      "Iter 2482 | Time 63.1611(62.2609) | Bit/dim 3.7274(3.7199) | Xent 0.2818(0.2950) | Loss 3.8683(3.8674) | Error 0.1052(0.1075) Steps 700(701.58) | Grad Norm 1.2803(1.7086) | Total Time 14.00(14.00)\n",
      "Iter 2483 | Time 62.1423(62.2573) | Bit/dim 3.7171(3.7198) | Xent 0.2817(0.2946) | Loss 3.8580(3.8671) | Error 0.1039(0.1074) Steps 700(701.53) | Grad Norm 1.6853(1.7079) | Total Time 14.00(14.00)\n",
      "Iter 2484 | Time 60.8859(62.2162) | Bit/dim 3.7157(3.7197) | Xent 0.2793(0.2941) | Loss 3.8553(3.8668) | Error 0.1035(0.1073) Steps 694(701.31) | Grad Norm 1.7232(1.7083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 25.2002, Epoch Time 412.6713(411.3267), Bit/dim 3.7326(best: 3.7310), Xent 2.0167, Loss 4.7409, Error 0.4270(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2485 | Time 62.5965(62.2276) | Bit/dim 3.7143(3.7196) | Xent 0.2768(0.2936) | Loss 3.8527(3.8664) | Error 0.1000(0.1071) Steps 718(701.81) | Grad Norm 1.1480(1.6915) | Total Time 14.00(14.00)\n",
      "Iter 2486 | Time 63.4492(62.2642) | Bit/dim 3.7175(3.7195) | Xent 0.2871(0.2934) | Loss 3.8611(3.8662) | Error 0.1059(0.1071) Steps 694(701.57) | Grad Norm 1.5017(1.6858) | Total Time 14.00(14.00)\n",
      "Iter 2487 | Time 63.7894(62.3100) | Bit/dim 3.7208(3.7195) | Xent 0.2795(0.2930) | Loss 3.8606(3.8660) | Error 0.1006(0.1069) Steps 700(701.53) | Grad Norm 1.0593(1.6670) | Total Time 14.00(14.00)\n",
      "Iter 2488 | Time 61.0047(62.2708) | Bit/dim 3.7263(3.7197) | Xent 0.2769(0.2925) | Loss 3.8648(3.8660) | Error 0.1020(0.1067) Steps 706(701.66) | Grad Norm 1.2527(1.6546) | Total Time 14.00(14.00)\n",
      "Iter 2489 | Time 60.5366(62.2188) | Bit/dim 3.7139(3.7196) | Xent 0.2716(0.2919) | Loss 3.8497(3.8655) | Error 0.0991(0.1065) Steps 700(701.61) | Grad Norm 1.6982(1.6559) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 61.3959(62.1941) | Bit/dim 3.7142(3.7194) | Xent 0.2719(0.2913) | Loss 3.8501(3.8650) | Error 0.0995(0.1063) Steps 694(701.38) | Grad Norm 0.9535(1.6348) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 25.6133, Epoch Time 414.3427(411.4172), Bit/dim 3.7318(best: 3.7310), Xent 2.0284, Loss 4.7460, Error 0.4268(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2491 | Time 59.5705(62.1154) | Bit/dim 3.7240(3.7195) | Xent 0.2837(0.2910) | Loss 3.8658(3.8651) | Error 0.1041(0.1062) Steps 706(701.52) | Grad Norm 1.3437(1.6261) | Total Time 14.00(14.00)\n",
      "Iter 2492 | Time 61.5640(62.0988) | Bit/dim 3.7107(3.7193) | Xent 0.2786(0.2907) | Loss 3.8500(3.8646) | Error 0.1024(0.1061) Steps 706(701.65) | Grad Norm 1.0053(1.6075) | Total Time 14.00(14.00)\n",
      "Iter 2493 | Time 61.9979(62.0958) | Bit/dim 3.7154(3.7192) | Xent 0.2745(0.2902) | Loss 3.8527(3.8643) | Error 0.0974(0.1058) Steps 700(701.61) | Grad Norm 0.8060(1.5834) | Total Time 14.00(14.00)\n",
      "Iter 2494 | Time 61.1754(62.0682) | Bit/dim 3.7131(3.7190) | Xent 0.2736(0.2897) | Loss 3.8499(3.8638) | Error 0.0955(0.1055) Steps 688(701.20) | Grad Norm 0.8559(1.5616) | Total Time 14.00(14.00)\n",
      "Iter 2495 | Time 62.9132(62.0936) | Bit/dim 3.7203(3.7190) | Xent 0.2835(0.2895) | Loss 3.8620(3.8638) | Error 0.1028(0.1054) Steps 700(701.16) | Grad Norm 1.8911(1.5715) | Total Time 14.00(14.00)\n",
      "Iter 2496 | Time 60.1061(62.0339) | Bit/dim 3.7183(3.7190) | Xent 0.2860(0.2894) | Loss 3.8613(3.8637) | Error 0.1030(0.1054) Steps 694(700.95) | Grad Norm 1.3484(1.5648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 25.4972, Epoch Time 408.1691(411.3198), Bit/dim 3.7311(best: 3.7310), Xent 2.0265, Loss 4.7443, Error 0.4240(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2497 | Time 61.9894(62.0326) | Bit/dim 3.7148(3.7189) | Xent 0.2753(0.2890) | Loss 3.8525(3.8634) | Error 0.1001(0.1052) Steps 694(700.74) | Grad Norm 1.3596(1.5587) | Total Time 14.00(14.00)\n",
      "Iter 2498 | Time 63.7185(62.0832) | Bit/dim 3.7175(3.7188) | Xent 0.2722(0.2885) | Loss 3.8536(3.8631) | Error 0.0989(0.1050) Steps 700(700.72) | Grad Norm 1.3870(1.5535) | Total Time 14.00(14.00)\n",
      "Iter 2499 | Time 61.9112(62.0780) | Bit/dim 3.7297(3.7192) | Xent 0.2734(0.2880) | Loss 3.8664(3.8632) | Error 0.1009(0.1049) Steps 700(700.69) | Grad Norm 1.4209(1.5495) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 60.4906(62.0304) | Bit/dim 3.7163(3.7191) | Xent 0.2754(0.2876) | Loss 3.8540(3.8629) | Error 0.1018(0.1048) Steps 706(700.85) | Grad Norm 2.3207(1.5727) | Total Time 14.00(14.00)\n",
      "Iter 2501 | Time 61.7277(62.0213) | Bit/dim 3.7185(3.7191) | Xent 0.2783(0.2874) | Loss 3.8576(3.8627) | Error 0.1024(0.1047) Steps 718(701.37) | Grad Norm 1.4889(1.5701) | Total Time 14.00(14.00)\n",
      "Iter 2502 | Time 60.8773(61.9870) | Bit/dim 3.7056(3.7187) | Xent 0.2894(0.2874) | Loss 3.8503(3.8624) | Error 0.1042(0.1047) Steps 700(701.33) | Grad Norm 1.3587(1.5638) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 25.2920, Epoch Time 411.7177(411.3317), Bit/dim 3.7309(best: 3.7310), Xent 2.0317, Loss 4.7467, Error 0.4234(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2503 | Time 61.3389(61.9675) | Bit/dim 3.7173(3.7186) | Xent 0.2659(0.2868) | Loss 3.8502(3.8620) | Error 0.0994(0.1046) Steps 706(701.47) | Grad Norm 1.0791(1.5493) | Total Time 14.00(14.00)\n",
      "Iter 2504 | Time 63.8006(62.0225) | Bit/dim 3.7208(3.7187) | Xent 0.2755(0.2864) | Loss 3.8586(3.8619) | Error 0.1018(0.1045) Steps 700(701.42) | Grad Norm 1.1423(1.5371) | Total Time 14.00(14.00)\n",
      "Iter 2505 | Time 59.4166(61.9444) | Bit/dim 3.7188(3.7187) | Xent 0.2789(0.2862) | Loss 3.8582(3.8618) | Error 0.0996(0.1043) Steps 718(701.92) | Grad Norm 1.6940(1.5418) | Total Time 14.00(14.00)\n",
      "Iter 2506 | Time 62.4050(61.9582) | Bit/dim 3.7220(3.7188) | Xent 0.2686(0.2857) | Loss 3.8563(3.8616) | Error 0.0985(0.1042) Steps 706(702.04) | Grad Norm 1.5120(1.5409) | Total Time 14.00(14.00)\n",
      "Iter 2507 | Time 59.7138(61.8908) | Bit/dim 3.7103(3.7185) | Xent 0.2765(0.2854) | Loss 3.8485(3.8612) | Error 0.1009(0.1041) Steps 706(702.16) | Grad Norm 1.7194(1.5462) | Total Time 14.00(14.00)\n",
      "Iter 2508 | Time 61.2975(61.8730) | Bit/dim 3.7170(3.7185) | Xent 0.2845(0.2854) | Loss 3.8593(3.8612) | Error 0.1058(0.1041) Steps 706(702.28) | Grad Norm 0.7921(1.5236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 25.1328, Epoch Time 408.9218(411.2594), Bit/dim 3.7311(best: 3.7309), Xent 2.0478, Loss 4.7550, Error 0.4267(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2509 | Time 62.5327(61.8928) | Bit/dim 3.7093(3.7182) | Xent 0.2843(0.2853) | Loss 3.8514(3.8609) | Error 0.1025(0.1041) Steps 700(702.21) | Grad Norm 1.4858(1.5225) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 59.6967(61.8270) | Bit/dim 3.7207(3.7183) | Xent 0.2679(0.2848) | Loss 3.8546(3.8607) | Error 0.0996(0.1039) Steps 700(702.14) | Grad Norm 1.3495(1.5173) | Total Time 14.00(14.00)\n",
      "Iter 2511 | Time 61.8926(61.8289) | Bit/dim 3.7248(3.7185) | Xent 0.2718(0.2844) | Loss 3.8608(3.8607) | Error 0.0984(0.1038) Steps 718(702.62) | Grad Norm 1.6286(1.5206) | Total Time 14.00(14.00)\n",
      "Iter 2512 | Time 61.4689(61.8181) | Bit/dim 3.7147(3.7184) | Xent 0.2829(0.2844) | Loss 3.8562(3.8606) | Error 0.1060(0.1038) Steps 700(702.54) | Grad Norm 0.9668(1.5040) | Total Time 14.00(14.00)\n",
      "Iter 2513 | Time 61.1747(61.7988) | Bit/dim 3.7238(3.7185) | Xent 0.2759(0.2841) | Loss 3.8618(3.8606) | Error 0.1010(0.1037) Steps 700(702.46) | Grad Norm 1.0499(1.4904) | Total Time 14.00(14.00)\n",
      "Iter 2514 | Time 60.7235(61.7666) | Bit/dim 3.7142(3.7184) | Xent 0.2749(0.2839) | Loss 3.8517(3.8603) | Error 0.0984(0.1036) Steps 706(702.57) | Grad Norm 0.9838(1.4752) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 25.1552, Epoch Time 408.5158(411.1771), Bit/dim 3.7310(best: 3.7309), Xent 2.0366, Loss 4.7494, Error 0.4257(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2515 | Time 62.0228(61.7742) | Bit/dim 3.7240(3.7186) | Xent 0.2774(0.2837) | Loss 3.8627(3.8604) | Error 0.1001(0.1035) Steps 700(702.49) | Grad Norm 0.9860(1.4605) | Total Time 14.00(14.00)\n",
      "Iter 2516 | Time 61.2889(61.7597) | Bit/dim 3.7253(3.7188) | Xent 0.2700(0.2832) | Loss 3.8604(3.8604) | Error 0.0952(0.1032) Steps 712(702.78) | Grad Norm 1.4025(1.4588) | Total Time 14.00(14.00)\n",
      "Iter 2517 | Time 61.1916(61.7426) | Bit/dim 3.7291(3.7191) | Xent 0.2660(0.2827) | Loss 3.8621(3.8604) | Error 0.0939(0.1030) Steps 706(702.87) | Grad Norm 1.0217(1.4457) | Total Time 14.00(14.00)\n",
      "Iter 2518 | Time 60.7236(61.7121) | Bit/dim 3.7172(3.7190) | Xent 0.2716(0.2824) | Loss 3.8530(3.8602) | Error 0.0966(0.1028) Steps 706(702.97) | Grad Norm 0.9788(1.4317) | Total Time 14.00(14.00)\n",
      "Iter 2519 | Time 62.7405(61.7429) | Bit/dim 3.7117(3.7188) | Xent 0.2690(0.2820) | Loss 3.8462(3.8598) | Error 0.0975(0.1026) Steps 700(702.88) | Grad Norm 1.4324(1.4317) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 60.4824(61.7051) | Bit/dim 3.6981(3.7182) | Xent 0.2758(0.2818) | Loss 3.8360(3.8591) | Error 0.1018(0.1026) Steps 700(702.79) | Grad Norm 1.2100(1.4250) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 25.3144, Epoch Time 409.1625(411.1166), Bit/dim 3.7326(best: 3.7309), Xent 2.0303, Loss 4.7477, Error 0.4258(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2521 | Time 62.3445(61.7243) | Bit/dim 3.7179(3.7182) | Xent 0.2692(0.2814) | Loss 3.8525(3.8589) | Error 0.0971(0.1024) Steps 712(703.07) | Grad Norm 0.9192(1.4099) | Total Time 14.00(14.00)\n",
      "Iter 2522 | Time 60.5616(61.6894) | Bit/dim 3.7182(3.7182) | Xent 0.2823(0.2815) | Loss 3.8593(3.8589) | Error 0.1054(0.1025) Steps 706(703.16) | Grad Norm 1.0806(1.4000) | Total Time 14.00(14.00)\n",
      "Iter 2523 | Time 63.9016(61.7558) | Bit/dim 3.7178(3.7182) | Xent 0.2758(0.2813) | Loss 3.8557(3.8588) | Error 0.1010(0.1025) Steps 694(702.88) | Grad Norm 1.0173(1.3885) | Total Time 14.00(14.00)\n",
      "Iter 2524 | Time 62.3331(61.7731) | Bit/dim 3.7115(3.7180) | Xent 0.2737(0.2811) | Loss 3.8484(3.8585) | Error 0.1000(0.1024) Steps 700(702.80) | Grad Norm 1.6338(1.3959) | Total Time 14.00(14.00)\n",
      "Iter 2525 | Time 63.3919(61.8217) | Bit/dim 3.7246(3.7182) | Xent 0.2542(0.2803) | Loss 3.8517(3.8583) | Error 0.0946(0.1022) Steps 706(702.89) | Grad Norm 1.3460(1.3944) | Total Time 14.00(14.00)\n",
      "Iter 2526 | Time 66.4405(61.9602) | Bit/dim 3.7154(3.7181) | Xent 0.2771(0.2802) | Loss 3.8540(3.8582) | Error 0.0988(0.1020) Steps 706(702.98) | Grad Norm 0.9013(1.3796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 25.3170, Epoch Time 420.0852(411.3857), Bit/dim 3.7313(best: 3.7309), Xent 2.0666, Loss 4.7646, Error 0.4257(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2527 | Time 60.0310(61.9024) | Bit/dim 3.7281(3.7184) | Xent 0.2651(0.2797) | Loss 3.8607(3.8582) | Error 0.0968(0.1019) Steps 706(703.08) | Grad Norm 1.0390(1.3693) | Total Time 14.00(14.00)\n",
      "Iter 2528 | Time 61.9010(61.9023) | Bit/dim 3.7043(3.7180) | Xent 0.2860(0.2799) | Loss 3.8473(3.8579) | Error 0.0985(0.1018) Steps 694(702.80) | Grad Norm 1.2030(1.3644) | Total Time 14.00(14.00)\n",
      "Iter 2529 | Time 62.5285(61.9211) | Bit/dim 3.7292(3.7183) | Xent 0.2595(0.2793) | Loss 3.8589(3.8579) | Error 0.0954(0.1016) Steps 700(702.72) | Grad Norm 1.0963(1.3563) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 61.2998(61.9025) | Bit/dim 3.7181(3.7183) | Xent 0.2748(0.2792) | Loss 3.8555(3.8579) | Error 0.1019(0.1016) Steps 700(702.64) | Grad Norm 0.8839(1.3421) | Total Time 14.00(14.00)\n",
      "Iter 2531 | Time 62.3924(61.9172) | Bit/dim 3.7070(3.7179) | Xent 0.2768(0.2791) | Loss 3.8454(3.8575) | Error 0.1004(0.1016) Steps 700(702.56) | Grad Norm 1.4415(1.3451) | Total Time 14.00(14.00)\n",
      "Iter 2532 | Time 60.7603(61.8825) | Bit/dim 3.7182(3.7180) | Xent 0.2724(0.2789) | Loss 3.8544(3.8574) | Error 0.0975(0.1014) Steps 700(702.48) | Grad Norm 0.7696(1.3279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 25.1241, Epoch Time 409.8405(411.3393), Bit/dim 3.7317(best: 3.7309), Xent 2.0655, Loss 4.7645, Error 0.4300(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2533 | Time 61.1451(61.8603) | Bit/dim 3.7197(3.7180) | Xent 0.2669(0.2785) | Loss 3.8531(3.8573) | Error 0.0946(0.1012) Steps 694(702.23) | Grad Norm 0.7767(1.3113) | Total Time 14.00(14.00)\n",
      "Iter 2534 | Time 61.4412(61.8478) | Bit/dim 3.7113(3.7178) | Xent 0.2680(0.2782) | Loss 3.8453(3.8569) | Error 0.0989(0.1012) Steps 706(702.34) | Grad Norm 1.0786(1.3043) | Total Time 14.00(14.00)\n",
      "Iter 2535 | Time 62.1354(61.8564) | Bit/dim 3.7156(3.7177) | Xent 0.2830(0.2783) | Loss 3.8571(3.8569) | Error 0.0996(0.1011) Steps 700(702.27) | Grad Norm 1.6495(1.3147) | Total Time 14.00(14.00)\n",
      "Iter 2536 | Time 60.8445(61.8260) | Bit/dim 3.7273(3.7180) | Xent 0.2813(0.2784) | Loss 3.8680(3.8572) | Error 0.1022(0.1012) Steps 706(702.38) | Grad Norm 0.9144(1.3027) | Total Time 14.00(14.00)\n",
      "Iter 2537 | Time 59.9118(61.7686) | Bit/dim 3.7280(3.7183) | Xent 0.2686(0.2781) | Loss 3.8623(3.8574) | Error 0.0992(0.1011) Steps 712(702.67) | Grad Norm 1.5515(1.3102) | Total Time 14.00(14.00)\n",
      "Iter 2538 | Time 61.7863(61.7691) | Bit/dim 3.7027(3.7179) | Xent 0.2694(0.2779) | Loss 3.8374(3.8568) | Error 0.0999(0.1011) Steps 730(703.49) | Grad Norm 0.9342(1.2989) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 25.2697, Epoch Time 408.4130(411.2516), Bit/dim 3.7304(best: 3.7309), Xent 2.0756, Loss 4.7681, Error 0.4324(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2539 | Time 61.7602(61.7689) | Bit/dim 3.7011(3.7174) | Xent 0.2716(0.2777) | Loss 3.8369(3.8562) | Error 0.1008(0.1011) Steps 712(703.75) | Grad Norm 1.0087(1.2902) | Total Time 14.00(14.00)\n",
      "Iter 2540 | Time 60.8856(61.7424) | Bit/dim 3.7246(3.7176) | Xent 0.2780(0.2777) | Loss 3.8636(3.8564) | Error 0.0995(0.1010) Steps 694(703.45) | Grad Norm 1.5089(1.2967) | Total Time 14.00(14.00)\n",
      "Iter 2541 | Time 61.8004(61.7441) | Bit/dim 3.7106(3.7174) | Xent 0.2673(0.2774) | Loss 3.8442(3.8561) | Error 0.0996(0.1010) Steps 706(703.53) | Grad Norm 1.1864(1.2934) | Total Time 14.00(14.00)\n",
      "Iter 2542 | Time 60.1985(61.6977) | Bit/dim 3.7206(3.7175) | Xent 0.2521(0.2766) | Loss 3.8467(3.8558) | Error 0.0891(0.1006) Steps 694(703.24) | Grad Norm 0.8996(1.2816) | Total Time 14.00(14.00)\n",
      "Iter 2543 | Time 60.5909(61.6645) | Bit/dim 3.7212(3.7176) | Xent 0.2678(0.2764) | Loss 3.8551(3.8558) | Error 0.1018(0.1006) Steps 700(703.15) | Grad Norm 1.1550(1.2778) | Total Time 14.00(14.00)\n",
      "Iter 2544 | Time 59.8056(61.6088) | Bit/dim 3.7173(3.7176) | Xent 0.2734(0.2763) | Loss 3.8540(3.8557) | Error 0.1009(0.1007) Steps 694(702.87) | Grad Norm 1.4642(1.2834) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 25.2829, Epoch Time 405.8510(411.0895), Bit/dim 3.7320(best: 3.7304), Xent 2.0691, Loss 4.7666, Error 0.4307(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2545 | Time 62.4923(61.6353) | Bit/dim 3.7098(3.7173) | Xent 0.2705(0.2761) | Loss 3.8451(3.8554) | Error 0.0988(0.1006) Steps 700(702.79) | Grad Norm 0.8915(1.2716) | Total Time 14.00(14.00)\n",
      "Iter 2546 | Time 61.0923(61.6190) | Bit/dim 3.7217(3.7175) | Xent 0.2758(0.2761) | Loss 3.8597(3.8555) | Error 0.1054(0.1007) Steps 706(702.88) | Grad Norm 1.1234(1.2672) | Total Time 14.00(14.00)\n",
      "Iter 2547 | Time 63.1494(61.6649) | Bit/dim 3.7204(3.7176) | Xent 0.2621(0.2757) | Loss 3.8515(3.8554) | Error 0.0941(0.1005) Steps 700(702.80) | Grad Norm 0.8670(1.2552) | Total Time 14.00(14.00)\n",
      "Iter 2548 | Time 66.3132(61.8043) | Bit/dim 3.7189(3.7176) | Xent 0.2641(0.2753) | Loss 3.8510(3.8553) | Error 0.0949(0.1004) Steps 700(702.71) | Grad Norm 1.3337(1.2575) | Total Time 14.00(14.00)\n",
      "Iter 2549 | Time 60.9506(61.7787) | Bit/dim 3.7209(3.7177) | Xent 0.2617(0.2749) | Loss 3.8517(3.8552) | Error 0.0961(0.1002) Steps 694(702.45) | Grad Norm 1.4030(1.2619) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 61.8211(61.7800) | Bit/dim 3.7099(3.7175) | Xent 0.2825(0.2751) | Loss 3.8512(3.8550) | Error 0.0991(0.1002) Steps 694(702.20) | Grad Norm 1.1208(1.2577) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 25.0234, Epoch Time 416.2079(411.2431), Bit/dim 3.7314(best: 3.7304), Xent 2.0664, Loss 4.7646, Error 0.4249(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2551 | Time 60.7282(61.7485) | Bit/dim 3.7130(3.7173) | Xent 0.2679(0.2749) | Loss 3.8470(3.8548) | Error 0.0990(0.1002) Steps 712(702.49) | Grad Norm 0.8893(1.2466) | Total Time 14.00(14.00)\n",
      "Iter 2552 | Time 61.2343(61.7330) | Bit/dim 3.7195(3.7174) | Xent 0.2703(0.2748) | Loss 3.8546(3.8548) | Error 0.0974(0.1001) Steps 694(702.24) | Grad Norm 1.0290(1.2401) | Total Time 14.00(14.00)\n",
      "Iter 2553 | Time 59.2379(61.6582) | Bit/dim 3.7121(3.7172) | Xent 0.2719(0.2747) | Loss 3.8480(3.8546) | Error 0.1000(0.1001) Steps 706(702.35) | Grad Norm 0.8774(1.2292) | Total Time 14.00(14.00)\n",
      "Iter 2554 | Time 62.1653(61.6734) | Bit/dim 3.7138(3.7171) | Xent 0.2754(0.2747) | Loss 3.8515(3.8545) | Error 0.1005(0.1001) Steps 706(702.46) | Grad Norm 1.0958(1.2252) | Total Time 14.00(14.00)\n",
      "Iter 2555 | Time 61.9711(61.6823) | Bit/dim 3.7250(3.7174) | Xent 0.2634(0.2744) | Loss 3.8568(3.8546) | Error 0.0960(0.1000) Steps 706(702.57) | Grad Norm 0.9170(1.2160) | Total Time 14.00(14.00)\n",
      "Iter 2556 | Time 62.8799(61.7182) | Bit/dim 3.7153(3.7173) | Xent 0.2485(0.2736) | Loss 3.8395(3.8541) | Error 0.0904(0.0997) Steps 694(702.31) | Grad Norm 1.2312(1.2164) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 24.9575, Epoch Time 408.3870(411.1574), Bit/dim 3.7313(best: 3.7304), Xent 2.0842, Loss 4.7734, Error 0.4274(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2557 | Time 61.1535(61.7013) | Bit/dim 3.7069(3.7170) | Xent 0.2626(0.2733) | Loss 3.8382(3.8536) | Error 0.0954(0.0996) Steps 724(702.96) | Grad Norm 0.9316(1.2079) | Total Time 14.00(14.00)\n",
      "Iter 2558 | Time 63.0391(61.7414) | Bit/dim 3.7189(3.7171) | Xent 0.2806(0.2735) | Loss 3.8593(3.8538) | Error 0.1038(0.0997) Steps 712(703.23) | Grad Norm 0.9502(1.2001) | Total Time 14.00(14.00)\n",
      "Iter 2559 | Time 60.7720(61.7124) | Bit/dim 3.7250(3.7173) | Xent 0.2646(0.2732) | Loss 3.8573(3.8539) | Error 0.0966(0.0996) Steps 694(702.95) | Grad Norm 1.2300(1.2010) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 62.2568(61.7287) | Bit/dim 3.7259(3.7175) | Xent 0.2682(0.2731) | Loss 3.8600(3.8541) | Error 0.0958(0.0995) Steps 706(703.04) | Grad Norm 1.0753(1.1973) | Total Time 14.00(14.00)\n",
      "Iter 2561 | Time 62.0743(61.7391) | Bit/dim 3.7158(3.7175) | Xent 0.2729(0.2731) | Loss 3.8522(3.8540) | Error 0.0986(0.0995) Steps 700(702.95) | Grad Norm 0.8339(1.1864) | Total Time 14.00(14.00)\n",
      "Iter 2562 | Time 63.6168(61.7954) | Bit/dim 3.7070(3.7172) | Xent 0.2650(0.2728) | Loss 3.8396(3.8536) | Error 0.1002(0.0995) Steps 706(703.04) | Grad Norm 1.4613(1.1946) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 25.4122, Epoch Time 414.0322(411.2437), Bit/dim 3.7320(best: 3.7304), Xent 2.0880, Loss 4.7760, Error 0.4309(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2563 | Time 61.9249(61.7993) | Bit/dim 3.7179(3.7172) | Xent 0.2667(0.2727) | Loss 3.8512(3.8535) | Error 0.0962(0.0994) Steps 694(702.77) | Grad Norm 1.0390(1.1899) | Total Time 14.00(14.00)\n",
      "Iter 2564 | Time 61.6835(61.7958) | Bit/dim 3.7130(3.7171) | Xent 0.2638(0.2724) | Loss 3.8449(3.8533) | Error 0.0938(0.0992) Steps 706(702.87) | Grad Norm 1.0398(1.1854) | Total Time 14.00(14.00)\n",
      "Iter 2565 | Time 61.3504(61.7824) | Bit/dim 3.7194(3.7171) | Xent 0.2689(0.2723) | Loss 3.8539(3.8533) | Error 0.0968(0.0991) Steps 694(702.60) | Grad Norm 1.0332(1.1809) | Total Time 14.00(14.00)\n",
      "Iter 2566 | Time 61.2200(61.7656) | Bit/dim 3.7124(3.7170) | Xent 0.2567(0.2718) | Loss 3.8408(3.8529) | Error 0.0921(0.0989) Steps 712(702.89) | Grad Norm 1.8209(1.2001) | Total Time 14.00(14.00)\n",
      "Iter 2567 | Time 60.9381(61.7407) | Bit/dim 3.7233(3.7172) | Xent 0.2644(0.2716) | Loss 3.8555(3.8530) | Error 0.0982(0.0989) Steps 706(702.98) | Grad Norm 1.8038(1.2182) | Total Time 14.00(14.00)\n",
      "Iter 2568 | Time 62.2758(61.7568) | Bit/dim 3.7131(3.7171) | Xent 0.2721(0.2716) | Loss 3.8491(3.8529) | Error 0.0971(0.0989) Steps 688(702.53) | Grad Norm 0.7903(1.2054) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 25.7596, Epoch Time 410.5210(411.2220), Bit/dim 3.7320(best: 3.7304), Xent 2.0921, Loss 4.7780, Error 0.4273(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2569 | Time 61.3057(61.7433) | Bit/dim 3.7215(3.7172) | Xent 0.2681(0.2715) | Loss 3.8556(3.8530) | Error 0.0961(0.0988) Steps 712(702.81) | Grad Norm 0.8851(1.1957) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 62.1215(61.7546) | Bit/dim 3.7115(3.7170) | Xent 0.2661(0.2713) | Loss 3.8445(3.8527) | Error 0.0988(0.0988) Steps 700(702.73) | Grad Norm 1.3462(1.2003) | Total Time 14.00(14.00)\n",
      "Iter 2571 | Time 61.4630(61.7459) | Bit/dim 3.7117(3.7169) | Xent 0.2618(0.2711) | Loss 3.8426(3.8524) | Error 0.0958(0.0987) Steps 706(702.83) | Grad Norm 1.3632(1.2051) | Total Time 14.00(14.00)\n",
      "Iter 2572 | Time 61.3299(61.7334) | Bit/dim 3.7147(3.7168) | Xent 0.2627(0.2708) | Loss 3.8460(3.8522) | Error 0.0931(0.0985) Steps 694(702.56) | Grad Norm 1.1415(1.2032) | Total Time 14.00(14.00)\n",
      "Iter 2573 | Time 65.0777(61.8337) | Bit/dim 3.7211(3.7169) | Xent 0.2575(0.2704) | Loss 3.8499(3.8521) | Error 0.0935(0.0984) Steps 700(702.49) | Grad Norm 1.0010(1.1972) | Total Time 14.00(14.00)\n",
      "Iter 2574 | Time 65.0169(61.9292) | Bit/dim 3.7228(3.7171) | Xent 0.2568(0.2700) | Loss 3.8512(3.8521) | Error 0.0926(0.0982) Steps 700(702.41) | Grad Norm 1.1435(1.1956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 25.3968, Epoch Time 420.7489(411.5078), Bit/dim 3.7309(best: 3.7304), Xent 2.1079, Loss 4.7849, Error 0.4308(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2575 | Time 63.0007(61.9614) | Bit/dim 3.7087(3.7169) | Xent 0.2533(0.2695) | Loss 3.8353(3.8516) | Error 0.0879(0.0979) Steps 706(702.52) | Grad Norm 1.4101(1.2020) | Total Time 14.00(14.00)\n",
      "Iter 2576 | Time 60.7276(61.9243) | Bit/dim 3.7142(3.7168) | Xent 0.2720(0.2696) | Loss 3.8502(3.8516) | Error 0.0985(0.0979) Steps 688(702.08) | Grad Norm 0.8806(1.1924) | Total Time 14.00(14.00)\n",
      "Iter 2577 | Time 60.2164(61.8731) | Bit/dim 3.7171(3.7168) | Xent 0.2615(0.2693) | Loss 3.8478(3.8514) | Error 0.0991(0.0979) Steps 688(701.66) | Grad Norm 1.5356(1.2026) | Total Time 14.00(14.00)\n",
      "Iter 2578 | Time 63.6454(61.9263) | Bit/dim 3.7228(3.7170) | Xent 0.2707(0.2694) | Loss 3.8582(3.8516) | Error 0.0975(0.0979) Steps 706(701.79) | Grad Norm 1.2425(1.2038) | Total Time 14.00(14.00)\n",
      "Iter 2579 | Time 63.5734(61.9757) | Bit/dim 3.7213(3.7171) | Xent 0.2708(0.2694) | Loss 3.8566(3.8518) | Error 0.1004(0.0980) Steps 712(702.10) | Grad Norm 0.8676(1.1938) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 62.0719(61.9786) | Bit/dim 3.7169(3.7171) | Xent 0.2630(0.2692) | Loss 3.8483(3.8517) | Error 0.0964(0.0979) Steps 688(701.67) | Grad Norm 1.0500(1.1894) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 25.2193, Epoch Time 414.0168(411.5831), Bit/dim 3.7302(best: 3.7304), Xent 2.0875, Loss 4.7740, Error 0.4256(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2581 | Time 61.1400(61.9534) | Bit/dim 3.7169(3.7171) | Xent 0.2686(0.2692) | Loss 3.8512(3.8517) | Error 0.0969(0.0979) Steps 706(701.80) | Grad Norm 1.0819(1.1862) | Total Time 14.00(14.00)\n",
      "Iter 2582 | Time 63.1419(61.9891) | Bit/dim 3.7191(3.7171) | Xent 0.2704(0.2692) | Loss 3.8543(3.8518) | Error 0.1032(0.0981) Steps 694(701.57) | Grad Norm 1.1525(1.1852) | Total Time 14.00(14.00)\n",
      "Iter 2583 | Time 61.4080(61.9716) | Bit/dim 3.7173(3.7171) | Xent 0.2748(0.2694) | Loss 3.8547(3.8518) | Error 0.1010(0.0982) Steps 700(701.52) | Grad Norm 0.9155(1.1771) | Total Time 14.00(14.00)\n",
      "Iter 2584 | Time 61.8324(61.9675) | Bit/dim 3.7183(3.7172) | Xent 0.2607(0.2691) | Loss 3.8487(3.8518) | Error 0.0920(0.0980) Steps 694(701.30) | Grad Norm 0.9122(1.1692) | Total Time 14.00(14.00)\n",
      "Iter 2585 | Time 60.9997(61.9384) | Bit/dim 3.7126(3.7170) | Xent 0.2717(0.2692) | Loss 3.8484(3.8517) | Error 0.0994(0.0980) Steps 706(701.44) | Grad Norm 1.0955(1.1670) | Total Time 14.00(14.00)\n",
      "Iter 2586 | Time 59.0772(61.8526) | Bit/dim 3.7147(3.7170) | Xent 0.2586(0.2689) | Loss 3.8440(3.8514) | Error 0.0960(0.0980) Steps 700(701.40) | Grad Norm 1.1987(1.1679) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 25.7673, Epoch Time 408.7391(411.4977), Bit/dim 3.7305(best: 3.7302), Xent 2.0959, Loss 4.7784, Error 0.4258(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2587 | Time 64.9513(61.9456) | Bit/dim 3.7281(3.7173) | Xent 0.2689(0.2689) | Loss 3.8625(3.8518) | Error 0.0958(0.0979) Steps 706(701.53) | Grad Norm 1.5890(1.1805) | Total Time 14.00(14.00)\n",
      "Iter 2588 | Time 60.6814(61.9076) | Bit/dim 3.7056(3.7170) | Xent 0.2600(0.2686) | Loss 3.8356(3.8513) | Error 0.0944(0.0978) Steps 712(701.85) | Grad Norm 2.1560(1.2098) | Total Time 14.00(14.00)\n",
      "Iter 2589 | Time 61.7890(61.9041) | Bit/dim 3.7034(3.7165) | Xent 0.2582(0.2683) | Loss 3.8325(3.8507) | Error 0.0904(0.0976) Steps 706(701.97) | Grad Norm 0.9986(1.2035) | Total Time 14.00(14.00)\n",
      "Iter 2590 | Time 62.6762(61.9272) | Bit/dim 3.7182(3.7166) | Xent 0.2618(0.2681) | Loss 3.8491(3.8507) | Error 0.0936(0.0974) Steps 694(701.73) | Grad Norm 0.9504(1.1959) | Total Time 14.00(14.00)\n",
      "Iter 2591 | Time 62.4081(61.9417) | Bit/dim 3.7280(3.7169) | Xent 0.2614(0.2679) | Loss 3.8587(3.8509) | Error 0.0939(0.0973) Steps 694(701.50) | Grad Norm 1.8572(1.2157) | Total Time 14.00(14.00)\n",
      "Iter 2592 | Time 58.9017(61.8505) | Bit/dim 3.7145(3.7169) | Xent 0.2603(0.2677) | Loss 3.8447(3.8507) | Error 0.0961(0.0973) Steps 694(701.28) | Grad Norm 1.3120(1.2186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 25.3295, Epoch Time 412.5517(411.5294), Bit/dim 3.7307(best: 3.7302), Xent 2.1036, Loss 4.7825, Error 0.4312(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2593 | Time 61.3022(61.8340) | Bit/dim 3.7112(3.7167) | Xent 0.2645(0.2676) | Loss 3.8434(3.8505) | Error 0.0988(0.0973) Steps 700(701.24) | Grad Norm 1.2138(1.2185) | Total Time 14.00(14.00)\n",
      "Iter 2594 | Time 61.3230(61.8187) | Bit/dim 3.7163(3.7167) | Xent 0.2707(0.2677) | Loss 3.8516(3.8505) | Error 0.0966(0.0973) Steps 700(701.20) | Grad Norm 1.0637(1.2138) | Total Time 14.00(14.00)\n",
      "Iter 2595 | Time 62.1740(61.8293) | Bit/dim 3.7115(3.7165) | Xent 0.2606(0.2675) | Loss 3.8418(3.8503) | Error 0.0966(0.0973) Steps 706(701.34) | Grad Norm 1.6226(1.2261) | Total Time 14.00(14.00)\n",
      "Iter 2596 | Time 62.0541(61.8361) | Bit/dim 3.7146(3.7165) | Xent 0.2691(0.2675) | Loss 3.8492(3.8502) | Error 0.0994(0.0974) Steps 682(700.76) | Grad Norm 1.5120(1.2347) | Total Time 14.00(14.00)\n",
      "Iter 2597 | Time 61.4697(61.8251) | Bit/dim 3.7216(3.7166) | Xent 0.2576(0.2672) | Loss 3.8504(3.8502) | Error 0.0940(0.0973) Steps 700(700.74) | Grad Norm 1.3382(1.2378) | Total Time 14.00(14.00)\n",
      "Iter 2598 | Time 62.4382(61.8435) | Bit/dim 3.7157(3.7166) | Xent 0.2699(0.2673) | Loss 3.8507(3.8502) | Error 0.0986(0.0973) Steps 712(701.08) | Grad Norm 1.1890(1.2363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 25.5642, Epoch Time 411.6229(411.5322), Bit/dim 3.7313(best: 3.7302), Xent 2.0979, Loss 4.7803, Error 0.4253(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2599 | Time 60.5799(61.8056) | Bit/dim 3.7137(3.7165) | Xent 0.2675(0.2673) | Loss 3.8475(3.8502) | Error 0.0979(0.0973) Steps 694(700.87) | Grad Norm 1.1363(1.2333) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 61.6624(61.8013) | Bit/dim 3.7126(3.7164) | Xent 0.2678(0.2673) | Loss 3.8464(3.8501) | Error 0.0989(0.0974) Steps 694(700.66) | Grad Norm 1.2983(1.2353) | Total Time 14.00(14.00)\n",
      "Iter 2601 | Time 64.5006(61.8823) | Bit/dim 3.7222(3.7166) | Xent 0.2629(0.2672) | Loss 3.8537(3.8502) | Error 0.0959(0.0973) Steps 706(700.82) | Grad Norm 1.5336(1.2442) | Total Time 14.00(14.00)\n",
      "Iter 2602 | Time 62.3932(61.8976) | Bit/dim 3.7159(3.7165) | Xent 0.2525(0.2668) | Loss 3.8422(3.8499) | Error 0.0921(0.0972) Steps 700(700.80) | Grad Norm 1.6112(1.2552) | Total Time 14.00(14.00)\n",
      "Iter 2603 | Time 59.3676(61.8217) | Bit/dim 3.7151(3.7165) | Xent 0.2607(0.2666) | Loss 3.8455(3.8498) | Error 0.0952(0.0971) Steps 700(700.77) | Grad Norm 1.3342(1.2576) | Total Time 14.00(14.00)\n",
      "Iter 2604 | Time 64.0787(61.8894) | Bit/dim 3.7155(3.7165) | Xent 0.2629(0.2665) | Loss 3.8469(3.8497) | Error 0.0994(0.0972) Steps 706(700.93) | Grad Norm 1.2018(1.2559) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 25.7999, Epoch Time 413.5241(411.5919), Bit/dim 3.7309(best: 3.7302), Xent 2.1090, Loss 4.7854, Error 0.4287(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2605 | Time 62.5144(61.9081) | Bit/dim 3.7217(3.7166) | Xent 0.2511(0.2660) | Loss 3.8473(3.8496) | Error 0.0917(0.0970) Steps 700(700.90) | Grad Norm 2.0730(1.2804) | Total Time 14.00(14.00)\n",
      "Iter 2606 | Time 59.9330(61.8489) | Bit/dim 3.7170(3.7166) | Xent 0.2592(0.2658) | Loss 3.8466(3.8495) | Error 0.0926(0.0969) Steps 700(700.87) | Grad Norm 1.4313(1.2849) | Total Time 14.00(14.00)\n",
      "Iter 2607 | Time 62.2279(61.8603) | Bit/dim 3.7182(3.7167) | Xent 0.2624(0.2657) | Loss 3.8494(3.8495) | Error 0.0949(0.0968) Steps 706(701.03) | Grad Norm 1.1754(1.2817) | Total Time 14.00(14.00)\n",
      "Iter 2608 | Time 61.1771(61.8398) | Bit/dim 3.7205(3.7168) | Xent 0.2512(0.2653) | Loss 3.8461(3.8494) | Error 0.0939(0.0967) Steps 700(701.00) | Grad Norm 1.5056(1.2884) | Total Time 14.00(14.00)\n",
      "Iter 2609 | Time 64.3354(61.9146) | Bit/dim 3.7091(3.7166) | Xent 0.2633(0.2652) | Loss 3.8408(3.8492) | Error 0.0981(0.0968) Steps 700(700.97) | Grad Norm 1.2802(1.2881) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 62.8425(61.9425) | Bit/dim 3.7145(3.7165) | Xent 0.2713(0.2654) | Loss 3.8501(3.8492) | Error 0.0989(0.0968) Steps 700(700.94) | Grad Norm 1.4101(1.2918) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 25.1081, Epoch Time 413.6237(411.6529), Bit/dim 3.7317(best: 3.7302), Xent 2.1208, Loss 4.7921, Error 0.4265(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2611 | Time 60.8443(61.9095) | Bit/dim 3.7079(3.7162) | Xent 0.2509(0.2650) | Loss 3.8334(3.8487) | Error 0.0944(0.0968) Steps 694(700.73) | Grad Norm 1.5246(1.2988) | Total Time 14.00(14.00)\n",
      "Iter 2612 | Time 60.8310(61.8772) | Bit/dim 3.7106(3.7161) | Xent 0.2618(0.2649) | Loss 3.8415(3.8485) | Error 0.0971(0.0968) Steps 706(700.89) | Grad Norm 1.5993(1.3078) | Total Time 14.00(14.00)\n",
      "Iter 2613 | Time 65.3134(61.9803) | Bit/dim 3.7207(3.7162) | Xent 0.2537(0.2645) | Loss 3.8475(3.8485) | Error 0.0915(0.0966) Steps 700(700.86) | Grad Norm 1.1192(1.3021) | Total Time 14.00(14.00)\n",
      "Iter 2614 | Time 63.3159(62.0203) | Bit/dim 3.7161(3.7162) | Xent 0.2532(0.2642) | Loss 3.8427(3.8483) | Error 0.0913(0.0965) Steps 694(700.66) | Grad Norm 1.9904(1.3228) | Total Time 14.00(14.00)\n",
      "Iter 2615 | Time 62.4379(62.0329) | Bit/dim 3.7268(3.7165) | Xent 0.2570(0.2640) | Loss 3.8552(3.8485) | Error 0.0931(0.0964) Steps 712(701.00) | Grad Norm 1.6407(1.3323) | Total Time 14.00(14.00)\n",
      "Iter 2616 | Time 63.0299(62.0628) | Bit/dim 3.7217(3.7167) | Xent 0.2567(0.2637) | Loss 3.8501(3.8486) | Error 0.0936(0.0963) Steps 700(700.97) | Grad Norm 1.1042(1.3255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 25.2277, Epoch Time 416.3734(411.7945), Bit/dim 3.7311(best: 3.7302), Xent 2.1194, Loss 4.7907, Error 0.4277(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2617 | Time 61.3992(62.0429) | Bit/dim 3.7324(3.7172) | Xent 0.2614(0.2637) | Loss 3.8631(3.8490) | Error 0.0942(0.0962) Steps 706(701.12) | Grad Norm 1.2006(1.3217) | Total Time 14.00(14.00)\n",
      "Iter 2618 | Time 60.2622(61.9894) | Bit/dim 3.7159(3.7171) | Xent 0.2699(0.2639) | Loss 3.8509(3.8491) | Error 0.0971(0.0962) Steps 694(700.90) | Grad Norm 1.5688(1.3291) | Total Time 14.00(14.00)\n",
      "Iter 2619 | Time 60.6984(61.9507) | Bit/dim 3.7140(3.7170) | Xent 0.2644(0.2639) | Loss 3.8462(3.8490) | Error 0.0986(0.0963) Steps 712(701.24) | Grad Norm 2.5315(1.3652) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 60.5538(61.9088) | Bit/dim 3.7113(3.7169) | Xent 0.2501(0.2635) | Loss 3.8363(3.8486) | Error 0.0933(0.0962) Steps 700(701.20) | Grad Norm 0.8542(1.3499) | Total Time 14.00(14.00)\n",
      "Iter 2621 | Time 63.3788(61.9529) | Bit/dim 3.7160(3.7168) | Xent 0.2448(0.2629) | Loss 3.8383(3.8483) | Error 0.0869(0.0959) Steps 724(701.88) | Grad Norm 1.2086(1.3456) | Total Time 14.00(14.00)\n",
      "Iter 2622 | Time 63.0848(61.9869) | Bit/dim 3.7099(3.7166) | Xent 0.2582(0.2628) | Loss 3.8390(3.8480) | Error 0.0935(0.0959) Steps 700(701.83) | Grad Norm 1.2919(1.3440) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 25.0391, Epoch Time 409.6862(411.7312), Bit/dim 3.7306(best: 3.7302), Xent 2.1276, Loss 4.7944, Error 0.4329(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2623 | Time 61.7699(61.9804) | Bit/dim 3.7207(3.7167) | Xent 0.2579(0.2626) | Loss 3.8496(3.8481) | Error 0.0917(0.0957) Steps 694(701.59) | Grad Norm 0.9360(1.3318) | Total Time 14.00(14.00)\n",
      "Iter 2624 | Time 59.5515(61.9075) | Bit/dim 3.7139(3.7167) | Xent 0.2679(0.2628) | Loss 3.8479(3.8480) | Error 0.0961(0.0958) Steps 694(701.36) | Grad Norm 1.7255(1.3436) | Total Time 14.00(14.00)\n",
      "Iter 2625 | Time 63.5162(61.9557) | Bit/dim 3.7359(3.7172) | Xent 0.2515(0.2624) | Loss 3.8617(3.8485) | Error 0.0930(0.0957) Steps 706(701.50) | Grad Norm 0.8562(1.3290) | Total Time 14.00(14.00)\n",
      "Iter 2626 | Time 60.7313(61.9190) | Bit/dim 3.7135(3.7171) | Xent 0.2454(0.2619) | Loss 3.8362(3.8481) | Error 0.0923(0.0956) Steps 694(701.28) | Grad Norm 1.1695(1.3242) | Total Time 14.00(14.00)\n",
      "Iter 2627 | Time 64.1241(61.9852) | Bit/dim 3.7017(3.7167) | Xent 0.2581(0.2618) | Loss 3.8307(3.8476) | Error 0.0927(0.0955) Steps 694(701.06) | Grad Norm 2.0366(1.3456) | Total Time 14.00(14.00)\n",
      "Iter 2628 | Time 61.1006(61.9586) | Bit/dim 3.7094(3.7164) | Xent 0.2660(0.2619) | Loss 3.8424(3.8474) | Error 0.0979(0.0956) Steps 700(701.03) | Grad Norm 1.5005(1.3502) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 25.7028, Epoch Time 412.0400(411.7405), Bit/dim 3.7308(best: 3.7302), Xent 2.1467, Loss 4.8042, Error 0.4301(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2629 | Time 59.2849(61.8784) | Bit/dim 3.7072(3.7162) | Xent 0.2514(0.2616) | Loss 3.8329(3.8470) | Error 0.0941(0.0955) Steps 700(701.00) | Grad Norm 1.2794(1.3481) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 60.4138(61.8345) | Bit/dim 3.7171(3.7162) | Xent 0.2602(0.2616) | Loss 3.8472(3.8470) | Error 0.0959(0.0955) Steps 694(700.79) | Grad Norm 0.9014(1.3347) | Total Time 14.00(14.00)\n",
      "Iter 2631 | Time 62.4230(61.8521) | Bit/dim 3.7194(3.7163) | Xent 0.2630(0.2616) | Loss 3.8509(3.8471) | Error 0.0974(0.0956) Steps 706(700.94) | Grad Norm 1.1117(1.3280) | Total Time 14.00(14.00)\n",
      "Iter 2632 | Time 61.4646(61.8405) | Bit/dim 3.7173(3.7163) | Xent 0.2619(0.2616) | Loss 3.8483(3.8471) | Error 0.0946(0.0956) Steps 706(701.10) | Grad Norm 1.1557(1.3228) | Total Time 14.00(14.00)\n",
      "Iter 2633 | Time 63.8219(61.9000) | Bit/dim 3.7165(3.7163) | Xent 0.2533(0.2614) | Loss 3.8432(3.8470) | Error 0.0916(0.0954) Steps 694(700.88) | Grad Norm 1.1960(1.3190) | Total Time 14.00(14.00)\n",
      "Iter 2634 | Time 62.9740(61.9322) | Bit/dim 3.7179(3.7164) | Xent 0.2594(0.2613) | Loss 3.8477(3.8470) | Error 0.0946(0.0954) Steps 688(700.50) | Grad Norm 1.1385(1.3136) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 25.6374, Epoch Time 411.4465(411.7317), Bit/dim 3.7305(best: 3.7302), Xent 2.1206, Loss 4.7908, Error 0.4279(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2635 | Time 61.9207(61.9318) | Bit/dim 3.7135(3.7163) | Xent 0.2534(0.2611) | Loss 3.8402(3.8468) | Error 0.0929(0.0953) Steps 718(701.02) | Grad Norm 0.9395(1.3024) | Total Time 14.00(14.00)\n",
      "Iter 2636 | Time 64.0698(61.9960) | Bit/dim 3.7066(3.7160) | Xent 0.2617(0.2611) | Loss 3.8375(3.8466) | Error 0.0936(0.0953) Steps 694(700.81) | Grad Norm 1.1700(1.2984) | Total Time 14.00(14.00)\n",
      "Iter 2637 | Time 60.3350(61.9461) | Bit/dim 3.7177(3.7161) | Xent 0.2591(0.2610) | Loss 3.8472(3.8466) | Error 0.0938(0.0952) Steps 706(700.97) | Grad Norm 1.4669(1.3035) | Total Time 14.00(14.00)\n",
      "Iter 2638 | Time 60.9630(61.9166) | Bit/dim 3.7134(3.7160) | Xent 0.2648(0.2612) | Loss 3.8458(3.8466) | Error 0.0969(0.0953) Steps 694(700.76) | Grad Norm 1.0002(1.2944) | Total Time 14.00(14.00)\n",
      "Iter 2639 | Time 59.2618(61.8370) | Bit/dim 3.7205(3.7161) | Xent 0.2548(0.2610) | Loss 3.8479(3.8466) | Error 0.0940(0.0952) Steps 700(700.73) | Grad Norm 1.0314(1.2865) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 63.7855(61.8955) | Bit/dim 3.7259(3.7164) | Xent 0.2616(0.2610) | Loss 3.8568(3.8469) | Error 0.0944(0.0952) Steps 694(700.53) | Grad Norm 0.9682(1.2769) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 25.6346, Epoch Time 414.1600(411.8045), Bit/dim 3.7297(best: 3.7302), Xent 2.1436, Loss 4.8014, Error 0.4315(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2641 | Time 61.5709(61.8857) | Bit/dim 3.7077(3.7161) | Xent 0.2600(0.2610) | Loss 3.8377(3.8466) | Error 0.0934(0.0952) Steps 700(700.52) | Grad Norm 1.2263(1.2754) | Total Time 14.00(14.00)\n",
      "Iter 2642 | Time 62.0785(61.8915) | Bit/dim 3.7192(3.7162) | Xent 0.2527(0.2607) | Loss 3.8455(3.8466) | Error 0.0940(0.0951) Steps 688(700.14) | Grad Norm 1.3418(1.2774) | Total Time 14.00(14.00)\n",
      "Iter 2643 | Time 64.1550(61.9594) | Bit/dim 3.7140(3.7162) | Xent 0.2612(0.2607) | Loss 3.8446(3.8465) | Error 0.0931(0.0951) Steps 706(700.32) | Grad Norm 1.1837(1.2746) | Total Time 14.00(14.00)\n",
      "Iter 2644 | Time 62.0464(61.9620) | Bit/dim 3.7037(3.7158) | Xent 0.2573(0.2606) | Loss 3.8323(3.8461) | Error 0.0929(0.0950) Steps 700(700.31) | Grad Norm 0.9714(1.2655) | Total Time 14.00(14.00)\n",
      "Iter 2645 | Time 62.8038(61.9873) | Bit/dim 3.7193(3.7159) | Xent 0.2670(0.2608) | Loss 3.8528(3.8463) | Error 0.0971(0.0951) Steps 700(700.30) | Grad Norm 1.4196(1.2701) | Total Time 14.00(14.00)\n",
      "Iter 2646 | Time 63.0621(62.0195) | Bit/dim 3.7264(3.7162) | Xent 0.2576(0.2607) | Loss 3.8553(3.8466) | Error 0.0942(0.0950) Steps 700(700.29) | Grad Norm 1.0858(1.2646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 25.7894, Epoch Time 417.0352(411.9615), Bit/dim 3.7304(best: 3.7297), Xent 2.1427, Loss 4.8018, Error 0.4299(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2647 | Time 61.2786(61.9973) | Bit/dim 3.7190(3.7163) | Xent 0.2650(0.2608) | Loss 3.8515(3.8467) | Error 0.0984(0.0951) Steps 706(700.46) | Grad Norm 1.7511(1.2792) | Total Time 14.00(14.00)\n",
      "Iter 2648 | Time 62.6252(62.0161) | Bit/dim 3.7100(3.7161) | Xent 0.2575(0.2607) | Loss 3.8387(3.8465) | Error 0.0960(0.0952) Steps 700(700.45) | Grad Norm 0.8750(1.2671) | Total Time 14.00(14.00)\n",
      "Iter 2649 | Time 64.3927(62.0874) | Bit/dim 3.7124(3.7160) | Xent 0.2640(0.2608) | Loss 3.8444(3.8464) | Error 0.0969(0.0952) Steps 694(700.25) | Grad Norm 1.0196(1.2596) | Total Time 14.00(14.00)\n",
      "Iter 2650 | Time 63.5761(62.1321) | Bit/dim 3.7137(3.7159) | Xent 0.2524(0.2606) | Loss 3.8399(3.8462) | Error 0.0933(0.0952) Steps 706(700.43) | Grad Norm 2.1888(1.2875) | Total Time 14.00(14.00)\n",
      "Iter 2651 | Time 64.4601(62.2019) | Bit/dim 3.7216(3.7161) | Xent 0.2466(0.2602) | Loss 3.8449(3.8462) | Error 0.0917(0.0951) Steps 700(700.41) | Grad Norm 1.2050(1.2850) | Total Time 14.00(14.00)\n",
      "Iter 2652 | Time 65.1815(62.2913) | Bit/dim 3.7188(3.7162) | Xent 0.2572(0.2601) | Loss 3.8474(3.8462) | Error 0.0921(0.0950) Steps 688(700.04) | Grad Norm 1.3397(1.2867) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 25.6822, Epoch Time 422.9700(412.2917), Bit/dim 3.7310(best: 3.7297), Xent 2.1306, Loss 4.7963, Error 0.4246(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2653 | Time 61.6190(62.2711) | Bit/dim 3.7130(3.7161) | Xent 0.2494(0.2598) | Loss 3.8377(3.8460) | Error 0.0903(0.0948) Steps 694(699.86) | Grad Norm 1.1462(1.2825) | Total Time 14.00(14.00)\n",
      "Iter 2654 | Time 62.5663(62.2800) | Bit/dim 3.7188(3.7162) | Xent 0.2634(0.2599) | Loss 3.8504(3.8461) | Error 0.0968(0.0949) Steps 712(700.22) | Grad Norm 1.1758(1.2793) | Total Time 14.00(14.00)\n",
      "Iter 2655 | Time 62.2725(62.2798) | Bit/dim 3.7233(3.7164) | Xent 0.2517(0.2596) | Loss 3.8492(3.8462) | Error 0.0931(0.0948) Steps 688(699.86) | Grad Norm 1.4937(1.2857) | Total Time 14.00(14.00)\n",
      "Iter 2656 | Time 63.1801(62.3068) | Bit/dim 3.7122(3.7163) | Xent 0.2509(0.2594) | Loss 3.8377(3.8459) | Error 0.0925(0.0948) Steps 700(699.86) | Grad Norm 1.2427(1.2844) | Total Time 14.00(14.00)\n",
      "Iter 2657 | Time 59.5894(62.2253) | Bit/dim 3.7141(3.7162) | Xent 0.2478(0.2590) | Loss 3.8380(3.8457) | Error 0.0906(0.0946) Steps 700(699.87) | Grad Norm 1.0133(1.2763) | Total Time 14.00(14.00)\n",
      "Iter 2658 | Time 62.2511(62.2260) | Bit/dim 3.7175(3.7162) | Xent 0.2585(0.2590) | Loss 3.8467(3.8457) | Error 0.0934(0.0946) Steps 706(700.05) | Grad Norm 1.0311(1.2689) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 25.6623, Epoch Time 412.7220(412.3046), Bit/dim 3.7302(best: 3.7297), Xent 2.1588, Loss 4.8096, Error 0.4292(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2659 | Time 60.1660(62.1642) | Bit/dim 3.7122(3.7161) | Xent 0.2609(0.2591) | Loss 3.8426(3.8456) | Error 0.0919(0.0945) Steps 706(700.23) | Grad Norm 1.3555(1.2715) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 61.8930(62.1561) | Bit/dim 3.7175(3.7161) | Xent 0.2490(0.2588) | Loss 3.8420(3.8455) | Error 0.0906(0.0944) Steps 694(700.04) | Grad Norm 0.9802(1.2628) | Total Time 14.00(14.00)\n",
      "Iter 2661 | Time 61.4209(62.1340) | Bit/dim 3.7221(3.7163) | Xent 0.2495(0.2585) | Loss 3.8469(3.8456) | Error 0.0940(0.0944) Steps 700(700.04) | Grad Norm 1.3164(1.2644) | Total Time 14.00(14.00)\n",
      "Iter 2662 | Time 62.1342(62.1340) | Bit/dim 3.7044(3.7160) | Xent 0.2559(0.2584) | Loss 3.8323(3.8452) | Error 0.0927(0.0943) Steps 694(699.86) | Grad Norm 1.5449(1.2728) | Total Time 14.00(14.00)\n",
      "Iter 2663 | Time 61.2209(62.1066) | Bit/dim 3.7154(3.7160) | Xent 0.2659(0.2586) | Loss 3.8483(3.8453) | Error 0.0991(0.0945) Steps 706(700.04) | Grad Norm 0.9472(1.2630) | Total Time 14.00(14.00)\n",
      "Iter 2664 | Time 62.5770(62.1208) | Bit/dim 3.7175(3.7160) | Xent 0.2458(0.2582) | Loss 3.8404(3.8451) | Error 0.0921(0.0944) Steps 706(700.22) | Grad Norm 1.3710(1.2663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 25.6948, Epoch Time 410.8236(412.2602), Bit/dim 3.7302(best: 3.7297), Xent 2.1337, Loss 4.7970, Error 0.4314(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2665 | Time 63.1643(62.1521) | Bit/dim 3.7190(3.7161) | Xent 0.2499(0.2580) | Loss 3.8440(3.8451) | Error 0.0885(0.0942) Steps 694(700.03) | Grad Norm 1.1860(1.2639) | Total Time 14.00(14.00)\n",
      "Iter 2666 | Time 62.2306(62.1544) | Bit/dim 3.7164(3.7161) | Xent 0.2576(0.2580) | Loss 3.8452(3.8451) | Error 0.0958(0.0943) Steps 706(700.21) | Grad Norm 1.2316(1.2629) | Total Time 14.00(14.00)\n",
      "Iter 2667 | Time 62.2988(62.1587) | Bit/dim 3.7020(3.7157) | Xent 0.2552(0.2579) | Loss 3.8296(3.8446) | Error 0.0934(0.0943) Steps 706(700.39) | Grad Norm 1.3024(1.2641) | Total Time 14.00(14.00)\n",
      "Iter 2668 | Time 63.2270(62.1908) | Bit/dim 3.7157(3.7157) | Xent 0.2582(0.2579) | Loss 3.8448(3.8446) | Error 0.0919(0.0942) Steps 700(700.38) | Grad Norm 1.7117(1.2775) | Total Time 14.00(14.00)\n",
      "Iter 2669 | Time 61.6487(62.1745) | Bit/dim 3.7220(3.7159) | Xent 0.2612(0.2580) | Loss 3.8526(3.8449) | Error 0.0929(0.0941) Steps 700(700.36) | Grad Norm 1.6239(1.2879) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 60.0215(62.1099) | Bit/dim 3.7144(3.7158) | Xent 0.2572(0.2580) | Loss 3.8430(3.8448) | Error 0.0933(0.0941) Steps 700(700.35) | Grad Norm 1.5207(1.2949) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 25.3701, Epoch Time 413.5402(412.2986), Bit/dim 3.7301(best: 3.7297), Xent 2.1601, Loss 4.8101, Error 0.4289(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2671 | Time 62.2491(62.1141) | Bit/dim 3.7115(3.7157) | Xent 0.2610(0.2581) | Loss 3.8420(3.8447) | Error 0.0926(0.0941) Steps 694(700.16) | Grad Norm 1.7993(1.3100) | Total Time 14.00(14.00)\n",
      "Iter 2672 | Time 64.8584(62.1964) | Bit/dim 3.7163(3.7157) | Xent 0.2469(0.2577) | Loss 3.8397(3.8446) | Error 0.0901(0.0940) Steps 694(699.98) | Grad Norm 1.1009(1.3037) | Total Time 14.00(14.00)\n",
      "Iter 2673 | Time 63.0755(62.2228) | Bit/dim 3.7086(3.7155) | Xent 0.2486(0.2575) | Loss 3.8329(3.8442) | Error 0.0875(0.0938) Steps 694(699.80) | Grad Norm 1.2565(1.3023) | Total Time 14.00(14.00)\n",
      "Iter 2674 | Time 60.1071(62.1593) | Bit/dim 3.7160(3.7155) | Xent 0.2587(0.2575) | Loss 3.8453(3.8443) | Error 0.0919(0.0937) Steps 700(699.80) | Grad Norm 1.7454(1.3156) | Total Time 14.00(14.00)\n",
      "Iter 2675 | Time 62.9950(62.1844) | Bit/dim 3.7216(3.7157) | Xent 0.2632(0.2577) | Loss 3.8532(3.8445) | Error 0.0972(0.0938) Steps 706(699.99) | Grad Norm 1.4834(1.3207) | Total Time 14.00(14.00)\n",
      "Iter 2676 | Time 62.0478(62.1803) | Bit/dim 3.7166(3.7157) | Xent 0.2669(0.2580) | Loss 3.8501(3.8447) | Error 0.0976(0.0939) Steps 688(699.63) | Grad Norm 1.0301(1.3119) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 25.4899, Epoch Time 415.9633(412.4085), Bit/dim 3.7298(best: 3.7297), Xent 2.1380, Loss 4.7988, Error 0.4280(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2677 | Time 63.0056(62.2051) | Bit/dim 3.7179(3.7158) | Xent 0.2386(0.2574) | Loss 3.8372(3.8445) | Error 0.0874(0.0937) Steps 700(699.64) | Grad Norm 0.9192(1.3002) | Total Time 14.00(14.00)\n",
      "Iter 2678 | Time 61.7062(62.1901) | Bit/dim 3.7167(3.7158) | Xent 0.2590(0.2574) | Loss 3.8462(3.8445) | Error 0.0929(0.0937) Steps 706(699.83) | Grad Norm 1.8357(1.3162) | Total Time 14.00(14.00)\n",
      "Iter 2679 | Time 59.8193(62.1190) | Bit/dim 3.7106(3.7157) | Xent 0.2614(0.2575) | Loss 3.8413(3.8444) | Error 0.0985(0.0938) Steps 700(699.84) | Grad Norm 1.2927(1.3155) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 61.0837(62.0879) | Bit/dim 3.7141(3.7156) | Xent 0.2468(0.2572) | Loss 3.8375(3.8442) | Error 0.0927(0.0938) Steps 700(699.84) | Grad Norm 1.6113(1.3244) | Total Time 14.00(14.00)\n",
      "Iter 2681 | Time 62.8313(62.1102) | Bit/dim 3.7147(3.7156) | Xent 0.2470(0.2569) | Loss 3.8383(3.8440) | Error 0.0899(0.0937) Steps 706(700.03) | Grad Norm 1.1654(1.3196) | Total Time 14.00(14.00)\n",
      "Iter 2682 | Time 61.4243(62.0897) | Bit/dim 3.7232(3.7158) | Xent 0.2409(0.2564) | Loss 3.8436(3.8440) | Error 0.0880(0.0935) Steps 688(699.67) | Grad Norm 1.6606(1.3298) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 25.2762, Epoch Time 410.5309(412.3522), Bit/dim 3.7311(best: 3.7297), Xent 2.1869, Loss 4.8246, Error 0.4312(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2683 | Time 64.9479(62.1754) | Bit/dim 3.7142(3.7158) | Xent 0.2581(0.2565) | Loss 3.8432(3.8440) | Error 0.0952(0.0936) Steps 700(699.68) | Grad Norm 1.7921(1.3437) | Total Time 14.00(14.00)\n",
      "Iter 2684 | Time 59.6153(62.0986) | Bit/dim 3.7154(3.7158) | Xent 0.2463(0.2562) | Loss 3.8386(3.8438) | Error 0.0906(0.0935) Steps 700(699.69) | Grad Norm 1.3607(1.3442) | Total Time 14.00(14.00)\n",
      "Iter 2685 | Time 62.7213(62.1173) | Bit/dim 3.7172(3.7158) | Xent 0.2619(0.2563) | Loss 3.8482(3.8440) | Error 0.0951(0.0935) Steps 694(699.52) | Grad Norm 1.6840(1.3544) | Total Time 14.00(14.00)\n",
      "Iter 2686 | Time 61.8069(62.1080) | Bit/dim 3.7173(3.7158) | Xent 0.2453(0.2560) | Loss 3.8399(3.8439) | Error 0.0914(0.0935) Steps 706(699.71) | Grad Norm 2.1919(1.3795) | Total Time 14.00(14.00)\n",
      "Iter 2687 | Time 60.8613(62.0706) | Bit/dim 3.7144(3.7158) | Xent 0.2558(0.2560) | Loss 3.8423(3.8438) | Error 0.0913(0.0934) Steps 694(699.54) | Grad Norm 1.8092(1.3924) | Total Time 14.00(14.00)\n",
      "Iter 2688 | Time 64.7754(62.1517) | Bit/dim 3.7157(3.7158) | Xent 0.2406(0.2555) | Loss 3.8360(3.8436) | Error 0.0893(0.0933) Steps 700(699.55) | Grad Norm 1.7074(1.4019) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 25.2517, Epoch Time 415.2648(412.4396), Bit/dim 3.7306(best: 3.7297), Xent 2.1671, Loss 4.8142, Error 0.4278(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2689 | Time 62.1637(62.1521) | Bit/dim 3.7160(3.7158) | Xent 0.2572(0.2556) | Loss 3.8446(3.8436) | Error 0.0938(0.0933) Steps 712(699.93) | Grad Norm 1.1488(1.3943) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 59.2663(62.0655) | Bit/dim 3.7155(3.7158) | Xent 0.2507(0.2554) | Loss 3.8409(3.8435) | Error 0.0896(0.0932) Steps 700(699.93) | Grad Norm 1.5690(1.3995) | Total Time 14.00(14.00)\n",
      "Iter 2691 | Time 61.9795(62.0629) | Bit/dim 3.7103(3.7156) | Xent 0.2547(0.2554) | Loss 3.8377(3.8433) | Error 0.0897(0.0931) Steps 706(700.11) | Grad Norm 2.2002(1.4236) | Total Time 14.00(14.00)\n",
      "Iter 2692 | Time 63.1302(62.0949) | Bit/dim 3.7137(3.7156) | Xent 0.2430(0.2550) | Loss 3.8352(3.8431) | Error 0.0910(0.0930) Steps 700(700.11) | Grad Norm 1.6410(1.4301) | Total Time 14.00(14.00)\n",
      "Iter 2693 | Time 61.9047(62.0892) | Bit/dim 3.7122(3.7155) | Xent 0.2432(0.2547) | Loss 3.8338(3.8428) | Error 0.0896(0.0929) Steps 694(699.92) | Grad Norm 0.7572(1.4099) | Total Time 14.00(14.00)\n",
      "Iter 2694 | Time 65.0452(62.1779) | Bit/dim 3.7240(3.7157) | Xent 0.2541(0.2547) | Loss 3.8510(3.8431) | Error 0.0939(0.0929) Steps 700(699.93) | Grad Norm 1.5272(1.4134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 25.3985, Epoch Time 414.2001(412.4924), Bit/dim 3.7306(best: 3.7297), Xent 2.1632, Loss 4.8122, Error 0.4288(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2695 | Time 62.3856(62.1841) | Bit/dim 3.7134(3.7157) | Xent 0.2388(0.2542) | Loss 3.8328(3.8428) | Error 0.0871(0.0928) Steps 700(699.93) | Grad Norm 1.2038(1.4071) | Total Time 14.00(14.00)\n",
      "Iter 2696 | Time 62.9268(62.2064) | Bit/dim 3.7139(3.7156) | Xent 0.2736(0.2548) | Loss 3.8507(3.8430) | Error 0.0988(0.0930) Steps 694(699.75) | Grad Norm 1.1143(1.3983) | Total Time 14.00(14.00)\n",
      "Iter 2697 | Time 62.3188(62.2098) | Bit/dim 3.7095(3.7154) | Xent 0.2485(0.2546) | Loss 3.8337(3.8427) | Error 0.0919(0.0929) Steps 712(700.12) | Grad Norm 1.7202(1.4080) | Total Time 14.00(14.00)\n",
      "Iter 2698 | Time 62.7106(62.2248) | Bit/dim 3.7223(3.7156) | Xent 0.2509(0.2545) | Loss 3.8478(3.8429) | Error 0.0909(0.0929) Steps 694(699.93) | Grad Norm 1.9454(1.4241) | Total Time 14.00(14.00)\n",
      "Iter 2699 | Time 61.4102(62.2004) | Bit/dim 3.7146(3.7156) | Xent 0.2562(0.2545) | Loss 3.8427(3.8429) | Error 0.0946(0.0929) Steps 706(700.12) | Grad Norm 0.7903(1.4051) | Total Time 14.00(14.00)\n",
      "Iter 2700 | Time 62.2594(62.2021) | Bit/dim 3.7186(3.7157) | Xent 0.2440(0.2542) | Loss 3.8406(3.8428) | Error 0.0893(0.0928) Steps 700(700.11) | Grad Norm 1.2382(1.4001) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 25.7902, Epoch Time 415.2996(412.5766), Bit/dim 3.7309(best: 3.7297), Xent 2.1778, Loss 4.8198, Error 0.4267(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2701 | Time 61.9698(62.1952) | Bit/dim 3.7164(3.7157) | Xent 0.2460(0.2540) | Loss 3.8394(3.8427) | Error 0.0899(0.0927) Steps 706(700.29) | Grad Norm 1.3629(1.3990) | Total Time 14.00(14.00)\n",
      "Iter 2702 | Time 62.1857(62.1949) | Bit/dim 3.7159(3.7157) | Xent 0.2436(0.2537) | Loss 3.8377(3.8425) | Error 0.0877(0.0926) Steps 706(700.46) | Grad Norm 1.2385(1.3942) | Total Time 14.00(14.00)\n",
      "Iter 2703 | Time 65.3989(62.2910) | Bit/dim 3.7169(3.7158) | Xent 0.2585(0.2538) | Loss 3.8461(3.8427) | Error 0.0948(0.0926) Steps 706(700.63) | Grad Norm 1.2186(1.3889) | Total Time 14.00(14.00)\n",
      "Iter 2704 | Time 60.5490(62.2388) | Bit/dim 3.7179(3.7158) | Xent 0.2549(0.2538) | Loss 3.8453(3.8427) | Error 0.0926(0.0926) Steps 694(700.43) | Grad Norm 1.1822(1.3827) | Total Time 14.00(14.00)\n",
      "Iter 2705 | Time 59.9368(62.1697) | Bit/dim 3.7175(3.7159) | Xent 0.2421(0.2535) | Loss 3.8386(3.8426) | Error 0.0877(0.0925) Steps 694(700.24) | Grad Norm 1.0940(1.3740) | Total Time 14.00(14.00)\n",
      "Iter 2706 | Time 63.6372(62.2137) | Bit/dim 3.7060(3.7156) | Xent 0.2465(0.2533) | Loss 3.8293(3.8422) | Error 0.0936(0.0925) Steps 700(700.23) | Grad Norm 1.1171(1.3663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 25.4262, Epoch Time 414.3781(412.6307), Bit/dim 3.7307(best: 3.7297), Xent 2.1842, Loss 4.8228, Error 0.4300(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2707 | Time 65.3506(62.3078) | Bit/dim 3.7189(3.7157) | Xent 0.2422(0.2529) | Loss 3.8399(3.8421) | Error 0.0870(0.0924) Steps 682(699.68) | Grad Norm 1.0679(1.3574) | Total Time 14.00(14.00)\n",
      "Iter 2708 | Time 61.3515(62.2791) | Bit/dim 3.7085(3.7155) | Xent 0.2499(0.2529) | Loss 3.8334(3.8419) | Error 0.0923(0.0923) Steps 700(699.69) | Grad Norm 1.7389(1.3688) | Total Time 14.00(14.00)\n",
      "Iter 2709 | Time 61.9942(62.2706) | Bit/dim 3.7215(3.7156) | Xent 0.2515(0.2528) | Loss 3.8473(3.8420) | Error 0.0901(0.0923) Steps 706(699.88) | Grad Norm 1.0366(1.3589) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 60.8379(62.2276) | Bit/dim 3.7139(3.7156) | Xent 0.2428(0.2525) | Loss 3.8353(3.8418) | Error 0.0881(0.0922) Steps 700(699.88) | Grad Norm 1.5021(1.3632) | Total Time 14.00(14.00)\n",
      "Iter 2711 | Time 61.7662(62.2138) | Bit/dim 3.7133(3.7155) | Xent 0.2389(0.2521) | Loss 3.8327(3.8416) | Error 0.0879(0.0920) Steps 694(699.71) | Grad Norm 1.8153(1.3767) | Total Time 14.00(14.00)\n",
      "Iter 2712 | Time 64.3563(62.2780) | Bit/dim 3.7155(3.7155) | Xent 0.2581(0.2523) | Loss 3.8446(3.8417) | Error 0.0913(0.0920) Steps 694(699.54) | Grad Norm 1.0598(1.3672) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 25.6135, Epoch Time 416.4534(412.7453), Bit/dim 3.7307(best: 3.7297), Xent 2.1988, Loss 4.8301, Error 0.4293(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2713 | Time 62.1423(62.2740) | Bit/dim 3.7179(3.7156) | Xent 0.2421(0.2520) | Loss 3.8390(3.8416) | Error 0.0863(0.0918) Steps 712(699.91) | Grad Norm 0.9593(1.3550) | Total Time 14.00(14.00)\n",
      "Iter 2714 | Time 64.6592(62.3455) | Bit/dim 3.7247(3.7159) | Xent 0.2492(0.2519) | Loss 3.8493(3.8418) | Error 0.0938(0.0919) Steps 694(699.73) | Grad Norm 1.2668(1.3523) | Total Time 14.00(14.00)\n",
      "Iter 2715 | Time 61.1895(62.3108) | Bit/dim 3.7208(3.7160) | Xent 0.2475(0.2518) | Loss 3.8446(3.8419) | Error 0.0903(0.0918) Steps 700(699.74) | Grad Norm 1.0262(1.3425) | Total Time 14.00(14.00)\n",
      "Iter 2716 | Time 62.6117(62.3199) | Bit/dim 3.7015(3.7156) | Xent 0.2535(0.2518) | Loss 3.8283(3.8415) | Error 0.0930(0.0919) Steps 700(699.75) | Grad Norm 1.0584(1.3340) | Total Time 14.00(14.00)\n",
      "Iter 2717 | Time 61.0620(62.2821) | Bit/dim 3.7023(3.7152) | Xent 0.2527(0.2518) | Loss 3.8286(3.8411) | Error 0.0924(0.0919) Steps 700(699.76) | Grad Norm 1.6660(1.3440) | Total Time 14.00(14.00)\n",
      "Iter 2718 | Time 64.6650(62.3536) | Bit/dim 3.7183(3.7153) | Xent 0.2481(0.2517) | Loss 3.8423(3.8411) | Error 0.0903(0.0918) Steps 700(699.76) | Grad Norm 1.0796(1.3360) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 25.3887, Epoch Time 417.1043(412.8761), Bit/dim 3.7313(best: 3.7297), Xent 2.1818, Loss 4.8222, Error 0.4266(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2719 | Time 59.6169(62.2715) | Bit/dim 3.7188(3.7154) | Xent 0.2478(0.2516) | Loss 3.8427(3.8412) | Error 0.0885(0.0917) Steps 694(699.59) | Grad Norm 1.0190(1.3265) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 61.4419(62.2466) | Bit/dim 3.7097(3.7152) | Xent 0.2398(0.2513) | Loss 3.8296(3.8408) | Error 0.0854(0.0916) Steps 700(699.60) | Grad Norm 0.9285(1.3146) | Total Time 14.00(14.00)\n",
      "Iter 2721 | Time 61.4126(62.2216) | Bit/dim 3.7149(3.7152) | Xent 0.2471(0.2511) | Loss 3.8385(3.8408) | Error 0.0894(0.0915) Steps 712(699.97) | Grad Norm 0.8174(1.2997) | Total Time 14.00(14.00)\n",
      "Iter 2722 | Time 60.9826(62.1844) | Bit/dim 3.7107(3.7151) | Xent 0.2502(0.2511) | Loss 3.8358(3.8406) | Error 0.0890(0.0914) Steps 706(700.16) | Grad Norm 1.1318(1.2946) | Total Time 14.00(14.00)\n",
      "Iter 2723 | Time 62.4000(62.1909) | Bit/dim 3.7104(3.7149) | Xent 0.2548(0.2512) | Loss 3.8379(3.8405) | Error 0.0907(0.0914) Steps 700(700.15) | Grad Norm 1.2954(1.2947) | Total Time 14.00(14.00)\n",
      "Iter 2724 | Time 62.8479(62.2106) | Bit/dim 3.7243(3.7152) | Xent 0.2413(0.2509) | Loss 3.8450(3.8407) | Error 0.0894(0.0913) Steps 688(699.79) | Grad Norm 0.9233(1.2835) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 25.4801, Epoch Time 409.6285(412.7787), Bit/dim 3.7298(best: 3.7297), Xent 2.1974, Loss 4.8285, Error 0.4315(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2725 | Time 60.9282(62.1721) | Bit/dim 3.7141(3.7152) | Xent 0.2340(0.2504) | Loss 3.8311(3.8404) | Error 0.0845(0.0911) Steps 706(699.97) | Grad Norm 1.2275(1.2818) | Total Time 14.00(14.00)\n",
      "Iter 2726 | Time 60.0713(62.1091) | Bit/dim 3.7249(3.7155) | Xent 0.2430(0.2502) | Loss 3.8464(3.8406) | Error 0.0893(0.0911) Steps 706(700.15) | Grad Norm 1.0867(1.2760) | Total Time 14.00(14.00)\n",
      "Iter 2727 | Time 58.9993(62.0158) | Bit/dim 3.7226(3.7157) | Xent 0.2534(0.2503) | Loss 3.8493(3.8408) | Error 0.0904(0.0910) Steps 712(700.51) | Grad Norm 0.9358(1.2658) | Total Time 14.00(14.00)\n",
      "Iter 2728 | Time 60.2714(61.9635) | Bit/dim 3.7135(3.7156) | Xent 0.2530(0.2504) | Loss 3.8400(3.8408) | Error 0.0901(0.0910) Steps 706(700.67) | Grad Norm 1.2839(1.2663) | Total Time 14.00(14.00)\n",
      "Iter 2729 | Time 63.6878(62.0152) | Bit/dim 3.7113(3.7155) | Xent 0.2501(0.2504) | Loss 3.8364(3.8407) | Error 0.0919(0.0910) Steps 700(700.65) | Grad Norm 0.9845(1.2579) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 64.7459(62.0971) | Bit/dim 3.7072(3.7152) | Xent 0.2498(0.2503) | Loss 3.8321(3.8404) | Error 0.0884(0.0910) Steps 694(700.45) | Grad Norm 1.1493(1.2546) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 25.4521, Epoch Time 409.5179(412.6809), Bit/dim 3.7299(best: 3.7297), Xent 2.2079, Loss 4.8339, Error 0.4298(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2731 | Time 62.9356(62.1223) | Bit/dim 3.7075(3.7150) | Xent 0.2589(0.2506) | Loss 3.8370(3.8403) | Error 0.0952(0.0911) Steps 700(700.44) | Grad Norm 1.4126(1.2594) | Total Time 14.00(14.00)\n",
      "Iter 2732 | Time 61.8656(62.1146) | Bit/dim 3.7188(3.7151) | Xent 0.2409(0.2503) | Loss 3.8392(3.8403) | Error 0.0880(0.0910) Steps 712(700.79) | Grad Norm 1.4384(1.2647) | Total Time 14.00(14.00)\n",
      "Iter 2733 | Time 62.1894(62.1168) | Bit/dim 3.7157(3.7151) | Xent 0.2484(0.2503) | Loss 3.8399(3.8403) | Error 0.0919(0.0910) Steps 694(700.58) | Grad Norm 0.9227(1.2545) | Total Time 14.00(14.00)\n",
      "Iter 2734 | Time 61.9018(62.1104) | Bit/dim 3.7194(3.7153) | Xent 0.2449(0.2501) | Loss 3.8418(3.8403) | Error 0.0886(0.0910) Steps 694(700.39) | Grad Norm 1.0570(1.2485) | Total Time 14.00(14.00)\n",
      "Iter 2735 | Time 66.2475(62.2345) | Bit/dim 3.7163(3.7153) | Xent 0.2437(0.2499) | Loss 3.8381(3.8402) | Error 0.0889(0.0909) Steps 694(700.19) | Grad Norm 1.1097(1.2444) | Total Time 14.00(14.00)\n",
      "Iter 2736 | Time 60.4022(62.1795) | Bit/dim 3.7118(3.7152) | Xent 0.2488(0.2499) | Loss 3.8361(3.8401) | Error 0.0905(0.0909) Steps 694(700.01) | Grad Norm 0.8169(1.2316) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 25.4214, Epoch Time 416.4849(412.7950), Bit/dim 3.7301(best: 3.7297), Xent 2.1965, Loss 4.8284, Error 0.4285(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2737 | Time 63.3164(62.2136) | Bit/dim 3.7165(3.7152) | Xent 0.2444(0.2497) | Loss 3.8387(3.8401) | Error 0.0880(0.0908) Steps 694(699.83) | Grad Norm 0.8658(1.2206) | Total Time 14.00(14.00)\n",
      "Iter 2738 | Time 61.5548(62.1939) | Bit/dim 3.7261(3.7155) | Xent 0.2428(0.2495) | Loss 3.8475(3.8403) | Error 0.0881(0.0907) Steps 694(699.65) | Grad Norm 1.6539(1.2336) | Total Time 14.00(14.00)\n",
      "Iter 2739 | Time 61.0003(62.1581) | Bit/dim 3.7088(3.7153) | Xent 0.2479(0.2494) | Loss 3.8328(3.8401) | Error 0.0896(0.0907) Steps 694(699.48) | Grad Norm 1.0869(1.2292) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 62.5013(62.1684) | Bit/dim 3.7096(3.7152) | Xent 0.2466(0.2494) | Loss 3.8329(3.8399) | Error 0.0924(0.0907) Steps 694(699.32) | Grad Norm 0.8507(1.2178) | Total Time 14.00(14.00)\n",
      "Iter 2741 | Time 61.0347(62.1343) | Bit/dim 3.7248(3.7155) | Xent 0.2500(0.2494) | Loss 3.8498(3.8402) | Error 0.0877(0.0906) Steps 706(699.52) | Grad Norm 1.5218(1.2269) | Total Time 14.00(14.00)\n",
      "Iter 2742 | Time 62.8356(62.1554) | Bit/dim 3.7047(3.7151) | Xent 0.2499(0.2494) | Loss 3.8296(3.8398) | Error 0.0920(0.0907) Steps 700(699.53) | Grad Norm 1.0774(1.2225) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 25.5215, Epoch Time 413.1498(412.8056), Bit/dim 3.7298(best: 3.7297), Xent 2.1968, Loss 4.8282, Error 0.4262(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2743 | Time 61.3110(62.1301) | Bit/dim 3.7183(3.7152) | Xent 0.2587(0.2497) | Loss 3.8476(3.8401) | Error 0.0936(0.0908) Steps 700(699.55) | Grad Norm 1.4514(1.2293) | Total Time 14.00(14.00)\n",
      "Iter 2744 | Time 59.9474(62.0646) | Bit/dim 3.7269(3.7156) | Xent 0.2305(0.2491) | Loss 3.8421(3.8401) | Error 0.0825(0.0905) Steps 700(699.56) | Grad Norm 1.5667(1.2395) | Total Time 14.00(14.00)\n",
      "Iter 2745 | Time 63.7169(62.1141) | Bit/dim 3.7116(3.7155) | Xent 0.2411(0.2489) | Loss 3.8321(3.8399) | Error 0.0885(0.0905) Steps 694(699.39) | Grad Norm 1.6852(1.2528) | Total Time 14.00(14.00)\n",
      "Iter 2746 | Time 63.6819(62.1612) | Bit/dim 3.7102(3.7153) | Xent 0.2395(0.2486) | Loss 3.8300(3.8396) | Error 0.0879(0.0904) Steps 700(699.41) | Grad Norm 1.8979(1.2722) | Total Time 14.00(14.00)\n",
      "Iter 2747 | Time 62.5561(62.1730) | Bit/dim 3.7118(3.7152) | Xent 0.2331(0.2481) | Loss 3.8284(3.8393) | Error 0.0841(0.0902) Steps 706(699.61) | Grad Norm 2.1118(1.2974) | Total Time 14.00(14.00)\n",
      "Iter 2748 | Time 62.4362(62.1809) | Bit/dim 3.7101(3.7151) | Xent 0.2500(0.2482) | Loss 3.8351(3.8391) | Error 0.0930(0.0903) Steps 700(699.62) | Grad Norm 2.3184(1.3280) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 25.2690, Epoch Time 414.1504(412.8460), Bit/dim 3.7298(best: 3.7297), Xent 2.2005, Loss 4.8301, Error 0.4282(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2749 | Time 62.7108(62.1968) | Bit/dim 3.7221(3.7153) | Xent 0.2453(0.2481) | Loss 3.8447(3.8393) | Error 0.0875(0.0902) Steps 694(699.45) | Grad Norm 0.9864(1.3177) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 61.4112(62.1732) | Bit/dim 3.7184(3.7154) | Xent 0.2414(0.2479) | Loss 3.8392(3.8393) | Error 0.0865(0.0901) Steps 694(699.29) | Grad Norm 1.4277(1.3210) | Total Time 14.00(14.00)\n",
      "Iter 2751 | Time 61.2491(62.1455) | Bit/dim 3.7195(3.7155) | Xent 0.2420(0.2477) | Loss 3.8405(3.8393) | Error 0.0877(0.0900) Steps 706(699.49) | Grad Norm 1.4045(1.3235) | Total Time 14.00(14.00)\n",
      "Iter 2752 | Time 59.8195(62.0757) | Bit/dim 3.7088(3.7153) | Xent 0.2360(0.2474) | Loss 3.8269(3.8390) | Error 0.0863(0.0899) Steps 694(699.33) | Grad Norm 1.2415(1.3211) | Total Time 14.00(14.00)\n",
      "Iter 2753 | Time 61.8724(62.0696) | Bit/dim 3.7120(3.7152) | Xent 0.2317(0.2469) | Loss 3.8278(3.8386) | Error 0.0845(0.0897) Steps 700(699.35) | Grad Norm 1.2822(1.3199) | Total Time 14.00(14.00)\n",
      "Iter 2754 | Time 60.3516(62.0181) | Bit/dim 3.7092(3.7150) | Xent 0.2439(0.2468) | Loss 3.8311(3.8384) | Error 0.0894(0.0897) Steps 706(699.55) | Grad Norm 1.3560(1.3210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 25.3560, Epoch Time 408.2498(412.7081), Bit/dim 3.7305(best: 3.7297), Xent 2.2139, Loss 4.8374, Error 0.4318(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2755 | Time 62.6364(62.0367) | Bit/dim 3.7160(3.7150) | Xent 0.2433(0.2467) | Loss 3.8377(3.8384) | Error 0.0923(0.0898) Steps 718(700.10) | Grad Norm 1.3713(1.3225) | Total Time 14.00(14.00)\n",
      "Iter 2756 | Time 67.8314(62.2105) | Bit/dim 3.7160(3.7151) | Xent 0.2446(0.2466) | Loss 3.8383(3.8384) | Error 0.0876(0.0897) Steps 700(700.10) | Grad Norm 1.6133(1.3312) | Total Time 14.00(14.00)\n",
      "Iter 2757 | Time 61.2867(62.1828) | Bit/dim 3.7189(3.7152) | Xent 0.2436(0.2465) | Loss 3.8407(3.8384) | Error 0.0886(0.0897) Steps 694(699.91) | Grad Norm 1.6442(1.3406) | Total Time 14.00(14.00)\n",
      "Iter 2758 | Time 61.8924(62.1741) | Bit/dim 3.7155(3.7152) | Xent 0.2379(0.2463) | Loss 3.8344(3.8383) | Error 0.0875(0.0896) Steps 700(699.92) | Grad Norm 1.2615(1.3383) | Total Time 14.00(14.00)\n",
      "Iter 2759 | Time 61.8052(62.1630) | Bit/dim 3.7136(3.7151) | Xent 0.2474(0.2463) | Loss 3.8373(3.8383) | Error 0.0926(0.0897) Steps 706(700.10) | Grad Norm 1.2083(1.3344) | Total Time 14.00(14.00)\n",
      "Iter 2760 | Time 64.7174(62.2396) | Bit/dim 3.7098(3.7150) | Xent 0.2493(0.2464) | Loss 3.8344(3.8382) | Error 0.0920(0.0898) Steps 706(700.28) | Grad Norm 1.0427(1.3256) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 25.5220, Epoch Time 420.9883(412.9565), Bit/dim 3.7310(best: 3.7297), Xent 2.2094, Loss 4.8357, Error 0.4304(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2761 | Time 63.0283(62.2633) | Bit/dim 3.7156(3.7150) | Xent 0.2373(0.2461) | Loss 3.8342(3.8381) | Error 0.0843(0.0896) Steps 718(700.81) | Grad Norm 1.1210(1.3195) | Total Time 14.00(14.00)\n",
      "Iter 2762 | Time 63.2883(62.2940) | Bit/dim 3.7128(3.7149) | Xent 0.2475(0.2462) | Loss 3.8366(3.8380) | Error 0.0906(0.0897) Steps 700(700.78) | Grad Norm 1.3471(1.3203) | Total Time 14.00(14.00)\n",
      "Iter 2763 | Time 61.9590(62.2840) | Bit/dim 3.7149(3.7149) | Xent 0.2439(0.2461) | Loss 3.8369(3.8380) | Error 0.0880(0.0896) Steps 700(700.76) | Grad Norm 1.1537(1.3153) | Total Time 14.00(14.00)\n",
      "Iter 2764 | Time 62.7421(62.2977) | Bit/dim 3.7193(3.7151) | Xent 0.2484(0.2462) | Loss 3.8435(3.8381) | Error 0.0929(0.0897) Steps 712(701.10) | Grad Norm 2.2014(1.3419) | Total Time 14.00(14.00)\n",
      "Iter 2765 | Time 59.5761(62.2161) | Bit/dim 3.7056(3.7148) | Xent 0.2443(0.2461) | Loss 3.8278(3.8378) | Error 0.0904(0.0897) Steps 700(701.06) | Grad Norm 1.3198(1.3412) | Total Time 14.00(14.00)\n",
      "Iter 2766 | Time 61.7710(62.2027) | Bit/dim 3.7110(3.7147) | Xent 0.2444(0.2461) | Loss 3.8332(3.8377) | Error 0.0863(0.0896) Steps 700(701.03) | Grad Norm 0.9313(1.3289) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 25.5210, Epoch Time 413.2690(412.9659), Bit/dim 3.7302(best: 3.7297), Xent 2.2058, Loss 4.8332, Error 0.4288(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2767 | Time 61.8804(62.1931) | Bit/dim 3.7158(3.7147) | Xent 0.2549(0.2463) | Loss 3.8432(3.8379) | Error 0.0939(0.0898) Steps 700(701.00) | Grad Norm 1.1077(1.3223) | Total Time 14.00(14.00)\n",
      "Iter 2768 | Time 61.1950(62.1631) | Bit/dim 3.7217(3.7149) | Xent 0.2351(0.2460) | Loss 3.8393(3.8379) | Error 0.0866(0.0897) Steps 688(700.61) | Grad Norm 2.1567(1.3473) | Total Time 14.00(14.00)\n",
      "Iter 2769 | Time 60.2977(62.1072) | Bit/dim 3.7116(3.7148) | Xent 0.2408(0.2458) | Loss 3.8320(3.8377) | Error 0.0894(0.0897) Steps 694(700.41) | Grad Norm 1.1743(1.3421) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 63.9102(62.1613) | Bit/dim 3.7126(3.7147) | Xent 0.2380(0.2456) | Loss 3.8316(3.8375) | Error 0.0859(0.0895) Steps 706(700.58) | Grad Norm 1.7678(1.3549) | Total Time 14.00(14.00)\n",
      "Iter 2771 | Time 64.5170(62.2319) | Bit/dim 3.7144(3.7147) | Xent 0.2361(0.2453) | Loss 3.8325(3.8374) | Error 0.0877(0.0895) Steps 706(700.74) | Grad Norm 1.0151(1.3447) | Total Time 14.00(14.00)\n",
      "Iter 2772 | Time 62.2438(62.2323) | Bit/dim 3.7101(3.7146) | Xent 0.2393(0.2451) | Loss 3.8298(3.8372) | Error 0.0866(0.0894) Steps 688(700.36) | Grad Norm 0.9317(1.3323) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 25.9025, Epoch Time 415.3217(413.0365), Bit/dim 3.7302(best: 3.7297), Xent 2.2032, Loss 4.8318, Error 0.4272(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2773 | Time 63.0332(62.2563) | Bit/dim 3.7160(3.7146) | Xent 0.2295(0.2447) | Loss 3.8308(3.8370) | Error 0.0866(0.0893) Steps 706(700.53) | Grad Norm 1.1478(1.3268) | Total Time 14.00(14.00)\n",
      "Iter 2774 | Time 61.2523(62.2262) | Bit/dim 3.7174(3.7147) | Xent 0.2377(0.2445) | Loss 3.8363(3.8369) | Error 0.0854(0.0892) Steps 700(700.51) | Grad Norm 1.0011(1.3170) | Total Time 14.00(14.00)\n",
      "Iter 2775 | Time 63.3277(62.2592) | Bit/dim 3.7085(3.7145) | Xent 0.2505(0.2446) | Loss 3.8337(3.8369) | Error 0.0885(0.0892) Steps 694(700.32) | Grad Norm 0.9989(1.3075) | Total Time 14.00(14.00)\n",
      "Iter 2776 | Time 62.7757(62.2747) | Bit/dim 3.7047(3.7142) | Xent 0.2307(0.2442) | Loss 3.8200(3.8363) | Error 0.0856(0.0891) Steps 706(700.49) | Grad Norm 1.5802(1.3156) | Total Time 14.00(14.00)\n",
      "Iter 2777 | Time 62.6768(62.2868) | Bit/dim 3.7168(3.7143) | Xent 0.2466(0.2443) | Loss 3.8401(3.8365) | Error 0.0879(0.0890) Steps 694(700.29) | Grad Norm 1.1089(1.3094) | Total Time 14.00(14.00)\n",
      "Iter 2778 | Time 61.8173(62.2727) | Bit/dim 3.7225(3.7146) | Xent 0.2485(0.2444) | Loss 3.8468(3.8368) | Error 0.0895(0.0890) Steps 688(699.93) | Grad Norm 0.8070(1.2944) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 25.5056, Epoch Time 415.8629(413.1213), Bit/dim 3.7311(best: 3.7297), Xent 2.2352, Loss 4.8487, Error 0.4323(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2779 | Time 61.9897(62.2642) | Bit/dim 3.7168(3.7146) | Xent 0.2505(0.2446) | Loss 3.8420(3.8369) | Error 0.0886(0.0890) Steps 700(699.93) | Grad Norm 1.5764(1.3028) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 62.6893(62.2770) | Bit/dim 3.7151(3.7146) | Xent 0.2356(0.2443) | Loss 3.8328(3.8368) | Error 0.0867(0.0890) Steps 718(700.47) | Grad Norm 1.5448(1.3101) | Total Time 14.00(14.00)\n",
      "Iter 2781 | Time 59.4156(62.1911) | Bit/dim 3.7149(3.7146) | Xent 0.2446(0.2443) | Loss 3.8372(3.8368) | Error 0.0946(0.0891) Steps 700(700.46) | Grad Norm 1.0042(1.3009) | Total Time 14.00(14.00)\n",
      "Iter 2782 | Time 61.5155(62.1709) | Bit/dim 3.7123(3.7146) | Xent 0.2349(0.2441) | Loss 3.8298(3.8366) | Error 0.0880(0.0891) Steps 706(700.62) | Grad Norm 1.8768(1.3182) | Total Time 14.00(14.00)\n",
      "Iter 2783 | Time 63.4163(62.2082) | Bit/dim 3.7137(3.7145) | Xent 0.2383(0.2439) | Loss 3.8329(3.8365) | Error 0.0890(0.0891) Steps 694(700.42) | Grad Norm 1.2700(1.3167) | Total Time 14.00(14.00)\n",
      "Iter 2784 | Time 60.8390(62.1671) | Bit/dim 3.7129(3.7145) | Xent 0.2355(0.2436) | Loss 3.8306(3.8363) | Error 0.0849(0.0890) Steps 694(700.23) | Grad Norm 1.2704(1.3154) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 25.5026, Epoch Time 410.8377(413.0528), Bit/dim 3.7309(best: 3.7297), Xent 2.2260, Loss 4.8439, Error 0.4303(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2785 | Time 63.2883(62.2008) | Bit/dim 3.7131(3.7145) | Xent 0.2317(0.2433) | Loss 3.8289(3.8361) | Error 0.0870(0.0889) Steps 700(700.22) | Grad Norm 1.0106(1.3062) | Total Time 14.00(14.00)\n",
      "Iter 2786 | Time 61.4550(62.1784) | Bit/dim 3.7183(3.7146) | Xent 0.2496(0.2435) | Loss 3.8431(3.8363) | Error 0.0866(0.0888) Steps 712(700.58) | Grad Norm 1.7646(1.3200) | Total Time 14.00(14.00)\n",
      "Iter 2787 | Time 62.8068(62.1973) | Bit/dim 3.7159(3.7146) | Xent 0.2448(0.2435) | Loss 3.8384(3.8364) | Error 0.0871(0.0888) Steps 724(701.28) | Grad Norm 1.1991(1.3163) | Total Time 14.00(14.00)\n",
      "Iter 2788 | Time 61.2166(62.1678) | Bit/dim 3.7142(3.7146) | Xent 0.2289(0.2431) | Loss 3.8286(3.8361) | Error 0.0841(0.0887) Steps 706(701.42) | Grad Norm 0.9364(1.3049) | Total Time 14.00(14.00)\n",
      "Iter 2789 | Time 59.7789(62.0962) | Bit/dim 3.7165(3.7147) | Xent 0.2444(0.2431) | Loss 3.8387(3.8362) | Error 0.0904(0.0887) Steps 700(701.38) | Grad Norm 1.0992(1.2988) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 62.9747(62.1225) | Bit/dim 3.7084(3.7145) | Xent 0.2420(0.2431) | Loss 3.8295(3.8360) | Error 0.0907(0.0888) Steps 700(701.34) | Grad Norm 1.4902(1.3045) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 25.6096, Epoch Time 412.4913(413.0360), Bit/dim 3.7299(best: 3.7297), Xent 2.2469, Loss 4.8533, Error 0.4299(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2791 | Time 62.3362(62.1289) | Bit/dim 3.7085(3.7143) | Xent 0.2342(0.2428) | Loss 3.8256(3.8357) | Error 0.0843(0.0886) Steps 694(701.12) | Grad Norm 1.3126(1.3048) | Total Time 14.00(14.00)\n",
      "Iter 2792 | Time 62.8596(62.1509) | Bit/dim 3.7094(3.7141) | Xent 0.2427(0.2428) | Loss 3.8308(3.8355) | Error 0.0863(0.0886) Steps 694(700.90) | Grad Norm 1.0909(1.2983) | Total Time 14.00(14.00)\n",
      "Iter 2793 | Time 62.3370(62.1564) | Bit/dim 3.7207(3.7143) | Xent 0.2372(0.2426) | Loss 3.8393(3.8357) | Error 0.0839(0.0884) Steps 706(701.06) | Grad Norm 0.8633(1.2853) | Total Time 14.00(14.00)\n",
      "Iter 2794 | Time 62.5799(62.1691) | Bit/dim 3.7149(3.7144) | Xent 0.2379(0.2425) | Loss 3.8338(3.8356) | Error 0.0856(0.0883) Steps 712(701.39) | Grad Norm 1.4441(1.2901) | Total Time 14.00(14.00)\n",
      "Iter 2795 | Time 64.4282(62.2369) | Bit/dim 3.7269(3.7147) | Xent 0.2302(0.2421) | Loss 3.8420(3.8358) | Error 0.0844(0.0882) Steps 700(701.34) | Grad Norm 1.9280(1.3092) | Total Time 14.00(14.00)\n",
      "Iter 2796 | Time 62.6340(62.2488) | Bit/dim 3.7045(3.7144) | Xent 0.2245(0.2416) | Loss 3.8168(3.8352) | Error 0.0793(0.0879) Steps 694(701.12) | Grad Norm 1.2229(1.3066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 25.3332, Epoch Time 417.7871(413.1785), Bit/dim 3.7299(best: 3.7297), Xent 2.2325, Loss 4.8461, Error 0.4324(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2797 | Time 62.0818(62.2438) | Bit/dim 3.7056(3.7142) | Xent 0.2506(0.2419) | Loss 3.8309(3.8351) | Error 0.0907(0.0880) Steps 694(700.91) | Grad Norm 1.0887(1.3001) | Total Time 14.00(14.00)\n",
      "Iter 2798 | Time 61.4351(62.2196) | Bit/dim 3.7131(3.7141) | Xent 0.2353(0.2417) | Loss 3.8307(3.8350) | Error 0.0856(0.0880) Steps 712(701.24) | Grad Norm 0.9941(1.2909) | Total Time 14.00(14.00)\n",
      "Iter 2799 | Time 62.4841(62.2275) | Bit/dim 3.7172(3.7142) | Xent 0.2253(0.2412) | Loss 3.8298(3.8348) | Error 0.0814(0.0878) Steps 694(701.02) | Grad Norm 1.2453(1.2895) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 65.3050(62.3198) | Bit/dim 3.7099(3.7141) | Xent 0.2461(0.2413) | Loss 3.8329(3.8348) | Error 0.0897(0.0878) Steps 694(700.81) | Grad Norm 1.1716(1.2860) | Total Time 14.00(14.00)\n",
      "Iter 2801 | Time 62.5799(62.3276) | Bit/dim 3.7177(3.7142) | Xent 0.2433(0.2414) | Loss 3.8394(3.8349) | Error 0.0873(0.0878) Steps 718(701.33) | Grad Norm 1.2631(1.2853) | Total Time 14.00(14.00)\n",
      "Iter 2802 | Time 61.3722(62.2990) | Bit/dim 3.7228(3.7145) | Xent 0.2361(0.2412) | Loss 3.8409(3.8351) | Error 0.0877(0.0878) Steps 700(701.29) | Grad Norm 1.0115(1.2771) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 25.7320, Epoch Time 416.4614(413.2770), Bit/dim 3.7309(best: 3.7297), Xent 2.2545, Loss 4.8581, Error 0.4308(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2803 | Time 61.1593(62.2648) | Bit/dim 3.7214(3.7147) | Xent 0.2324(0.2410) | Loss 3.8375(3.8351) | Error 0.0864(0.0878) Steps 706(701.43) | Grad Norm 1.1333(1.2728) | Total Time 14.00(14.00)\n",
      "Iter 2804 | Time 62.4176(62.2694) | Bit/dim 3.7192(3.7148) | Xent 0.2359(0.2408) | Loss 3.8372(3.8352) | Error 0.0859(0.0877) Steps 706(701.57) | Grad Norm 1.3421(1.2748) | Total Time 14.00(14.00)\n",
      "Iter 2805 | Time 60.6257(62.2200) | Bit/dim 3.7080(3.7146) | Xent 0.2447(0.2409) | Loss 3.8303(3.8351) | Error 0.0880(0.0877) Steps 712(701.88) | Grad Norm 0.7891(1.2603) | Total Time 14.00(14.00)\n",
      "Iter 2806 | Time 66.7728(62.3566) | Bit/dim 3.7134(3.7146) | Xent 0.2368(0.2408) | Loss 3.8318(3.8350) | Error 0.0867(0.0877) Steps 694(701.64) | Grad Norm 1.0333(1.2535) | Total Time 14.00(14.00)\n",
      "Iter 2807 | Time 64.3793(62.4173) | Bit/dim 3.7089(3.7144) | Xent 0.2322(0.2405) | Loss 3.8250(3.8347) | Error 0.0874(0.0877) Steps 712(701.96) | Grad Norm 1.3314(1.2558) | Total Time 14.00(14.00)\n",
      "Iter 2808 | Time 63.4567(62.4485) | Bit/dim 3.7075(3.7142) | Xent 0.2466(0.2407) | Loss 3.8308(3.8345) | Error 0.0903(0.0878) Steps 688(701.54) | Grad Norm 1.5479(1.2646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 25.6710, Epoch Time 419.7558(413.4714), Bit/dim 3.7299(best: 3.7297), Xent 2.2422, Loss 4.8510, Error 0.4294(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2809 | Time 62.2022(62.4411) | Bit/dim 3.7142(3.7142) | Xent 0.2356(0.2406) | Loss 3.8320(3.8345) | Error 0.0876(0.0877) Steps 694(701.31) | Grad Norm 1.2814(1.2651) | Total Time 14.00(14.00)\n",
      "Iter 2810 | Time 61.5208(62.4135) | Bit/dim 3.7053(3.7139) | Xent 0.2177(0.2399) | Loss 3.8142(3.8339) | Error 0.0790(0.0875) Steps 712(701.63) | Grad Norm 1.1542(1.2617) | Total Time 14.00(14.00)\n",
      "Iter 2811 | Time 65.5670(62.5081) | Bit/dim 3.7057(3.7137) | Xent 0.2459(0.2401) | Loss 3.8287(3.8337) | Error 0.0883(0.0875) Steps 694(701.40) | Grad Norm 1.4159(1.2664) | Total Time 14.00(14.00)\n",
      "Iter 2812 | Time 62.6358(62.5119) | Bit/dim 3.7196(3.7139) | Xent 0.2397(0.2401) | Loss 3.8395(3.8339) | Error 0.0873(0.0875) Steps 700(701.36) | Grad Norm 1.3487(1.2688) | Total Time 14.00(14.00)\n",
      "Iter 2813 | Time 60.6519(62.4561) | Bit/dim 3.7157(3.7139) | Xent 0.2360(0.2399) | Loss 3.8337(3.8339) | Error 0.0845(0.0874) Steps 712(701.68) | Grad Norm 1.5309(1.2767) | Total Time 14.00(14.00)\n",
      "Iter 2814 | Time 63.4200(62.4850) | Bit/dim 3.7162(3.7140) | Xent 0.2369(0.2398) | Loss 3.8347(3.8339) | Error 0.0849(0.0873) Steps 694(701.45) | Grad Norm 1.1836(1.2739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 25.4610, Epoch Time 417.1868(413.5828), Bit/dim 3.7299(best: 3.7297), Xent 2.2520, Loss 4.8559, Error 0.4307(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2815 | Time 60.1722(62.4157) | Bit/dim 3.7054(3.7137) | Xent 0.2384(0.2398) | Loss 3.8246(3.8336) | Error 0.0884(0.0874) Steps 706(701.59) | Grad Norm 1.4137(1.2781) | Total Time 14.00(14.00)\n",
      "Iter 2816 | Time 62.8580(62.4289) | Bit/dim 3.7276(3.7141) | Xent 0.2408(0.2398) | Loss 3.8480(3.8341) | Error 0.0847(0.0873) Steps 700(701.54) | Grad Norm 1.7585(1.2925) | Total Time 14.00(14.00)\n",
      "Iter 2817 | Time 62.4266(62.4289) | Bit/dim 3.7143(3.7141) | Xent 0.2360(0.2397) | Loss 3.8323(3.8340) | Error 0.0856(0.0872) Steps 700(701.49) | Grad Norm 1.5503(1.3002) | Total Time 14.00(14.00)\n",
      "Iter 2818 | Time 63.5118(62.4613) | Bit/dim 3.7170(3.7142) | Xent 0.2301(0.2394) | Loss 3.8321(3.8339) | Error 0.0854(0.0872) Steps 694(701.27) | Grad Norm 1.5390(1.3074) | Total Time 14.00(14.00)\n",
      "Iter 2819 | Time 63.6929(62.4983) | Bit/dim 3.7001(3.7138) | Xent 0.2436(0.2396) | Loss 3.8219(3.8336) | Error 0.0876(0.0872) Steps 694(701.05) | Grad Norm 1.3788(1.3096) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 60.2151(62.4298) | Bit/dim 3.7145(3.7138) | Xent 0.2230(0.2391) | Loss 3.8260(3.8334) | Error 0.0774(0.0869) Steps 718(701.56) | Grad Norm 2.0480(1.3317) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 25.4881, Epoch Time 413.6696(413.5854), Bit/dim 3.7305(best: 3.7297), Xent 2.2634, Loss 4.8622, Error 0.4327(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2821 | Time 64.4675(62.4909) | Bit/dim 3.7115(3.7138) | Xent 0.2334(0.2389) | Loss 3.8281(3.8332) | Error 0.0829(0.0868) Steps 712(701.87) | Grad Norm 1.8347(1.3468) | Total Time 14.00(14.00)\n",
      "Iter 2822 | Time 63.8480(62.5316) | Bit/dim 3.7084(3.7136) | Xent 0.2318(0.2387) | Loss 3.8243(3.8329) | Error 0.0844(0.0867) Steps 700(701.81) | Grad Norm 1.3471(1.3468) | Total Time 14.00(14.00)\n",
      "Iter 2823 | Time 62.1628(62.5206) | Bit/dim 3.7285(3.7140) | Xent 0.2319(0.2385) | Loss 3.8445(3.8333) | Error 0.0819(0.0866) Steps 694(701.58) | Grad Norm 1.5449(1.3527) | Total Time 14.00(14.00)\n",
      "Iter 2824 | Time 62.9176(62.5325) | Bit/dim 3.7182(3.7142) | Xent 0.2355(0.2384) | Loss 3.8359(3.8334) | Error 0.0826(0.0864) Steps 706(701.71) | Grad Norm 1.0352(1.3432) | Total Time 14.00(14.00)\n",
      "Iter 2825 | Time 62.9051(62.5437) | Bit/dim 3.7106(3.7141) | Xent 0.2415(0.2385) | Loss 3.8314(3.8333) | Error 0.0864(0.0864) Steps 700(701.66) | Grad Norm 1.5878(1.3506) | Total Time 14.00(14.00)\n",
      "Iter 2826 | Time 61.5959(62.5152) | Bit/dim 3.7058(3.7138) | Xent 0.2363(0.2384) | Loss 3.8240(3.8330) | Error 0.0837(0.0864) Steps 694(701.43) | Grad Norm 2.2663(1.3780) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 25.1877, Epoch Time 418.5123(413.7332), Bit/dim 3.7310(best: 3.7297), Xent 2.2590, Loss 4.8605, Error 0.4314(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2827 | Time 60.6063(62.4580) | Bit/dim 3.7141(3.7138) | Xent 0.2343(0.2383) | Loss 3.8313(3.8330) | Error 0.0876(0.0864) Steps 706(701.57) | Grad Norm 1.2492(1.3742) | Total Time 14.00(14.00)\n",
      "Iter 2828 | Time 61.6330(62.4332) | Bit/dim 3.7130(3.7138) | Xent 0.2178(0.2377) | Loss 3.8219(3.8326) | Error 0.0793(0.0862) Steps 712(701.88) | Grad Norm 1.4415(1.3762) | Total Time 14.00(14.00)\n",
      "Iter 2829 | Time 61.3919(62.4020) | Bit/dim 3.7121(3.7137) | Xent 0.2438(0.2379) | Loss 3.8340(3.8327) | Error 0.0889(0.0863) Steps 700(701.83) | Grad Norm 1.6712(1.3850) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 60.6501(62.3494) | Bit/dim 3.7106(3.7137) | Xent 0.2368(0.2378) | Loss 3.8290(3.8326) | Error 0.0876(0.0863) Steps 706(701.95) | Grad Norm 1.6501(1.3930) | Total Time 14.00(14.00)\n",
      "Iter 2831 | Time 64.8832(62.4254) | Bit/dim 3.7191(3.7138) | Xent 0.2455(0.2381) | Loss 3.8419(3.8328) | Error 0.0915(0.0865) Steps 706(702.07) | Grad Norm 1.3945(1.3930) | Total Time 14.00(14.00)\n",
      "Iter 2832 | Time 61.8976(62.4096) | Bit/dim 3.7100(3.7137) | Xent 0.2337(0.2379) | Loss 3.8268(3.8327) | Error 0.0836(0.0864) Steps 694(701.83) | Grad Norm 1.0742(1.3835) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 25.3876, Epoch Time 411.9233(413.6789), Bit/dim 3.7300(best: 3.7297), Xent 2.2620, Loss 4.8610, Error 0.4306(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2833 | Time 61.5174(62.3828) | Bit/dim 3.7038(3.7134) | Xent 0.2292(0.2377) | Loss 3.8185(3.8322) | Error 0.0824(0.0863) Steps 706(701.95) | Grad Norm 1.1548(1.3766) | Total Time 14.00(14.00)\n",
      "Iter 2834 | Time 62.2006(62.3774) | Bit/dim 3.7148(3.7134) | Xent 0.2495(0.2380) | Loss 3.8396(3.8325) | Error 0.0915(0.0864) Steps 688(701.54) | Grad Norm 1.0841(1.3678) | Total Time 14.00(14.00)\n",
      "Iter 2835 | Time 63.5647(62.4130) | Bit/dim 3.7115(3.7134) | Xent 0.2329(0.2379) | Loss 3.8279(3.8323) | Error 0.0843(0.0863) Steps 700(701.49) | Grad Norm 1.2164(1.3633) | Total Time 14.00(14.00)\n",
      "Iter 2836 | Time 62.7950(62.4244) | Bit/dim 3.7187(3.7135) | Xent 0.2377(0.2379) | Loss 3.8375(3.8325) | Error 0.0847(0.0863) Steps 694(701.27) | Grad Norm 1.0629(1.3543) | Total Time 14.00(14.00)\n",
      "Iter 2837 | Time 62.0494(62.4132) | Bit/dim 3.7292(3.7140) | Xent 0.2415(0.2380) | Loss 3.8499(3.8330) | Error 0.0867(0.0863) Steps 700(701.23) | Grad Norm 0.9109(1.3410) | Total Time 14.00(14.00)\n",
      "Iter 2838 | Time 62.0700(62.4029) | Bit/dim 3.7033(3.7137) | Xent 0.2242(0.2376) | Loss 3.8154(3.8325) | Error 0.0834(0.0862) Steps 718(701.73) | Grad Norm 1.3792(1.3421) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 25.3913, Epoch Time 415.2130(413.7250), Bit/dim 3.7290(best: 3.7297), Xent 2.2563, Loss 4.8572, Error 0.4284(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2839 | Time 59.5116(62.3162) | Bit/dim 3.7075(3.7135) | Xent 0.2380(0.2376) | Loss 3.8264(3.8323) | Error 0.0881(0.0863) Steps 700(701.68) | Grad Norm 1.0729(1.3340) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 62.0970(62.3096) | Bit/dim 3.7095(3.7134) | Xent 0.2258(0.2372) | Loss 3.8223(3.8320) | Error 0.0821(0.0862) Steps 700(701.63) | Grad Norm 1.1667(1.3290) | Total Time 14.00(14.00)\n",
      "Iter 2841 | Time 63.0589(62.3321) | Bit/dim 3.7103(3.7133) | Xent 0.2331(0.2371) | Loss 3.8268(3.8318) | Error 0.0870(0.0862) Steps 700(701.58) | Grad Norm 1.9267(1.3470) | Total Time 14.00(14.00)\n",
      "Iter 2842 | Time 61.0333(62.2931) | Bit/dim 3.7249(3.7136) | Xent 0.2324(0.2370) | Loss 3.8410(3.8321) | Error 0.0860(0.0862) Steps 706(701.71) | Grad Norm 0.9303(1.3345) | Total Time 14.00(14.00)\n",
      "Iter 2843 | Time 62.0494(62.2858) | Bit/dim 3.7124(3.7136) | Xent 0.2349(0.2369) | Loss 3.8298(3.8320) | Error 0.0846(0.0861) Steps 694(701.48) | Grad Norm 1.0839(1.3269) | Total Time 14.00(14.00)\n",
      "Iter 2844 | Time 62.5517(62.2938) | Bit/dim 3.7170(3.7137) | Xent 0.2347(0.2368) | Loss 3.8344(3.8321) | Error 0.0874(0.0862) Steps 700(701.44) | Grad Norm 1.6754(1.3374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 25.4405, Epoch Time 411.0105(413.6435), Bit/dim 3.7299(best: 3.7290), Xent 2.2737, Loss 4.8668, Error 0.4316(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2845 | Time 61.6252(62.2737) | Bit/dim 3.7104(3.7136) | Xent 0.2318(0.2367) | Loss 3.8263(3.8319) | Error 0.0814(0.0860) Steps 688(701.03) | Grad Norm 0.9875(1.3269) | Total Time 14.00(14.00)\n",
      "Iter 2846 | Time 61.9687(62.2646) | Bit/dim 3.7163(3.7137) | Xent 0.2353(0.2366) | Loss 3.8340(3.8320) | Error 0.0845(0.0860) Steps 706(701.18) | Grad Norm 1.6491(1.3366) | Total Time 14.00(14.00)\n",
      "Iter 2847 | Time 64.1537(62.3212) | Bit/dim 3.7119(3.7136) | Xent 0.2358(0.2366) | Loss 3.8298(3.8319) | Error 0.0893(0.0861) Steps 706(701.33) | Grad Norm 1.9685(1.3555) | Total Time 14.00(14.00)\n",
      "Iter 2848 | Time 63.3927(62.3534) | Bit/dim 3.7084(3.7135) | Xent 0.2381(0.2367) | Loss 3.8274(3.8318) | Error 0.0871(0.0861) Steps 712(701.65) | Grad Norm 1.0009(1.3449) | Total Time 14.00(14.00)\n",
      "Iter 2849 | Time 63.0976(62.3757) | Bit/dim 3.7143(3.7135) | Xent 0.2394(0.2367) | Loss 3.8340(3.8319) | Error 0.0847(0.0861) Steps 694(701.42) | Grad Norm 1.8329(1.3595) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 63.8227(62.4191) | Bit/dim 3.7170(3.7136) | Xent 0.2301(0.2365) | Loss 3.8320(3.8319) | Error 0.0809(0.0859) Steps 718(701.92) | Grad Norm 1.4335(1.3617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 25.5010, Epoch Time 418.8147(413.7987), Bit/dim 3.7299(best: 3.7290), Xent 2.2885, Loss 4.8741, Error 0.4318(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2851 | Time 63.1334(62.4405) | Bit/dim 3.7144(3.7136) | Xent 0.2272(0.2363) | Loss 3.8280(3.8318) | Error 0.0815(0.0858) Steps 700(701.86) | Grad Norm 2.2978(1.3898) | Total Time 14.00(14.00)\n",
      "Iter 2852 | Time 64.0707(62.4894) | Bit/dim 3.7154(3.7137) | Xent 0.2322(0.2361) | Loss 3.8315(3.8318) | Error 0.0851(0.0858) Steps 700(701.80) | Grad Norm 1.5711(1.3953) | Total Time 14.00(14.00)\n",
      "Iter 2853 | Time 60.9764(62.4441) | Bit/dim 3.7150(3.7137) | Xent 0.2428(0.2363) | Loss 3.8364(3.8319) | Error 0.0909(0.0859) Steps 694(701.57) | Grad Norm 1.9038(1.4105) | Total Time 14.00(14.00)\n",
      "Iter 2854 | Time 65.5308(62.5367) | Bit/dim 3.7099(3.7136) | Xent 0.2338(0.2363) | Loss 3.8267(3.8317) | Error 0.0866(0.0859) Steps 712(701.88) | Grad Norm 1.6444(1.4175) | Total Time 14.00(14.00)\n",
      "Iter 2855 | Time 65.1889(62.6162) | Bit/dim 3.7106(3.7135) | Xent 0.2348(0.2362) | Loss 3.8280(3.8316) | Error 0.0851(0.0859) Steps 682(701.28) | Grad Norm 2.6545(1.4546) | Total Time 14.00(14.00)\n",
      "Iter 2856 | Time 64.5226(62.6734) | Bit/dim 3.7120(3.7135) | Xent 0.2202(0.2357) | Loss 3.8221(3.8313) | Error 0.0783(0.0857) Steps 718(701.79) | Grad Norm 1.6928(1.4618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 25.5454, Epoch Time 424.6615(414.1245), Bit/dim 3.7288(best: 3.7290), Xent 2.2518, Loss 4.8547, Error 0.4272(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2857 | Time 64.0838(62.7157) | Bit/dim 3.7118(3.7134) | Xent 0.2302(0.2356) | Loss 3.8269(3.8312) | Error 0.0817(0.0856) Steps 700(701.73) | Grad Norm 1.5329(1.4639) | Total Time 14.00(14.00)\n",
      "Iter 2858 | Time 62.3818(62.7057) | Bit/dim 3.7204(3.7136) | Xent 0.2270(0.2353) | Loss 3.8339(3.8313) | Error 0.0811(0.0854) Steps 706(701.86) | Grad Norm 2.0389(1.4812) | Total Time 14.00(14.00)\n",
      "Iter 2859 | Time 63.4591(62.7283) | Bit/dim 3.7110(3.7136) | Xent 0.2320(0.2352) | Loss 3.8270(3.8312) | Error 0.0820(0.0853) Steps 694(701.62) | Grad Norm 2.5197(1.5123) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 61.3603(62.6873) | Bit/dim 3.7036(3.7133) | Xent 0.2273(0.2350) | Loss 3.8172(3.8307) | Error 0.0864(0.0854) Steps 718(702.12) | Grad Norm 2.0717(1.5291) | Total Time 14.00(14.00)\n",
      "Iter 2861 | Time 61.4580(62.6504) | Bit/dim 3.7198(3.7135) | Xent 0.2314(0.2349) | Loss 3.8356(3.8309) | Error 0.0835(0.0853) Steps 706(702.23) | Grad Norm 2.0985(1.5462) | Total Time 14.00(14.00)\n",
      "Iter 2862 | Time 62.0851(62.6334) | Bit/dim 3.7115(3.7134) | Xent 0.2316(0.2348) | Loss 3.8273(3.8308) | Error 0.0861(0.0853) Steps 694(701.99) | Grad Norm 1.2384(1.5370) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 25.3974, Epoch Time 415.5814(414.1683), Bit/dim 3.7301(best: 3.7288), Xent 2.2826, Loss 4.8714, Error 0.4347(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2863 | Time 63.0868(62.6470) | Bit/dim 3.7180(3.7135) | Xent 0.2378(0.2349) | Loss 3.8369(3.8310) | Error 0.0873(0.0854) Steps 718(702.47) | Grad Norm 1.2841(1.5294) | Total Time 14.00(14.00)\n",
      "Iter 2864 | Time 63.9398(62.6858) | Bit/dim 3.7074(3.7133) | Xent 0.2329(0.2348) | Loss 3.8238(3.8307) | Error 0.0856(0.0854) Steps 712(702.75) | Grad Norm 2.1193(1.5471) | Total Time 14.00(14.00)\n",
      "Iter 2865 | Time 63.7941(62.7191) | Bit/dim 3.7119(3.7133) | Xent 0.2277(0.2346) | Loss 3.8258(3.8306) | Error 0.0833(0.0853) Steps 712(703.03) | Grad Norm 1.5393(1.5468) | Total Time 14.00(14.00)\n",
      "Iter 2866 | Time 60.2969(62.6464) | Bit/dim 3.7120(3.7133) | Xent 0.2331(0.2345) | Loss 3.8285(3.8305) | Error 0.0856(0.0853) Steps 694(702.76) | Grad Norm 1.4821(1.5449) | Total Time 14.00(14.00)\n",
      "Iter 2867 | Time 60.0606(62.5688) | Bit/dim 3.7172(3.7134) | Xent 0.2375(0.2346) | Loss 3.8359(3.8307) | Error 0.0843(0.0853) Steps 694(702.50) | Grad Norm 1.4486(1.5420) | Total Time 14.00(14.00)\n",
      "Iter 2868 | Time 60.5328(62.5077) | Bit/dim 3.7106(3.7133) | Xent 0.2205(0.2342) | Loss 3.8209(3.8304) | Error 0.0810(0.0852) Steps 718(702.96) | Grad Norm 1.2468(1.5331) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 25.6151, Epoch Time 412.6726(414.1234), Bit/dim 3.7291(best: 3.7288), Xent 2.3048, Loss 4.8815, Error 0.4320(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2869 | Time 62.6067(62.5107) | Bit/dim 3.7123(3.7133) | Xent 0.2326(0.2342) | Loss 3.8286(3.8304) | Error 0.0844(0.0852) Steps 700(702.87) | Grad Norm 1.8810(1.5436) | Total Time 14.00(14.00)\n",
      "Iter 2870 | Time 66.1838(62.6209) | Bit/dim 3.7126(3.7133) | Xent 0.2340(0.2342) | Loss 3.8296(3.8303) | Error 0.0859(0.0852) Steps 694(702.61) | Grad Norm 1.3677(1.5383) | Total Time 14.00(14.00)\n",
      "Iter 2871 | Time 62.0703(62.6044) | Bit/dim 3.7169(3.7134) | Xent 0.2348(0.2342) | Loss 3.8343(3.8305) | Error 0.0844(0.0851) Steps 700(702.53) | Grad Norm 1.3887(1.5338) | Total Time 14.00(14.00)\n",
      "Iter 2872 | Time 63.7987(62.6402) | Bit/dim 3.7174(3.7135) | Xent 0.2303(0.2341) | Loss 3.8326(3.8305) | Error 0.0826(0.0851) Steps 700(702.45) | Grad Norm 2.1201(1.5514) | Total Time 14.00(14.00)\n",
      "Iter 2873 | Time 61.8026(62.6151) | Bit/dim 3.7094(3.7134) | Xent 0.2401(0.2342) | Loss 3.8294(3.8305) | Error 0.0887(0.0852) Steps 706(702.56) | Grad Norm 2.0468(1.5663) | Total Time 14.00(14.00)\n",
      "Iter 2874 | Time 64.6950(62.6775) | Bit/dim 3.7126(3.7133) | Xent 0.2323(0.2342) | Loss 3.8287(3.8304) | Error 0.0849(0.0852) Steps 700(702.48) | Grad Norm 2.9092(1.6066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 25.3286, Epoch Time 421.4954(414.3445), Bit/dim 3.7293(best: 3.7288), Xent 2.3020, Loss 4.8802, Error 0.4294(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2875 | Time 60.6603(62.6170) | Bit/dim 3.7195(3.7135) | Xent 0.2250(0.2339) | Loss 3.8320(3.8305) | Error 0.0827(0.0851) Steps 694(702.23) | Grad Norm 1.3745(1.5996) | Total Time 14.00(14.00)\n",
      "Iter 2876 | Time 60.6720(62.5586) | Bit/dim 3.7158(3.7136) | Xent 0.2287(0.2337) | Loss 3.8301(3.8305) | Error 0.0826(0.0850) Steps 700(702.16) | Grad Norm 1.2056(1.5878) | Total Time 14.00(14.00)\n",
      "Iter 2877 | Time 59.6591(62.4716) | Bit/dim 3.7096(3.7135) | Xent 0.2307(0.2337) | Loss 3.8249(3.8303) | Error 0.0836(0.0850) Steps 712(702.46) | Grad Norm 2.5957(1.6180) | Total Time 14.00(14.00)\n",
      "Iter 2878 | Time 62.0513(62.4590) | Bit/dim 3.7083(3.7133) | Xent 0.2273(0.2335) | Loss 3.8219(3.8300) | Error 0.0807(0.0849) Steps 700(702.38) | Grad Norm 1.6871(1.6201) | Total Time 14.00(14.00)\n",
      "Iter 2879 | Time 64.7574(62.5280) | Bit/dim 3.7118(3.7133) | Xent 0.2243(0.2332) | Loss 3.8239(3.8299) | Error 0.0801(0.0847) Steps 712(702.67) | Grad Norm 1.8706(1.6276) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 62.9887(62.5418) | Bit/dim 3.7069(3.7131) | Xent 0.2331(0.2332) | Loss 3.8235(3.8297) | Error 0.0826(0.0847) Steps 688(702.23) | Grad Norm 2.1806(1.6442) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 25.3011, Epoch Time 411.5342(414.2602), Bit/dim 3.7303(best: 3.7288), Xent 2.3037, Loss 4.8821, Error 0.4317(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2881 | Time 62.1021(62.5286) | Bit/dim 3.7153(3.7131) | Xent 0.2219(0.2328) | Loss 3.8262(3.8296) | Error 0.0776(0.0844) Steps 718(702.70) | Grad Norm 2.7167(1.6764) | Total Time 14.00(14.00)\n",
      "Iter 2882 | Time 64.4648(62.5867) | Bit/dim 3.7072(3.7130) | Xent 0.2230(0.2326) | Loss 3.8187(3.8292) | Error 0.0814(0.0844) Steps 694(702.44) | Grad Norm 2.4233(1.6988) | Total Time 14.00(14.00)\n",
      "Iter 2883 | Time 60.4413(62.5223) | Bit/dim 3.7129(3.7130) | Xent 0.2183(0.2321) | Loss 3.8221(3.8290) | Error 0.0784(0.0842) Steps 718(702.91) | Grad Norm 0.9191(1.6754) | Total Time 14.00(14.00)\n",
      "Iter 2884 | Time 63.6530(62.5563) | Bit/dim 3.7102(3.7129) | Xent 0.2358(0.2322) | Loss 3.8281(3.8290) | Error 0.0849(0.0842) Steps 706(703.00) | Grad Norm 3.5745(1.7324) | Total Time 14.00(14.00)\n",
      "Iter 2885 | Time 62.4648(62.5535) | Bit/dim 3.7225(3.7132) | Xent 0.2302(0.2322) | Loss 3.8376(3.8293) | Error 0.0833(0.0842) Steps 724(703.63) | Grad Norm 2.3239(1.7501) | Total Time 14.00(14.00)\n",
      "Iter 2886 | Time 59.6338(62.4659) | Bit/dim 3.7094(3.7131) | Xent 0.2417(0.2325) | Loss 3.8303(3.8293) | Error 0.0869(0.0842) Steps 718(704.06) | Grad Norm 1.8119(1.7520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 25.5898, Epoch Time 413.8251(414.2472), Bit/dim 3.7295(best: 3.7288), Xent 2.2903, Loss 4.8746, Error 0.4295(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2887 | Time 62.5421(62.4682) | Bit/dim 3.7377(3.7138) | Xent 0.2245(0.2322) | Loss 3.8499(3.8299) | Error 0.0799(0.0841) Steps 706(704.12) | Grad Norm 2.5427(1.7757) | Total Time 14.00(14.00)\n",
      "Iter 2888 | Time 60.9667(62.4232) | Bit/dim 3.7105(3.7137) | Xent 0.2453(0.2326) | Loss 3.8332(3.8300) | Error 0.0894(0.0843) Steps 682(703.46) | Grad Norm 2.0434(1.7837) | Total Time 14.00(14.00)\n",
      "Iter 2889 | Time 61.5427(62.3967) | Bit/dim 3.7063(3.7135) | Xent 0.2360(0.2327) | Loss 3.8243(3.8298) | Error 0.0826(0.0842) Steps 706(703.53) | Grad Norm 1.2381(1.7673) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 62.6532(62.4044) | Bit/dim 3.7207(3.7137) | Xent 0.2303(0.2326) | Loss 3.8358(3.8300) | Error 0.0817(0.0841) Steps 718(703.97) | Grad Norm 2.1529(1.7789) | Total Time 14.00(14.00)\n",
      "Iter 2891 | Time 63.3208(62.4319) | Bit/dim 3.7031(3.7134) | Xent 0.2278(0.2325) | Loss 3.8170(3.8296) | Error 0.0855(0.0842) Steps 718(704.39) | Grad Norm 2.5653(1.8025) | Total Time 14.00(14.00)\n",
      "Iter 2892 | Time 59.6098(62.3473) | Bit/dim 3.7096(3.7133) | Xent 0.2346(0.2326) | Loss 3.8269(3.8295) | Error 0.0865(0.0843) Steps 688(703.90) | Grad Norm 2.1325(1.8124) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 25.3490, Epoch Time 411.3376(414.1599), Bit/dim 3.7305(best: 3.7288), Xent 2.2714, Loss 4.8661, Error 0.4285(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2893 | Time 62.1704(62.3420) | Bit/dim 3.7040(3.7130) | Xent 0.2319(0.2325) | Loss 3.8200(3.8293) | Error 0.0854(0.0843) Steps 712(704.14) | Grad Norm 1.6984(1.8090) | Total Time 14.00(14.00)\n",
      "Iter 2894 | Time 61.5196(62.3173) | Bit/dim 3.7195(3.7132) | Xent 0.2217(0.2322) | Loss 3.8304(3.8293) | Error 0.0814(0.0842) Steps 706(704.20) | Grad Norm 4.1177(1.8782) | Total Time 14.00(14.00)\n",
      "Iter 2895 | Time 63.7485(62.3602) | Bit/dim 3.7138(3.7132) | Xent 0.2268(0.2321) | Loss 3.8273(3.8292) | Error 0.0825(0.0842) Steps 718(704.61) | Grad Norm 2.9457(1.9103) | Total Time 14.00(14.00)\n",
      "Iter 2896 | Time 63.6536(62.3990) | Bit/dim 3.7153(3.7133) | Xent 0.2238(0.2318) | Loss 3.8272(3.8292) | Error 0.0845(0.0842) Steps 694(704.29) | Grad Norm 1.0621(1.8848) | Total Time 14.00(14.00)\n",
      "Iter 2897 | Time 63.1417(62.4213) | Bit/dim 3.7149(3.7133) | Xent 0.2259(0.2316) | Loss 3.8278(3.8291) | Error 0.0829(0.0841) Steps 700(704.16) | Grad Norm 1.8039(1.8824) | Total Time 14.00(14.00)\n",
      "Iter 2898 | Time 63.4292(62.4515) | Bit/dim 3.7147(3.7133) | Xent 0.2327(0.2317) | Loss 3.8310(3.8292) | Error 0.0834(0.0841) Steps 712(704.40) | Grad Norm 2.2585(1.8937) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 25.5082, Epoch Time 418.6825(414.2956), Bit/dim 3.7297(best: 3.7288), Xent 2.2911, Loss 4.8752, Error 0.4313(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2899 | Time 64.3222(62.5077) | Bit/dim 3.7138(3.7134) | Xent 0.2322(0.2317) | Loss 3.8299(3.8292) | Error 0.0815(0.0840) Steps 700(704.27) | Grad Norm 2.0069(1.8971) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 64.9840(62.5819) | Bit/dim 3.7029(3.7130) | Xent 0.2342(0.2318) | Loss 3.8200(3.8289) | Error 0.0846(0.0840) Steps 700(704.14) | Grad Norm 1.0529(1.8717) | Total Time 14.00(14.00)\n",
      "Iter 2901 | Time 63.5503(62.6110) | Bit/dim 3.7168(3.7132) | Xent 0.2236(0.2315) | Loss 3.8287(3.8289) | Error 0.0824(0.0840) Steps 706(704.19) | Grad Norm 1.8726(1.8718) | Total Time 14.00(14.00)\n",
      "Iter 2902 | Time 60.8000(62.5567) | Bit/dim 3.7149(3.7132) | Xent 0.2339(0.2316) | Loss 3.8319(3.8290) | Error 0.0851(0.0840) Steps 700(704.07) | Grad Norm 1.1906(1.8513) | Total Time 14.00(14.00)\n",
      "Iter 2903 | Time 64.7488(62.6224) | Bit/dim 3.7111(3.7132) | Xent 0.2280(0.2315) | Loss 3.8251(3.8289) | Error 0.0855(0.0841) Steps 694(703.77) | Grad Norm 1.5813(1.8432) | Total Time 14.00(14.00)\n",
      "Iter 2904 | Time 64.3049(62.6729) | Bit/dim 3.7146(3.7132) | Xent 0.2251(0.2313) | Loss 3.8272(3.8288) | Error 0.0811(0.0840) Steps 718(704.19) | Grad Norm 1.8425(1.8432) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 25.2032, Epoch Time 423.2479(414.5641), Bit/dim 3.7291(best: 3.7288), Xent 2.3030, Loss 4.8806, Error 0.4328(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2905 | Time 63.5231(62.6984) | Bit/dim 3.7125(3.7132) | Xent 0.2356(0.2314) | Loss 3.8303(3.8289) | Error 0.0900(0.0842) Steps 706(704.25) | Grad Norm 1.4914(1.8327) | Total Time 14.00(14.00)\n",
      "Iter 2906 | Time 62.4692(62.6915) | Bit/dim 3.7131(3.7132) | Xent 0.2251(0.2312) | Loss 3.8256(3.8288) | Error 0.0823(0.0841) Steps 712(704.48) | Grad Norm 1.0257(1.8085) | Total Time 14.00(14.00)\n",
      "Iter 2907 | Time 64.5587(62.7476) | Bit/dim 3.7173(3.7133) | Xent 0.2207(0.2309) | Loss 3.8277(3.8288) | Error 0.0801(0.0840) Steps 706(704.53) | Grad Norm 1.4487(1.7977) | Total Time 14.00(14.00)\n",
      "Iter 2908 | Time 62.9064(62.7523) | Bit/dim 3.7019(3.7130) | Xent 0.2198(0.2306) | Loss 3.8118(3.8282) | Error 0.0794(0.0838) Steps 694(704.21) | Grad Norm 1.1091(1.7770) | Total Time 14.00(14.00)\n",
      "Iter 2909 | Time 61.8596(62.7255) | Bit/dim 3.7100(3.7129) | Xent 0.2238(0.2304) | Loss 3.8219(3.8281) | Error 0.0807(0.0838) Steps 718(704.62) | Grad Norm 1.3168(1.7632) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 63.3549(62.7444) | Bit/dim 3.7259(3.7133) | Xent 0.2324(0.2304) | Loss 3.8420(3.8285) | Error 0.0845(0.0838) Steps 706(704.66) | Grad Norm 1.2224(1.7470) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 25.1931, Epoch Time 419.0969(414.7001), Bit/dim 3.7293(best: 3.7288), Xent 2.3152, Loss 4.8868, Error 0.4301(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2911 | Time 62.2214(62.7287) | Bit/dim 3.7221(3.7135) | Xent 0.2302(0.2304) | Loss 3.8372(3.8287) | Error 0.0854(0.0838) Steps 706(704.70) | Grad Norm 2.0482(1.7560) | Total Time 14.00(14.00)\n",
      "Iter 2912 | Time 64.3240(62.7766) | Bit/dim 3.7122(3.7135) | Xent 0.2311(0.2304) | Loss 3.8278(3.8287) | Error 0.0870(0.0839) Steps 706(704.74) | Grad Norm 1.4467(1.7467) | Total Time 14.00(14.00)\n",
      "Iter 2913 | Time 62.8787(62.7796) | Bit/dim 3.7168(3.7136) | Xent 0.2239(0.2302) | Loss 3.8288(3.8287) | Error 0.0821(0.0839) Steps 700(704.60) | Grad Norm 1.1081(1.7276) | Total Time 14.00(14.00)\n",
      "Iter 2914 | Time 62.3284(62.7661) | Bit/dim 3.7121(3.7135) | Xent 0.2385(0.2305) | Loss 3.8314(3.8288) | Error 0.0854(0.0839) Steps 700(704.46) | Grad Norm 1.2444(1.7131) | Total Time 14.00(14.00)\n",
      "Iter 2915 | Time 63.2004(62.7791) | Bit/dim 3.7061(3.7133) | Xent 0.2369(0.2307) | Loss 3.8246(3.8287) | Error 0.0895(0.0841) Steps 718(704.87) | Grad Norm 2.0061(1.7219) | Total Time 14.00(14.00)\n",
      "Iter 2916 | Time 62.5954(62.7736) | Bit/dim 3.7079(3.7132) | Xent 0.2202(0.2304) | Loss 3.8180(3.8283) | Error 0.0794(0.0839) Steps 712(705.08) | Grad Norm 1.2022(1.7063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 25.4446, Epoch Time 418.6545(414.8188), Bit/dim 3.7294(best: 3.7288), Xent 2.3256, Loss 4.8922, Error 0.4315(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2917 | Time 66.0195(62.8710) | Bit/dim 3.7182(3.7133) | Xent 0.2241(0.2302) | Loss 3.8303(3.8284) | Error 0.0799(0.0838) Steps 718(705.47) | Grad Norm 1.1512(1.6896) | Total Time 14.00(14.00)\n",
      "Iter 2918 | Time 61.8910(62.8416) | Bit/dim 3.7161(3.7134) | Xent 0.2199(0.2299) | Loss 3.8261(3.8283) | Error 0.0779(0.0836) Steps 706(705.49) | Grad Norm 1.6646(1.6889) | Total Time 14.00(14.00)\n",
      "Iter 2919 | Time 59.9275(62.7542) | Bit/dim 3.7133(3.7134) | Xent 0.2307(0.2299) | Loss 3.8286(3.8283) | Error 0.0801(0.0835) Steps 718(705.86) | Grad Norm 2.1900(1.7039) | Total Time 14.00(14.00)\n",
      "Iter 2920 | Time 62.9349(62.7596) | Bit/dim 3.7142(3.7134) | Xent 0.2218(0.2297) | Loss 3.8251(3.8282) | Error 0.0795(0.0834) Steps 718(706.23) | Grad Norm 1.3677(1.6938) | Total Time 14.00(14.00)\n",
      "Iter 2921 | Time 61.0062(62.7070) | Bit/dim 3.7208(3.7136) | Xent 0.2220(0.2294) | Loss 3.8318(3.8283) | Error 0.0789(0.0833) Steps 724(706.76) | Grad Norm 1.9865(1.7026) | Total Time 14.00(14.00)\n",
      "Iter 2922 | Time 64.5287(62.7616) | Bit/dim 3.6902(3.7129) | Xent 0.2235(0.2293) | Loss 3.8020(3.8276) | Error 0.0840(0.0833) Steps 706(706.74) | Grad Norm 1.5150(1.6970) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 25.6878, Epoch Time 417.7812(414.9076), Bit/dim 3.7303(best: 3.7288), Xent 2.3147, Loss 4.8877, Error 0.4297(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2923 | Time 61.7069(62.7300) | Bit/dim 3.7101(3.7128) | Xent 0.2194(0.2290) | Loss 3.8198(3.8273) | Error 0.0777(0.0831) Steps 718(707.07) | Grad Norm 2.2909(1.7148) | Total Time 14.00(14.00)\n",
      "Iter 2924 | Time 63.0509(62.7396) | Bit/dim 3.7096(3.7127) | Xent 0.2239(0.2288) | Loss 3.8215(3.8271) | Error 0.0837(0.0831) Steps 712(707.22) | Grad Norm 1.6741(1.7136) | Total Time 14.00(14.00)\n",
      "Iter 2925 | Time 65.9839(62.8370) | Bit/dim 3.7082(3.7126) | Xent 0.2234(0.2286) | Loss 3.8199(3.8269) | Error 0.0805(0.0831) Steps 694(706.83) | Grad Norm 1.6789(1.7125) | Total Time 14.00(14.00)\n",
      "Iter 2926 | Time 62.4231(62.8245) | Bit/dim 3.7127(3.7126) | Xent 0.2256(0.2286) | Loss 3.8255(3.8269) | Error 0.0821(0.0830) Steps 706(706.80) | Grad Norm 1.3755(1.7024) | Total Time 14.00(14.00)\n",
      "Iter 2927 | Time 63.8510(62.8553) | Bit/dim 3.7120(3.7126) | Xent 0.2152(0.2282) | Loss 3.8196(3.8267) | Error 0.0836(0.0831) Steps 694(706.42) | Grad Norm 1.6401(1.7006) | Total Time 14.00(14.00)\n",
      "Iter 2928 | Time 62.1990(62.8356) | Bit/dim 3.7217(3.7129) | Xent 0.2308(0.2282) | Loss 3.8371(3.8270) | Error 0.0854(0.0831) Steps 712(706.58) | Grad Norm 2.0183(1.7101) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 25.5514, Epoch Time 420.3897(415.0721), Bit/dim 3.7299(best: 3.7288), Xent 2.3080, Loss 4.8838, Error 0.4312(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2929 | Time 61.4983(62.7955) | Bit/dim 3.7131(3.7129) | Xent 0.2241(0.2281) | Loss 3.8251(3.8269) | Error 0.0827(0.0831) Steps 718(706.93) | Grad Norm 1.3534(1.6994) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 62.6132(62.7901) | Bit/dim 3.7062(3.7127) | Xent 0.2210(0.2279) | Loss 3.8166(3.8266) | Error 0.0790(0.0830) Steps 712(707.08) | Grad Norm 1.7802(1.7018) | Total Time 14.00(14.00)\n",
      "Iter 2931 | Time 62.5083(62.7816) | Bit/dim 3.7136(3.7127) | Xent 0.2205(0.2277) | Loss 3.8239(3.8265) | Error 0.0770(0.0828) Steps 718(707.41) | Grad Norm 1.0899(1.6834) | Total Time 14.00(14.00)\n",
      "Iter 2932 | Time 64.5942(62.8360) | Bit/dim 3.7154(3.7128) | Xent 0.2203(0.2274) | Loss 3.8255(3.8265) | Error 0.0801(0.0827) Steps 712(707.54) | Grad Norm 1.3247(1.6727) | Total Time 14.00(14.00)\n",
      "Iter 2933 | Time 62.9716(62.8400) | Bit/dim 3.7116(3.7127) | Xent 0.2222(0.2273) | Loss 3.8227(3.8264) | Error 0.0799(0.0826) Steps 700(707.32) | Grad Norm 1.0656(1.6545) | Total Time 14.00(14.00)\n",
      "Iter 2934 | Time 60.8218(62.7795) | Bit/dim 3.7165(3.7129) | Xent 0.2198(0.2271) | Loss 3.8264(3.8264) | Error 0.0795(0.0826) Steps 712(707.46) | Grad Norm 1.4704(1.6490) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 25.3371, Epoch Time 415.6518(415.0895), Bit/dim 3.7294(best: 3.7288), Xent 2.3382, Loss 4.8985, Error 0.4305(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2935 | Time 61.4437(62.7394) | Bit/dim 3.7168(3.7130) | Xent 0.2191(0.2268) | Loss 3.8263(3.8264) | Error 0.0817(0.0825) Steps 706(707.41) | Grad Norm 1.3968(1.6414) | Total Time 14.00(14.00)\n",
      "Iter 2936 | Time 61.1310(62.6912) | Bit/dim 3.7177(3.7131) | Xent 0.2262(0.2268) | Loss 3.8308(3.8265) | Error 0.0836(0.0826) Steps 724(707.91) | Grad Norm 1.2839(1.6307) | Total Time 14.00(14.00)\n",
      "Iter 2937 | Time 67.1334(62.8244) | Bit/dim 3.7047(3.7129) | Xent 0.2343(0.2270) | Loss 3.8219(3.8264) | Error 0.0810(0.0825) Steps 712(708.03) | Grad Norm 1.9710(1.6409) | Total Time 14.00(14.00)\n",
      "Iter 2938 | Time 64.6299(62.8786) | Bit/dim 3.7077(3.7127) | Xent 0.2277(0.2271) | Loss 3.8215(3.8262) | Error 0.0834(0.0825) Steps 694(707.61) | Grad Norm 1.2468(1.6290) | Total Time 14.00(14.00)\n",
      "Iter 2939 | Time 60.7727(62.8154) | Bit/dim 3.7165(3.7128) | Xent 0.2226(0.2269) | Loss 3.8278(3.8263) | Error 0.0786(0.0824) Steps 700(707.39) | Grad Norm 1.2360(1.6173) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 63.0119(62.8213) | Bit/dim 3.7150(3.7129) | Xent 0.2215(0.2268) | Loss 3.8257(3.8263) | Error 0.0811(0.0824) Steps 718(707.70) | Grad Norm 1.6547(1.6184) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0490 | Time 25.3450, Epoch Time 418.8850(415.2033), Bit/dim 3.7292(best: 3.7288), Xent 2.3185, Loss 4.8885, Error 0.4293(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2941 | Time 65.9274(62.9145) | Bit/dim 3.7128(3.7129) | Xent 0.2202(0.2266) | Loss 3.8229(3.8262) | Error 0.0809(0.0823) Steps 706(707.65) | Grad Norm 1.2151(1.6063) | Total Time 14.00(14.00)\n",
      "Iter 2942 | Time 63.3188(62.9266) | Bit/dim 3.7182(3.7130) | Xent 0.2118(0.2261) | Loss 3.8241(3.8261) | Error 0.0766(0.0822) Steps 718(707.96) | Grad Norm 1.5850(1.6056) | Total Time 14.00(14.00)\n",
      "Iter 2943 | Time 61.6231(62.8875) | Bit/dim 3.7028(3.7127) | Xent 0.2290(0.2262) | Loss 3.8173(3.8258) | Error 0.0829(0.0822) Steps 706(707.90) | Grad Norm 1.3581(1.5982) | Total Time 14.00(14.00)\n",
      "Iter 2944 | Time 64.3133(62.9303) | Bit/dim 3.7099(3.7127) | Xent 0.2196(0.2260) | Loss 3.8197(3.8257) | Error 0.0775(0.0820) Steps 706(707.85) | Grad Norm 1.1995(1.5863) | Total Time 14.00(14.00)\n",
      "Iter 2945 | Time 63.8903(62.9591) | Bit/dim 3.7199(3.7129) | Xent 0.2146(0.2257) | Loss 3.8273(3.8257) | Error 0.0804(0.0820) Steps 712(707.97) | Grad Norm 0.8525(1.5642) | Total Time 14.00(14.00)\n",
      "Iter 2946 | Time 62.9128(62.9577) | Bit/dim 3.7102(3.7128) | Xent 0.2167(0.2254) | Loss 3.8186(3.8255) | Error 0.0800(0.0819) Steps 712(708.09) | Grad Norm 2.0010(1.5773) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0491 | Time 25.6405, Epoch Time 423.2607(415.4451), Bit/dim 3.7301(best: 3.7288), Xent 2.3404, Loss 4.9003, Error 0.4326(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2947 | Time 63.7970(62.9829) | Bit/dim 3.7163(3.7129) | Xent 0.2197(0.2252) | Loss 3.8262(3.8255) | Error 0.0803(0.0819) Steps 712(708.21) | Grad Norm 0.9439(1.5583) | Total Time 14.00(14.00)\n",
      "Iter 2948 | Time 63.2753(62.9917) | Bit/dim 3.7133(3.7129) | Xent 0.2212(0.2251) | Loss 3.8239(3.8255) | Error 0.0819(0.0819) Steps 718(708.50) | Grad Norm 1.7147(1.5630) | Total Time 14.00(14.00)\n",
      "Iter 2949 | Time 61.4759(62.9462) | Bit/dim 3.7144(3.7130) | Xent 0.2202(0.2250) | Loss 3.8245(3.8254) | Error 0.0843(0.0820) Steps 712(708.61) | Grad Norm 0.9009(1.5432) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 63.6922(62.9686) | Bit/dim 3.6974(3.7125) | Xent 0.2321(0.2252) | Loss 3.8135(3.8251) | Error 0.0819(0.0820) Steps 694(708.17) | Grad Norm 1.5193(1.5425) | Total Time 14.00(14.00)\n",
      "Iter 2951 | Time 62.8232(62.9642) | Bit/dim 3.7101(3.7124) | Xent 0.2194(0.2250) | Loss 3.8198(3.8249) | Error 0.0759(0.0818) Steps 706(708.11) | Grad Norm 1.9316(1.5541) | Total Time 14.00(14.00)\n",
      "Iter 2952 | Time 63.1041(62.9684) | Bit/dim 3.7204(3.7127) | Xent 0.2261(0.2250) | Loss 3.8334(3.8252) | Error 0.0784(0.0817) Steps 712(708.22) | Grad Norm 1.0405(1.5387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 25.9668, Epoch Time 419.5591(415.5685), Bit/dim 3.7283(best: 3.7288), Xent 2.3227, Loss 4.8897, Error 0.4322(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2953 | Time 61.1827(62.9148) | Bit/dim 3.7055(3.7124) | Xent 0.2310(0.2252) | Loss 3.8210(3.8250) | Error 0.0851(0.0818) Steps 706(708.16) | Grad Norm 1.5805(1.5400) | Total Time 14.00(14.00)\n",
      "Iter 2954 | Time 62.6848(62.9079) | Bit/dim 3.7165(3.7126) | Xent 0.2211(0.2251) | Loss 3.8271(3.8251) | Error 0.0801(0.0817) Steps 700(707.91) | Grad Norm 1.3802(1.5352) | Total Time 14.00(14.00)\n",
      "Iter 2955 | Time 63.3312(62.9206) | Bit/dim 3.6994(3.7122) | Xent 0.2089(0.2246) | Loss 3.8038(3.8245) | Error 0.0739(0.0815) Steps 712(708.03) | Grad Norm 0.9728(1.5183) | Total Time 14.00(14.00)\n",
      "Iter 2956 | Time 63.2697(62.9311) | Bit/dim 3.7178(3.7123) | Xent 0.2238(0.2246) | Loss 3.8298(3.8246) | Error 0.0806(0.0815) Steps 706(707.97) | Grad Norm 1.7060(1.5239) | Total Time 14.00(14.00)\n",
      "Iter 2957 | Time 61.6735(62.8934) | Bit/dim 3.7180(3.7125) | Xent 0.2258(0.2246) | Loss 3.8309(3.8248) | Error 0.0839(0.0815) Steps 724(708.45) | Grad Norm 1.6577(1.5280) | Total Time 14.00(14.00)\n",
      "Iter 2958 | Time 62.6671(62.8866) | Bit/dim 3.7116(3.7125) | Xent 0.2298(0.2248) | Loss 3.8265(3.8249) | Error 0.0845(0.0816) Steps 706(708.38) | Grad Norm 1.3544(1.5227) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 25.3023, Epoch Time 415.5935(415.5692), Bit/dim 3.7284(best: 3.7283), Xent 2.3692, Loss 4.9130, Error 0.4355(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2959 | Time 63.1704(62.8951) | Bit/dim 3.7228(3.7128) | Xent 0.2165(0.2245) | Loss 3.8310(3.8251) | Error 0.0819(0.0816) Steps 712(708.49) | Grad Norm 1.1459(1.5114) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 65.6574(62.9780) | Bit/dim 3.7066(3.7126) | Xent 0.2221(0.2245) | Loss 3.8177(3.8248) | Error 0.0797(0.0816) Steps 718(708.77) | Grad Norm 1.7681(1.5191) | Total Time 14.00(14.00)\n",
      "Iter 2961 | Time 65.7820(63.0621) | Bit/dim 3.7053(3.7124) | Xent 0.2286(0.2246) | Loss 3.8196(3.8247) | Error 0.0773(0.0814) Steps 718(709.05) | Grad Norm 0.9236(1.5013) | Total Time 14.00(14.00)\n",
      "Iter 2962 | Time 63.0296(63.0611) | Bit/dim 3.7166(3.7125) | Xent 0.2197(0.2244) | Loss 3.8264(3.8247) | Error 0.0785(0.0814) Steps 718(709.32) | Grad Norm 1.0746(1.4885) | Total Time 14.00(14.00)\n",
      "Iter 2963 | Time 61.3605(63.0101) | Bit/dim 3.7012(3.7122) | Xent 0.2266(0.2245) | Loss 3.8145(3.8244) | Error 0.0834(0.0814) Steps 706(709.22) | Grad Norm 1.4879(1.4885) | Total Time 14.00(14.00)\n",
      "Iter 2964 | Time 62.7708(63.0029) | Bit/dim 3.7171(3.7123) | Xent 0.2041(0.2239) | Loss 3.8192(3.8243) | Error 0.0734(0.0812) Steps 700(708.94) | Grad Norm 1.2935(1.4826) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 25.2362, Epoch Time 422.3933(415.7740), Bit/dim 3.7294(best: 3.7283), Xent 2.3323, Loss 4.8955, Error 0.4334(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2965 | Time 60.8482(62.9383) | Bit/dim 3.7070(3.7122) | Xent 0.2251(0.2239) | Loss 3.8195(3.8241) | Error 0.0814(0.0812) Steps 712(709.03) | Grad Norm 0.9605(1.4669) | Total Time 14.00(14.00)\n",
      "Iter 2966 | Time 65.0877(63.0028) | Bit/dim 3.7129(3.7122) | Xent 0.2319(0.2242) | Loss 3.8288(3.8243) | Error 0.0817(0.0812) Steps 694(708.58) | Grad Norm 2.1751(1.4882) | Total Time 14.00(14.00)\n",
      "Iter 2967 | Time 64.0101(63.0330) | Bit/dim 3.7017(3.7119) | Xent 0.2135(0.2238) | Loss 3.8085(3.8238) | Error 0.0781(0.0811) Steps 706(708.51) | Grad Norm 1.8135(1.4979) | Total Time 14.00(14.00)\n",
      "Iter 2968 | Time 61.3784(62.9833) | Bit/dim 3.7095(3.7118) | Xent 0.2162(0.2236) | Loss 3.8176(3.8236) | Error 0.0776(0.0810) Steps 712(708.61) | Grad Norm 1.3782(1.4944) | Total Time 14.00(14.00)\n",
      "Iter 2969 | Time 63.8673(63.0099) | Bit/dim 3.7137(3.7119) | Xent 0.2231(0.2236) | Loss 3.8252(3.8237) | Error 0.0809(0.0810) Steps 712(708.71) | Grad Norm 1.2015(1.4856) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 64.0979(63.0425) | Bit/dim 3.7238(3.7122) | Xent 0.2111(0.2232) | Loss 3.8294(3.8238) | Error 0.0765(0.0809) Steps 712(708.81) | Grad Norm 1.6764(1.4913) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 25.7285, Epoch Time 420.7101(415.9220), Bit/dim 3.7295(best: 3.7283), Xent 2.3506, Loss 4.9048, Error 0.4340(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2971 | Time 61.6365(63.0003) | Bit/dim 3.7090(3.7121) | Xent 0.2255(0.2233) | Loss 3.8218(3.8238) | Error 0.0807(0.0809) Steps 706(708.73) | Grad Norm 1.3773(1.4879) | Total Time 14.00(14.00)\n",
      "Iter 2972 | Time 61.5945(62.9581) | Bit/dim 3.7163(3.7122) | Xent 0.2308(0.2235) | Loss 3.8317(3.8240) | Error 0.0829(0.0809) Steps 706(708.64) | Grad Norm 1.1992(1.4792) | Total Time 14.00(14.00)\n",
      "Iter 2973 | Time 65.8534(63.0450) | Bit/dim 3.7088(3.7121) | Xent 0.2120(0.2232) | Loss 3.8148(3.8237) | Error 0.0761(0.0808) Steps 706(708.57) | Grad Norm 1.0876(1.4675) | Total Time 14.00(14.00)\n",
      "Iter 2974 | Time 63.4583(63.0574) | Bit/dim 3.7174(3.7123) | Xent 0.2115(0.2228) | Loss 3.8232(3.8237) | Error 0.0744(0.0806) Steps 712(708.67) | Grad Norm 1.2870(1.4620) | Total Time 14.00(14.00)\n",
      "Iter 2975 | Time 63.0745(63.0579) | Bit/dim 3.7156(3.7124) | Xent 0.2090(0.2224) | Loss 3.8201(3.8236) | Error 0.0761(0.0805) Steps 718(708.95) | Grad Norm 1.2252(1.4549) | Total Time 14.00(14.00)\n",
      "Iter 2976 | Time 65.7348(63.1382) | Bit/dim 3.7051(3.7122) | Xent 0.2148(0.2222) | Loss 3.8125(3.8233) | Error 0.0780(0.0804) Steps 706(708.86) | Grad Norm 1.5247(1.4570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 25.6229, Epoch Time 422.2548(416.1120), Bit/dim 3.7297(best: 3.7283), Xent 2.3625, Loss 4.9110, Error 0.4329(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2977 | Time 63.0085(63.1343) | Bit/dim 3.7008(3.7118) | Xent 0.2197(0.2221) | Loss 3.8106(3.8229) | Error 0.0804(0.0804) Steps 718(709.13) | Grad Norm 1.2491(1.4508) | Total Time 14.00(14.00)\n",
      "Iter 2978 | Time 62.6082(63.1185) | Bit/dim 3.7115(3.7118) | Xent 0.2134(0.2218) | Loss 3.8182(3.8227) | Error 0.0753(0.0802) Steps 712(709.22) | Grad Norm 1.0357(1.4383) | Total Time 14.00(14.00)\n",
      "Iter 2979 | Time 63.6643(63.1349) | Bit/dim 3.7111(3.7118) | Xent 0.2295(0.2221) | Loss 3.8258(3.8228) | Error 0.0866(0.0804) Steps 712(709.30) | Grad Norm 1.4263(1.4380) | Total Time 14.00(14.00)\n",
      "Iter 2980 | Time 62.5312(63.1168) | Bit/dim 3.7184(3.7120) | Xent 0.2128(0.2218) | Loss 3.8249(3.8229) | Error 0.0737(0.0802) Steps 706(709.20) | Grad Norm 1.4775(1.4392) | Total Time 14.00(14.00)\n",
      "Iter 2981 | Time 64.2297(63.1502) | Bit/dim 3.7027(3.7117) | Xent 0.2205(0.2218) | Loss 3.8130(3.8226) | Error 0.0811(0.0802) Steps 712(709.29) | Grad Norm 1.2944(1.4348) | Total Time 14.00(14.00)\n",
      "Iter 2982 | Time 62.2456(63.1231) | Bit/dim 3.7278(3.7122) | Xent 0.2205(0.2217) | Loss 3.8380(3.8231) | Error 0.0794(0.0802) Steps 718(709.55) | Grad Norm 2.3111(1.4611) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 25.3579, Epoch Time 419.0848(416.2012), Bit/dim 3.7292(best: 3.7283), Xent 2.3559, Loss 4.9072, Error 0.4335(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2983 | Time 61.4032(63.0715) | Bit/dim 3.7038(3.7120) | Xent 0.2058(0.2212) | Loss 3.8067(3.8226) | Error 0.0719(0.0800) Steps 706(709.44) | Grad Norm 0.8549(1.4429) | Total Time 14.00(14.00)\n",
      "Iter 2984 | Time 62.8085(63.0636) | Bit/dim 3.7191(3.7122) | Xent 0.2103(0.2209) | Loss 3.8242(3.8226) | Error 0.0763(0.0799) Steps 712(709.52) | Grad Norm 1.2714(1.4378) | Total Time 14.00(14.00)\n",
      "Iter 2985 | Time 59.7772(62.9650) | Bit/dim 3.7149(3.7123) | Xent 0.2202(0.2209) | Loss 3.8251(3.8227) | Error 0.0810(0.0799) Steps 706(709.41) | Grad Norm 1.8755(1.4509) | Total Time 14.00(14.00)\n",
      "Iter 2986 | Time 61.9520(62.9346) | Bit/dim 3.7065(3.7121) | Xent 0.2265(0.2211) | Loss 3.8198(3.8226) | Error 0.0819(0.0799) Steps 712(709.49) | Grad Norm 1.4582(1.4511) | Total Time 14.00(14.00)\n",
      "Iter 2987 | Time 59.4534(62.8302) | Bit/dim 3.7130(3.7121) | Xent 0.2176(0.2210) | Loss 3.8218(3.8226) | Error 0.0789(0.0799) Steps 712(709.57) | Grad Norm 1.1330(1.4416) | Total Time 14.00(14.00)\n",
      "Iter 2988 | Time 61.9944(62.8051) | Bit/dim 3.7128(3.7121) | Xent 0.2233(0.2210) | Loss 3.8244(3.8226) | Error 0.0805(0.0799) Steps 706(709.46) | Grad Norm 1.4038(1.4405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 25.8120, Epoch Time 408.5979(415.9731), Bit/dim 3.7290(best: 3.7283), Xent 2.3760, Loss 4.9170, Error 0.4364(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2989 | Time 63.5691(62.8280) | Bit/dim 3.7053(3.7119) | Xent 0.2200(0.2210) | Loss 3.8153(3.8224) | Error 0.0805(0.0800) Steps 712(709.54) | Grad Norm 1.8999(1.4542) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 61.2687(62.7812) | Bit/dim 3.7147(3.7120) | Xent 0.2065(0.2206) | Loss 3.8180(3.8223) | Error 0.0753(0.0798) Steps 706(709.43) | Grad Norm 1.0037(1.4407) | Total Time 14.00(14.00)\n",
      "Iter 2991 | Time 64.8369(62.8429) | Bit/dim 3.7152(3.7121) | Xent 0.2113(0.2203) | Loss 3.8209(3.8222) | Error 0.0755(0.0797) Steps 706(709.33) | Grad Norm 1.4204(1.4401) | Total Time 14.00(14.00)\n",
      "Iter 2992 | Time 63.2192(62.8542) | Bit/dim 3.7047(3.7119) | Xent 0.2223(0.2203) | Loss 3.8158(3.8221) | Error 0.0814(0.0797) Steps 706(709.23) | Grad Norm 1.5553(1.4436) | Total Time 14.00(14.00)\n",
      "Iter 2993 | Time 61.5210(62.8142) | Bit/dim 3.7060(3.7117) | Xent 0.2252(0.2205) | Loss 3.8186(3.8219) | Error 0.0823(0.0798) Steps 712(709.31) | Grad Norm 1.1019(1.4333) | Total Time 14.00(14.00)\n",
      "Iter 2994 | Time 60.7229(62.7515) | Bit/dim 3.7185(3.7119) | Xent 0.2069(0.2201) | Loss 3.8220(3.8219) | Error 0.0765(0.0797) Steps 712(709.39) | Grad Norm 1.8226(1.4450) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 25.3035, Epoch Time 415.8769(415.9702), Bit/dim 3.7289(best: 3.7283), Xent 2.3676, Loss 4.9127, Error 0.4305(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2995 | Time 62.4136(62.7413) | Bit/dim 3.7087(3.7118) | Xent 0.2140(0.2199) | Loss 3.8157(3.8218) | Error 0.0773(0.0796) Steps 706(709.29) | Grad Norm 1.8645(1.4576) | Total Time 14.00(14.00)\n",
      "Iter 2996 | Time 60.8654(62.6850) | Bit/dim 3.7193(3.7120) | Xent 0.2166(0.2198) | Loss 3.8276(3.8219) | Error 0.0780(0.0796) Steps 706(709.19) | Grad Norm 1.0955(1.4467) | Total Time 14.00(14.00)\n",
      "Iter 2997 | Time 61.8545(62.6601) | Bit/dim 3.7162(3.7122) | Xent 0.2238(0.2199) | Loss 3.8281(3.8221) | Error 0.0820(0.0797) Steps 706(709.10) | Grad Norm 1.3245(1.4431) | Total Time 14.00(14.00)\n",
      "Iter 2998 | Time 62.5033(62.6554) | Bit/dim 3.7102(3.7121) | Xent 0.2300(0.2202) | Loss 3.8252(3.8222) | Error 0.0861(0.0799) Steps 712(709.18) | Grad Norm 1.0777(1.4321) | Total Time 14.00(14.00)\n",
      "Iter 2999 | Time 61.8791(62.6321) | Bit/dim 3.7012(3.7118) | Xent 0.2196(0.2202) | Loss 3.8110(3.8219) | Error 0.0795(0.0798) Steps 706(709.09) | Grad Norm 1.4848(1.4337) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 62.9866(62.6428) | Bit/dim 3.7097(3.7117) | Xent 0.2196(0.2202) | Loss 3.8195(3.8218) | Error 0.0806(0.0799) Steps 718(709.35) | Grad Norm 1.8075(1.4449) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0500 | Time 25.5607, Epoch Time 413.7523(415.9037), Bit/dim 3.7289(best: 3.7283), Xent 2.3648, Loss 4.9113, Error 0.4331(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3001 | Time 61.2492(62.6010) | Bit/dim 3.7061(3.7115) | Xent 0.2227(0.2203) | Loss 3.8175(3.8217) | Error 0.0771(0.0798) Steps 706(709.25) | Grad Norm 1.1870(1.4371) | Total Time 14.00(14.00)\n",
      "Iter 3002 | Time 63.4501(62.6264) | Bit/dim 3.7210(3.7118) | Xent 0.2242(0.2204) | Loss 3.8332(3.8220) | Error 0.0796(0.0798) Steps 712(709.34) | Grad Norm 1.1996(1.4300) | Total Time 14.00(14.00)\n",
      "Iter 3003 | Time 63.5886(62.6553) | Bit/dim 3.7118(3.7118) | Xent 0.2162(0.2203) | Loss 3.8200(3.8220) | Error 0.0769(0.0797) Steps 706(709.24) | Grad Norm 1.3691(1.4282) | Total Time 14.00(14.00)\n",
      "Iter 3004 | Time 59.7493(62.5681) | Bit/dim 3.7080(3.7117) | Xent 0.2049(0.2198) | Loss 3.8105(3.8216) | Error 0.0743(0.0795) Steps 718(709.50) | Grad Norm 2.3199(1.4549) | Total Time 14.00(14.00)\n",
      "Iter 3005 | Time 61.0443(62.5224) | Bit/dim 3.7063(3.7116) | Xent 0.2126(0.2196) | Loss 3.8127(3.8213) | Error 0.0771(0.0795) Steps 724(709.93) | Grad Norm 1.7082(1.4625) | Total Time 14.00(14.00)\n",
      "Iter 3006 | Time 62.5644(62.5237) | Bit/dim 3.7087(3.7115) | Xent 0.2120(0.2194) | Loss 3.8147(3.8211) | Error 0.0784(0.0794) Steps 700(709.64) | Grad Norm 1.2820(1.4571) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0501 | Time 25.5586, Epoch Time 412.6413(415.8058), Bit/dim 3.7278(best: 3.7283), Xent 2.3459, Loss 4.9008, Error 0.4319(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3007 | Time 63.9244(62.5657) | Bit/dim 3.7098(3.7114) | Xent 0.2151(0.2192) | Loss 3.8173(3.8210) | Error 0.0763(0.0793) Steps 712(709.71) | Grad Norm 2.2918(1.4822) | Total Time 14.00(14.00)\n",
      "Iter 3008 | Time 64.0855(62.6113) | Bit/dim 3.7179(3.7116) | Xent 0.2194(0.2192) | Loss 3.8276(3.8212) | Error 0.0796(0.0793) Steps 718(709.96) | Grad Norm 1.8705(1.4938) | Total Time 14.00(14.00)\n",
      "Iter 3009 | Time 61.1854(62.5685) | Bit/dim 3.7066(3.7115) | Xent 0.2165(0.2191) | Loss 3.8148(3.8210) | Error 0.0831(0.0795) Steps 706(709.84) | Grad Norm 2.4075(1.5212) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 62.5907(62.5692) | Bit/dim 3.7169(3.7116) | Xent 0.2156(0.2190) | Loss 3.8247(3.8211) | Error 0.0757(0.0793) Steps 712(709.90) | Grad Norm 0.8978(1.5025) | Total Time 14.00(14.00)\n",
      "Iter 3011 | Time 60.6356(62.5112) | Bit/dim 3.7066(3.7115) | Xent 0.2010(0.2185) | Loss 3.8071(3.8207) | Error 0.0716(0.0791) Steps 706(709.78) | Grad Norm 2.0553(1.5191) | Total Time 14.00(14.00)\n",
      "Iter 3012 | Time 61.1646(62.4708) | Bit/dim 3.7112(3.7115) | Xent 0.2158(0.2184) | Loss 3.8191(3.8207) | Error 0.0753(0.0790) Steps 724(710.21) | Grad Norm 2.6717(1.5537) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0502 | Time 25.4038, Epoch Time 414.3984(415.7636), Bit/dim 3.7284(best: 3.7278), Xent 2.3934, Loss 4.9251, Error 0.4343(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3013 | Time 61.4252(62.4394) | Bit/dim 3.7241(3.7118) | Xent 0.2105(0.2182) | Loss 3.8293(3.8209) | Error 0.0743(0.0789) Steps 706(710.08) | Grad Norm 2.3130(1.5765) | Total Time 14.00(14.00)\n",
      "Iter 3014 | Time 62.6152(62.4447) | Bit/dim 3.7130(3.7119) | Xent 0.2119(0.2180) | Loss 3.8190(3.8209) | Error 0.0766(0.0788) Steps 718(710.32) | Grad Norm 1.3661(1.5702) | Total Time 14.00(14.00)\n",
      "Iter 3015 | Time 63.7175(62.4828) | Bit/dim 3.7085(3.7118) | Xent 0.2204(0.2181) | Loss 3.8187(3.8208) | Error 0.0794(0.0788) Steps 730(710.91) | Grad Norm 2.4563(1.5967) | Total Time 14.00(14.00)\n",
      "Iter 3016 | Time 63.1198(62.5020) | Bit/dim 3.7083(3.7117) | Xent 0.2243(0.2183) | Loss 3.8204(3.8208) | Error 0.0840(0.0790) Steps 718(711.13) | Grad Norm 2.5274(1.6247) | Total Time 14.00(14.00)\n",
      "Iter 3017 | Time 62.6946(62.5077) | Bit/dim 3.7086(3.7116) | Xent 0.2152(0.2182) | Loss 3.8162(3.8207) | Error 0.0810(0.0790) Steps 706(710.97) | Grad Norm 1.7229(1.6276) | Total Time 14.00(14.00)\n",
      "Iter 3018 | Time 61.9588(62.4913) | Bit/dim 3.7083(3.7115) | Xent 0.2153(0.2181) | Loss 3.8159(3.8205) | Error 0.0775(0.0790) Steps 712(711.00) | Grad Norm 1.5114(1.6241) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0503 | Time 25.6857, Epoch Time 416.6471(415.7901), Bit/dim 3.7288(best: 3.7278), Xent 2.3949, Loss 4.9262, Error 0.4335(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3019 | Time 62.0694(62.4786) | Bit/dim 3.7254(3.7119) | Xent 0.2253(0.2183) | Loss 3.8381(3.8210) | Error 0.0810(0.0790) Steps 712(711.03) | Grad Norm 1.9473(1.6338) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 62.1449(62.4686) | Bit/dim 3.7057(3.7117) | Xent 0.2298(0.2186) | Loss 3.8206(3.8210) | Error 0.0863(0.0793) Steps 724(711.42) | Grad Norm 2.1542(1.6494) | Total Time 14.00(14.00)\n",
      "Iter 3021 | Time 63.6252(62.5033) | Bit/dim 3.7022(3.7114) | Xent 0.2092(0.2184) | Loss 3.8068(3.8206) | Error 0.0747(0.0791) Steps 724(711.80) | Grad Norm 1.5754(1.6472) | Total Time 14.00(14.00)\n",
      "Iter 3022 | Time 64.3074(62.5574) | Bit/dim 3.7135(3.7115) | Xent 0.2180(0.2183) | Loss 3.8225(3.8207) | Error 0.0759(0.0790) Steps 712(711.80) | Grad Norm 1.7462(1.6502) | Total Time 14.00(14.00)\n",
      "Iter 3023 | Time 62.2784(62.5491) | Bit/dim 3.7137(3.7116) | Xent 0.2066(0.2180) | Loss 3.8170(3.8206) | Error 0.0759(0.0789) Steps 712(711.81) | Grad Norm 2.0331(1.6617) | Total Time 14.00(14.00)\n",
      "Iter 3024 | Time 61.9108(62.5299) | Bit/dim 3.7019(3.7113) | Xent 0.2032(0.2175) | Loss 3.8035(3.8200) | Error 0.0729(0.0787) Steps 712(711.82) | Grad Norm 2.1106(1.6751) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0504 | Time 25.4854, Epoch Time 417.4000(415.8384), Bit/dim 3.7294(best: 3.7278), Xent 2.3890, Loss 4.9239, Error 0.4340(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3025 | Time 62.5460(62.5304) | Bit/dim 3.7169(3.7114) | Xent 0.2085(0.2173) | Loss 3.8212(3.8201) | Error 0.0771(0.0787) Steps 718(712.00) | Grad Norm 1.6146(1.6733) | Total Time 14.00(14.00)\n",
      "Iter 3026 | Time 60.5705(62.4716) | Bit/dim 3.7147(3.7115) | Xent 0.2280(0.2176) | Loss 3.8287(3.8203) | Error 0.0827(0.0788) Steps 700(711.64) | Grad Norm 2.2812(1.6916) | Total Time 14.00(14.00)\n",
      "Iter 3027 | Time 59.4130(62.3798) | Bit/dim 3.7144(3.7116) | Xent 0.2193(0.2176) | Loss 3.8241(3.8204) | Error 0.0769(0.0788) Steps 712(711.65) | Grad Norm 3.3126(1.7402) | Total Time 14.00(14.00)\n",
      "Iter 3028 | Time 61.8630(62.3643) | Bit/dim 3.7129(3.7117) | Xent 0.2149(0.2176) | Loss 3.8204(3.8204) | Error 0.0789(0.0788) Steps 706(711.48) | Grad Norm 1.9801(1.7474) | Total Time 14.00(14.00)\n",
      "Iter 3029 | Time 60.2038(62.2995) | Bit/dim 3.7115(3.7117) | Xent 0.2126(0.2174) | Loss 3.8178(3.8204) | Error 0.0740(0.0786) Steps 718(711.68) | Grad Norm 1.4266(1.7378) | Total Time 14.00(14.00)\n",
      "Iter 3030 | Time 59.9615(62.2294) | Bit/dim 3.7005(3.7113) | Xent 0.2285(0.2177) | Loss 3.8147(3.8202) | Error 0.0833(0.0788) Steps 724(712.05) | Grad Norm 3.7654(1.7986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0505 | Time 25.3589, Epoch Time 405.5310(415.5292), Bit/dim 3.7292(best: 3.7278), Xent 2.3792, Loss 4.9188, Error 0.4348(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3031 | Time 63.4925(62.2673) | Bit/dim 3.7049(3.7111) | Xent 0.2123(0.2176) | Loss 3.8110(3.8199) | Error 0.0765(0.0787) Steps 712(712.05) | Grad Norm 3.0056(1.8348) | Total Time 14.00(14.00)\n",
      "Iter 3032 | Time 62.6560(62.2789) | Bit/dim 3.7085(3.7111) | Xent 0.2138(0.2175) | Loss 3.8154(3.8198) | Error 0.0730(0.0785) Steps 706(711.87) | Grad Norm 1.3046(1.8189) | Total Time 14.00(14.00)\n",
      "Iter 3033 | Time 61.5227(62.2562) | Bit/dim 3.7055(3.7109) | Xent 0.2250(0.2177) | Loss 3.8180(3.8197) | Error 0.0807(0.0786) Steps 706(711.69) | Grad Norm 2.7501(1.8468) | Total Time 14.00(14.00)\n",
      "Iter 3034 | Time 63.0059(62.2787) | Bit/dim 3.7185(3.7111) | Xent 0.2145(0.2176) | Loss 3.8257(3.8199) | Error 0.0763(0.0785) Steps 706(711.52) | Grad Norm 2.7342(1.8734) | Total Time 14.00(14.00)\n",
      "Iter 3035 | Time 62.4071(62.2826) | Bit/dim 3.7110(3.7111) | Xent 0.2107(0.2174) | Loss 3.8164(3.8198) | Error 0.0781(0.0785) Steps 712(711.53) | Grad Norm 1.4357(1.8603) | Total Time 14.00(14.00)\n",
      "Iter 3036 | Time 65.5542(62.3807) | Bit/dim 3.7172(3.7113) | Xent 0.2075(0.2171) | Loss 3.8209(3.8198) | Error 0.0765(0.0784) Steps 712(711.55) | Grad Norm 1.0466(1.8359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0506 | Time 25.5402, Epoch Time 419.6508(415.6528), Bit/dim 3.7289(best: 3.7278), Xent 2.3879, Loss 4.9228, Error 0.4314(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3037 | Time 59.7685(62.3024) | Bit/dim 3.7075(3.7112) | Xent 0.2190(0.2172) | Loss 3.8170(3.8198) | Error 0.0811(0.0785) Steps 712(711.56) | Grad Norm 2.2346(1.8479) | Total Time 14.00(14.00)\n",
      "Iter 3038 | Time 61.4919(62.2780) | Bit/dim 3.7149(3.7113) | Xent 0.2176(0.2172) | Loss 3.8237(3.8199) | Error 0.0785(0.0785) Steps 706(711.39) | Grad Norm 1.3423(1.8327) | Total Time 14.00(14.00)\n",
      "Iter 3039 | Time 64.3234(62.3394) | Bit/dim 3.7193(3.7115) | Xent 0.2147(0.2171) | Loss 3.8267(3.8201) | Error 0.0764(0.0785) Steps 712(711.41) | Grad Norm 1.6866(1.8283) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 62.2598(62.3370) | Bit/dim 3.7040(3.7113) | Xent 0.2295(0.2175) | Loss 3.8187(3.8200) | Error 0.0806(0.0785) Steps 706(711.25) | Grad Norm 1.9894(1.8331) | Total Time 14.00(14.00)\n",
      "Iter 3041 | Time 64.1139(62.3903) | Bit/dim 3.7090(3.7112) | Xent 0.2220(0.2176) | Loss 3.8200(3.8200) | Error 0.0787(0.0785) Steps 712(711.27) | Grad Norm 1.9352(1.8362) | Total Time 14.00(14.00)\n",
      "Iter 3042 | Time 60.7725(62.3418) | Bit/dim 3.7174(3.7114) | Xent 0.2021(0.2171) | Loss 3.8184(3.8200) | Error 0.0740(0.0784) Steps 724(711.65) | Grad Norm 1.5798(1.8285) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0507 | Time 25.4558, Epoch Time 413.4458(415.5866), Bit/dim 3.7289(best: 3.7278), Xent 2.3861, Loss 4.9220, Error 0.4333(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3043 | Time 61.6092(62.3198) | Bit/dim 3.7070(3.7113) | Xent 0.2068(0.2168) | Loss 3.8104(3.8197) | Error 0.0719(0.0782) Steps 712(711.66) | Grad Norm 1.2391(1.8108) | Total Time 14.00(14.00)\n",
      "Iter 3044 | Time 62.1945(62.3161) | Bit/dim 3.7118(3.7113) | Xent 0.2160(0.2168) | Loss 3.8198(3.8197) | Error 0.0773(0.0782) Steps 712(711.67) | Grad Norm 1.4896(1.8012) | Total Time 14.00(14.00)\n",
      "Iter 3045 | Time 60.9150(62.2740) | Bit/dim 3.7110(3.7113) | Xent 0.2227(0.2170) | Loss 3.8223(3.8198) | Error 0.0809(0.0783) Steps 706(711.50) | Grad Norm 1.5410(1.7934) | Total Time 14.00(14.00)\n",
      "Iter 3046 | Time 62.4175(62.2783) | Bit/dim 3.7074(3.7112) | Xent 0.2085(0.2167) | Loss 3.8117(3.8195) | Error 0.0717(0.0781) Steps 712(711.52) | Grad Norm 1.3882(1.7812) | Total Time 14.00(14.00)\n",
      "Iter 3047 | Time 63.2597(62.3078) | Bit/dim 3.7133(3.7112) | Xent 0.2110(0.2166) | Loss 3.8188(3.8195) | Error 0.0759(0.0780) Steps 712(711.53) | Grad Norm 2.5868(1.8054) | Total Time 14.00(14.00)\n",
      "Iter 3048 | Time 62.9354(62.3266) | Bit/dim 3.7172(3.7114) | Xent 0.2106(0.2164) | Loss 3.8225(3.8196) | Error 0.0744(0.0779) Steps 706(711.37) | Grad Norm 1.9339(1.8093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0508 | Time 25.5618, Epoch Time 414.1925(415.5448), Bit/dim 3.7302(best: 3.7278), Xent 2.4028, Loss 4.9316, Error 0.4354(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3049 | Time 63.2218(62.3534) | Bit/dim 3.7140(3.7115) | Xent 0.2090(0.2162) | Loss 3.8186(3.8196) | Error 0.0755(0.0778) Steps 712(711.39) | Grad Norm 2.4740(1.8292) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 62.1534(62.3474) | Bit/dim 3.7078(3.7114) | Xent 0.2062(0.2159) | Loss 3.8109(3.8193) | Error 0.0739(0.0777) Steps 712(711.40) | Grad Norm 2.6778(1.8547) | Total Time 14.00(14.00)\n",
      "Iter 3051 | Time 60.8720(62.3032) | Bit/dim 3.7142(3.7115) | Xent 0.2129(0.2158) | Loss 3.8206(3.8194) | Error 0.0767(0.0777) Steps 718(711.60) | Grad Norm 1.6505(1.8485) | Total Time 14.00(14.00)\n",
      "Iter 3052 | Time 64.8432(62.3794) | Bit/dim 3.7091(3.7114) | Xent 0.2111(0.2156) | Loss 3.8146(3.8192) | Error 0.0759(0.0776) Steps 712(711.61) | Grad Norm 1.5081(1.8383) | Total Time 14.00(14.00)\n",
      "Iter 3053 | Time 63.1634(62.4029) | Bit/dim 3.7048(3.7112) | Xent 0.2138(0.2156) | Loss 3.8117(3.8190) | Error 0.0789(0.0776) Steps 712(711.63) | Grad Norm 1.9258(1.8409) | Total Time 14.00(14.00)\n",
      "Iter 3054 | Time 61.9390(62.3890) | Bit/dim 3.7117(3.7112) | Xent 0.2032(0.2152) | Loss 3.8133(3.8188) | Error 0.0736(0.0775) Steps 712(711.64) | Grad Norm 2.3106(1.8550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0509 | Time 25.2764, Epoch Time 416.7482(415.5809), Bit/dim 3.7298(best: 3.7278), Xent 2.4266, Loss 4.9431, Error 0.4384(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3055 | Time 64.2171(62.4438) | Bit/dim 3.7143(3.7113) | Xent 0.2124(0.2151) | Loss 3.8205(3.8189) | Error 0.0765(0.0775) Steps 712(711.65) | Grad Norm 1.2745(1.8376) | Total Time 14.00(14.00)\n",
      "Iter 3056 | Time 62.3850(62.4421) | Bit/dim 3.7057(3.7111) | Xent 0.2065(0.2149) | Loss 3.8089(3.8186) | Error 0.0757(0.0774) Steps 706(711.48) | Grad Norm 2.3033(1.8516) | Total Time 14.00(14.00)\n",
      "Iter 3057 | Time 65.1044(62.5219) | Bit/dim 3.7203(3.7114) | Xent 0.2228(0.2151) | Loss 3.8317(3.8190) | Error 0.0813(0.0776) Steps 712(711.49) | Grad Norm 1.7090(1.8473) | Total Time 14.00(14.00)\n",
      "Iter 3058 | Time 62.8586(62.5320) | Bit/dim 3.7101(3.7114) | Xent 0.2156(0.2151) | Loss 3.8179(3.8189) | Error 0.0813(0.0777) Steps 712(711.51) | Grad Norm 0.9039(1.8190) | Total Time 14.00(14.00)\n",
      "Iter 3059 | Time 60.0387(62.4572) | Bit/dim 3.7136(3.7114) | Xent 0.2075(0.2149) | Loss 3.8173(3.8189) | Error 0.0753(0.0776) Steps 712(711.52) | Grad Norm 1.6674(1.8145) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 62.0308(62.4444) | Bit/dim 3.7094(3.7114) | Xent 0.2164(0.2149) | Loss 3.8176(3.8188) | Error 0.0805(0.0777) Steps 718(711.72) | Grad Norm 2.1734(1.8252) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0510 | Time 25.6426, Epoch Time 417.5828(415.6409), Bit/dim 3.7291(best: 3.7278), Xent 2.3913, Loss 4.9248, Error 0.4338(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3061 | Time 59.7837(62.3646) | Bit/dim 3.7161(3.7115) | Xent 0.2080(0.2147) | Loss 3.8200(3.8189) | Error 0.0749(0.0776) Steps 706(711.55) | Grad Norm 1.0911(1.8032) | Total Time 14.00(14.00)\n",
      "Iter 3062 | Time 63.1488(62.3881) | Bit/dim 3.7144(3.7116) | Xent 0.2086(0.2145) | Loss 3.8186(3.8189) | Error 0.0773(0.0776) Steps 712(711.56) | Grad Norm 1.5069(1.7943) | Total Time 14.00(14.00)\n",
      "Iter 3063 | Time 61.8654(62.3725) | Bit/dim 3.7178(3.7118) | Xent 0.2215(0.2147) | Loss 3.8286(3.8192) | Error 0.0811(0.0777) Steps 706(711.39) | Grad Norm 1.3376(1.7806) | Total Time 14.00(14.00)\n",
      "Iter 3064 | Time 62.3347(62.3713) | Bit/dim 3.7059(3.7116) | Xent 0.2246(0.2150) | Loss 3.8182(3.8191) | Error 0.0824(0.0778) Steps 712(711.41) | Grad Norm 1.1680(1.7622) | Total Time 14.00(14.00)\n",
      "Iter 3065 | Time 61.3909(62.3419) | Bit/dim 3.6990(3.7112) | Xent 0.2127(0.2150) | Loss 3.8054(3.8187) | Error 0.0780(0.0778) Steps 712(711.43) | Grad Norm 1.4742(1.7536) | Total Time 14.00(14.00)\n",
      "Iter 3066 | Time 62.0178(62.3322) | Bit/dim 3.7094(3.7112) | Xent 0.2074(0.2147) | Loss 3.8131(3.8186) | Error 0.0746(0.0777) Steps 712(711.45) | Grad Norm 1.3814(1.7424) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0511 | Time 25.3468, Epoch Time 411.3347(415.5118), Bit/dim 3.7292(best: 3.7278), Xent 2.4122, Loss 4.9353, Error 0.4347(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3067 | Time 64.2363(62.3893) | Bit/dim 3.7042(3.7110) | Xent 0.2106(0.2146) | Loss 3.8095(3.8183) | Error 0.0747(0.0777) Steps 700(711.10) | Grad Norm 1.0921(1.7229) | Total Time 14.00(14.00)\n",
      "Iter 3068 | Time 63.4256(62.4204) | Bit/dim 3.7042(3.7108) | Xent 0.2018(0.2142) | Loss 3.8051(3.8179) | Error 0.0737(0.0775) Steps 712(711.13) | Grad Norm 2.1119(1.7346) | Total Time 14.00(14.00)\n",
      "Iter 3069 | Time 63.0514(62.4393) | Bit/dim 3.7154(3.7109) | Xent 0.2066(0.2140) | Loss 3.8187(3.8179) | Error 0.0756(0.0775) Steps 706(710.98) | Grad Norm 1.8142(1.7370) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 61.6623(62.4160) | Bit/dim 3.7133(3.7110) | Xent 0.2016(0.2136) | Loss 3.8141(3.8178) | Error 0.0720(0.0773) Steps 718(711.19) | Grad Norm 1.5335(1.7309) | Total Time 14.00(14.00)\n",
      "Iter 3071 | Time 63.5779(62.4509) | Bit/dim 3.7117(3.7110) | Xent 0.2115(0.2136) | Loss 3.8174(3.8178) | Error 0.0764(0.0773) Steps 706(711.03) | Grad Norm 1.3715(1.7201) | Total Time 14.00(14.00)\n",
      "Iter 3072 | Time 64.0167(62.4979) | Bit/dim 3.7147(3.7111) | Xent 0.2215(0.2138) | Loss 3.8255(3.8180) | Error 0.0826(0.0774) Steps 706(710.88) | Grad Norm 2.3507(1.7390) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0512 | Time 25.3105, Epoch Time 420.6656(415.6664), Bit/dim 3.7279(best: 3.7278), Xent 2.4299, Loss 4.9428, Error 0.4363(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3073 | Time 61.9587(62.4817) | Bit/dim 3.7118(3.7111) | Xent 0.2115(0.2137) | Loss 3.8176(3.8180) | Error 0.0805(0.0775) Steps 712(710.91) | Grad Norm 1.0407(1.7181) | Total Time 14.00(14.00)\n",
      "Iter 3074 | Time 61.1099(62.4405) | Bit/dim 3.7087(3.7111) | Xent 0.2131(0.2137) | Loss 3.8153(3.8179) | Error 0.0783(0.0776) Steps 706(710.77) | Grad Norm 2.4852(1.7411) | Total Time 14.00(14.00)\n",
      "Iter 3075 | Time 60.8715(62.3935) | Bit/dim 3.7014(3.7108) | Xent 0.2135(0.2137) | Loss 3.8081(3.8176) | Error 0.0780(0.0776) Steps 712(710.80) | Grad Norm 1.7556(1.7415) | Total Time 14.00(14.00)\n",
      "Iter 3076 | Time 63.9240(62.4394) | Bit/dim 3.7190(3.7110) | Xent 0.2176(0.2138) | Loss 3.8278(3.8179) | Error 0.0834(0.0777) Steps 712(710.84) | Grad Norm 2.0886(1.7519) | Total Time 14.00(14.00)\n",
      "Iter 3077 | Time 60.0728(62.3684) | Bit/dim 3.7113(3.7110) | Xent 0.2041(0.2135) | Loss 3.8133(3.8178) | Error 0.0753(0.0777) Steps 712(710.87) | Grad Norm 2.3632(1.7703) | Total Time 14.00(14.00)\n",
      "Iter 3078 | Time 65.6560(62.4670) | Bit/dim 3.7107(3.7110) | Xent 0.2107(0.2134) | Loss 3.8161(3.8177) | Error 0.0810(0.0778) Steps 712(710.91) | Grad Norm 2.6520(1.7967) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0513 | Time 25.5788, Epoch Time 414.6020(415.6344), Bit/dim 3.7282(best: 3.7278), Xent 2.4139, Loss 4.9352, Error 0.4313(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3079 | Time 61.4215(62.4356) | Bit/dim 3.7072(3.7109) | Xent 0.1994(0.2130) | Loss 3.8069(3.8174) | Error 0.0707(0.0776) Steps 712(710.94) | Grad Norm 1.7744(1.7960) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 61.0267(62.3934) | Bit/dim 3.7090(3.7108) | Xent 0.2068(0.2128) | Loss 3.8123(3.8173) | Error 0.0770(0.0775) Steps 706(710.79) | Grad Norm 1.5361(1.7882) | Total Time 14.00(14.00)\n",
      "Iter 3081 | Time 61.2639(62.3595) | Bit/dim 3.7225(3.7112) | Xent 0.2174(0.2130) | Loss 3.8312(3.8177) | Error 0.0783(0.0776) Steps 712(710.83) | Grad Norm 2.8783(1.8209) | Total Time 14.00(14.00)\n",
      "Iter 3082 | Time 60.4308(62.3016) | Bit/dim 3.7239(3.7116) | Xent 0.2151(0.2130) | Loss 3.8314(3.8181) | Error 0.0779(0.0776) Steps 706(710.68) | Grad Norm 1.6526(1.8159) | Total Time 14.00(14.00)\n",
      "Iter 3083 | Time 60.6374(62.2517) | Bit/dim 3.7039(3.7113) | Xent 0.2104(0.2130) | Loss 3.8091(3.8178) | Error 0.0791(0.0776) Steps 706(710.54) | Grad Norm 2.0018(1.8215) | Total Time 14.00(14.00)\n",
      "Iter 3084 | Time 60.6720(62.2043) | Bit/dim 3.7042(3.7111) | Xent 0.2141(0.2130) | Loss 3.8112(3.8176) | Error 0.0773(0.0776) Steps 706(710.41) | Grad Norm 2.4728(1.8410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0514 | Time 25.6075, Epoch Time 406.6311(415.3643), Bit/dim 3.7291(best: 3.7278), Xent 2.4225, Loss 4.9403, Error 0.4342(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3085 | Time 60.4432(62.1515) | Bit/dim 3.7179(3.7113) | Xent 0.2077(0.2128) | Loss 3.8218(3.8178) | Error 0.0743(0.0775) Steps 706(710.27) | Grad Norm 3.0068(1.8760) | Total Time 14.00(14.00)\n",
      "Iter 3086 | Time 60.8862(62.1135) | Bit/dim 3.7127(3.7114) | Xent 0.2098(0.2127) | Loss 3.8176(3.8177) | Error 0.0713(0.0773) Steps 706(710.15) | Grad Norm 1.4406(1.8629) | Total Time 14.00(14.00)\n",
      "Iter 3087 | Time 63.2534(62.1477) | Bit/dim 3.7058(3.7112) | Xent 0.2054(0.2125) | Loss 3.8085(3.8175) | Error 0.0750(0.0773) Steps 706(710.02) | Grad Norm 2.8237(1.8917) | Total Time 14.00(14.00)\n",
      "Iter 3088 | Time 62.6773(62.1636) | Bit/dim 3.7092(3.7112) | Xent 0.2169(0.2127) | Loss 3.8177(3.8175) | Error 0.0791(0.0773) Steps 700(709.72) | Grad Norm 2.6384(1.9141) | Total Time 14.00(14.00)\n",
      "Iter 3089 | Time 61.8885(62.1554) | Bit/dim 3.7021(3.7109) | Xent 0.2104(0.2126) | Loss 3.8073(3.8172) | Error 0.0776(0.0773) Steps 706(709.61) | Grad Norm 2.5946(1.9346) | Total Time 14.00(14.00)\n",
      "Iter 3090 | Time 61.6412(62.1399) | Bit/dim 3.7157(3.7110) | Xent 0.2148(0.2127) | Loss 3.8231(3.8173) | Error 0.0763(0.0773) Steps 718(709.86) | Grad Norm 1.0070(1.9067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0515 | Time 25.4556, Epoch Time 411.4668(415.2474), Bit/dim 3.7286(best: 3.7278), Xent 2.4190, Loss 4.9382, Error 0.4336(best: 0.4227)\n",
      "Iter 3092 | Time 61.1269(62.1154) | Bit/dim 3.7056(3.7108) | Xent 0.2036(0.2121) | Loss 3.8074(3.8168) | Error 0.0744(0.0771) Steps 712(709.99) | Grad Norm 3.8464(2.0137) | Total Time 14.00(14.00)\n",
      "Iter 3093 | Time 62.0028(62.1120) | Bit/dim 3.7125(3.7108) | Xent 0.2189(0.2123) | Loss 3.8220(3.8169) | Error 0.0755(0.0770) Steps 700(709.69) | Grad Norm 1.9956(2.0132) | Total Time 14.00(14.00)\n",
      "Iter 3094 | Time 62.0011(62.1087) | Bit/dim 3.7069(3.7107) | Xent 0.2260(0.2127) | Loss 3.8200(3.8170) | Error 0.0809(0.0772) Steps 706(709.58) | Grad Norm 2.7126(2.0341) | Total Time 14.00(14.00)\n",
      "Iter 3095 | Time 64.7444(62.1878) | Bit/dim 3.7084(3.7106) | Xent 0.2122(0.2127) | Loss 3.8145(3.8170) | Error 0.0775(0.0772) Steps 712(709.65) | Grad Norm 2.7785(2.0565) | Total Time 14.00(14.00)\n",
      "Iter 3096 | Time 63.3839(62.2236) | Bit/dim 3.7240(3.7110) | Xent 0.2057(0.2125) | Loss 3.8269(3.8173) | Error 0.0750(0.0771) Steps 706(709.54) | Grad Norm 1.8581(2.0505) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0516 | Time 25.6085, Epoch Time 416.3682(415.2810), Bit/dim 3.7293(best: 3.7278), Xent 2.4003, Loss 4.9294, Error 0.4376(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3097 | Time 61.8698(62.2130) | Bit/dim 3.7035(3.7108) | Xent 0.2118(0.2124) | Loss 3.8094(3.8170) | Error 0.0774(0.0771) Steps 712(709.61) | Grad Norm 2.4694(2.0631) | Total Time 14.00(14.00)\n",
      "Iter 3098 | Time 62.9772(62.2359) | Bit/dim 3.7079(3.7107) | Xent 0.2078(0.2123) | Loss 3.8118(3.8169) | Error 0.0737(0.0770) Steps 712(709.69) | Grad Norm 1.6704(2.0513) | Total Time 14.00(14.00)\n",
      "Iter 3099 | Time 59.3103(62.1482) | Bit/dim 3.7134(3.7108) | Xent 0.2044(0.2121) | Loss 3.8156(3.8168) | Error 0.0725(0.0769) Steps 706(709.58) | Grad Norm 1.4255(2.0325) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 61.7588(62.1365) | Bit/dim 3.7103(3.7108) | Xent 0.2242(0.2124) | Loss 3.8224(3.8170) | Error 0.0810(0.0770) Steps 706(709.47) | Grad Norm 1.5788(2.0189) | Total Time 14.00(14.00)\n",
      "Iter 3101 | Time 62.7497(62.1549) | Bit/dim 3.7095(3.7107) | Xent 0.2046(0.2122) | Loss 3.8118(3.8168) | Error 0.0741(0.0769) Steps 712(709.54) | Grad Norm 1.1432(1.9927) | Total Time 14.00(14.00)\n",
      "Iter 3102 | Time 61.7547(62.1429) | Bit/dim 3.7115(3.7108) | Xent 0.1974(0.2117) | Loss 3.8102(3.8166) | Error 0.0751(0.0769) Steps 712(709.62) | Grad Norm 1.4650(1.9768) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0517 | Time 25.3223, Epoch Time 411.0245(415.1533), Bit/dim 3.7286(best: 3.7278), Xent 2.4187, Loss 4.9379, Error 0.4323(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3103 | Time 63.0116(62.1690) | Bit/dim 3.7093(3.7107) | Xent 0.2056(0.2116) | Loss 3.8120(3.8165) | Error 0.0741(0.0768) Steps 712(709.69) | Grad Norm 1.4688(1.9616) | Total Time 14.00(14.00)\n",
      "Iter 3104 | Time 64.3029(62.2330) | Bit/dim 3.7211(3.7110) | Xent 0.2035(0.2113) | Loss 3.8229(3.8167) | Error 0.0773(0.0768) Steps 706(709.58) | Grad Norm 0.8596(1.9285) | Total Time 14.00(14.00)\n",
      "Iter 3105 | Time 61.8561(62.2217) | Bit/dim 3.7133(3.7111) | Xent 0.1994(0.2110) | Loss 3.8130(3.8166) | Error 0.0709(0.0766) Steps 718(709.83) | Grad Norm 1.2863(1.9093) | Total Time 14.00(14.00)\n",
      "Iter 3106 | Time 60.8517(62.1806) | Bit/dim 3.7150(3.7112) | Xent 0.1990(0.2106) | Loss 3.8145(3.8165) | Error 0.0760(0.0766) Steps 706(709.72) | Grad Norm 1.9327(1.9100) | Total Time 14.00(14.00)\n",
      "Iter 3107 | Time 59.5561(62.1018) | Bit/dim 3.7091(3.7112) | Xent 0.2102(0.2106) | Loss 3.8142(3.8165) | Error 0.0740(0.0765) Steps 712(709.79) | Grad Norm 1.3920(1.8944) | Total Time 14.00(14.00)\n",
      "Iter 3108 | Time 62.3724(62.1099) | Bit/dim 3.7064(3.7110) | Xent 0.1997(0.2103) | Loss 3.8063(3.8161) | Error 0.0704(0.0763) Steps 706(709.67) | Grad Norm 1.8254(1.8924) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0518 | Time 25.3797, Epoch Time 412.8695(415.0848), Bit/dim 3.7281(best: 3.7278), Xent 2.4444, Loss 4.9503, Error 0.4348(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3109 | Time 61.2883(62.0853) | Bit/dim 3.7111(3.7110) | Xent 0.2054(0.2101) | Loss 3.8138(3.8161) | Error 0.0721(0.0762) Steps 706(709.56) | Grad Norm 1.3657(1.8766) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 62.3530(62.0933) | Bit/dim 3.7068(3.7109) | Xent 0.2048(0.2100) | Loss 3.8092(3.8159) | Error 0.0741(0.0761) Steps 712(709.63) | Grad Norm 1.5165(1.8657) | Total Time 14.00(14.00)\n",
      "Iter 3111 | Time 62.3796(62.1019) | Bit/dim 3.7154(3.7110) | Xent 0.1891(0.2093) | Loss 3.8100(3.8157) | Error 0.0721(0.0760) Steps 700(709.35) | Grad Norm 1.5797(1.8572) | Total Time 14.00(14.00)\n",
      "Iter 3112 | Time 61.8256(62.0936) | Bit/dim 3.7150(3.7111) | Xent 0.2072(0.2093) | Loss 3.8186(3.8158) | Error 0.0721(0.0759) Steps 706(709.25) | Grad Norm 1.6208(1.8501) | Total Time 14.00(14.00)\n",
      "Iter 3113 | Time 64.1083(62.1541) | Bit/dim 3.7151(3.7113) | Xent 0.2120(0.2093) | Loss 3.8211(3.8159) | Error 0.0749(0.0759) Steps 706(709.15) | Grad Norm 1.2268(1.8314) | Total Time 14.00(14.00)\n",
      "Iter 3114 | Time 63.4462(62.1928) | Bit/dim 3.7010(3.7110) | Xent 0.1968(0.2090) | Loss 3.7994(3.8154) | Error 0.0709(0.0757) Steps 706(709.05) | Grad Norm 1.4628(1.8203) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0519 | Time 25.6356, Epoch Time 416.4420(415.1255), Bit/dim 3.7292(best: 3.7278), Xent 2.4418, Loss 4.9501, Error 0.4332(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3115 | Time 64.5440(62.2634) | Bit/dim 3.7175(3.7112) | Xent 0.1959(0.2086) | Loss 3.8155(3.8154) | Error 0.0709(0.0756) Steps 718(709.32) | Grad Norm 1.5613(1.8125) | Total Time 14.00(14.00)\n",
      "Iter 3116 | Time 59.7999(62.1895) | Bit/dim 3.7078(3.7111) | Xent 0.2029(0.2084) | Loss 3.8092(3.8153) | Error 0.0744(0.0755) Steps 706(709.22) | Grad Norm 1.9290(1.8160) | Total Time 14.00(14.00)\n",
      "Iter 3117 | Time 62.4557(62.1974) | Bit/dim 3.7204(3.7113) | Xent 0.2177(0.2087) | Loss 3.8293(3.8157) | Error 0.0757(0.0755) Steps 712(709.31) | Grad Norm 1.5852(1.8091) | Total Time 14.00(14.00)\n",
      "Iter 3118 | Time 63.8780(62.2479) | Bit/dim 3.7163(3.7115) | Xent 0.2032(0.2085) | Loss 3.8179(3.8157) | Error 0.0720(0.0754) Steps 712(709.39) | Grad Norm 2.2877(1.8235) | Total Time 14.00(14.00)\n",
      "Iter 3119 | Time 63.0784(62.2728) | Bit/dim 3.6988(3.7111) | Xent 0.1979(0.2082) | Loss 3.7978(3.8152) | Error 0.0734(0.0754) Steps 706(709.28) | Grad Norm 2.2821(1.8372) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 63.7599(62.3174) | Bit/dim 3.7029(3.7109) | Xent 0.2080(0.2082) | Loss 3.8069(3.8150) | Error 0.0754(0.0754) Steps 712(709.37) | Grad Norm 2.2105(1.8484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0520 | Time 25.3573, Epoch Time 418.3567(415.2225), Bit/dim 3.7283(best: 3.7278), Xent 2.4624, Loss 4.9595, Error 0.4371(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3121 | Time 59.2733(62.2261) | Bit/dim 3.7030(3.7106) | Xent 0.2108(0.2083) | Loss 3.8084(3.8148) | Error 0.0764(0.0754) Steps 718(709.63) | Grad Norm 2.4060(1.8652) | Total Time 14.00(14.00)\n",
      "Iter 3122 | Time 62.1960(62.2252) | Bit/dim 3.7193(3.7109) | Xent 0.1981(0.2080) | Loss 3.8184(3.8149) | Error 0.0715(0.0753) Steps 706(709.52) | Grad Norm 2.0653(1.8712) | Total Time 14.00(14.00)\n",
      "Iter 3123 | Time 62.1034(62.2215) | Bit/dim 3.7058(3.7107) | Xent 0.2072(0.2079) | Loss 3.8094(3.8147) | Error 0.0723(0.0752) Steps 712(709.59) | Grad Norm 1.2002(1.8510) | Total Time 14.00(14.00)\n",
      "Iter 3124 | Time 63.1011(62.2479) | Bit/dim 3.7136(3.7108) | Xent 0.2211(0.2083) | Loss 3.8241(3.8150) | Error 0.0809(0.0754) Steps 712(709.66) | Grad Norm 2.2131(1.8619) | Total Time 14.00(14.00)\n",
      "Iter 3125 | Time 59.8923(62.1772) | Bit/dim 3.7013(3.7105) | Xent 0.1976(0.2080) | Loss 3.8001(3.8145) | Error 0.0706(0.0752) Steps 712(709.73) | Grad Norm 2.6681(1.8861) | Total Time 14.00(14.00)\n",
      "Iter 3126 | Time 65.9976(62.2918) | Bit/dim 3.7157(3.7107) | Xent 0.2102(0.2081) | Loss 3.8208(3.8147) | Error 0.0736(0.0752) Steps 712(709.80) | Grad Norm 1.8370(1.8846) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0521 | Time 25.4201, Epoch Time 413.6582(415.1755), Bit/dim 3.7280(best: 3.7278), Xent 2.4405, Loss 4.9483, Error 0.4348(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3128 | Time 62.2340(62.3541) | Bit/dim 3.7008(3.7107) | Xent 0.2076(0.2080) | Loss 3.8046(3.8147) | Error 0.0760(0.0751) Steps 706(709.58) | Grad Norm 1.3081(1.8676) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run3 --resume ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run3/epoch_400_checkpt.pth --seed 3 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
