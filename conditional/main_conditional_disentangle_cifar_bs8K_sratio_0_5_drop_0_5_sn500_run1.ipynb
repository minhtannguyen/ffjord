{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn500_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=500, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 102.3466(102.3466) | Bit/dim 11.0248(11.0248) | Xent 2.3026(2.3026) | Loss 12.1761(12.1761) | Error 0.8990(0.8990) Steps 574(574.00) | Grad Norm 68.4487(68.4487) | Total Time 14.00(14.00)\n",
      "Iter 0002 | Time 52.8879(100.8629) | Bit/dim 10.5590(11.0109) | Xent 2.2925(2.3023) | Loss 11.7052(12.1620) | Error 0.7655(0.8950) Steps 574(574.00) | Grad Norm 57.9497(68.1337) | Total Time 14.00(14.00)\n",
      "Iter 0003 | Time 52.3438(99.4073) | Bit/dim 10.0550(10.9822) | Xent 2.2796(2.3016) | Loss 11.1948(12.1330) | Error 0.7699(0.8912) Steps 574(574.00) | Grad Norm 45.0244(67.4404) | Total Time 14.00(14.00)\n",
      "Iter 0004 | Time 51.4816(97.9695) | Bit/dim 9.5928(10.9405) | Xent 2.2610(2.3004) | Loss 10.7233(12.0907) | Error 0.7635(0.8874) Steps 574(574.00) | Grad Norm 31.3385(66.3574) | Total Time 14.00(14.00)\n",
      "Iter 0005 | Time 51.7653(96.5834) | Bit/dim 9.1779(10.8876) | Xent 2.2416(2.2986) | Loss 10.2987(12.0369) | Error 0.7594(0.8836) Steps 574(574.00) | Grad Norm 19.2386(64.9438) | Total Time 14.00(14.00)\n",
      "Iter 0006 | Time 51.6446(95.2352) | Bit/dim 8.9904(10.8307) | Xent 2.2224(2.2963) | Loss 10.1016(11.9789) | Error 0.7679(0.8801) Steps 574(574.00) | Grad Norm 19.1249(63.5693) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.2827, Epoch Time 412.8647(412.8647), Bit/dim 8.8737(best: inf), Xent 2.1994, Loss 9.9734, Error 0.7611(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 61.7737(94.2314) | Bit/dim 8.8951(10.7727) | Xent 2.2046(2.2936) | Loss 9.9974(11.9194) | Error 0.7732(0.8769) Steps 574(574.00) | Grad Norm 25.0441(62.4135) | Total Time 14.00(14.00)\n",
      "Iter 0008 | Time 54.6695(93.0445) | Bit/dim 8.8697(10.7156) | Xent 2.1868(2.2904) | Loss 9.9631(11.8608) | Error 0.7666(0.8736) Steps 574(574.00) | Grad Norm 30.2148(61.4475) | Total Time 14.00(14.00)\n",
      "Iter 0009 | Time 52.5067(91.8284) | Bit/dim 8.8951(10.6610) | Xent 2.1637(2.2866) | Loss 9.9769(11.8042) | Error 0.7649(0.8703) Steps 574(574.00) | Grad Norm 31.3594(60.5449) | Total Time 14.00(14.00)\n",
      "Iter 0010 | Time 51.7577(90.6263) | Bit/dim 8.8299(10.6060) | Xent 2.1554(2.2826) | Loss 9.9076(11.7473) | Error 0.7468(0.8666) Steps 574(574.00) | Grad Norm 29.2085(59.6048) | Total Time 14.00(14.00)\n",
      "Iter 0011 | Time 52.4970(89.4824) | Bit/dim 8.7226(10.5495) | Xent 2.1340(2.2782) | Loss 9.7896(11.6886) | Error 0.7332(0.8626) Steps 574(574.00) | Grad Norm 23.9535(58.5353) | Total Time 14.00(14.00)\n",
      "Iter 0012 | Time 50.5606(88.3147) | Bit/dim 8.6132(10.4914) | Xent 2.1255(2.2736) | Loss 9.6760(11.6282) | Error 0.7281(0.8586) Steps 574(574.00) | Grad Norm 19.1252(57.3530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 26.0282, Epoch Time 365.5000(411.4437), Bit/dim 8.4984(best: 8.8737), Xent 2.1092, Loss 9.5531, Error 0.7152(best: 0.7611)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 52.1782(87.2307) | Bit/dim 8.5002(10.4317) | Xent 2.1265(2.2692) | Loss 9.5635(11.5663) | Error 0.7385(0.8550) Steps 574(574.00) | Grad Norm 17.1400(56.1466) | Total Time 14.00(14.00)\n",
      "Iter 0014 | Time 51.9327(86.1717) | Bit/dim 8.3882(10.3704) | Xent 2.1127(2.2645) | Loss 9.4446(11.5026) | Error 0.7368(0.8514) Steps 574(574.00) | Grad Norm 17.5263(54.9880) | Total Time 14.00(14.00)\n",
      "Iter 0015 | Time 51.1217(85.1202) | Bit/dim 8.2983(10.3082) | Xent 2.1010(2.2596) | Loss 9.3489(11.4380) | Error 0.7350(0.8479) Steps 574(574.00) | Grad Norm 17.8564(53.8740) | Total Time 14.00(14.00)\n",
      "Iter 0016 | Time 52.0394(84.1278) | Bit/dim 8.1755(10.2442) | Xent 2.0914(2.2545) | Loss 9.2212(11.3715) | Error 0.7256(0.8443) Steps 574(574.00) | Grad Norm 16.2149(52.7442) | Total Time 14.00(14.00)\n",
      "Iter 0017 | Time 51.2261(83.1407) | Bit/dim 8.0722(10.1791) | Xent 2.0713(2.2491) | Loss 9.1079(11.3036) | Error 0.7001(0.8399) Steps 574(574.00) | Grad Norm 14.1876(51.5875) | Total Time 14.00(14.00)\n",
      "Iter 0018 | Time 52.0917(82.2093) | Bit/dim 7.9489(10.1122) | Xent 2.0440(2.2429) | Loss 8.9709(11.2336) | Error 0.6838(0.8353) Steps 574(574.00) | Grad Norm 12.2513(50.4075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 25.5684, Epoch Time 351.5539(409.6470), Bit/dim 7.8375(best: 8.4984), Xent 2.0413, Loss 8.8582, Error 0.6809(best: 0.7152)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 50.7685(81.2660) | Bit/dim 7.8427(10.0441) | Xent 2.0489(2.2371) | Loss 8.8672(11.1626) | Error 0.6879(0.8308) Steps 574(574.00) | Grad Norm 11.4636(49.2391) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 50.4096(80.3403) | Bit/dim 7.7389(9.9749) | Xent 2.0496(2.2315) | Loss 8.7637(11.0907) | Error 0.6861(0.8265) Steps 574(574.00) | Grad Norm 10.7693(48.0850) | Total Time 14.00(14.00)\n",
      "Iter 0021 | Time 51.5982(79.4781) | Bit/dim 7.6365(9.9048) | Xent 2.0360(2.2256) | Loss 8.6545(11.0176) | Error 0.6895(0.8224) Steps 574(574.00) | Grad Norm 9.3399(46.9227) | Total Time 14.00(14.00)\n",
      "Iter 0022 | Time 51.6131(78.6421) | Bit/dim 7.5627(9.8345) | Xent 2.0442(2.2202) | Loss 8.5848(10.9446) | Error 0.7009(0.8187) Steps 574(574.00) | Grad Norm 8.1688(45.7601) | Total Time 14.00(14.00)\n",
      "Iter 0023 | Time 50.7021(77.8039) | Bit/dim 7.5090(9.7648) | Xent 2.0423(2.2148) | Loss 8.5301(10.8722) | Error 0.6991(0.8152) Steps 574(574.00) | Grad Norm 8.4392(44.6405) | Total Time 14.00(14.00)\n",
      "Iter 0024 | Time 50.7887(76.9935) | Bit/dim 7.4721(9.6960) | Xent 2.0598(2.2102) | Loss 8.5020(10.8011) | Error 0.7116(0.8120) Steps 574(574.00) | Grad Norm 9.3209(43.5809) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 25.7675, Epoch Time 347.0723(407.7698), Bit/dim 7.4309(best: 7.8375), Xent 2.0494, Loss 8.4556, Error 0.6992(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 50.2857(76.1922) | Bit/dim 7.4375(9.6282) | Xent 2.0530(2.2054) | Loss 8.4640(10.7309) | Error 0.7060(0.8089) Steps 574(574.00) | Grad Norm 9.1611(42.5483) | Total Time 14.00(14.00)\n",
      "Iter 0026 | Time 50.6760(75.4268) | Bit/dim 7.3735(9.5606) | Xent 2.0568(2.2010) | Loss 8.4019(10.6611) | Error 0.7091(0.8059) Steps 574(574.00) | Grad Norm 8.1070(41.5150) | Total Time 14.00(14.00)\n",
      "Iter 0027 | Time 50.4874(74.6786) | Bit/dim 7.3085(9.4930) | Xent 2.0625(2.1968) | Loss 8.3397(10.5914) | Error 0.7001(0.8027) Steps 574(574.00) | Grad Norm 6.9575(40.4783) | Total Time 14.00(14.00)\n",
      "Iter 0028 | Time 51.4429(73.9815) | Bit/dim 7.2425(9.4255) | Xent 2.0731(2.1931) | Loss 8.2791(10.5221) | Error 0.7104(0.7999) Steps 574(574.00) | Grad Norm 6.3178(39.4535) | Total Time 14.00(14.00)\n",
      "Iter 0029 | Time 50.6460(73.2814) | Bit/dim 7.1815(9.3582) | Xent 2.0666(2.1893) | Loss 8.2148(10.4528) | Error 0.7032(0.7970) Steps 574(574.00) | Grad Norm 5.4131(38.4323) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 53.2179(72.6795) | Bit/dim 7.1455(9.2918) | Xent 2.0646(2.1856) | Loss 8.1778(10.3846) | Error 0.7067(0.7943) Steps 580(574.18) | Grad Norm 4.6862(37.4199) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 25.8358, Epoch Time 348.0608(405.9785), Bit/dim 7.1266(best: 7.4309), Xent 2.0670, Loss 8.1601, Error 0.7100(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 53.3634(72.1000) | Bit/dim 7.1284(9.2269) | Xent 2.0687(2.1821) | Loss 8.1627(10.3179) | Error 0.7149(0.7919) Steps 580(574.35) | Grad Norm 4.7418(36.4396) | Total Time 14.00(14.00)\n",
      "Iter 0032 | Time 52.4322(71.5100) | Bit/dim 7.1100(9.1634) | Xent 2.0748(2.1789) | Loss 8.1474(10.2528) | Error 0.7355(0.7902) Steps 580(574.52) | Grad Norm 4.8379(35.4915) | Total Time 14.00(14.00)\n",
      "Iter 0033 | Time 52.4266(70.9375) | Bit/dim 7.1024(9.1016) | Xent 2.0779(2.1758) | Loss 8.1414(10.1895) | Error 0.7400(0.7887) Steps 580(574.69) | Grad Norm 4.9846(34.5763) | Total Time 14.00(14.00)\n",
      "Iter 0034 | Time 53.0903(70.4021) | Bit/dim 7.0888(9.0412) | Xent 2.0744(2.1728) | Loss 8.1260(10.1276) | Error 0.7448(0.7874) Steps 580(574.85) | Grad Norm 4.8199(33.6836) | Total Time 14.00(14.00)\n",
      "Iter 0035 | Time 53.1935(69.8858) | Bit/dim 7.0861(8.9825) | Xent 2.0595(2.1694) | Loss 8.1159(10.0672) | Error 0.7230(0.7855) Steps 580(575.00) | Grad Norm 5.1960(32.8290) | Total Time 14.00(14.00)\n",
      "Iter 0036 | Time 52.5337(69.3653) | Bit/dim 7.0570(8.9248) | Xent 2.0426(2.1656) | Loss 8.0783(10.0076) | Error 0.7159(0.7834) Steps 580(575.15) | Grad Norm 5.1146(31.9975) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 25.7203, Epoch Time 358.4671(404.5532), Bit/dim 7.0327(best: 7.1266), Xent 2.0371, Loss 8.0512, Error 0.6990(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 53.6733(68.8945) | Bit/dim 7.0303(8.8679) | Xent 2.0493(2.1621) | Loss 8.0549(9.9490) | Error 0.7077(0.7811) Steps 580(575.30) | Grad Norm 7.7955(31.2715) | Total Time 14.00(14.00)\n",
      "Iter 0038 | Time 53.6809(68.4381) | Bit/dim 7.0078(8.8121) | Xent 2.0725(2.1594) | Loss 8.0441(9.8918) | Error 0.7366(0.7798) Steps 580(575.44) | Grad Norm 14.0491(30.7548) | Total Time 14.00(14.00)\n",
      "Iter 0039 | Time 53.3048(67.9841) | Bit/dim 7.0105(8.7581) | Xent 2.1152(2.1581) | Loss 8.0681(9.8371) | Error 0.7728(0.7796) Steps 580(575.58) | Grad Norm 22.4503(30.5057) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 52.4975(67.5195) | Bit/dim 7.0084(8.7056) | Xent 2.1337(2.1574) | Loss 8.0752(9.7843) | Error 0.7737(0.7794) Steps 580(575.71) | Grad Norm 25.8517(30.3661) | Total Time 14.00(14.00)\n",
      "Iter 0041 | Time 52.4799(67.0683) | Bit/dim 6.9965(8.6543) | Xent 2.0754(2.1549) | Loss 8.0342(9.7318) | Error 0.7469(0.7784) Steps 580(575.84) | Grad Norm 14.8828(29.9016) | Total Time 14.00(14.00)\n",
      "Iter 0042 | Time 53.7804(66.6697) | Bit/dim 6.9806(8.6041) | Xent 2.0345(2.1513) | Loss 7.9979(9.6797) | Error 0.7169(0.7766) Steps 580(575.96) | Grad Norm 5.5876(29.1721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 26.2852, Epoch Time 362.3114(403.2859), Bit/dim 6.9823(best: 7.0327), Xent 2.0806, Loss 8.0226, Error 0.7632(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 54.5165(66.3051) | Bit/dim 6.9796(8.5554) | Xent 2.0979(2.1497) | Loss 8.0285(9.6302) | Error 0.7731(0.7765) Steps 586(576.26) | Grad Norm 21.6466(28.9464) | Total Time 14.00(14.00)\n",
      "Iter 0044 | Time 53.2579(65.9137) | Bit/dim 6.9722(8.5079) | Xent 2.1294(2.1491) | Loss 8.0369(9.5824) | Error 0.7731(0.7764) Steps 586(576.56) | Grad Norm 25.6098(28.8463) | Total Time 14.00(14.00)\n",
      "Iter 0045 | Time 53.4220(65.5389) | Bit/dim 6.9549(8.4613) | Xent 2.0389(2.1458) | Loss 7.9743(9.5342) | Error 0.7330(0.7751) Steps 592(577.02) | Grad Norm 13.3259(28.3807) | Total Time 14.00(14.00)\n",
      "Iter 0046 | Time 54.7128(65.2141) | Bit/dim 6.9349(8.4155) | Xent 2.0254(2.1422) | Loss 7.9476(9.4866) | Error 0.7090(0.7731) Steps 592(577.47) | Grad Norm 5.6497(27.6987) | Total Time 14.00(14.00)\n",
      "Iter 0047 | Time 54.7976(64.9016) | Bit/dim 6.9316(8.3710) | Xent 2.0434(2.1392) | Loss 7.9533(9.4406) | Error 0.7180(0.7714) Steps 592(577.90) | Grad Norm 14.6140(27.3062) | Total Time 14.00(14.00)\n",
      "Iter 0048 | Time 57.1724(64.6698) | Bit/dim 6.9093(8.3271) | Xent 2.0233(2.1357) | Loss 7.9210(9.3950) | Error 0.7154(0.7698) Steps 598(578.51) | Grad Norm 9.8799(26.7834) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 26.6656, Epoch Time 377.1637(402.5023), Bit/dim 6.9078(best: 6.9823), Xent 1.9973, Loss 7.9064, Error 0.6837(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 56.0720(64.4118) | Bit/dim 6.9079(8.2845) | Xent 2.0028(2.1317) | Loss 7.9092(9.3504) | Error 0.6895(0.7674) Steps 598(579.09) | Grad Norm 3.0501(26.0714) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 56.9485(64.1879) | Bit/dim 6.8975(8.2429) | Xent 2.0285(2.1286) | Loss 7.9118(9.3073) | Error 0.7069(0.7655) Steps 598(579.66) | Grad Norm 11.8272(25.6441) | Total Time 14.00(14.00)\n",
      "Iter 0051 | Time 56.1267(63.9461) | Bit/dim 6.8914(8.2024) | Xent 2.0300(2.1257) | Loss 7.9064(9.2652) | Error 0.7216(0.7642) Steps 598(580.21) | Grad Norm 12.0550(25.2364) | Total Time 14.00(14.00)\n",
      "Iter 0052 | Time 55.6685(63.6978) | Bit/dim 6.8757(8.1626) | Xent 1.9980(2.1218) | Loss 7.8747(9.2235) | Error 0.6715(0.7614) Steps 598(580.74) | Grad Norm 3.1892(24.5750) | Total Time 14.00(14.00)\n",
      "Iter 0053 | Time 55.6132(63.4552) | Bit/dim 6.8676(8.1237) | Xent 2.0195(2.1188) | Loss 7.8773(9.1831) | Error 0.6934(0.7594) Steps 598(581.26) | Grad Norm 10.5391(24.1539) | Total Time 14.00(14.00)\n",
      "Iter 0054 | Time 56.3965(63.2435) | Bit/dim 6.8563(8.0857) | Xent 2.0397(2.1164) | Loss 7.8761(9.1439) | Error 0.7286(0.7585) Steps 598(581.76) | Grad Norm 17.1726(23.9445) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 26.6675, Epoch Time 379.1251(401.8009), Bit/dim 6.8380(best: 6.9078), Xent 2.0132, Loss 7.8446, Error 0.6826(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 55.7467(63.0186) | Bit/dim 6.8344(8.0482) | Xent 2.0223(2.1136) | Loss 7.8455(9.1050) | Error 0.6934(0.7565) Steps 598(582.25) | Grad Norm 13.5875(23.6338) | Total Time 14.00(14.00)\n",
      "Iter 0056 | Time 57.1688(62.8431) | Bit/dim 6.8070(8.0109) | Xent 1.9979(2.1101) | Loss 7.8060(9.0660) | Error 0.6874(0.7544) Steps 598(582.72) | Grad Norm 3.1866(23.0203) | Total Time 14.00(14.00)\n",
      "Iter 0057 | Time 55.1774(62.6131) | Bit/dim 6.8074(7.9748) | Xent 2.0074(2.1070) | Loss 7.8111(9.0283) | Error 0.6943(0.7526) Steps 598(583.18) | Grad Norm 9.1664(22.6047) | Total Time 14.00(14.00)\n",
      "Iter 0058 | Time 55.9945(62.4145) | Bit/dim 6.7965(7.9395) | Xent 2.0253(2.1046) | Loss 7.8092(8.9918) | Error 0.6965(0.7510) Steps 598(583.63) | Grad Norm 16.7364(22.4287) | Total Time 14.00(14.00)\n",
      "Iter 0059 | Time 56.2753(62.2304) | Bit/dim 6.7806(7.9047) | Xent 2.0289(2.1023) | Loss 7.7950(8.9559) | Error 0.7272(0.7502) Steps 598(584.06) | Grad Norm 19.2568(22.3335) | Total Time 14.00(14.00)\n",
      "Iter 0060 | Time 56.9453(62.0718) | Bit/dim 6.7528(7.8702) | Xent 1.9997(2.0992) | Loss 7.7526(8.9198) | Error 0.6858(0.7483) Steps 598(584.47) | Grad Norm 15.6089(22.1318) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 27.0399, Epoch Time 380.0955(401.1498), Bit/dim 6.7239(best: 6.8380), Xent 1.9824, Loss 7.7151, Error 0.6693(best: 0.6809)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 54.8725(61.8558) | Bit/dim 6.7220(7.8357) | Xent 1.9980(2.0962) | Loss 7.7209(8.8838) | Error 0.6827(0.7463) Steps 598(584.88) | Grad Norm 9.3203(21.7474) | Total Time 14.00(14.00)\n",
      "Iter 0062 | Time 55.2488(61.6576) | Bit/dim 6.6918(7.8014) | Xent 2.0012(2.0933) | Loss 7.6925(8.8481) | Error 0.6816(0.7444) Steps 598(585.27) | Grad Norm 8.7470(21.3574) | Total Time 14.00(14.00)\n",
      "Iter 0063 | Time 56.6917(61.5086) | Bit/dim 6.6839(7.7679) | Xent 2.0199(2.0911) | Loss 7.6939(8.8134) | Error 0.7123(0.7434) Steps 592(585.48) | Grad Norm 12.8059(21.1009) | Total Time 14.00(14.00)\n",
      "Iter 0064 | Time 56.9855(61.3730) | Bit/dim 6.6486(7.7343) | Xent 2.0383(2.0896) | Loss 7.6678(8.7791) | Error 0.7160(0.7426) Steps 598(585.85) | Grad Norm 16.2087(20.9541) | Total Time 14.00(14.00)\n",
      "Iter 0065 | Time 55.6903(61.2025) | Bit/dim 6.6269(7.7011) | Xent 2.0546(2.0885) | Loss 7.6542(8.7453) | Error 0.7409(0.7426) Steps 592(586.04) | Grad Norm 20.1301(20.9294) | Total Time 14.00(14.00)\n",
      "Iter 0066 | Time 57.1919(61.0822) | Bit/dim 6.6169(7.6685) | Xent 2.1361(2.0899) | Loss 7.6850(8.7135) | Error 0.7456(0.7427) Steps 598(586.39) | Grad Norm 26.2055(21.0877) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 26.3195, Epoch Time 380.0415(400.5165), Bit/dim 6.5906(best: 6.7239), Xent 2.1476, Loss 7.6644, Error 0.7851(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 55.4413(60.9129) | Bit/dim 6.5833(7.6360) | Xent 2.1456(2.0916) | Loss 7.6562(8.6818) | Error 0.7853(0.7439) Steps 586(586.38) | Grad Norm 31.3522(21.3956) | Total Time 14.00(14.00)\n",
      "Iter 0068 | Time 55.9407(60.7638) | Bit/dim 6.5557(7.6036) | Xent 2.2244(2.0956) | Loss 7.6679(8.6514) | Error 0.7937(0.7454) Steps 592(586.55) | Grad Norm 35.5312(21.8197) | Total Time 14.00(14.00)\n",
      "Iter 0069 | Time 56.4901(60.6356) | Bit/dim 6.7083(7.5767) | Xent 2.6085(2.1110) | Loss 8.0125(8.6322) | Error 0.8320(0.7480) Steps 586(586.54) | Grad Norm 81.6578(23.6148) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 53.7886(60.4301) | Bit/dim 7.3922(7.5712) | Xent 3.4418(2.1509) | Loss 9.1131(8.6466) | Error 0.8759(0.7519) Steps 568(585.98) | Grad Norm 126.3307(26.6963) | Total Time 14.00(14.00)\n",
      "Iter 0071 | Time 56.2010(60.3033) | Bit/dim 6.5503(7.5406) | Xent 2.4350(2.1594) | Loss 7.7678(8.6203) | Error 0.8678(0.7553) Steps 586(585.98) | Grad Norm 47.8537(27.3310) | Total Time 14.00(14.00)\n",
      "Iter 0072 | Time 54.4793(60.1286) | Bit/dim 7.1307(7.5283) | Xent 2.1757(2.1599) | Loss 8.2186(8.6082) | Error 0.8106(0.7570) Steps 568(585.44) | Grad Norm 58.1162(28.2546) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 26.1021, Epoch Time 383.0523(399.9926), Bit/dim 6.6386(best: 6.5906), Xent 2.1719, Loss 7.7246, Error 0.7760(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 55.8287(59.9996) | Bit/dim 6.6326(7.5014) | Xent 2.1888(2.1608) | Loss 7.7270(8.5818) | Error 0.7777(0.7576) Steps 586(585.46) | Grad Norm 25.8656(28.1829) | Total Time 14.00(14.00)\n",
      "Iter 0074 | Time 55.4350(59.8626) | Bit/dim 6.5584(7.4731) | Xent 2.1391(2.1601) | Loss 7.6280(8.5532) | Error 0.7654(0.7579) Steps 574(585.11) | Grad Norm 18.4799(27.8918) | Total Time 14.00(14.00)\n",
      "Iter 0075 | Time 52.9701(59.6558) | Bit/dim 6.7398(7.4511) | Xent 2.2057(2.1615) | Loss 7.8426(8.5319) | Error 0.7428(0.7574) Steps 574(584.78) | Grad Norm 27.3866(27.8767) | Total Time 14.00(14.00)\n",
      "Iter 0076 | Time 52.5825(59.4436) | Bit/dim 6.6227(7.4263) | Xent 2.2042(2.1628) | Loss 7.7248(8.5076) | Error 0.7899(0.7584) Steps 574(584.46) | Grad Norm 16.8885(27.5470) | Total Time 14.00(14.00)\n",
      "Iter 0077 | Time 51.6801(59.2107) | Bit/dim 6.5139(7.3989) | Xent 2.2482(2.1653) | Loss 7.6380(8.4816) | Error 0.8001(0.7596) Steps 568(583.96) | Grad Norm 19.0395(27.2918) | Total Time 14.00(14.00)\n",
      "Iter 0078 | Time 53.0859(59.0270) | Bit/dim 6.4648(7.3709) | Xent 2.1674(2.1654) | Loss 7.5486(8.4536) | Error 0.7833(0.7603) Steps 568(583.48) | Grad Norm 10.7357(26.7951) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 25.2841, Epoch Time 367.6670(399.0228), Bit/dim 6.4798(best: 6.5906), Xent 2.2314, Loss 7.5955, Error 0.7835(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 53.4069(58.8584) | Bit/dim 6.4828(7.3442) | Xent 2.2461(2.1678) | Loss 7.6058(8.4281) | Error 0.7976(0.7615) Steps 568(583.02) | Grad Norm 15.3858(26.4528) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 51.8206(58.6473) | Bit/dim 6.4883(7.3185) | Xent 2.1802(2.1682) | Loss 7.5784(8.4026) | Error 0.7840(0.7621) Steps 568(582.57) | Grad Norm 10.9653(25.9882) | Total Time 14.00(14.00)\n",
      "Iter 0081 | Time 52.6424(58.4671) | Bit/dim 6.4340(7.2920) | Xent 2.2266(2.1699) | Loss 7.5472(8.3770) | Error 0.8113(0.7636) Steps 568(582.13) | Grad Norm 15.8253(25.6833) | Total Time 14.00(14.00)\n",
      "Iter 0082 | Time 53.8496(58.3286) | Bit/dim 6.3238(7.2630) | Xent 2.1583(2.1696) | Loss 7.4030(8.3478) | Error 0.7652(0.7637) Steps 574(581.89) | Grad Norm 6.8671(25.1188) | Total Time 14.00(14.00)\n",
      "Iter 0083 | Time 54.6741(58.2190) | Bit/dim 6.2988(7.2340) | Xent 2.1840(2.1700) | Loss 7.3907(8.3190) | Error 0.7695(0.7638) Steps 574(581.65) | Grad Norm 11.6915(24.7160) | Total Time 14.00(14.00)\n",
      "Iter 0084 | Time 54.9926(58.1222) | Bit/dim 6.2676(7.2050) | Xent 2.1599(2.1697) | Loss 7.3475(8.2899) | Error 0.7729(0.7641) Steps 580(581.60) | Grad Norm 8.1049(24.2177) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 25.7440, Epoch Time 362.6752(397.9324), Bit/dim 6.2251(best: 6.4798), Xent 2.1535, Loss 7.3018, Error 0.7754(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 54.7536(58.0211) | Bit/dim 6.2257(7.1757) | Xent 2.1512(2.1692) | Loss 7.3013(8.2602) | Error 0.7742(0.7644) Steps 586(581.73) | Grad Norm 8.3343(23.7412) | Total Time 14.00(14.00)\n",
      "Iter 0086 | Time 51.8733(57.8367) | Bit/dim 6.1777(7.1457) | Xent 2.1483(2.1685) | Loss 7.2518(8.2300) | Error 0.7642(0.7644) Steps 568(581.32) | Grad Norm 9.3962(23.3108) | Total Time 14.00(14.00)\n",
      "Iter 0087 | Time 53.2413(57.6988) | Bit/dim 6.1167(7.1149) | Xent 2.1526(2.1681) | Loss 7.1931(8.1989) | Error 0.7712(0.7646) Steps 562(580.74) | Grad Norm 5.6206(22.7801) | Total Time 14.00(14.00)\n",
      "Iter 0088 | Time 54.0032(57.5879) | Bit/dim 6.0591(7.0832) | Xent 2.1536(2.1676) | Loss 7.1358(8.1670) | Error 0.7671(0.7647) Steps 562(580.18) | Grad Norm 7.0807(22.3091) | Total Time 14.00(14.00)\n",
      "Iter 0089 | Time 52.5442(57.4366) | Bit/dim 6.0250(7.0514) | Xent 2.1314(2.1665) | Loss 7.0907(8.1347) | Error 0.7581(0.7645) Steps 568(579.81) | Grad Norm 5.5060(21.8050) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 53.4868(57.3181) | Bit/dim 5.9876(7.0195) | Xent 2.1291(2.1654) | Loss 7.0522(8.1022) | Error 0.7499(0.7640) Steps 580(579.82) | Grad Norm 7.0501(21.3624) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 25.6573, Epoch Time 363.2749(396.8927), Bit/dim 5.9466(best: 6.2251), Xent 2.1071, Loss 7.0001, Error 0.7431(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 53.4083(57.2008) | Bit/dim 5.9430(6.9872) | Xent 2.1239(2.1642) | Loss 7.0049(8.0693) | Error 0.7516(0.7637) Steps 580(579.83) | Grad Norm 3.4495(20.8250) | Total Time 14.00(14.00)\n",
      "Iter 0092 | Time 53.5036(57.0899) | Bit/dim 5.9431(6.9559) | Xent 2.1369(2.1634) | Loss 7.0115(8.0376) | Error 0.7791(0.7641) Steps 580(579.83) | Grad Norm 6.2166(20.3868) | Total Time 14.00(14.00)\n",
      "Iter 0093 | Time 52.2685(56.9453) | Bit/dim 5.9184(6.9248) | Xent 2.1105(2.1618) | Loss 6.9736(8.0057) | Error 0.7538(0.7638) Steps 568(579.48) | Grad Norm 3.9397(19.8933) | Total Time 14.00(14.00)\n",
      "Iter 0094 | Time 51.7657(56.7899) | Bit/dim 5.8879(6.8937) | Xent 2.1217(2.1606) | Loss 6.9487(7.9739) | Error 0.7419(0.7632) Steps 568(579.13) | Grad Norm 5.5580(19.4633) | Total Time 14.00(14.00)\n",
      "Iter 0095 | Time 52.4913(56.6609) | Bit/dim 5.8467(6.8623) | Xent 2.1160(2.1592) | Loss 6.9047(7.9419) | Error 0.7459(0.7626) Steps 574(578.98) | Grad Norm 4.5099(19.0147) | Total Time 14.00(14.00)\n",
      "Iter 0096 | Time 52.7457(56.5435) | Bit/dim 5.8409(6.8316) | Xent 2.1131(2.1578) | Loss 6.8975(7.9105) | Error 0.7415(0.7620) Steps 574(578.83) | Grad Norm 3.5333(18.5502) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 25.8176, Epoch Time 357.5813(395.7133), Bit/dim 5.8110(best: 5.9466), Xent 2.0730, Loss 6.8475, Error 0.7277(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 51.9292(56.4050) | Bit/dim 5.8159(6.8011) | Xent 2.0860(2.1557) | Loss 6.8590(7.8790) | Error 0.7398(0.7613) Steps 574(578.68) | Grad Norm 3.7062(18.1049) | Total Time 14.00(14.00)\n",
      "Iter 0098 | Time 53.6652(56.3229) | Bit/dim 5.7955(6.7710) | Xent 2.0904(2.1537) | Loss 6.8407(7.8478) | Error 0.7270(0.7603) Steps 574(578.54) | Grad Norm 3.9135(17.6792) | Total Time 14.00(14.00)\n",
      "Iter 0099 | Time 53.0019(56.2232) | Bit/dim 5.7838(6.7414) | Xent 2.1008(2.1521) | Loss 6.8342(7.8174) | Error 0.7341(0.7595) Steps 574(578.41) | Grad Norm 3.8067(17.2630) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 53.3294(56.1364) | Bit/dim 5.7412(6.7114) | Xent 2.0636(2.1495) | Loss 6.7730(7.7861) | Error 0.7145(0.7582) Steps 568(578.09) | Grad Norm 2.2892(16.8138) | Total Time 14.00(14.00)\n",
      "Iter 0101 | Time 50.5179(55.9679) | Bit/dim 5.7468(6.6824) | Xent 2.0930(2.1478) | Loss 6.7933(7.7563) | Error 0.7452(0.7578) Steps 562(577.61) | Grad Norm 4.5888(16.4470) | Total Time 14.00(14.00)\n",
      "Iter 0102 | Time 50.3959(55.8007) | Bit/dim 5.7230(6.6536) | Xent 2.0712(2.1455) | Loss 6.7586(7.7264) | Error 0.7212(0.7567) Steps 562(577.14) | Grad Norm 3.4887(16.0583) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 25.6487, Epoch Time 354.3786(394.4733), Bit/dim 5.7087(best: 5.8110), Xent 2.0465, Loss 6.7319, Error 0.6920(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 51.6857(55.6772) | Bit/dim 5.7065(6.6252) | Xent 2.0730(2.1433) | Loss 6.7430(7.6969) | Error 0.7100(0.7553) Steps 568(576.87) | Grad Norm 3.4788(15.6809) | Total Time 14.00(14.00)\n",
      "Iter 0104 | Time 51.1849(55.5425) | Bit/dim 5.6740(6.5967) | Xent 2.0654(2.1410) | Loss 6.7067(7.6672) | Error 0.7228(0.7543) Steps 568(576.60) | Grad Norm 4.4052(15.3426) | Total Time 14.00(14.00)\n",
      "Iter 0105 | Time 52.0403(55.4374) | Bit/dim 5.6931(6.5696) | Xent 2.0574(2.1385) | Loss 6.7218(7.6388) | Error 0.7246(0.7534) Steps 568(576.34) | Grad Norm 3.7897(14.9960) | Total Time 14.00(14.00)\n",
      "Iter 0106 | Time 51.2115(55.3106) | Bit/dim 5.6661(6.5425) | Xent 2.0520(2.1359) | Loss 6.6921(7.6104) | Error 0.7145(0.7523) Steps 568(576.09) | Grad Norm 3.2543(14.6438) | Total Time 14.00(14.00)\n",
      "Iter 0107 | Time 51.0941(55.1841) | Bit/dim 5.6642(6.5161) | Xent 2.0483(2.1332) | Loss 6.6883(7.5827) | Error 0.7009(0.7507) Steps 562(575.67) | Grad Norm 4.3419(14.3347) | Total Time 14.00(14.00)\n",
      "Iter 0108 | Time 51.0747(55.0609) | Bit/dim 5.6487(6.4901) | Xent 2.0463(2.1306) | Loss 6.6719(7.5554) | Error 0.7139(0.7496) Steps 562(575.26) | Grad Norm 3.8863(14.0213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 26.0045, Epoch Time 349.9001(393.1361), Bit/dim 5.6363(best: 5.7087), Xent 2.0178, Loss 6.6452, Error 0.6896(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 51.3333(54.9490) | Bit/dim 5.6408(6.4646) | Xent 2.0322(2.1277) | Loss 6.6569(7.5285) | Error 0.7077(0.7484) Steps 568(575.04) | Grad Norm 5.0888(13.7533) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 51.1381(54.8347) | Bit/dim 5.6150(6.4391) | Xent 2.0256(2.1246) | Loss 6.6278(7.5014) | Error 0.7124(0.7473) Steps 556(574.47) | Grad Norm 3.0759(13.4330) | Total Time 14.00(14.00)\n",
      "Iter 0111 | Time 50.7275(54.7115) | Bit/dim 5.6109(6.4143) | Xent 2.0289(2.1218) | Loss 6.6253(7.4752) | Error 0.6947(0.7457) Steps 556(573.92) | Grad Norm 2.9327(13.1180) | Total Time 14.00(14.00)\n",
      "Iter 0112 | Time 50.7788(54.5935) | Bit/dim 5.5981(6.3898) | Xent 2.0188(2.1187) | Loss 6.6075(7.4491) | Error 0.6959(0.7442) Steps 556(573.38) | Grad Norm 4.6930(12.8652) | Total Time 14.00(14.00)\n",
      "Iter 0113 | Time 50.7461(54.4781) | Bit/dim 5.5994(6.3661) | Xent 2.0286(2.1160) | Loss 6.6137(7.4241) | Error 0.7115(0.7432) Steps 550(572.68) | Grad Norm 6.7461(12.6817) | Total Time 14.00(14.00)\n",
      "Iter 0114 | Time 51.2180(54.3803) | Bit/dim 5.5880(6.3427) | Xent 2.0195(2.1131) | Loss 6.5978(7.3993) | Error 0.6954(0.7418) Steps 550(572.00) | Grad Norm 11.4885(12.6459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 24.7209, Epoch Time 346.4211(391.7346), Bit/dim 5.5935(best: 5.6363), Xent 2.0539, Loss 6.6204, Error 0.7553(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 50.2629(54.2568) | Bit/dim 5.5756(6.3197) | Xent 2.0578(2.1114) | Loss 6.6045(7.3754) | Error 0.7520(0.7421) Steps 550(571.34) | Grad Norm 23.2374(12.9636) | Total Time 14.00(14.00)\n",
      "Iter 0116 | Time 49.4319(54.1120) | Bit/dim 5.7015(6.3012) | Xent 2.2956(2.1169) | Loss 6.8493(7.3597) | Error 0.7960(0.7437) Steps 532(570.16) | Grad Norm 47.3826(13.9962) | Total Time 14.00(14.00)\n",
      "Iter 0117 | Time 53.0895(54.0813) | Bit/dim 5.7815(6.2856) | Xent 2.5000(2.1284) | Loss 7.0316(7.3498) | Error 0.8468(0.7468) Steps 550(569.55) | Grad Norm 64.0356(15.4974) | Total Time 14.00(14.00)\n",
      "Iter 0118 | Time 50.2658(53.9669) | Bit/dim 5.7009(6.2681) | Xent 2.2800(2.1330) | Loss 6.8409(7.3345) | Error 0.7921(0.7482) Steps 544(568.79) | Grad Norm 21.9805(15.6919) | Total Time 14.00(14.00)\n",
      "Iter 0119 | Time 51.9693(53.9069) | Bit/dim 5.7785(6.2534) | Xent 2.2615(2.1368) | Loss 6.9092(7.3218) | Error 0.7981(0.7497) Steps 544(568.04) | Grad Norm 26.2185(16.0077) | Total Time 14.00(14.00)\n",
      "Iter 0120 | Time 49.4036(53.7718) | Bit/dim 5.6690(6.2358) | Xent 2.0770(2.1350) | Loss 6.7075(7.3034) | Error 0.7330(0.7492) Steps 532(566.96) | Grad Norm 7.3536(15.7480) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 24.8233, Epoch Time 344.7313(390.3245), Bit/dim 5.6402(best: 5.5935), Xent 2.1111, Loss 6.6958, Error 0.7442(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 49.0072(53.6289) | Bit/dim 5.6281(6.2176) | Xent 2.1291(2.1349) | Loss 6.6927(7.2850) | Error 0.7481(0.7491) Steps 538(566.09) | Grad Norm 8.8823(15.5421) | Total Time 14.00(14.00)\n",
      "Iter 0122 | Time 51.1354(53.5541) | Bit/dim 5.6521(6.2006) | Xent 2.1617(2.1357) | Loss 6.7330(7.2685) | Error 0.7876(0.7503) Steps 538(565.25) | Grad Norm 9.8307(15.3707) | Total Time 14.00(14.00)\n",
      "Iter 0123 | Time 49.6353(53.4365) | Bit/dim 5.5880(6.1823) | Xent 2.1414(2.1358) | Loss 6.6587(7.2502) | Error 0.7685(0.7508) Steps 538(564.43) | Grad Norm 6.0357(15.0907) | Total Time 14.00(14.00)\n",
      "Iter 0124 | Time 50.1117(53.3368) | Bit/dim 5.5554(6.1635) | Xent 2.1307(2.1357) | Loss 6.6207(7.2313) | Error 0.7599(0.7511) Steps 544(563.82) | Grad Norm 4.9141(14.7854) | Total Time 14.00(14.00)\n",
      "Iter 0125 | Time 49.0025(53.2068) | Bit/dim 5.5669(6.1456) | Xent 2.1330(2.1356) | Loss 6.6334(7.2134) | Error 0.7608(0.7514) Steps 538(563.05) | Grad Norm 4.9217(14.4895) | Total Time 14.00(14.00)\n",
      "Iter 0126 | Time 48.7916(53.0743) | Bit/dim 5.5408(6.1274) | Xent 2.1452(2.1359) | Loss 6.6134(7.1954) | Error 0.7669(0.7519) Steps 526(561.93) | Grad Norm 4.2171(14.1813) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 24.1337, Epoch Time 337.2170(388.7313), Bit/dim 5.5378(best: 5.5935), Xent 2.1281, Loss 6.6019, Error 0.7698(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 49.0128(52.9525) | Bit/dim 5.5449(6.1099) | Xent 2.1422(2.1361) | Loss 6.6160(7.1780) | Error 0.7769(0.7526) Steps 526(560.86) | Grad Norm 4.8509(13.9014) | Total Time 14.00(14.00)\n",
      "Iter 0128 | Time 49.3069(52.8431) | Bit/dim 5.5047(6.0918) | Xent 2.1650(2.1369) | Loss 6.5872(7.1603) | Error 0.7752(0.7533) Steps 526(559.81) | Grad Norm 5.6595(13.6541) | Total Time 14.00(14.00)\n",
      "Iter 0129 | Time 49.4603(52.7416) | Bit/dim 5.5015(6.0741) | Xent 2.2200(2.1394) | Loss 6.6115(7.1438) | Error 0.8176(0.7552) Steps 526(558.80) | Grad Norm 13.8905(13.6612) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 49.5413(52.6456) | Bit/dim 5.5035(6.0570) | Xent 2.9827(2.1647) | Loss 6.9948(7.1393) | Error 0.8832(0.7591) Steps 550(558.53) | Grad Norm 60.0161(15.0519) | Total Time 14.00(14.00)\n",
      "Iter 0131 | Time 51.3840(52.6078) | Bit/dim 6.2308(6.0622) | Xent 4.6692(2.2399) | Loss 8.5654(7.1821) | Error 0.8929(0.7631) Steps 562(558.64) | Grad Norm 46.7944(16.0041) | Total Time 14.00(14.00)\n",
      "Iter 0132 | Time 52.5842(52.6070) | Bit/dim 6.2346(6.0673) | Xent 3.6713(2.2828) | Loss 8.0703(7.2088) | Error 0.8489(0.7656) Steps 556(558.56) | Grad Norm 29.5705(16.4111) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 24.6756, Epoch Time 341.3592(387.3102), Bit/dim 5.9998(best: 5.5378), Xent 2.5146, Loss 7.2571, Error 0.8160(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 51.3448(52.5692) | Bit/dim 5.9959(6.0652) | Xent 2.5331(2.2903) | Loss 7.2624(7.2104) | Error 0.8145(0.7671) Steps 556(558.48) | Grad Norm 14.4956(16.3537) | Total Time 14.00(14.00)\n",
      "Iter 0134 | Time 51.8081(52.5463) | Bit/dim 5.8602(6.0591) | Xent 2.1815(2.2871) | Loss 6.9510(7.2026) | Error 0.7754(0.7674) Steps 556(558.41) | Grad Norm 11.8343(16.2181) | Total Time 14.00(14.00)\n",
      "Iter 0135 | Time 52.7739(52.5532) | Bit/dim 5.8563(6.0530) | Xent 2.2202(2.2851) | Loss 6.9664(7.1955) | Error 0.8131(0.7687) Steps 562(558.51) | Grad Norm 11.6852(16.0821) | Total Time 14.00(14.00)\n",
      "Iter 0136 | Time 53.1458(52.5710) | Bit/dim 5.9191(6.0490) | Xent 2.2832(2.2850) | Loss 7.0607(7.1915) | Error 0.8129(0.7701) Steps 574(558.98) | Grad Norm 24.4606(16.3335) | Total Time 14.00(14.00)\n",
      "Iter 0137 | Time 53.4121(52.5962) | Bit/dim 5.7076(6.0387) | Xent 2.2630(2.2843) | Loss 6.8391(7.1809) | Error 0.8173(0.7715) Steps 568(559.25) | Grad Norm 9.0091(16.1137) | Total Time 14.00(14.00)\n",
      "Iter 0138 | Time 51.5465(52.5647) | Bit/dim 5.6708(6.0277) | Xent 2.2550(2.2835) | Loss 6.7983(7.1694) | Error 0.7910(0.7721) Steps 562(559.33) | Grad Norm 14.3741(16.0615) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 25.4394, Epoch Time 355.1170(386.3444), Bit/dim 5.6627(best: 5.5378), Xent 2.2496, Loss 6.7875, Error 0.8073(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 50.6719(52.5079) | Bit/dim 5.6574(6.0166) | Xent 2.2820(2.2834) | Loss 6.7984(7.1583) | Error 0.8107(0.7732) Steps 556(559.23) | Grad Norm 20.6974(16.2006) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 50.5020(52.4477) | Bit/dim 5.6253(6.0048) | Xent 2.7171(2.2964) | Loss 6.9839(7.1530) | Error 0.8419(0.7753) Steps 544(558.77) | Grad Norm 46.8427(17.1199) | Total Time 14.00(14.00)\n",
      "Iter 0141 | Time 50.1395(52.3785) | Bit/dim 5.9367(6.0028) | Xent 3.3038(2.3266) | Loss 7.5886(7.1661) | Error 0.8464(0.7774) Steps 556(558.69) | Grad Norm 52.7924(18.1900) | Total Time 14.00(14.00)\n",
      "Iter 0142 | Time 47.7864(52.2407) | Bit/dim 5.6426(5.9920) | Xent 2.3689(2.3279) | Loss 6.8270(7.1559) | Error 0.7850(0.7776) Steps 520(557.53) | Grad Norm 17.1772(18.1597) | Total Time 14.00(14.00)\n",
      "Iter 0143 | Time 49.4127(52.1559) | Bit/dim 5.7291(5.9841) | Xent 2.4764(2.3324) | Loss 6.9673(7.1503) | Error 0.8387(0.7795) Steps 538(556.94) | Grad Norm 31.1685(18.5499) | Total Time 14.00(14.00)\n",
      "Iter 0144 | Time 49.7086(52.0825) | Bit/dim 5.6900(5.9753) | Xent 2.1416(2.3266) | Loss 6.7608(7.1386) | Error 0.7608(0.7789) Steps 544(556.56) | Grad Norm 11.3146(18.3329) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 24.6092, Epoch Time 338.3475(384.9045), Bit/dim 5.6701(best: 5.5378), Xent 2.2923, Loss 6.8163, Error 0.7950(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 48.7906(51.9837) | Bit/dim 5.6719(5.9662) | Xent 2.2928(2.3256) | Loss 6.8183(7.1290) | Error 0.7916(0.7793) Steps 532(555.82) | Grad Norm 17.2970(18.3018) | Total Time 14.00(14.00)\n",
      "Iter 0146 | Time 48.8820(51.8907) | Bit/dim 5.5777(5.9545) | Xent 2.0657(2.3178) | Loss 6.6106(7.1134) | Error 0.7258(0.7777) Steps 532(555.11) | Grad Norm 5.6845(17.9233) | Total Time 14.00(14.00)\n",
      "Iter 0147 | Time 49.5331(51.8199) | Bit/dim 5.5315(5.9418) | Xent 2.2359(2.3154) | Loss 6.6494(7.0995) | Error 0.8104(0.7787) Steps 544(554.77) | Grad Norm 15.2904(17.8443) | Total Time 14.00(14.00)\n",
      "Iter 0148 | Time 50.4389(51.7785) | Bit/dim 5.4659(5.9275) | Xent 2.1735(2.3111) | Loss 6.5526(7.0831) | Error 0.7772(0.7786) Steps 544(554.45) | Grad Norm 11.4931(17.6537) | Total Time 14.00(14.00)\n",
      "Iter 0149 | Time 50.7680(51.7482) | Bit/dim 5.4676(5.9137) | Xent 2.1711(2.3069) | Loss 6.5532(7.0672) | Error 0.7451(0.7776) Steps 556(554.50) | Grad Norm 14.6440(17.5635) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 48.6802(51.6561) | Bit/dim 5.4431(5.8996) | Xent 2.3758(2.3090) | Loss 6.6310(7.0541) | Error 0.8064(0.7785) Steps 544(554.18) | Grad Norm 30.3268(17.9464) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 24.8594, Epoch Time 337.4452(383.4807), Bit/dim 5.4485(best: 5.5378), Xent 2.5694, Loss 6.7332, Error 0.7972(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 50.4695(51.6205) | Bit/dim 5.4512(5.8862) | Xent 2.5682(2.3168) | Loss 6.7353(7.0446) | Error 0.7961(0.7790) Steps 550(554.06) | Grad Norm 29.8724(18.3041) | Total Time 14.00(14.00)\n",
      "Iter 0152 | Time 49.8160(51.5664) | Bit/dim 5.4798(5.8740) | Xent 2.1362(2.3113) | Loss 6.5480(7.0297) | Error 0.7668(0.7786) Steps 544(553.75) | Grad Norm 14.0537(18.1766) | Total Time 14.00(14.00)\n",
      "Iter 0153 | Time 48.2225(51.4661) | Bit/dim 5.4693(5.8618) | Xent 2.4101(2.3143) | Loss 6.6744(7.0190) | Error 0.7977(0.7792) Steps 526(552.92) | Grad Norm 29.0930(18.5041) | Total Time 14.00(14.00)\n",
      "Iter 0154 | Time 49.4155(51.4046) | Bit/dim 5.3900(5.8477) | Xent 2.3356(2.3149) | Loss 6.5579(7.0052) | Error 0.7686(0.7789) Steps 526(552.11) | Grad Norm 16.2797(18.4374) | Total Time 14.00(14.00)\n",
      "Iter 0155 | Time 48.1174(51.3060) | Bit/dim 5.4123(5.8346) | Xent 2.4052(2.3176) | Loss 6.6149(6.9935) | Error 0.7943(0.7794) Steps 520(551.15) | Grad Norm 21.9076(18.5415) | Total Time 14.00(14.00)\n",
      "Iter 0156 | Time 49.5810(51.2542) | Bit/dim 5.3295(5.8195) | Xent 2.1823(2.3136) | Loss 6.4207(6.9763) | Error 0.7755(0.7792) Steps 526(550.40) | Grad Norm 12.0905(18.3480) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 23.8375, Epoch Time 334.9932(382.0261), Bit/dim 5.3499(best: 5.4485), Xent 2.2847, Loss 6.4922, Error 0.8173(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 48.4899(51.1713) | Bit/dim 5.3512(5.8054) | Xent 2.3307(2.3141) | Loss 6.5165(6.9625) | Error 0.8150(0.7803) Steps 514(549.30) | Grad Norm 17.4583(18.3213) | Total Time 14.00(14.00)\n",
      "Iter 0158 | Time 49.6659(51.1261) | Bit/dim 5.3122(5.7906) | Xent 2.1908(2.3104) | Loss 6.4076(6.9458) | Error 0.7757(0.7802) Steps 526(548.60) | Grad Norm 17.1393(18.2858) | Total Time 14.00(14.00)\n",
      "Iter 0159 | Time 50.5368(51.1084) | Bit/dim 5.2953(5.7758) | Xent 2.1855(2.3067) | Loss 6.3881(6.9291) | Error 0.7541(0.7794) Steps 532(548.11) | Grad Norm 12.3012(18.1063) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 50.2288(51.0821) | Bit/dim 5.3096(5.7618) | Xent 2.1385(2.3016) | Loss 6.3789(6.9126) | Error 0.7463(0.7784) Steps 508(546.90) | Grad Norm 14.5672(18.0001) | Total Time 14.00(14.00)\n",
      "Iter 0161 | Time 47.2215(50.9662) | Bit/dim 5.3003(5.7479) | Xent 2.1147(2.2960) | Loss 6.3576(6.8959) | Error 0.7571(0.7778) Steps 502(545.56) | Grad Norm 11.0752(17.7924) | Total Time 14.00(14.00)\n",
      "Iter 0162 | Time 49.2873(50.9159) | Bit/dim 5.2890(5.7342) | Xent 2.0832(2.2896) | Loss 6.3306(6.8790) | Error 0.7250(0.7762) Steps 502(544.25) | Grad Norm 8.0918(17.5013) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 23.3069, Epoch Time 334.7079(380.6065), Bit/dim 5.2688(best: 5.3499), Xent 2.1048, Loss 6.3212, Error 0.7188(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 49.8340(50.8834) | Bit/dim 5.2664(5.7201) | Xent 2.1167(2.2844) | Loss 6.3247(6.8624) | Error 0.7243(0.7746) Steps 514(543.34) | Grad Norm 12.5114(17.3516) | Total Time 14.00(14.00)\n",
      "Iter 0164 | Time 48.9374(50.8250) | Bit/dim 5.2346(5.7056) | Xent 2.0441(2.2772) | Loss 6.2567(6.8442) | Error 0.7157(0.7729) Steps 508(542.28) | Grad Norm 5.3382(16.9912) | Total Time 14.00(14.00)\n",
      "Iter 0165 | Time 50.4748(50.8145) | Bit/dim 5.2363(5.6915) | Xent 2.0484(2.2704) | Loss 6.2604(6.8267) | Error 0.7328(0.7717) Steps 508(541.25) | Grad Norm 7.5142(16.7069) | Total Time 14.00(14.00)\n",
      "Iter 0166 | Time 48.9201(50.7577) | Bit/dim 5.2435(5.6781) | Xent 2.0745(2.2645) | Loss 6.2808(6.8103) | Error 0.7372(0.7706) Steps 502(540.08) | Grad Norm 7.7374(16.4378) | Total Time 14.00(14.00)\n",
      "Iter 0167 | Time 50.3460(50.7453) | Bit/dim 5.2347(5.6648) | Xent 2.0252(2.2573) | Loss 6.2473(6.7934) | Error 0.7126(0.7689) Steps 508(539.11) | Grad Norm 3.8053(16.0589) | Total Time 14.00(14.00)\n",
      "Iter 0168 | Time 49.1860(50.6986) | Bit/dim 5.1980(5.6508) | Xent 2.0382(2.2507) | Loss 6.2171(6.7761) | Error 0.7136(0.7672) Steps 508(538.18) | Grad Norm 4.0940(15.6999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 23.8130, Epoch Time 337.1803(379.3037), Bit/dim 5.1859(best: 5.2688), Xent 2.0243, Loss 6.1981, Error 0.6999(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 49.5022(50.6627) | Bit/dim 5.1822(5.6367) | Xent 2.0542(2.2448) | Loss 6.2093(6.7591) | Error 0.7137(0.7656) Steps 514(537.45) | Grad Norm 4.8554(15.3746) | Total Time 14.00(14.00)\n",
      "Iter 0170 | Time 52.1895(50.7085) | Bit/dim 5.1909(5.6233) | Xent 2.0561(2.2392) | Loss 6.2190(6.7429) | Error 0.7175(0.7642) Steps 520(536.93) | Grad Norm 3.1399(15.0075) | Total Time 14.00(14.00)\n",
      "Iter 0171 | Time 49.8462(50.6826) | Bit/dim 5.1967(5.6105) | Xent 2.0334(2.2330) | Loss 6.2134(6.7270) | Error 0.7145(0.7627) Steps 514(536.24) | Grad Norm 3.6233(14.6660) | Total Time 14.00(14.00)\n",
      "Iter 0172 | Time 50.1817(50.6676) | Bit/dim 5.1740(5.5974) | Xent 2.0082(2.2263) | Loss 6.1781(6.7106) | Error 0.7035(0.7609) Steps 514(535.58) | Grad Norm 4.0591(14.3478) | Total Time 14.00(14.00)\n",
      "Iter 0173 | Time 49.6184(50.6361) | Bit/dim 5.1609(5.5843) | Xent 2.0188(2.2200) | Loss 6.1703(6.6943) | Error 0.7079(0.7593) Steps 514(534.93) | Grad Norm 2.1081(13.9806) | Total Time 14.00(14.00)\n",
      "Iter 0174 | Time 52.6729(50.6972) | Bit/dim 5.1538(5.5714) | Xent 2.0004(2.2134) | Loss 6.1540(6.6781) | Error 0.7031(0.7576) Steps 514(534.30) | Grad Norm 4.2112(13.6875) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 23.1375, Epoch Time 342.4747(378.1989), Bit/dim 5.1460(best: 5.1859), Xent 1.9833, Loss 6.1377, Error 0.6880(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 49.6352(50.6654) | Bit/dim 5.1408(5.5585) | Xent 1.9930(2.2068) | Loss 6.1373(6.6619) | Error 0.7025(0.7560) Steps 508(533.51) | Grad Norm 2.4045(13.3490) | Total Time 14.00(14.00)\n",
      "Iter 0176 | Time 49.0540(50.6170) | Bit/dim 5.1547(5.5464) | Xent 1.9837(2.2001) | Loss 6.1466(6.6465) | Error 0.6939(0.7541) Steps 508(532.75) | Grad Norm 3.1353(13.0426) | Total Time 14.00(14.00)\n",
      "Iter 0177 | Time 49.0286(50.5694) | Bit/dim 5.1435(5.5343) | Xent 1.9985(2.1941) | Loss 6.1428(6.6313) | Error 0.6969(0.7524) Steps 502(531.82) | Grad Norm 1.5593(12.6981) | Total Time 14.00(14.00)\n",
      "Iter 0178 | Time 49.3235(50.5320) | Bit/dim 5.1170(5.5218) | Xent 2.0015(2.1883) | Loss 6.1177(6.6159) | Error 0.6983(0.7508) Steps 496(530.75) | Grad Norm 2.5271(12.3930) | Total Time 14.00(14.00)\n",
      "Iter 0179 | Time 48.2422(50.4633) | Bit/dim 5.1179(5.5097) | Xent 1.9886(2.1823) | Loss 6.1122(6.6008) | Error 0.6905(0.7490) Steps 502(529.89) | Grad Norm 1.4549(12.0649) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 48.2599(50.3972) | Bit/dim 5.1088(5.4976) | Xent 1.9783(2.1762) | Loss 6.0979(6.5857) | Error 0.6891(0.7472) Steps 502(529.05) | Grad Norm 1.6827(11.7534) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 23.1016, Epoch Time 332.0539(376.8145), Bit/dim 5.1020(best: 5.1460), Xent 1.9630, Loss 6.0835, Error 0.6783(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 48.9061(50.3525) | Bit/dim 5.1034(5.4858) | Xent 1.9680(2.1700) | Loss 6.0874(6.5708) | Error 0.6873(0.7454) Steps 502(528.24) | Grad Norm 1.8318(11.4557) | Total Time 14.00(14.00)\n",
      "Iter 0182 | Time 48.0466(50.2833) | Bit/dim 5.0837(5.4737) | Xent 1.9722(2.1640) | Loss 6.0698(6.5558) | Error 0.6836(0.7435) Steps 490(527.09) | Grad Norm 1.1495(11.1466) | Total Time 14.00(14.00)\n",
      "Iter 0183 | Time 48.7098(50.2361) | Bit/dim 5.0979(5.4625) | Xent 1.9736(2.1583) | Loss 6.0848(6.5416) | Error 0.6871(0.7418) Steps 490(525.98) | Grad Norm 1.1218(10.8458) | Total Time 14.00(14.00)\n",
      "Iter 0184 | Time 48.6455(50.1884) | Bit/dim 5.0954(5.4515) | Xent 1.9573(2.1523) | Loss 6.0740(6.5276) | Error 0.6846(0.7401) Steps 490(524.90) | Grad Norm 1.5405(10.5667) | Total Time 14.00(14.00)\n",
      "Iter 0185 | Time 48.8718(50.1489) | Bit/dim 5.0926(5.4407) | Xent 1.9568(2.1464) | Loss 6.0709(6.5139) | Error 0.6815(0.7384) Steps 490(523.85) | Grad Norm 1.2144(10.2861) | Total Time 14.00(14.00)\n",
      "Iter 0186 | Time 48.9941(50.1142) | Bit/dim 5.0734(5.4297) | Xent 1.9515(2.1406) | Loss 6.0492(6.5000) | Error 0.6756(0.7365) Steps 490(522.84) | Grad Norm 1.2941(10.0163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 22.8378, Epoch Time 330.5718(375.4272), Bit/dim 5.0665(best: 5.1020), Xent 1.9309, Loss 6.0319, Error 0.6563(best: 0.6693)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 48.4961(50.0657) | Bit/dim 5.0779(5.4191) | Xent 1.9500(2.1348) | Loss 6.0529(6.4865) | Error 0.6725(0.7346) Steps 490(521.85) | Grad Norm 1.3092(9.7551) | Total Time 14.00(14.00)\n",
      "Iter 0188 | Time 48.1228(50.0074) | Bit/dim 5.0491(5.4080) | Xent 1.9290(2.1287) | Loss 6.0136(6.4724) | Error 0.6744(0.7327) Steps 490(520.90) | Grad Norm 1.2266(9.4993) | Total Time 14.00(14.00)\n",
      "Iter 0189 | Time 50.2797(50.0156) | Bit/dim 5.0579(5.3975) | Xent 1.9351(2.1229) | Loss 6.0254(6.4589) | Error 0.6767(0.7311) Steps 496(520.15) | Grad Norm 1.4330(9.2573) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 47.7717(49.9482) | Bit/dim 5.0575(5.3873) | Xent 1.9424(2.1175) | Loss 6.0287(6.4460) | Error 0.6687(0.7292) Steps 490(519.24) | Grad Norm 1.5874(9.0272) | Total Time 14.00(14.00)\n",
      "Iter 0191 | Time 47.7782(49.8831) | Bit/dim 5.0418(5.3769) | Xent 1.9392(2.1121) | Loss 6.0114(6.4330) | Error 0.6703(0.7274) Steps 490(518.37) | Grad Norm 0.8098(8.7807) | Total Time 14.00(14.00)\n",
      "Iter 0192 | Time 49.3008(49.8657) | Bit/dim 5.0364(5.3667) | Xent 1.9310(2.1067) | Loss 6.0019(6.4201) | Error 0.6723(0.7258) Steps 496(517.70) | Grad Norm 2.2763(8.5855) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 22.7771, Epoch Time 330.1541(374.0690), Bit/dim 5.0313(best: 5.0665), Xent 1.9128, Loss 5.9877, Error 0.6504(best: 0.6563)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 47.3971(49.7916) | Bit/dim 5.0432(5.3570) | Xent 1.9200(2.1011) | Loss 6.0032(6.4076) | Error 0.6607(0.7238) Steps 490(516.87) | Grad Norm 2.2681(8.3960) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 49.4906(49.7826) | Bit/dim 5.0281(5.3472) | Xent 1.9188(2.0956) | Loss 5.9875(6.3950) | Error 0.6680(0.7221) Steps 496(516.24) | Grad Norm 1.5592(8.1909) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 50.2339(49.7961) | Bit/dim 5.0100(5.3370) | Xent 1.9270(2.0905) | Loss 5.9735(6.3823) | Error 0.6655(0.7205) Steps 490(515.45) | Grad Norm 1.3868(7.9868) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 49.4477(49.7857) | Bit/dim 5.0109(5.3273) | Xent 1.9348(2.0859) | Loss 5.9783(6.3702) | Error 0.6627(0.7187) Steps 496(514.87) | Grad Norm 2.2043(7.8133) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 49.3569(49.7728) | Bit/dim 5.0218(5.3181) | Xent 1.9178(2.0808) | Loss 5.9807(6.3585) | Error 0.6627(0.7170) Steps 490(514.12) | Grad Norm 4.3701(7.7100) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 49.4991(49.7646) | Bit/dim 5.0190(5.3091) | Xent 1.9250(2.0762) | Loss 5.9815(6.3472) | Error 0.6667(0.7155) Steps 496(513.58) | Grad Norm 4.2801(7.6071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 22.7506, Epoch Time 333.6168(372.8555), Bit/dim 5.0084(best: 5.0313), Xent 1.8918, Loss 5.9543, Error 0.6429(best: 0.6504)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.3187(49.6912) | Bit/dim 5.0055(5.3000) | Xent 1.9025(2.0710) | Loss 5.9567(6.3355) | Error 0.6599(0.7139) Steps 490(512.87) | Grad Norm 3.6343(7.4879) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 50.8778(49.7268) | Bit/dim 4.9961(5.2909) | Xent 1.9095(2.0661) | Loss 5.9509(6.3240) | Error 0.6587(0.7122) Steps 496(512.37) | Grad Norm 2.9100(7.3506) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 48.5412(49.6912) | Bit/dim 4.9896(5.2819) | Xent 1.9214(2.0618) | Loss 5.9503(6.3127) | Error 0.6706(0.7110) Steps 490(511.69) | Grad Norm 2.7861(7.2136) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 49.6497(49.6900) | Bit/dim 4.9924(5.2732) | Xent 1.9066(2.0571) | Loss 5.9457(6.3017) | Error 0.6596(0.7094) Steps 496(511.22) | Grad Norm 5.5519(7.1638) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 47.7340(49.6313) | Bit/dim 5.0123(5.2653) | Xent 1.9422(2.0537) | Loss 5.9834(6.2922) | Error 0.6767(0.7084) Steps 490(510.59) | Grad Norm 12.0501(7.3104) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 49.3830(49.6239) | Bit/dim 5.0518(5.2589) | Xent 2.1550(2.0567) | Loss 6.1293(6.2873) | Error 0.7545(0.7098) Steps 496(510.15) | Grad Norm 31.9670(8.0501) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 23.1490, Epoch Time 332.3489(371.6403), Bit/dim 5.4659(best: 5.0084), Xent 2.6268, Loss 6.7793, Error 0.7721(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 46.8636(49.5411) | Bit/dim 5.4680(5.2652) | Xent 2.6795(2.0754) | Loss 6.8078(6.3029) | Error 0.7821(0.7120) Steps 490(509.54) | Grad Norm 27.2129(8.6250) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 47.3223(49.4745) | Bit/dim 5.3797(5.2686) | Xent 2.1305(2.0770) | Loss 6.4449(6.3072) | Error 0.7331(0.7126) Steps 502(509.32) | Grad Norm 13.7392(8.7784) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 47.3063(49.4095) | Bit/dim 5.1326(5.2646) | Xent 2.0240(2.0755) | Loss 6.1446(6.3023) | Error 0.7246(0.7130) Steps 484(508.56) | Grad Norm 4.8058(8.6592) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 48.6392(49.3864) | Bit/dim 5.3093(5.2659) | Xent 2.1729(2.0784) | Loss 6.3957(6.3051) | Error 0.7697(0.7147) Steps 496(508.18) | Grad Norm 24.0117(9.1198) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 46.5740(49.3020) | Bit/dim 5.2258(5.2647) | Xent 2.0935(2.0788) | Loss 6.2725(6.3041) | Error 0.7642(0.7162) Steps 490(507.64) | Grad Norm 17.0476(9.3576) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 45.5130(49.1883) | Bit/dim 5.2225(5.2634) | Xent 2.0225(2.0771) | Loss 6.2338(6.3020) | Error 0.7114(0.7160) Steps 478(506.75) | Grad Norm 4.7130(9.2183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 22.4214, Epoch Time 320.1124(370.0944), Bit/dim 5.1454(best: 5.0084), Xent 2.1030, Loss 6.1969, Error 0.7537(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 46.3793(49.1040) | Bit/dim 5.1561(5.2602) | Xent 2.1115(2.0782) | Loss 6.2118(6.2993) | Error 0.7461(0.7169) Steps 496(506.43) | Grad Norm 12.4469(9.3151) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 49.2803(49.1093) | Bit/dim 5.0890(5.2551) | Xent 2.1094(2.0791) | Loss 6.1438(6.2946) | Error 0.7421(0.7177) Steps 508(506.47) | Grad Norm 4.9366(9.1838) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 48.5297(49.0919) | Bit/dim 5.1334(5.2514) | Xent 2.2147(2.0832) | Loss 6.2407(6.2930) | Error 0.7695(0.7192) Steps 502(506.34) | Grad Norm 13.2382(9.3054) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 47.3238(49.0389) | Bit/dim 5.0796(5.2463) | Xent 2.0869(2.0833) | Loss 6.1230(6.2879) | Error 0.7363(0.7198) Steps 508(506.39) | Grad Norm 4.2814(9.1547) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 48.8608(49.0336) | Bit/dim 5.1107(5.2422) | Xent 2.0766(2.0831) | Loss 6.1490(6.2837) | Error 0.7444(0.7205) Steps 520(506.80) | Grad Norm 8.3043(9.1292) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 48.1113(49.0059) | Bit/dim 5.1009(5.2380) | Xent 2.0802(2.0830) | Loss 6.1410(6.2795) | Error 0.7541(0.7215) Steps 514(507.01) | Grad Norm 3.9647(8.9743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 22.7031, Epoch Time 326.6541(368.7912), Bit/dim 5.0746(best: 5.0084), Xent 2.0905, Loss 6.1199, Error 0.7426(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 50.2730(49.0439) | Bit/dim 5.0865(5.2334) | Xent 2.1245(2.0842) | Loss 6.1488(6.2755) | Error 0.7608(0.7227) Steps 502(506.86) | Grad Norm 7.2811(8.9235) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 49.5110(49.0579) | Bit/dim 5.0660(5.2284) | Xent 2.0624(2.0836) | Loss 6.0972(6.2702) | Error 0.7328(0.7230) Steps 508(506.90) | Grad Norm 4.9954(8.8056) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 50.3278(49.0960) | Bit/dim 5.0244(5.2223) | Xent 2.0955(2.0839) | Loss 6.0722(6.2643) | Error 0.7426(0.7236) Steps 514(507.11) | Grad Norm 5.5511(8.7080) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 49.2827(49.1016) | Bit/dim 5.0128(5.2160) | Xent 2.0563(2.0831) | Loss 6.0410(6.2576) | Error 0.7229(0.7235) Steps 508(507.14) | Grad Norm 5.0481(8.5982) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 52.6723(49.2087) | Bit/dim 4.9906(5.2092) | Xent 2.0388(2.0818) | Loss 6.0101(6.2501) | Error 0.7226(0.7235) Steps 514(507.34) | Grad Norm 2.7458(8.4226) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 51.2457(49.2698) | Bit/dim 5.0161(5.2034) | Xent 2.0618(2.0812) | Loss 6.0470(6.2440) | Error 0.7390(0.7240) Steps 514(507.54) | Grad Norm 4.2160(8.2964) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 23.2285, Epoch Time 342.1647(367.9924), Bit/dim 5.0047(best: 5.0084), Xent 2.0001, Loss 6.0048, Error 0.7095(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 50.8734(49.3179) | Bit/dim 5.0124(5.1977) | Xent 2.0081(2.0790) | Loss 6.0165(6.2372) | Error 0.7119(0.7236) Steps 514(507.74) | Grad Norm 2.4280(8.1204) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 50.4052(49.3506) | Bit/dim 5.0118(5.1921) | Xent 2.0294(2.0775) | Loss 6.0265(6.2309) | Error 0.7147(0.7234) Steps 496(507.38) | Grad Norm 4.3733(8.0079) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 49.8157(49.3645) | Bit/dim 4.9750(5.1856) | Xent 2.0076(2.0754) | Loss 5.9788(6.2233) | Error 0.7011(0.7227) Steps 496(507.04) | Grad Norm 2.1605(7.8325) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 49.3029(49.3627) | Bit/dim 4.9773(5.1794) | Xent 2.0052(2.0733) | Loss 5.9799(6.2160) | Error 0.7092(0.7223) Steps 496(506.71) | Grad Norm 4.2426(7.7248) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 49.4548(49.3654) | Bit/dim 4.9884(5.1736) | Xent 2.0062(2.0713) | Loss 5.9915(6.2093) | Error 0.7094(0.7219) Steps 496(506.39) | Grad Norm 2.6590(7.5729) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 50.6583(49.4042) | Bit/dim 4.9663(5.1674) | Xent 1.9731(2.0683) | Loss 5.9528(6.2016) | Error 0.6846(0.7208) Steps 496(506.08) | Grad Norm 3.0367(7.4368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 23.1833, Epoch Time 339.0112(367.1230), Bit/dim 4.9569(best: 5.0047), Xent 1.9734, Loss 5.9436, Error 0.6846(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 49.3599(49.4029) | Bit/dim 4.9570(5.1611) | Xent 1.9843(2.0658) | Loss 5.9492(6.1940) | Error 0.6996(0.7201) Steps 496(505.78) | Grad Norm 2.9116(7.3010) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 49.7930(49.4146) | Bit/dim 4.9569(5.1550) | Xent 1.9858(2.0634) | Loss 5.9498(6.1867) | Error 0.6967(0.7194) Steps 496(505.48) | Grad Norm 3.0096(7.1723) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 50.7373(49.4543) | Bit/dim 4.9471(5.1487) | Xent 1.9746(2.0608) | Loss 5.9345(6.1791) | Error 0.6887(0.7185) Steps 496(505.20) | Grad Norm 3.2893(7.0558) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 49.9289(49.4685) | Bit/dim 4.9272(5.1421) | Xent 1.9664(2.0579) | Loss 5.9104(6.1711) | Error 0.6917(0.7177) Steps 496(504.92) | Grad Norm 3.0359(6.9352) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 49.1090(49.4577) | Bit/dim 4.9269(5.1356) | Xent 1.9667(2.0552) | Loss 5.9103(6.1632) | Error 0.6880(0.7168) Steps 496(504.65) | Grad Norm 3.3169(6.8266) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 50.6741(49.4942) | Bit/dim 4.9187(5.1291) | Xent 1.9494(2.0520) | Loss 5.8934(6.1551) | Error 0.6796(0.7157) Steps 490(504.21) | Grad Norm 3.1052(6.7150) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 23.1103, Epoch Time 338.5765(366.2666), Bit/dim 4.9102(best: 4.9569), Xent 1.9251, Loss 5.8727, Error 0.6589(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 48.8187(49.4740) | Bit/dim 4.9138(5.1227) | Xent 1.9513(2.0490) | Loss 5.8894(6.1472) | Error 0.6798(0.7146) Steps 490(503.79) | Grad Norm 2.3468(6.5839) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 48.9919(49.4595) | Bit/dim 4.8982(5.1159) | Xent 1.9269(2.0453) | Loss 5.8616(6.1386) | Error 0.6745(0.7134) Steps 490(503.37) | Grad Norm 4.4021(6.5185) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 48.7767(49.4390) | Bit/dim 4.8963(5.1094) | Xent 1.9311(2.0419) | Loss 5.8619(6.1303) | Error 0.6645(0.7120) Steps 490(502.97) | Grad Norm 2.0615(6.3848) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 49.0892(49.4285) | Bit/dim 4.8854(5.1026) | Xent 1.9296(2.0385) | Loss 5.8502(6.1219) | Error 0.6746(0.7108) Steps 484(502.40) | Grad Norm 2.2905(6.2620) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 49.6885(49.4363) | Bit/dim 4.9012(5.0966) | Xent 1.9171(2.0349) | Loss 5.8598(6.1140) | Error 0.6651(0.7095) Steps 490(502.03) | Grad Norm 2.6224(6.1528) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 49.1252(49.4270) | Bit/dim 4.8912(5.0904) | Xent 1.8982(2.0308) | Loss 5.8404(6.1058) | Error 0.6481(0.7076) Steps 490(501.67) | Grad Norm 2.2681(6.0362) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 22.1394, Epoch Time 332.3333(365.2486), Bit/dim 4.8666(best: 4.9102), Xent 1.8886, Loss 5.8109, Error 0.6427(best: 0.6429)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 49.7446(49.4365) | Bit/dim 4.8646(5.0837) | Xent 1.9095(2.0272) | Loss 5.8194(6.0972) | Error 0.6626(0.7063) Steps 484(501.14) | Grad Norm 2.3303(5.9251) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 49.4845(49.4379) | Bit/dim 4.8732(5.0773) | Xent 1.8948(2.0232) | Loss 5.8206(6.0889) | Error 0.6564(0.7048) Steps 484(500.63) | Grad Norm 0.9171(5.7748) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 49.4151(49.4373) | Bit/dim 4.8693(5.0711) | Xent 1.9066(2.0197) | Loss 5.8226(6.0809) | Error 0.6598(0.7034) Steps 484(500.13) | Grad Norm 2.2841(5.6701) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 49.1753(49.4294) | Bit/dim 4.8545(5.0646) | Xent 1.8833(2.0156) | Loss 5.7961(6.0724) | Error 0.6502(0.7018) Steps 484(499.64) | Grad Norm 4.0296(5.6209) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 49.7765(49.4398) | Bit/dim 4.8495(5.0581) | Xent 1.9270(2.0129) | Loss 5.8130(6.0646) | Error 0.6753(0.7010) Steps 484(499.17) | Grad Norm 12.0906(5.8150) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 48.8798(49.4230) | Bit/dim 4.9355(5.0545) | Xent 2.1362(2.0166) | Loss 6.0036(6.0628) | Error 0.7455(0.7024) Steps 484(498.72) | Grad Norm 33.5770(6.6478) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 23.2457, Epoch Time 335.3252(364.3509), Bit/dim 5.2320(best: 4.8666), Xent 2.7016, Loss 6.5828, Error 0.7732(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 49.3886(49.4220) | Bit/dim 5.2129(5.0592) | Xent 2.8108(2.0405) | Loss 6.6183(6.0795) | Error 0.7768(0.7046) Steps 496(498.64) | Grad Norm 57.5445(8.1747) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 47.0131(49.3497) | Bit/dim 5.2308(5.0644) | Xent 1.9929(2.0390) | Loss 6.2273(6.0839) | Error 0.7077(0.7047) Steps 496(498.56) | Grad Norm 11.7847(8.2830) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 46.8810(49.2757) | Bit/dim 5.3884(5.0741) | Xent 2.0570(2.0396) | Loss 6.4170(6.0939) | Error 0.7392(0.7057) Steps 502(498.66) | Grad Norm 14.4462(8.4679) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 49.9306(49.2953) | Bit/dim 5.2102(5.0782) | Xent 2.0424(2.0397) | Loss 6.2314(6.0980) | Error 0.7269(0.7064) Steps 520(499.30) | Grad Norm 11.9056(8.5711) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 50.0960(49.3193) | Bit/dim 5.0311(5.0768) | Xent 2.0545(2.0401) | Loss 6.0584(6.0968) | Error 0.7329(0.7072) Steps 520(499.92) | Grad Norm 7.1082(8.5272) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 50.6602(49.3596) | Bit/dim 5.0580(5.0762) | Xent 2.1211(2.0425) | Loss 6.1185(6.0975) | Error 0.7680(0.7090) Steps 520(500.53) | Grad Norm 11.3458(8.6117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 25.5323, Epoch Time 335.2970(363.4793), Bit/dim 5.1513(best: 4.8666), Xent 2.2961, Loss 6.2993, Error 0.7836(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 52.4213(49.4514) | Bit/dim 5.1584(5.0787) | Xent 2.3471(2.0517) | Loss 6.3320(6.1045) | Error 0.7762(0.7110) Steps 550(502.01) | Grad Norm 18.3054(8.9025) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 49.7310(49.4598) | Bit/dim 5.3982(5.0883) | Xent 2.4562(2.0638) | Loss 6.6263(6.1202) | Error 0.7953(0.7135) Steps 544(503.27) | Grad Norm 18.8114(9.1998) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 49.5950(49.4638) | Bit/dim 5.4088(5.0979) | Xent 2.3175(2.0714) | Loss 6.5676(6.1336) | Error 0.7708(0.7153) Steps 532(504.13) | Grad Norm 7.9743(9.1630) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 50.0561(49.4816) | Bit/dim 5.1908(5.1007) | Xent 2.4303(2.0822) | Loss 6.4059(6.1417) | Error 0.7830(0.7173) Steps 532(504.97) | Grad Norm 8.2520(9.1357) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 53.6788(49.6075) | Bit/dim 5.2301(5.1045) | Xent 2.1031(2.0828) | Loss 6.2817(6.1459) | Error 0.7308(0.7177) Steps 562(506.68) | Grad Norm 15.3387(9.3218) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 49.0997(49.5923) | Bit/dim 5.2140(5.1078) | Xent 2.2414(2.0876) | Loss 6.3346(6.1516) | Error 0.7731(0.7194) Steps 544(507.80) | Grad Norm 9.6427(9.3314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 24.1608, Epoch Time 343.9868(362.8945), Bit/dim 5.2910(best: 4.8666), Xent 2.0052, Loss 6.2936, Error 0.7049(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 47.8541(49.5401) | Bit/dim 5.2889(5.1133) | Xent 2.0112(2.0853) | Loss 6.2945(6.1559) | Error 0.7052(0.7189) Steps 526(508.34) | Grad Norm 6.7798(9.2549) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 48.8776(49.5203) | Bit/dim 5.2104(5.1162) | Xent 2.1541(2.0873) | Loss 6.2875(6.1598) | Error 0.7396(0.7195) Steps 520(508.69) | Grad Norm 7.1704(9.1923) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 48.7097(49.4960) | Bit/dim 5.1841(5.1182) | Xent 2.1432(2.0890) | Loss 6.2557(6.1627) | Error 0.7494(0.7204) Steps 514(508.85) | Grad Norm 12.6371(9.2957) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 49.8999(49.5081) | Bit/dim 5.1269(5.1185) | Xent 2.0035(2.0865) | Loss 6.1287(6.1617) | Error 0.7021(0.7199) Steps 520(509.19) | Grad Norm 3.0259(9.1076) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 48.2083(49.4691) | Bit/dim 5.1213(5.1186) | Xent 2.0944(2.0867) | Loss 6.1685(6.1619) | Error 0.7562(0.7210) Steps 526(509.69) | Grad Norm 9.6069(9.1226) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 48.4688(49.4391) | Bit/dim 5.0908(5.1177) | Xent 1.9939(2.0839) | Loss 6.0877(6.1597) | Error 0.6936(0.7202) Steps 520(510.00) | Grad Norm 4.2322(8.9759) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 24.2295, Epoch Time 332.0020(361.9677), Bit/dim 5.0666(best: 4.8666), Xent 1.9702, Loss 6.0518, Error 0.6978(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 50.7825(49.4794) | Bit/dim 5.0728(5.1164) | Xent 1.9979(2.0813) | Loss 6.0718(6.1570) | Error 0.7061(0.7197) Steps 532(510.66) | Grad Norm 7.1763(8.9219) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 49.7241(49.4867) | Bit/dim 5.0570(5.1146) | Xent 1.9919(2.0786) | Loss 6.0529(6.1539) | Error 0.7080(0.7194) Steps 520(510.94) | Grad Norm 5.7253(8.8260) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 45.7510(49.3746) | Bit/dim 5.0226(5.1118) | Xent 1.9705(2.0754) | Loss 6.0078(6.1495) | Error 0.6926(0.7186) Steps 496(510.49) | Grad Norm 4.5778(8.6985) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 46.7328(49.2954) | Bit/dim 5.0018(5.1085) | Xent 1.9691(2.0722) | Loss 5.9863(6.1446) | Error 0.6851(0.7176) Steps 496(510.06) | Grad Norm 4.7548(8.5802) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 50.5472(49.3329) | Bit/dim 4.9616(5.1041) | Xent 1.9944(2.0699) | Loss 5.9588(6.1391) | Error 0.7031(0.7171) Steps 502(509.82) | Grad Norm 5.1215(8.4765) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 48.5212(49.3086) | Bit/dim 4.9372(5.0991) | Xent 1.9702(2.0669) | Loss 5.9223(6.1326) | Error 0.6932(0.7164) Steps 490(509.22) | Grad Norm 4.0894(8.3448) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 22.7316, Epoch Time 330.5650(361.0256), Bit/dim 4.9402(best: 4.8666), Xent 1.9345, Loss 5.9075, Error 0.6711(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 48.4005(49.2814) | Bit/dim 4.9383(5.0943) | Xent 1.9423(2.0632) | Loss 5.9095(6.1259) | Error 0.6851(0.7155) Steps 490(508.65) | Grad Norm 3.1692(8.1896) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 50.7398(49.3251) | Bit/dim 4.9282(5.0893) | Xent 1.9868(2.0609) | Loss 5.9216(6.1197) | Error 0.6996(0.7150) Steps 502(508.45) | Grad Norm 5.5144(8.1093) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 50.7649(49.3683) | Bit/dim 4.9108(5.0840) | Xent 1.9400(2.0572) | Loss 5.8808(6.1126) | Error 0.6794(0.7139) Steps 496(508.07) | Grad Norm 1.9852(7.9256) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 51.1460(49.4216) | Bit/dim 4.9028(5.0785) | Xent 1.9702(2.0546) | Loss 5.8879(6.1058) | Error 0.7044(0.7137) Steps 502(507.89) | Grad Norm 5.0978(7.8408) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 50.2977(49.4479) | Bit/dim 4.8800(5.0726) | Xent 1.9396(2.0512) | Loss 5.8498(6.0982) | Error 0.6770(0.7126) Steps 502(507.71) | Grad Norm 1.9739(7.6648) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 49.3816(49.4459) | Bit/dim 4.8827(5.0669) | Xent 1.9723(2.0488) | Loss 5.8689(6.0913) | Error 0.6899(0.7119) Steps 502(507.54) | Grad Norm 4.9292(7.5827) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.7160, Epoch Time 338.8074(360.3591), Bit/dim 4.8742(best: 4.8666), Xent 1.9249, Loss 5.8367, Error 0.6686(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 48.9100(49.4298) | Bit/dim 4.8816(5.0613) | Xent 1.9376(2.0455) | Loss 5.8504(6.0840) | Error 0.6824(0.7110) Steps 496(507.20) | Grad Norm 4.2647(7.4831) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 51.1988(49.4829) | Bit/dim 4.8599(5.0553) | Xent 1.9423(2.0424) | Loss 5.8310(6.0765) | Error 0.6761(0.7099) Steps 496(506.86) | Grad Norm 2.7478(7.3411) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 49.9618(49.4973) | Bit/dim 4.8533(5.0492) | Xent 1.9554(2.0398) | Loss 5.8310(6.0691) | Error 0.6847(0.7092) Steps 496(506.53) | Grad Norm 4.1902(7.2466) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 49.7441(49.5047) | Bit/dim 4.8435(5.0430) | Xent 1.9185(2.0361) | Loss 5.8027(6.0611) | Error 0.6700(0.7080) Steps 496(506.22) | Grad Norm 2.8099(7.1135) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 50.1144(49.5230) | Bit/dim 4.8263(5.0365) | Xent 1.9292(2.0329) | Loss 5.7909(6.0530) | Error 0.6751(0.7070) Steps 502(506.09) | Grad Norm 2.5128(6.9754) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 50.2847(49.5458) | Bit/dim 4.8039(5.0296) | Xent 1.8971(2.0288) | Loss 5.7524(6.0440) | Error 0.6585(0.7056) Steps 502(505.97) | Grad Norm 2.5388(6.8423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.7546, Epoch Time 338.5267(359.7041), Bit/dim 4.8094(best: 4.8666), Xent 1.8913, Loss 5.7550, Error 0.6557(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 50.6849(49.5800) | Bit/dim 4.8101(5.0230) | Xent 1.9163(2.0255) | Loss 5.7683(6.0357) | Error 0.6640(0.7043) Steps 502(505.85) | Grad Norm 1.4856(6.6816) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 49.3493(49.5731) | Bit/dim 4.8037(5.0164) | Xent 1.9087(2.0220) | Loss 5.7581(6.0274) | Error 0.6694(0.7033) Steps 496(505.55) | Grad Norm 1.8854(6.5378) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 49.9697(49.5850) | Bit/dim 4.7858(5.0095) | Xent 1.8947(2.0181) | Loss 5.7331(6.0186) | Error 0.6594(0.7020) Steps 502(505.45) | Grad Norm 2.0724(6.4038) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 50.7400(49.6196) | Bit/dim 4.7903(5.0029) | Xent 1.8889(2.0143) | Loss 5.7347(6.0100) | Error 0.6554(0.7006) Steps 508(505.52) | Grad Norm 2.2836(6.2802) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 50.4383(49.6442) | Bit/dim 4.7745(4.9960) | Xent 1.9120(2.0112) | Loss 5.7305(6.0016) | Error 0.6724(0.6997) Steps 508(505.60) | Grad Norm 1.4552(6.1354) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 50.1312(49.6588) | Bit/dim 4.7760(4.9894) | Xent 1.9058(2.0080) | Loss 5.7290(5.9935) | Error 0.6669(0.6987) Steps 508(505.67) | Grad Norm 1.4614(5.9952) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 23.0793, Epoch Time 339.6929(359.1038), Bit/dim 4.7641(best: 4.8094), Xent 1.8692, Loss 5.6987, Error 0.6417(best: 0.6427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 51.2765(49.7073) | Bit/dim 4.7752(4.9830) | Xent 1.8810(2.0042) | Loss 5.7157(5.9851) | Error 0.6522(0.6973) Steps 508(505.74) | Grad Norm 1.7547(5.8680) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 51.8990(49.7731) | Bit/dim 4.7675(4.9766) | Xent 1.8816(2.0005) | Loss 5.7083(5.9768) | Error 0.6560(0.6961) Steps 508(505.81) | Grad Norm 4.6358(5.8310) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 51.0633(49.8118) | Bit/dim 4.7783(4.9706) | Xent 1.9090(1.9978) | Loss 5.7328(5.9695) | Error 0.6701(0.6953) Steps 496(505.51) | Grad Norm 8.9832(5.9256) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 51.7019(49.8685) | Bit/dim 4.8197(4.9661) | Xent 1.9751(1.9971) | Loss 5.8073(5.9646) | Error 0.6929(0.6952) Steps 508(505.59) | Grad Norm 20.6985(6.3688) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 45.7017(49.7435) | Bit/dim 5.1587(4.9719) | Xent 2.1123(2.0006) | Loss 6.2148(5.9721) | Error 0.7452(0.6967) Steps 484(504.94) | Grad Norm 18.0867(6.7203) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 45.8332(49.6262) | Bit/dim 5.0624(4.9746) | Xent 1.9293(1.9984) | Loss 6.0271(5.9738) | Error 0.6783(0.6962) Steps 484(504.31) | Grad Norm 8.9893(6.7884) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.0776, Epoch Time 336.4722(358.4248), Bit/dim 4.8697(best: 4.7641), Xent 1.9709, Loss 5.8552, Error 0.6945(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 49.7702(49.6305) | Bit/dim 4.8690(4.9714) | Xent 1.9910(1.9982) | Loss 5.8645(5.9705) | Error 0.6917(0.6961) Steps 502(504.24) | Grad Norm 9.0413(6.8560) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 51.3550(49.6822) | Bit/dim 5.1163(4.9758) | Xent 1.9968(1.9982) | Loss 6.1147(5.9748) | Error 0.6951(0.6960) Steps 520(504.72) | Grad Norm 21.1936(7.2861) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 44.8146(49.5362) | Bit/dim 5.4430(4.9898) | Xent 2.0499(1.9997) | Loss 6.4680(5.9896) | Error 0.7194(0.6967) Steps 478(503.91) | Grad Norm 12.2862(7.4361) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 44.8064(49.3943) | Bit/dim 5.6930(5.0109) | Xent 1.9722(1.9989) | Loss 6.6791(6.0103) | Error 0.7001(0.6968) Steps 472(502.96) | Grad Norm 6.6611(7.4129) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 45.4752(49.2767) | Bit/dim 5.7269(5.0324) | Xent 1.9498(1.9974) | Loss 6.7019(6.0311) | Error 0.6863(0.6965) Steps 478(502.21) | Grad Norm 4.5627(7.3274) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 45.4151(49.1609) | Bit/dim 5.6454(5.0507) | Xent 2.0075(1.9977) | Loss 6.6491(6.0496) | Error 0.7077(0.6969) Steps 478(501.48) | Grad Norm 6.5200(7.3031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 21.9851, Epoch Time 319.4003(357.2541), Bit/dim 5.4427(best: 4.7641), Xent 2.0455, Loss 6.4654, Error 0.7429(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 45.1684(49.0411) | Bit/dim 5.4380(5.0624) | Xent 2.0515(1.9993) | Loss 6.4637(6.0620) | Error 0.7423(0.6982) Steps 472(500.60) | Grad Norm 8.2558(7.3317) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 49.2570(49.0476) | Bit/dim 5.1274(5.0643) | Xent 1.9922(1.9991) | Loss 6.1235(6.0639) | Error 0.7037(0.6984) Steps 490(500.28) | Grad Norm 5.6518(7.2813) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 48.1292(49.0200) | Bit/dim 5.1601(5.0672) | Xent 2.0003(1.9992) | Loss 6.1603(6.0668) | Error 0.7071(0.6986) Steps 496(500.15) | Grad Norm 9.2920(7.3416) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 45.8996(48.9264) | Bit/dim 5.0588(5.0669) | Xent 1.9496(1.9977) | Loss 6.0336(6.0658) | Error 0.6840(0.6982) Steps 484(499.67) | Grad Norm 5.0900(7.2741) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 45.8691(48.8347) | Bit/dim 5.1353(5.0690) | Xent 1.9359(1.9958) | Loss 6.1032(6.0669) | Error 0.6824(0.6977) Steps 490(499.38) | Grad Norm 5.1189(7.2094) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 45.8499(48.7452) | Bit/dim 5.1434(5.0712) | Xent 1.9390(1.9941) | Loss 6.1129(6.0683) | Error 0.6741(0.6970) Steps 496(499.28) | Grad Norm 3.4888(7.0978) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 23.1132, Epoch Time 319.4497(356.1200), Bit/dim 5.0937(best: 4.7641), Xent 1.9245, Loss 6.0560, Error 0.6706(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 48.2363(48.7299) | Bit/dim 5.0949(5.0719) | Xent 1.9399(1.9925) | Loss 6.0649(6.0682) | Error 0.6764(0.6964) Steps 502(499.36) | Grad Norm 3.0609(6.9767) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 46.6057(48.6662) | Bit/dim 5.0741(5.0720) | Xent 1.9544(1.9913) | Loss 6.0513(6.0677) | Error 0.6811(0.6959) Steps 502(499.44) | Grad Norm 5.0049(6.9176) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 45.6895(48.5769) | Bit/dim 5.0160(5.0703) | Xent 1.9447(1.9899) | Loss 5.9884(6.0653) | Error 0.6831(0.6956) Steps 484(498.97) | Grad Norm 3.7232(6.8217) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 46.1246(48.5033) | Bit/dim 4.9967(5.0681) | Xent 1.9427(1.9885) | Loss 5.9681(6.0624) | Error 0.6739(0.6949) Steps 472(498.16) | Grad Norm 2.5267(6.6929) | Total Time 14.00(14.00)\n",
      "Iter 0311 | Time 46.3082(48.4374) | Bit/dim 4.9741(5.0653) | Xent 1.9347(1.9869) | Loss 5.9415(6.0587) | Error 0.6770(0.6944) Steps 472(497.38) | Grad Norm 3.2491(6.5896) | Total Time 14.00(14.00)\n",
      "Iter 0312 | Time 46.2268(48.3711) | Bit/dim 4.9675(5.0624) | Xent 1.9511(1.9858) | Loss 5.9430(6.0553) | Error 0.6887(0.6942) Steps 478(496.80) | Grad Norm 2.9621(6.4807) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 22.4113, Epoch Time 317.2123(354.9527), Bit/dim 4.9385(best: 4.7641), Xent 1.9070, Loss 5.8920, Error 0.6695(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 48.7096(48.3813) | Bit/dim 4.9466(5.0589) | Xent 1.9350(1.9843) | Loss 5.9141(6.0510) | Error 0.6793(0.6938) Steps 496(496.77) | Grad Norm 3.1132(6.3797) | Total Time 14.00(14.00)\n",
      "Iter 0314 | Time 49.8720(48.4260) | Bit/dim 4.9263(5.0549) | Xent 1.9031(1.9819) | Loss 5.8779(6.0458) | Error 0.6603(0.6927) Steps 496(496.75) | Grad Norm 2.5730(6.2655) | Total Time 14.00(14.00)\n",
      "Iter 0315 | Time 48.5013(48.4283) | Bit/dim 4.9070(5.0505) | Xent 1.8951(1.9793) | Loss 5.8545(6.0401) | Error 0.6696(0.6921) Steps 490(496.55) | Grad Norm 2.4627(6.1514) | Total Time 14.00(14.00)\n",
      "Iter 0316 | Time 47.7726(48.4086) | Bit/dim 4.9020(5.0460) | Xent 1.9158(1.9774) | Loss 5.8599(6.0347) | Error 0.6683(0.6913) Steps 490(496.35) | Grad Norm 2.6996(6.0479) | Total Time 14.00(14.00)\n",
      "Iter 0317 | Time 51.9224(48.5140) | Bit/dim 4.8615(5.0405) | Xent 1.8847(1.9746) | Loss 5.8038(6.0278) | Error 0.6575(0.6903) Steps 490(496.16) | Grad Norm 2.0371(5.9275) | Total Time 14.00(14.00)\n",
      "Iter 0318 | Time 51.8866(48.6152) | Bit/dim 4.8506(5.0348) | Xent 1.9006(1.9724) | Loss 5.8009(6.0210) | Error 0.6631(0.6895) Steps 502(496.34) | Grad Norm 2.2893(5.8184) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.8780, Epoch Time 337.5093(354.4294), Bit/dim 4.8507(best: 4.7641), Xent 1.8714, Loss 5.7864, Error 0.6492(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 52.1863(48.7223) | Bit/dim 4.8397(5.0289) | Xent 1.8996(1.9702) | Loss 5.7895(6.0140) | Error 0.6571(0.6885) Steps 502(496.51) | Grad Norm 2.3081(5.7131) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 52.8240(48.8454) | Bit/dim 4.8309(5.0230) | Xent 1.9003(1.9681) | Loss 5.7810(6.0070) | Error 0.6640(0.6878) Steps 502(496.67) | Grad Norm 2.8062(5.6259) | Total Time 14.00(14.00)\n",
      "Iter 0321 | Time 49.7924(48.8738) | Bit/dim 4.8349(5.0173) | Xent 1.8866(1.9656) | Loss 5.7782(6.0002) | Error 0.6639(0.6871) Steps 496(496.65) | Grad Norm 1.9153(5.5146) | Total Time 14.00(14.00)\n",
      "Iter 0322 | Time 50.0693(48.9096) | Bit/dim 4.8219(5.0115) | Xent 1.8815(1.9631) | Loss 5.7626(5.9930) | Error 0.6634(0.6864) Steps 496(496.63) | Grad Norm 2.6538(5.4287) | Total Time 14.00(14.00)\n",
      "Iter 0323 | Time 50.4657(48.9563) | Bit/dim 4.8142(5.0056) | Xent 1.8750(1.9605) | Loss 5.7517(5.9858) | Error 0.6584(0.6855) Steps 496(496.61) | Grad Norm 2.0814(5.3283) | Total Time 14.00(14.00)\n",
      "Iter 0324 | Time 49.3869(48.9692) | Bit/dim 4.8076(4.9996) | Xent 1.8755(1.9579) | Loss 5.7453(5.9786) | Error 0.6595(0.6848) Steps 496(496.59) | Grad Norm 1.8185(5.2230) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.7166, Epoch Time 343.1890(354.0922), Bit/dim 4.7875(best: 4.7641), Xent 1.8509, Loss 5.7130, Error 0.6423(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 50.4104(49.0125) | Bit/dim 4.7964(4.9935) | Xent 1.8761(1.9555) | Loss 5.7344(5.9713) | Error 0.6539(0.6838) Steps 496(496.58) | Grad Norm 1.6673(5.1164) | Total Time 14.00(14.00)\n",
      "Iter 0326 | Time 50.0207(49.0427) | Bit/dim 4.7777(4.9870) | Xent 1.8891(1.9535) | Loss 5.7222(5.9638) | Error 0.6689(0.6834) Steps 496(496.56) | Grad Norm 2.0296(5.0237) | Total Time 14.00(14.00)\n",
      "Iter 0327 | Time 50.5268(49.0872) | Bit/dim 4.7768(4.9807) | Xent 1.8832(1.9514) | Loss 5.7184(5.9564) | Error 0.6634(0.6828) Steps 496(496.54) | Grad Norm 1.8432(4.9283) | Total Time 14.00(14.00)\n",
      "Iter 0328 | Time 49.8334(49.1096) | Bit/dim 4.7547(4.9740) | Xent 1.8392(1.9480) | Loss 5.6743(5.9480) | Error 0.6384(0.6814) Steps 496(496.53) | Grad Norm 2.1087(4.8437) | Total Time 14.00(14.00)\n",
      "Iter 0329 | Time 50.0940(49.1392) | Bit/dim 4.7479(4.9672) | Xent 1.8620(1.9454) | Loss 5.6789(5.9399) | Error 0.6516(0.6805) Steps 496(496.51) | Grad Norm 1.8980(4.7554) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 49.4124(49.1474) | Bit/dim 4.7454(4.9605) | Xent 1.8537(1.9427) | Loss 5.6722(5.9319) | Error 0.6438(0.6794) Steps 496(496.49) | Grad Norm 1.9321(4.6707) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 22.5219, Epoch Time 338.2841(353.6180), Bit/dim 4.7327(best: 4.7641), Xent 1.8212, Loss 5.6433, Error 0.6302(best: 0.6417)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 49.8422(49.1682) | Bit/dim 4.7369(4.9538) | Xent 1.8458(1.9398) | Loss 5.6598(5.9237) | Error 0.6446(0.6784) Steps 496(496.48) | Grad Norm 1.6445(4.5799) | Total Time 14.00(14.00)\n",
      "Iter 0332 | Time 49.8901(49.1899) | Bit/dim 4.7201(4.9468) | Xent 1.8403(1.9368) | Loss 5.6402(5.9152) | Error 0.6434(0.6774) Steps 496(496.47) | Grad Norm 2.7501(4.5250) | Total Time 14.00(14.00)\n",
      "Iter 0333 | Time 50.8354(49.2392) | Bit/dim 4.7197(4.9400) | Xent 1.8524(1.9342) | Loss 5.6458(5.9071) | Error 0.6479(0.6765) Steps 496(496.45) | Grad Norm 5.2972(4.5482) | Total Time 14.00(14.00)\n",
      "Iter 0334 | Time 49.5817(49.2495) | Bit/dim 4.7163(4.9333) | Xent 1.9052(1.9334) | Loss 5.6689(5.9000) | Error 0.6618(0.6760) Steps 496(496.44) | Grad Norm 9.9975(4.7116) | Total Time 14.00(14.00)\n",
      "Iter 0335 | Time 49.7923(49.2658) | Bit/dim 4.7233(4.9270) | Xent 2.1013(1.9384) | Loss 5.7740(5.8962) | Error 0.7215(0.6774) Steps 496(496.42) | Grad Norm 22.4760(5.2446) | Total Time 14.00(14.00)\n",
      "Iter 0336 | Time 47.6729(49.2180) | Bit/dim 4.8890(4.9258) | Xent 2.5935(1.9581) | Loss 6.1857(5.9049) | Error 0.7850(0.6806) Steps 490(496.23) | Grad Norm 21.5742(5.7345) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 23.1432, Epoch Time 336.7294(353.1113), Bit/dim 4.7779(best: 4.7327), Xent 2.0876, Loss 5.8217, Error 0.7529(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 51.2081(49.2777) | Bit/dim 4.7748(4.9213) | Xent 2.0956(1.9622) | Loss 5.8226(5.9024) | Error 0.7520(0.6828) Steps 496(496.23) | Grad Norm 13.0386(5.9536) | Total Time 14.00(14.00)\n",
      "Iter 0338 | Time 49.8322(49.2943) | Bit/dim 4.7559(4.9163) | Xent 2.0209(1.9640) | Loss 5.7663(5.8983) | Error 0.7246(0.6840) Steps 502(496.40) | Grad Norm 7.8526(6.0105) | Total Time 14.00(14.00)\n",
      "Iter 0339 | Time 51.6012(49.3635) | Bit/dim 4.8351(4.9139) | Xent 1.9958(1.9649) | Loss 5.8330(5.8964) | Error 0.6994(0.6845) Steps 508(496.75) | Grad Norm 9.5399(6.1164) | Total Time 14.00(14.00)\n",
      "Iter 0340 | Time 48.7773(49.3460) | Bit/dim 4.8008(4.9105) | Xent 2.0246(1.9667) | Loss 5.8131(5.8939) | Error 0.7239(0.6857) Steps 496(496.72) | Grad Norm 4.7515(6.0755) | Total Time 14.00(14.00)\n",
      "Iter 0341 | Time 47.4094(49.2879) | Bit/dim 4.7897(4.9069) | Xent 2.0480(1.9691) | Loss 5.8138(5.8915) | Error 0.7314(0.6870) Steps 490(496.52) | Grad Norm 4.3228(6.0229) | Total Time 14.00(14.00)\n",
      "Iter 0342 | Time 49.4719(49.2934) | Bit/dim 4.7582(4.9024) | Xent 2.0428(1.9713) | Loss 5.7796(5.8881) | Error 0.7332(0.6884) Steps 496(496.51) | Grad Norm 5.3721(6.0034) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 23.6304, Epoch Time 337.6709(352.6481), Bit/dim 4.7435(best: 4.7327), Xent 2.0116, Loss 5.7493, Error 0.7014(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 49.7874(49.3082) | Bit/dim 4.7438(4.8977) | Xent 2.0352(1.9733) | Loss 5.7614(5.8843) | Error 0.7020(0.6888) Steps 496(496.49) | Grad Norm 4.7958(5.9672) | Total Time 14.00(14.00)\n",
      "Iter 0344 | Time 51.4030(49.3710) | Bit/dim 4.7267(4.8925) | Xent 2.0044(1.9742) | Loss 5.7289(5.8796) | Error 0.7032(0.6893) Steps 514(497.02) | Grad Norm 3.2405(5.8854) | Total Time 14.00(14.00)\n",
      "Iter 0345 | Time 50.0798(49.3923) | Bit/dim 4.7429(4.8881) | Xent 1.9745(1.9742) | Loss 5.7301(5.8752) | Error 0.7009(0.6896) Steps 514(497.53) | Grad Norm 4.4091(5.8411) | Total Time 14.00(14.00)\n",
      "Iter 0346 | Time 50.5542(49.4272) | Bit/dim 4.7360(4.8835) | Xent 1.9744(1.9742) | Loss 5.7232(5.8706) | Error 0.7021(0.6900) Steps 502(497.66) | Grad Norm 3.3925(5.7676) | Total Time 14.00(14.00)\n",
      "Iter 0347 | Time 50.8042(49.4685) | Bit/dim 4.7310(4.8789) | Xent 1.9590(1.9738) | Loss 5.7105(5.8658) | Error 0.6912(0.6900) Steps 502(497.79) | Grad Norm 4.1121(5.7179) | Total Time 14.00(14.00)\n",
      "Iter 0348 | Time 47.9246(49.4222) | Bit/dim 4.7166(4.8740) | Xent 1.9434(1.9728) | Loss 5.6882(5.8605) | Error 0.6917(0.6901) Steps 496(497.74) | Grad Norm 3.2063(5.6426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.0383, Epoch Time 339.3388(352.2488), Bit/dim 4.7151(best: 4.7327), Xent 1.8929, Loss 5.6615, Error 0.6578(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 47.9173(49.3770) | Bit/dim 4.7039(4.8689) | Xent 1.9294(1.9715) | Loss 5.6686(5.8547) | Error 0.6781(0.6897) Steps 490(497.50) | Grad Norm 3.2793(5.5717) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 49.9796(49.3951) | Bit/dim 4.7106(4.8642) | Xent 1.9101(1.9697) | Loss 5.6657(5.8490) | Error 0.6880(0.6897) Steps 496(497.46) | Grad Norm 3.2997(5.5035) | Total Time 14.00(14.00)\n",
      "Iter 0351 | Time 48.2810(49.3617) | Bit/dim 4.6965(4.8592) | Xent 1.9078(1.9678) | Loss 5.6504(5.8431) | Error 0.6660(0.6890) Steps 496(497.42) | Grad Norm 3.3804(5.4398) | Total Time 14.00(14.00)\n",
      "Iter 0352 | Time 49.8539(49.3764) | Bit/dim 4.6857(4.8540) | Xent 1.9247(1.9665) | Loss 5.6481(5.8372) | Error 0.6844(0.6888) Steps 502(497.55) | Grad Norm 4.1825(5.4021) | Total Time 14.00(14.00)\n",
      "Iter 0353 | Time 50.8933(49.4219) | Bit/dim 4.6736(4.8485) | Xent 1.9048(1.9647) | Loss 5.6260(5.8309) | Error 0.6793(0.6885) Steps 502(497.69) | Grad Norm 2.7447(5.3224) | Total Time 14.00(14.00)\n",
      "Iter 0354 | Time 49.8390(49.4344) | Bit/dim 4.6707(4.8432) | Xent 1.8859(1.9623) | Loss 5.6137(5.8244) | Error 0.6601(0.6877) Steps 502(497.82) | Grad Norm 2.8695(5.2488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 23.4887, Epoch Time 335.7126(351.7527), Bit/dim 4.6623(best: 4.7151), Xent 1.8476, Loss 5.5862, Error 0.6433(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 50.1667(49.4564) | Bit/dim 4.6644(4.8378) | Xent 1.8732(1.9597) | Loss 5.6010(5.8177) | Error 0.6576(0.6868) Steps 496(497.76) | Grad Norm 2.9227(5.1790) | Total Time 14.00(14.00)\n",
      "Iter 0356 | Time 51.5167(49.5182) | Bit/dim 4.6609(4.8325) | Xent 1.8817(1.9573) | Loss 5.6017(5.8112) | Error 0.6589(0.6859) Steps 502(497.89) | Grad Norm 3.1982(5.1196) | Total Time 14.00(14.00)\n",
      "Iter 0357 | Time 49.8322(49.5276) | Bit/dim 4.6457(4.8269) | Xent 1.8521(1.9542) | Loss 5.5717(5.8040) | Error 0.6616(0.6852) Steps 502(498.01) | Grad Norm 2.2078(5.0322) | Total Time 14.00(14.00)\n",
      "Iter 0358 | Time 50.5045(49.5570) | Bit/dim 4.6309(4.8211) | Xent 1.8650(1.9515) | Loss 5.5634(5.7968) | Error 0.6611(0.6845) Steps 502(498.13) | Grad Norm 2.5696(4.9584) | Total Time 14.00(14.00)\n",
      "Iter 0359 | Time 49.5433(49.5565) | Bit/dim 4.6322(4.8154) | Xent 1.8291(1.9478) | Loss 5.5467(5.7893) | Error 0.6392(0.6831) Steps 508(498.43) | Grad Norm 2.2305(4.8765) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 50.5340(49.5859) | Bit/dim 4.6262(4.8097) | Xent 1.8409(1.9446) | Loss 5.5467(5.7820) | Error 0.6516(0.6822) Steps 508(498.72) | Grad Norm 4.4598(4.8640) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 23.4347, Epoch Time 341.0380(351.4313), Bit/dim 4.6338(best: 4.6623), Xent 1.8317, Loss 5.5497, Error 0.6341(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 51.0916(49.6310) | Bit/dim 4.6289(4.8043) | Xent 1.8587(1.9420) | Loss 5.5583(5.7753) | Error 0.6502(0.6812) Steps 496(498.63) | Grad Norm 9.5655(5.0051) | Total Time 14.00(14.00)\n",
      "Iter 0362 | Time 50.4941(49.6569) | Bit/dim 4.7341(4.8022) | Xent 2.0961(1.9467) | Loss 5.7821(5.7755) | Error 0.7214(0.6824) Steps 508(498.91) | Grad Norm 25.5048(5.6201) | Total Time 14.00(14.00)\n",
      "Iter 0363 | Time 46.9094(49.5745) | Bit/dim 5.0162(4.8086) | Xent 2.6138(1.9667) | Loss 6.3231(5.7919) | Error 0.7535(0.6846) Steps 496(498.83) | Grad Norm 25.0198(6.2021) | Total Time 14.00(14.00)\n",
      "Iter 0364 | Time 47.8393(49.5224) | Bit/dim 4.8437(4.8097) | Xent 2.0563(1.9694) | Loss 5.8718(5.7943) | Error 0.7183(0.6856) Steps 502(498.92) | Grad Norm 12.7432(6.3983) | Total Time 14.00(14.00)\n",
      "Iter 0365 | Time 50.8188(49.5613) | Bit/dim 4.7633(4.8083) | Xent 1.9382(1.9684) | Loss 5.7325(5.7925) | Error 0.6906(0.6857) Steps 514(499.37) | Grad Norm 6.4022(6.3984) | Total Time 14.00(14.00)\n",
      "Iter 0366 | Time 50.7370(49.5966) | Bit/dim 4.8636(4.8099) | Xent 1.9829(1.9689) | Loss 5.8551(5.7944) | Error 0.7072(0.6864) Steps 508(499.63) | Grad Norm 8.3091(6.4557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 23.6283, Epoch Time 337.0731(351.0006), Bit/dim 4.8597(best: 4.6338), Xent 1.9362, Loss 5.8278, Error 0.6764(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 49.9680(49.6077) | Bit/dim 4.8616(4.8115) | Xent 1.9640(1.9687) | Loss 5.8437(5.7958) | Error 0.6989(0.6867) Steps 508(499.88) | Grad Norm 9.5299(6.5480) | Total Time 14.00(14.00)\n",
      "Iter 0368 | Time 51.6636(49.6694) | Bit/dim 4.7598(4.8099) | Xent 1.9591(1.9684) | Loss 5.7393(5.7941) | Error 0.6952(0.6870) Steps 514(500.31) | Grad Norm 3.9188(6.4691) | Total Time 14.00(14.00)\n",
      "Iter 0369 | Time 51.3831(49.7208) | Bit/dim 4.7376(4.8078) | Xent 1.9542(1.9680) | Loss 5.7147(5.7918) | Error 0.6873(0.6870) Steps 514(500.72) | Grad Norm 3.7204(6.3866) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 49.9850(49.7288) | Bit/dim 4.7632(4.8064) | Xent 1.9455(1.9673) | Loss 5.7359(5.7901) | Error 0.6840(0.6869) Steps 514(501.12) | Grad Norm 3.6698(6.3051) | Total Time 14.00(14.00)\n",
      "Iter 0371 | Time 51.2566(49.7746) | Bit/dim 4.7628(4.8051) | Xent 1.9580(1.9670) | Loss 5.7417(5.7886) | Error 0.6877(0.6869) Steps 514(501.50) | Grad Norm 4.0644(6.2379) | Total Time 14.00(14.00)\n",
      "Iter 0372 | Time 51.3604(49.8222) | Bit/dim 4.7374(4.8031) | Xent 1.9125(1.9654) | Loss 5.6937(5.7858) | Error 0.6703(0.6864) Steps 514(501.88) | Grad Norm 2.5812(6.1282) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.7678, Epoch Time 344.8414(350.8158), Bit/dim 4.7251(best: 4.6338), Xent 1.9169, Loss 5.6836, Error 0.6736(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 50.6504(49.8470) | Bit/dim 4.7304(4.8009) | Xent 1.9481(1.9649) | Loss 5.7044(5.7833) | Error 0.6884(0.6865) Steps 508(502.06) | Grad Norm 2.8600(6.0301) | Total Time 14.00(14.00)\n",
      "Iter 0374 | Time 50.5023(49.8667) | Bit/dim 4.7135(4.7983) | Xent 1.9543(1.9646) | Loss 5.6907(5.7806) | Error 0.6827(0.6864) Steps 508(502.24) | Grad Norm 4.7447(5.9916) | Total Time 14.00(14.00)\n",
      "Iter 0375 | Time 49.8269(49.8655) | Bit/dim 4.6866(4.7949) | Xent 1.9442(1.9640) | Loss 5.6587(5.7769) | Error 0.6810(0.6862) Steps 508(502.41) | Grad Norm 5.7704(5.9849) | Total Time 14.00(14.00)\n",
      "Iter 0376 | Time 50.8917(49.8963) | Bit/dim 4.7030(4.7922) | Xent 1.9576(1.9638) | Loss 5.6818(5.7741) | Error 0.6916(0.6864) Steps 508(502.58) | Grad Norm 5.5757(5.9727) | Total Time 14.00(14.00)\n",
      "Iter 0377 | Time 50.9292(49.9273) | Bit/dim 4.6948(4.7892) | Xent 1.9411(1.9631) | Loss 5.6653(5.7708) | Error 0.6783(0.6861) Steps 508(502.74) | Grad Norm 6.6739(5.9937) | Total Time 14.00(14.00)\n",
      "Iter 0378 | Time 51.0563(49.9611) | Bit/dim 4.6739(4.7858) | Xent 1.9433(1.9625) | Loss 5.6456(5.7670) | Error 0.6804(0.6860) Steps 508(502.90) | Grad Norm 7.5505(6.0404) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 23.7200, Epoch Time 343.2127(350.5877), Bit/dim 4.6638(best: 4.6338), Xent 1.9374, Loss 5.6326, Error 0.6817(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 51.3602(50.0031) | Bit/dim 4.6634(4.7821) | Xent 1.9591(1.9624) | Loss 5.6430(5.7633) | Error 0.6900(0.6861) Steps 514(503.23) | Grad Norm 8.9387(6.1274) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 50.7192(50.0246) | Bit/dim 4.6838(4.7792) | Xent 2.0210(1.9641) | Loss 5.6942(5.7612) | Error 0.7249(0.6873) Steps 508(503.38) | Grad Norm 9.7963(6.2374) | Total Time 14.00(14.00)\n",
      "Iter 0381 | Time 50.8749(50.0501) | Bit/dim 4.6510(4.7753) | Xent 1.9465(1.9636) | Loss 5.6243(5.7571) | Error 0.6858(0.6872) Steps 508(503.52) | Grad Norm 7.5174(6.2758) | Total Time 14.00(14.00)\n",
      "Iter 0382 | Time 51.2534(50.0862) | Bit/dim 4.6340(4.7711) | Xent 1.8974(1.9616) | Loss 5.5827(5.7519) | Error 0.6653(0.6866) Steps 514(503.83) | Grad Norm 2.8326(6.1725) | Total Time 14.00(14.00)\n",
      "Iter 0383 | Time 51.2958(50.1225) | Bit/dim 4.6261(4.7667) | Xent 1.8947(1.9596) | Loss 5.5735(5.7465) | Error 0.6604(0.6858) Steps 514(504.14) | Grad Norm 2.7510(6.0699) | Total Time 14.00(14.00)\n",
      "Iter 0384 | Time 50.9845(50.1483) | Bit/dim 4.6134(4.7621) | Xent 1.8933(1.9576) | Loss 5.5601(5.7410) | Error 0.6686(0.6853) Steps 508(504.25) | Grad Norm 5.7261(6.0596) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 23.8018, Epoch Time 345.9295(350.4479), Bit/dim 4.6215(best: 4.6338), Xent 1.8943, Loss 5.5686, Error 0.6611(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 50.3818(50.1553) | Bit/dim 4.6289(4.7581) | Xent 1.9315(1.9569) | Loss 5.5946(5.7366) | Error 0.6829(0.6852) Steps 508(504.36) | Grad Norm 6.1678(6.0628) | Total Time 14.00(14.00)\n",
      "Iter 0386 | Time 51.1294(50.1846) | Bit/dim 4.5983(4.7533) | Xent 1.8860(1.9547) | Loss 5.5413(5.7307) | Error 0.6644(0.6846) Steps 508(504.47) | Grad Norm 2.9124(5.9683) | Total Time 14.00(14.00)\n",
      "Iter 0387 | Time 51.1589(50.2138) | Bit/dim 4.5959(4.7486) | Xent 1.8432(1.9514) | Loss 5.5175(5.7243) | Error 0.6486(0.6835) Steps 508(504.58) | Grad Norm 2.1840(5.8548) | Total Time 14.00(14.00)\n",
      "Iter 0388 | Time 51.5958(50.2553) | Bit/dim 4.5986(4.7441) | Xent 1.8725(1.9490) | Loss 5.5348(5.7186) | Error 0.6551(0.6826) Steps 514(504.86) | Grad Norm 3.7953(5.7930) | Total Time 14.00(14.00)\n",
      "Iter 0389 | Time 51.6168(50.2961) | Bit/dim 4.5947(4.7396) | Xent 1.8555(1.9462) | Loss 5.5225(5.7127) | Error 0.6492(0.6816) Steps 514(505.14) | Grad Norm 4.2861(5.7478) | Total Time 14.00(14.00)\n",
      "Iter 0390 | Time 49.9324(50.2852) | Bit/dim 4.5893(4.7351) | Xent 1.8510(1.9434) | Loss 5.5148(5.7068) | Error 0.6486(0.6806) Steps 502(505.04) | Grad Norm 4.9853(5.7249) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 23.7456, Epoch Time 345.3633(350.2954), Bit/dim 4.6235(best: 4.6215), Xent 1.8345, Loss 5.5408, Error 0.6464(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 51.0649(50.3086) | Bit/dim 4.6110(4.7314) | Xent 1.8822(1.9415) | Loss 5.5521(5.7022) | Error 0.6620(0.6801) Steps 514(505.31) | Grad Norm 11.1409(5.8874) | Total Time 14.00(14.00)\n",
      "Iter 0392 | Time 49.3284(50.2792) | Bit/dim 4.9302(4.7374) | Xent 1.9211(1.9409) | Loss 5.8908(5.7078) | Error 0.6706(0.6798) Steps 502(505.21) | Grad Norm 11.1496(6.0453) | Total Time 14.00(14.00)\n",
      "Iter 0393 | Time 48.0113(50.2111) | Bit/dim 4.7537(4.7379) | Xent 1.8635(1.9386) | Loss 5.6854(5.7071) | Error 0.6472(0.6788) Steps 490(504.75) | Grad Norm 6.6358(6.0630) | Total Time 14.00(14.00)\n",
      "Iter 0394 | Time 50.3275(50.2146) | Bit/dim 4.9388(4.7439) | Xent 1.9520(1.9390) | Loss 5.9148(5.7134) | Error 0.6611(0.6783) Steps 508(504.85) | Grad Norm 21.1415(6.5153) | Total Time 14.00(14.00)\n",
      "Iter 0395 | Time 45.3067(50.0674) | Bit/dim 5.1253(4.7553) | Xent 1.9568(1.9395) | Loss 6.1037(5.7251) | Error 0.6870(0.6785) Steps 478(504.05) | Grad Norm 6.0881(6.5025) | Total Time 14.00(14.00)\n",
      "Iter 0396 | Time 47.0201(49.9760) | Bit/dim 5.3893(4.7743) | Xent 1.9692(1.9404) | Loss 6.3739(5.7446) | Error 0.6983(0.6791) Steps 496(503.80) | Grad Norm 6.4797(6.5018) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 23.0314, Epoch Time 329.5517(349.6731), Bit/dim 5.3073(best: 4.6215), Xent 1.9342, Loss 6.2743, Error 0.6617(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 47.5359(49.9028) | Bit/dim 5.3031(4.7902) | Xent 1.9377(1.9403) | Loss 6.2720(5.7604) | Error 0.6715(0.6789) Steps 490(503.39) | Grad Norm 6.4013(6.4988) | Total Time 14.00(14.00)\n",
      "Iter 0398 | Time 46.7932(49.8095) | Bit/dim 5.0262(4.7973) | Xent 1.9467(1.9405) | Loss 5.9995(5.7675) | Error 0.6794(0.6789) Steps 490(502.99) | Grad Norm 5.3745(6.4651) | Total Time 14.00(14.00)\n",
      "Iter 0399 | Time 50.6227(49.8339) | Bit/dim 4.8600(4.7992) | Xent 1.9929(1.9421) | Loss 5.8564(5.7702) | Error 0.7156(0.6800) Steps 508(503.14) | Grad Norm 7.2344(6.4882) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 50.5516(49.8554) | Bit/dim 4.9023(4.8023) | Xent 2.0187(1.9444) | Loss 5.9117(5.7745) | Error 0.7040(0.6807) Steps 514(503.47) | Grad Norm 12.5761(6.6708) | Total Time 14.00(14.00)\n",
      "Iter 0401 | Time 50.6942(49.8806) | Bit/dim 4.9400(4.8064) | Xent 1.9962(1.9459) | Loss 5.9381(5.7794) | Error 0.7123(0.6817) Steps 550(504.86) | Grad Norm 8.0151(6.7111) | Total Time 14.00(14.00)\n",
      "Iter 0402 | Time 50.9694(49.9132) | Bit/dim 4.9139(4.8096) | Xent 1.9131(1.9450) | Loss 5.8704(5.7821) | Error 0.6695(0.6813) Steps 556(506.40) | Grad Norm 5.3366(6.6699) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 24.3199, Epoch Time 337.3492(349.3034), Bit/dim 4.8501(best: 4.6215), Xent 1.9023, Loss 5.8013, Error 0.6800(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 54.0022(50.0359) | Bit/dim 4.8452(4.8107) | Xent 1.9204(1.9442) | Loss 5.8055(5.7828) | Error 0.6916(0.6816) Steps 550(507.70) | Grad Norm 4.5075(6.6050) | Total Time 14.00(14.00)\n",
      "Iter 0404 | Time 50.9322(50.0628) | Bit/dim 4.8673(4.8124) | Xent 1.9525(1.9445) | Loss 5.8436(5.7846) | Error 0.6951(0.6820) Steps 508(507.71) | Grad Norm 4.5334(6.5429) | Total Time 14.00(14.00)\n",
      "Iter 0405 | Time 50.6011(50.0789) | Bit/dim 4.8386(4.8132) | Xent 1.9330(1.9441) | Loss 5.8050(5.7852) | Error 0.6825(0.6821) Steps 502(507.54) | Grad Norm 3.4159(6.4491) | Total Time 14.00(14.00)\n",
      "Iter 0406 | Time 48.9738(50.0458) | Bit/dim 4.8249(4.8135) | Xent 1.8905(1.9425) | Loss 5.7701(5.7848) | Error 0.6650(0.6815) Steps 496(507.19) | Grad Norm 4.4484(6.3890) | Total Time 14.00(14.00)\n",
      "Iter 0407 | Time 48.3418(49.9947) | Bit/dim 4.7810(4.8125) | Xent 1.8922(1.9410) | Loss 5.7271(5.7831) | Error 0.6665(0.6811) Steps 484(506.50) | Grad Norm 5.7170(6.3689) | Total Time 14.00(14.00)\n",
      "Iter 0408 | Time 49.1425(49.9691) | Bit/dim 4.7528(4.8108) | Xent 1.8781(1.9391) | Loss 5.6919(5.7803) | Error 0.6610(0.6805) Steps 502(506.36) | Grad Norm 4.6667(6.3178) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.1824, Epoch Time 340.9637(349.0532), Bit/dim 4.7682(best: 4.6215), Xent 1.8353, Loss 5.6859, Error 0.6438(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 46.7550(49.8727) | Bit/dim 4.7622(4.8093) | Xent 1.8612(1.9368) | Loss 5.6928(5.7777) | Error 0.6556(0.6797) Steps 496(506.05) | Grad Norm 4.8843(6.2748) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 48.9976(49.8464) | Bit/dim 4.7554(4.8077) | Xent 1.8424(1.9340) | Loss 5.6766(5.7747) | Error 0.6507(0.6789) Steps 496(505.75) | Grad Norm 3.3726(6.1877) | Total Time 14.00(14.00)\n",
      "Iter 0411 | Time 48.4405(49.8043) | Bit/dim 4.7195(4.8050) | Xent 1.8710(1.9321) | Loss 5.6550(5.7711) | Error 0.6558(0.6782) Steps 496(505.46) | Grad Norm 3.5493(6.1086) | Total Time 14.00(14.00)\n",
      "Iter 0412 | Time 51.2906(49.8488) | Bit/dim 4.7006(4.8019) | Xent 1.8736(1.9303) | Loss 5.6374(5.7671) | Error 0.6544(0.6775) Steps 496(505.18) | Grad Norm 5.1560(6.0800) | Total Time 14.00(14.00)\n",
      "Iter 0413 | Time 51.6491(49.9029) | Bit/dim 4.6860(4.7984) | Xent 1.8692(1.9285) | Loss 5.6206(5.7627) | Error 0.6599(0.6769) Steps 514(505.44) | Grad Norm 5.9796(6.0770) | Total Time 14.00(14.00)\n",
      "Iter 0414 | Time 50.5258(49.9215) | Bit/dim 4.6733(4.7947) | Xent 1.8680(1.9267) | Loss 5.6073(5.7580) | Error 0.6539(0.6762) Steps 502(505.34) | Grad Norm 6.5887(6.0924) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 23.7584, Epoch Time 337.1143(348.6950), Bit/dim 4.6733(best: 4.6215), Xent 1.8415, Loss 5.5940, Error 0.6485(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 51.3616(49.9647) | Bit/dim 4.6757(4.7911) | Xent 1.8502(1.9244) | Loss 5.6008(5.7533) | Error 0.6570(0.6757) Steps 514(505.60) | Grad Norm 6.0781(6.0919) | Total Time 14.00(14.00)\n",
      "Iter 0416 | Time 52.9583(50.0545) | Bit/dim 4.6626(4.7872) | Xent 1.8498(1.9221) | Loss 5.5875(5.7483) | Error 0.6467(0.6748) Steps 514(505.85) | Grad Norm 3.8071(6.0234) | Total Time 14.00(14.00)\n",
      "Iter 0417 | Time 52.3689(50.1240) | Bit/dim 4.6294(4.7825) | Xent 1.8312(1.9194) | Loss 5.5450(5.7422) | Error 0.6431(0.6738) Steps 520(506.27) | Grad Norm 2.4981(5.9176) | Total Time 14.00(14.00)\n",
      "Iter 0418 | Time 51.1563(50.1549) | Bit/dim 4.6352(4.7781) | Xent 1.8454(1.9172) | Loss 5.5579(5.7367) | Error 0.6492(0.6731) Steps 514(506.51) | Grad Norm 4.1296(5.8640) | Total Time 14.00(14.00)\n",
      "Iter 0419 | Time 52.2154(50.2168) | Bit/dim 4.6301(4.7737) | Xent 1.8483(1.9151) | Loss 5.5543(5.7312) | Error 0.6540(0.6725) Steps 520(506.91) | Grad Norm 5.1619(5.8429) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 50.7774(50.2336) | Bit/dim 4.6148(4.7689) | Xent 1.8304(1.9126) | Loss 5.5300(5.7252) | Error 0.6395(0.6715) Steps 514(507.12) | Grad Norm 2.8859(5.7542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.5739, Epoch Time 350.1733(348.7394), Bit/dim 4.6022(best: 4.6215), Xent 1.8108, Loss 5.5075, Error 0.6279(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 51.4745(50.2708) | Bit/dim 4.6060(4.7640) | Xent 1.8442(1.9105) | Loss 5.5281(5.7193) | Error 0.6481(0.6708) Steps 514(507.33) | Grad Norm 4.3845(5.7131) | Total Time 14.00(14.00)\n",
      "Iter 0422 | Time 50.4696(50.2768) | Bit/dim 4.5975(4.7590) | Xent 1.8720(1.9094) | Loss 5.5335(5.7137) | Error 0.6575(0.6704) Steps 508(507.35) | Grad Norm 7.1319(5.7557) | Total Time 14.00(14.00)\n",
      "Iter 0423 | Time 50.1817(50.2739) | Bit/dim 4.6178(4.7548) | Xent 1.8567(1.9078) | Loss 5.5461(5.7087) | Error 0.6560(0.6700) Steps 502(507.19) | Grad Norm 9.6053(5.8712) | Total Time 14.00(14.00)\n",
      "Iter 0424 | Time 51.1012(50.2987) | Bit/dim 4.6343(4.7512) | Xent 1.9001(1.9076) | Loss 5.5843(5.7049) | Error 0.6780(0.6702) Steps 502(507.03) | Grad Norm 13.5872(6.1027) | Total Time 14.00(14.00)\n",
      "Iter 0425 | Time 50.3050(50.2989) | Bit/dim 4.6284(4.7475) | Xent 1.8494(1.9058) | Loss 5.5530(5.7004) | Error 0.6499(0.6696) Steps 502(506.88) | Grad Norm 10.0164(6.2201) | Total Time 14.00(14.00)\n",
      "Iter 0426 | Time 51.9671(50.3490) | Bit/dim 4.5657(4.7420) | Xent 1.7945(1.9025) | Loss 5.4630(5.6933) | Error 0.6269(0.6684) Steps 508(506.92) | Grad Norm 2.0744(6.0957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 23.4603, Epoch Time 344.4980(348.6121), Bit/dim 4.5921(best: 4.6022), Xent 1.8137, Loss 5.4990, Error 0.6378(best: 0.6279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 50.4868(50.3531) | Bit/dim 4.5873(4.7374) | Xent 1.8613(1.9012) | Loss 5.5180(5.6880) | Error 0.6659(0.6683) Steps 502(506.77) | Grad Norm 7.6601(6.1426) | Total Time 14.00(14.00)\n",
      "Iter 0428 | Time 50.7688(50.3656) | Bit/dim 4.5603(4.7321) | Xent 1.8083(1.8985) | Loss 5.4644(5.6813) | Error 0.6338(0.6672) Steps 508(506.80) | Grad Norm 3.2196(6.0549) | Total Time 14.00(14.00)\n",
      "Iter 0429 | Time 50.3651(50.3656) | Bit/dim 4.5772(4.7274) | Xent 1.7972(1.8954) | Loss 5.4758(5.6751) | Error 0.6334(0.6662) Steps 496(506.48) | Grad Norm 4.7352(6.0153) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 51.0752(50.3869) | Bit/dim 4.5459(4.7220) | Xent 1.8113(1.8929) | Loss 5.4515(5.6684) | Error 0.6321(0.6652) Steps 508(506.53) | Grad Norm 1.9870(5.8945) | Total Time 14.00(14.00)\n",
      "Iter 0431 | Time 50.0111(50.3756) | Bit/dim 4.5515(4.7169) | Xent 1.8152(1.8906) | Loss 5.4591(5.6621) | Error 0.6449(0.6646) Steps 502(506.39) | Grad Norm 4.8233(5.8624) | Total Time 14.00(14.00)\n",
      "Iter 0432 | Time 50.5825(50.3818) | Bit/dim 4.5326(4.7113) | Xent 1.7987(1.8878) | Loss 5.4320(5.6552) | Error 0.6365(0.6638) Steps 502(506.26) | Grad Norm 2.0258(5.7473) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 23.4208, Epoch Time 342.2026(348.4198), Bit/dim 4.5435(best: 4.5921), Xent 1.7690, Loss 5.4280, Error 0.6184(best: 0.6279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 49.8985(50.3673) | Bit/dim 4.5376(4.7061) | Xent 1.7941(1.8850) | Loss 5.4346(5.6486) | Error 0.6329(0.6628) Steps 502(506.13) | Grad Norm 4.2387(5.7020) | Total Time 14.00(14.00)\n",
      "Iter 0434 | Time 50.7093(50.3775) | Bit/dim 4.5131(4.7003) | Xent 1.7991(1.8824) | Loss 5.4126(5.6415) | Error 0.6326(0.6619) Steps 508(506.19) | Grad Norm 3.1838(5.6265) | Total Time 14.00(14.00)\n",
      "Iter 0435 | Time 49.6396(50.3554) | Bit/dim 4.5254(4.6951) | Xent 1.8276(1.8808) | Loss 5.4392(5.6355) | Error 0.6505(0.6616) Steps 502(506.06) | Grad Norm 8.6813(5.7181) | Total Time 14.00(14.00)\n",
      "Iter 0436 | Time 52.9685(50.4338) | Bit/dim 4.5565(4.6909) | Xent 1.9732(1.8835) | Loss 5.5431(5.6327) | Error 0.6976(0.6627) Steps 526(506.66) | Grad Norm 13.6907(5.9573) | Total Time 14.00(14.00)\n",
      "Iter 0437 | Time 49.6228(50.4095) | Bit/dim 4.6222(4.6889) | Xent 2.0592(1.8888) | Loss 5.6518(5.6333) | Error 0.7221(0.6644) Steps 490(506.16) | Grad Norm 18.7387(6.3407) | Total Time 14.00(14.00)\n",
      "Iter 0438 | Time 52.3826(50.4687) | Bit/dim 4.6213(4.6868) | Xent 1.9880(1.8918) | Loss 5.6153(5.6327) | Error 0.7094(0.6658) Steps 538(507.12) | Grad Norm 9.7830(6.4440) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 23.6745, Epoch Time 344.3106(348.2966), Bit/dim 4.6050(best: 4.5435), Xent 1.9423, Loss 5.5762, Error 0.6901(best: 0.6184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 52.2395(50.5218) | Bit/dim 4.6057(4.6844) | Xent 2.0014(1.8951) | Loss 5.6065(5.6319) | Error 0.7208(0.6674) Steps 538(508.04) | Grad Norm 7.1911(6.4664) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 53.1687(50.6012) | Bit/dim 4.6304(4.6828) | Xent 1.9963(1.8981) | Loss 5.6285(5.6318) | Error 0.7057(0.6686) Steps 562(509.66) | Grad Norm 8.5905(6.5301) | Total Time 14.00(14.00)\n",
      "Iter 0441 | Time 51.4112(50.6255) | Bit/dim 4.6429(4.6816) | Xent 1.8991(1.8981) | Loss 5.5925(5.6307) | Error 0.6786(0.6689) Steps 544(510.69) | Grad Norm 3.8919(6.4510) | Total Time 14.00(14.00)\n",
      "Iter 0442 | Time 50.3777(50.6181) | Bit/dim 4.6263(4.6799) | Xent 1.8934(1.8980) | Loss 5.5730(5.6289) | Error 0.6784(0.6692) Steps 514(510.79) | Grad Norm 4.3155(6.3869) | Total Time 14.00(14.00)\n",
      "Iter 0443 | Time 52.1593(50.6643) | Bit/dim 4.6030(4.6776) | Xent 1.9107(1.8984) | Loss 5.5584(5.6268) | Error 0.6765(0.6694) Steps 520(511.07) | Grad Norm 6.1520(6.3799) | Total Time 14.00(14.00)\n",
      "Iter 0444 | Time 52.2852(50.7129) | Bit/dim 4.5743(4.6745) | Xent 1.8749(1.8977) | Loss 5.5117(5.6234) | Error 0.6623(0.6692) Steps 538(511.87) | Grad Norm 5.7747(6.3617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 25.0632, Epoch Time 352.2889(348.4163), Bit/dim 4.5906(best: 4.5435), Xent 1.8238, Loss 5.5025, Error 0.6369(best: 0.6184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 53.1759(50.7868) | Bit/dim 4.5925(4.6721) | Xent 1.8406(1.8960) | Loss 5.5128(5.6200) | Error 0.6484(0.6686) Steps 538(512.66) | Grad Norm 4.6724(6.3110) | Total Time 14.00(14.00)\n",
      "Iter 0446 | Time 52.5341(50.8392) | Bit/dim 4.5665(4.6689) | Xent 1.8415(1.8943) | Loss 5.4873(5.6161) | Error 0.6525(0.6681) Steps 538(513.42) | Grad Norm 3.6587(6.2315) | Total Time 14.00(14.00)\n",
      "Iter 0447 | Time 53.1429(50.9083) | Bit/dim 4.5675(4.6658) | Xent 1.8420(1.8928) | Loss 5.4885(5.6122) | Error 0.6527(0.6676) Steps 544(514.34) | Grad Norm 9.0860(6.3171) | Total Time 14.00(14.00)\n",
      "Iter 0448 | Time 51.9275(50.9389) | Bit/dim 4.5708(4.6630) | Xent 1.8669(1.8920) | Loss 5.5042(5.6090) | Error 0.6615(0.6674) Steps 538(515.05) | Grad Norm 13.7537(6.5402) | Total Time 14.00(14.00)\n",
      "Iter 0449 | Time 51.4720(50.9549) | Bit/dim 4.5700(4.6602) | Xent 1.9208(1.8929) | Loss 5.5304(5.6066) | Error 0.6746(0.6676) Steps 538(515.73) | Grad Norm 11.4061(6.6862) | Total Time 14.00(14.00)\n",
      "Iter 0450 | Time 50.6007(50.9443) | Bit/dim 4.5487(4.6569) | Xent 1.8030(1.8902) | Loss 5.4502(5.6019) | Error 0.6347(0.6667) Steps 514(515.68) | Grad Norm 5.1357(6.6397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 24.3377, Epoch Time 352.6129(348.5422), Bit/dim 4.5653(best: 4.5435), Xent 1.7946, Loss 5.4626, Error 0.6342(best: 0.6184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 51.3254(50.9557) | Bit/dim 4.5622(4.6540) | Xent 1.8255(1.8882) | Loss 5.4749(5.5981) | Error 0.6427(0.6659) Steps 532(516.17) | Grad Norm 10.8872(6.7671) | Total Time 14.00(14.00)\n",
      "Iter 0452 | Time 50.6913(50.9478) | Bit/dim 4.5445(4.6507) | Xent 1.8459(1.8869) | Loss 5.4675(5.5942) | Error 0.6540(0.6656) Steps 520(516.29) | Grad Norm 9.1072(6.8373) | Total Time 14.00(14.00)\n",
      "Iter 0453 | Time 51.4538(50.9630) | Bit/dim 4.4941(4.6460) | Xent 1.8014(1.8844) | Loss 5.3948(5.5882) | Error 0.6381(0.6648) Steps 520(516.40) | Grad Norm 3.0344(6.7232) | Total Time 14.00(14.00)\n",
      "Iter 0454 | Time 51.6255(50.9828) | Bit/dim 4.5348(4.6427) | Xent 1.8325(1.8828) | Loss 5.4511(5.5841) | Error 0.6420(0.6641) Steps 526(516.69) | Grad Norm 7.1315(6.7355) | Total Time 14.00(14.00)\n",
      "Iter 0455 | Time 51.9618(51.0122) | Bit/dim 4.4901(4.6381) | Xent 1.7889(1.8800) | Loss 5.3845(5.5781) | Error 0.6189(0.6627) Steps 526(516.97) | Grad Norm 3.1079(6.6266) | Total Time 14.00(14.00)\n",
      "Iter 0456 | Time 52.0936(51.0447) | Bit/dim 4.5022(4.6340) | Xent 1.8270(1.8784) | Loss 5.4157(5.5732) | Error 0.6490(0.6623) Steps 526(517.24) | Grad Norm 7.1713(6.6430) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 24.0174, Epoch Time 349.0855(348.5585), Bit/dim 4.5194(best: 4.5435), Xent 1.7480, Loss 5.3934, Error 0.6010(best: 0.6184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 52.2397(51.0805) | Bit/dim 4.5170(4.6305) | Xent 1.7853(1.8756) | Loss 5.4097(5.5683) | Error 0.6178(0.6610) Steps 532(517.68) | Grad Norm 5.6039(6.6118) | Total Time 14.00(14.00)\n",
      "Iter 0458 | Time 52.6924(51.1289) | Bit/dim 4.5055(4.6268) | Xent 1.8190(1.8739) | Loss 5.4150(5.5637) | Error 0.6424(0.6604) Steps 532(518.11) | Grad Norm 6.5877(6.6111) | Total Time 14.00(14.00)\n",
      "Iter 0459 | Time 51.5455(51.1414) | Bit/dim 4.5090(4.6232) | Xent 1.7992(1.8717) | Loss 5.4086(5.5591) | Error 0.6334(0.6596) Steps 532(518.53) | Grad Norm 6.8614(6.6186) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 51.4908(51.1518) | Bit/dim 4.4714(4.6187) | Xent 1.7659(1.8685) | Loss 5.3544(5.5529) | Error 0.6305(0.6587) Steps 532(518.93) | Grad Norm 3.9060(6.5372) | Total Time 14.00(14.00)\n",
      "Iter 0461 | Time 53.0136(51.2077) | Bit/dim 4.4752(4.6144) | Xent 1.7738(1.8657) | Loss 5.3621(5.5472) | Error 0.6245(0.6577) Steps 538(519.50) | Grad Norm 7.9919(6.5808) | Total Time 14.00(14.00)\n",
      "Iter 0462 | Time 52.7406(51.2537) | Bit/dim 4.4544(4.6096) | Xent 1.7612(1.8625) | Loss 5.3350(5.5409) | Error 0.6235(0.6567) Steps 538(520.06) | Grad Norm 3.3921(6.4852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 24.5665, Epoch Time 353.7551(348.7144), Bit/dim 4.4642(best: 4.5194), Xent 1.7329, Loss 5.3307, Error 0.6127(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 51.2288(51.2529) | Bit/dim 4.4638(4.6052) | Xent 1.7701(1.8598) | Loss 5.3488(5.5351) | Error 0.6249(0.6557) Steps 526(520.24) | Grad Norm 7.4426(6.5139) | Total Time 14.00(14.00)\n",
      "Iter 0464 | Time 51.6756(51.2656) | Bit/dim 4.5072(4.6023) | Xent 1.7643(1.8569) | Loss 5.3894(5.5307) | Error 0.6164(0.6545) Steps 532(520.59) | Grad Norm 9.2010(6.5945) | Total Time 14.00(14.00)\n",
      "Iter 0465 | Time 50.8393(51.2528) | Bit/dim 4.6985(4.6052) | Xent 2.3196(1.8708) | Loss 5.8583(5.5405) | Error 0.7303(0.6568) Steps 526(520.75) | Grad Norm 33.1282(7.3905) | Total Time 14.00(14.00)\n",
      "Iter 0466 | Time 48.2500(51.1627) | Bit/dim 5.5566(4.6337) | Xent 2.7680(1.8977) | Loss 6.9406(5.5825) | Error 0.8282(0.6620) Steps 526(520.91) | Grad Norm 28.9311(8.0367) | Total Time 14.00(14.00)\n",
      "Iter 0467 | Time 48.4115(51.0802) | Bit/dim 5.9693(4.6738) | Xent 1.9458(1.8991) | Loss 6.9422(5.6233) | Error 0.6849(0.6626) Steps 520(520.88) | Grad Norm 10.7814(8.1191) | Total Time 14.00(14.00)\n",
      "Iter 0468 | Time 47.4798(50.9722) | Bit/dim 5.8449(4.7089) | Xent 2.1460(1.9065) | Loss 6.9179(5.6622) | Error 0.7614(0.6656) Steps 520(520.85) | Grad Norm 10.7235(8.1972) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 25.0708, Epoch Time 338.4480(348.4064), Bit/dim 5.2663(best: 4.4642), Xent 2.2036, Loss 6.3681, Error 0.7443(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 53.3467(51.0434) | Bit/dim 5.2700(4.7257) | Xent 2.2462(1.9167) | Loss 6.3930(5.6841) | Error 0.7601(0.6684) Steps 550(521.73) | Grad Norm 6.9943(8.1611) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 54.7014(51.1532) | Bit/dim 5.1702(4.7391) | Xent 2.2156(1.9257) | Loss 6.2780(5.7019) | Error 0.7685(0.6714) Steps 598(524.02) | Grad Norm 8.7614(8.1791) | Total Time 14.00(14.00)\n",
      "Iter 0471 | Time 55.1218(51.2722) | Bit/dim 5.0586(4.7487) | Xent 2.2185(1.9345) | Loss 6.1679(5.7159) | Error 0.7790(0.6747) Steps 610(526.60) | Grad Norm 10.0360(8.2348) | Total Time 14.00(14.00)\n",
      "Iter 0472 | Time 55.7660(51.4070) | Bit/dim 5.1764(4.7615) | Xent 2.4434(1.9497) | Loss 6.3981(5.7364) | Error 0.8036(0.6785) Steps 592(528.56) | Grad Norm 19.6862(8.5784) | Total Time 14.00(14.00)\n",
      "Iter 0473 | Time 53.4781(51.4692) | Bit/dim 5.2180(4.7752) | Xent 2.1491(1.9557) | Loss 6.2926(5.7530) | Error 0.7638(0.6811) Steps 598(530.64) | Grad Norm 12.5585(8.6978) | Total Time 14.00(14.00)\n",
      "Iter 0474 | Time 56.5361(51.6212) | Bit/dim 5.3490(4.7924) | Xent 2.4364(1.9701) | Loss 6.5672(5.7775) | Error 0.8070(0.6849) Steps 634(533.74) | Grad Norm 22.6944(9.1177) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 27.6093, Epoch Time 372.0631(349.1161), Bit/dim 5.1117(best: 4.4642), Xent 2.9854, Loss 6.6044, Error 0.8674(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 54.6104(51.7109) | Bit/dim 5.1142(4.8021) | Xent 3.0214(2.0017) | Loss 6.6249(5.8029) | Error 0.8608(0.6901) Steps 610(536.03) | Grad Norm 19.4829(9.4286) | Total Time 14.00(14.00)\n",
      "Iter 0476 | Time 53.5246(51.7653) | Bit/dim 5.0898(4.8107) | Xent 2.2025(2.0077) | Loss 6.1911(5.8145) | Error 0.7837(0.6930) Steps 580(537.35) | Grad Norm 8.0394(9.3870) | Total Time 14.00(14.00)\n",
      "Iter 0477 | Time 55.9958(51.8922) | Bit/dim 4.9858(4.8159) | Xent 2.9962(2.0374) | Loss 6.4839(5.8346) | Error 0.8668(0.6982) Steps 598(539.17) | Grad Norm 24.7062(9.8465) | Total Time 14.00(14.00)\n",
      "Iter 0478 | Time 60.3343(52.1454) | Bit/dim 5.1068(4.8247) | Xent 2.7806(2.0597) | Loss 6.4971(5.8545) | Error 0.8235(0.7019) Steps 634(542.01) | Grad Norm 15.1014(10.0042) | Total Time 14.00(14.00)\n",
      "Iter 0479 | Time 55.5407(52.2473) | Bit/dim 5.3085(4.8392) | Xent 2.2477(2.0653) | Loss 6.4323(5.8718) | Error 0.8125(0.7052) Steps 598(543.69) | Grad Norm 7.0139(9.9145) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 54.8051(52.3240) | Bit/dim 5.4684(4.8581) | Xent 2.7420(2.0856) | Loss 6.8394(5.9009) | Error 0.8558(0.7098) Steps 604(545.50) | Grad Norm 30.0944(10.5199) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 27.8166, Epoch Time 378.3182(349.9922), Bit/dim 5.4353(best: 4.4642), Xent 3.1254, Loss 6.9980, Error 0.7714(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 56.1551(52.4390) | Bit/dim 5.4344(4.8753) | Xent 3.1483(2.1175) | Loss 7.0085(5.9341) | Error 0.7910(0.7122) Steps 628(547.98) | Grad Norm 12.9312(10.5922) | Total Time 14.00(14.00)\n",
      "Iter 0482 | Time 56.0182(52.5463) | Bit/dim 5.3328(4.8891) | Xent 3.0993(2.1469) | Loss 6.8825(5.9625) | Error 0.8181(0.7154) Steps 628(550.38) | Grad Norm 13.2217(10.6711) | Total Time 14.00(14.00)\n",
      "Iter 0483 | Time 58.0349(52.7110) | Bit/dim 5.1754(4.8977) | Xent 2.2342(2.1496) | Loss 6.2925(5.9724) | Error 0.7934(0.7177) Steps 634(552.89) | Grad Norm 6.1674(10.5360) | Total Time 14.00(14.00)\n",
      "Iter 0484 | Time 53.7647(52.7426) | Bit/dim 5.2018(4.9068) | Xent 2.4572(2.1588) | Loss 6.4304(5.9862) | Error 0.8662(0.7222) Steps 586(553.88) | Grad Norm 15.4584(10.6837) | Total Time 14.00(14.00)\n",
      "Iter 0485 | Time 54.7299(52.8022) | Bit/dim 5.0475(4.9110) | Xent 2.3002(2.1630) | Loss 6.1976(5.9925) | Error 0.7971(0.7244) Steps 574(554.48) | Grad Norm 8.4935(10.6180) | Total Time 14.00(14.00)\n",
      "Iter 0486 | Time 53.9587(52.8369) | Bit/dim 4.9663(4.9127) | Xent 2.1511(2.1627) | Loss 6.0419(5.9940) | Error 0.7844(0.7262) Steps 562(554.71) | Grad Norm 4.2692(10.4275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 24.2583, Epoch Time 372.6947(350.6733), Bit/dim 5.0383(best: 4.4642), Xent 2.3289, Loss 6.2028, Error 0.8510(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 51.8248(52.8066) | Bit/dim 5.0590(4.9171) | Xent 2.3798(2.1692) | Loss 6.2488(6.0016) | Error 0.8444(0.7298) Steps 520(553.67) | Grad Norm 11.5235(10.4604) | Total Time 14.00(14.00)\n",
      "Iter 0488 | Time 50.7112(52.7437) | Bit/dim 4.9767(4.9188) | Xent 2.3279(2.1739) | Loss 6.1406(6.0058) | Error 0.7843(0.7314) Steps 532(553.02) | Grad Norm 7.0996(10.3596) | Total Time 14.00(14.00)\n",
      "Iter 0489 | Time 53.1077(52.7546) | Bit/dim 4.9087(4.9185) | Xent 2.2893(2.1774) | Loss 6.0533(6.0072) | Error 0.7688(0.7325) Steps 538(552.57) | Grad Norm 5.3427(10.2090) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 53.3118(52.7713) | Bit/dim 4.8624(4.9169) | Xent 2.1377(2.1762) | Loss 5.9313(6.0050) | Error 0.7641(0.7335) Steps 532(551.95) | Grad Norm 3.7993(10.0168) | Total Time 14.00(14.00)\n",
      "Iter 0491 | Time 53.8764(52.8045) | Bit/dim 4.8695(4.9154) | Xent 2.2459(2.1783) | Loss 5.9924(6.0046) | Error 0.8119(0.7358) Steps 538(551.53) | Grad Norm 6.6900(9.9170) | Total Time 14.00(14.00)\n",
      "Iter 0492 | Time 53.4547(52.8240) | Bit/dim 4.8672(4.9140) | Xent 2.0552(2.1746) | Loss 5.8948(6.0013) | Error 0.7400(0.7359) Steps 532(550.95) | Grad Norm 3.4106(9.7218) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 24.8182, Epoch Time 356.5648(350.8500), Bit/dim 4.8762(best: 4.4642), Xent 2.1272, Loss 5.9398, Error 0.7757(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 54.2118(52.8656) | Bit/dim 4.8694(4.9126) | Xent 2.1528(2.1740) | Loss 5.9458(5.9996) | Error 0.7806(0.7373) Steps 532(550.38) | Grad Norm 4.8329(9.5751) | Total Time 14.00(14.00)\n",
      "Iter 0494 | Time 54.4486(52.9131) | Bit/dim 4.8248(4.9100) | Xent 2.0475(2.1702) | Loss 5.8486(5.9951) | Error 0.7383(0.7373) Steps 532(549.83) | Grad Norm 3.7617(9.4007) | Total Time 14.00(14.00)\n",
      "Iter 0495 | Time 54.6937(52.9665) | Bit/dim 4.7979(4.9066) | Xent 2.0801(2.1675) | Loss 5.8380(5.9904) | Error 0.7344(0.7372) Steps 550(549.83) | Grad Norm 5.3387(9.2788) | Total Time 14.00(14.00)\n",
      "Iter 0496 | Time 55.4297(53.0404) | Bit/dim 4.7820(4.9029) | Xent 2.0344(2.1635) | Loss 5.7992(5.9846) | Error 0.7280(0.7370) Steps 538(549.48) | Grad Norm 3.3040(9.0996) | Total Time 14.00(14.00)\n",
      "Iter 0497 | Time 54.6515(53.0888) | Bit/dim 4.7880(4.8995) | Xent 2.0488(2.1600) | Loss 5.8123(5.9795) | Error 0.7261(0.7366) Steps 538(549.13) | Grad Norm 6.6283(9.0254) | Total Time 14.00(14.00)\n",
      "Iter 0498 | Time 57.2670(53.2141) | Bit/dim 4.7641(4.8954) | Xent 1.9884(2.1549) | Loss 5.7583(5.9728) | Error 0.7026(0.7356) Steps 532(548.62) | Grad Norm 2.1275(8.8185) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 23.8516, Epoch Time 370.0318(351.4255), Bit/dim 4.7691(best: 4.4642), Xent 1.9654, Loss 5.7518, Error 0.7068(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 57.5459(53.3441) | Bit/dim 4.7664(4.8915) | Xent 2.0068(2.1504) | Loss 5.7698(5.9667) | Error 0.7083(0.7348) Steps 538(548.30) | Grad Norm 5.5546(8.7206) | Total Time 14.00(14.00)\n",
      "Iter 0500 | Time 57.7829(53.4772) | Bit/dim 4.7395(4.8870) | Xent 1.9889(2.1456) | Loss 5.7339(5.9598) | Error 0.7103(0.7340) Steps 544(548.17) | Grad Norm 2.8718(8.5451) | Total Time 14.00(14.00)\n",
      "Iter 0501 | Time 58.3546(53.6235) | Bit/dim 4.7223(4.8820) | Xent 1.9843(2.1408) | Loss 5.7145(5.9524) | Error 0.7026(0.7331) Steps 544(548.05) | Grad Norm 4.9528(8.4374) | Total Time 14.00(14.00)\n",
      "Iter 0502 | Time 57.6833(53.7453) | Bit/dim 4.7129(4.8770) | Xent 1.9269(2.1343) | Loss 5.6764(5.9441) | Error 0.6825(0.7316) Steps 532(547.56) | Grad Norm 1.6141(8.2327) | Total Time 14.00(14.00)\n",
      "Iter 0503 | Time 54.9774(53.7823) | Bit/dim 4.6988(4.8716) | Xent 1.9918(2.1301) | Loss 5.6946(5.9366) | Error 0.7049(0.7308) Steps 532(547.10) | Grad Norm 5.9605(8.1645) | Total Time 14.00(14.00)\n",
      "Iter 0504 | Time 54.3897(53.8005) | Bit/dim 4.6943(4.8663) | Xent 1.9504(2.1247) | Loss 5.6695(5.9286) | Error 0.6924(0.7296) Steps 532(546.64) | Grad Norm 5.7250(8.0913) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 24.0368, Epoch Time 380.2922(352.2915), Bit/dim 4.6825(best: 4.4642), Xent 1.9024, Loss 5.6337, Error 0.6609(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 53.2579(53.7842) | Bit/dim 4.6909(4.8610) | Xent 1.9363(2.1190) | Loss 5.6591(5.9205) | Error 0.6816(0.7282) Steps 532(546.21) | Grad Norm 2.1045(7.9117) | Total Time 14.00(14.00)\n",
      "Iter 0506 | Time 54.0377(53.7919) | Bit/dim 4.6660(4.8552) | Xent 1.9164(2.1129) | Loss 5.6242(5.9116) | Error 0.6816(0.7268) Steps 526(545.60) | Grad Norm 3.8260(7.7891) | Total Time 14.00(14.00)\n",
      "Iter 0507 | Time 53.2715(53.7762) | Bit/dim 4.6536(4.8491) | Xent 1.9205(2.1072) | Loss 5.6138(5.9027) | Error 0.6826(0.7255) Steps 526(545.01) | Grad Norm 3.1283(7.6493) | Total Time 14.00(14.00)\n",
      "Iter 0508 | Time 54.6563(53.8026) | Bit/dim 4.6750(4.8439) | Xent 1.9028(2.1010) | Loss 5.6264(5.8944) | Error 0.6707(0.7238) Steps 532(544.62) | Grad Norm 3.1159(7.5133) | Total Time 14.00(14.00)\n",
      "Iter 0509 | Time 55.4110(53.8509) | Bit/dim 4.6445(4.8379) | Xent 1.8871(2.0946) | Loss 5.5880(5.8852) | Error 0.6665(0.7221) Steps 532(544.24) | Grad Norm 1.9397(7.3461) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 55.7245(53.9071) | Bit/dim 4.6488(4.8323) | Xent 1.8898(2.0885) | Loss 5.5937(5.8765) | Error 0.6684(0.7205) Steps 532(543.87) | Grad Norm 4.3512(7.2563) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 22.8626, Epoch Time 364.7311(352.6647), Bit/dim 4.6484(best: 4.4642), Xent 1.8660, Loss 5.5815, Error 0.6429(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 53.1205(53.8835) | Bit/dim 4.6500(4.8268) | Xent 1.8923(2.0826) | Loss 5.5961(5.8681) | Error 0.6693(0.7190) Steps 526(543.34) | Grad Norm 3.9191(7.1561) | Total Time 14.00(14.00)\n",
      "Iter 0512 | Time 53.5618(53.8739) | Bit/dim 4.6362(4.8211) | Xent 1.8935(2.0769) | Loss 5.5829(5.8595) | Error 0.6747(0.7176) Steps 532(543.00) | Grad Norm 6.3644(7.1324) | Total Time 14.00(14.00)\n",
      "Iter 0513 | Time 54.7235(53.8993) | Bit/dim 4.6294(4.8153) | Xent 1.8925(2.0714) | Loss 5.5757(5.8510) | Error 0.6651(0.7161) Steps 538(542.85) | Grad Norm 7.3722(7.1396) | Total Time 14.00(14.00)\n",
      "Iter 0514 | Time 55.7218(53.9540) | Bit/dim 4.6016(4.8089) | Xent 1.8699(2.0653) | Loss 5.5366(5.8416) | Error 0.6587(0.7143) Steps 544(542.88) | Grad Norm 5.5948(7.0932) | Total Time 14.00(14.00)\n",
      "Iter 0515 | Time 52.3481(53.9058) | Bit/dim 4.6047(4.8028) | Xent 1.8596(2.0592) | Loss 5.5345(5.8324) | Error 0.6509(0.7124) Steps 520(542.20) | Grad Norm 3.0219(6.9711) | Total Time 14.00(14.00)\n",
      "Iter 0516 | Time 54.2561(53.9163) | Bit/dim 4.5894(4.7964) | Xent 1.8545(2.0530) | Loss 5.5167(5.8229) | Error 0.6469(0.7105) Steps 526(541.71) | Grad Norm 4.8080(6.9062) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 23.5931, Epoch Time 362.9111(352.9721), Bit/dim 4.5942(best: 4.4642), Xent 1.9087, Loss 5.5486, Error 0.6687(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 54.4241(53.9316) | Bit/dim 4.5828(4.7900) | Xent 1.9368(2.0495) | Loss 5.5512(5.8147) | Error 0.6846(0.7097) Steps 532(541.42) | Grad Norm 10.6796(7.0194) | Total Time 14.00(14.00)\n",
      "Iter 0518 | Time 55.6299(53.9825) | Bit/dim 4.6503(4.7858) | Xent 2.2893(2.0567) | Loss 5.7949(5.8141) | Error 0.7596(0.7112) Steps 556(541.86) | Grad Norm 23.8927(7.5256) | Total Time 14.00(14.00)\n",
      "Iter 0519 | Time 51.5223(53.9087) | Bit/dim 4.8702(4.7883) | Xent 2.8794(2.0814) | Loss 6.3099(5.8290) | Error 0.8377(0.7150) Steps 514(541.02) | Grad Norm 34.0366(8.3209) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 51.9703(53.8506) | Bit/dim 4.7102(4.7860) | Xent 2.1512(2.0835) | Loss 5.7858(5.8277) | Error 0.7276(0.7154) Steps 538(540.93) | Grad Norm 9.3651(8.3523) | Total Time 14.00(14.00)\n",
      "Iter 0521 | Time 52.9317(53.8230) | Bit/dim 4.8165(4.7869) | Xent 2.3441(2.0913) | Loss 5.9886(5.8326) | Error 0.7463(0.7163) Steps 556(541.38) | Grad Norm 10.0277(8.4025) | Total Time 14.00(14.00)\n",
      "Iter 0522 | Time 54.9929(53.8581) | Bit/dim 4.6947(4.7841) | Xent 2.1478(2.0930) | Loss 5.7685(5.8306) | Error 0.7338(0.7168) Steps 556(541.82) | Grad Norm 5.0885(8.3031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 25.3591, Epoch Time 362.5193(353.2585), Bit/dim 4.6730(best: 4.4642), Xent 1.9948, Loss 5.6705, Error 0.7192(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 53.3444(53.8427) | Bit/dim 4.6751(4.7809) | Xent 2.0438(2.0915) | Loss 5.6970(5.8266) | Error 0.7195(0.7169) Steps 550(542.07) | Grad Norm 6.9170(8.2615) | Total Time 14.00(14.00)\n",
      "Iter 0524 | Time 53.7925(53.8412) | Bit/dim 4.6673(4.7774) | Xent 1.9826(2.0883) | Loss 5.6586(5.8216) | Error 0.7101(0.7167) Steps 538(541.94) | Grad Norm 8.6644(8.2736) | Total Time 14.00(14.00)\n",
      "Iter 0525 | Time 54.6404(53.8652) | Bit/dim 4.6599(4.7739) | Xent 2.0008(2.0856) | Loss 5.6603(5.8167) | Error 0.7081(0.7164) Steps 544(542.01) | Grad Norm 6.0712(8.2075) | Total Time 14.00(14.00)\n",
      "Iter 0526 | Time 54.3454(53.8796) | Bit/dim 4.6703(4.7708) | Xent 1.9439(2.0814) | Loss 5.6423(5.8115) | Error 0.6887(0.7156) Steps 532(541.71) | Grad Norm 2.9857(8.0509) | Total Time 14.00(14.00)\n",
      "Iter 0527 | Time 54.1931(53.8890) | Bit/dim 4.6603(4.7675) | Xent 1.9878(2.0786) | Loss 5.6542(5.8068) | Error 0.7069(0.7153) Steps 538(541.59) | Grad Norm 4.2146(7.9358) | Total Time 14.00(14.00)\n",
      "Iter 0528 | Time 54.4133(53.9047) | Bit/dim 4.6425(4.7637) | Xent 2.0159(2.0767) | Loss 5.6504(5.8021) | Error 0.7159(0.7154) Steps 538(541.49) | Grad Norm 5.2301(7.8546) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 24.4099, Epoch Time 364.8185(353.6053), Bit/dim 4.6382(best: 4.4642), Xent 1.9403, Loss 5.6083, Error 0.6870(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 54.7289(53.9294) | Bit/dim 4.6255(4.7596) | Xent 1.9788(2.0738) | Loss 5.6149(5.7965) | Error 0.7010(0.7149) Steps 550(541.74) | Grad Norm 2.6136(7.6974) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 53.7425(53.9238) | Bit/dim 4.6334(4.7558) | Xent 1.9824(2.0710) | Loss 5.6246(5.7913) | Error 0.7116(0.7148) Steps 550(541.99) | Grad Norm 6.2362(7.6535) | Total Time 14.00(14.00)\n",
      "Iter 0531 | Time 52.9801(53.8955) | Bit/dim 4.6188(4.7517) | Xent 1.9561(2.0676) | Loss 5.5969(5.7855) | Error 0.7010(0.7144) Steps 544(542.05) | Grad Norm 2.5457(7.5003) | Total Time 14.00(14.00)\n",
      "Iter 0532 | Time 52.9207(53.8663) | Bit/dim 4.6203(4.7478) | Xent 1.9306(2.0635) | Loss 5.5856(5.7795) | Error 0.6831(0.7135) Steps 526(541.57) | Grad Norm 3.1305(7.3692) | Total Time 14.00(14.00)\n",
      "Iter 0533 | Time 51.7077(53.8015) | Bit/dim 4.5997(4.7433) | Xent 1.9568(2.0603) | Loss 5.5781(5.7735) | Error 0.6916(0.7128) Steps 526(541.10) | Grad Norm 3.2508(7.2457) | Total Time 14.00(14.00)\n",
      "Iter 0534 | Time 53.1974(53.7834) | Bit/dim 4.5869(4.7386) | Xent 1.9411(2.0567) | Loss 5.5574(5.7670) | Error 0.6906(0.7122) Steps 520(540.47) | Grad Norm 2.3781(7.0996) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 23.0978, Epoch Time 358.0271(353.7379), Bit/dim 4.5752(best: 4.4642), Xent 1.9222, Loss 5.5363, Error 0.6890(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 52.5939(53.7477) | Bit/dim 4.5732(4.7337) | Xent 1.9488(2.0535) | Loss 5.5476(5.7604) | Error 0.7014(0.7118) Steps 514(539.67) | Grad Norm 3.9568(7.0054) | Total Time 14.00(14.00)\n",
      "Iter 0536 | Time 51.4207(53.6779) | Bit/dim 4.5599(4.7285) | Xent 1.9369(2.0500) | Loss 5.5284(5.7534) | Error 0.6925(0.7113) Steps 508(538.72) | Grad Norm 2.0183(6.8557) | Total Time 14.00(14.00)\n",
      "Iter 0537 | Time 51.1841(53.6031) | Bit/dim 4.5534(4.7232) | Xent 1.9149(2.0459) | Loss 5.5109(5.7462) | Error 0.6795(0.7103) Steps 520(538.16) | Grad Norm 2.9745(6.7393) | Total Time 14.00(14.00)\n",
      "Iter 0538 | Time 51.9281(53.5528) | Bit/dim 4.5621(4.7184) | Xent 1.9371(2.0427) | Loss 5.5307(5.7397) | Error 0.6963(0.7099) Steps 514(537.44) | Grad Norm 4.8299(6.6820) | Total Time 14.00(14.00)\n",
      "Iter 0539 | Time 51.3739(53.4875) | Bit/dim 4.5483(4.7133) | Xent 1.9316(2.0393) | Loss 5.5141(5.7329) | Error 0.6824(0.7091) Steps 520(536.91) | Grad Norm 2.8840(6.5681) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 52.1613(53.4477) | Bit/dim 4.5333(4.7079) | Xent 1.9017(2.0352) | Loss 5.4841(5.7255) | Error 0.6691(0.7079) Steps 526(536.59) | Grad Norm 2.2596(6.4388) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 23.3090, Epoch Time 349.7089(353.6171), Bit/dim 4.5286(best: 4.4642), Xent 1.8766, Loss 5.4669, Error 0.6630(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 50.6551(53.3639) | Bit/dim 4.5197(4.7022) | Xent 1.9019(2.0312) | Loss 5.4707(5.7178) | Error 0.6709(0.7067) Steps 514(535.91) | Grad Norm 3.5168(6.3512) | Total Time 14.00(14.00)\n",
      "Iter 0542 | Time 51.5780(53.3103) | Bit/dim 4.5290(4.6970) | Xent 1.8936(2.0271) | Loss 5.4758(5.7106) | Error 0.6678(0.7056) Steps 526(535.61) | Grad Norm 4.3367(6.2907) | Total Time 14.00(14.00)\n",
      "Iter 0543 | Time 51.0535(53.2426) | Bit/dim 4.5343(4.6921) | Xent 1.9015(2.0233) | Loss 5.4851(5.7038) | Error 0.6699(0.7045) Steps 514(534.96) | Grad Norm 3.2586(6.1998) | Total Time 14.00(14.00)\n",
      "Iter 0544 | Time 51.2893(53.1840) | Bit/dim 4.5081(4.6866) | Xent 1.8926(2.0194) | Loss 5.4544(5.6963) | Error 0.6729(0.7036) Steps 514(534.33) | Grad Norm 1.3729(6.0550) | Total Time 14.00(14.00)\n",
      "Iter 0545 | Time 51.5344(53.1345) | Bit/dim 4.5079(4.6813) | Xent 1.9110(2.0161) | Loss 5.4634(5.6893) | Error 0.6773(0.7028) Steps 514(533.72) | Grad Norm 4.2345(6.0004) | Total Time 14.00(14.00)\n",
      "Iter 0546 | Time 52.4225(53.1132) | Bit/dim 4.5232(4.6765) | Xent 1.9157(2.0131) | Loss 5.4811(5.6831) | Error 0.6718(0.7018) Steps 514(533.13) | Grad Norm 6.2083(6.0066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 23.3897, Epoch Time 347.5287(353.4344), Bit/dim 4.5053(best: 4.4642), Xent 1.8766, Loss 5.4436, Error 0.6675(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 51.7992(53.0737) | Bit/dim 4.4974(4.6711) | Xent 1.9013(2.0098) | Loss 5.4480(5.6760) | Error 0.6759(0.7011) Steps 514(532.56) | Grad Norm 5.6097(5.9947) | Total Time 14.00(14.00)\n",
      "Iter 0548 | Time 51.4430(53.0248) | Bit/dim 4.4971(4.6659) | Xent 1.8995(2.0065) | Loss 5.4468(5.6692) | Error 0.6751(0.7003) Steps 496(531.46) | Grad Norm 4.0222(5.9355) | Total Time 14.00(14.00)\n",
      "Iter 0549 | Time 50.2591(52.9419) | Bit/dim 4.4902(4.6607) | Xent 1.8776(2.0026) | Loss 5.4290(5.6619) | Error 0.6593(0.6990) Steps 514(530.94) | Grad Norm 2.2149(5.8239) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 51.6644(52.9035) | Bit/dim 4.4860(4.6554) | Xent 1.8653(1.9985) | Loss 5.4186(5.6546) | Error 0.6542(0.6977) Steps 490(529.71) | Grad Norm 2.1949(5.7150) | Total Time 14.00(14.00)\n",
      "Iter 0551 | Time 51.0562(52.8481) | Bit/dim 4.4893(4.6504) | Xent 1.8733(1.9947) | Loss 5.4259(5.6478) | Error 0.6621(0.6966) Steps 514(529.24) | Grad Norm 3.4880(5.6482) | Total Time 14.00(14.00)\n",
      "Iter 0552 | Time 51.3530(52.8033) | Bit/dim 4.4795(4.6453) | Xent 1.9001(1.9919) | Loss 5.4295(5.6412) | Error 0.6686(0.6958) Steps 514(528.78) | Grad Norm 4.0055(5.5989) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 22.7794, Epoch Time 345.8369(353.2065), Bit/dim 4.4774(best: 4.4642), Xent 1.8477, Loss 5.4012, Error 0.6478(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 52.1481(52.7836) | Bit/dim 4.4692(4.6400) | Xent 1.8913(1.9889) | Loss 5.4148(5.6344) | Error 0.6665(0.6949) Steps 514(528.34) | Grad Norm 4.4777(5.5653) | Total Time 14.00(14.00)\n",
      "Iter 0554 | Time 52.9077(52.7873) | Bit/dim 4.4737(4.6350) | Xent 1.8865(1.9858) | Loss 5.4169(5.6279) | Error 0.6715(0.6942) Steps 514(527.91) | Grad Norm 3.7477(5.5108) | Total Time 14.00(14.00)\n",
      "Iter 0555 | Time 51.7700(52.7568) | Bit/dim 4.4785(4.6303) | Xent 1.8454(1.9816) | Loss 5.4012(5.6211) | Error 0.6465(0.6928) Steps 496(526.95) | Grad Norm 3.4380(5.4486) | Total Time 14.00(14.00)\n",
      "Iter 0556 | Time 52.1936(52.7399) | Bit/dim 4.4650(4.6254) | Xent 1.8522(1.9777) | Loss 5.3911(5.6142) | Error 0.6527(0.6916) Steps 520(526.74) | Grad Norm 3.1074(5.3783) | Total Time 14.00(14.00)\n",
      "Iter 0557 | Time 52.1986(52.7237) | Bit/dim 4.4610(4.6204) | Xent 1.8506(1.9739) | Loss 5.3863(5.6074) | Error 0.6539(0.6905) Steps 520(526.54) | Grad Norm 2.4173(5.2895) | Total Time 14.00(14.00)\n",
      "Iter 0558 | Time 52.1806(52.7074) | Bit/dim 4.4631(4.6157) | Xent 1.8464(1.9701) | Loss 5.3863(5.6007) | Error 0.6489(0.6892) Steps 520(526.34) | Grad Norm 1.4192(5.1734) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 23.0127, Epoch Time 352.1341(353.1743), Bit/dim 4.4528(best: 4.4642), Xent 1.8067, Loss 5.3561, Error 0.6325(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 51.2524(52.6637) | Bit/dim 4.4528(4.6108) | Xent 1.8371(1.9661) | Loss 5.3714(5.5939) | Error 0.6505(0.6880) Steps 520(526.15) | Grad Norm 1.2451(5.0556) | Total Time 14.00(14.00)\n",
      "Iter 0560 | Time 51.6091(52.6321) | Bit/dim 4.4371(4.6056) | Xent 1.8450(1.9624) | Loss 5.3596(5.5868) | Error 0.6499(0.6869) Steps 520(525.97) | Grad Norm 3.1389(4.9981) | Total Time 14.00(14.00)\n",
      "Iter 0561 | Time 51.7232(52.6048) | Bit/dim 4.4537(4.6011) | Xent 1.8702(1.9597) | Loss 5.3888(5.5809) | Error 0.6646(0.6862) Steps 520(525.79) | Grad Norm 6.1009(5.0311) | Total Time 14.00(14.00)\n",
      "Iter 0562 | Time 52.2970(52.5956) | Bit/dim 4.5126(4.5984) | Xent 1.9295(1.9588) | Loss 5.4773(5.5778) | Error 0.6771(0.6860) Steps 520(525.62) | Grad Norm 9.6177(5.1687) | Total Time 14.00(14.00)\n",
      "Iter 0563 | Time 51.3394(52.5579) | Bit/dim 4.4760(4.5947) | Xent 1.8945(1.9568) | Loss 5.4232(5.5732) | Error 0.6649(0.6853) Steps 520(525.45) | Grad Norm 9.8139(5.3081) | Total Time 14.00(14.00)\n",
      "Iter 0564 | Time 52.0649(52.5431) | Bit/dim 4.4720(4.5911) | Xent 1.8884(1.9548) | Loss 5.4162(5.5684) | Error 0.6756(0.6850) Steps 514(525.10) | Grad Norm 6.8633(5.3548) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 23.0598, Epoch Time 348.6684(353.0391), Bit/dim 4.4386(best: 4.4528), Xent 1.8135, Loss 5.3454, Error 0.6336(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 53.0034(52.5569) | Bit/dim 4.4325(4.5863) | Xent 1.8554(1.9518) | Loss 5.3602(5.5622) | Error 0.6482(0.6839) Steps 496(524.23) | Grad Norm 1.9533(5.2527) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 52.0656(52.5422) | Bit/dim 4.4595(4.5825) | Xent 1.8697(1.9493) | Loss 5.3944(5.5572) | Error 0.6556(0.6831) Steps 526(524.28) | Grad Norm 6.2207(5.2817) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 51.8081(52.5202) | Bit/dim 4.4445(4.5784) | Xent 1.8871(1.9475) | Loss 5.3880(5.5521) | Error 0.6627(0.6825) Steps 526(524.34) | Grad Norm 5.2285(5.2801) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 53.5243(52.5503) | Bit/dim 4.4433(4.5743) | Xent 1.8390(1.9442) | Loss 5.3628(5.5464) | Error 0.6525(0.6816) Steps 526(524.39) | Grad Norm 2.7524(5.2043) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 52.1357(52.5378) | Bit/dim 4.4578(4.5708) | Xent 1.8910(1.9426) | Loss 5.4033(5.5421) | Error 0.6640(0.6810) Steps 526(524.43) | Grad Norm 5.7221(5.2199) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 52.5882(52.5393) | Bit/dim 4.4594(4.5675) | Xent 1.8614(1.9402) | Loss 5.3901(5.5376) | Error 0.6685(0.6807) Steps 526(524.48) | Grad Norm 3.8796(5.1796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 23.7102, Epoch Time 354.6786(353.0883), Bit/dim 4.4334(best: 4.4386), Xent 1.8094, Loss 5.3380, Error 0.6317(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 52.8997(52.5502) | Bit/dim 4.4358(4.5635) | Xent 1.8482(1.9374) | Loss 5.3599(5.5322) | Error 0.6558(0.6799) Steps 532(524.71) | Grad Norm 3.3199(5.1239) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 51.5101(52.5190) | Bit/dim 4.4373(4.5597) | Xent 1.8714(1.9354) | Loss 5.3730(5.5275) | Error 0.6618(0.6794) Steps 526(524.75) | Grad Norm 5.2955(5.1290) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 51.6330(52.4924) | Bit/dim 4.4445(4.5563) | Xent 1.8309(1.9323) | Loss 5.3599(5.5224) | Error 0.6550(0.6786) Steps 526(524.78) | Grad Norm 2.8400(5.0603) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 51.5210(52.4632) | Bit/dim 4.4241(4.5523) | Xent 1.8498(1.9298) | Loss 5.3490(5.5172) | Error 0.6591(0.6781) Steps 526(524.82) | Grad Norm 3.8439(5.0238) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 51.6379(52.4385) | Bit/dim 4.4346(4.5488) | Xent 1.8084(1.9262) | Loss 5.3388(5.5119) | Error 0.6355(0.6768) Steps 526(524.86) | Grad Norm 4.0443(4.9945) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 51.9764(52.4246) | Bit/dim 4.4367(4.5454) | Xent 1.8263(1.9232) | Loss 5.3498(5.5070) | Error 0.6455(0.6758) Steps 526(524.89) | Grad Norm 3.8309(4.9595) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 23.4819, Epoch Time 350.3896(353.0074), Bit/dim 4.4228(best: 4.4334), Xent 1.7650, Loss 5.3054, Error 0.6149(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 51.7980(52.4058) | Bit/dim 4.4245(4.5418) | Xent 1.8069(1.9197) | Loss 5.3279(5.5016) | Error 0.6379(0.6747) Steps 526(524.92) | Grad Norm 4.1594(4.9355) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 52.3163(52.4031) | Bit/dim 4.4242(4.5383) | Xent 1.8275(1.9169) | Loss 5.3379(5.4967) | Error 0.6462(0.6739) Steps 526(524.96) | Grad Norm 6.0807(4.9699) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 51.7837(52.3845) | Bit/dim 4.4524(4.5357) | Xent 1.8146(1.9139) | Loss 5.3597(5.4926) | Error 0.6419(0.6729) Steps 520(524.81) | Grad Norm 6.0096(5.0011) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 52.8650(52.3990) | Bit/dim 4.4195(4.5322) | Xent 1.8145(1.9109) | Loss 5.3268(5.4876) | Error 0.6426(0.6720) Steps 526(524.84) | Grad Norm 3.9369(4.9692) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 53.4997(52.4320) | Bit/dim 4.4075(4.5285) | Xent 1.7733(1.9068) | Loss 5.2941(5.4818) | Error 0.6236(0.6705) Steps 526(524.88) | Grad Norm 1.7189(4.8717) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 51.3466(52.3994) | Bit/dim 4.4020(4.5247) | Xent 1.8145(1.9040) | Loss 5.3093(5.4767) | Error 0.6402(0.6696) Steps 526(524.91) | Grad Norm 2.7942(4.8093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 23.6148, Epoch Time 352.9170(353.0046), Bit/dim 4.4062(best: 4.4228), Xent 1.7501, Loss 5.2812, Error 0.6133(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 51.7200(52.3790) | Bit/dim 4.4046(4.5211) | Xent 1.7794(1.9003) | Loss 5.2942(5.4712) | Error 0.6304(0.6684) Steps 526(524.94) | Grad Norm 3.7104(4.7764) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 52.2009(52.3737) | Bit/dim 4.4059(4.5176) | Xent 1.7942(1.8971) | Loss 5.3030(5.4661) | Error 0.6246(0.6671) Steps 526(524.97) | Grad Norm 4.0944(4.7559) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 52.4814(52.3769) | Bit/dim 4.4041(4.5142) | Xent 1.7827(1.8936) | Loss 5.2954(5.4610) | Error 0.6260(0.6659) Steps 532(525.19) | Grad Norm 4.3007(4.7422) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 52.4805(52.3800) | Bit/dim 4.4137(4.5112) | Xent 1.8136(1.8912) | Loss 5.3205(5.4568) | Error 0.6434(0.6652) Steps 526(525.21) | Grad Norm 6.5899(4.7977) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 52.7398(52.3908) | Bit/dim 4.5144(4.5113) | Xent 1.8538(1.8901) | Loss 5.4413(5.4563) | Error 0.6624(0.6651) Steps 556(526.13) | Grad Norm 12.9624(5.0426) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 51.1356(52.3532) | Bit/dim 4.6050(4.5141) | Xent 1.9424(1.8917) | Loss 5.5761(5.4599) | Error 0.6799(0.6656) Steps 532(526.31) | Grad Norm 11.8523(5.2469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 23.7133, Epoch Time 352.4070(352.9867), Bit/dim 4.4827(best: 4.4062), Xent 1.9882, Loss 5.4768, Error 0.6970(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 49.9356(52.2806) | Bit/dim 4.4865(4.5133) | Xent 2.0352(1.8960) | Loss 5.5041(5.4613) | Error 0.7000(0.6666) Steps 526(526.30) | Grad Norm 11.9177(5.4470) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 54.1250(52.3360) | Bit/dim 4.5863(4.5155) | Xent 2.2717(1.9073) | Loss 5.7222(5.4691) | Error 0.7561(0.6693) Steps 562(527.37) | Grad Norm 20.5748(5.9009) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 49.6486(52.2554) | Bit/dim 4.7638(4.5229) | Xent 2.1401(1.9142) | Loss 5.8339(5.4800) | Error 0.7525(0.6718) Steps 520(527.15) | Grad Norm 10.7075(6.0451) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 50.3608(52.1985) | Bit/dim 4.7742(4.5304) | Xent 2.0396(1.9180) | Loss 5.7940(5.4894) | Error 0.7271(0.6735) Steps 514(526.76) | Grad Norm 8.2111(6.1100) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 52.2165(52.1991) | Bit/dim 4.6336(4.5335) | Xent 1.9852(1.9200) | Loss 5.6262(5.4935) | Error 0.7094(0.6745) Steps 526(526.73) | Grad Norm 4.7823(6.0702) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 51.6089(52.1814) | Bit/dim 4.6068(4.5357) | Xent 1.9864(1.9220) | Loss 5.6000(5.4967) | Error 0.7020(0.6754) Steps 544(527.25) | Grad Norm 4.0067(6.0083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 24.6198, Epoch Time 348.0830(352.8396), Bit/dim 4.6704(best: 4.4062), Xent 1.9669, Loss 5.6538, Error 0.6933(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 52.3703(52.1870) | Bit/dim 4.6733(4.5399) | Xent 2.0095(1.9246) | Loss 5.6780(5.5022) | Error 0.7146(0.6765) Steps 550(527.93) | Grad Norm 4.9720(5.9772) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 52.3713(52.1925) | Bit/dim 4.6188(4.5422) | Xent 1.9778(1.9262) | Loss 5.6077(5.5053) | Error 0.6967(0.6771) Steps 550(528.60) | Grad Norm 3.5725(5.9051) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 50.7815(52.1502) | Bit/dim 4.5761(4.5432) | Xent 1.9581(1.9272) | Loss 5.5551(5.5068) | Error 0.6901(0.6775) Steps 526(528.52) | Grad Norm 2.9773(5.8172) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 53.5603(52.1925) | Bit/dim 4.5410(4.5432) | Xent 1.9657(1.9283) | Loss 5.5238(5.5073) | Error 0.7054(0.6784) Steps 526(528.44) | Grad Norm 2.9711(5.7319) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 54.2259(52.2535) | Bit/dim 4.5717(4.5440) | Xent 1.9468(1.9289) | Loss 5.5451(5.5085) | Error 0.6960(0.6789) Steps 526(528.37) | Grad Norm 4.2331(5.6869) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 52.4776(52.2602) | Bit/dim 4.5610(4.5445) | Xent 1.9483(1.9295) | Loss 5.5352(5.5093) | Error 0.6984(0.6795) Steps 538(528.66) | Grad Norm 3.8232(5.6310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 23.9834, Epoch Time 355.4342(352.9174), Bit/dim 4.5272(best: 4.4062), Xent 1.9018, Loss 5.4781, Error 0.6696(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 53.6025(52.3005) | Bit/dim 4.5283(4.5441) | Xent 1.9393(1.9298) | Loss 5.4980(5.5089) | Error 0.6876(0.6797) Steps 532(528.76) | Grad Norm 2.9120(5.5494) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 54.0818(52.3540) | Bit/dim 4.5056(4.5429) | Xent 1.8929(1.9287) | Loss 5.4521(5.5072) | Error 0.6714(0.6795) Steps 544(529.22) | Grad Norm 2.6800(5.4633) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 54.4425(52.4166) | Bit/dim 4.5228(4.5423) | Xent 1.8957(1.9277) | Loss 5.4707(5.5061) | Error 0.6763(0.6794) Steps 550(529.84) | Grad Norm 2.9763(5.3887) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 52.6344(52.4231) | Bit/dim 4.5022(4.5411) | Xent 1.8845(1.9264) | Loss 5.4444(5.5043) | Error 0.6685(0.6790) Steps 544(530.26) | Grad Norm 2.3834(5.2986) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 54.0044(52.4706) | Bit/dim 4.4844(4.5394) | Xent 1.8797(1.9250) | Loss 5.4243(5.5019) | Error 0.6671(0.6787) Steps 538(530.50) | Grad Norm 2.9722(5.2288) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 54.5384(52.5326) | Bit/dim 4.4692(4.5373) | Xent 1.8661(1.9232) | Loss 5.4023(5.4989) | Error 0.6556(0.6780) Steps 538(530.72) | Grad Norm 1.8171(5.1264) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 24.5446, Epoch Time 363.4119(353.2323), Bit/dim 4.4785(best: 4.4062), Xent 1.8264, Loss 5.3917, Error 0.6278(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 52.8641(52.5426) | Bit/dim 4.4884(4.5358) | Xent 1.8517(1.9211) | Loss 5.4142(5.4964) | Error 0.6472(0.6771) Steps 538(530.94) | Grad Norm 2.8515(5.0582) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 52.6417(52.5455) | Bit/dim 4.4556(4.5334) | Xent 1.8533(1.9190) | Loss 5.3822(5.4929) | Error 0.6467(0.6762) Steps 532(530.97) | Grad Norm 2.0657(4.9684) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 56.7359(52.6712) | Bit/dim 4.4439(4.5307) | Xent 1.8270(1.9163) | Loss 5.3574(5.4889) | Error 0.6401(0.6751) Steps 544(531.36) | Grad Norm 3.2029(4.9154) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 55.1872(52.7467) | Bit/dim 4.4477(4.5282) | Xent 1.8488(1.9143) | Loss 5.3720(5.4854) | Error 0.6518(0.6744) Steps 538(531.56) | Grad Norm 3.7902(4.8817) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 54.9135(52.8117) | Bit/dim 4.4393(4.5256) | Xent 1.8074(1.9110) | Loss 5.3430(5.4811) | Error 0.6376(0.6733) Steps 538(531.75) | Grad Norm 2.0272(4.7960) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 54.9856(52.8769) | Bit/dim 4.4286(4.5227) | Xent 1.8468(1.9091) | Loss 5.3520(5.4772) | Error 0.6509(0.6726) Steps 538(531.94) | Grad Norm 3.4408(4.7554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 23.9124, Epoch Time 366.8658(353.6413), Bit/dim 4.4401(best: 4.4062), Xent 1.8138, Loss 5.3470, Error 0.6245(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 52.5279(52.8665) | Bit/dim 4.4399(4.5202) | Xent 1.8759(1.9081) | Loss 5.3779(5.4742) | Error 0.6502(0.6719) Steps 532(531.94) | Grad Norm 5.4235(4.7754) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 51.8380(52.8356) | Bit/dim 4.4128(4.5170) | Xent 1.8400(1.9061) | Loss 5.3328(5.4700) | Error 0.6491(0.6713) Steps 526(531.77) | Grad Norm 4.5065(4.7674) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 52.2787(52.8189) | Bit/dim 4.4039(4.5136) | Xent 1.8182(1.9034) | Loss 5.3130(5.4653) | Error 0.6258(0.6699) Steps 526(531.59) | Grad Norm 2.3390(4.6945) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 51.9815(52.7938) | Bit/dim 4.3957(4.5100) | Xent 1.8104(1.9007) | Loss 5.3009(5.4604) | Error 0.6318(0.6687) Steps 520(531.24) | Grad Norm 2.1391(4.6178) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 53.3318(52.8099) | Bit/dim 4.3908(4.5064) | Xent 1.8330(1.8986) | Loss 5.3073(5.4558) | Error 0.6459(0.6681) Steps 526(531.09) | Grad Norm 4.3218(4.6090) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 51.7209(52.7773) | Bit/dim 4.3905(4.5030) | Xent 1.8201(1.8963) | Loss 5.3005(5.4511) | Error 0.6454(0.6674) Steps 532(531.11) | Grad Norm 5.3023(4.6298) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 24.3201, Epoch Time 353.5490(353.6385), Bit/dim 4.3976(best: 4.4062), Xent 1.8021, Loss 5.2986, Error 0.6338(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 52.6006(52.7720) | Bit/dim 4.4087(4.5001) | Xent 1.8419(1.8946) | Loss 5.3296(5.4475) | Error 0.6441(0.6667) Steps 532(531.14) | Grad Norm 6.6695(4.6910) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 54.7534(52.8314) | Bit/dim 4.4068(4.4973) | Xent 1.8293(1.8927) | Loss 5.3214(5.4437) | Error 0.6424(0.6660) Steps 538(531.35) | Grad Norm 7.5340(4.7762) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 53.4717(52.8506) | Bit/dim 4.3874(4.4940) | Xent 1.8252(1.8906) | Loss 5.3000(5.4394) | Error 0.6366(0.6651) Steps 538(531.55) | Grad Norm 6.2180(4.8195) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 51.9369(52.8232) | Bit/dim 4.3747(4.4905) | Xent 1.8078(1.8882) | Loss 5.2786(5.4345) | Error 0.6404(0.6643) Steps 532(531.56) | Grad Norm 2.5475(4.7513) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 52.1536(52.8031) | Bit/dim 4.3618(4.4866) | Xent 1.7838(1.8850) | Loss 5.2537(5.4291) | Error 0.6295(0.6633) Steps 538(531.75) | Grad Norm 3.1879(4.7044) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 54.8055(52.8632) | Bit/dim 4.3718(4.4832) | Xent 1.8031(1.8826) | Loss 5.2733(5.4244) | Error 0.6371(0.6625) Steps 544(532.12) | Grad Norm 5.3828(4.7248) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 24.0140, Epoch Time 359.6386(353.8185), Bit/dim 4.3978(best: 4.3976), Xent 1.7680, Loss 5.2818, Error 0.6113(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 52.7499(52.8598) | Bit/dim 4.3949(4.4805) | Xent 1.7890(1.8798) | Loss 5.2894(5.4204) | Error 0.6276(0.6615) Steps 538(532.30) | Grad Norm 4.6317(4.7220) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 54.6634(52.9139) | Bit/dim 4.3589(4.4769) | Xent 1.7887(1.8770) | Loss 5.2533(5.4154) | Error 0.6345(0.6606) Steps 544(532.65) | Grad Norm 3.6240(4.6891) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 52.5902(52.9042) | Bit/dim 4.3805(4.4740) | Xent 1.7887(1.8744) | Loss 5.2749(5.4112) | Error 0.6367(0.6599) Steps 538(532.81) | Grad Norm 6.6402(4.7476) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 53.9365(52.9352) | Bit/dim 4.3834(4.4713) | Xent 1.8307(1.8731) | Loss 5.2988(5.4078) | Error 0.6464(0.6595) Steps 538(532.96) | Grad Norm 7.3105(4.8245) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 53.5410(52.9533) | Bit/dim 4.3766(4.4684) | Xent 1.8072(1.8711) | Loss 5.2802(5.4040) | Error 0.6379(0.6589) Steps 538(533.12) | Grad Norm 5.8793(4.8561) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 52.8769(52.9510) | Bit/dim 4.3833(4.4659) | Xent 1.7774(1.8683) | Loss 5.2720(5.4000) | Error 0.6320(0.6581) Steps 538(533.26) | Grad Norm 7.3277(4.9303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 23.9959, Epoch Time 360.1964(354.0098), Bit/dim 4.4710(best: 4.3976), Xent 1.9228, Loss 5.4324, Error 0.6930(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 57.0797(53.0749) | Bit/dim 4.4700(4.4660) | Xent 1.9411(1.8705) | Loss 5.4406(5.4012) | Error 0.6934(0.6591) Steps 544(533.58) | Grad Norm 13.4201(5.1850) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 49.9860(52.9822) | Bit/dim 4.5932(4.4698) | Xent 1.8969(1.8713) | Loss 5.5417(5.4054) | Error 0.6747(0.6596) Steps 526(533.36) | Grad Norm 11.0416(5.3607) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 51.7149(52.9442) | Bit/dim 4.5512(4.4722) | Xent 1.8271(1.8699) | Loss 5.4647(5.4072) | Error 0.6564(0.6595) Steps 532(533.32) | Grad Norm 5.7653(5.3728) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 53.5975(52.9638) | Bit/dim 4.4462(4.4715) | Xent 1.8524(1.8694) | Loss 5.3724(5.4062) | Error 0.6593(0.6595) Steps 538(533.46) | Grad Norm 6.6651(5.4116) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 52.5359(52.9510) | Bit/dim 4.5267(4.4731) | Xent 1.9129(1.8707) | Loss 5.4831(5.4085) | Error 0.6785(0.6601) Steps 532(533.41) | Grad Norm 8.3166(5.4987) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 51.5547(52.9091) | Bit/dim 4.4791(4.4733) | Xent 1.8502(1.8701) | Loss 5.4042(5.4083) | Error 0.6555(0.6599) Steps 532(533.37) | Grad Norm 3.8165(5.4483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 24.4084, Epoch Time 356.8703(354.0957), Bit/dim 4.4668(best: 4.3976), Xent 1.8557, Loss 5.3947, Error 0.6572(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 55.2579(52.9795) | Bit/dim 4.4623(4.4730) | Xent 1.8717(1.8701) | Loss 5.3982(5.4080) | Error 0.6554(0.6598) Steps 544(533.69) | Grad Norm 7.5794(5.5122) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 54.0238(53.0109) | Bit/dim 4.4214(4.4714) | Xent 1.8522(1.8696) | Loss 5.3475(5.4062) | Error 0.6549(0.6596) Steps 538(533.82) | Grad Norm 4.8741(5.4930) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 52.4145(52.9930) | Bit/dim 4.4475(4.4707) | Xent 1.8882(1.8702) | Loss 5.3916(5.4058) | Error 0.6769(0.6602) Steps 532(533.76) | Grad Norm 4.7473(5.4707) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 54.4906(53.0379) | Bit/dim 4.4275(4.4694) | Xent 1.8109(1.8684) | Loss 5.3329(5.4036) | Error 0.6384(0.6595) Steps 538(533.89) | Grad Norm 4.1862(5.4321) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 53.1915(53.0425) | Bit/dim 4.4282(4.4682) | Xent 1.8309(1.8673) | Loss 5.3437(5.4018) | Error 0.6520(0.6593) Steps 532(533.83) | Grad Norm 3.6880(5.3798) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 54.3090(53.0805) | Bit/dim 4.4241(4.4669) | Xent 1.8251(1.8660) | Loss 5.3367(5.3999) | Error 0.6501(0.6590) Steps 538(533.96) | Grad Norm 3.3432(5.3187) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 23.9245, Epoch Time 363.3775(354.3741), Bit/dim 4.3907(best: 4.3976), Xent 1.7561, Loss 5.2688, Error 0.6130(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 54.6987(53.1291) | Bit/dim 4.3980(4.4648) | Xent 1.7873(1.8636) | Loss 5.2916(5.3966) | Error 0.6278(0.6581) Steps 538(534.08) | Grad Norm 2.2172(5.2257) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 54.6358(53.1743) | Bit/dim 4.3899(4.4625) | Xent 1.7928(1.8615) | Loss 5.2863(5.3933) | Error 0.6321(0.6573) Steps 538(534.20) | Grad Norm 3.0499(5.1604) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 53.4403(53.1822) | Bit/dim 4.3859(4.4602) | Xent 1.7824(1.8591) | Loss 5.2770(5.3898) | Error 0.6270(0.6564) Steps 538(534.31) | Grad Norm 2.4579(5.0793) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 54.6070(53.2250) | Bit/dim 4.3865(4.4580) | Xent 1.7482(1.8558) | Loss 5.2606(5.3859) | Error 0.6209(0.6553) Steps 544(534.60) | Grad Norm 2.2929(4.9957) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 53.6991(53.2392) | Bit/dim 4.3737(4.4555) | Xent 1.7618(1.8530) | Loss 5.2546(5.3820) | Error 0.6204(0.6543) Steps 544(534.88) | Grad Norm 3.7954(4.9597) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 55.2033(53.2981) | Bit/dim 4.3738(4.4530) | Xent 1.7372(1.8495) | Loss 5.2424(5.3778) | Error 0.6079(0.6529) Steps 544(535.16) | Grad Norm 3.4516(4.9145) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 24.2683, Epoch Time 366.4326(354.7359), Bit/dim 4.3648(best: 4.3907), Xent 1.7229, Loss 5.2263, Error 0.6056(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 54.2735(53.3274) | Bit/dim 4.3611(4.4503) | Xent 1.7661(1.8470) | Loss 5.2441(5.3738) | Error 0.6282(0.6521) Steps 514(534.52) | Grad Norm 3.8094(4.8813) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 51.6321(53.2765) | Bit/dim 4.3563(4.4475) | Xent 1.7618(1.8445) | Loss 5.2372(5.3697) | Error 0.6165(0.6511) Steps 532(534.45) | Grad Norm 4.2735(4.8631) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 51.6637(53.2281) | Bit/dim 4.3642(4.4450) | Xent 1.7553(1.8418) | Loss 5.2419(5.3659) | Error 0.6262(0.6503) Steps 526(534.19) | Grad Norm 5.8753(4.8935) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 54.3335(53.2613) | Bit/dim 4.3502(4.4421) | Xent 1.7623(1.8394) | Loss 5.2314(5.3618) | Error 0.6221(0.6495) Steps 532(534.13) | Grad Norm 7.6302(4.9756) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 52.3503(53.2340) | Bit/dim 4.3638(4.4398) | Xent 1.7973(1.8381) | Loss 5.2624(5.3588) | Error 0.6395(0.6492) Steps 532(534.06) | Grad Norm 8.6990(5.0873) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 52.3852(53.2085) | Bit/dim 4.3646(4.4375) | Xent 1.8026(1.8371) | Loss 5.2659(5.3561) | Error 0.6351(0.6488) Steps 532(534.00) | Grad Norm 8.0196(5.1752) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 24.0755, Epoch Time 356.4571(354.7875), Bit/dim 4.3440(best: 4.3648), Xent 1.7197, Loss 5.2038, Error 0.6155(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 53.1921(53.2080) | Bit/dim 4.3394(4.4346) | Xent 1.7704(1.8351) | Loss 5.2246(5.3521) | Error 0.6298(0.6482) Steps 538(534.12) | Grad Norm 6.8919(5.2267) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 52.0551(53.1734) | Bit/dim 4.3938(4.4334) | Xent 1.8806(1.8364) | Loss 5.3340(5.3516) | Error 0.6676(0.6488) Steps 538(534.24) | Grad Norm 9.0112(5.3403) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 55.4962(53.2431) | Bit/dim 4.3691(4.4314) | Xent 1.7729(1.8345) | Loss 5.2555(5.3487) | Error 0.6281(0.6481) Steps 544(534.53) | Grad Norm 6.0402(5.3613) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 53.2959(53.2447) | Bit/dim 4.3921(4.4302) | Xent 1.7708(1.8326) | Loss 5.2775(5.3466) | Error 0.6286(0.6476) Steps 544(534.82) | Grad Norm 4.8803(5.3468) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 53.4136(53.2498) | Bit/dim 4.3294(4.4272) | Xent 1.7614(1.8305) | Loss 5.2101(5.3425) | Error 0.6220(0.6468) Steps 538(534.91) | Grad Norm 4.2254(5.3132) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 54.0276(53.2731) | Bit/dim 4.3445(4.4247) | Xent 1.7719(1.8287) | Loss 5.2305(5.3391) | Error 0.6224(0.6461) Steps 550(535.36) | Grad Norm 6.0855(5.3364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 24.4123, Epoch Time 361.3761(354.9852), Bit/dim 4.4067(best: 4.3440), Xent 1.7247, Loss 5.2690, Error 0.5978(best: 0.6010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 51.2917(53.2137) | Bit/dim 4.4062(4.4242) | Xent 1.7468(1.8263) | Loss 5.2796(5.3373) | Error 0.6130(0.6451) Steps 532(535.26) | Grad Norm 5.4979(5.3412) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 53.4125(53.2196) | Bit/dim 4.3569(4.4222) | Xent 1.7394(1.8237) | Loss 5.2266(5.3340) | Error 0.6171(0.6442) Steps 538(535.35) | Grad Norm 6.4290(5.3738) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 54.6362(53.2621) | Bit/dim 4.3594(4.4203) | Xent 1.7932(1.8227) | Loss 5.2560(5.3317) | Error 0.6260(0.6437) Steps 544(535.60) | Grad Norm 9.0271(5.4834) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 54.3136(53.2937) | Bit/dim 4.3811(4.4191) | Xent 1.7265(1.8199) | Loss 5.2443(5.3290) | Error 0.6071(0.6426) Steps 550(536.04) | Grad Norm 4.6153(5.4574) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 53.0977(53.2878) | Bit/dim 4.3698(4.4176) | Xent 1.7533(1.8179) | Loss 5.2464(5.3266) | Error 0.6206(0.6419) Steps 538(536.10) | Grad Norm 8.3188(5.5432) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 55.2344(53.3462) | Bit/dim 4.3655(4.4161) | Xent 1.8092(1.8176) | Loss 5.2701(5.3249) | Error 0.6381(0.6418) Steps 550(536.51) | Grad Norm 7.3032(5.5960) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 24.3014, Epoch Time 362.0792(355.1980), Bit/dim 4.3417(best: 4.3440), Xent 1.7584, Loss 5.2209, Error 0.6190(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 52.2419(53.3131) | Bit/dim 4.3365(4.4137) | Xent 1.7965(1.8170) | Loss 5.2347(5.3222) | Error 0.6374(0.6417) Steps 538(536.56) | Grad Norm 6.9580(5.6369) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 54.3399(53.3439) | Bit/dim 4.3740(4.4125) | Xent 1.8429(1.8177) | Loss 5.2954(5.3214) | Error 0.6504(0.6419) Steps 538(536.60) | Grad Norm 10.0689(5.7699) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 52.5584(53.3203) | Bit/dim 4.4357(4.4132) | Xent 1.9270(1.8210) | Loss 5.3992(5.3237) | Error 0.6861(0.6433) Steps 538(536.64) | Grad Norm 12.1873(5.9624) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 51.3163(53.2602) | Bit/dim 4.4450(4.4141) | Xent 1.8780(1.8227) | Loss 5.3840(5.3255) | Error 0.6700(0.6441) Steps 538(536.68) | Grad Norm 8.4093(6.0358) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 55.6462(53.3318) | Bit/dim 4.3503(4.4122) | Xent 1.8791(1.8244) | Loss 5.2899(5.3244) | Error 0.6689(0.6448) Steps 556(537.26) | Grad Norm 4.1457(5.9791) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 53.2824(53.3303) | Bit/dim 4.4242(4.4126) | Xent 1.8440(1.8250) | Loss 5.3462(5.3251) | Error 0.6539(0.6451) Steps 544(537.46) | Grad Norm 6.3827(5.9912) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 24.3214, Epoch Time 359.4330(355.3250), Bit/dim 4.4083(best: 4.3417), Xent 1.7925, Loss 5.3046, Error 0.6119(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 50.6616(53.2502) | Bit/dim 4.4101(4.4125) | Xent 1.8141(1.8247) | Loss 5.3171(5.3248) | Error 0.6390(0.6449) Steps 532(537.30) | Grad Norm 3.9532(5.9301) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 52.1377(53.2168) | Bit/dim 4.3764(4.4114) | Xent 1.8411(1.8252) | Loss 5.2970(5.3240) | Error 0.6565(0.6453) Steps 532(537.14) | Grad Norm 5.0551(5.9038) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 53.9500(53.2388) | Bit/dim 4.3584(4.4098) | Xent 1.8358(1.8255) | Loss 5.2763(5.3226) | Error 0.6395(0.6451) Steps 532(536.99) | Grad Norm 4.7039(5.8678) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 52.7551(53.2243) | Bit/dim 4.3636(4.4084) | Xent 1.8208(1.8253) | Loss 5.2740(5.3211) | Error 0.6427(0.6450) Steps 544(537.20) | Grad Norm 3.6417(5.8010) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 52.2051(53.1937) | Bit/dim 4.3550(4.4068) | Xent 1.7780(1.8239) | Loss 5.2440(5.3188) | Error 0.6265(0.6445) Steps 544(537.40) | Grad Norm 2.9446(5.7153) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 56.7190(53.2995) | Bit/dim 4.3425(4.4049) | Xent 1.7623(1.8221) | Loss 5.2237(5.3160) | Error 0.6231(0.6438) Steps 556(537.96) | Grad Norm 3.9221(5.6615) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 25.0390, Epoch Time 358.9375(355.4334), Bit/dim 4.3538(best: 4.3417), Xent 1.7299, Loss 5.2187, Error 0.6085(best: 0.5978)\n",
      "Iter 0680 | Time 55.6082(53.4302) | Bit/dim 4.3299(4.4012) | Xent 1.7287(1.8178) | Loss 5.1942(5.3101) | Error 0.6196(0.6427) Steps 556(538.85) | Grad Norm 3.2837(5.5534) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 56.1084(53.5105) | Bit/dim 4.3476(4.3996) | Xent 1.7272(1.8151) | Loss 5.2112(5.3072) | Error 0.6054(0.6416) Steps 544(539.01) | Grad Norm 3.8454(5.5022) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 55.3826(53.5667) | Bit/dim 4.3473(4.3981) | Xent 1.7365(1.8127) | Loss 5.2156(5.3044) | Error 0.6150(0.6408) Steps 550(539.34) | Grad Norm 3.7701(5.4502) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 55.0391(53.6109) | Bit/dim 4.3469(4.3965) | Xent 1.7746(1.8116) | Loss 5.2342(5.3023) | Error 0.6338(0.6406) Steps 550(539.66) | Grad Norm 6.9360(5.4948) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 55.1468(53.6569) | Bit/dim 4.3728(4.3958) | Xent 1.7382(1.8094) | Loss 5.2419(5.3005) | Error 0.6128(0.6398) Steps 544(539.79) | Grad Norm 8.6748(5.5902) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 25.5077, Epoch Time 373.8858(355.9870), Bit/dim 4.3816(best: 4.3417), Xent 1.8168, Loss 5.2900, Error 0.6501(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 55.9455(53.7256) | Bit/dim 4.3738(4.3952) | Xent 1.8513(1.8106) | Loss 5.2994(5.3005) | Error 0.6547(0.6402) Steps 550(540.09) | Grad Norm 12.7159(5.8040) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 53.4211(53.7165) | Bit/dim 4.5319(4.3993) | Xent 2.1116(1.8197) | Loss 5.5877(5.3091) | Error 0.7203(0.6426) Steps 532(539.85) | Grad Norm 24.3629(6.3607) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 52.8925(53.6917) | Bit/dim 4.6913(4.4080) | Xent 2.0633(1.8270) | Loss 5.7229(5.3215) | Error 0.7060(0.6445) Steps 562(540.51) | Grad Norm 9.3578(6.4507) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 56.5535(53.7776) | Bit/dim 4.6491(4.4152) | Xent 2.1400(1.8364) | Loss 5.7190(5.3334) | Error 0.7599(0.6480) Steps 580(541.70) | Grad Norm 11.9741(6.6164) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 54.9221(53.8119) | Bit/dim 4.4925(4.4176) | Xent 1.9125(1.8386) | Loss 5.4487(5.3369) | Error 0.6724(0.6487) Steps 568(542.49) | Grad Norm 3.8717(6.5340) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 54.0522(53.8191) | Bit/dim 4.4985(4.4200) | Xent 1.9633(1.8424) | Loss 5.4801(5.3412) | Error 0.6854(0.6498) Steps 556(542.89) | Grad Norm 4.9156(6.4855) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 24.8878, Epoch Time 368.5392(356.3635), Bit/dim 4.5251(best: 4.3417), Xent 1.8408, Loss 5.4456, Error 0.6551(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 51.6262(53.7533) | Bit/dim 4.5237(4.4231) | Xent 1.8827(1.8436) | Loss 5.4651(5.3449) | Error 0.6656(0.6503) Steps 550(543.11) | Grad Norm 3.2327(6.3879) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 56.2584(53.8285) | Bit/dim 4.4956(4.4253) | Xent 1.8874(1.8449) | Loss 5.4394(5.3477) | Error 0.6754(0.6510) Steps 568(543.85) | Grad Norm 2.8999(6.2832) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 54.1121(53.8370) | Bit/dim 4.4659(4.4265) | Xent 1.8962(1.8465) | Loss 5.4141(5.3497) | Error 0.6783(0.6519) Steps 562(544.40) | Grad Norm 2.9323(6.1827) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 57.6621(53.9867) | Bit/dim 4.4270(4.4263) | Xent 1.8369(1.8474) | Loss 5.3455(5.3500) | Error 0.6531(0.6525) Steps 610(547.72) | Grad Norm 2.3775(5.8667) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 26.3310, Epoch Time 370.4110(356.7850), Bit/dim 4.4184(best: 4.3417), Xent 1.7871, Loss 5.3119, Error 0.6260(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 58.9038(54.1342) | Bit/dim 4.4170(4.4260) | Xent 1.8176(1.8465) | Loss 5.3258(5.3492) | Error 0.6458(0.6523) Steps 586(548.87) | Grad Norm 2.4898(5.7654) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 57.2328(54.2272) | Bit/dim 4.4114(4.4255) | Xent 1.8246(1.8458) | Loss 5.3237(5.3485) | Error 0.6494(0.6522) Steps 580(549.81) | Grad Norm 2.3709(5.6635) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 57.3404(54.3206) | Bit/dim 4.3893(4.4245) | Xent 1.8168(1.8450) | Loss 5.2977(5.3469) | Error 0.6472(0.6521) Steps 598(551.25) | Grad Norm 2.6831(5.5741) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 56.4156(54.3834) | Bit/dim 4.3852(4.4233) | Xent 1.7894(1.8433) | Loss 5.2799(5.3449) | Error 0.6390(0.6517) Steps 598(552.65) | Grad Norm 3.0236(5.4976) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 55.6630(54.4218) | Bit/dim 4.3805(4.4220) | Xent 1.8025(1.8421) | Loss 5.2818(5.3430) | Error 0.6440(0.6514) Steps 598(554.01) | Grad Norm 2.8013(5.4167) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 56.7113(54.4905) | Bit/dim 4.3580(4.4201) | Xent 1.7989(1.8408) | Loss 5.2574(5.3405) | Error 0.6376(0.6510) Steps 592(555.15) | Grad Norm 3.3582(5.3550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 24.8586, Epoch Time 382.9733(357.5706), Bit/dim 4.3562(best: 4.3417), Xent 1.7364, Loss 5.2244, Error 0.6077(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 53.4371(54.4589) | Bit/dim 4.3487(4.4179) | Xent 1.7800(1.8390) | Loss 5.2387(5.3374) | Error 0.6351(0.6505) Steps 562(555.36) | Grad Norm 3.6438(5.3036) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 55.0017(54.4752) | Bit/dim 4.3333(4.4154) | Xent 1.7919(1.8375) | Loss 5.2293(5.3342) | Error 0.6294(0.6499) Steps 550(555.20) | Grad Norm 4.2743(5.2728) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 53.2343(54.4380) | Bit/dim 4.3458(4.4133) | Xent 1.8265(1.8372) | Loss 5.2590(5.3319) | Error 0.6554(0.6501) Steps 544(554.86) | Grad Norm 6.1974(5.3005) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 53.2402(54.4020) | Bit/dim 4.4570(4.4146) | Xent 1.8870(1.8387) | Loss 5.4005(5.3340) | Error 0.6769(0.6509) Steps 550(554.72) | Grad Norm 14.4640(5.5754) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 52.2928(54.3387) | Bit/dim 4.7311(4.4241) | Xent 2.1585(1.8483) | Loss 5.8104(5.3483) | Error 0.7165(0.6528) Steps 556(554.76) | Grad Norm 12.9230(5.7958) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 54.9960(54.3585) | Bit/dim 4.6314(4.4303) | Xent 1.9896(1.8525) | Loss 5.6262(5.3566) | Error 0.7157(0.6547) Steps 532(554.07) | Grad Norm 8.0324(5.8629) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 25.1284, Epoch Time 362.8412(357.7287), Bit/dim 4.5734(best: 4.3417), Xent 1.9636, Loss 5.5553, Error 0.7001(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 54.4889(54.3624) | Bit/dim 4.5762(4.4347) | Xent 2.0408(1.8582) | Loss 5.5966(5.3638) | Error 0.7208(0.6567) Steps 562(554.31) | Grad Norm 6.7493(5.8895) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 51.2305(54.2684) | Bit/dim 4.5803(4.4391) | Xent 1.9635(1.8613) | Loss 5.5621(5.3698) | Error 0.6961(0.6579) Steps 520(553.28) | Grad Norm 5.4168(5.8753) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 52.3856(54.2119) | Bit/dim 4.5366(4.4420) | Xent 1.9431(1.8638) | Loss 5.5082(5.3739) | Error 0.6850(0.6587) Steps 508(551.92) | Grad Norm 2.8112(5.7834) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 52.8977(54.1725) | Bit/dim 4.5416(4.4450) | Xent 1.9454(1.8662) | Loss 5.5143(5.3781) | Error 0.6952(0.6598) Steps 538(551.51) | Grad Norm 3.8336(5.7249) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 52.2757(54.1156) | Bit/dim 4.4952(4.4465) | Xent 1.9290(1.8681) | Loss 5.4597(5.3806) | Error 0.6959(0.6609) Steps 526(550.74) | Grad Norm 3.1445(5.6475) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 50.6635(54.0120) | Bit/dim 4.4763(4.4474) | Xent 1.9271(1.8699) | Loss 5.4398(5.3823) | Error 0.6818(0.6615) Steps 502(549.28) | Grad Norm 3.3672(5.5791) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 23.7695, Epoch Time 353.2889(357.5955), Bit/dim 4.4823(best: 4.3417), Xent 1.8652, Loss 5.4149, Error 0.6590(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 51.6813(53.9421) | Bit/dim 4.4766(4.4483) | Xent 1.9044(1.8709) | Loss 5.4288(5.3837) | Error 0.6715(0.6618) Steps 526(548.58) | Grad Norm 3.4189(5.5143) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 53.7403(53.9361) | Bit/dim 4.4743(4.4490) | Xent 1.8795(1.8712) | Loss 5.4140(5.3846) | Error 0.6681(0.6620) Steps 520(547.72) | Grad Norm 3.1086(5.4421) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 53.7453(53.9303) | Bit/dim 4.4526(4.4492) | Xent 1.8597(1.8708) | Loss 5.3825(5.3846) | Error 0.6574(0.6619) Steps 532(547.25) | Grad Norm 2.7238(5.3606) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 53.6784(53.9228) | Bit/dim 4.4384(4.4488) | Xent 1.8351(1.8698) | Loss 5.3560(5.3837) | Error 0.6511(0.6615) Steps 532(546.79) | Grad Norm 2.2109(5.2661) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 54.3887(53.9368) | Bit/dim 4.4385(4.4485) | Xent 1.8510(1.8692) | Loss 5.3640(5.3831) | Error 0.6554(0.6614) Steps 538(546.53) | Grad Norm 2.9934(5.1979) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 52.9911(53.9084) | Bit/dim 4.4116(4.4474) | Xent 1.8501(1.8686) | Loss 5.3366(5.3817) | Error 0.6545(0.6611) Steps 514(545.55) | Grad Norm 2.2103(5.1083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 23.6644, Epoch Time 359.3546(357.6483), Bit/dim 4.3999(best: 4.3417), Xent 1.7872, Loss 5.2935, Error 0.6241(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 52.7000(53.8721) | Bit/dim 4.4086(4.4463) | Xent 1.8197(1.8672) | Loss 5.3184(5.3798) | Error 0.6466(0.6607) Steps 514(544.61) | Grad Norm 2.1940(5.0208) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 58.8539(54.0216) | Bit/dim 4.3878(4.4445) | Xent 1.8282(1.8660) | Loss 5.3019(5.3775) | Error 0.6494(0.6604) Steps 550(544.77) | Grad Norm 2.0864(4.9328) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 58.2538(54.1486) | Bit/dim 4.3689(4.4422) | Xent 1.8011(1.8640) | Loss 5.2694(5.3743) | Error 0.6371(0.6597) Steps 550(544.93) | Grad Norm 1.6780(4.8352) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 57.1697(54.2392) | Bit/dim 4.3587(4.4397) | Xent 1.8282(1.8630) | Loss 5.2728(5.3712) | Error 0.6435(0.6592) Steps 532(544.54) | Grad Norm 1.8428(4.7454) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 57.8483(54.3475) | Bit/dim 4.3502(4.4370) | Xent 1.8214(1.8617) | Loss 5.2609(5.3679) | Error 0.6449(0.6588) Steps 532(544.16) | Grad Norm 1.7130(4.6544) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn500_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 500 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
