{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.75, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_75_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=4608, bias=True)\n",
      "  (project_class): LinearZeros(in_features=2304, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1441846\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.2681(30.1294) | Bit/dim 9.0584(9.2176) | Xent 2.3026(2.3026) | Loss 9.0584(9.2176) | Error 0.8944(0.8964) Steps 574(574.00) | Grad Norm 13.3610(16.8282) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.4757(25.6899) | Bit/dim 8.5611(9.0922) | Xent 2.3026(2.3026) | Loss 8.5611(9.0922) | Error 0.9067(0.8980) Steps 574(574.00) | Grad Norm 4.6660(14.4920) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 13.3304(22.4499) | Bit/dim 8.4222(8.9243) | Xent 2.3026(2.3026) | Loss 8.4222(8.9243) | Error 0.9111(0.8972) Steps 574(574.00) | Grad Norm 3.4669(11.6665) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.2556(20.0915) | Bit/dim 8.2403(8.7600) | Xent 2.3026(2.3026) | Loss 8.2403(8.7600) | Error 0.8944(0.8958) Steps 574(574.00) | Grad Norm 2.9641(9.4145) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.1883(18.3008) | Bit/dim 7.9882(8.5785) | Xent 2.3026(2.3026) | Loss 7.9882(8.5785) | Error 0.9011(0.8989) Steps 574(574.00) | Grad Norm 2.9950(7.6810) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 81.4122, Epoch Time 858.6646(858.6646), Bit/dim 7.7829(best: inf), Xent 2.3026, Loss 7.7829, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.3857(17.0015) | Bit/dim 7.6750(8.3722) | Xent 2.3026(2.3026) | Loss 7.6750(8.3722) | Error 0.9167(0.9015) Steps 574(574.00) | Grad Norm 2.6505(6.3576) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.3363(16.0344) | Bit/dim 7.3526(8.1395) | Xent 2.3026(2.3026) | Loss 7.3526(8.1395) | Error 0.8844(0.8997) Steps 574(574.00) | Grad Norm 2.0638(5.2840) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 13.0827(15.3139) | Bit/dim 7.1664(7.9022) | Xent 2.3026(2.3026) | Loss 7.1664(7.9022) | Error 0.9022(0.9014) Steps 574(574.00) | Grad Norm 1.4108(4.3294) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.7945(14.9614) | Bit/dim 7.0841(7.6925) | Xent 2.3026(2.3026) | Loss 7.0841(7.6925) | Error 0.9067(0.9007) Steps 598(578.06) | Grad Norm 1.0035(3.4854) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 14.9289(15.0194) | Bit/dim 7.0169(7.5212) | Xent 2.3026(2.3026) | Loss 7.0169(7.5212) | Error 0.8978(0.9003) Steps 610(586.17) | Grad Norm 1.1870(2.7942) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.7337(15.1193) | Bit/dim 6.9537(7.3828) | Xent 2.3026(2.3026) | Loss 6.9537(7.3828) | Error 0.8989(0.8995) Steps 622(593.76) | Grad Norm 0.4560(2.2517) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 81.9332, Epoch Time 878.2065(859.2509), Bit/dim 6.9743(best: 7.7829), Xent 2.3026, Loss 6.9743, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 15.9667(15.3437) | Bit/dim 6.9422(7.2702) | Xent 2.3026(2.3026) | Loss 6.9422(7.2702) | Error 0.9078(0.9001) Steps 628(602.33) | Grad Norm 0.8449(1.8263) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.4284(15.5034) | Bit/dim 6.8997(7.1759) | Xent 2.3026(2.3026) | Loss 6.8997(7.1759) | Error 0.9144(0.8997) Steps 628(609.07) | Grad Norm 0.3733(1.5085) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 16.4171(15.7639) | Bit/dim 6.8308(7.0926) | Xent 2.3026(2.3026) | Loss 6.8308(7.0926) | Error 0.9033(0.8990) Steps 634(615.19) | Grad Norm 0.5439(1.3216) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.4223(15.9489) | Bit/dim 6.7546(7.0100) | Xent 2.3026(2.3026) | Loss 6.7546(7.0100) | Error 0.8933(0.8991) Steps 640(620.65) | Grad Norm 1.2810(1.1472) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 16.4597(16.1542) | Bit/dim 6.5870(6.9187) | Xent 2.3026(2.3026) | Loss 6.5870(6.9187) | Error 0.9078(0.9003) Steps 640(625.73) | Grad Norm 2.9575(1.2729) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 84.5438, Epoch Time 1003.3224(863.5730), Bit/dim 6.5691(best: 6.9743), Xent 2.3026, Loss 6.5691, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 16.2739(16.2976) | Bit/dim 6.3640(6.8075) | Xent 2.3026(2.3026) | Loss 6.3640(6.8075) | Error 0.8778(0.9002) Steps 640(629.31) | Grad Norm 5.1976(4.8313) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 15.9694(16.3503) | Bit/dim 6.1751(6.6701) | Xent 2.3026(2.3026) | Loss 6.1751(6.6701) | Error 0.8856(0.9002) Steps 640(631.84) | Grad Norm 15.0730(8.2249) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.5932(16.5612) | Bit/dim 5.9894(6.5126) | Xent 2.3026(2.3026) | Loss 5.9894(6.5126) | Error 0.8833(0.8997) Steps 670(638.21) | Grad Norm 5.2564(11.1628) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.7278(16.8689) | Bit/dim 5.8460(6.3535) | Xent 2.3026(2.3026) | Loss 5.8460(6.3535) | Error 0.9078(0.8998) Steps 670(646.56) | Grad Norm 16.6480(12.9241) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.8983(17.0294) | Bit/dim 5.7211(6.1990) | Xent 2.3026(2.3026) | Loss 5.7211(6.1990) | Error 0.8944(0.8998) Steps 658(650.93) | Grad Norm 7.4154(11.2962) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 17.4962(17.1404) | Bit/dim 5.6702(6.0660) | Xent 2.3026(2.3026) | Loss 5.6702(6.0660) | Error 0.8811(0.8998) Steps 670(655.94) | Grad Norm 12.0966(10.4420) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 89.7893, Epoch Time 1052.1322(869.2298), Bit/dim 5.6583(best: 6.5691), Xent 2.3026, Loss 5.6583, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.6073(17.1699) | Bit/dim 5.6192(5.9505) | Xent 2.3026(2.3026) | Loss 5.6192(5.9505) | Error 0.9067(0.9022) Steps 682(661.24) | Grad Norm 14.0466(9.7823) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 16.8114(17.1236) | Bit/dim 5.5866(5.8605) | Xent 2.3026(2.3026) | Loss 5.5866(5.8605) | Error 0.9078(0.9002) Steps 664(662.71) | Grad Norm 3.9118(11.1881) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 15.8946(17.0053) | Bit/dim 5.5533(5.7898) | Xent 2.3026(2.3026) | Loss 5.5533(5.7898) | Error 0.8922(0.9002) Steps 640(660.31) | Grad Norm 5.6143(9.7434) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.8083(16.8965) | Bit/dim 5.5134(5.7332) | Xent 2.3026(2.3026) | Loss 5.5134(5.7332) | Error 0.9044(0.9009) Steps 652(657.36) | Grad Norm 1.3017(7.9817) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 17.6891(17.1479) | Bit/dim 5.5191(5.6831) | Xent 2.3026(2.3026) | Loss 5.5191(5.6831) | Error 0.9133(0.8995) Steps 676(662.09) | Grad Norm 5.8340(7.3171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 88.2307, Epoch Time 1047.9375(874.5910), Bit/dim 5.5216(best: 5.6583), Xent 2.3026, Loss 5.5216, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 17.0223(17.2469) | Bit/dim 5.5379(5.6377) | Xent 2.3026(2.3026) | Loss 5.5379(5.6377) | Error 0.9156(0.8996) Steps 676(666.22) | Grad Norm 9.9184(6.9936) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.1334(17.2495) | Bit/dim 5.5089(5.6003) | Xent 2.3026(2.3026) | Loss 5.5089(5.6003) | Error 0.9044(0.8998) Steps 676(668.49) | Grad Norm 3.4365(6.0061) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 17.8982(17.3029) | Bit/dim 5.4173(5.5615) | Xent 2.3026(2.3026) | Loss 5.4173(5.5615) | Error 0.8922(0.9002) Steps 682(670.17) | Grad Norm 4.5359(6.1984) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.2909(17.4017) | Bit/dim 5.4382(5.5323) | Xent 2.3026(2.3026) | Loss 5.4382(5.5323) | Error 0.8933(0.8984) Steps 676(672.14) | Grad Norm 5.4282(7.4302) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 16.7399(17.3651) | Bit/dim 5.3756(5.4964) | Xent 2.3026(2.3026) | Loss 5.3756(5.4964) | Error 0.8822(0.8984) Steps 664(671.12) | Grad Norm 24.4253(8.4656) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.5282(17.3346) | Bit/dim 5.3454(5.4622) | Xent 2.3026(2.3026) | Loss 5.3454(5.4622) | Error 0.9133(0.9001) Steps 664(670.97) | Grad Norm 10.2956(8.7705) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 87.2837, Epoch Time 1061.3678(880.1943), Bit/dim 5.3103(best: 5.5216), Xent 2.3026, Loss 5.3103, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 17.8407(17.3084) | Bit/dim 5.2716(5.4138) | Xent 2.3026(2.3026) | Loss 5.2716(5.4138) | Error 0.8978(0.8982) Steps 688(673.08) | Grad Norm 15.1883(8.2183) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 16.9344(17.3792) | Bit/dim 5.2629(5.3895) | Xent 2.3026(2.3026) | Loss 5.2629(5.3895) | Error 0.8900(0.8989) Steps 676(676.99) | Grad Norm 9.5360(10.5029) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 16.4704(17.2796) | Bit/dim 5.1691(5.3431) | Xent 2.3026(2.3026) | Loss 5.1691(5.3431) | Error 0.8944(0.8989) Steps 682(679.36) | Grad Norm 3.7500(9.7145) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 17.3556(17.2245) | Bit/dim 5.1204(5.2990) | Xent 2.3026(2.3026) | Loss 5.1204(5.2990) | Error 0.8989(0.8997) Steps 688(681.06) | Grad Norm 3.4187(8.8065) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.2987(17.2742) | Bit/dim 5.0851(5.2527) | Xent 2.3026(2.3026) | Loss 5.0851(5.2527) | Error 0.8978(0.9004) Steps 682(682.99) | Grad Norm 7.6253(7.6111) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 90.1162, Epoch Time 1057.6408(885.5177), Bit/dim 5.1054(best: 5.3103), Xent 2.3026, Loss 5.1054, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 16.3009(17.1977) | Bit/dim 5.0895(5.2132) | Xent 2.3026(2.3026) | Loss 5.0895(5.2132) | Error 0.8967(0.9002) Steps 676(682.17) | Grad Norm 14.5773(7.8225) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.9506(17.0680) | Bit/dim 5.0189(5.1713) | Xent 2.3026(2.3026) | Loss 5.0189(5.1713) | Error 0.9000(0.8994) Steps 670(677.61) | Grad Norm 8.5317(8.3535) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 16.7068(17.0159) | Bit/dim 4.9585(5.1281) | Xent 2.3026(2.3026) | Loss 4.9585(5.1281) | Error 0.9089(0.8994) Steps 670(675.59) | Grad Norm 6.1696(7.7428) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 17.2809(16.9436) | Bit/dim 4.8840(5.0823) | Xent 2.3026(2.3026) | Loss 4.8840(5.0823) | Error 0.8911(0.8993) Steps 670(673.98) | Grad Norm 2.1379(6.9792) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 16.8788(16.8820) | Bit/dim 4.9118(5.0428) | Xent 2.3026(2.3026) | Loss 4.9118(5.0428) | Error 0.9078(0.9008) Steps 670(672.94) | Grad Norm 4.0007(7.1286) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 17.0364(16.8812) | Bit/dim 4.8574(4.9978) | Xent 2.3026(2.3026) | Loss 4.8574(4.9978) | Error 0.9100(0.9012) Steps 670(672.17) | Grad Norm 1.4994(6.3097) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 90.9195, Epoch Time 1031.0994(889.8852), Bit/dim 4.8593(best: 5.1054), Xent 2.3026, Loss 4.8593, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 17.2026(16.8443) | Bit/dim 4.8725(4.9562) | Xent 2.3026(2.3026) | Loss 4.8725(4.9562) | Error 0.9067(0.9011) Steps 670(671.06) | Grad Norm 10.6646(5.8338) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 16.8238(16.8639) | Bit/dim 4.9760(4.9391) | Xent 2.3026(2.3026) | Loss 4.9760(4.9391) | Error 0.8989(0.9011) Steps 664(669.70) | Grad Norm 22.2513(8.4951) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 18.5527(17.0704) | Bit/dim 4.8730(4.9344) | Xent 2.3026(2.3026) | Loss 4.8730(4.9344) | Error 0.9056(0.9022) Steps 682(672.16) | Grad Norm 6.0033(10.1720) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 18.3864(17.4112) | Bit/dim 4.7608(4.8995) | Xent 2.3026(2.3026) | Loss 4.7608(4.8995) | Error 0.9000(0.9017) Steps 712(680.20) | Grad Norm 6.3045(8.8850) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.6336(17.7319) | Bit/dim 4.7367(4.8669) | Xent 2.3026(2.3026) | Loss 4.7367(4.8669) | Error 0.8944(0.9005) Steps 700(685.74) | Grad Norm 2.0250(7.9132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 95.2222, Epoch Time 1088.3778(895.8400), Bit/dim 4.7165(best: 4.8593), Xent 2.3026, Loss 4.7165, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.5141(17.9734) | Bit/dim 4.6892(4.8257) | Xent 2.3026(2.3026) | Loss 4.6892(4.8257) | Error 0.9100(0.8998) Steps 706(691.88) | Grad Norm 2.8820(6.6992) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 17.7071(18.0904) | Bit/dim 4.6483(4.7894) | Xent 2.3026(2.3026) | Loss 4.6483(4.7894) | Error 0.9033(0.8990) Steps 700(695.11) | Grad Norm 4.7820(6.0660) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 17.5386(18.2044) | Bit/dim 5.0925(4.7884) | Xent 2.3026(2.3026) | Loss 5.0925(4.7884) | Error 0.8978(0.8995) Steps 658(695.33) | Grad Norm 25.7702(9.5438) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 17.8889(18.0446) | Bit/dim 4.8007(4.8090) | Xent 2.3026(2.3026) | Loss 4.8007(4.8090) | Error 0.8978(0.8992) Steps 694(693.22) | Grad Norm 11.3776(10.3705) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.1776(18.0848) | Bit/dim 4.6627(4.7841) | Xent 2.3026(2.3026) | Loss 4.6627(4.7841) | Error 0.9156(0.8996) Steps 700(695.26) | Grad Norm 2.3251(9.4154) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.8399(18.2603) | Bit/dim 4.6739(4.7533) | Xent 2.3026(2.3026) | Loss 4.6739(4.7533) | Error 0.9011(0.9003) Steps 718(701.07) | Grad Norm 3.2859(8.0391) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 98.0757, Epoch Time 1124.2046(902.6909), Bit/dim 4.6351(best: 4.7165), Xent 2.3026, Loss 4.6351, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 19.7637(18.4478) | Bit/dim 4.6172(4.7192) | Xent 2.3026(2.3026) | Loss 4.6172(4.7192) | Error 0.8989(0.8998) Steps 730(708.43) | Grad Norm 2.0618(6.5022) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 19.0893(18.6252) | Bit/dim 4.5868(4.6852) | Xent 2.3026(2.3026) | Loss 4.5868(4.6852) | Error 0.9078(0.9003) Steps 718(713.68) | Grad Norm 3.2671(5.5831) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.9533(18.7660) | Bit/dim 4.5732(4.6583) | Xent 2.3026(2.3026) | Loss 4.5732(4.6583) | Error 0.8978(0.8996) Steps 724(717.21) | Grad Norm 8.5035(6.2659) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.0497(18.7941) | Bit/dim 4.5988(4.6357) | Xent 2.3026(2.3026) | Loss 4.5988(4.6357) | Error 0.9000(0.9005) Steps 718(720.32) | Grad Norm 12.1036(6.4937) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 19.4961(18.8880) | Bit/dim 4.5320(4.6130) | Xent 2.3026(2.3026) | Loss 4.5320(4.6130) | Error 0.9067(0.9002) Steps 730(723.46) | Grad Norm 6.9668(6.6716) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 98.6791, Epoch Time 1166.6198(910.6088), Bit/dim 4.5183(best: 4.6351), Xent 2.3026, Loss 4.5183, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 19.6005(19.0044) | Bit/dim 4.5433(4.5898) | Xent 2.3026(2.3026) | Loss 4.5433(4.5898) | Error 0.8944(0.8992) Steps 736(728.49) | Grad Norm 7.5356(6.1208) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 19.3008(19.0632) | Bit/dim 4.4944(4.5730) | Xent 2.3026(2.3026) | Loss 4.4944(4.5730) | Error 0.9133(0.8993) Steps 736(731.32) | Grad Norm 7.5048(6.7764) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 19.5556(19.1155) | Bit/dim 4.5178(4.5573) | Xent 2.3026(2.3026) | Loss 4.5178(4.5573) | Error 0.8989(0.8989) Steps 742(732.51) | Grad Norm 8.9444(7.2306) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 18.7891(19.0551) | Bit/dim 4.4558(4.5384) | Xent 2.3026(2.3026) | Loss 4.4558(4.5384) | Error 0.8933(0.9009) Steps 724(731.76) | Grad Norm 4.3863(7.0951) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 18.5885(18.9392) | Bit/dim 4.4590(4.5246) | Xent 2.3026(2.3026) | Loss 4.4590(4.5246) | Error 0.8867(0.9000) Steps 706(725.77) | Grad Norm 10.0671(8.1249) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.8005(18.8936) | Bit/dim 4.4685(4.5104) | Xent 2.3026(2.3026) | Loss 4.4685(4.5104) | Error 0.9056(0.9004) Steps 724(722.75) | Grad Norm 9.4347(7.9218) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 92.1479, Epoch Time 1153.7428(917.9028), Bit/dim 4.4905(best: 4.5183), Xent 2.3026, Loss 4.4905, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 18.4106(18.6452) | Bit/dim 4.4351(4.4951) | Xent 2.3026(2.3026) | Loss 4.4351(4.4951) | Error 0.8867(0.8996) Steps 712(718.40) | Grad Norm 8.2338(7.3993) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 18.0338(18.5183) | Bit/dim 4.4222(4.4815) | Xent 2.3026(2.3026) | Loss 4.4222(4.4815) | Error 0.8967(0.8989) Steps 706(715.66) | Grad Norm 6.1198(7.9192) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 18.3607(18.4829) | Bit/dim 4.4476(4.4633) | Xent 2.3026(2.3026) | Loss 4.4476(4.4633) | Error 0.9011(0.8999) Steps 724(715.50) | Grad Norm 10.7137(7.3654) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 18.5421(18.4600) | Bit/dim 4.4798(4.4613) | Xent 2.3026(2.3026) | Loss 4.4798(4.4613) | Error 0.8989(0.8997) Steps 718(716.29) | Grad Norm 15.0393(8.7980) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 18.3980(18.4626) | Bit/dim 4.4407(4.4508) | Xent 2.3026(2.3026) | Loss 4.4407(4.4508) | Error 0.9078(0.9004) Steps 700(713.55) | Grad Norm 9.1436(8.2985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 92.8907, Epoch Time 1117.1550(923.8804), Bit/dim 4.3711(best: 4.4905), Xent 2.3026, Loss 4.3711, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 18.5789(18.4425) | Bit/dim 4.3882(4.4353) | Xent 2.3026(2.3026) | Loss 4.3882(4.4353) | Error 0.9033(0.9012) Steps 712(711.58) | Grad Norm 7.7537(7.9469) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 19.7318(18.5582) | Bit/dim 4.3524(4.4167) | Xent 2.3026(2.3026) | Loss 4.3524(4.4167) | Error 0.8967(0.9003) Steps 730(714.74) | Grad Norm 4.5518(7.3992) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 18.9836(18.6716) | Bit/dim 4.3477(4.3999) | Xent 2.3026(2.3026) | Loss 4.3477(4.3999) | Error 0.8956(0.8996) Steps 730(718.65) | Grad Norm 2.0966(7.0411) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 18.5878(18.7263) | Bit/dim 4.3371(4.3865) | Xent 2.3026(2.3026) | Loss 4.3371(4.3865) | Error 0.9100(0.9011) Steps 754(726.07) | Grad Norm 7.2802(6.7408) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.4453(18.8482) | Bit/dim 4.3723(4.3705) | Xent 2.3026(2.3026) | Loss 4.3723(4.3705) | Error 0.9089(0.9006) Steps 748(733.73) | Grad Norm 13.3680(6.9103) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 19.3077(18.9472) | Bit/dim 4.2985(4.3579) | Xent 2.3026(2.3026) | Loss 4.2985(4.3579) | Error 0.8978(0.9006) Steps 754(738.12) | Grad Norm 3.2869(6.4264) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 98.0557, Epoch Time 1158.7536(930.9266), Bit/dim 4.2986(best: 4.3711), Xent 2.3026, Loss 4.2986, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.4189(18.9739) | Bit/dim 4.3037(4.3478) | Xent 2.3026(2.3026) | Loss 4.3037(4.3478) | Error 0.8889(0.9007) Steps 736(738.54) | Grad Norm 7.7825(6.8898) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 18.9642(18.9317) | Bit/dim 4.3204(4.3364) | Xent 2.3026(2.3026) | Loss 4.3204(4.3364) | Error 0.9044(0.9009) Steps 754(738.15) | Grad Norm 7.2240(6.7147) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 20.0563(19.0000) | Bit/dim 4.2683(4.3234) | Xent 2.3026(2.3026) | Loss 4.2683(4.3234) | Error 0.8844(0.8998) Steps 754(738.75) | Grad Norm 4.0905(6.1843) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 20.4389(19.1435) | Bit/dim 4.3310(4.3270) | Xent 2.3026(2.3026) | Loss 4.3310(4.3270) | Error 0.9044(0.8995) Steps 778(743.40) | Grad Norm 12.3615(7.4897) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 19.1103(19.2015) | Bit/dim 4.2779(4.3229) | Xent 2.3026(2.3026) | Loss 4.2779(4.3229) | Error 0.8967(0.9001) Steps 736(745.90) | Grad Norm 3.6873(7.3854) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 95.5692, Epoch Time 1169.1758(938.0740), Bit/dim 4.2556(best: 4.2986), Xent 2.3026, Loss 4.2556, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 19.8694(19.3405) | Bit/dim 4.2289(4.3025) | Xent 2.3026(2.3026) | Loss 4.2289(4.3025) | Error 0.9033(0.8999) Steps 784(750.13) | Grad Norm 2.8241(6.3402) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 20.3232(19.6184) | Bit/dim 4.4002(4.2901) | Xent 2.3026(2.3026) | Loss 4.4002(4.2901) | Error 0.8978(0.8996) Steps 820(758.66) | Grad Norm 13.7906(5.8140) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 20.1061(19.5851) | Bit/dim 4.1848(4.2733) | Xent 2.3026(2.3026) | Loss 4.1848(4.2733) | Error 0.9122(0.9000) Steps 802(766.19) | Grad Norm 2.1688(5.4879) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 18.8540(19.6787) | Bit/dim 4.2002(4.2595) | Xent 2.3026(2.3026) | Loss 4.2002(4.2595) | Error 0.8956(0.9007) Steps 760(773.17) | Grad Norm 5.1626(5.7325) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 19.5952(19.6098) | Bit/dim 4.1827(4.2432) | Xent 2.3026(2.3026) | Loss 4.1827(4.2432) | Error 0.9044(0.9002) Steps 784(774.82) | Grad Norm 2.7738(5.4378) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 19.5595(19.6268) | Bit/dim 4.1684(4.2298) | Xent 2.3026(2.3026) | Loss 4.1684(4.2298) | Error 0.9056(0.8999) Steps 790(777.57) | Grad Norm 4.5019(5.4452) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 102.7300, Epoch Time 1210.4255(946.2446), Bit/dim 4.1706(best: 4.2556), Xent 2.3026, Loss 4.1706, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 19.7808(19.7942) | Bit/dim 4.1592(4.2140) | Xent 2.3026(2.3026) | Loss 4.1592(4.2140) | Error 0.8967(0.8999) Steps 802(783.28) | Grad Norm 3.5549(5.0368) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 20.8888(20.0419) | Bit/dim 4.2540(4.2026) | Xent 2.3026(2.3026) | Loss 4.2540(4.2026) | Error 0.9289(0.9002) Steps 832(792.34) | Grad Norm 11.7680(5.2591) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 20.1054(20.1218) | Bit/dim 4.1845(4.2021) | Xent 2.3026(2.3026) | Loss 4.1845(4.2021) | Error 0.8878(0.8989) Steps 826(797.99) | Grad Norm 6.7960(6.1514) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 20.2580(20.1604) | Bit/dim 4.1628(4.1933) | Xent 2.3026(2.3026) | Loss 4.1628(4.1933) | Error 0.9044(0.9011) Steps 802(797.91) | Grad Norm 2.1641(5.5940) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 20.8414(20.2518) | Bit/dim 4.1149(4.1759) | Xent 2.3026(2.3026) | Loss 4.1149(4.1759) | Error 0.8889(0.9001) Steps 814(802.04) | Grad Norm 2.1936(4.7743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 105.6255, Epoch Time 1247.8524(955.2928), Bit/dim 4.1115(best: 4.1706), Xent 2.3026, Loss 4.1115, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 21.0904(20.4084) | Bit/dim 4.1154(4.1642) | Xent 2.3026(2.3026) | Loss 4.1154(4.1642) | Error 0.8989(0.8999) Steps 838(810.13) | Grad Norm 9.4874(4.7358) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 20.9404(20.5132) | Bit/dim 4.1015(4.1513) | Xent 2.3026(2.3026) | Loss 4.1015(4.1513) | Error 0.8978(0.9009) Steps 814(813.31) | Grad Norm 4.1111(4.9981) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 20.6297(20.6309) | Bit/dim 4.0703(4.1394) | Xent 2.3026(2.3026) | Loss 4.0703(4.1394) | Error 0.8922(0.9002) Steps 814(814.39) | Grad Norm 3.2526(4.7951) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 21.5107(20.8442) | Bit/dim 4.0710(4.1264) | Xent 2.3026(2.3026) | Loss 4.0710(4.1264) | Error 0.8944(0.9010) Steps 826(817.34) | Grad Norm 1.3864(4.5783) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 21.3002(20.9760) | Bit/dim 4.0909(4.1154) | Xent 2.3026(2.3026) | Loss 4.0909(4.1154) | Error 0.9156(0.9010) Steps 838(824.57) | Grad Norm 7.8696(4.7992) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 21.7188(21.0836) | Bit/dim 4.0739(4.1041) | Xent 2.3026(2.3026) | Loss 4.0739(4.1041) | Error 0.9044(0.8997) Steps 856(828.21) | Grad Norm 6.2821(4.8281) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 104.9390, Epoch Time 1287.3124(965.2534), Bit/dim 4.0892(best: 4.1115), Xent 2.3026, Loss 4.0892, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 21.6073(21.1270) | Bit/dim 4.0605(4.0945) | Xent 2.3026(2.3026) | Loss 4.0605(4.0945) | Error 0.9244(0.9013) Steps 814(827.15) | Grad Norm 5.5814(5.0921) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 20.8317(21.2039) | Bit/dim 4.0276(4.0849) | Xent 2.3026(2.3026) | Loss 4.0276(4.0849) | Error 0.8889(0.8996) Steps 814(826.45) | Grad Norm 4.3978(5.2377) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 21.3931(21.2214) | Bit/dim 4.0460(4.0711) | Xent 2.3026(2.3026) | Loss 4.0460(4.0711) | Error 0.9144(0.9000) Steps 820(826.08) | Grad Norm 2.0102(4.6496) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 21.1949(21.2757) | Bit/dim 4.0466(4.0577) | Xent 2.3026(2.3026) | Loss 4.0466(4.0577) | Error 0.9033(0.8990) Steps 826(828.33) | Grad Norm 2.3032(3.8822) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 21.4858(21.3200) | Bit/dim 4.0108(4.0483) | Xent 2.3026(2.3026) | Loss 4.0108(4.0483) | Error 0.9067(0.8991) Steps 832(832.12) | Grad Norm 5.5913(4.1434) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 106.7101, Epoch Time 1297.2735(975.2140), Bit/dim 4.0096(best: 4.0892), Xent 2.3026, Loss 4.0096, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 21.7118(21.2761) | Bit/dim 4.0177(4.0424) | Xent 2.3026(2.3026) | Loss 4.0177(4.0424) | Error 0.9044(0.9005) Steps 826(831.54) | Grad Norm 6.4499(4.4260) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 21.6493(21.2810) | Bit/dim 4.0015(4.0349) | Xent 2.3026(2.3026) | Loss 4.0015(4.0349) | Error 0.8922(0.9000) Steps 826(830.04) | Grad Norm 3.2883(4.3027) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 21.0598(21.2750) | Bit/dim 3.9691(4.0224) | Xent 2.3026(2.3026) | Loss 3.9691(4.0224) | Error 0.8889(0.9005) Steps 814(828.19) | Grad Norm 5.6734(3.9949) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 21.0574(21.2801) | Bit/dim 3.9685(4.0137) | Xent 2.3026(2.3026) | Loss 3.9685(4.0137) | Error 0.8944(0.8993) Steps 826(829.01) | Grad Norm 3.1458(4.3542) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 20.8067(21.2647) | Bit/dim 3.9576(4.0019) | Xent 2.3026(2.3026) | Loss 3.9576(4.0019) | Error 0.9089(0.9004) Steps 844(832.90) | Grad Norm 4.2653(4.2348) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 20.8200(21.2494) | Bit/dim 3.9361(3.9912) | Xent 2.3026(2.3026) | Loss 3.9361(3.9912) | Error 0.8878(0.8994) Steps 820(834.46) | Grad Norm 4.8586(3.9823) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 103.7231, Epoch Time 1291.6024(984.7057), Bit/dim 3.9668(best: 4.0096), Xent 2.3026, Loss 3.9668, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 21.3501(21.2099) | Bit/dim 3.9443(3.9854) | Xent 2.3026(2.3026) | Loss 3.9443(3.9854) | Error 0.8956(0.8999) Steps 814(830.93) | Grad Norm 5.0344(4.7101) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 21.2033(21.1606) | Bit/dim 3.9501(3.9776) | Xent 2.3026(2.3026) | Loss 3.9501(3.9776) | Error 0.9022(0.9006) Steps 832(828.50) | Grad Norm 2.4280(4.4245) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 21.4348(21.1681) | Bit/dim 3.8934(3.9687) | Xent 2.3026(2.3026) | Loss 3.8934(3.9687) | Error 0.8878(0.8991) Steps 826(828.13) | Grad Norm 2.6379(3.9254) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 20.9936(21.1871) | Bit/dim 3.9285(3.9590) | Xent 2.3026(2.3026) | Loss 3.9285(3.9590) | Error 0.9033(0.8996) Steps 838(829.14) | Grad Norm 4.9229(3.9525) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 20.4773(21.1515) | Bit/dim 3.9272(3.9559) | Xent 2.3026(2.3026) | Loss 3.9272(3.9559) | Error 0.9089(0.9001) Steps 820(828.91) | Grad Norm 4.8890(4.5435) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 103.0950, Epoch Time 1280.5526(993.5811), Bit/dim 3.9248(best: 3.9668), Xent 2.3026, Loss 3.9248, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 20.8698(21.0216) | Bit/dim 3.8983(3.9481) | Xent 2.3026(2.3026) | Loss 3.8983(3.9481) | Error 0.8967(0.9003) Steps 820(827.28) | Grad Norm 1.9335(4.1837) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 20.0038(20.8507) | Bit/dim 3.9187(3.9429) | Xent 2.3026(2.3026) | Loss 3.9187(3.9429) | Error 0.8944(0.9009) Steps 814(823.26) | Grad Norm 5.5649(4.1769) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 21.2310(20.8531) | Bit/dim 3.9863(3.9368) | Xent 2.3026(2.3026) | Loss 3.9863(3.9368) | Error 0.9100(0.9013) Steps 820(822.75) | Grad Norm 4.6767(4.1985) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.9939(20.8634) | Bit/dim 3.9242(3.9270) | Xent 2.3026(2.3026) | Loss 3.9242(3.9270) | Error 0.8889(0.9003) Steps 838(823.65) | Grad Norm 3.9138(4.3005) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 21.4087(20.8757) | Bit/dim 3.9214(3.9204) | Xent 2.3026(2.3026) | Loss 3.9214(3.9204) | Error 0.9044(0.8994) Steps 784(820.41) | Grad Norm 1.8943(3.9934) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 21.0680(20.8772) | Bit/dim 3.8815(3.9177) | Xent 2.3026(2.3026) | Loss 3.8815(3.9177) | Error 0.8867(0.8995) Steps 832(818.46) | Grad Norm 4.5673(4.5845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 102.1430, Epoch Time 1262.3098(1001.6429), Bit/dim 3.9045(best: 3.9248), Xent 2.3026, Loss 3.9045, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 20.4073(20.8597) | Bit/dim 3.8932(3.9131) | Xent 2.3026(2.3026) | Loss 3.8932(3.9131) | Error 0.8911(0.8997) Steps 814(818.28) | Grad Norm 6.0368(4.9294) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 21.0393(20.8817) | Bit/dim 3.9196(3.9089) | Xent 2.3026(2.3026) | Loss 3.9196(3.9089) | Error 0.9000(0.8997) Steps 826(818.07) | Grad Norm 2.1747(4.6599) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 21.1113(20.8875) | Bit/dim 3.8923(3.8995) | Xent 2.3026(2.3026) | Loss 3.8923(3.8995) | Error 0.9144(0.8994) Steps 832(818.84) | Grad Norm 1.8301(4.0320) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 21.2637(20.8429) | Bit/dim 3.8947(3.8957) | Xent 2.3026(2.3026) | Loss 3.8947(3.8957) | Error 0.8922(0.9002) Steps 832(819.30) | Grad Norm 5.8434(4.3276) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 20.7351(20.8593) | Bit/dim 3.8839(3.8879) | Xent 2.3026(2.3026) | Loss 3.8839(3.8879) | Error 0.9044(0.8993) Steps 808(818.29) | Grad Norm 2.6451(3.8317) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 102.2071, Epoch Time 1265.4071(1009.5559), Bit/dim 3.8610(best: 3.9045), Xent 2.3026, Loss 3.8610, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 20.2362(20.8193) | Bit/dim 3.8771(3.8825) | Xent 2.3026(2.3026) | Loss 3.8771(3.8825) | Error 0.8956(0.9001) Steps 826(816.64) | Grad Norm 2.6568(3.5845) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 20.7672(20.7219) | Bit/dim 3.8945(3.8803) | Xent 2.3026(2.3026) | Loss 3.8945(3.8803) | Error 0.9078(0.9004) Steps 832(816.53) | Grad Norm 6.4150(4.3133) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 19.8092(20.6782) | Bit/dim 3.8498(3.8741) | Xent 2.3026(2.3026) | Loss 3.8498(3.8741) | Error 0.8956(0.9001) Steps 802(814.00) | Grad Norm 2.8938(4.5101) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 19.7360(20.6136) | Bit/dim 3.8420(3.8687) | Xent 2.3026(2.3026) | Loss 3.8420(3.8687) | Error 0.9044(0.8991) Steps 796(809.94) | Grad Norm 6.6078(4.4011) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 20.1160(20.5217) | Bit/dim 3.8669(3.8648) | Xent 2.3026(2.3026) | Loss 3.8669(3.8648) | Error 0.8956(0.9004) Steps 808(807.02) | Grad Norm 3.6901(4.2369) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 20.2743(20.4905) | Bit/dim 3.8380(3.8612) | Xent 2.3026(2.3026) | Loss 3.8380(3.8612) | Error 0.8922(0.9004) Steps 790(805.19) | Grad Norm 4.7107(4.3096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 99.8968, Epoch Time 1243.2419(1016.5664), Bit/dim 3.8437(best: 3.8610), Xent 2.3026, Loss 3.8437, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 20.1873(20.4079) | Bit/dim 3.8156(3.8558) | Xent 2.3026(2.3026) | Loss 3.8156(3.8558) | Error 0.8933(0.8985) Steps 820(803.86) | Grad Norm 5.0327(4.3607) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 20.7004(20.3803) | Bit/dim 3.8608(3.8526) | Xent 2.3026(2.3026) | Loss 3.8608(3.8526) | Error 0.9011(0.8990) Steps 820(804.67) | Grad Norm 3.1590(4.4942) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 19.6401(20.3797) | Bit/dim 3.7985(3.8488) | Xent 2.3026(2.3026) | Loss 3.7985(3.8488) | Error 0.9044(0.9004) Steps 808(805.73) | Grad Norm 3.0740(4.6341) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 20.1058(20.3099) | Bit/dim 3.8290(3.8473) | Xent 2.3026(2.3026) | Loss 3.8290(3.8473) | Error 0.8911(0.8996) Steps 808(806.91) | Grad Norm 5.1111(4.6255) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 20.4133(20.2658) | Bit/dim 3.8140(3.8404) | Xent 2.3026(2.3026) | Loss 3.8140(3.8404) | Error 0.8922(0.9004) Steps 808(805.41) | Grad Norm 4.9265(4.6356) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 96.7579, Epoch Time 1226.1727(1022.8546), Bit/dim 3.8173(best: 3.8437), Xent 2.3026, Loss 3.8173, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 20.1016(20.2771) | Bit/dim 3.8553(3.8358) | Xent 2.3026(2.3026) | Loss 3.8553(3.8358) | Error 0.9089(0.9007) Steps 808(804.41) | Grad Norm 5.7782(4.3822) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 19.7326(20.2808) | Bit/dim 3.7959(3.8307) | Xent 2.3026(2.3026) | Loss 3.7959(3.8307) | Error 0.8922(0.9011) Steps 802(805.14) | Grad Norm 2.1763(4.2942) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 19.9892(20.2861) | Bit/dim 3.8199(3.8267) | Xent 2.3026(2.3026) | Loss 3.8199(3.8267) | Error 0.8956(0.8997) Steps 814(805.20) | Grad Norm 2.1934(4.1309) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 20.3205(20.2996) | Bit/dim 3.8310(3.8241) | Xent 2.3026(2.3026) | Loss 3.8310(3.8241) | Error 0.9189(0.8983) Steps 808(806.42) | Grad Norm 5.4089(4.4344) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 20.3423(20.3117) | Bit/dim 3.8206(3.8194) | Xent 2.3026(2.3026) | Loss 3.8206(3.8194) | Error 0.8967(0.8984) Steps 808(804.52) | Grad Norm 2.5863(4.5084) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 19.9988(20.2908) | Bit/dim 3.7702(3.8167) | Xent 2.3026(2.3026) | Loss 3.7702(3.8167) | Error 0.9067(0.9013) Steps 814(802.37) | Grad Norm 3.1175(4.2722) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 97.5459, Epoch Time 1233.1927(1029.1648), Bit/dim 3.8010(best: 3.8173), Xent 2.3026, Loss 3.8010, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 19.8114(20.2027) | Bit/dim 3.7805(3.8132) | Xent 2.3026(2.3026) | Loss 3.7805(3.8132) | Error 0.9022(0.8997) Steps 802(801.55) | Grad Norm 2.4283(3.9460) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 19.7047(20.2076) | Bit/dim 3.7798(3.8118) | Xent 2.3026(2.3026) | Loss 3.7798(3.8118) | Error 0.8911(0.8994) Steps 790(800.97) | Grad Norm 2.0355(4.2574) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 20.1173(20.1876) | Bit/dim 3.8143(3.8070) | Xent 2.3026(2.3026) | Loss 3.8143(3.8070) | Error 0.8989(0.8993) Steps 784(799.00) | Grad Norm 7.3117(4.2393) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 19.7119(20.1309) | Bit/dim 3.7658(3.8008) | Xent 2.3026(2.3026) | Loss 3.7658(3.8008) | Error 0.8889(0.8991) Steps 808(797.86) | Grad Norm 3.4797(4.1653) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 20.4331(20.1884) | Bit/dim 3.7714(3.7968) | Xent 2.3026(2.3026) | Loss 3.7714(3.7968) | Error 0.8967(0.9012) Steps 784(796.21) | Grad Norm 6.8062(4.2571) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 99.6289, Epoch Time 1225.1511(1035.0444), Bit/dim 3.7874(best: 3.8010), Xent 2.3026, Loss 3.7874, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 20.0892(20.2002) | Bit/dim 3.7943(3.7945) | Xent 2.3026(2.3026) | Loss 3.7943(3.7945) | Error 0.8944(0.9008) Steps 802(797.33) | Grad Norm 2.3277(4.0135) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 20.4470(20.2144) | Bit/dim 3.7630(3.7920) | Xent 2.3026(2.3026) | Loss 3.7630(3.7920) | Error 0.9067(0.9016) Steps 820(797.98) | Grad Norm 7.1335(4.3152) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 20.0680(20.2514) | Bit/dim 3.7934(3.7874) | Xent 2.3026(2.3026) | Loss 3.7934(3.7874) | Error 0.8967(0.8998) Steps 796(798.87) | Grad Norm 3.3151(4.5895) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 19.9944(20.2822) | Bit/dim 3.7874(3.7862) | Xent 2.3026(2.3026) | Loss 3.7874(3.7862) | Error 0.9056(0.9000) Steps 790(797.90) | Grad Norm 2.7714(4.3463) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 20.9709(20.3871) | Bit/dim 3.7740(3.7829) | Xent 2.3026(2.3026) | Loss 3.7740(3.7829) | Error 0.9089(0.8996) Steps 814(797.25) | Grad Norm 4.6693(3.9800) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 20.9458(20.4338) | Bit/dim 3.7461(3.7787) | Xent 2.3026(2.3026) | Loss 3.7461(3.7787) | Error 0.9100(0.9009) Steps 802(798.64) | Grad Norm 2.7841(4.1665) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 100.3448, Epoch Time 1240.9726(1041.2222), Bit/dim 3.7657(best: 3.7874), Xent 2.3026, Loss 3.7657, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 19.7872(20.4063) | Bit/dim 3.7614(3.7761) | Xent 2.3026(2.3026) | Loss 3.7614(3.7761) | Error 0.9033(0.8996) Steps 802(800.22) | Grad Norm 5.7714(4.4843) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 20.1059(20.3391) | Bit/dim 3.7994(3.7777) | Xent 2.3026(2.3026) | Loss 3.7994(3.7777) | Error 0.9178(0.8999) Steps 808(799.89) | Grad Norm 3.0067(4.5558) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 19.7264(20.3009) | Bit/dim 3.7764(3.7735) | Xent 2.3026(2.3026) | Loss 3.7764(3.7735) | Error 0.8967(0.8990) Steps 784(800.17) | Grad Norm 3.9034(4.3964) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 20.7582(20.3205) | Bit/dim 3.7536(3.7718) | Xent 2.3026(2.3026) | Loss 3.7536(3.7718) | Error 0.9122(0.9003) Steps 802(799.69) | Grad Norm 3.1260(3.9278) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 20.3136(20.2475) | Bit/dim 3.7388(3.7691) | Xent 2.3026(2.3026) | Loss 3.7388(3.7691) | Error 0.9078(0.9018) Steps 802(799.10) | Grad Norm 6.4353(4.1842) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 100.3650, Epoch Time 1232.1647(1046.9505), Bit/dim 3.7541(best: 3.7657), Xent 2.3026, Loss 3.7541, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 20.6030(20.3117) | Bit/dim 3.7374(3.7638) | Xent 2.3026(2.3026) | Loss 3.7374(3.7638) | Error 0.9089(0.9012) Steps 814(799.27) | Grad Norm 4.5560(4.4257) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 20.1184(20.4257) | Bit/dim 3.7689(3.7626) | Xent 2.3026(2.3026) | Loss 3.7689(3.7626) | Error 0.8956(0.9013) Steps 784(797.54) | Grad Norm 1.6898(4.1981) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 20.6430(20.4353) | Bit/dim 3.7608(3.7590) | Xent 2.3026(2.3026) | Loss 3.7608(3.7590) | Error 0.9089(0.9018) Steps 784(798.16) | Grad Norm 3.4838(3.9782) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 20.3461(20.4655) | Bit/dim 3.7178(3.7569) | Xent 2.3026(2.3026) | Loss 3.7178(3.7569) | Error 0.8911(0.9014) Steps 808(799.94) | Grad Norm 3.1551(3.9098) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 22.0111(20.5828) | Bit/dim 3.7280(3.7524) | Xent 2.3026(2.3026) | Loss 3.7280(3.7524) | Error 0.8778(0.9001) Steps 832(803.18) | Grad Norm 7.3613(4.2237) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 22.1636(20.6532) | Bit/dim 3.7335(3.7499) | Xent 2.3026(2.3026) | Loss 3.7335(3.7499) | Error 0.8911(0.8996) Steps 868(805.32) | Grad Norm 6.5965(4.3786) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 100.8446, Epoch Time 1255.8565(1053.2177), Bit/dim 3.7451(best: 3.7541), Xent 2.3026, Loss 3.7451, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 20.3645(20.7168) | Bit/dim 3.7571(3.7461) | Xent 2.3026(2.3026) | Loss 3.7571(3.7461) | Error 0.9067(0.8995) Steps 802(807.17) | Grad Norm 1.5993(4.1054) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 20.6392(20.6622) | Bit/dim 3.7416(3.7444) | Xent 2.3026(2.3026) | Loss 3.7416(3.7444) | Error 0.9056(0.8996) Steps 790(806.08) | Grad Norm 5.4358(3.7199) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 21.4953(20.7185) | Bit/dim 3.7261(3.7407) | Xent 2.3026(2.3026) | Loss 3.7261(3.7407) | Error 0.8878(0.8996) Steps 838(805.76) | Grad Norm 4.0900(4.0445) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 21.6472(20.6725) | Bit/dim 3.7768(3.7409) | Xent 2.3026(2.3026) | Loss 3.7768(3.7409) | Error 0.9100(0.8999) Steps 826(807.12) | Grad Norm 5.0750(4.4756) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 21.0011(20.5983) | Bit/dim 3.7171(3.7397) | Xent 2.3026(2.3026) | Loss 3.7171(3.7397) | Error 0.9089(0.9002) Steps 808(806.07) | Grad Norm 3.4274(4.3794) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 98.9332, Epoch Time 1250.3539(1059.1317), Bit/dim 3.7282(best: 3.7451), Xent 2.3026, Loss 3.7282, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 21.0171(20.5500) | Bit/dim 3.7330(3.7398) | Xent 2.3026(2.3026) | Loss 3.7330(3.7398) | Error 0.9089(0.9001) Steps 832(805.91) | Grad Norm 2.0192(3.9552) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 20.1812(20.5939) | Bit/dim 3.7013(3.7347) | Xent 2.3026(2.3026) | Loss 3.7013(3.7347) | Error 0.9211(0.9011) Steps 814(807.16) | Grad Norm 2.9794(3.8715) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 20.2712(20.6199) | Bit/dim 3.7147(3.7320) | Xent 2.3026(2.3026) | Loss 3.7147(3.7320) | Error 0.8967(0.9012) Steps 802(808.10) | Grad Norm 1.8621(3.8534) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 20.7641(20.6567) | Bit/dim 3.7348(3.7313) | Xent 2.3026(2.3026) | Loss 3.7348(3.7313) | Error 0.9100(0.9004) Steps 820(808.55) | Grad Norm 3.8940(4.1086) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 21.0348(20.7092) | Bit/dim 3.7439(3.7306) | Xent 2.3026(2.3026) | Loss 3.7439(3.7306) | Error 0.9144(0.9000) Steps 832(813.12) | Grad Norm 2.9848(4.1831) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 20.8587(20.7858) | Bit/dim 3.6881(3.7273) | Xent 2.3026(2.3026) | Loss 3.6881(3.7273) | Error 0.8944(0.8999) Steps 814(815.46) | Grad Norm 7.4891(4.0557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 101.0478, Epoch Time 1262.0099(1065.2181), Bit/dim 3.7301(best: 3.7282), Xent 2.3026, Loss 3.7301, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 21.7914(20.8829) | Bit/dim 3.6816(3.7265) | Xent 2.3026(2.3026) | Loss 3.6816(3.7265) | Error 0.8967(0.9005) Steps 826(815.73) | Grad Norm 2.3648(4.1175) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 21.1514(20.9368) | Bit/dim 3.7048(3.7229) | Xent 2.3026(2.3026) | Loss 3.7048(3.7229) | Error 0.9022(0.8994) Steps 826(816.84) | Grad Norm 1.3199(3.9156) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 20.6955(21.0165) | Bit/dim 3.6806(3.7173) | Xent 2.3026(2.3026) | Loss 3.6806(3.7173) | Error 0.8678(0.8990) Steps 802(819.02) | Grad Norm 5.8849(3.8396) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 20.9226(21.0390) | Bit/dim 3.6946(3.7162) | Xent 2.3026(2.3026) | Loss 3.6946(3.7162) | Error 0.9011(0.8996) Steps 844(821.22) | Grad Norm 5.4158(4.3596) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 21.8989(21.0422) | Bit/dim 3.7223(3.7170) | Xent 2.3026(2.3026) | Loss 3.7223(3.7170) | Error 0.9000(0.8988) Steps 808(824.26) | Grad Norm 4.1137(4.1109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 100.7154, Epoch Time 1281.2148(1071.6980), Bit/dim 3.7052(best: 3.7282), Xent 2.3026, Loss 3.7052, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 21.1222(21.1122) | Bit/dim 3.7054(3.7149) | Xent 2.3026(2.3026) | Loss 3.7054(3.7149) | Error 0.9033(0.9003) Steps 838(824.83) | Grad Norm 3.9046(4.0517) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 20.2790(20.9952) | Bit/dim 3.7027(3.7109) | Xent 2.3026(2.3026) | Loss 3.7027(3.7109) | Error 0.8956(0.9003) Steps 820(825.40) | Grad Norm 2.1683(3.8737) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 21.5966(21.1285) | Bit/dim 3.6817(3.7055) | Xent 2.3026(2.3026) | Loss 3.6817(3.7055) | Error 0.8967(0.8998) Steps 838(827.16) | Grad Norm 2.6663(3.9104) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 21.1688(21.1020) | Bit/dim 3.6999(3.7063) | Xent 2.3026(2.3026) | Loss 3.6999(3.7063) | Error 0.8922(0.9001) Steps 838(829.16) | Grad Norm 3.9583(3.9861) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 21.2703(21.0584) | Bit/dim 3.6899(3.7052) | Xent 2.3026(2.3026) | Loss 3.6899(3.7052) | Error 0.8878(0.9001) Steps 826(830.06) | Grad Norm 1.2380(4.0342) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 21.6278(21.1073) | Bit/dim 3.7065(3.7053) | Xent 2.3026(2.3026) | Loss 3.7065(3.7053) | Error 0.9011(0.8998) Steps 844(831.74) | Grad Norm 3.9843(4.0763) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 99.1181, Epoch Time 1277.3471(1077.8675), Bit/dim 3.6945(best: 3.7052), Xent 2.3026, Loss 3.6945, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 22.0325(21.1399) | Bit/dim 3.6612(3.7013) | Xent 2.3026(2.3026) | Loss 3.6612(3.7013) | Error 0.8933(0.9009) Steps 832(831.40) | Grad Norm 2.0271(3.5625) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 20.8485(21.1555) | Bit/dim 3.6875(3.6995) | Xent 2.3026(2.3026) | Loss 3.6875(3.6995) | Error 0.8933(0.9000) Steps 814(832.56) | Grad Norm 6.6976(3.5471) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 20.4895(21.1125) | Bit/dim 3.6973(3.6959) | Xent 2.3026(2.3026) | Loss 3.6973(3.6959) | Error 0.8922(0.8986) Steps 838(833.98) | Grad Norm 2.7539(3.8168) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 20.4065(21.0305) | Bit/dim 3.6840(3.6961) | Xent 2.3026(2.3026) | Loss 3.6840(3.6961) | Error 0.9000(0.8989) Steps 832(833.07) | Grad Norm 3.6075(3.8372) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 20.8078(21.0332) | Bit/dim 3.7111(3.6969) | Xent 2.3026(2.3026) | Loss 3.7111(3.6969) | Error 0.9011(0.8990) Steps 838(832.97) | Grad Norm 1.4825(4.0656) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 98.9141, Epoch Time 1273.9240(1083.7492), Bit/dim 3.6992(best: 3.6945), Xent 2.3026, Loss 3.6992, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 21.4697(21.0229) | Bit/dim 3.6826(3.6953) | Xent 2.3026(2.3026) | Loss 3.6826(3.6953) | Error 0.8978(0.9001) Steps 832(831.95) | Grad Norm 4.4302(4.1212) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 20.8054(21.0412) | Bit/dim 3.6846(3.6955) | Xent 2.3026(2.3026) | Loss 3.6846(3.6955) | Error 0.9000(0.9013) Steps 832(832.63) | Grad Norm 2.7625(3.9659) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 20.5615(21.0108) | Bit/dim 3.6998(3.6930) | Xent 2.3026(2.3026) | Loss 3.6998(3.6930) | Error 0.9189(0.9026) Steps 826(832.02) | Grad Norm 2.6295(3.6361) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 21.2681(21.0573) | Bit/dim 3.7235(3.6898) | Xent 2.3026(2.3026) | Loss 3.7235(3.6898) | Error 0.9089(0.9012) Steps 832(832.20) | Grad Norm 4.6328(4.0224) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 21.6760(21.1290) | Bit/dim 3.7135(3.6905) | Xent 2.3026(2.3026) | Loss 3.7135(3.6905) | Error 0.9144(0.9007) Steps 832(832.37) | Grad Norm 2.2104(4.0473) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 21.3787(21.2417) | Bit/dim 3.6978(3.6852) | Xent 2.3026(2.3026) | Loss 3.6978(3.6852) | Error 0.8967(0.8988) Steps 826(832.39) | Grad Norm 5.9208(3.9307) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 103.1146, Epoch Time 1287.6574(1089.8664), Bit/dim 3.6853(best: 3.6945), Xent 2.3026, Loss 3.6853, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 21.6328(21.1984) | Bit/dim 3.7060(3.6848) | Xent 2.3026(2.3026) | Loss 3.7060(3.6848) | Error 0.8956(0.8991) Steps 832(832.74) | Grad Norm 5.6360(3.8991) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 21.5189(21.1596) | Bit/dim 3.6799(3.6841) | Xent 2.3026(2.3026) | Loss 3.6799(3.6841) | Error 0.8778(0.8998) Steps 844(833.41) | Grad Norm 4.5206(4.0899) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 20.3951(21.0943) | Bit/dim 3.6779(3.6821) | Xent 2.3026(2.3026) | Loss 3.6779(3.6821) | Error 0.9044(0.8997) Steps 832(833.33) | Grad Norm 2.3097(3.8826) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 20.8144(21.0637) | Bit/dim 3.6502(3.6789) | Xent 2.3026(2.3026) | Loss 3.6502(3.6789) | Error 0.9078(0.9000) Steps 832(833.46) | Grad Norm 1.9303(3.4100) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 21.2485(21.1051) | Bit/dim 3.6867(3.6800) | Xent 2.3026(2.3026) | Loss 3.6867(3.6800) | Error 0.9056(0.8997) Steps 844(834.25) | Grad Norm 5.2253(3.7302) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 99.8023, Epoch Time 1273.5303(1095.3763), Bit/dim 3.6723(best: 3.6853), Xent 2.3026, Loss 3.6723, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 20.8234(21.0554) | Bit/dim 3.6467(3.6770) | Xent 2.3026(2.3026) | Loss 3.6467(3.6770) | Error 0.9056(0.8994) Steps 826(833.40) | Grad Norm 3.2723(3.7530) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 19.7388(21.0237) | Bit/dim 3.6705(3.6750) | Xent 2.3026(2.3026) | Loss 3.6705(3.6750) | Error 0.8822(0.8984) Steps 832(832.56) | Grad Norm 4.8861(3.9919) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 21.2672(20.9804) | Bit/dim 3.6490(3.6719) | Xent 2.3026(2.3026) | Loss 3.6490(3.6719) | Error 0.8989(0.8991) Steps 832(832.27) | Grad Norm 3.8900(3.8139) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 20.9895(20.9540) | Bit/dim 3.6668(3.6717) | Xent 2.3026(2.3026) | Loss 3.6668(3.6717) | Error 0.9167(0.9012) Steps 838(831.94) | Grad Norm 4.6403(3.6306) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 20.7488(20.9522) | Bit/dim 3.6606(3.6694) | Xent 2.3026(2.3026) | Loss 3.6606(3.6694) | Error 0.8978(0.9017) Steps 826(833.04) | Grad Norm 3.7253(3.7054) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 20.5125(20.9582) | Bit/dim 3.6590(3.6702) | Xent 2.3026(2.3026) | Loss 3.6590(3.6702) | Error 0.9011(0.9000) Steps 832(831.93) | Grad Norm 2.8343(3.5078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 100.5817, Epoch Time 1270.5095(1100.6303), Bit/dim 3.6613(best: 3.6723), Xent 2.3026, Loss 3.6613, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 21.0569(20.9206) | Bit/dim 3.6982(3.6692) | Xent 2.3026(2.3026) | Loss 3.6982(3.6692) | Error 0.8944(0.8998) Steps 832(831.85) | Grad Norm 6.0353(3.8491) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 21.0906(20.9057) | Bit/dim 3.6782(3.6694) | Xent 2.3026(2.3026) | Loss 3.6782(3.6694) | Error 0.9133(0.9002) Steps 832(833.62) | Grad Norm 6.4117(3.9702) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 21.3641(20.9050) | Bit/dim 3.6507(3.6664) | Xent 2.3026(2.3026) | Loss 3.6507(3.6664) | Error 0.8911(0.8993) Steps 826(830.07) | Grad Norm 4.0567(4.0949) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 21.2908(20.8826) | Bit/dim 3.6808(3.6684) | Xent 2.3026(2.3026) | Loss 3.6808(3.6684) | Error 0.9033(0.9000) Steps 826(828.85) | Grad Norm 1.8954(3.8320) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 21.3551(20.9423) | Bit/dim 3.6375(3.6654) | Xent 2.3026(2.3026) | Loss 3.6375(3.6654) | Error 0.8911(0.9002) Steps 832(829.68) | Grad Norm 4.1951(3.4918) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 100.1380, Epoch Time 1266.4844(1105.6059), Bit/dim 3.6584(best: 3.6613), Xent 2.3026, Loss 3.6584, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 20.4092(20.9031) | Bit/dim 3.6534(3.6621) | Xent 2.3026(2.3026) | Loss 3.6534(3.6621) | Error 0.8978(0.9004) Steps 820(829.98) | Grad Norm 3.3142(3.8571) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 21.6799(20.8802) | Bit/dim 3.6722(3.6592) | Xent 2.3026(2.3026) | Loss 3.6722(3.6592) | Error 0.8956(0.9006) Steps 820(829.08) | Grad Norm 3.0421(3.6450) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 20.7698(20.9069) | Bit/dim 3.6432(3.6573) | Xent 2.3026(2.3026) | Loss 3.6432(3.6573) | Error 0.9100(0.9012) Steps 826(828.47) | Grad Norm 2.7634(3.5600) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 20.9572(20.9073) | Bit/dim 3.6730(3.6584) | Xent 2.3026(2.3026) | Loss 3.6730(3.6584) | Error 0.9044(0.8997) Steps 844(829.87) | Grad Norm 5.7011(3.6045) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 21.3262(20.9688) | Bit/dim 3.6416(3.6590) | Xent 2.3026(2.3026) | Loss 3.6416(3.6590) | Error 0.9033(0.8992) Steps 820(830.97) | Grad Norm 2.2926(3.5712) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 20.3400(20.8480) | Bit/dim 3.6361(3.6552) | Xent 2.3026(2.3026) | Loss 3.6361(3.6552) | Error 0.9111(0.8994) Steps 814(828.88) | Grad Norm 6.6080(3.5189) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 101.6405, Epoch Time 1267.6614(1110.4676), Bit/dim 3.6676(best: 3.6584), Xent 2.3026, Loss 3.6676, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 21.2553(20.9229) | Bit/dim 3.6447(3.6543) | Xent 2.3026(2.3026) | Loss 3.6447(3.6543) | Error 0.9133(0.8999) Steps 844(827.71) | Grad Norm 3.3889(3.6347) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 20.8241(20.9076) | Bit/dim 3.6303(3.6519) | Xent 2.3026(2.3026) | Loss 3.6303(3.6519) | Error 0.9078(0.9005) Steps 826(825.41) | Grad Norm 4.4270(3.5585) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 20.7646(20.8552) | Bit/dim 3.6320(3.6515) | Xent 2.3026(2.3026) | Loss 3.6320(3.6515) | Error 0.9022(0.9005) Steps 826(824.32) | Grad Norm 2.6992(3.5739) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 20.7602(20.8489) | Bit/dim 3.6247(3.6466) | Xent 2.3026(2.3026) | Loss 3.6247(3.6466) | Error 0.8956(0.8996) Steps 814(824.89) | Grad Norm 2.1628(3.5433) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 20.8815(20.8249) | Bit/dim 3.6761(3.6478) | Xent 2.3026(2.3026) | Loss 3.6761(3.6478) | Error 0.9033(0.8995) Steps 820(823.54) | Grad Norm 4.5673(3.7329) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 100.3348, Epoch Time 1264.8556(1115.0992), Bit/dim 3.6445(best: 3.6584), Xent 2.3026, Loss 3.6445, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 21.9082(20.8038) | Bit/dim 3.6351(3.6494) | Xent 2.3026(2.3026) | Loss 3.6351(3.6494) | Error 0.8944(0.9004) Steps 814(821.75) | Grad Norm 3.6333(3.6182) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 20.4831(20.7971) | Bit/dim 3.6720(3.6474) | Xent 2.3026(2.3026) | Loss 3.6720(3.6474) | Error 0.9044(0.9001) Steps 820(820.92) | Grad Norm 2.6991(3.5773) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 21.0971(20.7536) | Bit/dim 3.6437(3.6473) | Xent 2.3026(2.3026) | Loss 3.6437(3.6473) | Error 0.8878(0.9012) Steps 826(820.05) | Grad Norm 2.9030(3.6032) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 20.0095(20.7054) | Bit/dim 3.6366(3.6460) | Xent 2.3026(2.3026) | Loss 3.6366(3.6460) | Error 0.8989(0.9012) Steps 814(820.24) | Grad Norm 4.1328(3.3180) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 20.6283(20.6538) | Bit/dim 3.6188(3.6439) | Xent 2.3026(2.3026) | Loss 3.6188(3.6439) | Error 0.8933(0.8991) Steps 814(818.99) | Grad Norm 3.3608(3.6991) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 20.0653(20.6098) | Bit/dim 3.6379(3.6422) | Xent 2.3026(2.3026) | Loss 3.6379(3.6422) | Error 0.9089(0.8990) Steps 814(817.95) | Grad Norm 4.0364(3.5721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 100.2357, Epoch Time 1251.3467(1119.1867), Bit/dim 3.6389(best: 3.6445), Xent 2.3026, Loss 3.6389, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 19.7787(20.4894) | Bit/dim 3.6273(3.6408) | Xent 2.3026(2.3026) | Loss 3.6273(3.6408) | Error 0.8867(0.8980) Steps 808(816.32) | Grad Norm 3.3749(3.7428) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 20.1712(20.5502) | Bit/dim 3.6264(3.6416) | Xent 2.3026(2.3026) | Loss 3.6264(3.6416) | Error 0.8933(0.8990) Steps 808(816.16) | Grad Norm 2.1918(3.7353) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 21.0582(20.5642) | Bit/dim 3.6393(3.6375) | Xent 2.3026(2.3026) | Loss 3.6393(3.6375) | Error 0.9078(0.8989) Steps 808(815.84) | Grad Norm 3.0323(3.7162) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 20.8808(20.5662) | Bit/dim 3.6676(3.6388) | Xent 2.3026(2.3026) | Loss 3.6676(3.6388) | Error 0.8956(0.8995) Steps 826(814.60) | Grad Norm 3.4231(3.5640) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 21.0254(20.6026) | Bit/dim 3.6320(3.6377) | Xent 2.3026(2.3026) | Loss 3.6320(3.6377) | Error 0.9033(0.8999) Steps 802(812.58) | Grad Norm 2.8957(3.2872) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 100.1804, Epoch Time 1245.9365(1122.9892), Bit/dim 3.6321(best: 3.6389), Xent 2.3026, Loss 3.6321, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 20.3075(20.4904) | Bit/dim 3.6442(3.6367) | Xent 2.3026(2.3026) | Loss 3.6442(3.6367) | Error 0.9111(0.9003) Steps 838(812.79) | Grad Norm 2.2858(3.3706) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 20.1392(20.5961) | Bit/dim 3.6145(3.6349) | Xent 2.3026(2.3026) | Loss 3.6145(3.6349) | Error 0.9056(0.9004) Steps 802(812.72) | Grad Norm 2.2963(3.3462) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 19.9354(20.6115) | Bit/dim 3.6319(3.6351) | Xent 2.3026(2.3026) | Loss 3.6319(3.6351) | Error 0.8978(0.9007) Steps 814(813.34) | Grad Norm 3.6568(3.4174) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 20.6304(20.6030) | Bit/dim 3.6487(3.6360) | Xent 2.3026(2.3026) | Loss 3.6487(3.6360) | Error 0.8989(0.9003) Steps 808(812.07) | Grad Norm 2.7405(3.4332) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 20.0044(20.5602) | Bit/dim 3.6186(3.6342) | Xent 2.3026(2.3026) | Loss 3.6186(3.6342) | Error 0.8956(0.9004) Steps 802(811.23) | Grad Norm 3.6297(3.6935) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 19.5413(20.4967) | Bit/dim 3.6185(3.6301) | Xent 2.3026(2.3026) | Loss 3.6185(3.6301) | Error 0.9067(0.8998) Steps 808(811.95) | Grad Norm 2.8309(3.6006) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 99.2116, Epoch Time 1247.7877(1126.7331), Bit/dim 3.6265(best: 3.6321), Xent 2.3026, Loss 3.6265, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 20.2976(20.4984) | Bit/dim 3.6177(3.6299) | Xent 2.3026(2.3026) | Loss 3.6177(3.6299) | Error 0.9033(0.8998) Steps 814(811.22) | Grad Norm 3.5162(3.7130) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 20.7981(20.5196) | Bit/dim 3.6024(3.6291) | Xent 2.3026(2.3026) | Loss 3.6024(3.6291) | Error 0.8933(0.9005) Steps 790(808.85) | Grad Norm 4.7259(3.5520) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 20.6112(20.4401) | Bit/dim 3.6236(3.6325) | Xent 2.3026(2.3026) | Loss 3.6236(3.6325) | Error 0.8878(0.9003) Steps 814(806.92) | Grad Norm 1.8429(3.7546) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 20.2641(20.3535) | Bit/dim 3.6265(3.6304) | Xent 2.3026(2.3026) | Loss 3.6265(3.6304) | Error 0.8933(0.8990) Steps 802(805.82) | Grad Norm 2.0967(3.5993) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 21.0932(20.3866) | Bit/dim 3.6532(3.6262) | Xent 2.3026(2.3026) | Loss 3.6532(3.6262) | Error 0.9289(0.9000) Steps 802(804.67) | Grad Norm 4.0203(3.4229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 96.3709, Epoch Time 1234.7071(1129.9723), Bit/dim 3.6196(best: 3.6265), Xent 2.3026, Loss 3.6196, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 20.3295(20.3898) | Bit/dim 3.6181(3.6248) | Xent 2.3026(2.3026) | Loss 3.6181(3.6248) | Error 0.9022(0.8999) Steps 802(804.41) | Grad Norm 4.3723(3.2629) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 20.1965(20.4574) | Bit/dim 3.6177(3.6243) | Xent 2.3026(2.3026) | Loss 3.6177(3.6243) | Error 0.8856(0.9003) Steps 790(804.72) | Grad Norm 2.1399(3.4896) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 19.8526(20.3856) | Bit/dim 3.6161(3.6223) | Xent 2.3026(2.3026) | Loss 3.6161(3.6223) | Error 0.9044(0.9002) Steps 796(803.99) | Grad Norm 1.9982(3.4157) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 19.8312(20.2864) | Bit/dim 3.6635(3.6214) | Xent 2.3026(2.3026) | Loss 3.6635(3.6214) | Error 0.9078(0.9008) Steps 802(803.47) | Grad Norm 3.5441(3.3489) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 19.9298(20.2837) | Bit/dim 3.6129(3.6230) | Xent 2.3026(2.3026) | Loss 3.6129(3.6230) | Error 0.9022(0.9010) Steps 790(801.84) | Grad Norm 4.4655(3.5495) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 19.5509(20.3100) | Bit/dim 3.6089(3.6197) | Xent 2.3026(2.3026) | Loss 3.6089(3.6197) | Error 0.9022(0.8995) Steps 802(801.65) | Grad Norm 2.0285(3.5152) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 20.6739(20.3272) | Bit/dim 3.6009(3.6178) | Xent 2.3026(2.3026) | Loss 3.6009(3.6178) | Error 0.9044(0.8994) Steps 802(801.14) | Grad Norm 2.3664(3.2802) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 20.5889(20.2957) | Bit/dim 3.6220(3.6182) | Xent 2.3026(2.3026) | Loss 3.6220(3.6182) | Error 0.8989(0.8988) Steps 802(799.77) | Grad Norm 2.6129(3.2501) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 20.8226(20.2823) | Bit/dim 3.5699(3.6155) | Xent 2.3026(2.3026) | Loss 3.5699(3.6155) | Error 0.9000(0.8992) Steps 802(799.72) | Grad Norm 2.6952(3.2978) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 20.7080(20.2661) | Bit/dim 3.6053(3.6138) | Xent 2.3026(2.3026) | Loss 3.6053(3.6138) | Error 0.9178(0.9002) Steps 790(799.06) | Grad Norm 3.0488(3.1049) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 20.5070(20.2172) | Bit/dim 3.5689(3.6145) | Xent 2.3026(2.3026) | Loss 3.5689(3.6145) | Error 0.8878(0.8996) Steps 808(799.87) | Grad Norm 3.3747(3.3203) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 98.5514, Epoch Time 1227.0644(1135.8391), Bit/dim 3.6173(best: 3.6196), Xent 2.3026, Loss 3.6173, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 20.8085(20.1648) | Bit/dim 3.6075(3.6132) | Xent 2.3026(2.3026) | Loss 3.6075(3.6132) | Error 0.9122(0.8999) Steps 790(798.54) | Grad Norm 3.8492(3.3299) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 19.8806(20.1502) | Bit/dim 3.6109(3.6129) | Xent 2.3026(2.3026) | Loss 3.6109(3.6129) | Error 0.8900(0.8999) Steps 796(797.29) | Grad Norm 2.4202(3.4532) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 21.0364(20.2455) | Bit/dim 3.6254(3.6141) | Xent 2.3026(2.3026) | Loss 3.6254(3.6141) | Error 0.8933(0.8998) Steps 802(798.81) | Grad Norm 3.7068(3.4530) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 19.7835(20.1913) | Bit/dim 3.6231(3.6121) | Xent 2.3026(2.3026) | Loss 3.6231(3.6121) | Error 0.9056(0.9002) Steps 796(796.74) | Grad Norm 1.9744(3.1892) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 20.3904(20.2520) | Bit/dim 3.5944(3.6107) | Xent 2.3026(2.3026) | Loss 3.5944(3.6107) | Error 0.8922(0.9008) Steps 808(796.95) | Grad Norm 4.5335(3.0253) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 20.6218(20.2427) | Bit/dim 3.6658(3.6120) | Xent 2.3026(2.3026) | Loss 3.6658(3.6120) | Error 0.8922(0.9000) Steps 802(797.61) | Grad Norm 3.7113(3.4275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 100.5889, Epoch Time 1231.4134(1138.7063), Bit/dim 3.6112(best: 3.6173), Xent 2.3026, Loss 3.6112, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 19.9811(20.2303) | Bit/dim 3.6183(3.6094) | Xent 2.3026(2.3026) | Loss 3.6183(3.6094) | Error 0.9189(0.9000) Steps 790(797.41) | Grad Norm 3.3833(3.3147) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 19.9625(20.2049) | Bit/dim 3.6120(3.6093) | Xent 2.3026(2.3026) | Loss 3.6120(3.6093) | Error 0.9133(0.9003) Steps 796(797.34) | Grad Norm 3.0678(3.3187) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 19.9394(20.1405) | Bit/dim 3.5798(3.6065) | Xent 2.3026(2.3026) | Loss 3.5798(3.6065) | Error 0.8900(0.9002) Steps 790(796.46) | Grad Norm 2.4725(3.2238) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 19.8190(20.0414) | Bit/dim 3.5882(3.6068) | Xent 2.3026(2.3026) | Loss 3.5882(3.6068) | Error 0.8922(0.8988) Steps 808(795.64) | Grad Norm 2.7265(3.3435) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 19.8910(20.1047) | Bit/dim 3.6215(3.6063) | Xent 2.3026(2.3026) | Loss 3.6215(3.6063) | Error 0.8911(0.8994) Steps 790(796.61) | Grad Norm 2.8453(3.3390) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 100.2739, Epoch Time 1224.0787(1141.2675), Bit/dim 3.6016(best: 3.6112), Xent 2.3026, Loss 3.6016, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 21.1815(20.1552) | Bit/dim 3.6353(3.6042) | Xent 2.3026(2.3026) | Loss 3.6353(3.6042) | Error 0.9200(0.8997) Steps 814(797.11) | Grad Norm 2.4650(3.1725) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 20.1443(20.1586) | Bit/dim 3.6140(3.6035) | Xent 2.3026(2.3026) | Loss 3.6140(3.6035) | Error 0.9078(0.8997) Steps 784(794.94) | Grad Norm 2.9388(3.3601) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 20.3823(20.1667) | Bit/dim 3.6005(3.6040) | Xent 2.3026(2.3026) | Loss 3.6005(3.6040) | Error 0.9178(0.9006) Steps 808(795.69) | Grad Norm 3.5793(3.4733) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 19.9490(20.0906) | Bit/dim 3.5590(3.6027) | Xent 2.3026(2.3026) | Loss 3.5590(3.6027) | Error 0.9089(0.9000) Steps 808(795.49) | Grad Norm 2.6888(3.2449) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 20.3103(20.1472) | Bit/dim 3.6200(3.6043) | Xent 2.3026(2.3026) | Loss 3.6200(3.6043) | Error 0.9056(0.9017) Steps 790(795.25) | Grad Norm 4.3047(3.4779) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 19.4855(20.1054) | Bit/dim 3.5923(3.6012) | Xent 2.3026(2.3026) | Loss 3.5923(3.6012) | Error 0.9044(0.8998) Steps 790(795.18) | Grad Norm 2.5937(3.3383) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 100.2091, Epoch Time 1225.2062(1143.7856), Bit/dim 3.6029(best: 3.6016), Xent 2.3026, Loss 3.6029, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 19.8814(20.0268) | Bit/dim 3.5904(3.6018) | Xent 2.3026(2.3026) | Loss 3.5904(3.6018) | Error 0.9089(0.9003) Steps 796(794.48) | Grad Norm 2.8875(3.2062) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 20.1733(20.0558) | Bit/dim 3.6000(3.6005) | Xent 2.3026(2.3026) | Loss 3.6000(3.6005) | Error 0.8822(0.9001) Steps 814(796.55) | Grad Norm 3.4831(3.1034) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 20.8570(20.0588) | Bit/dim 3.6058(3.5981) | Xent 2.3026(2.3026) | Loss 3.6058(3.5981) | Error 0.9111(0.9006) Steps 802(795.78) | Grad Norm 2.8174(3.1944) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 20.0986(20.0542) | Bit/dim 3.6304(3.5991) | Xent 2.3026(2.3026) | Loss 3.6304(3.5991) | Error 0.9156(0.9004) Steps 796(795.32) | Grad Norm 1.7049(2.9901) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 20.7480(20.0798) | Bit/dim 3.6125(3.5985) | Xent 2.3026(2.3026) | Loss 3.6125(3.5985) | Error 0.8922(0.9003) Steps 790(794.85) | Grad Norm 3.9888(3.2677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 99.4285, Epoch Time 1221.7744(1146.1253), Bit/dim 3.5964(best: 3.6016), Xent 2.3026, Loss 3.5964, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 20.0453(20.1305) | Bit/dim 3.6071(3.5966) | Xent 2.3026(2.3026) | Loss 3.6071(3.5966) | Error 0.9033(0.8995) Steps 784(795.58) | Grad Norm 3.1604(3.2516) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 20.4562(20.0932) | Bit/dim 3.5748(3.5955) | Xent 2.3026(2.3026) | Loss 3.5748(3.5955) | Error 0.8967(0.8999) Steps 796(795.55) | Grad Norm 3.1064(3.4066) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 20.4701(20.1061) | Bit/dim 3.6415(3.5965) | Xent 2.3026(2.3026) | Loss 3.6415(3.5965) | Error 0.9122(0.9000) Steps 796(797.05) | Grad Norm 3.4561(3.3342) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 19.7799(20.0530) | Bit/dim 3.5757(3.5947) | Xent 2.3026(2.3026) | Loss 3.5757(3.5947) | Error 0.9067(0.9000) Steps 796(797.20) | Grad Norm 4.5444(3.4091) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 20.1401(20.0668) | Bit/dim 3.5849(3.5934) | Xent 2.3026(2.3026) | Loss 3.5849(3.5934) | Error 0.8922(0.9001) Steps 796(796.75) | Grad Norm 2.4666(3.2232) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 19.7599(20.0066) | Bit/dim 3.5650(3.5904) | Xent 2.3026(2.3026) | Loss 3.5650(3.5904) | Error 0.9144(0.8998) Steps 790(796.83) | Grad Norm 2.7851(3.2369) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 101.1707, Epoch Time 1219.2678(1148.3196), Bit/dim 3.5889(best: 3.5964), Xent 2.3026, Loss 3.5889, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 19.9105(19.9722) | Bit/dim 3.6094(3.5892) | Xent 2.3026(2.3026) | Loss 3.6094(3.5892) | Error 0.9022(0.9003) Steps 796(797.81) | Grad Norm 5.1733(3.3577) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 19.8401(19.9636) | Bit/dim 3.6036(3.5890) | Xent 2.3026(2.3026) | Loss 3.6036(3.5890) | Error 0.9022(0.9000) Steps 802(797.35) | Grad Norm 3.0811(3.3588) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 19.6328(19.9414) | Bit/dim 3.6070(3.5899) | Xent 2.3026(2.3026) | Loss 3.6070(3.5899) | Error 0.8878(0.8992) Steps 796(797.51) | Grad Norm 4.2197(3.2476) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 19.6150(19.9180) | Bit/dim 3.5591(3.5894) | Xent 2.3026(2.3026) | Loss 3.5591(3.5894) | Error 0.8944(0.8992) Steps 790(796.78) | Grad Norm 2.1461(3.0894) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 19.4672(19.9391) | Bit/dim 3.5907(3.5884) | Xent 2.3026(2.3026) | Loss 3.5907(3.5884) | Error 0.8889(0.8999) Steps 802(797.01) | Grad Norm 3.4708(3.3195) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 101.7862, Epoch Time 1215.7258(1150.3418), Bit/dim 3.5886(best: 3.5889), Xent 2.3026, Loss 3.5886, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 20.5219(19.9535) | Bit/dim 3.5397(3.5880) | Xent 2.3026(2.3026) | Loss 3.5397(3.5880) | Error 0.8889(0.8994) Steps 802(798.04) | Grad Norm 2.0292(3.2286) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 20.2994(19.9644) | Bit/dim 3.5844(3.5856) | Xent 2.3026(2.3026) | Loss 3.5844(3.5856) | Error 0.9078(0.9002) Steps 802(798.75) | Grad Norm 2.1323(3.2208) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 19.5363(19.9500) | Bit/dim 3.5679(3.5847) | Xent 2.3026(2.3026) | Loss 3.5679(3.5847) | Error 0.9000(0.8996) Steps 802(798.45) | Grad Norm 4.4415(3.2063) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 19.8912(20.0162) | Bit/dim 3.5934(3.5835) | Xent 2.3026(2.3026) | Loss 3.5934(3.5835) | Error 0.8933(0.9002) Steps 802(800.01) | Grad Norm 3.4794(3.2827) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 20.4539(20.0526) | Bit/dim 3.5905(3.5859) | Xent 2.3026(2.3026) | Loss 3.5905(3.5859) | Error 0.9056(0.9007) Steps 808(800.36) | Grad Norm 1.7634(3.0770) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 21.4783(20.1375) | Bit/dim 3.5588(3.5852) | Xent 2.3026(2.3026) | Loss 3.5588(3.5852) | Error 0.8789(0.8999) Steps 808(800.65) | Grad Norm 3.2549(3.1730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 101.3702, Epoch Time 1225.6738(1152.6017), Bit/dim 3.5878(best: 3.5886), Xent 2.3026, Loss 3.5878, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 19.5824(20.0856) | Bit/dim 3.5625(3.5844) | Xent 2.3026(2.3026) | Loss 3.5625(3.5844) | Error 0.8956(0.8992) Steps 802(800.48) | Grad Norm 1.9500(3.1026) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 19.3014(20.0513) | Bit/dim 3.5855(3.5827) | Xent 2.3026(2.3026) | Loss 3.5855(3.5827) | Error 0.9233(0.9003) Steps 802(800.08) | Grad Norm 4.4192(3.2657) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 19.7512(20.0024) | Bit/dim 3.6177(3.5811) | Xent 2.3026(2.3026) | Loss 3.6177(3.5811) | Error 0.9211(0.8998) Steps 790(800.26) | Grad Norm 2.0337(2.9104) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 19.9242(20.0076) | Bit/dim 3.5805(3.5817) | Xent 2.3026(2.3026) | Loss 3.5805(3.5817) | Error 0.9089(0.9010) Steps 790(799.66) | Grad Norm 1.7564(3.0632) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 20.2101(20.0139) | Bit/dim 3.5829(3.5809) | Xent 2.3026(2.3026) | Loss 3.5829(3.5809) | Error 0.9133(0.9007) Steps 796(799.95) | Grad Norm 2.7638(3.0971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 100.9019, Epoch Time 1216.0404(1154.5049), Bit/dim 3.5760(best: 3.5878), Xent 2.3026, Loss 3.5760, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 19.3515(19.9608) | Bit/dim 3.5864(3.5790) | Xent 2.3026(2.3026) | Loss 3.5864(3.5790) | Error 0.8900(0.8999) Steps 796(799.99) | Grad Norm 3.5642(3.0600) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 19.3831(19.9243) | Bit/dim 3.5806(3.5781) | Xent 2.3026(2.3026) | Loss 3.5806(3.5781) | Error 0.8978(0.8993) Steps 802(800.05) | Grad Norm 5.2273(3.3652) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 19.7298(19.9881) | Bit/dim 3.6074(3.5778) | Xent 2.3026(2.3026) | Loss 3.6074(3.5778) | Error 0.9133(0.9003) Steps 802(800.13) | Grad Norm 1.9752(3.1955) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 20.8649(19.9113) | Bit/dim 3.5665(3.5785) | Xent 2.3026(2.3026) | Loss 3.5665(3.5785) | Error 0.8933(0.9004) Steps 814(799.86) | Grad Norm 2.3234(3.2389) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 20.2363(19.8878) | Bit/dim 3.5851(3.5783) | Xent 2.3026(2.3026) | Loss 3.5851(3.5783) | Error 0.8833(0.9003) Steps 808(800.24) | Grad Norm 3.6999(3.1399) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 20.0824(19.8000) | Bit/dim 3.5421(3.5766) | Xent 2.3026(2.3026) | Loss 3.5421(3.5766) | Error 0.8944(0.8996) Steps 808(800.01) | Grad Norm 3.0857(3.3223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 101.9760, Epoch Time 1209.5513(1156.1563), Bit/dim 3.5750(best: 3.5760), Xent 2.3026, Loss 3.5750, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 19.8605(19.7833) | Bit/dim 3.6048(3.5762) | Xent 2.3026(2.3026) | Loss 3.6048(3.5762) | Error 0.9156(0.9007) Steps 790(800.06) | Grad Norm 1.5844(2.9998) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 19.0691(19.7555) | Bit/dim 3.6107(3.5776) | Xent 2.3026(2.3026) | Loss 3.6107(3.5776) | Error 0.9100(0.9016) Steps 796(800.05) | Grad Norm 3.6476(3.2856) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 19.5817(19.7990) | Bit/dim 3.5569(3.5764) | Xent 2.3026(2.3026) | Loss 3.5569(3.5764) | Error 0.8967(0.9001) Steps 796(800.28) | Grad Norm 3.0373(3.1912) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 19.8379(19.8268) | Bit/dim 3.5721(3.5721) | Xent 2.3026(2.3026) | Loss 3.5721(3.5721) | Error 0.8922(0.8980) Steps 802(800.16) | Grad Norm 2.4551(2.9231) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 20.1072(19.7961) | Bit/dim 3.5570(3.5694) | Xent 2.3026(2.3026) | Loss 3.5570(3.5694) | Error 0.9011(0.8983) Steps 814(799.92) | Grad Norm 4.1944(3.0830) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 102.5006, Epoch Time 1209.2714(1157.7497), Bit/dim 3.5741(best: 3.5750), Xent 2.3026, Loss 3.5741, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 20.2026(19.7764) | Bit/dim 3.5625(3.5736) | Xent 2.3026(2.3026) | Loss 3.5625(3.5736) | Error 0.8911(0.8993) Steps 796(799.82) | Grad Norm 2.5533(2.9991) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 20.1815(19.8463) | Bit/dim 3.5890(3.5732) | Xent 2.3026(2.3026) | Loss 3.5890(3.5732) | Error 0.9122(0.8993) Steps 802(799.42) | Grad Norm 4.7039(2.9849) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 19.9600(19.8257) | Bit/dim 3.5577(3.5713) | Xent 2.3026(2.3026) | Loss 3.5577(3.5713) | Error 0.9133(0.9001) Steps 796(799.43) | Grad Norm 2.0745(2.9056) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 19.4267(19.6984) | Bit/dim 3.5726(3.5710) | Xent 2.3026(2.3026) | Loss 3.5726(3.5710) | Error 0.9100(0.9010) Steps 796(799.42) | Grad Norm 4.3162(3.0807) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 18.9190(19.6344) | Bit/dim 3.5712(3.5680) | Xent 2.3026(2.3026) | Loss 3.5712(3.5680) | Error 0.9056(0.9004) Steps 796(798.17) | Grad Norm 3.2018(2.9142) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 18.9639(19.5682) | Bit/dim 3.6052(3.5706) | Xent 2.3026(2.3026) | Loss 3.6052(3.5706) | Error 0.8911(0.9002) Steps 796(797.46) | Grad Norm 2.8179(2.9461) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 101.3293, Epoch Time 1197.7337(1158.9492), Bit/dim 3.5711(best: 3.5741), Xent 2.3026, Loss 3.5711, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 19.1748(19.5929) | Bit/dim 3.5709(3.5707) | Xent 2.3026(2.3026) | Loss 3.5709(3.5707) | Error 0.8911(0.9002) Steps 796(797.08) | Grad Norm 3.1284(3.0538) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 19.6779(19.5725) | Bit/dim 3.5561(3.5693) | Xent 2.3026(2.3026) | Loss 3.5561(3.5693) | Error 0.8878(0.9009) Steps 802(797.80) | Grad Norm 3.1441(3.2524) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 19.8097(19.6081) | Bit/dim 3.5206(3.5666) | Xent 2.3026(2.3026) | Loss 3.5206(3.5666) | Error 0.9000(0.9002) Steps 802(799.50) | Grad Norm 2.0730(3.1109) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 18.7393(19.5778) | Bit/dim 3.5294(3.5658) | Xent 2.3026(2.3026) | Loss 3.5294(3.5658) | Error 0.8978(0.8984) Steps 796(799.35) | Grad Norm 3.9307(3.2070) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 18.9002(19.5775) | Bit/dim 3.5330(3.5654) | Xent 2.3026(2.3026) | Loss 3.5330(3.5654) | Error 0.8989(0.8998) Steps 802(799.43) | Grad Norm 2.1072(2.9978) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 103.1254, Epoch Time 1200.4847(1160.1953), Bit/dim 3.5648(best: 3.5711), Xent 2.3026, Loss 3.5648, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 19.2888(19.6162) | Bit/dim 3.5497(3.5657) | Xent 2.3026(2.3026) | Loss 3.5497(3.5657) | Error 0.9033(0.8991) Steps 802(799.98) | Grad Norm 4.4191(3.2080) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 19.2125(19.6480) | Bit/dim 3.5589(3.5665) | Xent 2.3026(2.3026) | Loss 3.5589(3.5665) | Error 0.8900(0.8994) Steps 802(800.49) | Grad Norm 4.2259(3.2587) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 19.3852(19.7177) | Bit/dim 3.5478(3.5664) | Xent 2.3026(2.3026) | Loss 3.5478(3.5664) | Error 0.8944(0.8992) Steps 790(800.82) | Grad Norm 2.9563(3.0848) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 20.0293(19.7572) | Bit/dim 3.5606(3.5654) | Xent 2.3026(2.3026) | Loss 3.5606(3.5654) | Error 0.8922(0.9001) Steps 808(800.81) | Grad Norm 2.1738(2.9378) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 19.5703(19.7638) | Bit/dim 3.5722(3.5628) | Xent 2.3026(2.3026) | Loss 3.5722(3.5628) | Error 0.8956(0.9004) Steps 802(800.95) | Grad Norm 6.1610(3.0368) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 19.3229(19.6806) | Bit/dim 3.5470(3.5639) | Xent 2.3026(2.3026) | Loss 3.5470(3.5639) | Error 0.8889(0.9000) Steps 796(799.15) | Grad Norm 3.3808(3.1873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 102.1694, Epoch Time 1205.9643(1161.5684), Bit/dim 3.5661(best: 3.5648), Xent 2.3026, Loss 3.5661, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 19.7991(19.6732) | Bit/dim 3.5635(3.5636) | Xent 2.3026(2.3026) | Loss 3.5635(3.5636) | Error 0.8911(0.8990) Steps 802(799.45) | Grad Norm 2.0155(2.9174) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 19.7665(19.5897) | Bit/dim 3.5956(3.5624) | Xent 2.3026(2.3026) | Loss 3.5956(3.5624) | Error 0.9056(0.9004) Steps 802(799.47) | Grad Norm 2.0752(2.9388) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 19.7734(19.6001) | Bit/dim 3.6005(3.5632) | Xent 2.3026(2.3026) | Loss 3.6005(3.5632) | Error 0.9011(0.8996) Steps 808(799.55) | Grad Norm 4.4266(3.2674) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 19.8574(19.6362) | Bit/dim 3.5514(3.5628) | Xent 2.3026(2.3026) | Loss 3.5514(3.5628) | Error 0.8956(0.8996) Steps 802(800.94) | Grad Norm 3.1359(3.1066) | Total Time 14.00(14.00)\n",
      "Epoch 0061 | Time 101.5174, Epoch Time 1201.4090(1162.7636), Bit/dim 3.5581(best: 3.5648), Xent 2.3026, Loss 3.5581, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 20.2700(19.7522) | Bit/dim 3.5295(3.5562) | Xent 2.3026(2.3026) | Loss 3.5295(3.5562) | Error 0.8900(0.8996) Steps 808(801.76) | Grad Norm 2.8332(3.0553) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 19.6976(19.8262) | Bit/dim 3.5902(3.5557) | Xent 2.3026(2.3026) | Loss 3.5902(3.5557) | Error 0.9133(0.8996) Steps 802(801.67) | Grad Norm 2.3621(3.0763) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 19.4922(19.8363) | Bit/dim 3.5242(3.5550) | Xent 2.3026(2.3026) | Loss 3.5242(3.5550) | Error 0.9056(0.8999) Steps 808(802.85) | Grad Norm 2.8870(2.9376) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 19.9889(19.8904) | Bit/dim 3.5371(3.5576) | Xent 2.3026(2.3026) | Loss 3.5371(3.5576) | Error 0.9000(0.9006) Steps 814(803.24) | Grad Norm 2.3244(2.9962) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 19.6335(19.8464) | Bit/dim 3.5281(3.5588) | Xent 2.3026(2.3026) | Loss 3.5281(3.5588) | Error 0.8844(0.9010) Steps 802(803.85) | Grad Norm 3.6390(3.1154) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 20.1022(19.8444) | Bit/dim 3.5795(3.5580) | Xent 2.3026(2.3026) | Loss 3.5795(3.5580) | Error 0.8911(0.9001) Steps 802(803.92) | Grad Norm 1.7892(2.8427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 102.2100, Epoch Time 1214.5237(1164.3164), Bit/dim 3.5533(best: 3.5581), Xent 2.3026, Loss 3.5533, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 19.8744(19.8374) | Bit/dim 3.5425(3.5571) | Xent 2.3026(2.3026) | Loss 3.5425(3.5571) | Error 0.8911(0.9000) Steps 826(804.74) | Grad Norm 2.8424(2.8317) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 20.1559(19.8217) | Bit/dim 3.5651(3.5563) | Xent 2.3026(2.3026) | Loss 3.5651(3.5563) | Error 0.8889(0.8983) Steps 808(804.73) | Grad Norm 2.2882(2.9677) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 20.5221(19.8128) | Bit/dim 3.5458(3.5555) | Xent 2.3026(2.3026) | Loss 3.5458(3.5555) | Error 0.9056(0.8986) Steps 808(803.94) | Grad Norm 4.3411(2.9749) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 20.0485(19.9088) | Bit/dim 3.5720(3.5536) | Xent 2.3026(2.3026) | Loss 3.5720(3.5536) | Error 0.9189(0.8992) Steps 808(803.79) | Grad Norm 4.9617(3.0743) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 20.6597(19.9608) | Bit/dim 3.5577(3.5549) | Xent 2.3026(2.3026) | Loss 3.5577(3.5549) | Error 0.8933(0.9003) Steps 808(805.97) | Grad Norm 4.8297(3.1329) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 102.6394, Epoch Time 1218.0729(1165.9291), Bit/dim 3.5530(best: 3.5533), Xent 2.3026, Loss 3.5530, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 19.7647(19.9756) | Bit/dim 3.5650(3.5566) | Xent 2.3026(2.3026) | Loss 3.5650(3.5566) | Error 0.9033(0.9008) Steps 802(806.13) | Grad Norm 2.7060(3.1471) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 20.3929(19.9926) | Bit/dim 3.5268(3.5532) | Xent 2.3026(2.3026) | Loss 3.5268(3.5532) | Error 0.9033(0.9013) Steps 820(807.71) | Grad Norm 3.1208(2.8382) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 20.2741(20.0339) | Bit/dim 3.5636(3.5539) | Xent 2.3026(2.3026) | Loss 3.5636(3.5539) | Error 0.9167(0.9017) Steps 814(809.42) | Grad Norm 2.8024(2.9009) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 19.4860(20.0177) | Bit/dim 3.5702(3.5535) | Xent 2.3026(2.3026) | Loss 3.5702(3.5535) | Error 0.8989(0.9015) Steps 796(808.92) | Grad Norm 2.7487(2.9762) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 19.9917(19.9642) | Bit/dim 3.5399(3.5518) | Xent 2.3026(2.3026) | Loss 3.5399(3.5518) | Error 0.9022(0.9011) Steps 820(808.54) | Grad Norm 4.6675(3.0358) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 19.4417(19.9473) | Bit/dim 3.5920(3.5495) | Xent 2.3026(2.3026) | Loss 3.5920(3.5495) | Error 0.9044(0.8997) Steps 808(807.91) | Grad Norm 3.3686(3.0488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 103.1150, Epoch Time 1219.6418(1167.5405), Bit/dim 3.5483(best: 3.5530), Xent 2.3026, Loss 3.5483, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 20.1095(19.9550) | Bit/dim 3.5510(3.5459) | Xent 2.3026(2.3026) | Loss 3.5510(3.5459) | Error 0.8911(0.8985) Steps 808(808.29) | Grad Norm 2.9097(2.9464) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 19.9819(19.9223) | Bit/dim 3.5621(3.5481) | Xent 2.3026(2.3026) | Loss 3.5621(3.5481) | Error 0.8922(0.8986) Steps 790(807.32) | Grad Norm 3.0156(2.9885) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 19.9045(19.8827) | Bit/dim 3.5434(3.5507) | Xent 2.3026(2.3026) | Loss 3.5434(3.5507) | Error 0.8856(0.8989) Steps 796(806.82) | Grad Norm 2.5718(3.0148) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 19.8286(19.8095) | Bit/dim 3.5854(3.5496) | Xent 2.3026(2.3026) | Loss 3.5854(3.5496) | Error 0.9089(0.8999) Steps 814(807.61) | Grad Norm 4.3607(3.0806) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 19.5968(19.7733) | Bit/dim 3.5430(3.5473) | Xent 2.3026(2.3026) | Loss 3.5430(3.5473) | Error 0.9278(0.9012) Steps 814(806.95) | Grad Norm 2.0964(2.9181) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 102.7682, Epoch Time 1210.3842(1168.8258), Bit/dim 3.5439(best: 3.5483), Xent 2.3026, Loss 3.5439, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 19.4791(19.8331) | Bit/dim 3.5332(3.5457) | Xent 2.3026(2.3026) | Loss 3.5332(3.5457) | Error 0.8989(0.9005) Steps 814(809.38) | Grad Norm 3.2182(2.7132) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 20.4563(19.8863) | Bit/dim 3.5367(3.5456) | Xent 2.3026(2.3026) | Loss 3.5367(3.5456) | Error 0.8856(0.9001) Steps 826(808.75) | Grad Norm 2.3506(2.8767) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 19.8787(19.9430) | Bit/dim 3.5281(3.5464) | Xent 2.3026(2.3026) | Loss 3.5281(3.5464) | Error 0.9178(0.9005) Steps 808(809.16) | Grad Norm 2.2274(2.8807) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 20.0445(20.0270) | Bit/dim 3.5293(3.5466) | Xent 2.3026(2.3026) | Loss 3.5293(3.5466) | Error 0.9000(0.8990) Steps 802(810.13) | Grad Norm 1.7393(3.0647) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 19.8930(19.9613) | Bit/dim 3.5241(3.5461) | Xent 2.3026(2.3026) | Loss 3.5241(3.5461) | Error 0.8989(0.9000) Steps 808(808.78) | Grad Norm 2.1383(3.0629) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 21.3628(20.0817) | Bit/dim 3.5347(3.5440) | Xent 2.3026(2.3026) | Loss 3.5347(3.5440) | Error 0.9089(0.9010) Steps 808(810.30) | Grad Norm 3.1631(2.7881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 102.9735, Epoch Time 1225.6681(1170.5311), Bit/dim 3.5454(best: 3.5439), Xent 2.3026, Loss 3.5454, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 19.5402(20.0055) | Bit/dim 3.5393(3.5440) | Xent 2.3026(2.3026) | Loss 3.5393(3.5440) | Error 0.9111(0.9004) Steps 814(810.19) | Grad Norm 2.2814(2.9069) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 20.5206(20.1064) | Bit/dim 3.5658(3.5451) | Xent 2.3026(2.3026) | Loss 3.5658(3.5451) | Error 0.8889(0.9003) Steps 802(811.04) | Grad Norm 3.0373(3.0688) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 19.1652(20.0820) | Bit/dim 3.5319(3.5438) | Xent 2.3026(2.3026) | Loss 3.5319(3.5438) | Error 0.8933(0.9008) Steps 802(810.70) | Grad Norm 1.7866(2.8440) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 20.4063(20.0215) | Bit/dim 3.5077(3.5425) | Xent 2.3026(2.3026) | Loss 3.5077(3.5425) | Error 0.8989(0.9011) Steps 838(811.67) | Grad Norm 2.0374(2.9355) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 19.3975(19.9853) | Bit/dim 3.5247(3.5406) | Xent 2.3026(2.3026) | Loss 3.5247(3.5406) | Error 0.8900(0.8998) Steps 802(811.45) | Grad Norm 2.4444(2.8701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 103.6576, Epoch Time 1220.8956(1172.0420), Bit/dim 3.5455(best: 3.5439), Xent 2.3026, Loss 3.5455, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 19.5165(19.9242) | Bit/dim 3.5177(3.5428) | Xent 2.3026(2.3026) | Loss 3.5177(3.5428) | Error 0.8856(0.9004) Steps 814(810.76) | Grad Norm 2.9329(2.9854) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 19.7457(19.8985) | Bit/dim 3.5226(3.5420) | Xent 2.3026(2.3026) | Loss 3.5226(3.5420) | Error 0.9022(0.9000) Steps 802(810.28) | Grad Norm 2.3902(2.7787) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 19.2155(19.9240) | Bit/dim 3.5658(3.5419) | Xent 2.3026(2.3026) | Loss 3.5658(3.5419) | Error 0.9044(0.8991) Steps 808(810.38) | Grad Norm 3.5025(2.9810) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 20.0715(19.9652) | Bit/dim 3.5415(3.5415) | Xent 2.3026(2.3026) | Loss 3.5415(3.5415) | Error 0.8956(0.8997) Steps 808(810.09) | Grad Norm 2.6910(2.7946) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 20.7353(20.0591) | Bit/dim 3.5009(3.5386) | Xent 2.3026(2.3026) | Loss 3.5009(3.5386) | Error 0.8822(0.9004) Steps 826(810.80) | Grad Norm 3.0564(2.7303) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 20.7068(20.0841) | Bit/dim 3.5250(3.5385) | Xent 2.3026(2.3026) | Loss 3.5250(3.5385) | Error 0.9122(0.9003) Steps 802(810.09) | Grad Norm 2.6487(2.8703) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 103.0633, Epoch Time 1222.4492(1173.5542), Bit/dim 3.5387(best: 3.5439), Xent 2.3026, Loss 3.5387, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 20.3178(20.0827) | Bit/dim 3.5480(3.5390) | Xent 2.3026(2.3026) | Loss 3.5480(3.5390) | Error 0.8922(0.9000) Steps 832(813.56) | Grad Norm 5.3088(2.9817) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 20.2270(20.1008) | Bit/dim 3.5056(3.5395) | Xent 2.3026(2.3026) | Loss 3.5056(3.5395) | Error 0.8967(0.9003) Steps 796(812.26) | Grad Norm 2.7654(2.9099) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 19.6166(20.0715) | Bit/dim 3.5093(3.5351) | Xent 2.3026(2.3026) | Loss 3.5093(3.5351) | Error 0.8967(0.9003) Steps 808(812.17) | Grad Norm 3.0154(2.8411) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 20.1873(20.1056) | Bit/dim 3.5010(3.5354) | Xent 2.3026(2.3026) | Loss 3.5010(3.5354) | Error 0.8878(0.8997) Steps 832(810.60) | Grad Norm 1.8985(2.8175) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 20.3968(20.1566) | Bit/dim 3.5461(3.5366) | Xent 2.3026(2.3026) | Loss 3.5461(3.5366) | Error 0.8800(0.8991) Steps 826(811.88) | Grad Norm 2.9235(2.8920) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 103.0983, Epoch Time 1228.0820(1175.1900), Bit/dim 3.5376(best: 3.5387), Xent 2.3026, Loss 3.5376, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 20.3540(20.1671) | Bit/dim 3.5670(3.5372) | Xent 2.3026(2.3026) | Loss 3.5670(3.5372) | Error 0.8956(0.8999) Steps 808(810.64) | Grad Norm 1.8351(2.7921) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 20.0806(20.2523) | Bit/dim 3.5156(3.5327) | Xent 2.3026(2.3026) | Loss 3.5156(3.5327) | Error 0.8978(0.8990) Steps 820(812.51) | Grad Norm 3.4367(2.7209) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 20.1578(20.2729) | Bit/dim 3.5479(3.5333) | Xent 2.3026(2.3026) | Loss 3.5479(3.5333) | Error 0.9033(0.8998) Steps 808(812.45) | Grad Norm 2.4490(2.6641) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 19.9212(20.3129) | Bit/dim 3.5156(3.5335) | Xent 2.3026(2.3026) | Loss 3.5156(3.5335) | Error 0.8933(0.8993) Steps 814(811.75) | Grad Norm 3.4125(2.7932) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 20.4344(20.2594) | Bit/dim 3.5441(3.5374) | Xent 2.3026(2.3026) | Loss 3.5441(3.5374) | Error 0.9078(0.9008) Steps 814(811.15) | Grad Norm 1.3183(2.9176) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 20.1654(20.2528) | Bit/dim 3.4935(3.5351) | Xent 2.3026(2.3026) | Loss 3.4935(3.5351) | Error 0.9089(0.9005) Steps 802(811.79) | Grad Norm 4.0828(2.8484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 104.6615, Epoch Time 1240.2468(1177.1417), Bit/dim 3.5401(best: 3.5376), Xent 2.3026, Loss 3.5401, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 20.9534(20.3451) | Bit/dim 3.5415(3.5340) | Xent 2.3026(2.3026) | Loss 3.5415(3.5340) | Error 0.9089(0.9000) Steps 820(811.87) | Grad Norm 3.0807(2.9650) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 20.5683(20.3732) | Bit/dim 3.5321(3.5337) | Xent 2.3026(2.3026) | Loss 3.5321(3.5337) | Error 0.9111(0.9010) Steps 832(813.54) | Grad Norm 2.2781(2.8117) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 20.7035(20.3752) | Bit/dim 3.5219(3.5326) | Xent 2.3026(2.3026) | Loss 3.5219(3.5326) | Error 0.9100(0.9000) Steps 844(813.98) | Grad Norm 4.5407(2.9682) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 19.8653(20.3790) | Bit/dim 3.5424(3.5367) | Xent 2.3026(2.3026) | Loss 3.5424(3.5367) | Error 0.9144(0.9001) Steps 814(813.44) | Grad Norm 3.2642(2.9970) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 20.6880(20.3858) | Bit/dim 3.5325(3.5332) | Xent 2.3026(2.3026) | Loss 3.5325(3.5332) | Error 0.8933(0.9001) Steps 826(813.28) | Grad Norm 2.5788(2.9819) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 103.8017, Epoch Time 1245.8448(1179.2028), Bit/dim 3.5343(best: 3.5376), Xent 2.3026, Loss 3.5343, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 20.5746(20.3763) | Bit/dim 3.4951(3.5309) | Xent 2.3026(2.3026) | Loss 3.4951(3.5309) | Error 0.8967(0.9002) Steps 814(812.85) | Grad Norm 2.5795(2.9468) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 20.1526(20.3438) | Bit/dim 3.5533(3.5309) | Xent 2.3026(2.3026) | Loss 3.5533(3.5309) | Error 0.8956(0.9007) Steps 820(814.04) | Grad Norm 1.5483(2.6445) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 19.9972(20.3735) | Bit/dim 3.5516(3.5305) | Xent 2.3026(2.3026) | Loss 3.5516(3.5305) | Error 0.8956(0.9000) Steps 820(814.65) | Grad Norm 4.1565(2.6728) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 19.9287(20.3232) | Bit/dim 3.5351(3.5300) | Xent 2.3026(2.3026) | Loss 3.5351(3.5300) | Error 0.8867(0.9005) Steps 808(812.08) | Grad Norm 4.8147(2.6447) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 19.9734(20.3029) | Bit/dim 3.5208(3.5296) | Xent 2.3026(2.3026) | Loss 3.5208(3.5296) | Error 0.8844(0.9003) Steps 820(811.57) | Grad Norm 2.8982(2.8584) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 20.4713(20.2962) | Bit/dim 3.4971(3.5293) | Xent 2.3026(2.3026) | Loss 3.4971(3.5293) | Error 0.8844(0.8991) Steps 826(812.11) | Grad Norm 2.8663(2.8863) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 102.8998, Epoch Time 1237.2998(1180.9457), Bit/dim 3.5276(best: 3.5343), Xent 2.3026, Loss 3.5276, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 20.6823(20.3356) | Bit/dim 3.4853(3.5294) | Xent 2.3026(2.3026) | Loss 3.4853(3.5294) | Error 0.9100(0.8991) Steps 796(810.30) | Grad Norm 2.2645(2.9255) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 20.3464(20.3334) | Bit/dim 3.5100(3.5268) | Xent 2.3026(2.3026) | Loss 3.5100(3.5268) | Error 0.8944(0.8998) Steps 802(810.28) | Grad Norm 3.4359(2.7532) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 20.6999(20.3216) | Bit/dim 3.5554(3.5283) | Xent 2.3026(2.3026) | Loss 3.5554(3.5283) | Error 0.9022(0.8993) Steps 826(809.40) | Grad Norm 3.8098(2.9615) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 20.1859(20.3324) | Bit/dim 3.4974(3.5291) | Xent 2.3026(2.3026) | Loss 3.4974(3.5291) | Error 0.8978(0.8996) Steps 826(812.29) | Grad Norm 3.0223(2.8527) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 20.7028(20.3688) | Bit/dim 3.5608(3.5268) | Xent 2.3026(2.3026) | Loss 3.5608(3.5268) | Error 0.9089(0.8990) Steps 826(815.65) | Grad Norm 3.7025(3.0861) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 103.1474, Epoch Time 1240.4642(1182.7313), Bit/dim 3.5258(best: 3.5276), Xent 2.3026, Loss 3.5258, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 20.5500(20.3590) | Bit/dim 3.5517(3.5279) | Xent 2.3026(2.3026) | Loss 3.5517(3.5279) | Error 0.9000(0.8997) Steps 808(815.62) | Grad Norm 1.4941(2.8267) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 20.3622(20.3941) | Bit/dim 3.5492(3.5277) | Xent 2.3026(2.3026) | Loss 3.5492(3.5277) | Error 0.9056(0.9001) Steps 838(817.31) | Grad Norm 3.2651(2.7905) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 20.1314(20.3994) | Bit/dim 3.5443(3.5248) | Xent 2.3026(2.3026) | Loss 3.5443(3.5248) | Error 0.9022(0.8998) Steps 820(816.55) | Grad Norm 2.2752(2.8134) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 20.0055(20.4461) | Bit/dim 3.5127(3.5252) | Xent 2.3026(2.3026) | Loss 3.5127(3.5252) | Error 0.9156(0.8988) Steps 808(816.45) | Grad Norm 3.0196(2.9040) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 20.2848(20.4080) | Bit/dim 3.5013(3.5248) | Xent 2.3026(2.3026) | Loss 3.5013(3.5248) | Error 0.8833(0.8990) Steps 820(816.72) | Grad Norm 2.4047(2.9025) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 21.0336(20.4216) | Bit/dim 3.5140(3.5253) | Xent 2.3026(2.3026) | Loss 3.5140(3.5253) | Error 0.9100(0.9007) Steps 814(817.96) | Grad Norm 2.2486(2.8566) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 104.3491, Epoch Time 1247.0035(1184.6595), Bit/dim 3.5251(best: 3.5258), Xent 2.3026, Loss 3.5251, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 20.7555(20.4507) | Bit/dim 3.5325(3.5270) | Xent 2.3026(2.3026) | Loss 3.5325(3.5270) | Error 0.8989(0.9001) Steps 820(819.04) | Grad Norm 2.2542(2.9618) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 20.3911(20.4696) | Bit/dim 3.5367(3.5260) | Xent 2.3026(2.3026) | Loss 3.5367(3.5260) | Error 0.9167(0.8998) Steps 820(817.17) | Grad Norm 2.7385(2.8286) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 20.4840(20.4829) | Bit/dim 3.5025(3.5228) | Xent 2.3026(2.3026) | Loss 3.5025(3.5228) | Error 0.9144(0.8999) Steps 814(816.75) | Grad Norm 3.4864(2.8465) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 20.4790(20.4847) | Bit/dim 3.5276(3.5223) | Xent 2.3026(2.3026) | Loss 3.5276(3.5223) | Error 0.8933(0.8996) Steps 802(816.04) | Grad Norm 2.4160(2.8058) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 20.0406(20.4826) | Bit/dim 3.5363(3.5223) | Xent 2.3026(2.3026) | Loss 3.5363(3.5223) | Error 0.9011(0.8997) Steps 832(815.30) | Grad Norm 4.3693(2.8632) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 104.3944, Epoch Time 1251.6636(1186.6696), Bit/dim 3.5272(best: 3.5251), Xent 2.3026, Loss 3.5272, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 20.6582(20.5481) | Bit/dim 3.5265(3.5224) | Xent 2.3026(2.3026) | Loss 3.5265(3.5224) | Error 0.9167(0.8999) Steps 832(817.92) | Grad Norm 2.7431(2.8099) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 21.1444(20.6214) | Bit/dim 3.5391(3.5242) | Xent 2.3026(2.3026) | Loss 3.5391(3.5242) | Error 0.9011(0.8992) Steps 826(820.47) | Grad Norm 3.4092(2.7486) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 20.2727(20.5743) | Bit/dim 3.5008(3.5243) | Xent 2.3026(2.3026) | Loss 3.5008(3.5243) | Error 0.9000(0.8989) Steps 826(819.62) | Grad Norm 2.0821(2.7829) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 21.2298(20.5988) | Bit/dim 3.5189(3.5229) | Xent 2.3026(2.3026) | Loss 3.5189(3.5229) | Error 0.8878(0.8998) Steps 814(820.42) | Grad Norm 2.3052(2.8373) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 20.1189(20.5277) | Bit/dim 3.4764(3.5210) | Xent 2.3026(2.3026) | Loss 3.4764(3.5210) | Error 0.9022(0.9011) Steps 826(821.24) | Grad Norm 3.0887(2.6814) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 20.7105(20.5591) | Bit/dim 3.5124(3.5194) | Xent 2.3026(2.3026) | Loss 3.5124(3.5194) | Error 0.8967(0.9005) Steps 820(822.83) | Grad Norm 2.7728(2.8789) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 103.6359, Epoch Time 1254.9676(1188.7185), Bit/dim 3.5251(best: 3.5251), Xent 2.3026, Loss 3.5251, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 21.1910(20.6351) | Bit/dim 3.5343(3.5200) | Xent 2.3026(2.3026) | Loss 3.5343(3.5200) | Error 0.9100(0.9007) Steps 814(823.65) | Grad Norm 1.4965(2.7256) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 20.6642(20.7074) | Bit/dim 3.4911(3.5168) | Xent 2.3026(2.3026) | Loss 3.4911(3.5168) | Error 0.9000(0.8996) Steps 826(825.38) | Grad Norm 3.4099(2.9022) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 20.6805(20.6926) | Bit/dim 3.5177(3.5164) | Xent 2.3026(2.3026) | Loss 3.5177(3.5164) | Error 0.9022(0.9004) Steps 838(824.78) | Grad Norm 2.8504(2.8712) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 20.4486(20.6719) | Bit/dim 3.5133(3.5172) | Xent 2.3026(2.3026) | Loss 3.5133(3.5172) | Error 0.8867(0.8988) Steps 820(822.85) | Grad Norm 2.9739(2.8853) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 20.6078(20.6753) | Bit/dim 3.5016(3.5179) | Xent 2.3026(2.3026) | Loss 3.5016(3.5179) | Error 0.9011(0.8994) Steps 838(822.38) | Grad Norm 1.8263(2.8131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 104.4458, Epoch Time 1264.7267(1190.9988), Bit/dim 3.5175(best: 3.5251), Xent 2.3026, Loss 3.5175, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 21.2097(20.7602) | Bit/dim 3.4930(3.5187) | Xent 2.3026(2.3026) | Loss 3.4930(3.5187) | Error 0.9033(0.9010) Steps 838(823.22) | Grad Norm 2.3415(2.8450) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 20.9133(20.7972) | Bit/dim 3.5232(3.5182) | Xent 2.3026(2.3026) | Loss 3.5232(3.5182) | Error 0.9022(0.9004) Steps 832(824.28) | Grad Norm 2.6989(2.8400) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 20.7953(20.8545) | Bit/dim 3.5273(3.5186) | Xent 2.3026(2.3026) | Loss 3.5273(3.5186) | Error 0.8833(0.9003) Steps 832(824.45) | Grad Norm 2.7399(2.8387) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 20.6170(20.8274) | Bit/dim 3.4765(3.5150) | Xent 2.3026(2.3026) | Loss 3.4765(3.5150) | Error 0.8889(0.8996) Steps 832(826.54) | Grad Norm 2.2675(2.7680) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 20.5892(20.8288) | Bit/dim 3.5267(3.5151) | Xent 2.3026(2.3026) | Loss 3.5267(3.5151) | Error 0.9156(0.9011) Steps 844(827.89) | Grad Norm 3.1129(2.8332) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 21.2671(20.8697) | Bit/dim 3.5359(3.5169) | Xent 2.3026(2.3026) | Loss 3.5359(3.5169) | Error 0.8944(0.8997) Steps 826(825.84) | Grad Norm 2.2809(2.8664) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 104.4145, Epoch Time 1272.0948(1193.4317), Bit/dim 3.5130(best: 3.5175), Xent 2.3026, Loss 3.5130, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 20.8560(20.9071) | Bit/dim 3.4862(3.5146) | Xent 2.3026(2.3026) | Loss 3.4862(3.5146) | Error 0.8844(0.9003) Steps 844(828.30) | Grad Norm 2.0686(2.7356) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 20.4811(20.8782) | Bit/dim 3.5372(3.5158) | Xent 2.3026(2.3026) | Loss 3.5372(3.5158) | Error 0.8922(0.8997) Steps 826(829.31) | Grad Norm 2.5707(2.7824) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 20.4350(20.8930) | Bit/dim 3.5043(3.5158) | Xent 2.3026(2.3026) | Loss 3.5043(3.5158) | Error 0.8900(0.8994) Steps 832(829.81) | Grad Norm 1.8617(2.7689) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 20.6661(20.8704) | Bit/dim 3.5229(3.5152) | Xent 2.3026(2.3026) | Loss 3.5229(3.5152) | Error 0.8967(0.9003) Steps 844(829.24) | Grad Norm 3.5232(2.7127) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 21.1841(20.8467) | Bit/dim 3.5271(3.5143) | Xent 2.3026(2.3026) | Loss 3.5271(3.5143) | Error 0.9156(0.8986) Steps 832(829.57) | Grad Norm 2.9185(2.8945) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 106.0109, Epoch Time 1272.8907(1195.8154), Bit/dim 3.5126(best: 3.5130), Xent 2.3026, Loss 3.5126, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 20.7861(20.8940) | Bit/dim 3.5383(3.5170) | Xent 2.3026(2.3026) | Loss 3.5383(3.5170) | Error 0.9000(0.9007) Steps 844(830.34) | Grad Norm 3.0912(2.8349) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 20.2955(20.8570) | Bit/dim 3.5008(3.5129) | Xent 2.3026(2.3026) | Loss 3.5008(3.5129) | Error 0.8944(0.9003) Steps 802(829.38) | Grad Norm 3.9941(2.7219) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 20.9499(20.8080) | Bit/dim 3.5183(3.5146) | Xent 2.3026(2.3026) | Loss 3.5183(3.5146) | Error 0.9078(0.9017) Steps 844(829.29) | Grad Norm 2.5924(2.8246) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 20.6156(20.8127) | Bit/dim 3.5122(3.5132) | Xent 2.3026(2.3026) | Loss 3.5122(3.5132) | Error 0.9056(0.9009) Steps 838(831.28) | Grad Norm 2.4281(2.6534) | Total Time 14.00(14.00)\n",
      "Iter 4390 | Time 20.5096(20.8463) | Bit/dim 3.5118(3.5124) | Xent 2.3026(2.3026) | Loss 3.5118(3.5124) | Error 0.8989(0.8995) Steps 832(832.15) | Grad Norm 2.6179(2.7304) | Total Time 14.00(14.00)\n",
      "Iter 4400 | Time 20.4688(20.8542) | Bit/dim 3.4981(3.5117) | Xent 2.3026(2.3026) | Loss 3.4981(3.5117) | Error 0.9000(0.8989) Steps 838(833.54) | Grad Norm 2.7586(2.9092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 104.6793, Epoch Time 1268.5179(1197.9965), Bit/dim 3.5117(best: 3.5126), Xent 2.3026, Loss 3.5117, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 20.5856(20.8422) | Bit/dim 3.5003(3.5106) | Xent 2.3026(2.3026) | Loss 3.5003(3.5106) | Error 0.9133(0.8990) Steps 838(833.62) | Grad Norm 2.6470(2.7283) | Total Time 14.00(14.00)\n",
      "Iter 4420 | Time 20.9477(20.8069) | Bit/dim 3.5127(3.5105) | Xent 2.3026(2.3026) | Loss 3.5127(3.5105) | Error 0.9000(0.8989) Steps 826(833.18) | Grad Norm 2.4690(2.8690) | Total Time 14.00(14.00)\n",
      "Iter 4430 | Time 20.7397(20.7895) | Bit/dim 3.4972(3.5100) | Xent 2.3026(2.3026) | Loss 3.4972(3.5100) | Error 0.8956(0.8991) Steps 838(833.13) | Grad Norm 2.3926(2.6706) | Total Time 14.00(14.00)\n",
      "Iter 4440 | Time 20.8330(20.7836) | Bit/dim 3.4982(3.5111) | Xent 2.3026(2.3026) | Loss 3.4982(3.5111) | Error 0.9089(0.9003) Steps 832(832.21) | Grad Norm 2.4782(2.7839) | Total Time 14.00(14.00)\n",
      "Iter 4450 | Time 21.7287(20.8592) | Bit/dim 3.5066(3.5113) | Xent 2.3026(2.3026) | Loss 3.5066(3.5113) | Error 0.8911(0.9001) Steps 826(832.47) | Grad Norm 2.7066(2.6252) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 107.1257, Epoch Time 1270.7220(1200.1783), Bit/dim 3.5136(best: 3.5117), Xent 2.3026, Loss 3.5136, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 20.9130(20.9120) | Bit/dim 3.5275(3.5101) | Xent 2.3026(2.3026) | Loss 3.5275(3.5101) | Error 0.9122(0.9011) Steps 826(833.57) | Grad Norm 3.5226(2.7181) | Total Time 14.00(14.00)\n",
      "Iter 4470 | Time 21.3143(21.0131) | Bit/dim 3.5123(3.5088) | Xent 2.3026(2.3026) | Loss 3.5123(3.5088) | Error 0.8989(0.9014) Steps 838(833.68) | Grad Norm 2.3814(2.5811) | Total Time 14.00(14.00)\n",
      "Iter 4480 | Time 21.5740(21.0241) | Bit/dim 3.4796(3.5099) | Xent 2.3026(2.3026) | Loss 3.4796(3.5099) | Error 0.8989(0.9018) Steps 838(835.61) | Grad Norm 2.9201(2.7600) | Total Time 14.00(14.00)\n",
      "Iter 4490 | Time 21.2420(20.9271) | Bit/dim 3.5335(3.5092) | Xent 2.3026(2.3026) | Loss 3.5335(3.5092) | Error 0.8944(0.9010) Steps 808(833.28) | Grad Norm 3.2875(2.7353) | Total Time 14.00(14.00)\n",
      "Iter 4500 | Time 21.1839(20.9300) | Bit/dim 3.5134(3.5090) | Xent 2.3026(2.3026) | Loss 3.5134(3.5090) | Error 0.8967(0.8996) Steps 844(834.39) | Grad Norm 2.1265(2.6333) | Total Time 14.00(14.00)\n",
      "Iter 4510 | Time 20.9416(20.9608) | Bit/dim 3.5397(3.5091) | Xent 2.3026(2.3026) | Loss 3.5397(3.5091) | Error 0.9022(0.8986) Steps 832(835.22) | Grad Norm 2.6053(2.7309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 106.1890, Epoch Time 1279.7683(1202.5660), Bit/dim 3.5094(best: 3.5117), Xent 2.3026, Loss 3.5094, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 20.5376(20.9885) | Bit/dim 3.4862(3.5086) | Xent 2.3026(2.3026) | Loss 3.4862(3.5086) | Error 0.8933(0.9001) Steps 826(835.90) | Grad Norm 2.2514(2.7859) | Total Time 14.00(14.00)\n",
      "Iter 4530 | Time 20.1335(20.9971) | Bit/dim 3.5308(3.5104) | Xent 2.3026(2.3026) | Loss 3.5308(3.5104) | Error 0.8856(0.9002) Steps 838(836.23) | Grad Norm 4.3291(2.7788) | Total Time 14.00(14.00)\n",
      "Iter 4540 | Time 20.5155(20.9749) | Bit/dim 3.4912(3.5071) | Xent 2.3026(2.3026) | Loss 3.4912(3.5071) | Error 0.9022(0.8993) Steps 808(836.22) | Grad Norm 2.7095(2.6931) | Total Time 14.00(14.00)\n",
      "Iter 4550 | Time 20.2948(20.9342) | Bit/dim 3.5059(3.5077) | Xent 2.3026(2.3026) | Loss 3.5059(3.5077) | Error 0.8933(0.8985) Steps 844(837.45) | Grad Norm 2.5767(2.8102) | Total Time 14.00(14.00)\n",
      "Iter 4560 | Time 20.8287(20.9841) | Bit/dim 3.4859(3.5049) | Xent 2.3026(2.3026) | Loss 3.4859(3.5049) | Error 0.9111(0.8997) Steps 832(837.81) | Grad Norm 1.7348(2.7299) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 107.4492, Epoch Time 1282.6736(1204.9692), Bit/dim 3.5047(best: 3.5094), Xent 2.3026, Loss 3.5047, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 20.8287(21.0737) | Bit/dim 3.4626(3.5060) | Xent 2.3026(2.3026) | Loss 3.4626(3.5060) | Error 0.8800(0.8995) Steps 832(839.68) | Grad Norm 3.2815(2.7145) | Total Time 14.00(14.00)\n",
      "Iter 4580 | Time 20.6843(21.0245) | Bit/dim 3.5022(3.5062) | Xent 2.3026(2.3026) | Loss 3.5022(3.5062) | Error 0.9067(0.9000) Steps 844(840.77) | Grad Norm 2.0912(2.6975) | Total Time 14.00(14.00)\n",
      "Iter 4590 | Time 20.8741(21.0160) | Bit/dim 3.4990(3.5060) | Xent 2.3026(2.3026) | Loss 3.4990(3.5060) | Error 0.9000(0.9005) Steps 832(839.51) | Grad Norm 3.0228(2.7995) | Total Time 14.00(14.00)\n",
      "Iter 4600 | Time 21.1167(21.0326) | Bit/dim 3.4993(3.5066) | Xent 2.3026(2.3026) | Loss 3.4993(3.5066) | Error 0.9033(0.9004) Steps 844(841.59) | Grad Norm 2.9522(2.6869) | Total Time 14.00(14.00)\n",
      "Iter 4610 | Time 21.9587(21.1084) | Bit/dim 3.5158(3.5042) | Xent 2.3026(2.3026) | Loss 3.5158(3.5042) | Error 0.9000(0.8994) Steps 844(839.12) | Grad Norm 1.7408(2.7933) | Total Time 14.00(14.00)\n",
      "Iter 4620 | Time 20.7121(21.1584) | Bit/dim 3.5096(3.5042) | Xent 2.3026(2.3026) | Loss 3.5096(3.5042) | Error 0.8989(0.8995) Steps 856(839.20) | Grad Norm 3.8241(2.8047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 106.0077, Epoch Time 1286.3022(1207.4092), Bit/dim 3.5124(best: 3.5047), Xent 2.3026, Loss 3.5124, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 20.9423(21.1107) | Bit/dim 3.5181(3.5056) | Xent 2.3026(2.3026) | Loss 3.5181(3.5056) | Error 0.8989(0.8990) Steps 838(840.43) | Grad Norm 2.9343(2.8131) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 20.8947(21.0852) | Bit/dim 3.4952(3.5068) | Xent 2.3026(2.3026) | Loss 3.4952(3.5068) | Error 0.9156(0.8997) Steps 826(837.92) | Grad Norm 3.4843(3.0022) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 21.6548(21.0883) | Bit/dim 3.4880(3.5031) | Xent 2.3026(2.3026) | Loss 3.4880(3.5031) | Error 0.9022(0.9002) Steps 826(839.02) | Grad Norm 2.8742(2.8396) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 21.5220(21.1250) | Bit/dim 3.5055(3.5039) | Xent 2.3026(2.3026) | Loss 3.5055(3.5039) | Error 0.8900(0.8997) Steps 832(840.16) | Grad Norm 2.4656(2.8335) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 20.3023(21.0895) | Bit/dim 3.4958(3.5001) | Xent 2.3026(2.3026) | Loss 3.4958(3.5001) | Error 0.8978(0.8998) Steps 838(838.97) | Grad Norm 1.9103(2.6447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 108.2373, Epoch Time 1284.8431(1209.7322), Bit/dim 3.4998(best: 3.5047), Xent 2.3026, Loss 3.4998, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 21.1708(21.1397) | Bit/dim 3.4975(3.5011) | Xent 2.3026(2.3026) | Loss 3.4975(3.5011) | Error 0.8956(0.8991) Steps 862(840.57) | Grad Norm 3.9281(2.7046) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 20.8691(21.1705) | Bit/dim 3.4803(3.5023) | Xent 2.3026(2.3026) | Loss 3.4803(3.5023) | Error 0.9044(0.8990) Steps 838(840.53) | Grad Norm 2.9538(2.7782) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 21.8953(21.1438) | Bit/dim 3.4978(3.5002) | Xent 2.3026(2.3026) | Loss 3.4978(3.5002) | Error 0.9044(0.8980) Steps 868(841.05) | Grad Norm 2.1610(2.6413) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 21.3639(21.1477) | Bit/dim 3.4800(3.5028) | Xent 2.3026(2.3026) | Loss 3.4800(3.5028) | Error 0.9044(0.9007) Steps 862(841.33) | Grad Norm 2.7081(2.4848) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 21.4099(21.0826) | Bit/dim 3.5106(3.5015) | Xent 2.3026(2.3026) | Loss 3.5106(3.5015) | Error 0.9122(0.9011) Steps 862(842.32) | Grad Norm 4.6463(2.6654) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 21.1981(21.0709) | Bit/dim 3.5222(3.5007) | Xent 2.3026(2.3026) | Loss 3.5222(3.5007) | Error 0.8889(0.9006) Steps 850(843.15) | Grad Norm 2.8036(2.5761) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 106.8410, Epoch Time 1287.4435(1212.0635), Bit/dim 3.5015(best: 3.4998), Xent 2.3026, Loss 3.5015, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.9939(21.0265) | Bit/dim 3.4944(3.5004) | Xent 2.3026(2.3026) | Loss 3.4944(3.5004) | Error 0.8944(0.9011) Steps 868(844.33) | Grad Norm 1.9012(2.5994) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 19.4927(20.9550) | Bit/dim 3.5249(3.5012) | Xent 2.3026(2.3026) | Loss 3.5249(3.5012) | Error 0.9067(0.9023) Steps 850(845.85) | Grad Norm 2.8328(2.6346) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 20.6908(20.9701) | Bit/dim 3.4834(3.5004) | Xent 2.3026(2.3026) | Loss 3.4834(3.5004) | Error 0.8867(0.9016) Steps 844(846.27) | Grad Norm 1.5782(2.7099) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 21.0952(20.9976) | Bit/dim 3.4678(3.4994) | Xent 2.3026(2.3026) | Loss 3.4678(3.4994) | Error 0.8922(0.9008) Steps 850(845.54) | Grad Norm 1.9436(2.6495) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 21.2152(20.9547) | Bit/dim 3.4931(3.5001) | Xent 2.3026(2.3026) | Loss 3.4931(3.5001) | Error 0.9078(0.8994) Steps 850(844.77) | Grad Norm 3.6254(2.6604) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 108.9197, Epoch Time 1279.9451(1214.1000), Bit/dim 3.5053(best: 3.4998), Xent 2.3026, Loss 3.5053, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 20.8853(21.0207) | Bit/dim 3.5030(3.4972) | Xent 2.3026(2.3026) | Loss 3.5030(3.4972) | Error 0.8933(0.8998) Steps 850(845.45) | Grad Norm 2.2827(2.7344) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 21.1872(21.0195) | Bit/dim 3.5102(3.4970) | Xent 2.3026(2.3026) | Loss 3.5102(3.4970) | Error 0.9033(0.9000) Steps 832(842.31) | Grad Norm 3.3250(2.7886) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 20.8810(21.0348) | Bit/dim 3.4831(3.4981) | Xent 2.3026(2.3026) | Loss 3.4831(3.4981) | Error 0.8967(0.8995) Steps 862(844.04) | Grad Norm 2.8648(2.6619) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 20.6687(21.0247) | Bit/dim 3.4751(3.4961) | Xent 2.3026(2.3026) | Loss 3.4751(3.4961) | Error 0.9122(0.9004) Steps 832(843.72) | Grad Norm 2.2626(2.6155) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 20.5732(21.0672) | Bit/dim 3.5015(3.4977) | Xent 2.3026(2.3026) | Loss 3.5015(3.4977) | Error 0.9044(0.8999) Steps 850(843.82) | Grad Norm 3.2630(2.7041) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 21.3612(21.1477) | Bit/dim 3.4905(3.4992) | Xent 2.3026(2.3026) | Loss 3.4905(3.4992) | Error 0.8933(0.8995) Steps 850(844.79) | Grad Norm 3.4669(2.7253) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 107.8173, Epoch Time 1287.9078(1216.3142), Bit/dim 3.5037(best: 3.4998), Xent 2.3026, Loss 3.5037, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 21.8327(21.2052) | Bit/dim 3.4673(3.4994) | Xent 2.3026(2.3026) | Loss 3.4673(3.4994) | Error 0.9233(0.8991) Steps 868(845.25) | Grad Norm 2.1412(2.6056) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 21.4624(21.1897) | Bit/dim 3.5051(3.4988) | Xent 2.3026(2.3026) | Loss 3.5051(3.4988) | Error 0.9056(0.8983) Steps 850(845.85) | Grad Norm 2.2937(2.5798) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 20.8432(21.1484) | Bit/dim 3.4767(3.4946) | Xent 2.3026(2.3026) | Loss 3.4767(3.4946) | Error 0.8878(0.8986) Steps 838(845.60) | Grad Norm 3.5904(2.7704) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 21.0609(21.1220) | Bit/dim 3.5144(3.4956) | Xent 2.3026(2.3026) | Loss 3.5144(3.4956) | Error 0.8989(0.8996) Steps 844(846.53) | Grad Norm 3.7378(2.8480) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 21.2185(21.0981) | Bit/dim 3.4691(3.4955) | Xent 2.3026(2.3026) | Loss 3.4691(3.4955) | Error 0.8978(0.9004) Steps 832(847.71) | Grad Norm 2.5735(2.8295) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 109.0562, Epoch Time 1288.0955(1218.4677), Bit/dim 3.4988(best: 3.4998), Xent 2.3026, Loss 3.4988, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 21.2154(21.0764) | Bit/dim 3.5007(3.4961) | Xent 2.3026(2.3026) | Loss 3.5007(3.4961) | Error 0.9100(0.9009) Steps 826(845.82) | Grad Norm 2.4991(2.7637) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 21.1737(21.0567) | Bit/dim 3.5112(3.4963) | Xent 2.3026(2.3026) | Loss 3.5112(3.4963) | Error 0.9067(0.9002) Steps 844(846.80) | Grad Norm 3.5425(2.7607) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 21.4986(21.0767) | Bit/dim 3.5032(3.4981) | Xent 2.3026(2.3026) | Loss 3.5032(3.4981) | Error 0.9078(0.8999) Steps 838(846.81) | Grad Norm 2.7286(2.7596) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 21.0266(21.1682) | Bit/dim 3.4325(3.4941) | Xent 2.3026(2.3026) | Loss 3.4325(3.4941) | Error 0.8700(0.8993) Steps 856(847.41) | Grad Norm 3.9254(2.7515) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 21.7493(21.1977) | Bit/dim 3.4757(3.4928) | Xent 2.3026(2.3026) | Loss 3.4757(3.4928) | Error 0.9011(0.8994) Steps 850(846.85) | Grad Norm 2.4864(2.8020) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 21.5146(21.2032) | Bit/dim 3.4891(3.4938) | Xent 2.3026(2.3026) | Loss 3.4891(3.4938) | Error 0.8922(0.9001) Steps 868(847.87) | Grad Norm 1.6893(2.8356) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 107.7689, Epoch Time 1290.9038(1220.6407), Bit/dim 3.4931(best: 3.4988), Xent 2.3026, Loss 3.4931, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 22.3539(21.2768) | Bit/dim 3.4785(3.4926) | Xent 2.3026(2.3026) | Loss 3.4785(3.4926) | Error 0.9056(0.9006) Steps 844(848.93) | Grad Norm 2.7481(2.8219) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 21.6133(21.2668) | Bit/dim 3.4766(3.4917) | Xent 2.3026(2.3026) | Loss 3.4766(3.4917) | Error 0.8956(0.8999) Steps 856(851.18) | Grad Norm 1.4068(2.6050) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 21.0024(21.2915) | Bit/dim 3.4651(3.4922) | Xent 2.3026(2.3026) | Loss 3.4651(3.4922) | Error 0.9211(0.9002) Steps 856(852.19) | Grad Norm 1.3327(2.6588) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 21.3185(21.2946) | Bit/dim 3.5047(3.4915) | Xent 2.3026(2.3026) | Loss 3.5047(3.4915) | Error 0.9078(0.9012) Steps 868(853.97) | Grad Norm 1.9770(2.6585) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 21.6981(21.3064) | Bit/dim 3.4657(3.4901) | Xent 2.3026(2.3026) | Loss 3.4657(3.4901) | Error 0.9011(0.9006) Steps 850(851.60) | Grad Norm 4.4052(2.7713) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 110.0946, Epoch Time 1300.6782(1223.0419), Bit/dim 3.4910(best: 3.4931), Xent 2.3026, Loss 3.4910, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 20.9984(21.2675) | Bit/dim 3.4858(3.4908) | Xent 2.3026(2.3026) | Loss 3.4858(3.4908) | Error 0.8900(0.8990) Steps 868(852.40) | Grad Norm 2.3357(2.5962) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 20.7555(21.2408) | Bit/dim 3.4788(3.4922) | Xent 2.3026(2.3026) | Loss 3.4788(3.4922) | Error 0.8933(0.8988) Steps 838(850.63) | Grad Norm 1.7326(2.6763) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 21.5068(21.2378) | Bit/dim 3.4427(3.4912) | Xent 2.3026(2.3026) | Loss 3.4427(3.4912) | Error 0.9067(0.8996) Steps 862(852.17) | Grad Norm 3.2898(2.5382) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 21.7702(21.3512) | Bit/dim 3.4899(3.4913) | Xent 2.3026(2.3026) | Loss 3.4899(3.4913) | Error 0.9000(0.8988) Steps 826(853.63) | Grad Norm 2.1748(2.6829) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 20.3981(21.3672) | Bit/dim 3.5061(3.4908) | Xent 2.3026(2.3026) | Loss 3.5061(3.4908) | Error 0.9067(0.9000) Steps 856(853.32) | Grad Norm 2.3435(2.6351) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 21.1443(21.3787) | Bit/dim 3.4958(3.4900) | Xent 2.3026(2.3026) | Loss 3.4958(3.4900) | Error 0.9000(0.9004) Steps 868(853.79) | Grad Norm 2.8255(2.7276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 109.1435, Epoch Time 1302.1535(1225.4152), Bit/dim 3.4926(best: 3.4910), Xent 2.3026, Loss 3.4926, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 21.4714(21.3651) | Bit/dim 3.4906(3.4894) | Xent 2.3026(2.3026) | Loss 3.4906(3.4894) | Error 0.9100(0.9013) Steps 874(853.08) | Grad Norm 5.1502(2.7663) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 21.5393(21.3765) | Bit/dim 3.5124(3.4908) | Xent 2.3026(2.3026) | Loss 3.5124(3.4908) | Error 0.9122(0.9009) Steps 850(852.31) | Grad Norm 2.3492(2.7116) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 21.8181(21.3460) | Bit/dim 3.4867(3.4873) | Xent 2.3026(2.3026) | Loss 3.4867(3.4873) | Error 0.8956(0.9002) Steps 844(853.64) | Grad Norm 2.6298(2.6140) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 20.1865(21.3274) | Bit/dim 3.4886(3.4882) | Xent 2.3026(2.3026) | Loss 3.4886(3.4882) | Error 0.8878(0.8989) Steps 850(851.31) | Grad Norm 1.5144(2.5809) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 21.8004(21.3899) | Bit/dim 3.4703(3.4886) | Xent 2.3026(2.3026) | Loss 3.4703(3.4886) | Error 0.8889(0.8992) Steps 874(851.77) | Grad Norm 2.7974(2.4526) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 110.8388, Epoch Time 1304.7198(1227.7944), Bit/dim 3.4945(best: 3.4910), Xent 2.3026, Loss 3.4945, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 21.8883(21.4274) | Bit/dim 3.5254(3.4905) | Xent 2.3026(2.3026) | Loss 3.5254(3.4905) | Error 0.9111(0.9008) Steps 874(854.36) | Grad Norm 3.7835(2.6251) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 21.3211(21.3625) | Bit/dim 3.5166(3.4916) | Xent 2.3026(2.3026) | Loss 3.5166(3.4916) | Error 0.9067(0.9007) Steps 868(854.27) | Grad Norm 1.8845(2.6071) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 21.0191(21.3013) | Bit/dim 3.4667(3.4898) | Xent 2.3026(2.3026) | Loss 3.4667(3.4898) | Error 0.8933(0.9001) Steps 838(854.84) | Grad Norm 1.8374(2.7626) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 21.4603(21.3267) | Bit/dim 3.4874(3.4912) | Xent 2.3026(2.3026) | Loss 3.4874(3.4912) | Error 0.8856(0.9002) Steps 850(854.97) | Grad Norm 1.9920(2.6054) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 21.3003(21.3600) | Bit/dim 3.5004(3.4891) | Xent 2.3026(2.3026) | Loss 3.5004(3.4891) | Error 0.8878(0.8982) Steps 832(854.54) | Grad Norm 3.4145(2.6876) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 22.3141(21.4430) | Bit/dim 3.4947(3.4876) | Xent 2.3026(2.3026) | Loss 3.4947(3.4876) | Error 0.9067(0.8996) Steps 874(856.12) | Grad Norm 2.5477(2.5999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 109.9053, Epoch Time 1303.8409(1230.0758), Bit/dim 3.4892(best: 3.4910), Xent 2.3026, Loss 3.4892, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 21.1242(21.4263) | Bit/dim 3.5084(3.4875) | Xent 2.3026(2.3026) | Loss 3.5084(3.4875) | Error 0.9044(0.8996) Steps 844(857.65) | Grad Norm 2.4683(2.6954) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 21.2973(21.4439) | Bit/dim 3.4678(3.4865) | Xent 2.3026(2.3026) | Loss 3.4678(3.4865) | Error 0.8933(0.8994) Steps 868(856.84) | Grad Norm 3.6147(2.5859) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 21.9793(21.4739) | Bit/dim 3.4481(3.4863) | Xent 2.3026(2.3026) | Loss 3.4481(3.4863) | Error 0.9011(0.8998) Steps 880(858.36) | Grad Norm 2.3529(2.6924) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 21.7673(21.4831) | Bit/dim 3.5457(3.4887) | Xent 2.3026(2.3026) | Loss 3.5457(3.4887) | Error 0.9033(0.9014) Steps 868(857.94) | Grad Norm 3.8277(2.6936) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 21.7595(21.4820) | Bit/dim 3.4599(3.4876) | Xent 2.3026(2.3026) | Loss 3.4599(3.4876) | Error 0.8922(0.9005) Steps 862(857.74) | Grad Norm 2.8518(2.7437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 110.0875, Epoch Time 1310.4595(1232.4873), Bit/dim 3.4865(best: 3.4892), Xent 2.3026, Loss 3.4865, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.9639(21.4630) | Bit/dim 3.4640(3.4841) | Xent 2.3026(2.3026) | Loss 3.4640(3.4841) | Error 0.9067(0.8987) Steps 838(855.42) | Grad Norm 1.9777(2.4946) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 21.4173(21.4518) | Bit/dim 3.5097(3.4859) | Xent 2.3026(2.3026) | Loss 3.5097(3.4859) | Error 0.9089(0.8988) Steps 856(855.29) | Grad Norm 2.8945(2.6684) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 21.1458(21.4608) | Bit/dim 3.4950(3.4852) | Xent 2.3026(2.3026) | Loss 3.4950(3.4852) | Error 0.8878(0.8994) Steps 850(855.18) | Grad Norm 4.1434(2.6313) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 21.6271(21.3848) | Bit/dim 3.5170(3.4857) | Xent 2.3026(2.3026) | Loss 3.5170(3.4857) | Error 0.9056(0.9005) Steps 868(856.15) | Grad Norm 2.9149(2.7318) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 21.2280(21.3729) | Bit/dim 3.4616(3.4837) | Xent 2.3026(2.3026) | Loss 3.4616(3.4837) | Error 0.8978(0.9002) Steps 856(857.94) | Grad Norm 1.8452(2.5491) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 22.0097(21.4962) | Bit/dim 3.4808(3.4853) | Xent 2.3026(2.3026) | Loss 3.4808(3.4853) | Error 0.8856(0.9005) Steps 880(860.68) | Grad Norm 2.6142(2.6704) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 109.2707, Epoch Time 1305.4692(1234.6767), Bit/dim 3.4870(best: 3.4865), Xent 2.3026, Loss 3.4870, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 20.7227(21.4379) | Bit/dim 3.5099(3.4875) | Xent 2.3026(2.3026) | Loss 3.5099(3.4875) | Error 0.8889(0.9008) Steps 826(856.83) | Grad Norm 2.7396(2.6060) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 21.0091(21.4135) | Bit/dim 3.4903(3.4866) | Xent 2.3026(2.3026) | Loss 3.4903(3.4866) | Error 0.9000(0.9021) Steps 856(853.79) | Grad Norm 3.1882(2.5453) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 21.6881(21.5057) | Bit/dim 3.4585(3.4834) | Xent 2.3026(2.3026) | Loss 3.4585(3.4834) | Error 0.8911(0.9001) Steps 886(858.37) | Grad Norm 2.3526(2.7525) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 21.3244(21.5087) | Bit/dim 3.4669(3.4828) | Xent 2.3026(2.3026) | Loss 3.4669(3.4828) | Error 0.8711(0.8994) Steps 868(861.07) | Grad Norm 2.0098(2.6302) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 21.4430(21.5047) | Bit/dim 3.5067(3.4835) | Xent 2.3026(2.3026) | Loss 3.5067(3.4835) | Error 0.9122(0.8999) Steps 844(860.14) | Grad Norm 3.0017(2.7052) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 109.8083, Epoch Time 1309.1037(1236.9095), Bit/dim 3.4884(best: 3.4865), Xent 2.3026, Loss 3.4884, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 21.0315(21.4539) | Bit/dim 3.4884(3.4849) | Xent 2.3026(2.3026) | Loss 3.4884(3.4849) | Error 0.8889(0.8992) Steps 826(858.45) | Grad Norm 2.0125(2.7063) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 21.0901(21.4689) | Bit/dim 3.5024(3.4835) | Xent 2.3026(2.3026) | Loss 3.5024(3.4835) | Error 0.9033(0.8987) Steps 862(860.88) | Grad Norm 2.8507(2.7042) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 21.2715(21.4771) | Bit/dim 3.4806(3.4839) | Xent 2.3026(2.3026) | Loss 3.4806(3.4839) | Error 0.9100(0.9002) Steps 874(859.82) | Grad Norm 1.8688(2.6871) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 21.3817(21.4723) | Bit/dim 3.5027(3.4815) | Xent 2.3026(2.3026) | Loss 3.5027(3.4815) | Error 0.9167(0.9006) Steps 832(860.05) | Grad Norm 2.8876(2.6552) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 21.6185(21.3951) | Bit/dim 3.4854(3.4831) | Xent 2.3026(2.3026) | Loss 3.4854(3.4831) | Error 0.9078(0.9005) Steps 874(858.32) | Grad Norm 3.6519(2.7095) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 21.2794(21.4540) | Bit/dim 3.4661(3.4826) | Xent 2.3026(2.3026) | Loss 3.4661(3.4826) | Error 0.9056(0.8999) Steps 862(861.68) | Grad Norm 1.9234(2.6992) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 110.6258, Epoch Time 1307.4874(1239.0269), Bit/dim 3.4816(best: 3.4865), Xent 2.3026, Loss 3.4816, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 21.7924(21.4326) | Bit/dim 3.4930(3.4832) | Xent 2.3026(2.3026) | Loss 3.4930(3.4832) | Error 0.8956(0.9003) Steps 862(862.90) | Grad Norm 2.7632(2.5817) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 21.5305(21.4559) | Bit/dim 3.4423(3.4844) | Xent 2.3026(2.3026) | Loss 3.4423(3.4844) | Error 0.8889(0.9000) Steps 880(863.19) | Grad Norm 3.1300(2.6897) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 21.3227(21.4778) | Bit/dim 3.4796(3.4820) | Xent 2.3026(2.3026) | Loss 3.4796(3.4820) | Error 0.8744(0.8980) Steps 874(863.79) | Grad Norm 2.4498(2.5004) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 21.5093(21.5128) | Bit/dim 3.4715(3.4797) | Xent 2.3026(2.3026) | Loss 3.4715(3.4797) | Error 0.8856(0.8990) Steps 874(863.43) | Grad Norm 1.9919(2.5229) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 21.5348(21.5760) | Bit/dim 3.5042(3.4820) | Xent 2.3026(2.3026) | Loss 3.5042(3.4820) | Error 0.8833(0.9002) Steps 880(864.95) | Grad Norm 2.5382(2.6131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 108.0571, Epoch Time 1312.5842(1241.2336), Bit/dim 3.4825(best: 3.4816), Xent 2.3026, Loss 3.4825, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 21.7220(21.5763) | Bit/dim 3.5004(3.4789) | Xent 2.3026(2.3026) | Loss 3.5004(3.4789) | Error 0.9044(0.9007) Steps 844(862.85) | Grad Norm 2.1696(2.6570) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 21.5230(21.5396) | Bit/dim 3.4905(3.4815) | Xent 2.3026(2.3026) | Loss 3.4905(3.4815) | Error 0.9067(0.9008) Steps 886(864.64) | Grad Norm 2.0304(2.6559) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 22.0931(21.6107) | Bit/dim 3.4754(3.4791) | Xent 2.3026(2.3026) | Loss 3.4754(3.4791) | Error 0.8733(0.8991) Steps 838(865.39) | Grad Norm 2.6380(2.6197) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 21.9804(21.5708) | Bit/dim 3.4869(3.4799) | Xent 2.3026(2.3026) | Loss 3.4869(3.4799) | Error 0.9056(0.8995) Steps 880(865.28) | Grad Norm 3.6022(2.5304) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 21.4922(21.5588) | Bit/dim 3.4582(3.4777) | Xent 2.3026(2.3026) | Loss 3.4582(3.4777) | Error 0.9000(0.8993) Steps 868(865.47) | Grad Norm 3.8973(2.6847) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 21.8199(21.5762) | Bit/dim 3.4773(3.4790) | Xent 2.3026(2.3026) | Loss 3.4773(3.4790) | Error 0.9067(0.9003) Steps 868(865.98) | Grad Norm 1.7287(2.6458) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 111.7202, Epoch Time 1315.8037(1243.4707), Bit/dim 3.4803(best: 3.4816), Xent 2.3026, Loss 3.4803, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 22.6004(21.6289) | Bit/dim 3.4822(3.4782) | Xent 2.3026(2.3026) | Loss 3.4822(3.4782) | Error 0.8833(0.9002) Steps 850(865.95) | Grad Norm 3.2462(2.7859) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 20.9007(21.6018) | Bit/dim 3.5039(3.4800) | Xent 2.3026(2.3026) | Loss 3.5039(3.4800) | Error 0.9222(0.9006) Steps 844(865.48) | Grad Norm 1.7633(2.7440) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 21.8372(21.5369) | Bit/dim 3.4727(3.4803) | Xent 2.3026(2.3026) | Loss 3.4727(3.4803) | Error 0.9011(0.9002) Steps 868(864.30) | Grad Norm 1.7395(2.6378) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 21.1418(21.5014) | Bit/dim 3.4662(3.4793) | Xent 2.3026(2.3026) | Loss 3.4662(3.4793) | Error 0.9122(0.9013) Steps 856(864.17) | Grad Norm 2.7553(2.3840) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 21.9784(21.5447) | Bit/dim 3.4782(3.4814) | Xent 2.3026(2.3026) | Loss 3.4782(3.4814) | Error 0.9133(0.9005) Steps 874(865.46) | Grad Norm 2.7567(2.6853) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 110.8411, Epoch Time 1312.5118(1245.5419), Bit/dim 3.4812(best: 3.4803), Xent 2.3026, Loss 3.4812, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 22.4605(21.5609) | Bit/dim 3.4947(3.4796) | Xent 2.3026(2.3026) | Loss 3.4947(3.4796) | Error 0.9111(0.9011) Steps 880(865.37) | Grad Norm 2.2120(2.6103) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 21.1430(21.5768) | Bit/dim 3.4445(3.4770) | Xent 2.3026(2.3026) | Loss 3.4445(3.4770) | Error 0.9056(0.9013) Steps 850(865.91) | Grad Norm 2.9459(2.4728) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 21.4832(21.5959) | Bit/dim 3.4946(3.4794) | Xent 2.3026(2.3026) | Loss 3.4946(3.4794) | Error 0.8811(0.9001) Steps 886(866.04) | Grad Norm 2.5914(2.6248) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 21.7964(21.6389) | Bit/dim 3.4638(3.4785) | Xent 2.3026(2.3026) | Loss 3.4638(3.4785) | Error 0.8911(0.8990) Steps 856(866.80) | Grad Norm 1.9405(2.7430) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 21.2117(21.6288) | Bit/dim 3.4839(3.4783) | Xent 2.3026(2.3026) | Loss 3.4839(3.4783) | Error 0.9089(0.8993) Steps 874(867.06) | Grad Norm 1.5413(2.7122) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 21.5123(21.6976) | Bit/dim 3.4891(3.4763) | Xent 2.3026(2.3026) | Loss 3.4891(3.4763) | Error 0.9078(0.8996) Steps 862(868.53) | Grad Norm 2.7860(2.6912) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 109.9756, Epoch Time 1323.0394(1247.8668), Bit/dim 3.4828(best: 3.4803), Xent 2.3026, Loss 3.4828, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 22.3412(21.6794) | Bit/dim 3.5262(3.4767) | Xent 2.3026(2.3026) | Loss 3.5262(3.4767) | Error 0.8956(0.8992) Steps 880(869.70) | Grad Norm 2.6514(2.6731) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 20.8673(21.6701) | Bit/dim 3.4568(3.4749) | Xent 2.3026(2.3026) | Loss 3.4568(3.4749) | Error 0.9000(0.9000) Steps 868(871.90) | Grad Norm 2.7302(2.6276) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 21.1085(21.6992) | Bit/dim 3.4658(3.4724) | Xent 2.3026(2.3026) | Loss 3.4658(3.4724) | Error 0.9056(0.8994) Steps 874(872.94) | Grad Norm 2.5700(2.6131) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 21.4672(21.7226) | Bit/dim 3.4741(3.4748) | Xent 2.3026(2.3026) | Loss 3.4741(3.4748) | Error 0.9144(0.9010) Steps 850(870.92) | Grad Norm 1.6573(2.5133) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 21.8930(21.7580) | Bit/dim 3.4952(3.4768) | Xent 2.3026(2.3026) | Loss 3.4952(3.4768) | Error 0.8944(0.8998) Steps 880(871.57) | Grad Norm 2.0542(2.5844) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 110.7816, Epoch Time 1324.4426(1250.1641), Bit/dim 3.4785(best: 3.4803), Xent 2.3026, Loss 3.4785, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 21.8967(21.7396) | Bit/dim 3.4333(3.4736) | Xent 2.3026(2.3026) | Loss 3.4333(3.4736) | Error 0.8933(0.8989) Steps 874(871.43) | Grad Norm 2.4764(2.5919) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 21.1670(21.6900) | Bit/dim 3.4710(3.4737) | Xent 2.3026(2.3026) | Loss 3.4710(3.4737) | Error 0.8789(0.8977) Steps 874(868.98) | Grad Norm 1.7782(2.5694) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 21.6391(21.7630) | Bit/dim 3.4679(3.4749) | Xent 2.3026(2.3026) | Loss 3.4679(3.4749) | Error 0.9044(0.8985) Steps 856(869.28) | Grad Norm 3.4388(2.4818) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 21.6324(21.7672) | Bit/dim 3.4918(3.4752) | Xent 2.3026(2.3026) | Loss 3.4918(3.4752) | Error 0.8978(0.8997) Steps 856(868.17) | Grad Norm 3.2308(2.5717) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 21.0967(21.7535) | Bit/dim 3.4578(3.4736) | Xent 2.3026(2.3026) | Loss 3.4578(3.4736) | Error 0.8756(0.9003) Steps 868(870.33) | Grad Norm 2.6578(2.4513) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 21.3704(21.7165) | Bit/dim 3.4602(3.4754) | Xent 2.3026(2.3026) | Loss 3.4602(3.4754) | Error 0.8933(0.9013) Steps 856(868.97) | Grad Norm 2.2719(2.5875) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 110.5042, Epoch Time 1323.5853(1252.3668), Bit/dim 3.4765(best: 3.4785), Xent 2.3026, Loss 3.4765, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 21.1685(21.6834) | Bit/dim 3.4672(3.4738) | Xent 2.3026(2.3026) | Loss 3.4672(3.4738) | Error 0.9122(0.9005) Steps 886(869.84) | Grad Norm 2.6347(2.4372) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 21.9177(21.7473) | Bit/dim 3.4591(3.4745) | Xent 2.3026(2.3026) | Loss 3.4591(3.4745) | Error 0.9100(0.8995) Steps 874(870.70) | Grad Norm 1.5766(2.5223) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 22.5532(21.7747) | Bit/dim 3.4886(3.4735) | Xent 2.3026(2.3026) | Loss 3.4886(3.4735) | Error 0.9056(0.8990) Steps 880(869.79) | Grad Norm 3.0975(2.5410) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 21.8002(21.7819) | Bit/dim 3.4794(3.4744) | Xent 2.3026(2.3026) | Loss 3.4794(3.4744) | Error 0.9189(0.8999) Steps 868(869.95) | Grad Norm 3.0668(2.5580) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 21.8405(21.8341) | Bit/dim 3.4757(3.4718) | Xent 2.3026(2.3026) | Loss 3.4757(3.4718) | Error 0.9089(0.9009) Steps 886(871.69) | Grad Norm 2.1014(2.5299) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 111.1493, Epoch Time 1330.1542(1254.7004), Bit/dim 3.4759(best: 3.4765), Xent 2.3026, Loss 3.4759, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 21.6544(21.8263) | Bit/dim 3.4612(3.4711) | Xent 2.3026(2.3026) | Loss 3.4612(3.4711) | Error 0.9056(0.9013) Steps 862(872.79) | Grad Norm 1.7730(2.5557) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 21.5861(21.7756) | Bit/dim 3.4589(3.4693) | Xent 2.3026(2.3026) | Loss 3.4589(3.4693) | Error 0.9011(0.9012) Steps 886(872.56) | Grad Norm 1.7003(2.5275) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 22.0730(21.8027) | Bit/dim 3.4722(3.4712) | Xent 2.3026(2.3026) | Loss 3.4722(3.4712) | Error 0.9011(0.9014) Steps 868(873.88) | Grad Norm 2.4551(2.6041) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 21.3061(21.8686) | Bit/dim 3.4562(3.4709) | Xent 2.3026(2.3026) | Loss 3.4562(3.4709) | Error 0.9044(0.9007) Steps 862(874.24) | Grad Norm 2.6245(2.6980) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 21.9876(21.8900) | Bit/dim 3.5077(3.4728) | Xent 2.3026(2.3026) | Loss 3.5077(3.4728) | Error 0.9044(0.9004) Steps 880(874.08) | Grad Norm 3.3077(2.5639) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 22.3369(21.8677) | Bit/dim 3.4832(3.4717) | Xent 2.3026(2.3026) | Loss 3.4832(3.4717) | Error 0.9133(0.8997) Steps 868(874.97) | Grad Norm 2.3011(2.5805) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 111.3557, Epoch Time 1331.4981(1257.0043), Bit/dim 3.4726(best: 3.4759), Xent 2.3026, Loss 3.4726, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 21.5405(21.8625) | Bit/dim 3.4709(3.4696) | Xent 2.3026(2.3026) | Loss 3.4709(3.4696) | Error 0.8978(0.8998) Steps 856(874.61) | Grad Norm 2.2428(2.6493) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 22.0071(21.7499) | Bit/dim 3.4562(3.4716) | Xent 2.3026(2.3026) | Loss 3.4562(3.4716) | Error 0.8967(0.9015) Steps 874(873.96) | Grad Norm 3.2862(2.5377) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 21.8736(21.7197) | Bit/dim 3.5051(3.4710) | Xent 2.3026(2.3026) | Loss 3.5051(3.4710) | Error 0.9078(0.9012) Steps 874(874.49) | Grad Norm 2.6997(2.6865) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 22.1718(21.7063) | Bit/dim 3.4903(3.4725) | Xent 2.3026(2.3026) | Loss 3.4903(3.4725) | Error 0.8889(0.9003) Steps 844(873.16) | Grad Norm 1.7436(2.5252) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 21.5479(21.7527) | Bit/dim 3.4788(3.4696) | Xent 2.3026(2.3026) | Loss 3.4788(3.4696) | Error 0.9000(0.8994) Steps 874(872.18) | Grad Norm 1.9165(2.3879) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 109.5470, Epoch Time 1321.4289(1258.9370), Bit/dim 3.4752(best: 3.4726), Xent 2.3026, Loss 3.4752, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 22.0879(21.8051) | Bit/dim 3.4590(3.4703) | Xent 2.3026(2.3026) | Loss 3.4590(3.4703) | Error 0.8978(0.8999) Steps 874(872.83) | Grad Norm 2.5214(2.5817) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 21.4642(21.7736) | Bit/dim 3.4766(3.4685) | Xent 2.3026(2.3026) | Loss 3.4766(3.4685) | Error 0.9033(0.9003) Steps 862(873.28) | Grad Norm 2.2942(2.5333) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 21.6543(21.7543) | Bit/dim 3.4703(3.4691) | Xent 2.3026(2.3026) | Loss 3.4703(3.4691) | Error 0.9044(0.9001) Steps 880(873.79) | Grad Norm 3.1255(2.5261) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 21.7120(21.6881) | Bit/dim 3.4784(3.4689) | Xent 2.3026(2.3026) | Loss 3.4784(3.4689) | Error 0.9022(0.8992) Steps 880(873.69) | Grad Norm 2.8092(2.5259) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 21.8624(21.6845) | Bit/dim 3.4792(3.4694) | Xent 2.3026(2.3026) | Loss 3.4792(3.4694) | Error 0.9122(0.9001) Steps 862(873.59) | Grad Norm 3.4628(2.4824) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 21.3278(21.6196) | Bit/dim 3.4543(3.4699) | Xent 2.3026(2.3026) | Loss 3.4543(3.4699) | Error 0.8933(0.9000) Steps 868(873.29) | Grad Norm 3.4139(2.5695) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 111.1403, Epoch Time 1319.8047(1260.7631), Bit/dim 3.4741(best: 3.4726), Xent 2.3026, Loss 3.4741, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 21.3260(21.5888) | Bit/dim 3.4556(3.4684) | Xent 2.3026(2.3026) | Loss 3.4556(3.4684) | Error 0.9089(0.9011) Steps 874(871.51) | Grad Norm 2.5059(2.4678) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 22.5064(21.6981) | Bit/dim 3.4820(3.4685) | Xent 2.3026(2.3026) | Loss 3.4820(3.4685) | Error 0.8867(0.9009) Steps 868(875.43) | Grad Norm 2.5330(2.5036) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 21.5824(21.7069) | Bit/dim 3.4593(3.4679) | Xent 2.3026(2.3026) | Loss 3.4593(3.4679) | Error 0.8889(0.8988) Steps 862(874.55) | Grad Norm 1.5503(2.5626) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 22.4990(21.7637) | Bit/dim 3.4636(3.4664) | Xent 2.3026(2.3026) | Loss 3.4636(3.4664) | Error 0.9089(0.8979) Steps 892(874.60) | Grad Norm 2.5859(2.5741) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 22.2155(21.7851) | Bit/dim 3.4903(3.4696) | Xent 2.3026(2.3026) | Loss 3.4903(3.4696) | Error 0.8944(0.8990) Steps 862(871.58) | Grad Norm 2.6811(2.7114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 111.9748, Epoch Time 1329.2575(1262.8179), Bit/dim 3.4715(best: 3.4726), Xent 2.3026, Loss 3.4715, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 21.2396(21.7839) | Bit/dim 3.4635(3.4716) | Xent 2.3026(2.3026) | Loss 3.4635(3.4716) | Error 0.9056(0.9002) Steps 850(871.64) | Grad Norm 2.4318(2.7134) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 21.4427(21.7587) | Bit/dim 3.4322(3.4686) | Xent 2.3026(2.3026) | Loss 3.4322(3.4686) | Error 0.8967(0.9004) Steps 856(870.83) | Grad Norm 1.6994(2.5303) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 21.7810(21.7661) | Bit/dim 3.4643(3.4687) | Xent 2.3026(2.3026) | Loss 3.4643(3.4687) | Error 0.9011(0.8994) Steps 880(873.47) | Grad Norm 3.9124(2.6088) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 21.9515(21.7991) | Bit/dim 3.4793(3.4675) | Xent 2.3026(2.3026) | Loss 3.4793(3.4675) | Error 0.8967(0.8998) Steps 886(877.11) | Grad Norm 2.8796(2.5585) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 22.2855(21.8307) | Bit/dim 3.4677(3.4667) | Xent 2.3026(2.3026) | Loss 3.4677(3.4667) | Error 0.9144(0.9003) Steps 880(877.58) | Grad Norm 1.4340(2.5608) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 21.7560(21.8806) | Bit/dim 3.4833(3.4687) | Xent 2.3026(2.3026) | Loss 3.4833(3.4687) | Error 0.8867(0.9003) Steps 868(877.40) | Grad Norm 2.7939(2.6921) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 109.3169, Epoch Time 1329.2826(1264.8119), Bit/dim 3.4699(best: 3.4715), Xent 2.3026, Loss 3.4699, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 21.4966(21.8782) | Bit/dim 3.4567(3.4702) | Xent 2.3026(2.3026) | Loss 3.4567(3.4702) | Error 0.9000(0.9005) Steps 874(874.83) | Grad Norm 2.6607(2.7010) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 21.9279(21.8778) | Bit/dim 3.4398(3.4671) | Xent 2.3026(2.3026) | Loss 3.4398(3.4671) | Error 0.8978(0.9001) Steps 856(875.11) | Grad Norm 2.0927(2.6588) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 22.3461(21.8810) | Bit/dim 3.4198(3.4663) | Xent 2.3026(2.3026) | Loss 3.4198(3.4663) | Error 0.9000(0.8999) Steps 892(875.27) | Grad Norm 3.8281(2.5860) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 21.2165(21.8465) | Bit/dim 3.4579(3.4634) | Xent 2.3026(2.3026) | Loss 3.4579(3.4634) | Error 0.8867(0.8998) Steps 874(874.66) | Grad Norm 1.9978(2.5064) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 21.9137(21.9033) | Bit/dim 3.4264(3.4641) | Xent 2.3026(2.3026) | Loss 3.4264(3.4641) | Error 0.9111(0.8999) Steps 880(877.44) | Grad Norm 2.0106(2.5714) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 110.2467, Epoch Time 1332.5028(1266.8426), Bit/dim 3.4643(best: 3.4699), Xent 2.3026, Loss 3.4643, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 21.4020(21.8958) | Bit/dim 3.4389(3.4650) | Xent 2.3026(2.3026) | Loss 3.4389(3.4650) | Error 0.8967(0.9003) Steps 844(875.91) | Grad Norm 3.6008(2.5040) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 21.6750(21.8887) | Bit/dim 3.4505(3.4638) | Xent 2.3026(2.3026) | Loss 3.4505(3.4638) | Error 0.8978(0.9005) Steps 886(874.63) | Grad Norm 2.5047(2.5055) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 21.4869(21.8902) | Bit/dim 3.4735(3.4644) | Xent 2.3026(2.3026) | Loss 3.4735(3.4644) | Error 0.8911(0.9001) Steps 874(873.88) | Grad Norm 2.2348(2.4984) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 22.3807(21.8541) | Bit/dim 3.4478(3.4645) | Xent 2.3026(2.3026) | Loss 3.4478(3.4645) | Error 0.8956(0.9001) Steps 898(873.23) | Grad Norm 3.4291(2.5226) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 21.8334(21.9067) | Bit/dim 3.4421(3.4634) | Xent 2.3026(2.3026) | Loss 3.4421(3.4634) | Error 0.8989(0.8997) Steps 892(876.76) | Grad Norm 2.7518(2.5433) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 21.5124(21.9771) | Bit/dim 3.5123(3.4654) | Xent 2.3026(2.3026) | Loss 3.5123(3.4654) | Error 0.8967(0.9001) Steps 856(877.15) | Grad Norm 1.9911(2.4948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 110.8585, Epoch Time 1336.1143(1268.9207), Bit/dim 3.4652(best: 3.4643), Xent 2.3026, Loss 3.4652, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 21.9535(21.9476) | Bit/dim 3.4520(3.4675) | Xent 2.3026(2.3026) | Loss 3.4520(3.4675) | Error 0.9089(0.8998) Steps 874(876.36) | Grad Norm 2.6886(2.4270) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 22.6895(21.9755) | Bit/dim 3.4558(3.4670) | Xent 2.3026(2.3026) | Loss 3.4558(3.4670) | Error 0.8922(0.9000) Steps 874(878.57) | Grad Norm 2.3263(2.4206) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 22.0862(21.9935) | Bit/dim 3.4828(3.4654) | Xent 2.3026(2.3026) | Loss 3.4828(3.4654) | Error 0.8956(0.8995) Steps 904(880.38) | Grad Norm 1.8266(2.4298) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 21.7903(21.9403) | Bit/dim 3.4568(3.4632) | Xent 2.3026(2.3026) | Loss 3.4568(3.4632) | Error 0.9089(0.8998) Steps 868(880.27) | Grad Norm 3.5848(2.6086) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 22.0739(21.9838) | Bit/dim 3.4432(3.4615) | Xent 2.3026(2.3026) | Loss 3.4432(3.4615) | Error 0.9122(0.9002) Steps 892(879.71) | Grad Norm 1.8908(2.5975) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 110.6220, Epoch Time 1337.4669(1270.9771), Bit/dim 3.4645(best: 3.4643), Xent 2.3026, Loss 3.4645, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 21.7242(21.9949) | Bit/dim 3.4651(3.4622) | Xent 2.3026(2.3026) | Loss 3.4651(3.4622) | Error 0.9122(0.9004) Steps 856(875.93) | Grad Norm 2.7049(2.5475) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 21.8005(22.0777) | Bit/dim 3.4731(3.4650) | Xent 2.3026(2.3026) | Loss 3.4731(3.4650) | Error 0.9122(0.9016) Steps 874(877.55) | Grad Norm 2.0021(2.5134) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 21.8612(22.0907) | Bit/dim 3.4280(3.4625) | Xent 2.3026(2.3026) | Loss 3.4280(3.4625) | Error 0.9089(0.9016) Steps 856(880.51) | Grad Norm 2.5889(2.5422) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 22.1539(22.0718) | Bit/dim 3.4700(3.4607) | Xent 2.3026(2.3026) | Loss 3.4700(3.4607) | Error 0.9078(0.9024) Steps 886(879.48) | Grad Norm 3.2474(2.4489) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 21.3118(22.0750) | Bit/dim 3.4036(3.4599) | Xent 2.3026(2.3026) | Loss 3.4036(3.4599) | Error 0.8711(0.8992) Steps 898(881.53) | Grad Norm 1.6601(2.4559) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 22.3258(22.0708) | Bit/dim 3.4726(3.4615) | Xent 2.3026(2.3026) | Loss 3.4726(3.4615) | Error 0.9033(0.8994) Steps 862(881.12) | Grad Norm 2.6218(2.5096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 110.7066, Epoch Time 1344.9037(1273.1949), Bit/dim 3.4640(best: 3.4643), Xent 2.3026, Loss 3.4640, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 21.3963(22.0776) | Bit/dim 3.4525(3.4598) | Xent 2.3026(2.3026) | Loss 3.4525(3.4598) | Error 0.8989(0.8995) Steps 880(881.10) | Grad Norm 1.9653(2.4405) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 23.0892(22.1722) | Bit/dim 3.4854(3.4609) | Xent 2.3026(2.3026) | Loss 3.4854(3.4609) | Error 0.9111(0.8989) Steps 868(882.48) | Grad Norm 1.5907(2.5020) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 22.0877(22.0667) | Bit/dim 3.4292(3.4609) | Xent 2.3026(2.3026) | Loss 3.4292(3.4609) | Error 0.8944(0.8989) Steps 862(879.29) | Grad Norm 2.2634(2.4383) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 22.0318(22.0739) | Bit/dim 3.4740(3.4605) | Xent 2.3026(2.3026) | Loss 3.4740(3.4605) | Error 0.8978(0.8996) Steps 844(881.41) | Grad Norm 3.1114(2.6061) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 21.4658(22.1081) | Bit/dim 3.4650(3.4606) | Xent 2.3026(2.3026) | Loss 3.4650(3.4606) | Error 0.9111(0.8996) Steps 874(884.33) | Grad Norm 2.1934(2.5299) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 110.5012, Epoch Time 1345.1312(1275.3530), Bit/dim 3.4615(best: 3.4640), Xent 2.3026, Loss 3.4615, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 21.6689(22.0996) | Bit/dim 3.5074(3.4634) | Xent 2.3026(2.3026) | Loss 3.5074(3.4634) | Error 0.8944(0.8998) Steps 880(885.19) | Grad Norm 3.7331(2.5669) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 21.8753(22.0675) | Bit/dim 3.4576(3.4620) | Xent 2.3026(2.3026) | Loss 3.4576(3.4620) | Error 0.8989(0.8994) Steps 892(883.65) | Grad Norm 2.0768(2.4978) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 22.0642(22.0443) | Bit/dim 3.4552(3.4622) | Xent 2.3026(2.3026) | Loss 3.4552(3.4622) | Error 0.9056(0.9007) Steps 910(886.94) | Grad Norm 2.2229(2.4327) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 22.1608(22.0854) | Bit/dim 3.4381(3.4614) | Xent 2.3026(2.3026) | Loss 3.4381(3.4614) | Error 0.8956(0.9012) Steps 880(885.34) | Grad Norm 1.9618(2.4435) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 22.0563(22.1648) | Bit/dim 3.4646(3.4608) | Xent 2.3026(2.3026) | Loss 3.4646(3.4608) | Error 0.9011(0.8994) Steps 886(886.74) | Grad Norm 3.2328(2.5374) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 22.3463(22.1658) | Bit/dim 3.4689(3.4598) | Xent 2.3026(2.3026) | Loss 3.4689(3.4598) | Error 0.9056(0.8999) Steps 892(885.83) | Grad Norm 2.3961(2.5928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 109.1923, Epoch Time 1344.9266(1277.4402), Bit/dim 3.4613(best: 3.4615), Xent 2.3026, Loss 3.4613, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 21.4600(22.0695) | Bit/dim 3.4719(3.4606) | Xent 2.3026(2.3026) | Loss 3.4719(3.4606) | Error 0.8989(0.8993) Steps 862(884.81) | Grad Norm 2.0464(2.4733) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 22.9554(22.1050) | Bit/dim 3.4904(3.4624) | Xent 2.3026(2.3026) | Loss 3.4904(3.4624) | Error 0.9100(0.8994) Steps 874(882.71) | Grad Norm 3.3314(2.5428) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 21.9105(22.0654) | Bit/dim 3.4232(3.4586) | Xent 2.3026(2.3026) | Loss 3.4232(3.4586) | Error 0.8956(0.8990) Steps 862(883.30) | Grad Norm 1.8406(2.3634) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 22.6513(22.0588) | Bit/dim 3.4807(3.4602) | Xent 2.3026(2.3026) | Loss 3.4807(3.4602) | Error 0.9200(0.9013) Steps 886(883.82) | Grad Norm 2.0380(2.5469) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 22.6348(22.0864) | Bit/dim 3.4793(3.4589) | Xent 2.3026(2.3026) | Loss 3.4793(3.4589) | Error 0.9100(0.9004) Steps 904(884.96) | Grad Norm 1.9988(2.3645) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 108.8176, Epoch Time 1340.0075(1279.3172), Bit/dim 3.4579(best: 3.4613), Xent 2.3026, Loss 3.4579, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 22.4007(22.1561) | Bit/dim 3.4233(3.4564) | Xent 2.3026(2.3026) | Loss 3.4233(3.4564) | Error 0.8889(0.8994) Steps 904(886.32) | Grad Norm 2.3319(2.2410) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 21.4839(22.1085) | Bit/dim 3.4878(3.4580) | Xent 2.3026(2.3026) | Loss 3.4878(3.4580) | Error 0.9222(0.8998) Steps 892(886.40) | Grad Norm 1.6616(2.3894) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 21.9823(22.1168) | Bit/dim 3.4391(3.4591) | Xent 2.3026(2.3026) | Loss 3.4391(3.4591) | Error 0.9089(0.9000) Steps 910(888.75) | Grad Norm 2.8758(2.3229) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 21.4996(22.1136) | Bit/dim 3.4507(3.4579) | Xent 2.3026(2.3026) | Loss 3.4507(3.4579) | Error 0.8967(0.9006) Steps 904(889.14) | Grad Norm 1.5712(2.3192) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 22.2062(22.1827) | Bit/dim 3.4595(3.4543) | Xent 2.3026(2.3026) | Loss 3.4595(3.4543) | Error 0.9033(0.8997) Steps 880(888.45) | Grad Norm 3.0944(2.3614) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 22.4069(22.1878) | Bit/dim 3.4681(3.4572) | Xent 2.3026(2.3026) | Loss 3.4681(3.4572) | Error 0.9100(0.9005) Steps 880(889.15) | Grad Norm 1.1990(2.3205) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 110.2219, Epoch Time 1348.3884(1281.3894), Bit/dim 3.4579(best: 3.4579), Xent 2.3026, Loss 3.4579, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 22.0183(22.1005) | Bit/dim 3.5059(3.4580) | Xent 2.3026(2.3026) | Loss 3.5059(3.4580) | Error 0.9167(0.9011) Steps 892(888.92) | Grad Norm 2.7005(2.4205) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 22.1785(22.1243) | Bit/dim 3.4304(3.4574) | Xent 2.3026(2.3026) | Loss 3.4304(3.4574) | Error 0.9056(0.8997) Steps 880(887.00) | Grad Norm 1.8551(2.5375) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 20.9447(22.1042) | Bit/dim 3.4623(3.4553) | Xent 2.3026(2.3026) | Loss 3.4623(3.4553) | Error 0.8944(0.8990) Steps 880(887.67) | Grad Norm 2.1313(2.5071) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 21.8953(22.1188) | Bit/dim 3.4680(3.4573) | Xent 2.3026(2.3026) | Loss 3.4680(3.4573) | Error 0.9111(0.9011) Steps 880(887.94) | Grad Norm 3.7310(2.4740) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 22.0635(22.1052) | Bit/dim 3.4459(3.4581) | Xent 2.3026(2.3026) | Loss 3.4459(3.4581) | Error 0.8900(0.9003) Steps 874(888.03) | Grad Norm 1.8468(2.4007) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 110.0873, Epoch Time 1343.9532(1283.2663), Bit/dim 3.4601(best: 3.4579), Xent 2.3026, Loss 3.4601, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 22.6673(22.1543) | Bit/dim 3.4383(3.4565) | Xent 2.3026(2.3026) | Loss 3.4383(3.4565) | Error 0.9000(0.9002) Steps 910(891.55) | Grad Norm 1.7159(2.4152) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 22.0103(22.2057) | Bit/dim 3.4388(3.4560) | Xent 2.3026(2.3026) | Loss 3.4388(3.4560) | Error 0.8967(0.8997) Steps 868(890.71) | Grad Norm 2.4557(2.3765) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 22.5507(22.3320) | Bit/dim 3.4675(3.4585) | Xent 2.3026(2.3026) | Loss 3.4675(3.4585) | Error 0.8989(0.9001) Steps 892(890.59) | Grad Norm 2.4364(2.5256) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 22.0556(22.3067) | Bit/dim 3.4944(3.4582) | Xent 2.3026(2.3026) | Loss 3.4944(3.4582) | Error 0.8933(0.8993) Steps 904(889.18) | Grad Norm 4.1101(2.5278) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 22.4175(22.3618) | Bit/dim 3.4337(3.4580) | Xent 2.3026(2.3026) | Loss 3.4337(3.4580) | Error 0.8911(0.9003) Steps 898(890.24) | Grad Norm 2.2868(2.5241) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 22.6581(22.3093) | Bit/dim 3.4524(3.4572) | Xent 2.3026(2.3026) | Loss 3.4524(3.4572) | Error 0.9044(0.9005) Steps 910(890.41) | Grad Norm 1.5591(2.3166) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 111.0536, Epoch Time 1359.8973(1285.5652), Bit/dim 3.4545(best: 3.4579), Xent 2.3026, Loss 3.4545, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 22.4222(22.2842) | Bit/dim 3.4482(3.4560) | Xent 2.3026(2.3026) | Loss 3.4482(3.4560) | Error 0.9144(0.9002) Steps 892(891.01) | Grad Norm 1.9725(2.2736) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 22.5815(22.2548) | Bit/dim 3.4930(3.4557) | Xent 2.3026(2.3026) | Loss 3.4930(3.4557) | Error 0.9244(0.8991) Steps 886(890.60) | Grad Norm 2.5986(2.3478) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 22.3142(22.2418) | Bit/dim 3.4742(3.4568) | Xent 2.3026(2.3026) | Loss 3.4742(3.4568) | Error 0.9122(0.8998) Steps 862(889.61) | Grad Norm 2.4434(2.4497) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 22.6589(22.2776) | Bit/dim 3.4533(3.4564) | Xent 2.3026(2.3026) | Loss 3.4533(3.4564) | Error 0.9000(0.9006) Steps 886(889.89) | Grad Norm 3.5677(2.4076) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 22.9434(22.3003) | Bit/dim 3.4170(3.4547) | Xent 2.3026(2.3026) | Loss 3.4170(3.4547) | Error 0.8789(0.9007) Steps 880(890.25) | Grad Norm 2.8420(2.4316) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 110.1194, Epoch Time 1351.5249(1287.5440), Bit/dim 3.4623(best: 3.4545), Xent 2.3026, Loss 3.4623, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 21.9182(22.2580) | Bit/dim 3.4423(3.4559) | Xent 2.3026(2.3026) | Loss 3.4423(3.4559) | Error 0.8967(0.9000) Steps 874(891.21) | Grad Norm 1.6111(2.4738) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 22.2292(22.2151) | Bit/dim 3.4416(3.4560) | Xent 2.3026(2.3026) | Loss 3.4416(3.4560) | Error 0.8944(0.9004) Steps 892(892.37) | Grad Norm 2.9103(2.4272) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 21.2707(22.1684) | Bit/dim 3.4390(3.4522) | Xent 2.3026(2.3026) | Loss 3.4390(3.4522) | Error 0.9056(0.8990) Steps 892(892.95) | Grad Norm 1.7464(2.3855) | Total Time 14.00(14.00)\n",
      "Iter 6690 | Time 21.8724(22.1212) | Bit/dim 3.4789(3.4536) | Xent 2.3026(2.3026) | Loss 3.4789(3.4536) | Error 0.9000(0.8998) Steps 880(892.01) | Grad Norm 2.5179(2.4533) | Total Time 14.00(14.00)\n",
      "Iter 6700 | Time 22.4949(22.2333) | Bit/dim 3.4579(3.4546) | Xent 2.3026(2.3026) | Loss 3.4579(3.4546) | Error 0.9044(0.8999) Steps 898(892.23) | Grad Norm 2.0969(2.3836) | Total Time 14.00(14.00)\n",
      "Iter 6710 | Time 22.8498(22.3119) | Bit/dim 3.4534(3.4526) | Xent 2.3026(2.3026) | Loss 3.4534(3.4526) | Error 0.9011(0.9002) Steps 880(892.03) | Grad Norm 3.1678(2.3459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 110.7107, Epoch Time 1352.4124(1289.4901), Bit/dim 3.4590(best: 3.4545), Xent 2.3026, Loss 3.4590, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 22.6432(22.2986) | Bit/dim 3.4769(3.4537) | Xent 2.3026(2.3026) | Loss 3.4769(3.4537) | Error 0.9044(0.9005) Steps 898(892.83) | Grad Norm 3.4318(2.4208) | Total Time 14.00(14.00)\n",
      "Iter 6730 | Time 22.1847(22.3096) | Bit/dim 3.4998(3.4543) | Xent 2.3026(2.3026) | Loss 3.4998(3.4543) | Error 0.9178(0.9028) Steps 904(895.02) | Grad Norm 2.6894(2.4123) | Total Time 14.00(14.00)\n",
      "Iter 6740 | Time 22.2659(22.2715) | Bit/dim 3.4746(3.4539) | Xent 2.3026(2.3026) | Loss 3.4746(3.4539) | Error 0.8989(0.9002) Steps 892(893.43) | Grad Norm 3.0622(2.4909) | Total Time 14.00(14.00)\n",
      "Iter 6750 | Time 21.2135(22.2231) | Bit/dim 3.4371(3.4540) | Xent 2.3026(2.3026) | Loss 3.4371(3.4540) | Error 0.9022(0.8996) Steps 886(891.37) | Grad Norm 2.6354(2.4018) | Total Time 14.00(14.00)\n",
      "Iter 6760 | Time 22.2044(22.2566) | Bit/dim 3.4563(3.4541) | Xent 2.3026(2.3026) | Loss 3.4563(3.4541) | Error 0.8844(0.8997) Steps 898(892.15) | Grad Norm 2.0317(2.3799) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 108.7553, Epoch Time 1350.7211(1291.3270), Bit/dim 3.4597(best: 3.4545), Xent 2.3026, Loss 3.4597, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 21.4714(22.2412) | Bit/dim 3.4747(3.4531) | Xent 2.3026(2.3026) | Loss 3.4747(3.4531) | Error 0.9022(0.9003) Steps 874(893.12) | Grad Norm 2.4047(2.5226) | Total Time 14.00(14.00)\n",
      "Iter 6780 | Time 22.3664(22.2316) | Bit/dim 3.4540(3.4541) | Xent 2.3026(2.3026) | Loss 3.4540(3.4541) | Error 0.8911(0.9004) Steps 886(892.68) | Grad Norm 2.6403(2.4232) | Total Time 14.00(14.00)\n",
      "Iter 6790 | Time 22.6207(22.3031) | Bit/dim 3.4439(3.4523) | Xent 2.3026(2.3026) | Loss 3.4439(3.4523) | Error 0.9022(0.8996) Steps 868(892.53) | Grad Norm 3.2704(2.5839) | Total Time 14.00(14.00)\n",
      "Iter 6800 | Time 22.2479(22.2647) | Bit/dim 3.4231(3.4514) | Xent 2.3026(2.3026) | Loss 3.4231(3.4514) | Error 0.8844(0.8989) Steps 910(892.42) | Grad Norm 2.1085(2.4619) | Total Time 14.00(14.00)\n",
      "Iter 6810 | Time 21.7244(22.3003) | Bit/dim 3.4308(3.4505) | Xent 2.3026(2.3026) | Loss 3.4308(3.4505) | Error 0.8856(0.8989) Steps 892(894.75) | Grad Norm 3.1583(2.4043) | Total Time 14.00(14.00)\n",
      "Iter 6820 | Time 22.1097(22.2996) | Bit/dim 3.4407(3.4494) | Xent 2.3026(2.3026) | Loss 3.4407(3.4494) | Error 0.9022(0.9000) Steps 910(897.10) | Grad Norm 1.8673(2.3333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 110.3797, Epoch Time 1355.1972(1293.2431), Bit/dim 3.4546(best: 3.4545), Xent 2.3026, Loss 3.4546, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 22.7643(22.3143) | Bit/dim 3.4566(3.4511) | Xent 2.3026(2.3026) | Loss 3.4566(3.4511) | Error 0.9000(0.9002) Steps 892(897.72) | Grad Norm 3.0133(2.3590) | Total Time 14.00(14.00)\n",
      "Iter 6840 | Time 22.4672(22.2950) | Bit/dim 3.4036(3.4502) | Xent 2.3026(2.3026) | Loss 3.4036(3.4502) | Error 0.8933(0.8998) Steps 916(898.56) | Grad Norm 1.6659(2.4491) | Total Time 14.00(14.00)\n",
      "Iter 6850 | Time 22.3217(22.3446) | Bit/dim 3.4645(3.4490) | Xent 2.3026(2.3026) | Loss 3.4645(3.4490) | Error 0.9089(0.8999) Steps 904(899.36) | Grad Norm 2.2155(2.3984) | Total Time 14.00(14.00)\n",
      "Iter 6860 | Time 22.3639(22.3589) | Bit/dim 3.4527(3.4493) | Xent 2.3026(2.3026) | Loss 3.4527(3.4493) | Error 0.9000(0.8997) Steps 892(897.82) | Grad Norm 2.7149(2.4926) | Total Time 14.00(14.00)\n",
      "Iter 6870 | Time 21.7496(22.4236) | Bit/dim 3.4443(3.4509) | Xent 2.3026(2.3026) | Loss 3.4443(3.4509) | Error 0.9044(0.9000) Steps 874(898.94) | Grad Norm 1.6387(2.3744) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 110.8839, Epoch Time 1361.6975(1295.2967), Bit/dim 3.4535(best: 3.4545), Xent 2.3026, Loss 3.4535, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 22.4338(22.3986) | Bit/dim 3.4817(3.4530) | Xent 2.3026(2.3026) | Loss 3.4817(3.4530) | Error 0.8856(0.8994) Steps 898(900.45) | Grad Norm 3.0164(2.4449) | Total Time 14.00(14.00)\n",
      "Iter 6890 | Time 22.2206(22.3430) | Bit/dim 3.4169(3.4523) | Xent 2.3026(2.3026) | Loss 3.4169(3.4523) | Error 0.9011(0.8991) Steps 892(897.68) | Grad Norm 1.5123(2.2920) | Total Time 14.00(14.00)\n",
      "Iter 6900 | Time 22.3934(22.3500) | Bit/dim 3.4241(3.4499) | Xent 2.3026(2.3026) | Loss 3.4241(3.4499) | Error 0.8956(0.8996) Steps 904(898.72) | Grad Norm 2.7435(2.4395) | Total Time 14.00(14.00)\n",
      "Iter 6910 | Time 22.4080(22.3495) | Bit/dim 3.4476(3.4500) | Xent 2.3026(2.3026) | Loss 3.4476(3.4500) | Error 0.9167(0.9006) Steps 910(900.44) | Grad Norm 2.0809(2.3635) | Total Time 14.00(14.00)\n",
      "Iter 6920 | Time 21.8800(22.3488) | Bit/dim 3.4670(3.4511) | Xent 2.3026(2.3026) | Loss 3.4670(3.4511) | Error 0.9044(0.9004) Steps 904(900.01) | Grad Norm 3.6335(2.5053) | Total Time 14.00(14.00)\n",
      "Iter 6930 | Time 22.1478(22.3362) | Bit/dim 3.4273(3.4485) | Xent 2.3026(2.3026) | Loss 3.4273(3.4485) | Error 0.8911(0.9003) Steps 910(899.99) | Grad Norm 1.6560(2.4346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 109.9766, Epoch Time 1355.5693(1297.1049), Bit/dim 3.4522(best: 3.4535), Xent 2.3026, Loss 3.4522, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 22.1283(22.3009) | Bit/dim 3.4514(3.4491) | Xent 2.3026(2.3026) | Loss 3.4514(3.4491) | Error 0.8811(0.8993) Steps 898(899.94) | Grad Norm 2.8915(2.3811) | Total Time 14.00(14.00)\n",
      "Iter 6950 | Time 23.3965(22.3157) | Bit/dim 3.4605(3.4484) | Xent 2.3026(2.3026) | Loss 3.4605(3.4484) | Error 0.9056(0.8993) Steps 904(898.45) | Grad Norm 2.5079(2.5300) | Total Time 14.00(14.00)\n",
      "Iter 6960 | Time 22.3324(22.2921) | Bit/dim 3.4355(3.4459) | Xent 2.3026(2.3026) | Loss 3.4355(3.4459) | Error 0.9078(0.8997) Steps 898(899.12) | Grad Norm 2.1685(2.3793) | Total Time 14.00(14.00)\n",
      "Iter 6970 | Time 22.4384(22.3471) | Bit/dim 3.4972(3.4483) | Xent 2.3026(2.3026) | Loss 3.4972(3.4483) | Error 0.8978(0.9009) Steps 904(899.03) | Grad Norm 2.1848(2.4334) | Total Time 14.00(14.00)\n",
      "Iter 6980 | Time 21.9141(22.3339) | Bit/dim 3.4731(3.4489) | Xent 2.3026(2.3026) | Loss 3.4731(3.4489) | Error 0.9100(0.9003) Steps 874(898.51) | Grad Norm 1.7210(2.3395) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 110.0169, Epoch Time 1354.4331(1298.8247), Bit/dim 3.4520(best: 3.4522), Xent 2.3026, Loss 3.4520, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 22.5235(22.2893) | Bit/dim 3.4685(3.4485) | Xent 2.3026(2.3026) | Loss 3.4685(3.4485) | Error 0.9122(0.9000) Steps 892(898.24) | Grad Norm 2.9521(2.3971) | Total Time 14.00(14.00)\n",
      "Iter 7000 | Time 21.9389(22.3308) | Bit/dim 3.4709(3.4497) | Xent 2.3026(2.3026) | Loss 3.4709(3.4497) | Error 0.9156(0.9021) Steps 898(896.08) | Grad Norm 3.0119(2.3877) | Total Time 14.00(14.00)\n",
      "Iter 7010 | Time 22.3425(22.3209) | Bit/dim 3.4364(3.4486) | Xent 2.3026(2.3026) | Loss 3.4364(3.4486) | Error 0.9022(0.9014) Steps 910(894.89) | Grad Norm 2.9741(2.3906) | Total Time 14.00(14.00)\n",
      "Iter 7020 | Time 22.2939(22.2760) | Bit/dim 3.4204(3.4480) | Xent 2.3026(2.3026) | Loss 3.4204(3.4480) | Error 0.8944(0.8995) Steps 898(896.21) | Grad Norm 2.5996(2.3552) | Total Time 14.00(14.00)\n",
      "Iter 7030 | Time 22.1003(22.2835) | Bit/dim 3.4496(3.4466) | Xent 2.3026(2.3026) | Loss 3.4496(3.4466) | Error 0.9089(0.8998) Steps 910(895.80) | Grad Norm 2.7460(2.3350) | Total Time 14.00(14.00)\n",
      "Iter 7040 | Time 22.2794(22.3398) | Bit/dim 3.4616(3.4479) | Xent 2.3026(2.3026) | Loss 3.4616(3.4479) | Error 0.9033(0.9001) Steps 892(895.87) | Grad Norm 1.8467(2.4332) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 108.7914, Epoch Time 1355.1423(1300.5143), Bit/dim 3.4478(best: 3.4520), Xent 2.3026, Loss 3.4478, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 22.2445(22.3684) | Bit/dim 3.4294(3.4463) | Xent 2.3026(2.3026) | Loss 3.4294(3.4463) | Error 0.9000(0.9014) Steps 892(896.41) | Grad Norm 2.9850(2.4131) | Total Time 14.00(14.00)\n",
      "Iter 7060 | Time 23.1665(22.3756) | Bit/dim 3.4545(3.4491) | Xent 2.3026(2.3026) | Loss 3.4545(3.4491) | Error 0.9067(0.9017) Steps 916(898.39) | Grad Norm 2.2690(2.3814) | Total Time 14.00(14.00)\n",
      "Iter 7070 | Time 21.7849(22.3121) | Bit/dim 3.4076(3.4446) | Xent 2.3026(2.3026) | Loss 3.4076(3.4446) | Error 0.8956(0.9001) Steps 898(900.21) | Grad Norm 1.6335(2.3285) | Total Time 14.00(14.00)\n",
      "Iter 7080 | Time 22.1258(22.3169) | Bit/dim 3.4366(3.4457) | Xent 2.3026(2.3026) | Loss 3.4366(3.4457) | Error 0.8967(0.8995) Steps 892(900.59) | Grad Norm 2.5611(2.3697) | Total Time 14.00(14.00)\n",
      "Iter 7090 | Time 22.0080(22.2547) | Bit/dim 3.4505(3.4470) | Xent 2.3026(2.3026) | Loss 3.4505(3.4470) | Error 0.9078(0.8990) Steps 904(901.12) | Grad Norm 2.6594(2.2649) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 110.0027, Epoch Time 1352.0369(1302.0600), Bit/dim 3.4481(best: 3.4478), Xent 2.3026, Loss 3.4481, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 22.8634(22.2577) | Bit/dim 3.4754(3.4477) | Xent 2.3026(2.3026) | Loss 3.4754(3.4477) | Error 0.9089(0.8994) Steps 910(900.37) | Grad Norm 1.9243(2.3415) | Total Time 14.00(14.00)\n",
      "Iter 7110 | Time 21.7392(22.2854) | Bit/dim 3.4933(3.4463) | Xent 2.3026(2.3026) | Loss 3.4933(3.4463) | Error 0.9156(0.9000) Steps 892(901.60) | Grad Norm 4.0901(2.3480) | Total Time 14.00(14.00)\n",
      "Iter 7120 | Time 22.1637(22.3301) | Bit/dim 3.4476(3.4468) | Xent 2.3026(2.3026) | Loss 3.4476(3.4468) | Error 0.8989(0.9002) Steps 880(900.54) | Grad Norm 2.2066(2.2891) | Total Time 14.00(14.00)\n",
      "Iter 7130 | Time 21.8234(22.3553) | Bit/dim 3.4768(3.4459) | Xent 2.3026(2.3026) | Loss 3.4768(3.4459) | Error 0.8933(0.9007) Steps 910(899.91) | Grad Norm 1.8621(2.2944) | Total Time 14.00(14.00)\n",
      "Iter 7140 | Time 22.4638(22.3733) | Bit/dim 3.4612(3.4481) | Xent 2.3026(2.3026) | Loss 3.4612(3.4481) | Error 0.8944(0.9016) Steps 910(900.91) | Grad Norm 3.2586(2.3027) | Total Time 14.00(14.00)\n",
      "Iter 7150 | Time 21.8806(22.3738) | Bit/dim 3.4600(3.4464) | Xent 2.3026(2.3026) | Loss 3.4600(3.4464) | Error 0.8967(0.8996) Steps 892(902.40) | Grad Norm 2.2245(2.4127) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 109.8474, Epoch Time 1361.3411(1303.8384), Bit/dim 3.4499(best: 3.4478), Xent 2.3026, Loss 3.4499, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 22.0187(22.3901) | Bit/dim 3.4463(3.4485) | Xent 2.3026(2.3026) | Loss 3.4463(3.4485) | Error 0.9100(0.9013) Steps 898(901.68) | Grad Norm 2.4701(2.2797) | Total Time 14.00(14.00)\n",
      "Iter 7170 | Time 21.7346(22.3652) | Bit/dim 3.4546(3.4496) | Xent 2.3026(2.3026) | Loss 3.4546(3.4496) | Error 0.9000(0.9003) Steps 898(899.64) | Grad Norm 2.9240(2.4495) | Total Time 14.00(14.00)\n",
      "Iter 7180 | Time 21.5440(22.3437) | Bit/dim 3.4572(3.4465) | Xent 2.3026(2.3026) | Loss 3.4572(3.4465) | Error 0.8889(0.8992) Steps 880(900.89) | Grad Norm 2.2417(2.3342) | Total Time 14.00(14.00)\n",
      "Iter 7190 | Time 21.9120(22.2955) | Bit/dim 3.4541(3.4477) | Xent 2.3026(2.3026) | Loss 3.4541(3.4477) | Error 0.9089(0.9002) Steps 916(901.55) | Grad Norm 2.5533(2.3904) | Total Time 14.00(14.00)\n",
      "Iter 7200 | Time 22.8596(22.3231) | Bit/dim 3.4465(3.4447) | Xent 2.3026(2.3026) | Loss 3.4465(3.4447) | Error 0.9000(0.8993) Steps 916(904.13) | Grad Norm 2.0098(2.3455) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 107.5770, Epoch Time 1354.5640(1305.3602), Bit/dim 3.4565(best: 3.4478), Xent 2.3026, Loss 3.4565, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 21.7522(22.3289) | Bit/dim 3.4404(3.4445) | Xent 2.3026(2.3026) | Loss 3.4404(3.4445) | Error 0.9000(0.8983) Steps 910(903.96) | Grad Norm 2.1370(2.3725) | Total Time 14.00(14.00)\n",
      "Iter 7220 | Time 23.5273(22.3176) | Bit/dim 3.4347(3.4445) | Xent 2.3026(2.3026) | Loss 3.4347(3.4445) | Error 0.8978(0.9000) Steps 910(904.28) | Grad Norm 1.6498(2.3520) | Total Time 14.00(14.00)\n",
      "Iter 7230 | Time 22.0300(22.3478) | Bit/dim 3.4468(3.4431) | Xent 2.3026(2.3026) | Loss 3.4468(3.4431) | Error 0.9144(0.8999) Steps 910(904.76) | Grad Norm 1.9245(2.3007) | Total Time 14.00(14.00)\n",
      "Iter 7240 | Time 22.6828(22.3095) | Bit/dim 3.4369(3.4456) | Xent 2.3026(2.3026) | Loss 3.4369(3.4456) | Error 0.8778(0.8999) Steps 928(903.42) | Grad Norm 1.8323(2.3966) | Total Time 14.00(14.00)\n",
      "Iter 7250 | Time 22.7379(22.3423) | Bit/dim 3.4232(3.4451) | Xent 2.3026(2.3026) | Loss 3.4232(3.4451) | Error 0.8867(0.8994) Steps 916(905.90) | Grad Norm 2.9322(2.4994) | Total Time 14.00(14.00)\n",
      "Iter 7260 | Time 22.1759(22.3934) | Bit/dim 3.4269(3.4445) | Xent 2.3026(2.3026) | Loss 3.4269(3.4445) | Error 0.8967(0.9000) Steps 898(906.32) | Grad Norm 2.3088(2.3893) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 109.4356, Epoch Time 1357.0916(1306.9121), Bit/dim 3.4481(best: 3.4478), Xent 2.3026, Loss 3.4481, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 22.6186(22.3755) | Bit/dim 3.4700(3.4469) | Xent 2.3026(2.3026) | Loss 3.4700(3.4469) | Error 0.8989(0.9014) Steps 898(906.25) | Grad Norm 3.0360(2.4685) | Total Time 14.00(14.00)\n",
      "Iter 7280 | Time 22.2837(22.3516) | Bit/dim 3.4205(3.4475) | Xent 2.3026(2.3026) | Loss 3.4205(3.4475) | Error 0.8989(0.9010) Steps 880(904.82) | Grad Norm 3.0636(2.4706) | Total Time 14.00(14.00)\n",
      "Iter 7290 | Time 22.4668(22.3423) | Bit/dim 3.4596(3.4448) | Xent 2.3026(2.3026) | Loss 3.4596(3.4448) | Error 0.9000(0.9012) Steps 898(905.01) | Grad Norm 2.7724(2.4461) | Total Time 14.00(14.00)\n",
      "Iter 7300 | Time 22.7782(22.3610) | Bit/dim 3.4671(3.4438) | Xent 2.3026(2.3026) | Loss 3.4671(3.4438) | Error 0.9000(0.9005) Steps 910(904.12) | Grad Norm 3.1951(2.5429) | Total Time 14.00(14.00)\n",
      "Iter 7310 | Time 22.9182(22.4322) | Bit/dim 3.4272(3.4422) | Xent 2.3026(2.3026) | Loss 3.4272(3.4422) | Error 0.8978(0.8990) Steps 916(903.36) | Grad Norm 1.8907(2.3304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 108.1665, Epoch Time 1358.0387(1308.4459), Bit/dim 3.4444(best: 3.4478), Xent 2.3026, Loss 3.4444, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 22.6355(22.4472) | Bit/dim 3.4497(3.4402) | Xent 2.3026(2.3026) | Loss 3.4497(3.4402) | Error 0.9078(0.8998) Steps 898(903.26) | Grad Norm 2.9640(2.2600) | Total Time 14.00(14.00)\n",
      "Iter 7330 | Time 22.4550(22.4677) | Bit/dim 3.4309(3.4400) | Xent 2.3026(2.3026) | Loss 3.4309(3.4400) | Error 0.9144(0.9005) Steps 904(903.16) | Grad Norm 2.1161(2.3876) | Total Time 14.00(14.00)\n",
      "Iter 7340 | Time 22.8631(22.4506) | Bit/dim 3.4470(3.4436) | Xent 2.3026(2.3026) | Loss 3.4470(3.4436) | Error 0.8978(0.9008) Steps 910(904.49) | Grad Norm 1.7409(2.2055) | Total Time 14.00(14.00)\n",
      "Iter 7350 | Time 22.6003(22.3801) | Bit/dim 3.4431(3.4424) | Xent 2.3026(2.3026) | Loss 3.4431(3.4424) | Error 0.8978(0.8992) Steps 922(904.91) | Grad Norm 2.8649(2.3161) | Total Time 14.00(14.00)\n",
      "Iter 7360 | Time 22.3707(22.3719) | Bit/dim 3.4385(3.4429) | Xent 2.3026(2.3026) | Loss 3.4385(3.4429) | Error 0.8900(0.8987) Steps 874(904.19) | Grad Norm 2.7437(2.3212) | Total Time 14.00(14.00)\n",
      "Iter 7370 | Time 22.2723(22.3854) | Bit/dim 3.4038(3.4426) | Xent 2.3026(2.3026) | Loss 3.4038(3.4426) | Error 0.8856(0.8992) Steps 910(903.56) | Grad Norm 2.0232(2.4429) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 109.5941, Epoch Time 1358.9190(1309.9601), Bit/dim 3.4470(best: 3.4444), Xent 2.3026, Loss 3.4470, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 21.3168(22.3759) | Bit/dim 3.4545(3.4413) | Xent 2.3026(2.3026) | Loss 3.4545(3.4413) | Error 0.9122(0.8992) Steps 886(903.70) | Grad Norm 1.3253(2.3973) | Total Time 14.00(14.00)\n",
      "Iter 7390 | Time 22.2086(22.3657) | Bit/dim 3.4476(3.4410) | Xent 2.3026(2.3026) | Loss 3.4476(3.4410) | Error 0.9067(0.8977) Steps 910(905.01) | Grad Norm 1.4534(2.3587) | Total Time 14.00(14.00)\n",
      "Iter 7400 | Time 22.2360(22.3906) | Bit/dim 3.4463(3.4406) | Xent 2.3026(2.3026) | Loss 3.4463(3.4406) | Error 0.9100(0.8985) Steps 916(905.60) | Grad Norm 1.8546(2.1636) | Total Time 14.00(14.00)\n",
      "Iter 7410 | Time 22.3410(22.4094) | Bit/dim 3.4730(3.4424) | Xent 2.3026(2.3026) | Loss 3.4730(3.4424) | Error 0.9133(0.8994) Steps 904(907.13) | Grad Norm 2.0864(2.2994) | Total Time 14.00(14.00)\n",
      "Iter 7420 | Time 22.1475(22.3756) | Bit/dim 3.4540(3.4434) | Xent 2.3026(2.3026) | Loss 3.4540(3.4434) | Error 0.9133(0.9007) Steps 898(908.14) | Grad Norm 1.3939(2.1465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 109.0137, Epoch Time 1357.7590(1311.3941), Bit/dim 3.4497(best: 3.4444), Xent 2.3026, Loss 3.4497, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 22.5218(22.3674) | Bit/dim 3.4483(3.4431) | Xent 2.3026(2.3026) | Loss 3.4483(3.4431) | Error 0.9189(0.9002) Steps 922(908.50) | Grad Norm 2.6698(2.3069) | Total Time 14.00(14.00)\n",
      "Iter 7440 | Time 22.6922(22.4124) | Bit/dim 3.4317(3.4453) | Xent 2.3026(2.3026) | Loss 3.4317(3.4453) | Error 0.8944(0.9003) Steps 922(910.56) | Grad Norm 2.3508(2.2804) | Total Time 14.00(14.00)\n",
      "Iter 7450 | Time 22.1188(22.3556) | Bit/dim 3.3810(3.4417) | Xent 2.3026(2.3026) | Loss 3.3810(3.4417) | Error 0.8822(0.9014) Steps 892(908.20) | Grad Norm 1.2346(2.3633) | Total Time 14.00(14.00)\n",
      "Iter 7460 | Time 22.2605(22.4295) | Bit/dim 3.4205(3.4377) | Xent 2.3026(2.3026) | Loss 3.4205(3.4377) | Error 0.8878(0.8999) Steps 910(908.22) | Grad Norm 1.8964(2.3580) | Total Time 14.00(14.00)\n",
      "Iter 7470 | Time 22.8233(22.4584) | Bit/dim 3.4247(3.4383) | Xent 2.3026(2.3026) | Loss 3.4247(3.4383) | Error 0.9067(0.9003) Steps 898(907.19) | Grad Norm 2.0476(2.2710) | Total Time 14.00(14.00)\n",
      "Iter 7480 | Time 22.2802(22.4501) | Bit/dim 3.4743(3.4420) | Xent 2.3026(2.3026) | Loss 3.4743(3.4420) | Error 0.8844(0.9000) Steps 910(905.98) | Grad Norm 2.7430(2.2680) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 109.4770, Epoch Time 1363.1306(1312.9462), Bit/dim 3.4405(best: 3.4444), Xent 2.3026, Loss 3.4405, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 22.5335(22.4755) | Bit/dim 3.4242(3.4406) | Xent 2.3026(2.3026) | Loss 3.4242(3.4406) | Error 0.8911(0.9002) Steps 922(907.63) | Grad Norm 1.5839(2.3792) | Total Time 14.00(14.00)\n",
      "Iter 7500 | Time 22.6898(22.4898) | Bit/dim 3.4519(3.4404) | Xent 2.3026(2.3026) | Loss 3.4519(3.4404) | Error 0.8944(0.8997) Steps 910(907.84) | Grad Norm 2.4515(2.4344) | Total Time 14.00(14.00)\n",
      "Iter 7510 | Time 21.9003(22.5031) | Bit/dim 3.3890(3.4385) | Xent 2.3026(2.3026) | Loss 3.3890(3.4385) | Error 0.9000(0.8991) Steps 898(908.02) | Grad Norm 2.6663(2.3933) | Total Time 14.00(14.00)\n",
      "Iter 7520 | Time 22.3745(22.5702) | Bit/dim 3.4630(3.4418) | Xent 2.3026(2.3026) | Loss 3.4630(3.4418) | Error 0.9056(0.8998) Steps 916(910.97) | Grad Norm 2.6654(2.5369) | Total Time 14.00(14.00)\n",
      "Iter 7530 | Time 23.1535(22.5669) | Bit/dim 3.4280(3.4423) | Xent 2.3026(2.3026) | Loss 3.4280(3.4423) | Error 0.9056(0.9002) Steps 886(911.18) | Grad Norm 1.9922(2.4393) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 110.1915, Epoch Time 1371.5406(1314.7040), Bit/dim 3.4433(best: 3.4405), Xent 2.3026, Loss 3.4433, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 22.3158(22.6094) | Bit/dim 3.4079(3.4386) | Xent 2.3026(2.3026) | Loss 3.4079(3.4386) | Error 0.8911(0.8997) Steps 916(909.75) | Grad Norm 1.5488(2.4303) | Total Time 14.00(14.00)\n",
      "Iter 7550 | Time 22.4832(22.5541) | Bit/dim 3.4466(3.4385) | Xent 2.3026(2.3026) | Loss 3.4466(3.4385) | Error 0.8944(0.8991) Steps 916(908.97) | Grad Norm 2.3926(2.3777) | Total Time 14.00(14.00)\n",
      "Iter 7560 | Time 22.5897(22.5325) | Bit/dim 3.4407(3.4382) | Xent 2.3026(2.3026) | Loss 3.4407(3.4382) | Error 0.8844(0.8989) Steps 934(909.18) | Grad Norm 2.3426(2.3619) | Total Time 14.00(14.00)\n",
      "Iter 7570 | Time 23.0552(22.4764) | Bit/dim 3.4285(3.4406) | Xent 2.3026(2.3026) | Loss 3.4285(3.4406) | Error 0.9011(0.8998) Steps 922(910.42) | Grad Norm 3.1917(2.2713) | Total Time 14.00(14.00)\n",
      "Iter 7580 | Time 22.2886(22.4783) | Bit/dim 3.4240(3.4397) | Xent 2.3026(2.3026) | Loss 3.4240(3.4397) | Error 0.9111(0.9012) Steps 892(910.42) | Grad Norm 2.2141(2.2745) | Total Time 14.00(14.00)\n",
      "Iter 7590 | Time 21.4943(22.4614) | Bit/dim 3.4418(3.4400) | Xent 2.3026(2.3026) | Loss 3.4418(3.4400) | Error 0.8800(0.9004) Steps 910(909.81) | Grad Norm 2.7020(2.3073) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 112.6999, Epoch Time 1365.2629(1316.2208), Bit/dim 3.4433(best: 3.4405), Xent 2.3026, Loss 3.4433, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 22.3629(22.5199) | Bit/dim 3.4323(3.4405) | Xent 2.3026(2.3026) | Loss 3.4323(3.4405) | Error 0.8989(0.9010) Steps 880(911.28) | Grad Norm 2.4241(2.2353) | Total Time 14.00(14.00)\n",
      "Iter 7610 | Time 21.9950(22.5005) | Bit/dim 3.4333(3.4411) | Xent 2.3026(2.3026) | Loss 3.4333(3.4411) | Error 0.8944(0.9011) Steps 916(912.20) | Grad Norm 2.3202(2.2331) | Total Time 14.00(14.00)\n",
      "Iter 7620 | Time 22.4209(22.4810) | Bit/dim 3.4249(3.4405) | Xent 2.3026(2.3026) | Loss 3.4249(3.4405) | Error 0.9022(0.9011) Steps 916(912.77) | Grad Norm 1.9759(2.1699) | Total Time 14.00(14.00)\n",
      "Iter 7630 | Time 22.2704(22.4371) | Bit/dim 3.4393(3.4409) | Xent 2.3026(2.3026) | Loss 3.4393(3.4409) | Error 0.8922(0.8996) Steps 934(913.77) | Grad Norm 1.8975(2.3416) | Total Time 14.00(14.00)\n",
      "Iter 7640 | Time 22.4289(22.4862) | Bit/dim 3.4185(3.4377) | Xent 2.3026(2.3026) | Loss 3.4185(3.4377) | Error 0.9089(0.8997) Steps 934(917.41) | Grad Norm 2.6276(2.2730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 108.7277, Epoch Time 1365.6647(1317.7041), Bit/dim 3.4451(best: 3.4405), Xent 2.3026, Loss 3.4451, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 23.3029(22.5288) | Bit/dim 3.4513(3.4388) | Xent 2.3026(2.3026) | Loss 3.4513(3.4388) | Error 0.9089(0.9004) Steps 934(917.60) | Grad Norm 2.9948(2.3898) | Total Time 14.00(14.00)\n",
      "Iter 7660 | Time 23.2483(22.5441) | Bit/dim 3.4064(3.4362) | Xent 2.3026(2.3026) | Loss 3.4064(3.4362) | Error 0.8722(0.8989) Steps 928(917.72) | Grad Norm 1.2086(2.2938) | Total Time 14.00(14.00)\n",
      "Iter 7670 | Time 23.5893(22.5850) | Bit/dim 3.4816(3.4399) | Xent 2.3026(2.3026) | Loss 3.4816(3.4399) | Error 0.8933(0.8996) Steps 934(917.20) | Grad Norm 2.3402(2.2849) | Total Time 14.00(14.00)\n",
      "Iter 7680 | Time 22.2234(22.6527) | Bit/dim 3.4374(3.4399) | Xent 2.3026(2.3026) | Loss 3.4374(3.4399) | Error 0.8844(0.8990) Steps 898(916.36) | Grad Norm 3.1218(2.3564) | Total Time 14.00(14.00)\n",
      "Iter 7690 | Time 22.0440(22.5638) | Bit/dim 3.4497(3.4406) | Xent 2.3026(2.3026) | Loss 3.4497(3.4406) | Error 0.8900(0.9000) Steps 916(916.55) | Grad Norm 2.5612(2.2997) | Total Time 14.00(14.00)\n",
      "Iter 7700 | Time 23.3735(22.5143) | Bit/dim 3.4642(3.4353) | Xent 2.3026(2.3026) | Loss 3.4642(3.4353) | Error 0.9100(0.9002) Steps 904(913.63) | Grad Norm 2.0855(2.2675) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 109.0498, Epoch Time 1368.8674(1319.2390), Bit/dim 3.4400(best: 3.4405), Xent 2.3026, Loss 3.4400, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 22.7445(22.5247) | Bit/dim 3.4373(3.4378) | Xent 2.3026(2.3026) | Loss 3.4373(3.4378) | Error 0.8989(0.9002) Steps 940(915.38) | Grad Norm 1.9712(2.3210) | Total Time 14.00(14.00)\n",
      "Iter 7720 | Time 21.7749(22.5337) | Bit/dim 3.4392(3.4379) | Xent 2.3026(2.3026) | Loss 3.4392(3.4379) | Error 0.9033(0.8997) Steps 904(916.81) | Grad Norm 2.9259(2.4078) | Total Time 14.00(14.00)\n",
      "Iter 7730 | Time 22.5547(22.5104) | Bit/dim 3.4322(3.4393) | Xent 2.3026(2.3026) | Loss 3.4322(3.4393) | Error 0.9056(0.8994) Steps 898(914.24) | Grad Norm 1.8620(2.3855) | Total Time 14.00(14.00)\n",
      "Iter 7740 | Time 22.7386(22.5708) | Bit/dim 3.4260(3.4367) | Xent 2.3026(2.3026) | Loss 3.4260(3.4367) | Error 0.8833(0.9000) Steps 934(914.48) | Grad Norm 1.8108(2.2948) | Total Time 14.00(14.00)\n",
      "Iter 7750 | Time 22.9912(22.5510) | Bit/dim 3.4226(3.4358) | Xent 2.3026(2.3026) | Loss 3.4226(3.4358) | Error 0.8978(0.9000) Steps 922(914.02) | Grad Norm 2.9675(2.3194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 112.4208, Epoch Time 1372.6911(1320.8425), Bit/dim 3.4405(best: 3.4400), Xent 2.3026, Loss 3.4405, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 22.6648(22.5973) | Bit/dim 3.4474(3.4355) | Xent 2.3026(2.3026) | Loss 3.4474(3.4355) | Error 0.9122(0.9001) Steps 910(916.13) | Grad Norm 1.6420(2.2828) | Total Time 14.00(14.00)\n",
      "Iter 7770 | Time 22.8222(22.5947) | Bit/dim 3.4121(3.4326) | Xent 2.3026(2.3026) | Loss 3.4121(3.4326) | Error 0.8911(0.9003) Steps 922(915.82) | Grad Norm 3.0449(2.2692) | Total Time 14.00(14.00)\n",
      "Iter 7780 | Time 22.7776(22.5497) | Bit/dim 3.4190(3.4334) | Xent 2.3026(2.3026) | Loss 3.4190(3.4334) | Error 0.8967(0.8998) Steps 904(913.69) | Grad Norm 3.0955(2.3508) | Total Time 14.00(14.00)\n",
      "Iter 7790 | Time 22.3737(22.4941) | Bit/dim 3.4167(3.4349) | Xent 2.3026(2.3026) | Loss 3.4167(3.4349) | Error 0.9000(0.9002) Steps 892(912.94) | Grad Norm 1.8847(2.3209) | Total Time 14.00(14.00)\n",
      "Iter 7800 | Time 22.4552(22.5052) | Bit/dim 3.4465(3.4357) | Xent 2.3026(2.3026) | Loss 3.4465(3.4357) | Error 0.9022(0.8998) Steps 922(914.84) | Grad Norm 1.7016(2.2841) | Total Time 14.00(14.00)\n",
      "Iter 7810 | Time 22.2868(22.4870) | Bit/dim 3.4420(3.4382) | Xent 2.3026(2.3026) | Loss 3.4420(3.4382) | Error 0.9033(0.9000) Steps 922(915.37) | Grad Norm 3.2441(2.3068) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 108.6118, Epoch Time 1363.1213(1322.1109), Bit/dim 3.4357(best: 3.4400), Xent 2.3026, Loss 3.4357, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 22.3889(22.5236) | Bit/dim 3.4053(3.4354) | Xent 2.3026(2.3026) | Loss 3.4053(3.4354) | Error 0.9100(0.8995) Steps 910(914.01) | Grad Norm 1.7266(2.2756) | Total Time 14.00(14.00)\n",
      "Iter 7830 | Time 22.9253(22.5034) | Bit/dim 3.3986(3.4335) | Xent 2.3026(2.3026) | Loss 3.3986(3.4335) | Error 0.8811(0.8987) Steps 910(913.57) | Grad Norm 2.2048(2.3112) | Total Time 14.00(14.00)\n",
      "Iter 7840 | Time 22.8267(22.5765) | Bit/dim 3.4722(3.4374) | Xent 2.3026(2.3026) | Loss 3.4722(3.4374) | Error 0.8978(0.8999) Steps 916(917.01) | Grad Norm 1.9326(2.3389) | Total Time 14.00(14.00)\n",
      "Iter 7850 | Time 22.3549(22.5708) | Bit/dim 3.4345(3.4368) | Xent 2.3026(2.3026) | Loss 3.4345(3.4368) | Error 0.9033(0.9001) Steps 910(916.37) | Grad Norm 2.7710(2.2899) | Total Time 14.00(14.00)\n",
      "Iter 7860 | Time 23.5336(22.5569) | Bit/dim 3.4482(3.4365) | Xent 2.3026(2.3026) | Loss 3.4482(3.4365) | Error 0.8922(0.8999) Steps 940(914.12) | Grad Norm 1.9144(2.2475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 110.4730, Epoch Time 1371.0861(1323.5802), Bit/dim 3.4355(best: 3.4357), Xent 2.3026, Loss 3.4355, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 23.0691(22.5560) | Bit/dim 3.4387(3.4351) | Xent 2.3026(2.3026) | Loss 3.4387(3.4351) | Error 0.9156(0.9007) Steps 916(913.03) | Grad Norm 2.8798(2.1902) | Total Time 14.00(14.00)\n",
      "Iter 7880 | Time 22.9445(22.6255) | Bit/dim 3.4445(3.4341) | Xent 2.3026(2.3026) | Loss 3.4445(3.4341) | Error 0.9222(0.9005) Steps 934(913.94) | Grad Norm 2.1901(2.1185) | Total Time 14.00(14.00)\n",
      "Iter 7890 | Time 22.5474(22.6330) | Bit/dim 3.4573(3.4356) | Xent 2.3026(2.3026) | Loss 3.4573(3.4356) | Error 0.9022(0.9003) Steps 922(915.53) | Grad Norm 2.3142(2.2828) | Total Time 14.00(14.00)\n",
      "Iter 7900 | Time 21.7889(22.5146) | Bit/dim 3.4235(3.4351) | Xent 2.3026(2.3026) | Loss 3.4235(3.4351) | Error 0.8867(0.8991) Steps 910(913.98) | Grad Norm 2.4779(2.2572) | Total Time 14.00(14.00)\n",
      "Iter 7910 | Time 22.5582(22.4788) | Bit/dim 3.4765(3.4371) | Xent 2.3026(2.3026) | Loss 3.4765(3.4371) | Error 0.9078(0.9004) Steps 886(913.71) | Grad Norm 2.1971(2.2866) | Total Time 14.00(14.00)\n",
      "Iter 7920 | Time 21.9324(22.4735) | Bit/dim 3.4042(3.4353) | Xent 2.3026(2.3026) | Loss 3.4042(3.4353) | Error 0.8856(0.9004) Steps 916(914.35) | Grad Norm 2.7356(2.3861) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 111.2813, Epoch Time 1366.9494(1324.8812), Bit/dim 3.4365(best: 3.4355), Xent 2.3026, Loss 3.4365, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 22.4377(22.5109) | Bit/dim 3.4363(3.4351) | Xent 2.3026(2.3026) | Loss 3.4363(3.4351) | Error 0.9100(0.8996) Steps 928(916.59) | Grad Norm 1.6275(2.2645) | Total Time 14.00(14.00)\n",
      "Iter 7940 | Time 22.2087(22.4940) | Bit/dim 3.4190(3.4336) | Xent 2.3026(2.3026) | Loss 3.4190(3.4336) | Error 0.9033(0.8987) Steps 928(918.48) | Grad Norm 2.2676(2.3336) | Total Time 14.00(14.00)\n",
      "Iter 7950 | Time 22.2258(22.4652) | Bit/dim 3.4306(3.4338) | Xent 2.3026(2.3026) | Loss 3.4306(3.4338) | Error 0.9022(0.9006) Steps 892(916.49) | Grad Norm 1.4345(2.1522) | Total Time 14.00(14.00)\n",
      "Iter 7960 | Time 22.1893(22.4811) | Bit/dim 3.4802(3.4359) | Xent 2.3026(2.3026) | Loss 3.4802(3.4359) | Error 0.9111(0.9010) Steps 922(918.53) | Grad Norm 1.7610(2.3086) | Total Time 14.00(14.00)\n",
      "Iter 7970 | Time 23.1507(22.5113) | Bit/dim 3.4292(3.4358) | Xent 2.3026(2.3026) | Loss 3.4292(3.4358) | Error 0.9000(0.9004) Steps 922(917.41) | Grad Norm 2.1199(2.2136) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 111.8934, Epoch Time 1368.7652(1326.1978), Bit/dim 3.4364(best: 3.4355), Xent 2.3026, Loss 3.4364, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 21.9826(22.4884) | Bit/dim 3.4500(3.4355) | Xent 2.3026(2.3026) | Loss 3.4500(3.4355) | Error 0.8867(0.8997) Steps 904(915.80) | Grad Norm 1.7861(2.2109) | Total Time 14.00(14.00)\n",
      "Iter 7990 | Time 22.2950(22.4932) | Bit/dim 3.4296(3.4369) | Xent 2.3026(2.3026) | Loss 3.4296(3.4369) | Error 0.8956(0.9001) Steps 910(913.45) | Grad Norm 3.1307(2.2864) | Total Time 14.00(14.00)\n",
      "Iter 8000 | Time 22.5939(22.4230) | Bit/dim 3.4357(3.4352) | Xent 2.3026(2.3026) | Loss 3.4357(3.4352) | Error 0.8933(0.9009) Steps 928(914.52) | Grad Norm 1.4643(2.2044) | Total Time 14.00(14.00)\n",
      "Iter 8010 | Time 22.8098(22.4913) | Bit/dim 3.4290(3.4337) | Xent 2.3026(2.3026) | Loss 3.4290(3.4337) | Error 0.8978(0.9013) Steps 946(917.53) | Grad Norm 3.1911(2.1775) | Total Time 14.00(14.00)\n",
      "Iter 8020 | Time 22.6807(22.5305) | Bit/dim 3.4129(3.4322) | Xent 2.3026(2.3026) | Loss 3.4129(3.4322) | Error 0.9033(0.9003) Steps 934(918.69) | Grad Norm 1.8542(2.2355) | Total Time 14.00(14.00)\n",
      "Iter 8030 | Time 23.0029(22.5527) | Bit/dim 3.4283(3.4320) | Xent 2.3026(2.3026) | Loss 3.4283(3.4320) | Error 0.8944(0.8998) Steps 928(920.00) | Grad Norm 2.1837(2.2412) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 111.8251, Epoch Time 1368.2584(1327.4596), Bit/dim 3.4339(best: 3.4355), Xent 2.3026, Loss 3.4339, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 22.8607(22.5736) | Bit/dim 3.4085(3.4313) | Xent 2.3026(2.3026) | Loss 3.4085(3.4313) | Error 0.8900(0.9009) Steps 904(919.50) | Grad Norm 2.3395(2.1762) | Total Time 14.00(14.00)\n",
      "Iter 8050 | Time 23.2051(22.5938) | Bit/dim 3.4263(3.4337) | Xent 2.3026(2.3026) | Loss 3.4263(3.4337) | Error 0.8922(0.9018) Steps 940(919.76) | Grad Norm 2.7438(2.3651) | Total Time 14.00(14.00)\n",
      "Iter 8060 | Time 23.0664(22.6823) | Bit/dim 3.4287(3.4333) | Xent 2.3026(2.3026) | Loss 3.4287(3.4333) | Error 0.9089(0.9022) Steps 910(919.56) | Grad Norm 2.6454(2.3081) | Total Time 14.00(14.00)\n",
      "Iter 8070 | Time 23.1249(22.7837) | Bit/dim 3.4398(3.4314) | Xent 2.3026(2.3026) | Loss 3.4398(3.4314) | Error 0.9089(0.9008) Steps 946(923.02) | Grad Norm 2.3224(2.2764) | Total Time 14.00(14.00)\n",
      "Iter 8080 | Time 21.9761(22.7704) | Bit/dim 3.4622(3.4316) | Xent 2.3026(2.3026) | Loss 3.4622(3.4316) | Error 0.8844(0.8985) Steps 916(922.52) | Grad Norm 0.9737(2.2055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 111.7518, Epoch Time 1382.6958(1329.1167), Bit/dim 3.4342(best: 3.4339), Xent 2.3026, Loss 3.4342, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 22.1036(22.7497) | Bit/dim 3.4237(3.4315) | Xent 2.3026(2.3026) | Loss 3.4237(3.4315) | Error 0.8933(0.8985) Steps 916(919.93) | Grad Norm 3.3224(2.1869) | Total Time 14.00(14.00)\n",
      "Iter 8100 | Time 22.3397(22.7272) | Bit/dim 3.4019(3.4284) | Xent 2.3026(2.3026) | Loss 3.4019(3.4284) | Error 0.8911(0.8987) Steps 916(919.72) | Grad Norm 1.9609(2.2286) | Total Time 14.00(14.00)\n",
      "Iter 8110 | Time 22.4386(22.7338) | Bit/dim 3.4618(3.4323) | Xent 2.3026(2.3026) | Loss 3.4618(3.4323) | Error 0.9078(0.8999) Steps 916(921.49) | Grad Norm 1.7167(2.2370) | Total Time 14.00(14.00)\n",
      "Iter 8120 | Time 23.3354(22.7387) | Bit/dim 3.4775(3.4334) | Xent 2.3026(2.3026) | Loss 3.4775(3.4334) | Error 0.9000(0.9004) Steps 946(920.61) | Grad Norm 2.4684(2.3584) | Total Time 14.00(14.00)\n",
      "Iter 8130 | Time 22.7433(22.7437) | Bit/dim 3.4429(3.4329) | Xent 2.3026(2.3026) | Loss 3.4429(3.4329) | Error 0.9033(0.8999) Steps 934(922.72) | Grad Norm 2.3755(2.3974) | Total Time 14.00(14.00)\n",
      "Iter 8140 | Time 22.1304(22.6746) | Bit/dim 3.4340(3.4318) | Xent 2.3026(2.3026) | Loss 3.4340(3.4318) | Error 0.9000(0.8997) Steps 898(920.98) | Grad Norm 3.2506(2.3611) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 111.3652, Epoch Time 1378.0519(1330.5847), Bit/dim 3.4368(best: 3.4339), Xent 2.3026, Loss 3.4368, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 22.7606(22.7143) | Bit/dim 3.4465(3.4330) | Xent 2.3026(2.3026) | Loss 3.4465(3.4330) | Error 0.8989(0.9003) Steps 910(921.12) | Grad Norm 2.2050(2.2677) | Total Time 14.00(14.00)\n",
      "Iter 8160 | Time 22.7252(22.7169) | Bit/dim 3.3825(3.4328) | Xent 2.3026(2.3026) | Loss 3.3825(3.4328) | Error 0.9100(0.9022) Steps 916(921.03) | Grad Norm 2.1849(2.2230) | Total Time 14.00(14.00)\n",
      "Iter 8170 | Time 22.7619(22.7385) | Bit/dim 3.4508(3.4345) | Xent 2.3026(2.3026) | Loss 3.4508(3.4345) | Error 0.9067(0.9007) Steps 940(923.90) | Grad Norm 2.1963(2.2431) | Total Time 14.00(14.00)\n",
      "Iter 8180 | Time 22.5539(22.6965) | Bit/dim 3.4006(3.4316) | Xent 2.3026(2.3026) | Loss 3.4006(3.4316) | Error 0.8911(0.8986) Steps 910(920.94) | Grad Norm 1.7577(2.2844) | Total Time 14.00(14.00)\n",
      "Iter 8190 | Time 22.2018(22.6671) | Bit/dim 3.4266(3.4296) | Xent 2.3026(2.3026) | Loss 3.4266(3.4296) | Error 0.9178(0.8999) Steps 934(921.22) | Grad Norm 2.8882(2.2879) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 112.9327, Epoch Time 1377.9623(1332.0060), Bit/dim 3.4317(best: 3.4339), Xent 2.3026, Loss 3.4317, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 23.2131(22.7062) | Bit/dim 3.4624(3.4292) | Xent 2.3026(2.3026) | Loss 3.4624(3.4292) | Error 0.9156(0.9000) Steps 934(924.49) | Grad Norm 2.5921(2.3270) | Total Time 14.00(14.00)\n",
      "Iter 8210 | Time 23.6565(22.7855) | Bit/dim 3.3980(3.4296) | Xent 2.3026(2.3026) | Loss 3.3980(3.4296) | Error 0.8811(0.9003) Steps 922(925.69) | Grad Norm 1.3492(2.2544) | Total Time 14.00(14.00)\n",
      "Iter 8220 | Time 22.3423(22.7675) | Bit/dim 3.4656(3.4322) | Xent 2.3026(2.3026) | Loss 3.4656(3.4322) | Error 0.8911(0.8999) Steps 934(925.98) | Grad Norm 2.3215(2.3205) | Total Time 14.00(14.00)\n",
      "Iter 8230 | Time 22.3945(22.7452) | Bit/dim 3.4330(3.4291) | Xent 2.3026(2.3026) | Loss 3.4330(3.4291) | Error 0.9067(0.8991) Steps 910(923.75) | Grad Norm 2.0767(2.3545) | Total Time 14.00(14.00)\n",
      "Iter 8240 | Time 23.1139(22.8121) | Bit/dim 3.3910(3.4288) | Xent 2.3026(2.3026) | Loss 3.3910(3.4288) | Error 0.8911(0.8998) Steps 928(922.64) | Grad Norm 1.8992(2.3182) | Total Time 14.00(14.00)\n",
      "Iter 8250 | Time 22.0965(22.7768) | Bit/dim 3.4448(3.4295) | Xent 2.3026(2.3026) | Loss 3.4448(3.4295) | Error 0.9144(0.8997) Steps 916(921.59) | Grad Norm 1.8471(2.1739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 112.7929, Epoch Time 1387.7932(1333.6797), Bit/dim 3.4329(best: 3.4317), Xent 2.3026, Loss 3.4329, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 23.5901(22.8241) | Bit/dim 3.4152(3.4285) | Xent 2.3026(2.3026) | Loss 3.4152(3.4285) | Error 0.8811(0.8995) Steps 922(923.11) | Grad Norm 2.0655(2.1314) | Total Time 14.00(14.00)\n",
      "Iter 8270 | Time 22.6670(22.7761) | Bit/dim 3.4629(3.4269) | Xent 2.3026(2.3026) | Loss 3.4629(3.4269) | Error 0.9056(0.8997) Steps 946(924.24) | Grad Norm 1.4958(2.2481) | Total Time 14.00(14.00)\n",
      "Iter 8280 | Time 23.0348(22.7404) | Bit/dim 3.4229(3.4295) | Xent 2.3026(2.3026) | Loss 3.4229(3.4295) | Error 0.8822(0.8993) Steps 904(920.75) | Grad Norm 2.3563(2.2829) | Total Time 14.00(14.00)\n",
      "Iter 8290 | Time 22.9474(22.7217) | Bit/dim 3.4504(3.4306) | Xent 2.3026(2.3026) | Loss 3.4504(3.4306) | Error 0.9033(0.9000) Steps 940(923.34) | Grad Norm 2.2683(2.2346) | Total Time 14.00(14.00)\n",
      "Iter 8300 | Time 23.6120(22.7539) | Bit/dim 3.4541(3.4291) | Xent 2.3026(2.3026) | Loss 3.4541(3.4291) | Error 0.9156(0.9003) Steps 928(923.67) | Grad Norm 2.6525(2.2161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 113.8280, Epoch Time 1384.0011(1335.1893), Bit/dim 3.4301(best: 3.4317), Xent 2.3026, Loss 3.4301, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 23.0928(22.7763) | Bit/dim 3.4106(3.4318) | Xent 2.3026(2.3026) | Loss 3.4106(3.4318) | Error 0.8967(0.9000) Steps 928(925.62) | Grad Norm 1.3791(2.1738) | Total Time 14.00(14.00)\n",
      "Iter 8320 | Time 23.5220(22.8613) | Bit/dim 3.4118(3.4288) | Xent 2.3026(2.3026) | Loss 3.4118(3.4288) | Error 0.9056(0.9002) Steps 928(925.54) | Grad Norm 2.8260(2.1380) | Total Time 14.00(14.00)\n",
      "Iter 8330 | Time 23.3585(22.9348) | Bit/dim 3.3949(3.4302) | Xent 2.3026(2.3026) | Loss 3.3949(3.4302) | Error 0.9056(0.9004) Steps 958(928.83) | Grad Norm 2.0130(2.2226) | Total Time 14.00(14.00)\n",
      "Iter 8340 | Time 23.5744(22.9527) | Bit/dim 3.4113(3.4277) | Xent 2.3026(2.3026) | Loss 3.4113(3.4277) | Error 0.8956(0.8993) Steps 952(929.45) | Grad Norm 2.4653(2.2826) | Total Time 14.00(14.00)\n",
      "Iter 8350 | Time 23.1509(22.8813) | Bit/dim 3.4213(3.4285) | Xent 2.3026(2.3026) | Loss 3.4213(3.4285) | Error 0.8856(0.8988) Steps 940(927.72) | Grad Norm 1.0657(2.1657) | Total Time 14.00(14.00)\n",
      "Iter 8360 | Time 23.0631(22.8675) | Bit/dim 3.4355(3.4288) | Xent 2.3026(2.3026) | Loss 3.4355(3.4288) | Error 0.9100(0.8999) Steps 910(926.53) | Grad Norm 1.9509(2.1870) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 111.3812, Epoch Time 1390.2461(1336.8410), Bit/dim 3.4306(best: 3.4301), Xent 2.3026, Loss 3.4306, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 23.6285(22.9180) | Bit/dim 3.4233(3.4276) | Xent 2.3026(2.3026) | Loss 3.4233(3.4276) | Error 0.9111(0.9004) Steps 934(924.93) | Grad Norm 3.0170(2.1732) | Total Time 14.00(14.00)\n",
      "Iter 8380 | Time 23.0741(22.8647) | Bit/dim 3.3705(3.4298) | Xent 2.3026(2.3026) | Loss 3.3705(3.4298) | Error 0.8856(0.9013) Steps 940(926.71) | Grad Norm 1.7645(2.1705) | Total Time 14.00(14.00)\n",
      "Iter 8390 | Time 23.5619(22.8367) | Bit/dim 3.3953(3.4253) | Xent 2.3026(2.3026) | Loss 3.3953(3.4253) | Error 0.9067(0.9016) Steps 946(928.52) | Grad Norm 2.8784(2.3385) | Total Time 14.00(14.00)\n",
      "Iter 8400 | Time 22.5440(22.8345) | Bit/dim 3.4112(3.4257) | Xent 2.3026(2.3026) | Loss 3.4112(3.4257) | Error 0.8889(0.8986) Steps 934(928.98) | Grad Norm 2.5105(2.3651) | Total Time 14.00(14.00)\n",
      "Iter 8410 | Time 23.4947(22.8790) | Bit/dim 3.4127(3.4269) | Xent 2.3026(2.3026) | Loss 3.4127(3.4269) | Error 0.8978(0.8985) Steps 952(931.75) | Grad Norm 2.8757(2.3704) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 112.3119, Epoch Time 1389.4259(1338.4186), Bit/dim 3.4315(best: 3.4301), Xent 2.3026, Loss 3.4315, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 22.7189(22.8860) | Bit/dim 3.4507(3.4272) | Xent 2.3026(2.3026) | Loss 3.4507(3.4272) | Error 0.9022(0.9001) Steps 940(931.99) | Grad Norm 2.0814(2.2958) | Total Time 14.00(14.00)\n",
      "Iter 8430 | Time 22.3445(22.8860) | Bit/dim 3.4556(3.4304) | Xent 2.3026(2.3026) | Loss 3.4556(3.4304) | Error 0.9133(0.9005) Steps 934(932.29) | Grad Norm 2.6331(2.3329) | Total Time 14.00(14.00)\n",
      "Iter 8440 | Time 22.6515(22.8347) | Bit/dim 3.4417(3.4294) | Xent 2.3026(2.3026) | Loss 3.4417(3.4294) | Error 0.9056(0.8995) Steps 928(930.27) | Grad Norm 2.4122(2.3037) | Total Time 14.00(14.00)\n",
      "Iter 8450 | Time 22.2730(22.8362) | Bit/dim 3.4240(3.4274) | Xent 2.3026(2.3026) | Loss 3.4240(3.4274) | Error 0.9011(0.8993) Steps 928(931.78) | Grad Norm 2.7474(2.2081) | Total Time 14.00(14.00)\n",
      "Iter 8460 | Time 23.2616(22.8708) | Bit/dim 3.4413(3.4283) | Xent 2.3026(2.3026) | Loss 3.4413(3.4283) | Error 0.8811(0.8995) Steps 934(930.46) | Grad Norm 2.4574(2.2341) | Total Time 14.00(14.00)\n",
      "Iter 8470 | Time 23.4441(22.9123) | Bit/dim 3.4524(3.4266) | Xent 2.3026(2.3026) | Loss 3.4524(3.4266) | Error 0.9144(0.9005) Steps 940(931.23) | Grad Norm 2.4654(2.2235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 113.1974, Epoch Time 1389.8818(1339.9625), Bit/dim 3.4279(best: 3.4301), Xent 2.3026, Loss 3.4279, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 22.7768(22.9464) | Bit/dim 3.4164(3.4261) | Xent 2.3026(2.3026) | Loss 3.4164(3.4261) | Error 0.8933(0.8997) Steps 940(931.92) | Grad Norm 2.1160(2.2581) | Total Time 14.00(14.00)\n",
      "Iter 8490 | Time 22.6301(22.9298) | Bit/dim 3.4259(3.4286) | Xent 2.3026(2.3026) | Loss 3.4259(3.4286) | Error 0.8967(0.8993) Steps 910(928.69) | Grad Norm 2.3290(2.1582) | Total Time 14.00(14.00)\n",
      "Iter 8500 | Time 23.3172(23.0148) | Bit/dim 3.4524(3.4317) | Xent 2.3026(2.3026) | Loss 3.4524(3.4317) | Error 0.8989(0.9002) Steps 946(928.48) | Grad Norm 1.8869(2.1715) | Total Time 14.00(14.00)\n",
      "Iter 8510 | Time 23.2290(23.0280) | Bit/dim 3.4362(3.4282) | Xent 2.3026(2.3026) | Loss 3.4362(3.4282) | Error 0.9189(0.9004) Steps 946(929.89) | Grad Norm 3.1535(2.2425) | Total Time 14.00(14.00)\n",
      "Iter 8520 | Time 22.0392(23.0078) | Bit/dim 3.4159(3.4242) | Xent 2.3026(2.3026) | Loss 3.4159(3.4242) | Error 0.9044(0.9006) Steps 928(930.36) | Grad Norm 2.5329(2.2183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 112.3005, Epoch Time 1398.9104(1341.7309), Bit/dim 3.4298(best: 3.4279), Xent 2.3026, Loss 3.4298, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 23.5109(23.0490) | Bit/dim 3.4256(3.4241) | Xent 2.3026(2.3026) | Loss 3.4256(3.4241) | Error 0.9100(0.9000) Steps 910(931.35) | Grad Norm 2.1641(2.1586) | Total Time 14.00(14.00)\n",
      "Iter 8540 | Time 22.2439(22.9521) | Bit/dim 3.4471(3.4256) | Xent 2.3026(2.3026) | Loss 3.4471(3.4256) | Error 0.9167(0.9008) Steps 916(931.81) | Grad Norm 2.6479(2.2095) | Total Time 14.00(14.00)\n",
      "Iter 8550 | Time 22.7701(22.9207) | Bit/dim 3.4395(3.4257) | Xent 2.3026(2.3026) | Loss 3.4395(3.4257) | Error 0.9089(0.9004) Steps 922(930.83) | Grad Norm 1.8974(2.0893) | Total Time 14.00(14.00)\n",
      "Iter 8560 | Time 22.5840(22.8970) | Bit/dim 3.4539(3.4254) | Xent 2.3026(2.3026) | Loss 3.4539(3.4254) | Error 0.8956(0.9004) Steps 946(932.37) | Grad Norm 2.2191(2.2465) | Total Time 14.00(14.00)\n",
      "Iter 8570 | Time 23.6034(22.9184) | Bit/dim 3.4171(3.4264) | Xent 2.3026(2.3026) | Loss 3.4171(3.4264) | Error 0.8922(0.9002) Steps 940(931.89) | Grad Norm 1.3654(2.2372) | Total Time 14.00(14.00)\n",
      "Iter 8580 | Time 23.1740(22.9011) | Bit/dim 3.4274(3.4273) | Xent 2.3026(2.3026) | Loss 3.4274(3.4273) | Error 0.8878(0.8993) Steps 952(932.40) | Grad Norm 2.7109(2.1558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 112.7496, Epoch Time 1388.5091(1343.1342), Bit/dim 3.4308(best: 3.4279), Xent 2.3026, Loss 3.4308, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 22.4687(22.8632) | Bit/dim 3.4136(3.4265) | Xent 2.3026(2.3026) | Loss 3.4136(3.4265) | Error 0.9011(0.9002) Steps 928(931.44) | Grad Norm 1.7605(2.2429) | Total Time 14.00(14.00)\n",
      "Iter 8600 | Time 23.4759(22.8889) | Bit/dim 3.4026(3.4258) | Xent 2.3026(2.3026) | Loss 3.4026(3.4258) | Error 0.9122(0.9000) Steps 946(932.67) | Grad Norm 1.4655(2.2279) | Total Time 14.00(14.00)\n",
      "Iter 8610 | Time 23.2203(22.8629) | Bit/dim 3.4387(3.4262) | Xent 2.3026(2.3026) | Loss 3.4387(3.4262) | Error 0.9167(0.9008) Steps 934(933.00) | Grad Norm 1.6217(2.0634) | Total Time 14.00(14.00)\n",
      "Iter 8620 | Time 22.9737(22.8959) | Bit/dim 3.3963(3.4260) | Xent 2.3026(2.3026) | Loss 3.3963(3.4260) | Error 0.8856(0.9002) Steps 946(934.67) | Grad Norm 2.3010(2.1958) | Total Time 14.00(14.00)\n",
      "Iter 8630 | Time 22.9923(22.8606) | Bit/dim 3.4408(3.4267) | Xent 2.3026(2.3026) | Loss 3.4408(3.4267) | Error 0.9167(0.8997) Steps 940(934.72) | Grad Norm 2.2021(2.1834) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 114.8150, Epoch Time 1389.8810(1344.5366), Bit/dim 3.4254(best: 3.4279), Xent 2.3026, Loss 3.4254, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 22.5257(22.8506) | Bit/dim 3.4092(3.4252) | Xent 2.3026(2.3026) | Loss 3.4092(3.4252) | Error 0.8767(0.8992) Steps 922(934.13) | Grad Norm 2.6608(2.2577) | Total Time 14.00(14.00)\n",
      "Iter 8650 | Time 22.3456(22.8726) | Bit/dim 3.4208(3.4247) | Xent 2.3026(2.3026) | Loss 3.4208(3.4247) | Error 0.9100(0.9011) Steps 928(933.16) | Grad Norm 1.7133(2.1644) | Total Time 14.00(14.00)\n",
      "Iter 8660 | Time 23.3354(22.8808) | Bit/dim 3.4467(3.4256) | Xent 2.3026(2.3026) | Loss 3.4467(3.4256) | Error 0.9011(0.9005) Steps 940(931.90) | Grad Norm 2.4063(2.1421) | Total Time 14.00(14.00)\n",
      "Iter 8670 | Time 22.6743(22.8823) | Bit/dim 3.4122(3.4237) | Xent 2.3026(2.3026) | Loss 3.4122(3.4237) | Error 0.8944(0.9005) Steps 928(930.81) | Grad Norm 1.7082(2.2127) | Total Time 14.00(14.00)\n",
      "Iter 8680 | Time 23.5414(22.8484) | Bit/dim 3.4369(3.4259) | Xent 2.3026(2.3026) | Loss 3.4369(3.4259) | Error 0.8944(0.8995) Steps 970(930.97) | Grad Norm 1.9375(2.2032) | Total Time 14.00(14.00)\n",
      "Iter 8690 | Time 22.6255(22.8588) | Bit/dim 3.4027(3.4249) | Xent 2.3026(2.3026) | Loss 3.4027(3.4249) | Error 0.9056(0.8995) Steps 922(930.10) | Grad Norm 2.0463(2.2548) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 112.4203, Epoch Time 1388.7655(1345.8635), Bit/dim 3.4256(best: 3.4254), Xent 2.3026, Loss 3.4256, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 23.0217(22.8292) | Bit/dim 3.4096(3.4285) | Xent 2.3026(2.3026) | Loss 3.4096(3.4285) | Error 0.9056(0.9001) Steps 916(927.84) | Grad Norm 1.6757(2.1768) | Total Time 14.00(14.00)\n",
      "Iter 8710 | Time 23.6074(22.8540) | Bit/dim 3.3784(3.4248) | Xent 2.3026(2.3026) | Loss 3.3784(3.4248) | Error 0.8889(0.8992) Steps 928(924.19) | Grad Norm 2.3607(2.1348) | Total Time 14.00(14.00)\n",
      "Iter 8720 | Time 22.6654(22.8381) | Bit/dim 3.3974(3.4228) | Xent 2.3026(2.3026) | Loss 3.3974(3.4228) | Error 0.8822(0.8986) Steps 928(926.30) | Grad Norm 1.8975(2.1662) | Total Time 14.00(14.00)\n",
      "Iter 8730 | Time 23.3915(22.8772) | Bit/dim 3.4065(3.4237) | Xent 2.3026(2.3026) | Loss 3.4065(3.4237) | Error 0.9078(0.8996) Steps 922(926.51) | Grad Norm 2.0384(2.2705) | Total Time 14.00(14.00)\n",
      "Iter 8740 | Time 22.6702(22.9333) | Bit/dim 3.4369(3.4227) | Xent 2.3026(2.3026) | Loss 3.4369(3.4227) | Error 0.9044(0.9000) Steps 946(928.59) | Grad Norm 2.7753(2.2267) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 112.3914, Epoch Time 1390.7252(1347.2094), Bit/dim 3.4308(best: 3.4254), Xent 2.3026, Loss 3.4308, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 23.7379(22.9990) | Bit/dim 3.4196(3.4227) | Xent 2.3026(2.3026) | Loss 3.4196(3.4227) | Error 0.9011(0.9003) Steps 928(932.42) | Grad Norm 2.7198(2.2733) | Total Time 14.00(14.00)\n",
      "Iter 8760 | Time 23.4658(23.0440) | Bit/dim 3.4526(3.4236) | Xent 2.3026(2.3026) | Loss 3.4526(3.4236) | Error 0.9089(0.9006) Steps 952(933.68) | Grad Norm 1.8826(2.3421) | Total Time 14.00(14.00)\n",
      "Iter 8770 | Time 23.9323(23.0140) | Bit/dim 3.4166(3.4217) | Xent 2.3026(2.3026) | Loss 3.4166(3.4217) | Error 0.8933(0.9003) Steps 928(932.40) | Grad Norm 2.0872(2.2546) | Total Time 14.00(14.00)\n",
      "Iter 8780 | Time 22.6410(23.0047) | Bit/dim 3.4430(3.4229) | Xent 2.3026(2.3026) | Loss 3.4430(3.4229) | Error 0.8744(0.8995) Steps 910(931.46) | Grad Norm 1.1291(2.1902) | Total Time 14.00(14.00)\n",
      "Iter 8790 | Time 23.2336(22.9625) | Bit/dim 3.4355(3.4242) | Xent 2.3026(2.3026) | Loss 3.4355(3.4242) | Error 0.9067(0.9007) Steps 916(928.44) | Grad Norm 3.3741(2.1206) | Total Time 14.00(14.00)\n",
      "Iter 8800 | Time 23.0066(22.9507) | Bit/dim 3.4506(3.4243) | Xent 2.3026(2.3026) | Loss 3.4506(3.4243) | Error 0.9000(0.8998) Steps 928(929.64) | Grad Norm 3.0309(2.2304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 113.6963, Epoch Time 1397.5563(1348.7198), Bit/dim 3.4276(best: 3.4254), Xent 2.3026, Loss 3.4276, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 22.5172(22.9085) | Bit/dim 3.4493(3.4233) | Xent 2.3026(2.3026) | Loss 3.4493(3.4233) | Error 0.9056(0.9001) Steps 910(928.97) | Grad Norm 1.8906(2.1987) | Total Time 14.00(14.00)\n",
      "Iter 8820 | Time 23.5438(22.9487) | Bit/dim 3.4202(3.4242) | Xent 2.3026(2.3026) | Loss 3.4202(3.4242) | Error 0.8867(0.9000) Steps 958(928.84) | Grad Norm 1.4940(2.1290) | Total Time 14.00(14.00)\n",
      "Iter 8830 | Time 22.3597(22.9247) | Bit/dim 3.4206(3.4218) | Xent 2.3026(2.3026) | Loss 3.4206(3.4218) | Error 0.8889(0.8992) Steps 934(928.08) | Grad Norm 1.6482(2.1558) | Total Time 14.00(14.00)\n",
      "Iter 8840 | Time 22.9481(22.9286) | Bit/dim 3.4339(3.4234) | Xent 2.3026(2.3026) | Loss 3.4339(3.4234) | Error 0.8933(0.8998) Steps 922(925.98) | Grad Norm 1.8421(2.0978) | Total Time 14.00(14.00)\n",
      "Iter 8850 | Time 23.9302(23.0098) | Bit/dim 3.4095(3.4233) | Xent 2.3026(2.3026) | Loss 3.4095(3.4233) | Error 0.9011(0.9002) Steps 952(928.97) | Grad Norm 1.9338(2.2146) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 114.1130, Epoch Time 1396.5515(1350.1547), Bit/dim 3.4261(best: 3.4254), Xent 2.3026, Loss 3.4261, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 22.9295(23.0012) | Bit/dim 3.4251(3.4235) | Xent 2.3026(2.3026) | Loss 3.4251(3.4235) | Error 0.8933(0.8996) Steps 934(930.00) | Grad Norm 1.8627(2.1775) | Total Time 14.00(14.00)\n",
      "Iter 8870 | Time 22.8928(23.0367) | Bit/dim 3.3998(3.4254) | Xent 2.3026(2.3026) | Loss 3.3998(3.4254) | Error 0.8967(0.9000) Steps 916(930.09) | Grad Norm 2.0197(2.0907) | Total Time 14.00(14.00)\n",
      "Iter 8880 | Time 23.2358(22.9641) | Bit/dim 3.4214(3.4224) | Xent 2.3026(2.3026) | Loss 3.4214(3.4224) | Error 0.9033(0.8991) Steps 946(929.51) | Grad Norm 2.4317(2.2493) | Total Time 14.00(14.00)\n",
      "Iter 8890 | Time 22.7784(23.0091) | Bit/dim 3.4358(3.4237) | Xent 2.3026(2.3026) | Loss 3.4358(3.4237) | Error 0.9244(0.9000) Steps 946(930.34) | Grad Norm 2.2249(2.2064) | Total Time 14.00(14.00)\n",
      "Iter 8900 | Time 23.3322(23.0600) | Bit/dim 3.4116(3.4201) | Xent 2.3026(2.3026) | Loss 3.4116(3.4201) | Error 0.8878(0.9002) Steps 898(928.60) | Grad Norm 2.8224(2.1503) | Total Time 14.00(14.00)\n",
      "Iter 8910 | Time 23.2469(23.0601) | Bit/dim 3.4085(3.4202) | Xent 2.3026(2.3026) | Loss 3.4085(3.4202) | Error 0.9033(0.9008) Steps 946(929.03) | Grad Norm 1.8626(2.1822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 113.9615, Epoch Time 1400.2928(1351.6589), Bit/dim 3.4246(best: 3.4254), Xent 2.3026, Loss 3.4246, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 22.7660(23.0181) | Bit/dim 3.4548(3.4216) | Xent 2.3026(2.3026) | Loss 3.4548(3.4216) | Error 0.9056(0.9013) Steps 940(930.48) | Grad Norm 1.8012(2.1768) | Total Time 14.00(14.00)\n",
      "Iter 8930 | Time 23.1511(23.0416) | Bit/dim 3.3924(3.4219) | Xent 2.3026(2.3026) | Loss 3.3924(3.4219) | Error 0.8900(0.9017) Steps 916(932.56) | Grad Norm 2.8451(2.1638) | Total Time 14.00(14.00)\n",
      "Iter 8940 | Time 23.3279(23.0313) | Bit/dim 3.3945(3.4193) | Xent 2.3026(2.3026) | Loss 3.3945(3.4193) | Error 0.8911(0.8997) Steps 934(934.60) | Grad Norm 3.4282(2.2490) | Total Time 14.00(14.00)\n",
      "Iter 8950 | Time 23.5368(23.1039) | Bit/dim 3.4203(3.4209) | Xent 2.3026(2.3026) | Loss 3.4203(3.4209) | Error 0.8944(0.9001) Steps 952(934.81) | Grad Norm 2.2929(2.2307) | Total Time 14.00(14.00)\n",
      "Iter 8960 | Time 23.0707(23.1256) | Bit/dim 3.3768(3.4215) | Xent 2.3026(2.3026) | Loss 3.3768(3.4215) | Error 0.8889(0.8993) Steps 958(935.90) | Grad Norm 2.4388(2.3180) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 113.5635, Epoch Time 1402.0567(1353.1708), Bit/dim 3.4240(best: 3.4246), Xent 2.3026, Loss 3.4240, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 22.9278(23.1028) | Bit/dim 3.3920(3.4220) | Xent 2.3026(2.3026) | Loss 3.3920(3.4220) | Error 0.9000(0.9000) Steps 940(934.82) | Grad Norm 1.4475(2.1795) | Total Time 14.00(14.00)\n",
      "Iter 8980 | Time 23.5252(23.1198) | Bit/dim 3.4288(3.4225) | Xent 2.3026(2.3026) | Loss 3.4288(3.4225) | Error 0.8978(0.9001) Steps 922(933.62) | Grad Norm 2.9124(2.1612) | Total Time 14.00(14.00)\n",
      "Iter 8990 | Time 23.5093(23.1592) | Bit/dim 3.4730(3.4235) | Xent 2.3026(2.3026) | Loss 3.4730(3.4235) | Error 0.8989(0.8996) Steps 940(935.06) | Grad Norm 1.0812(2.0143) | Total Time 14.00(14.00)\n",
      "Iter 9000 | Time 23.3832(23.1537) | Bit/dim 3.4105(3.4220) | Xent 2.3026(2.3026) | Loss 3.4105(3.4220) | Error 0.8833(0.8985) Steps 952(935.82) | Grad Norm 2.4216(2.1129) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_75_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.75 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
