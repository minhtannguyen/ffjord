{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 41.0093(71.9427) | Bit/dim 9.2690(9.3725) | Xent 2.2818(2.3001) | Loss 10.4099(10.5226) | Error 0.7689(0.8795) Steps 562(562.00) | Grad Norm 17.2267(20.6892) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 40.0175(63.5421) | Bit/dim 8.7310(9.2491) | Xent 2.2257(2.2867) | Loss 9.8439(10.3925) | Error 0.7478(0.8451) Steps 562(562.00) | Grad Norm 8.1859(18.4166) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 40.0869(57.4350) | Bit/dim 8.4654(9.0632) | Xent 2.1722(2.2625) | Loss 9.5515(10.1945) | Error 0.7489(0.8197) Steps 556(561.82) | Grad Norm 3.8203(14.8759) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 38.9673(52.8643) | Bit/dim 8.2365(8.8653) | Xent 2.1242(2.2323) | Loss 9.2985(9.9815) | Error 0.7367(0.8000) Steps 544(559.59) | Grad Norm 3.6205(11.9963) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 40.8922(49.4602) | Bit/dim 7.9707(8.6726) | Xent 2.0889(2.2008) | Loss 9.0152(9.7730) | Error 0.7122(0.7809) Steps 544(556.08) | Grad Norm 2.6458(9.6779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 100.3571, Epoch Time 2365.1351(2365.1351), Bit/dim 7.8596(best: inf), Xent 2.0722, Loss 8.8958, Error 0.6995(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 38.6318(47.0279) | Bit/dim 7.7290(8.4627) | Xent 2.0832(2.1691) | Loss 8.7706(9.5473) | Error 0.7089(0.7599) Steps 550(554.06) | Grad Norm 2.6063(7.9004) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 39.9127(45.0196) | Bit/dim 7.4596(8.2311) | Xent 2.0712(2.1409) | Loss 8.4952(9.3016) | Error 0.6922(0.7412) Steps 556(553.35) | Grad Norm 2.3138(6.4821) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 40.1569(43.7452) | Bit/dim 7.2539(7.9949) | Xent 2.0527(2.1195) | Loss 8.2802(9.0546) | Error 0.6767(0.7277) Steps 556(554.04) | Grad Norm 1.8556(5.3248) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 39.8181(42.8638) | Bit/dim 7.1397(7.7813) | Xent 2.0717(2.1056) | Loss 8.1755(8.8341) | Error 0.7000(0.7165) Steps 562(554.91) | Grad Norm 1.7627(4.3895) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 39.7530(42.0576) | Bit/dim 7.0567(7.6021) | Xent 2.0744(2.0937) | Loss 8.0939(8.6489) | Error 0.6978(0.7106) Steps 556(555.93) | Grad Norm 1.6862(3.6073) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 40.1959(41.6865) | Bit/dim 7.0212(7.4534) | Xent 2.0552(2.0845) | Loss 8.0488(8.4957) | Error 0.6711(0.7054) Steps 568(558.21) | Grad Norm 0.9130(3.1000) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 96.6157, Epoch Time 2322.7674(2363.8641), Bit/dim 7.0242(best: 7.8596), Xent 2.0442, Loss 8.0463, Error 0.6798(best: 0.6995)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 39.6187(41.4558) | Bit/dim 7.0224(7.3375) | Xent 2.0476(2.0775) | Loss 8.0462(8.3763) | Error 0.6900(0.7014) Steps 568(560.78) | Grad Norm 2.0320(2.7002) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 40.6692(41.2653) | Bit/dim 6.9470(7.2426) | Xent 2.0602(2.0690) | Loss 7.9771(8.2771) | Error 0.7044(0.6992) Steps 574(563.20) | Grad Norm 0.9615(2.2809) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 40.8003(41.1696) | Bit/dim 6.9292(7.1631) | Xent 2.0235(2.0594) | Loss 7.9410(8.1928) | Error 0.6911(0.6961) Steps 574(566.04) | Grad Norm 2.0049(2.2166) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 40.9499(41.0333) | Bit/dim 6.8838(7.0932) | Xent 2.0258(2.0501) | Loss 7.8967(8.1183) | Error 0.6878(0.6945) Steps 574(568.13) | Grad Norm 1.3835(2.5979) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 42.0613(40.9701) | Bit/dim 6.8339(7.0272) | Xent 2.0211(2.0423) | Loss 7.8444(8.0484) | Error 0.6989(0.6913) Steps 574(569.51) | Grad Norm 2.3472(2.8288) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 96.2909, Epoch Time 2360.3750(2363.7594), Bit/dim 6.7827(best: 7.0242), Xent 2.0214, Loss 7.7934, Error 0.6853(best: 0.6798)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 44.3381(41.4654) | Bit/dim 6.7558(6.9620) | Xent 2.0303(2.0377) | Loss 7.7709(7.9809) | Error 0.6767(0.6912) Steps 574(569.76) | Grad Norm 2.8459(3.8764) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 42.5318(41.8794) | Bit/dim 6.6335(6.8890) | Xent 1.9993(2.0324) | Loss 7.6331(7.9052) | Error 0.6667(0.6925) Steps 574(570.74) | Grad Norm 2.7613(6.0997) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 42.9613(42.4219) | Bit/dim 6.5082(6.8068) | Xent 1.9949(2.0233) | Loss 7.5057(7.8185) | Error 0.6711(0.6896) Steps 574(571.59) | Grad Norm 6.1046(7.1549) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 45.2534(42.6827) | Bit/dim 6.3778(6.7093) | Xent 2.0330(2.0338) | Loss 7.3943(7.7262) | Error 0.7067(0.6980) Steps 580(572.74) | Grad Norm 26.2599(12.1339) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 42.7299(42.8602) | Bit/dim 6.1863(6.5925) | Xent 1.9849(2.0380) | Loss 7.1787(7.6115) | Error 0.7000(0.7056) Steps 574(574.01) | Grad Norm 15.2976(14.6337) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 43.6802(43.1179) | Bit/dim 6.1250(6.4718) | Xent 2.3098(2.0673) | Loss 7.2799(7.5055) | Error 0.8256(0.7146) Steps 580(575.27) | Grad Norm 81.7631(22.1831) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 97.8074, Epoch Time 2511.1194(2368.1802), Bit/dim 6.0675(best: 6.7827), Xent 2.0870, Loss 7.1110, Error 0.7292(best: 0.6798)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 43.4159(43.2683) | Bit/dim 5.9572(6.3547) | Xent 2.0311(2.0668) | Loss 6.9728(7.3882) | Error 0.6833(0.7195) Steps 580(576.36) | Grad Norm 8.2912(24.4163) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 43.9304(43.4189) | Bit/dim 5.8444(6.2366) | Xent 2.0447(2.0650) | Loss 6.8667(7.2691) | Error 0.7056(0.7204) Steps 580(577.32) | Grad Norm 5.9304(21.7508) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 40.5932(43.4235) | Bit/dim 5.7525(6.1223) | Xent 2.0234(2.0586) | Loss 6.7642(7.1516) | Error 0.6789(0.7159) Steps 574(577.19) | Grad Norm 1.8210(17.9409) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 40.8112(42.7712) | Bit/dim 5.7007(6.0206) | Xent 1.9557(2.0430) | Loss 6.6786(7.0421) | Error 0.6667(0.7065) Steps 574(576.36) | Grad Norm 7.4484(14.6857) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 43.7827(42.3874) | Bit/dim 5.6802(5.9323) | Xent 1.9522(2.0262) | Loss 6.6564(6.9454) | Error 0.6633(0.6956) Steps 580(575.92) | Grad Norm 5.3490(12.2880) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 98.6102, Epoch Time 2459.0217(2370.9055), Bit/dim 5.6503(best: 6.0675), Xent 1.9390, Loss 6.6198, Error 0.6415(best: 0.6798)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 45.9258(42.3632) | Bit/dim 5.6734(5.8622) | Xent 2.0405(2.0128) | Loss 6.6937(6.8686) | Error 0.7100(0.6882) Steps 574(576.65) | Grad Norm 29.5577(12.7546) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 41.0425(42.2266) | Bit/dim 5.5764(5.8016) | Xent 1.9476(2.0043) | Loss 6.5502(6.8037) | Error 0.6844(0.6885) Steps 580(577.53) | Grad Norm 20.5404(15.0878) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 43.9089(42.1315) | Bit/dim 5.5847(5.7476) | Xent 1.9168(1.9960) | Loss 6.5431(6.7455) | Error 0.6533(0.6869) Steps 580(578.18) | Grad Norm 12.7638(16.7028) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 40.6316(41.9927) | Bit/dim 5.5186(5.6972) | Xent 2.0168(1.9803) | Loss 6.5270(6.6873) | Error 0.7111(0.6833) Steps 580(578.66) | Grad Norm 26.4034(16.2102) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 41.4301(41.8711) | Bit/dim 5.4740(5.6539) | Xent 1.9400(1.9933) | Loss 6.4440(6.6505) | Error 0.6700(0.6907) Steps 580(579.01) | Grad Norm 9.5889(18.4916) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 41.3873(41.7784) | Bit/dim 5.4621(5.6057) | Xent 2.0136(2.0023) | Loss 6.4688(6.6069) | Error 0.7067(0.6973) Steps 580(579.27) | Grad Norm 16.7204(16.6328) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 98.3886, Epoch Time 2411.4269(2372.1211), Bit/dim 5.4475(best: 5.6503), Xent 1.9942, Loss 6.4446, Error 0.6890(best: 0.6415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 41.6713(41.7468) | Bit/dim 5.4175(5.5651) | Xent 1.9558(1.9981) | Loss 6.3955(6.5642) | Error 0.6478(0.6945) Steps 580(579.94) | Grad Norm 6.6371(16.0629) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 40.7947(41.6388) | Bit/dim 5.2745(5.5089) | Xent 1.9161(1.9870) | Loss 6.2325(6.5024) | Error 0.6411(0.6888) Steps 574(579.48) | Grad Norm 6.4849(15.2130) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 42.7387(41.6909) | Bit/dim 5.3589(5.4640) | Xent 1.9552(1.9729) | Loss 6.3365(6.4505) | Error 0.6689(0.6813) Steps 580(579.00) | Grad Norm 18.8208(13.2195) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 43.6508(42.1961) | Bit/dim 5.2486(5.4122) | Xent 1.8887(1.9632) | Loss 6.1930(6.3938) | Error 0.6444(0.6786) Steps 586(580.27) | Grad Norm 3.7733(13.1110) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 43.9807(42.7926) | Bit/dim 5.2068(5.3743) | Xent 1.8674(1.9520) | Loss 6.1405(6.3503) | Error 0.6067(0.6759) Steps 586(582.41) | Grad Norm 5.9004(13.8585) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 99.8659, Epoch Time 2465.4000(2374.9195), Bit/dim 5.2125(best: 5.4475), Xent 1.9089, Loss 6.1670, Error 0.6600(best: 0.6415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 44.4957(43.0671) | Bit/dim 5.2260(5.3297) | Xent 1.8945(1.9401) | Loss 6.1733(6.2997) | Error 0.6600(0.6704) Steps 586(583.35) | Grad Norm 13.2457(13.1433) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 45.3969(43.2356) | Bit/dim 5.1262(5.2900) | Xent 1.9009(1.9282) | Loss 6.0767(6.2541) | Error 0.6856(0.6686) Steps 586(584.05) | Grad Norm 8.2050(12.1730) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 44.3392(43.6337) | Bit/dim 5.2389(5.2587) | Xent 1.9042(1.9235) | Loss 6.1910(6.2204) | Error 0.6844(0.6673) Steps 592(585.83) | Grad Norm 11.0901(12.5666) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 43.5674(43.9083) | Bit/dim 5.1972(5.2270) | Xent 2.0649(1.9284) | Loss 6.2297(6.1912) | Error 0.7356(0.6713) Steps 586(587.10) | Grad Norm 42.9648(13.9493) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 44.2044(43.9440) | Bit/dim 5.2227(5.2352) | Xent 2.0712(2.0011) | Loss 6.2583(6.2357) | Error 0.7522(0.6975) Steps 586(588.09) | Grad Norm 16.2590(20.0028) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 45.3491(44.0623) | Bit/dim 5.1825(5.2246) | Xent 2.0243(2.0029) | Loss 6.1946(6.2260) | Error 0.7078(0.7005) Steps 598(589.19) | Grad Norm 10.1751(17.0295) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 108.1084, Epoch Time 2560.0308(2380.4728), Bit/dim 5.1578(best: 5.2125), Xent 1.9796, Loss 6.1477, Error 0.6954(best: 0.6415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 46.2181(44.5479) | Bit/dim 5.0975(5.2000) | Xent 1.9686(1.9952) | Loss 6.0818(6.1976) | Error 0.7033(0.6967) Steps 598(591.33) | Grad Norm 4.3615(13.8071) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 46.0165(44.6485) | Bit/dim 5.0793(5.1663) | Xent 2.0182(1.9868) | Loss 6.0883(6.1598) | Error 0.6989(0.6932) Steps 592(591.80) | Grad Norm 2.9169(11.0529) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 44.7680(44.7590) | Bit/dim 4.9865(5.1330) | Xent 1.9215(1.9796) | Loss 5.9472(6.1229) | Error 0.6689(0.6915) Steps 586(591.33) | Grad Norm 1.9095(8.8032) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 45.2186(44.8398) | Bit/dim 5.0016(5.0997) | Xent 1.9092(1.9651) | Loss 5.9563(6.0822) | Error 0.6644(0.6863) Steps 592(591.20) | Grad Norm 2.9006(7.4603) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 47.1766(44.9813) | Bit/dim 4.9884(5.0692) | Xent 1.9530(1.9508) | Loss 5.9649(6.0446) | Error 0.6767(0.6814) Steps 598(590.85) | Grad Norm 9.8181(7.0125) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 106.5215, Epoch Time 2615.4045(2387.5208), Bit/dim 4.9489(best: 5.1578), Xent 1.8719, Loss 5.8849, Error 0.6470(best: 0.6415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 49.2635(45.4705) | Bit/dim 4.9156(5.0371) | Xent 1.8464(1.9387) | Loss 5.8388(6.0065) | Error 0.6533(0.6779) Steps 610(592.17) | Grad Norm 2.6213(6.0099) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 49.3816(46.4171) | Bit/dim 4.8965(5.0084) | Xent 1.8684(1.9222) | Loss 5.8307(5.9694) | Error 0.6622(0.6703) Steps 616(596.65) | Grad Norm 4.4658(5.3734) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 52.5874(47.5817) | Bit/dim 4.9137(4.9908) | Xent 1.9401(1.9312) | Loss 5.8838(5.9564) | Error 0.6778(0.6748) Steps 610(602.20) | Grad Norm 4.2286(9.2870) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 49.6666(48.2307) | Bit/dim 4.8800(4.9688) | Xent 1.8958(1.9338) | Loss 5.8279(5.9357) | Error 0.6644(0.6774) Steps 622(607.69) | Grad Norm 6.6629(9.0355) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 49.4952(48.5530) | Bit/dim 4.8553(4.9435) | Xent 1.9438(1.9316) | Loss 5.8272(5.9093) | Error 0.6911(0.6780) Steps 622(612.43) | Grad Norm 7.4011(8.3327) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 53.4232(49.0203) | Bit/dim 4.8556(4.9186) | Xent 1.8773(1.9229) | Loss 5.7943(5.8800) | Error 0.6733(0.6760) Steps 640(618.09) | Grad Norm 9.5499(7.9748) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 115.9059, Epoch Time 2868.0502(2401.9367), Bit/dim 4.8299(best: 4.9489), Xent 1.8384, Loss 5.7491, Error 0.6357(best: 0.6415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 54.8355(49.8516) | Bit/dim 4.8345(4.8935) | Xent 1.8320(1.9053) | Loss 5.7505(5.8462) | Error 0.6667(0.6718) Steps 652(623.79) | Grad Norm 8.4325(7.6558) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 53.5179(50.7527) | Bit/dim 4.7932(4.8712) | Xent 1.8585(1.8920) | Loss 5.7225(5.8172) | Error 0.6433(0.6661) Steps 652(631.04) | Grad Norm 13.9092(9.0752) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 51.3994(51.2088) | Bit/dim 4.8328(4.8481) | Xent 1.8070(1.8835) | Loss 5.7363(5.7899) | Error 0.6278(0.6631) Steps 664(636.90) | Grad Norm 11.1571(8.9814) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 51.6235(51.6544) | Bit/dim 4.7486(4.8265) | Xent 1.8557(1.8752) | Loss 5.6765(5.7641) | Error 0.6367(0.6614) Steps 664(642.62) | Grad Norm 3.8688(9.5368) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 52.2039(52.0090) | Bit/dim 4.7491(4.8071) | Xent 1.9162(1.8741) | Loss 5.7073(5.7442) | Error 0.6922(0.6620) Steps 646(646.50) | Grad Norm 16.4797(10.7681) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 114.9759, Epoch Time 3040.0311(2421.0795), Bit/dim 4.7211(best: 4.8299), Xent 1.7828, Loss 5.6125, Error 0.6223(best: 0.6357)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 54.5831(52.4245) | Bit/dim 4.7550(4.7935) | Xent 1.8243(1.8657) | Loss 5.6671(5.7263) | Error 0.6467(0.6602) Steps 652(649.83) | Grad Norm 3.9803(10.8778) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 51.4078(52.4875) | Bit/dim 4.7502(4.7805) | Xent 1.8694(1.8649) | Loss 5.6850(5.7130) | Error 0.6856(0.6595) Steps 658(651.45) | Grad Norm 13.9913(12.0190) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 50.6716(52.6759) | Bit/dim 4.7288(4.7631) | Xent 1.7996(1.8538) | Loss 5.6286(5.6900) | Error 0.6289(0.6559) Steps 664(652.51) | Grad Norm 3.5688(11.1735) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 54.7272(52.8547) | Bit/dim 4.6636(4.7444) | Xent 1.7750(1.8396) | Loss 5.5511(5.6642) | Error 0.6144(0.6504) Steps 658(654.55) | Grad Norm 4.2389(9.7938) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 50.1294(52.9286) | Bit/dim 4.6849(4.7288) | Xent 1.9232(1.8460) | Loss 5.6465(5.6518) | Error 0.7111(0.6552) Steps 658(656.06) | Grad Norm 14.6479(11.9530) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 55.4084(53.2160) | Bit/dim 4.6505(4.7127) | Xent 1.7811(1.8459) | Loss 5.5410(5.6357) | Error 0.6356(0.6555) Steps 652(655.55) | Grad Norm 3.1532(10.7923) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 112.6100, Epoch Time 3061.9782(2440.3064), Bit/dim 4.6629(best: 4.7211), Xent 1.7732, Loss 5.5495, Error 0.6265(best: 0.6223)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 52.2592(53.3628) | Bit/dim 4.6382(4.7016) | Xent 1.8235(1.8295) | Loss 5.5499(5.6163) | Error 0.6400(0.6498) Steps 658(655.46) | Grad Norm 10.7114(9.8507) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 56.1679(53.4979) | Bit/dim 4.6289(4.6844) | Xent 1.8362(1.8239) | Loss 5.5470(5.5964) | Error 0.6322(0.6462) Steps 652(655.80) | Grad Norm 9.5504(10.3737) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 58.3042(53.6772) | Bit/dim 4.6041(4.6659) | Xent 1.7534(1.8134) | Loss 5.4808(5.5726) | Error 0.6211(0.6415) Steps 658(655.12) | Grad Norm 10.5228(10.4820) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 51.0592(53.5360) | Bit/dim 4.6742(4.6647) | Xent 1.8032(1.8131) | Loss 5.5759(5.5713) | Error 0.6400(0.6432) Steps 658(655.10) | Grad Norm 12.6602(11.6843) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 52.0439(53.3932) | Bit/dim 4.6536(4.6552) | Xent 1.8412(1.8131) | Loss 5.5742(5.5617) | Error 0.6644(0.6440) Steps 652(653.79) | Grad Norm 15.3954(11.9202) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 113.5382, Epoch Time 3075.3586(2459.3580), Bit/dim 4.6001(best: 4.6629), Xent 1.7143, Loss 5.4573, Error 0.6002(best: 0.6223)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 53.9003(53.3527) | Bit/dim 4.5788(4.6411) | Xent 1.7369(1.8022) | Loss 5.4472(5.5421) | Error 0.6000(0.6412) Steps 652(652.59) | Grad Norm 11.3217(10.7355) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 53.5410(53.0774) | Bit/dim 4.6081(4.6269) | Xent 1.7690(1.7881) | Loss 5.4926(5.5209) | Error 0.6044(0.6366) Steps 646(651.82) | Grad Norm 22.4499(11.1784) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 53.1580(52.8111) | Bit/dim 4.5867(4.6244) | Xent 1.7643(1.7923) | Loss 5.4689(5.5206) | Error 0.6611(0.6403) Steps 646(649.95) | Grad Norm 6.2719(10.8979) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 52.1400(52.8781) | Bit/dim 4.5282(4.6069) | Xent 1.7607(1.7821) | Loss 5.4085(5.4980) | Error 0.6322(0.6366) Steps 640(648.46) | Grad Norm 10.4064(9.8229) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 55.3956(52.7630) | Bit/dim 4.5278(4.5896) | Xent 1.7611(1.7731) | Loss 5.4083(5.4761) | Error 0.6411(0.6351) Steps 646(646.88) | Grad Norm 12.6476(9.3377) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 50.3320(52.5191) | Bit/dim 4.8493(4.5875) | Xent 1.9693(1.7778) | Loss 5.8340(5.4764) | Error 0.7100(0.6365) Steps 652(645.45) | Grad Norm 27.0838(10.9853) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 114.9885, Epoch Time 3016.5860(2476.0749), Bit/dim 4.6269(best: 4.6001), Xent 1.7394, Loss 5.4965, Error 0.6167(best: 0.6002)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 49.8531(52.0388) | Bit/dim 4.5973(4.5984) | Xent 1.7929(1.7892) | Loss 5.4938(5.4930) | Error 0.6600(0.6412) Steps 634(643.83) | Grad Norm 5.8203(11.3326) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 50.9890(52.0021) | Bit/dim 4.5246(4.5852) | Xent 1.7007(1.7800) | Loss 5.3749(5.4752) | Error 0.6089(0.6383) Steps 634(641.23) | Grad Norm 4.7979(9.9290) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 48.9693(52.0680) | Bit/dim 4.5153(4.5681) | Xent 1.7353(1.7591) | Loss 5.3829(5.4477) | Error 0.5978(0.6319) Steps 634(639.14) | Grad Norm 4.1420(8.8558) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 54.3858(51.8122) | Bit/dim 4.4571(4.5477) | Xent 1.6903(1.7440) | Loss 5.3022(5.4197) | Error 0.6189(0.6264) Steps 634(637.48) | Grad Norm 5.2210(8.1066) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 53.9901(51.9785) | Bit/dim 4.4801(4.5277) | Xent 1.6939(1.7321) | Loss 5.3271(5.3938) | Error 0.6100(0.6230) Steps 640(636.97) | Grad Norm 9.2995(8.5961) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 109.8813, Epoch Time 2972.6100(2490.9709), Bit/dim 4.4598(best: 4.6001), Xent 1.6076, Loss 5.2636, Error 0.5831(best: 0.6002)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 51.5137(52.1121) | Bit/dim 4.4641(4.5132) | Xent 1.5863(1.7097) | Loss 5.2572(5.3680) | Error 0.5867(0.6155) Steps 640(638.72) | Grad Norm 6.7524(8.0268) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 49.8274(51.9224) | Bit/dim 4.5714(4.5292) | Xent 1.8541(1.7371) | Loss 5.4985(5.3977) | Error 0.6556(0.6210) Steps 622(640.40) | Grad Norm 13.0962(10.6855) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 47.9642(51.3271) | Bit/dim 4.5123(4.5293) | Xent 1.7178(1.7498) | Loss 5.3712(5.4042) | Error 0.6344(0.6260) Steps 616(638.17) | Grad Norm 5.3793(10.0288) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 53.0230(50.8908) | Bit/dim 4.4308(4.5121) | Xent 1.6605(1.7450) | Loss 5.2611(5.3846) | Error 0.6122(0.6248) Steps 604(632.01) | Grad Norm 4.5516(8.8285) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 54.0740(50.8833) | Bit/dim 4.4796(4.4943) | Xent 1.8040(1.7352) | Loss 5.3817(5.3619) | Error 0.6356(0.6231) Steps 598(628.91) | Grad Norm 15.5921(8.5314) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 48.9924(50.5674) | Bit/dim 4.4131(4.4781) | Xent 1.7405(1.7272) | Loss 5.2833(5.3417) | Error 0.6444(0.6196) Steps 610(623.84) | Grad Norm 5.6173(8.7061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 100.9163, Epoch Time 2895.4075(2503.1040), Bit/dim 4.4003(best: 4.4598), Xent 1.6052, Loss 5.2029, Error 0.5757(best: 0.5831)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 50.1128(50.3612) | Bit/dim 4.3734(4.4566) | Xent 1.6671(1.7080) | Loss 5.2070(5.3106) | Error 0.5922(0.6126) Steps 628(622.47) | Grad Norm 3.0854(7.8545) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 49.0315(50.2608) | Bit/dim 4.3999(4.4394) | Xent 1.6379(1.6912) | Loss 5.2189(5.2850) | Error 0.6078(0.6086) Steps 640(621.21) | Grad Norm 6.2853(7.8294) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 49.6897(50.1803) | Bit/dim 4.3327(4.4254) | Xent 1.6761(1.6846) | Loss 5.1708(5.2676) | Error 0.6133(0.6078) Steps 634(620.98) | Grad Norm 4.4195(7.9771) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 55.0156(50.4914) | Bit/dim 4.3979(4.4204) | Xent 1.7175(1.6822) | Loss 5.2566(5.2615) | Error 0.6344(0.6057) Steps 622(622.18) | Grad Norm 13.4953(9.2667) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 53.1026(50.7144) | Bit/dim 4.3198(4.4065) | Xent 1.7148(1.6848) | Loss 5.1772(5.2489) | Error 0.6356(0.6063) Steps 628(623.34) | Grad Norm 4.6204(9.1197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 100.5497, Epoch Time 2893.2209(2514.8075), Bit/dim 4.3425(best: 4.4003), Xent 1.5604, Loss 5.1227, Error 0.5685(best: 0.5757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 54.3988(51.1834) | Bit/dim 4.3465(4.3882) | Xent 1.6120(1.6695) | Loss 5.1525(5.2230) | Error 0.5778(0.6014) Steps 658(625.17) | Grad Norm 5.8712(8.0653) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 50.6867(51.7414) | Bit/dim 4.3957(4.3876) | Xent 1.7390(1.6624) | Loss 5.2652(5.2188) | Error 0.6078(0.5995) Steps 634(627.38) | Grad Norm 16.8225(9.4537) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 51.8541(51.4537) | Bit/dim 4.3185(4.3795) | Xent 1.6356(1.6615) | Loss 5.1363(5.2102) | Error 0.6044(0.5992) Steps 604(623.31) | Grad Norm 6.6817(8.9805) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 53.3603(51.6625) | Bit/dim 4.3151(4.3651) | Xent 1.5903(1.6438) | Loss 5.1103(5.1870) | Error 0.5644(0.5932) Steps 640(623.76) | Grad Norm 3.9480(8.1764) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 50.8919(51.6819) | Bit/dim 4.3044(4.3484) | Xent 1.5916(1.6314) | Loss 5.1002(5.1641) | Error 0.5656(0.5893) Steps 592(619.69) | Grad Norm 7.3948(7.3981) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 52.0949(51.8097) | Bit/dim 4.3017(4.3310) | Xent 1.6721(1.6234) | Loss 5.1377(5.1427) | Error 0.5822(0.5855) Steps 616(616.28) | Grad Norm 13.5471(7.2862) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 110.5435, Epoch Time 2999.7911(2529.3570), Bit/dim 4.3688(best: 4.3425), Xent 1.8784, Loss 5.3080, Error 0.6285(best: 0.5685)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 53.0990(52.3622) | Bit/dim 4.6470(4.4498) | Xent 1.8669(1.7594) | Loss 5.5805(5.3295) | Error 0.6767(0.6179) Steps 694(628.83) | Grad Norm 5.0310(12.0053) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 50.2396(51.8301) | Bit/dim 4.4612(4.4797) | Xent 1.8636(1.7924) | Loss 5.3931(5.3759) | Error 0.6578(0.6349) Steps 598(629.52) | Grad Norm 3.0125(10.1337) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 50.6000(51.5365) | Bit/dim 4.3876(4.4659) | Xent 1.7914(1.7940) | Loss 5.2833(5.3629) | Error 0.6533(0.6368) Steps 556(614.08) | Grad Norm 2.5118(8.2048) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 49.1809(51.0814) | Bit/dim 4.3386(4.4354) | Xent 1.7808(1.7831) | Loss 5.2289(5.3270) | Error 0.6533(0.6372) Steps 556(599.34) | Grad Norm 2.9787(6.7386) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 50.3998(51.1382) | Bit/dim 4.3074(4.4021) | Xent 1.6651(1.7563) | Loss 5.1400(5.2802) | Error 0.6144(0.6315) Steps 568(591.17) | Grad Norm 2.0070(5.7020) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 93.9977, Epoch Time 2930.2861(2541.3849), Bit/dim 4.2799(best: 4.3425), Xent 1.5811, Loss 5.0704, Error 0.5689(best: 0.5685)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 54.1996(51.3846) | Bit/dim 4.2566(4.3685) | Xent 1.5786(1.7269) | Loss 5.0459(5.2320) | Error 0.5822(0.6230) Steps 568(586.00) | Grad Norm 1.8111(4.8571) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 53.4028(51.6980) | Bit/dim 4.2401(4.3387) | Xent 1.6650(1.7012) | Loss 5.0726(5.1893) | Error 0.6033(0.6145) Steps 562(582.29) | Grad Norm 6.4895(4.4626) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 53.0993(51.7685) | Bit/dim 4.2219(4.3150) | Xent 1.6284(1.6825) | Loss 5.0361(5.1562) | Error 0.5667(0.6091) Steps 580(578.74) | Grad Norm 5.4597(4.4339) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 53.8328(51.7646) | Bit/dim 4.2250(4.2947) | Xent 1.6354(1.6618) | Loss 5.0427(5.1256) | Error 0.5778(0.6023) Steps 586(577.29) | Grad Norm 5.6565(4.6213) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 53.3123(52.0286) | Bit/dim 4.2160(4.2739) | Xent 1.5842(1.6433) | Loss 5.0081(5.0955) | Error 0.5644(0.5959) Steps 586(575.96) | Grad Norm 4.8795(4.5380) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 50.3516(52.0845) | Bit/dim 4.2132(4.2582) | Xent 1.5751(1.6226) | Loss 5.0007(5.0695) | Error 0.5656(0.5884) Steps 568(576.97) | Grad Norm 7.7196(5.2542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 101.2625, Epoch Time 2994.1519(2554.9679), Bit/dim 4.2139(best: 4.2799), Xent 1.5298, Loss 4.9788, Error 0.5562(best: 0.5685)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 53.8404(52.4892) | Bit/dim 4.2196(4.2449) | Xent 1.6530(1.6108) | Loss 5.0461(5.0502) | Error 0.5978(0.5841) Steps 586(579.04) | Grad Norm 13.5848(6.1606) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 50.3263(52.5626) | Bit/dim 4.1842(4.2358) | Xent 1.5728(1.6000) | Loss 4.9706(5.0358) | Error 0.5667(0.5788) Steps 580(579.27) | Grad Norm 8.0207(6.8463) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 54.1770(52.5970) | Bit/dim 4.2045(4.2256) | Xent 1.5751(1.5932) | Loss 4.9921(5.0222) | Error 0.5800(0.5767) Steps 598(579.85) | Grad Norm 4.8719(6.7574) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 51.1848(52.6569) | Bit/dim 4.1732(4.2127) | Xent 1.6005(1.5865) | Loss 4.9735(5.0060) | Error 0.5811(0.5733) Steps 574(582.32) | Grad Norm 3.9575(6.7163) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 54.9459(52.9571) | Bit/dim 4.2018(4.2024) | Xent 1.5278(1.5779) | Loss 4.9657(4.9913) | Error 0.5600(0.5715) Steps 604(585.76) | Grad Norm 4.3232(6.1558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 106.7398, Epoch Time 3043.7842(2569.6324), Bit/dim 4.1757(best: 4.2139), Xent 1.5371, Loss 4.9442, Error 0.5577(best: 0.5562)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 51.3530(53.0241) | Bit/dim 4.1853(4.1954) | Xent 1.5626(1.5624) | Loss 4.9665(4.9766) | Error 0.5533(0.5648) Steps 586(589.97) | Grad Norm 4.8790(6.2661) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 53.2759(52.8582) | Bit/dim 4.1385(4.1827) | Xent 1.4789(1.5507) | Loss 4.8780(4.9580) | Error 0.5322(0.5601) Steps 580(590.14) | Grad Norm 6.4095(6.8945) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 51.9561(52.7685) | Bit/dim 4.1271(4.1817) | Xent 1.4507(1.5543) | Loss 4.8524(4.9589) | Error 0.5078(0.5601) Steps 598(591.05) | Grad Norm 3.2887(7.8512) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 52.7568(52.9955) | Bit/dim 4.1216(4.1739) | Xent 1.5182(1.5469) | Loss 4.8807(4.9474) | Error 0.5478(0.5582) Steps 586(592.74) | Grad Norm 3.6433(7.3574) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 54.2455(53.1636) | Bit/dim 4.1128(4.1645) | Xent 1.4688(1.5383) | Loss 4.8471(4.9337) | Error 0.5233(0.5553) Steps 592(592.95) | Grad Norm 5.7642(7.0506) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 56.8060(53.4105) | Bit/dim 4.1125(4.1562) | Xent 1.4972(1.5327) | Loss 4.8611(4.9226) | Error 0.5522(0.5536) Steps 610(593.72) | Grad Norm 3.7309(6.5511) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 107.6152, Epoch Time 3056.3736(2584.2346), Bit/dim 4.1210(best: 4.1757), Xent 1.4408, Loss 4.8414, Error 0.5184(best: 0.5562)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 50.9856(52.9794) | Bit/dim 4.1213(4.1469) | Xent 1.4971(1.5186) | Loss 4.8698(4.9061) | Error 0.5533(0.5491) Steps 574(592.56) | Grad Norm 4.5033(6.1370) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 51.6679(52.5300) | Bit/dim 4.1287(4.1385) | Xent 1.5036(1.5095) | Loss 4.8805(4.8932) | Error 0.5411(0.5466) Steps 592(592.34) | Grad Norm 9.6664(6.3306) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 53.2844(52.2101) | Bit/dim 4.1523(4.1332) | Xent 1.5286(1.5034) | Loss 4.9166(4.8849) | Error 0.5400(0.5434) Steps 592(593.09) | Grad Norm 14.2083(6.2341) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 51.7702(52.0929) | Bit/dim 4.1292(4.1316) | Xent 1.4782(1.5162) | Loss 4.8683(4.8897) | Error 0.5411(0.5473) Steps 592(592.50) | Grad Norm 10.8220(7.5245) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 55.0800(52.4676) | Bit/dim 4.1264(4.1276) | Xent 1.4594(1.5229) | Loss 4.8561(4.8890) | Error 0.5389(0.5487) Steps 586(592.88) | Grad Norm 4.9366(7.4163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 104.7616, Epoch Time 2979.8367(2596.1027), Bit/dim 4.1102(best: 4.1210), Xent 1.4114, Loss 4.8159, Error 0.5046(best: 0.5184)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 52.0275(52.4684) | Bit/dim 4.0856(4.1163) | Xent 1.4743(1.5065) | Loss 4.8228(4.8695) | Error 0.5511(0.5416) Steps 586(592.64) | Grad Norm 7.8278(6.9043) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 51.8838(52.2470) | Bit/dim 4.1129(4.1106) | Xent 1.5182(1.4915) | Loss 4.8720(4.8564) | Error 0.5711(0.5375) Steps 592(593.65) | Grad Norm 11.5275(7.0950) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 52.9712(52.3073) | Bit/dim 4.0936(4.1028) | Xent 1.4787(1.4880) | Loss 4.8329(4.8468) | Error 0.5522(0.5369) Steps 628(594.94) | Grad Norm 10.8522(7.3913) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 49.5819(52.1659) | Bit/dim 4.0628(4.0969) | Xent 1.4650(1.4746) | Loss 4.7953(4.8342) | Error 0.5478(0.5321) Steps 592(594.48) | Grad Norm 5.7332(6.7992) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 50.2036(51.8927) | Bit/dim 4.0766(4.0885) | Xent 1.5210(1.4698) | Loss 4.8371(4.8234) | Error 0.5489(0.5309) Steps 592(594.49) | Grad Norm 5.1486(6.5627) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 52.5062(51.7468) | Bit/dim 4.1463(4.0892) | Xent 1.5250(1.4788) | Loss 4.9088(4.8286) | Error 0.5589(0.5340) Steps 610(595.06) | Grad Norm 9.1212(7.7003) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 95.2285, Epoch Time 2961.2442(2607.0569), Bit/dim 4.1512(best: 4.1102), Xent 1.5019, Loss 4.9021, Error 0.5360(best: 0.5046)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 52.0476(51.7532) | Bit/dim 4.1032(4.0972) | Xent 1.4722(1.4804) | Loss 4.8393(4.8373) | Error 0.5222(0.5335) Steps 598(596.06) | Grad Norm 5.7002(7.3890) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 56.1193(52.0712) | Bit/dim 4.0589(4.0910) | Xent 1.4798(1.4634) | Loss 4.7988(4.8227) | Error 0.5311(0.5287) Steps 616(598.68) | Grad Norm 5.4449(6.8432) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 49.5483(51.6715) | Bit/dim 4.0613(4.0818) | Xent 1.3382(1.4480) | Loss 4.7304(4.8058) | Error 0.4989(0.5230) Steps 586(596.38) | Grad Norm 5.2152(6.5860) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 53.2789(51.5930) | Bit/dim 4.0310(4.0690) | Xent 1.4698(1.4438) | Loss 4.7659(4.7909) | Error 0.5244(0.5209) Steps 598(596.58) | Grad Norm 7.3797(6.5937) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 53.9984(51.5762) | Bit/dim 4.0428(4.0627) | Xent 1.4342(1.4376) | Loss 4.7599(4.7815) | Error 0.5244(0.5192) Steps 592(596.36) | Grad Norm 10.9552(6.6795) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 110.9688, Epoch Time 2969.4390(2617.9284), Bit/dim 4.0452(best: 4.1102), Xent 1.3478, Loss 4.7191, Error 0.4889(best: 0.5046)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 50.3410(51.5637) | Bit/dim 4.0376(4.0552) | Xent 1.3484(1.4260) | Loss 4.7118(4.7682) | Error 0.4856(0.5161) Steps 592(596.80) | Grad Norm 5.4489(6.5176) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 49.9040(51.6805) | Bit/dim 4.0366(4.0502) | Xent 1.5015(1.4196) | Loss 4.7874(4.7600) | Error 0.5544(0.5147) Steps 598(598.17) | Grad Norm 11.3503(6.6778) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 51.2019(51.5509) | Bit/dim 4.0209(4.0410) | Xent 1.3958(1.4144) | Loss 4.7188(4.7482) | Error 0.4867(0.5117) Steps 586(597.03) | Grad Norm 7.3369(6.3333) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 51.0333(51.7178) | Bit/dim 3.9977(4.0359) | Xent 1.3615(1.4049) | Loss 4.6785(4.7384) | Error 0.4989(0.5084) Steps 598(598.31) | Grad Norm 3.3192(6.2321) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 51.0850(51.8351) | Bit/dim 3.9950(4.0348) | Xent 1.3811(1.4059) | Loss 4.6855(4.7378) | Error 0.4989(0.5079) Steps 592(597.69) | Grad Norm 6.5091(6.7187) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 55.7355(52.0939) | Bit/dim 4.1258(4.0332) | Xent 1.5100(1.4108) | Loss 4.8808(4.7386) | Error 0.5433(0.5107) Steps 616(597.55) | Grad Norm 18.0964(7.5210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 102.8992, Epoch Time 2981.6832(2628.8410), Bit/dim 4.0694(best: 4.0452), Xent 1.3588, Loss 4.7488, Error 0.4840(best: 0.4889)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 55.1615(52.0784) | Bit/dim 4.0441(4.0344) | Xent 1.4194(1.4051) | Loss 4.7538(4.7370) | Error 0.5044(0.5068) Steps 592(595.04) | Grad Norm 6.1766(7.7086) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 54.3653(52.3138) | Bit/dim 3.9947(4.0304) | Xent 1.3960(1.3996) | Loss 4.6928(4.7301) | Error 0.5044(0.5043) Steps 592(594.90) | Grad Norm 9.4492(7.6728) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 50.0538(51.9748) | Bit/dim 3.9752(4.0252) | Xent 1.4036(1.3979) | Loss 4.6770(4.7242) | Error 0.5278(0.5042) Steps 604(596.21) | Grad Norm 4.6341(7.6726) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 50.6267(51.6941) | Bit/dim 3.9927(4.0163) | Xent 1.3450(1.3878) | Loss 4.6652(4.7103) | Error 0.4700(0.5004) Steps 604(595.88) | Grad Norm 4.6686(6.8042) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 52.2217(51.4211) | Bit/dim 3.9858(4.0107) | Xent 1.3685(1.3801) | Loss 4.6701(4.7007) | Error 0.4767(0.4970) Steps 592(594.12) | Grad Norm 10.9564(6.9319) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 110.7691, Epoch Time 2964.6854(2638.9164), Bit/dim 3.9947(best: 4.0452), Xent 1.2859, Loss 4.6376, Error 0.4656(best: 0.4840)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 52.8572(51.6283) | Bit/dim 4.0052(4.0085) | Xent 1.3158(1.3766) | Loss 4.6631(4.6968) | Error 0.4667(0.4945) Steps 580(593.27) | Grad Norm 6.2571(6.6766) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 51.9315(51.4444) | Bit/dim 4.0194(4.0036) | Xent 1.4141(1.3747) | Loss 4.7264(4.6910) | Error 0.5200(0.4947) Steps 592(593.05) | Grad Norm 7.9499(7.2659) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 49.0583(51.0237) | Bit/dim 4.0006(4.0010) | Xent 1.3605(1.3679) | Loss 4.6808(4.6850) | Error 0.4789(0.4916) Steps 592(592.25) | Grad Norm 7.8679(7.2735) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 50.2506(50.7781) | Bit/dim 3.9853(3.9962) | Xent 1.3107(1.3624) | Loss 4.6406(4.6774) | Error 0.4767(0.4902) Steps 586(591.08) | Grad Norm 5.2273(6.7025) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 48.8274(50.6456) | Bit/dim 3.9401(3.9897) | Xent 1.3573(1.3593) | Loss 4.6187(4.6693) | Error 0.4711(0.4891) Steps 580(589.90) | Grad Norm 3.7484(7.0785) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 48.9705(50.5512) | Bit/dim 3.9746(3.9829) | Xent 1.2955(1.3525) | Loss 4.6223(4.6591) | Error 0.4789(0.4863) Steps 586(589.53) | Grad Norm 3.6000(6.4351) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 108.6155, Epoch Time 2901.2771(2646.7872), Bit/dim 3.9715(best: 3.9947), Xent 1.2788, Loss 4.6109, Error 0.4600(best: 0.4656)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 49.4687(50.5808) | Bit/dim 3.9750(3.9809) | Xent 1.3079(1.3455) | Loss 4.6289(4.6536) | Error 0.4611(0.4848) Steps 586(588.03) | Grad Norm 10.0470(6.9532) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 52.6823(50.6019) | Bit/dim 3.9437(3.9781) | Xent 1.2708(1.3373) | Loss 4.5791(4.6467) | Error 0.4811(0.4825) Steps 586(587.33) | Grad Norm 3.6016(6.6341) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 52.1267(50.4524) | Bit/dim 3.9527(3.9704) | Xent 1.3056(1.3381) | Loss 4.6055(4.6395) | Error 0.4944(0.4841) Steps 586(587.64) | Grad Norm 3.9191(7.0763) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 49.0820(50.4375) | Bit/dim 3.9583(3.9685) | Xent 1.2822(1.3332) | Loss 4.5994(4.6351) | Error 0.4611(0.4830) Steps 580(588.40) | Grad Norm 7.3127(6.7219) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 49.0209(50.4187) | Bit/dim 3.9322(3.9653) | Xent 1.3061(1.3293) | Loss 4.5852(4.6299) | Error 0.4644(0.4799) Steps 580(587.70) | Grad Norm 7.2813(6.6396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 109.0161, Epoch Time 2896.9277(2654.2914), Bit/dim 3.9537(best: 3.9715), Xent 1.2655, Loss 4.5864, Error 0.4560(best: 0.4600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 51.6983(50.4058) | Bit/dim 3.9503(3.9626) | Xent 1.2901(1.3216) | Loss 4.5954(4.6234) | Error 0.4833(0.4780) Steps 580(586.50) | Grad Norm 6.7841(6.6282) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 50.2630(50.2808) | Bit/dim 3.9642(3.9581) | Xent 1.1962(1.3090) | Loss 4.5624(4.6126) | Error 0.4433(0.4743) Steps 586(586.51) | Grad Norm 3.3533(6.0926) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 54.8053(50.4702) | Bit/dim 3.9687(3.9557) | Xent 1.3783(1.3092) | Loss 4.6579(4.6103) | Error 0.4978(0.4732) Steps 586(586.05) | Grad Norm 7.5134(6.5800) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 54.1566(50.5589) | Bit/dim 3.8970(3.9519) | Xent 1.2807(1.3055) | Loss 4.5374(4.6046) | Error 0.4611(0.4714) Steps 586(586.72) | Grad Norm 5.6594(6.4224) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 51.0304(50.5543) | Bit/dim 3.9539(3.9486) | Xent 1.2709(1.2980) | Loss 4.5893(4.5976) | Error 0.4744(0.4685) Steps 598(587.81) | Grad Norm 3.8066(5.9856) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 55.6701(50.7768) | Bit/dim 3.9183(3.9456) | Xent 1.3909(1.3083) | Loss 4.6137(4.5998) | Error 0.5011(0.4724) Steps 586(588.22) | Grad Norm 6.6670(6.7818) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 109.4136, Epoch Time 2915.5249(2662.1284), Bit/dim 3.9381(best: 3.9537), Xent 1.2124, Loss 4.5443, Error 0.4370(best: 0.4560)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 52.5604(51.0162) | Bit/dim 3.9413(3.9459) | Xent 1.3233(1.3008) | Loss 4.6030(4.5963) | Error 0.4844(0.4693) Steps 592(588.86) | Grad Norm 5.6046(6.6221) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 51.5065(51.0032) | Bit/dim 3.9387(3.9430) | Xent 1.2892(1.2864) | Loss 4.5833(4.5862) | Error 0.4611(0.4630) Steps 586(587.40) | Grad Norm 9.0823(6.3423) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 52.1669(50.9067) | Bit/dim 3.9118(3.9380) | Xent 1.2919(1.2859) | Loss 4.5578(4.5809) | Error 0.4511(0.4611) Steps 598(588.97) | Grad Norm 9.2523(6.5094) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 52.5891(51.1461) | Bit/dim 3.9474(3.9347) | Xent 1.2554(1.2913) | Loss 4.5751(4.5803) | Error 0.4489(0.4647) Steps 592(589.34) | Grad Norm 6.9028(7.2473) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 51.9074(51.2678) | Bit/dim 3.8831(3.9312) | Xent 1.2440(1.2860) | Loss 4.5051(4.5742) | Error 0.4478(0.4637) Steps 592(588.48) | Grad Norm 3.7469(6.8203) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 109.6177, Epoch Time 2952.8472(2670.8500), Bit/dim 3.9185(best: 3.9381), Xent 1.2161, Loss 4.5266, Error 0.4403(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 52.1182(51.2612) | Bit/dim 3.9364(3.9288) | Xent 1.1826(1.2771) | Loss 4.5277(4.5674) | Error 0.4033(0.4602) Steps 586(588.17) | Grad Norm 5.0131(6.5088) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 51.2138(51.4284) | Bit/dim 3.9116(3.9251) | Xent 1.2602(1.2607) | Loss 4.5417(4.5554) | Error 0.4411(0.4533) Steps 580(588.26) | Grad Norm 6.2007(6.0245) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 51.6633(51.3849) | Bit/dim 3.9604(3.9245) | Xent 1.2722(1.2671) | Loss 4.5965(4.5581) | Error 0.4611(0.4559) Steps 580(587.62) | Grad Norm 6.8244(6.6952) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 50.4528(51.5222) | Bit/dim 3.9322(3.9202) | Xent 1.2098(1.2717) | Loss 4.5372(4.5560) | Error 0.4478(0.4562) Steps 574(587.34) | Grad Norm 4.2368(6.6687) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 52.1619(51.5247) | Bit/dim 3.9067(3.9183) | Xent 1.2078(1.2672) | Loss 4.5106(4.5519) | Error 0.4278(0.4543) Steps 580(587.26) | Grad Norm 8.5594(6.7152) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 51.7690(51.5542) | Bit/dim 3.8964(3.9156) | Xent 1.2243(1.2654) | Loss 4.5085(4.5483) | Error 0.4389(0.4535) Steps 598(587.14) | Grad Norm 5.9168(6.6899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 108.8037, Epoch Time 2964.3951(2679.6563), Bit/dim 3.9123(best: 3.9185), Xent 1.1812, Loss 4.5029, Error 0.4276(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 51.0863(51.4413) | Bit/dim 3.9175(3.9143) | Xent 1.2852(1.2579) | Loss 4.5601(4.5432) | Error 0.4633(0.4517) Steps 574(586.03) | Grad Norm 3.1301(6.3123) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 51.0749(51.2735) | Bit/dim 3.8907(3.9142) | Xent 1.2300(1.2596) | Loss 4.5057(4.5440) | Error 0.4500(0.4531) Steps 586(584.45) | Grad Norm 6.7807(7.2566) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 52.3392(51.2090) | Bit/dim 3.8967(3.9125) | Xent 1.2987(1.2560) | Loss 4.5461(4.5406) | Error 0.4411(0.4512) Steps 586(584.38) | Grad Norm 6.6885(6.8090) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 51.0720(51.0547) | Bit/dim 3.9248(3.9116) | Xent 1.2346(1.2532) | Loss 4.5421(4.5382) | Error 0.4533(0.4520) Steps 580(584.30) | Grad Norm 6.5104(6.7295) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 51.1466(51.0220) | Bit/dim 3.8917(3.9041) | Xent 1.1359(1.2385) | Loss 4.4596(4.5233) | Error 0.3967(0.4462) Steps 592(584.49) | Grad Norm 7.5279(6.5269) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 107.6634, Epoch Time 2927.2674(2687.0847), Bit/dim 3.8984(best: 3.9123), Xent 1.1547, Loss 4.4758, Error 0.4149(best: 0.4276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 49.0947(50.9498) | Bit/dim 3.9103(3.9027) | Xent 1.1361(1.2223) | Loss 4.4783(4.5139) | Error 0.4089(0.4400) Steps 580(584.12) | Grad Norm 2.6764(6.4970) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 49.4899(50.9883) | Bit/dim 3.8921(3.9017) | Xent 1.1759(1.2180) | Loss 4.4801(4.5107) | Error 0.4311(0.4388) Steps 586(583.98) | Grad Norm 7.1294(6.6020) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 48.8362(50.9016) | Bit/dim 3.8741(3.8982) | Xent 1.2578(1.2181) | Loss 4.5030(4.5073) | Error 0.4689(0.4377) Steps 580(582.79) | Grad Norm 4.8366(6.4477) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 50.8793(50.7698) | Bit/dim 3.9154(3.8970) | Xent 1.1426(1.2135) | Loss 4.4867(4.5037) | Error 0.3856(0.4350) Steps 580(582.01) | Grad Norm 3.8725(6.3001) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 51.6076(51.0018) | Bit/dim 3.8804(3.8907) | Xent 1.2790(1.2098) | Loss 4.5199(4.4955) | Error 0.4533(0.4331) Steps 586(582.43) | Grad Norm 6.8752(6.2438) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 51.9321(51.2285) | Bit/dim 3.8547(3.8872) | Xent 1.1383(1.2059) | Loss 4.4239(4.4901) | Error 0.4200(0.4318) Steps 574(582.19) | Grad Norm 2.9924(5.8737) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 108.2642, Epoch Time 2933.6803(2694.4825), Bit/dim 3.8789(best: 3.8984), Xent 1.1018, Loss 4.4298, Error 0.3951(best: 0.4149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 49.9963(51.0649) | Bit/dim 3.8897(3.8852) | Xent 1.1407(1.1922) | Loss 4.4600(4.4813) | Error 0.3822(0.4266) Steps 586(581.51) | Grad Norm 4.7669(5.8661) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 51.2897(51.0529) | Bit/dim 3.8758(3.8854) | Xent 1.2264(1.2021) | Loss 4.4890(4.4865) | Error 0.4367(0.4294) Steps 586(582.08) | Grad Norm 6.4927(6.4814) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 49.4111(51.0489) | Bit/dim 3.8972(3.8840) | Xent 1.1958(1.1990) | Loss 4.4951(4.4835) | Error 0.4400(0.4287) Steps 586(582.22) | Grad Norm 5.4108(6.4311) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 51.0100(50.9906) | Bit/dim 3.8719(3.8789) | Xent 1.1936(1.1886) | Loss 4.4687(4.4732) | Error 0.4333(0.4240) Steps 580(581.63) | Grad Norm 5.6787(6.0750) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 49.9738(51.2081) | Bit/dim 3.8355(3.8740) | Xent 1.1242(1.1800) | Loss 4.3976(4.4640) | Error 0.4211(0.4216) Steps 574(580.88) | Grad Norm 2.9240(5.8016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 106.4019, Epoch Time 2929.5392(2701.5342), Bit/dim 3.8642(best: 3.8789), Xent 1.1072, Loss 4.4178, Error 0.3932(best: 0.3951)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 51.4362(50.9350) | Bit/dim 3.8565(3.8729) | Xent 1.1143(1.1686) | Loss 4.4136(4.4572) | Error 0.4100(0.4192) Steps 574(580.45) | Grad Norm 3.2533(5.5501) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 51.1328(50.8222) | Bit/dim 3.8704(3.8711) | Xent 1.1617(1.1652) | Loss 4.4513(4.4537) | Error 0.4344(0.4169) Steps 574(579.69) | Grad Norm 4.7076(5.7582) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 49.3381(51.0805) | Bit/dim 3.8418(3.8675) | Xent 1.1859(1.1659) | Loss 4.4347(4.4505) | Error 0.4389(0.4176) Steps 586(580.91) | Grad Norm 3.4425(5.9656) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 49.0467(51.1169) | Bit/dim 3.8817(3.8657) | Xent 1.1197(1.1598) | Loss 4.4416(4.4456) | Error 0.3844(0.4149) Steps 574(581.11) | Grad Norm 3.8626(5.7559) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 50.2083(50.9217) | Bit/dim 3.8449(3.8646) | Xent 1.1210(1.1560) | Loss 4.4054(4.4426) | Error 0.3944(0.4139) Steps 574(580.36) | Grad Norm 5.6051(5.6042) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 48.7072(50.6405) | Bit/dim 3.8655(3.8645) | Xent 1.1789(1.1663) | Loss 4.4550(4.4477) | Error 0.4211(0.4187) Steps 580(579.96) | Grad Norm 9.1723(6.1916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 108.0715, Epoch Time 2914.9679(2707.9372), Bit/dim 3.8573(best: 3.8642), Xent 1.0935, Loss 4.4041, Error 0.3911(best: 0.3932)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 52.3182(50.6760) | Bit/dim 3.8494(3.8640) | Xent 1.1091(1.1634) | Loss 4.4040(4.4457) | Error 0.4056(0.4171) Steps 580(580.39) | Grad Norm 5.4208(6.0598) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 49.5106(50.8573) | Bit/dim 3.8445(3.8615) | Xent 1.1512(1.1558) | Loss 4.4201(4.4394) | Error 0.4067(0.4135) Steps 580(580.77) | Grad Norm 4.6681(5.6973) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 53.0741(50.9519) | Bit/dim 3.8276(3.8576) | Xent 1.1578(1.1446) | Loss 4.4065(4.4299) | Error 0.4078(0.4097) Steps 580(580.40) | Grad Norm 6.3362(5.7715) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 53.0851(51.1452) | Bit/dim 3.8565(3.8558) | Xent 1.2249(1.1389) | Loss 4.4689(4.4253) | Error 0.4389(0.4076) Steps 580(580.09) | Grad Norm 7.1639(6.1988) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 51.5264(51.3616) | Bit/dim 3.8613(3.8554) | Xent 1.1622(1.1410) | Loss 4.4424(4.4260) | Error 0.4144(0.4081) Steps 580(580.03) | Grad Norm 6.4330(6.1264) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 106.0956, Epoch Time 2951.6032(2715.2472), Bit/dim 3.8503(best: 3.8573), Xent 1.0709, Loss 4.3857, Error 0.3822(best: 0.3911)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 52.9395(51.2228) | Bit/dim 3.8571(3.8535) | Xent 1.1202(1.1421) | Loss 4.4172(4.4245) | Error 0.4133(0.4090) Steps 574(579.52) | Grad Norm 6.4136(6.2466) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 53.2273(51.4627) | Bit/dim 3.8542(3.8515) | Xent 1.0600(1.1342) | Loss 4.3842(4.4186) | Error 0.3867(0.4060) Steps 568(578.50) | Grad Norm 4.7410(6.1692) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 51.8042(51.7967) | Bit/dim 3.8384(3.8496) | Xent 1.0822(1.1224) | Loss 4.3795(4.4108) | Error 0.3767(0.4009) Steps 574(578.08) | Grad Norm 3.0021(5.8858) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 48.5333(51.5962) | Bit/dim 3.8498(3.8465) | Xent 1.0925(1.1136) | Loss 4.3961(4.4032) | Error 0.3844(0.3963) Steps 580(577.72) | Grad Norm 4.1628(5.4330) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 51.9482(51.7133) | Bit/dim 3.8158(3.8443) | Xent 1.1297(1.1064) | Loss 4.3806(4.3975) | Error 0.4133(0.3947) Steps 580(577.85) | Grad Norm 4.3473(5.3818) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 53.1848(51.9006) | Bit/dim 3.8314(3.8430) | Xent 1.1352(1.1081) | Loss 4.3990(4.3971) | Error 0.4100(0.3958) Steps 580(576.82) | Grad Norm 3.6697(5.3674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 108.3659, Epoch Time 2983.3509(2723.2903), Bit/dim 3.8321(best: 3.8503), Xent 1.0301, Loss 4.3472, Error 0.3708(best: 0.3822)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 48.6145(51.9860) | Bit/dim 3.8446(3.8411) | Xent 1.1627(1.1089) | Loss 4.4259(4.3955) | Error 0.4089(0.3957) Steps 574(576.66) | Grad Norm 6.8146(5.5826) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 51.1317(51.7454) | Bit/dim 3.8048(3.8407) | Xent 1.1100(1.1021) | Loss 4.3597(4.3917) | Error 0.3967(0.3936) Steps 580(577.41) | Grad Norm 6.2975(5.5465) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 49.2582(51.4487) | Bit/dim 3.8189(3.8376) | Xent 1.1532(1.0961) | Loss 4.3955(4.3856) | Error 0.4044(0.3924) Steps 574(576.92) | Grad Norm 7.3902(5.6268) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 53.1246(51.8684) | Bit/dim 3.8435(3.8343) | Xent 1.0412(1.0873) | Loss 4.3641(4.3780) | Error 0.3667(0.3894) Steps 574(577.42) | Grad Norm 3.2730(5.1520) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 54.6862(52.2745) | Bit/dim 3.8026(3.8344) | Xent 1.0983(1.0874) | Loss 4.3517(4.3781) | Error 0.3878(0.3894) Steps 574(577.73) | Grad Norm 4.6146(5.2300) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 107.0288, Epoch Time 2990.2138(2731.2980), Bit/dim 3.8226(best: 3.8321), Xent 1.0150, Loss 4.3301, Error 0.3612(best: 0.3708)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 53.0675(52.1526) | Bit/dim 3.8520(3.8304) | Xent 1.0742(1.0828) | Loss 4.3890(4.3718) | Error 0.3900(0.3872) Steps 574(577.55) | Grad Norm 7.6260(5.3036) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 55.4711(52.1351) | Bit/dim 3.8162(3.8272) | Xent 1.0094(1.0878) | Loss 4.3209(4.3711) | Error 0.3567(0.3890) Steps 586(577.59) | Grad Norm 4.6321(5.6988) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 55.1375(52.3280) | Bit/dim 3.8172(3.8279) | Xent 1.1376(1.0895) | Loss 4.3860(4.3727) | Error 0.4122(0.3897) Steps 586(577.33) | Grad Norm 8.4207(5.8945) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 49.0148(52.4251) | Bit/dim 3.8327(3.8281) | Xent 1.0503(1.0854) | Loss 4.3578(4.3708) | Error 0.3756(0.3879) Steps 586(578.14) | Grad Norm 3.7052(5.9207) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 51.1539(52.3740) | Bit/dim 3.8238(3.8255) | Xent 1.1309(1.0852) | Loss 4.3892(4.3681) | Error 0.3933(0.3884) Steps 574(576.44) | Grad Norm 5.1651(5.6506) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 52.5176(52.2838) | Bit/dim 3.8240(3.8265) | Xent 1.1107(1.0829) | Loss 4.3794(4.3679) | Error 0.3967(0.3868) Steps 586(576.35) | Grad Norm 6.5399(5.5932) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 111.2834, Epoch Time 3004.6606(2739.4989), Bit/dim 3.8293(best: 3.8226), Xent 1.0994, Loss 4.3790, Error 0.3891(best: 0.3612)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 53.0415(52.2894) | Bit/dim 3.8245(3.8290) | Xent 1.0871(1.0889) | Loss 4.3680(4.3735) | Error 0.3767(0.3873) Steps 580(577.33) | Grad Norm 8.3776(6.2421) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 51.1355(52.5003) | Bit/dim 3.7980(3.8296) | Xent 1.0532(1.0772) | Loss 4.3246(4.3682) | Error 0.3711(0.3823) Steps 574(577.47) | Grad Norm 4.9043(6.0040) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 53.1642(52.8727) | Bit/dim 3.8211(3.8280) | Xent 1.0450(1.0692) | Loss 4.3436(4.3626) | Error 0.3656(0.3797) Steps 580(579.21) | Grad Norm 3.4501(5.5235) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 51.0514(52.5891) | Bit/dim 3.8071(3.8242) | Xent 1.0237(1.0653) | Loss 4.3190(4.3568) | Error 0.3511(0.3784) Steps 580(578.37) | Grad Norm 8.0202(5.3828) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 56.6338(52.8439) | Bit/dim 3.8003(3.8207) | Xent 1.0856(1.0668) | Loss 4.3431(4.3541) | Error 0.3856(0.3788) Steps 574(578.42) | Grad Norm 7.8754(5.2957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 109.7308, Epoch Time 3037.0741(2748.4262), Bit/dim 3.8145(best: 3.8226), Xent 1.0255, Loss 4.3273, Error 0.3622(best: 0.3612)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 53.3548(53.0471) | Bit/dim 3.7470(3.8149) | Xent 1.1260(1.0620) | Loss 4.3100(4.3459) | Error 0.4178(0.3783) Steps 592(580.39) | Grad Norm 5.4196(5.5797) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 55.4162(53.2426) | Bit/dim 3.8299(3.8123) | Xent 1.0214(1.0590) | Loss 4.3407(4.3418) | Error 0.3689(0.3768) Steps 592(580.63) | Grad Norm 6.1864(5.5568) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 51.7020(53.2652) | Bit/dim 3.7923(3.8116) | Xent 1.0131(1.0516) | Loss 4.2989(4.3374) | Error 0.3700(0.3743) Steps 574(579.30) | Grad Norm 5.5307(5.5326) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 56.7110(53.4670) | Bit/dim 3.8412(3.8134) | Xent 1.0622(1.0532) | Loss 4.3723(4.3400) | Error 0.3911(0.3751) Steps 574(579.43) | Grad Norm 5.5433(5.7609) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 52.1032(53.5799) | Bit/dim 3.8268(3.8140) | Xent 1.0789(1.0612) | Loss 4.3662(4.3446) | Error 0.3722(0.3790) Steps 586(580.29) | Grad Norm 6.2289(5.9668) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 54.4365(53.6312) | Bit/dim 3.8361(3.8122) | Xent 1.0038(1.0578) | Loss 4.3380(4.3411) | Error 0.3611(0.3793) Steps 586(580.17) | Grad Norm 4.9651(5.6194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 108.8356, Epoch Time 3088.5642(2758.6303), Bit/dim 3.8073(best: 3.8145), Xent 0.9847, Loss 4.2997, Error 0.3517(best: 0.3612)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 54.6154(53.5563) | Bit/dim 3.7814(3.8081) | Xent 1.0245(1.0421) | Loss 4.2936(4.3291) | Error 0.3544(0.3732) Steps 586(581.54) | Grad Norm 3.5763(5.1096) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 55.7017(54.1583) | Bit/dim 3.7983(3.8047) | Xent 1.0950(1.0381) | Loss 4.3458(4.3238) | Error 0.3789(0.3720) Steps 586(583.71) | Grad Norm 5.5638(4.9989) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 57.0091(54.4971) | Bit/dim 3.8161(3.8090) | Xent 1.0391(1.0394) | Loss 4.3357(4.3287) | Error 0.3644(0.3723) Steps 586(586.71) | Grad Norm 4.4836(5.3253) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 56.2353(54.9901) | Bit/dim 3.7806(3.8076) | Xent 0.9764(1.0309) | Loss 4.2688(4.3231) | Error 0.3522(0.3694) Steps 586(586.83) | Grad Norm 4.3524(5.1063) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 54.4348(55.1106) | Bit/dim 3.7778(3.8028) | Xent 1.0302(1.0336) | Loss 4.2929(4.3196) | Error 0.3578(0.3696) Steps 580(586.37) | Grad Norm 6.4177(5.0089) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 108.1623, Epoch Time 3174.4589(2771.1052), Bit/dim 3.7976(best: 3.8073), Xent 0.9730, Loss 4.2841, Error 0.3424(best: 0.3517)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 53.0468(55.2196) | Bit/dim 3.7903(3.8017) | Xent 1.0639(1.0308) | Loss 4.3222(4.3171) | Error 0.3956(0.3695) Steps 574(585.86) | Grad Norm 6.6092(4.8072) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 54.7881(55.2136) | Bit/dim 3.7838(3.7995) | Xent 1.0365(1.0293) | Loss 4.3021(4.3142) | Error 0.3633(0.3688) Steps 592(586.25) | Grad Norm 6.6418(5.1433) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 57.1982(55.4495) | Bit/dim 3.8396(3.8007) | Xent 1.1014(1.0420) | Loss 4.3903(4.3217) | Error 0.3833(0.3713) Steps 586(586.66) | Grad Norm 5.2416(5.7292) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 56.5299(55.3925) | Bit/dim 3.8377(3.8012) | Xent 0.9942(1.0426) | Loss 4.3348(4.3225) | Error 0.3589(0.3728) Steps 586(587.35) | Grad Norm 6.5393(5.9011) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 58.6984(55.3029) | Bit/dim 3.8113(3.8015) | Xent 0.9675(1.0386) | Loss 4.2951(4.3208) | Error 0.3644(0.3707) Steps 598(587.27) | Grad Norm 3.7531(5.7326) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 53.9842(55.2863) | Bit/dim 3.7987(3.7980) | Xent 0.9983(1.0322) | Loss 4.2978(4.3141) | Error 0.3544(0.3688) Steps 586(587.50) | Grad Norm 8.0401(5.5263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 110.6745, Epoch Time 3167.5725(2782.9992), Bit/dim 3.7954(best: 3.7976), Xent 1.0261, Loss 4.3085, Error 0.3694(best: 0.3424)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 57.6534(55.5261) | Bit/dim 3.7943(3.7961) | Xent 1.0275(1.0289) | Loss 4.3080(4.3106) | Error 0.3567(0.3685) Steps 610(589.97) | Grad Norm 4.7363(5.6617) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 55.4345(55.6290) | Bit/dim 3.8098(3.7939) | Xent 0.9713(1.0173) | Loss 4.2954(4.3025) | Error 0.3511(0.3651) Steps 580(589.89) | Grad Norm 4.9563(5.5781) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 55.3165(55.9723) | Bit/dim 3.7861(3.7925) | Xent 1.0732(1.0182) | Loss 4.3227(4.3016) | Error 0.3800(0.3645) Steps 610(592.27) | Grad Norm 7.3631(5.5821) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 53.9831(55.8315) | Bit/dim 3.8248(3.7917) | Xent 0.9957(1.0178) | Loss 4.3226(4.3007) | Error 0.3733(0.3648) Steps 592(592.69) | Grad Norm 6.4504(5.3808) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 55.5089(55.7783) | Bit/dim 3.7897(3.7888) | Xent 1.0330(1.0130) | Loss 4.3062(4.2953) | Error 0.3689(0.3642) Steps 592(592.23) | Grad Norm 6.6367(5.1610) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 110.3740, Epoch Time 3212.1367(2795.8733), Bit/dim 3.7884(best: 3.7954), Xent 0.9622, Loss 4.2695, Error 0.3383(best: 0.3424)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 57.1808(55.8861) | Bit/dim 3.8021(3.7870) | Xent 1.0208(1.0116) | Loss 4.3125(4.2929) | Error 0.3600(0.3631) Steps 598(591.84) | Grad Norm 7.6224(5.2286) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 54.9768(56.0386) | Bit/dim 3.7702(3.7808) | Xent 1.0886(1.0028) | Loss 4.3145(4.2822) | Error 0.3989(0.3596) Steps 598(592.96) | Grad Norm 6.6841(5.2380) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 54.8661(56.1012) | Bit/dim 3.7856(3.7818) | Xent 0.9508(0.9914) | Loss 4.2610(4.2775) | Error 0.3333(0.3555) Steps 592(593.68) | Grad Norm 4.5331(5.0769) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 57.7299(56.2676) | Bit/dim 3.7454(3.7815) | Xent 1.0591(1.0039) | Loss 4.2749(4.2835) | Error 0.3700(0.3602) Steps 604(593.58) | Grad Norm 5.1075(5.4046) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 54.5538(56.0057) | Bit/dim 3.7650(3.7833) | Xent 0.9943(1.0003) | Loss 4.2621(4.2834) | Error 0.3522(0.3595) Steps 592(593.50) | Grad Norm 6.9156(5.1264) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 56.1985(55.9632) | Bit/dim 3.7606(3.7823) | Xent 1.0530(1.0068) | Loss 4.2871(4.2857) | Error 0.3856(0.3618) Steps 574(591.30) | Grad Norm 2.8053(5.3573) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 107.5314, Epoch Time 3210.4724(2808.3113), Bit/dim 3.7874(best: 3.7884), Xent 0.9812, Loss 4.2780, Error 0.3470(best: 0.3383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 53.6643(56.0373) | Bit/dim 3.7787(3.7807) | Xent 0.9777(1.0011) | Loss 4.2676(4.2812) | Error 0.3478(0.3599) Steps 598(591.59) | Grad Norm 5.3011(5.5243) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 56.2536(55.9901) | Bit/dim 3.7728(3.7794) | Xent 0.9895(0.9988) | Loss 4.2676(4.2788) | Error 0.3611(0.3589) Steps 586(590.61) | Grad Norm 4.9792(5.5004) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 58.3867(56.0067) | Bit/dim 3.7942(3.7791) | Xent 0.9107(0.9948) | Loss 4.2495(4.2765) | Error 0.3144(0.3569) Steps 598(590.39) | Grad Norm 2.9682(5.2427) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 54.9436(56.0301) | Bit/dim 3.8293(3.7804) | Xent 0.9837(0.9960) | Loss 4.3211(4.2784) | Error 0.3500(0.3564) Steps 598(591.37) | Grad Norm 6.2320(5.5406) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 55.9156(56.1720) | Bit/dim 3.7809(3.7802) | Xent 1.0008(1.0008) | Loss 4.2813(4.2806) | Error 0.3589(0.3569) Steps 604(593.31) | Grad Norm 3.2360(5.4774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 110.2101, Epoch Time 3220.5150(2820.6774), Bit/dim 3.7832(best: 3.7874), Xent 0.9487, Loss 4.2575, Error 0.3383(best: 0.3383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 57.3759(56.3476) | Bit/dim 3.7577(3.7760) | Xent 0.9563(0.9950) | Loss 4.2359(4.2735) | Error 0.3689(0.3581) Steps 592(592.76) | Grad Norm 3.2375(5.1880) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 56.7669(56.0349) | Bit/dim 3.7635(3.7737) | Xent 1.0062(0.9824) | Loss 4.2666(4.2649) | Error 0.3556(0.3534) Steps 580(592.26) | Grad Norm 6.1242(5.0786) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 60.7903(56.3498) | Bit/dim 3.7885(3.7737) | Xent 1.0231(0.9817) | Loss 4.3001(4.2646) | Error 0.3656(0.3522) Steps 586(591.64) | Grad Norm 5.3295(5.1741) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 53.9249(56.1632) | Bit/dim 3.7844(3.7718) | Xent 0.9949(0.9766) | Loss 4.2818(4.2601) | Error 0.3644(0.3498) Steps 586(590.19) | Grad Norm 3.7135(4.8041) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 54.8051(56.0740) | Bit/dim 3.7813(3.7702) | Xent 1.0166(0.9765) | Loss 4.2896(4.2584) | Error 0.3656(0.3503) Steps 574(589.33) | Grad Norm 4.9765(4.6858) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 54.4682(55.9835) | Bit/dim 3.7675(3.7705) | Xent 0.8934(0.9779) | Loss 4.2142(4.2594) | Error 0.3300(0.3514) Steps 586(588.79) | Grad Norm 3.6406(5.2118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 110.9964, Epoch Time 3208.1574(2832.3018), Bit/dim 3.7756(best: 3.7832), Xent 0.9647, Loss 4.2580, Error 0.3430(best: 0.3383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 55.4525(55.9637) | Bit/dim 3.7514(3.7675) | Xent 0.9732(0.9767) | Loss 4.2380(4.2558) | Error 0.3233(0.3508) Steps 586(589.35) | Grad Norm 5.9333(5.4754) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 54.3270(55.8746) | Bit/dim 3.7775(3.7648) | Xent 0.9838(0.9732) | Loss 4.2694(4.2514) | Error 0.3489(0.3491) Steps 586(588.35) | Grad Norm 4.8295(5.3036) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 57.2112(55.8906) | Bit/dim 3.7612(3.7649) | Xent 0.9702(0.9756) | Loss 4.2463(4.2527) | Error 0.3578(0.3507) Steps 598(588.90) | Grad Norm 6.6893(5.4335) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 54.1595(55.6930) | Bit/dim 3.7688(3.7659) | Xent 0.9357(0.9661) | Loss 4.2366(4.2489) | Error 0.3378(0.3456) Steps 592(590.10) | Grad Norm 3.9943(4.9351) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 53.9520(55.6983) | Bit/dim 3.7449(3.7651) | Xent 0.9733(0.9630) | Loss 4.2315(4.2466) | Error 0.3444(0.3447) Steps 586(590.70) | Grad Norm 6.5336(4.7186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 110.6219, Epoch Time 3184.1191(2842.8563), Bit/dim 3.7579(best: 3.7756), Xent 0.9482, Loss 4.2320, Error 0.3365(best: 0.3383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 56.7241(55.7572) | Bit/dim 3.7545(3.7649) | Xent 0.9429(0.9548) | Loss 4.2259(4.2423) | Error 0.3256(0.3417) Steps 580(591.17) | Grad Norm 5.2962(4.7454) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 54.8481(55.6705) | Bit/dim 3.6929(3.7609) | Xent 0.9186(0.9543) | Loss 4.1522(4.2381) | Error 0.2978(0.3408) Steps 592(590.56) | Grad Norm 5.3001(5.0695) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 54.5141(55.6503) | Bit/dim 3.7724(3.7639) | Xent 0.9856(0.9574) | Loss 4.2653(4.2426) | Error 0.3544(0.3419) Steps 586(590.30) | Grad Norm 8.6013(5.6178) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 57.2227(55.8624) | Bit/dim 3.7557(3.7648) | Xent 0.9288(0.9573) | Loss 4.2202(4.2435) | Error 0.3522(0.3429) Steps 592(589.37) | Grad Norm 6.1946(5.9646) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 53.9050(55.8281) | Bit/dim 3.7735(3.7635) | Xent 1.0408(0.9617) | Loss 4.2939(4.2443) | Error 0.3822(0.3434) Steps 592(590.48) | Grad Norm 8.7073(6.1143) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 56.2826(56.0092) | Bit/dim 3.7727(3.7622) | Xent 0.9198(0.9610) | Loss 4.2326(4.2427) | Error 0.3089(0.3425) Steps 592(590.50) | Grad Norm 6.0564(5.7095) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 108.6419, Epoch Time 3210.1387(2853.8748), Bit/dim 3.7685(best: 3.7579), Xent 0.9568, Loss 4.2469, Error 0.3390(best: 0.3365)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 54.5790(55.8972) | Bit/dim 3.7831(3.7614) | Xent 0.9349(0.9609) | Loss 4.2506(4.2418) | Error 0.3278(0.3438) Steps 586(588.72) | Grad Norm 4.2089(5.9280) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 56.4473(55.7021) | Bit/dim 3.7606(3.7591) | Xent 0.8766(0.9587) | Loss 4.1990(4.2384) | Error 0.3100(0.3419) Steps 586(587.92) | Grad Norm 6.5196(6.0129) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 56.5151(55.6852) | Bit/dim 3.7458(3.7598) | Xent 0.9917(0.9547) | Loss 4.2417(4.2371) | Error 0.3589(0.3409) Steps 586(588.80) | Grad Norm 6.2632(6.1044) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 54.2608(55.5068) | Bit/dim 3.7222(3.7576) | Xent 0.9706(0.9518) | Loss 4.2074(4.2335) | Error 0.3444(0.3404) Steps 586(588.67) | Grad Norm 3.7859(5.7011) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 53.9430(55.5939) | Bit/dim 3.7886(3.7544) | Xent 0.9758(0.9445) | Loss 4.2765(4.2267) | Error 0.3611(0.3392) Steps 598(589.74) | Grad Norm 4.7123(5.2886) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 110.0006, Epoch Time 3182.0038(2863.7187), Bit/dim 3.7469(best: 3.7579), Xent 0.8950, Loss 4.1944, Error 0.3141(best: 0.3365)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 54.3919(55.5578) | Bit/dim 3.7558(3.7526) | Xent 0.9593(0.9395) | Loss 4.2355(4.2223) | Error 0.3311(0.3365) Steps 574(588.54) | Grad Norm 6.7059(5.0436) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 53.1506(55.5086) | Bit/dim 3.7661(3.7483) | Xent 0.9490(0.9299) | Loss 4.2407(4.2133) | Error 0.3300(0.3331) Steps 586(588.59) | Grad Norm 3.2501(5.0947) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 56.6146(55.6968) | Bit/dim 3.7229(3.7492) | Xent 0.9314(0.9330) | Loss 4.1886(4.2157) | Error 0.3378(0.3337) Steps 592(589.33) | Grad Norm 3.0978(5.0378) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 57.2803(55.4557) | Bit/dim 3.7496(3.7493) | Xent 0.8994(0.9334) | Loss 4.1993(4.2160) | Error 0.3289(0.3336) Steps 580(587.78) | Grad Norm 3.9011(5.0791) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 54.9594(55.6142) | Bit/dim 3.7593(3.7515) | Xent 0.8603(0.9318) | Loss 4.1894(4.2174) | Error 0.2867(0.3331) Steps 592(588.69) | Grad Norm 5.1107(5.1604) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 53.7405(55.2231) | Bit/dim 3.7412(3.7476) | Xent 0.9149(0.9351) | Loss 4.1986(4.2151) | Error 0.3400(0.3358) Steps 592(588.21) | Grad Norm 3.6542(5.4255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 108.0991, Epoch Time 3165.8592(2872.7829), Bit/dim 3.7470(best: 3.7469), Xent 0.9187, Loss 4.2064, Error 0.3254(best: 0.3141)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 53.3903(55.1119) | Bit/dim 3.7640(3.7455) | Xent 0.8978(0.9243) | Loss 4.2129(4.2076) | Error 0.3333(0.3325) Steps 598(587.73) | Grad Norm 4.1888(5.3178) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 52.8875(55.2837) | Bit/dim 3.7226(3.7437) | Xent 0.8624(0.9143) | Loss 4.1538(4.2009) | Error 0.3133(0.3280) Steps 586(588.09) | Grad Norm 4.8304(5.1689) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 55.5684(55.3388) | Bit/dim 3.7453(3.7455) | Xent 0.9658(0.9225) | Loss 4.2282(4.2067) | Error 0.3422(0.3313) Steps 592(587.97) | Grad Norm 6.1128(5.2215) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 57.2067(55.4449) | Bit/dim 3.7294(3.7447) | Xent 0.9348(0.9213) | Loss 4.1968(4.2053) | Error 0.3244(0.3301) Steps 586(586.11) | Grad Norm 6.6861(5.1388) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 55.5890(55.5302) | Bit/dim 3.7678(3.7482) | Xent 0.9149(0.9278) | Loss 4.2252(4.2121) | Error 0.3256(0.3323) Steps 592(586.70) | Grad Norm 6.0926(5.2303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 108.5046, Epoch Time 3178.5907(2881.9571), Bit/dim 3.7433(best: 3.7469), Xent 0.9074, Loss 4.1970, Error 0.3234(best: 0.3141)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 56.7875(55.5661) | Bit/dim 3.7517(3.7474) | Xent 0.9278(0.9373) | Loss 4.2155(4.2161) | Error 0.3256(0.3344) Steps 592(586.99) | Grad Norm 3.6917(5.8709) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 54.9386(55.3767) | Bit/dim 3.7275(3.7446) | Xent 0.9592(0.9341) | Loss 4.2071(4.2116) | Error 0.3378(0.3342) Steps 598(587.62) | Grad Norm 6.6596(5.8287) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 57.1319(55.5000) | Bit/dim 3.7370(3.7462) | Xent 0.9069(0.9327) | Loss 4.1904(4.2125) | Error 0.3333(0.3338) Steps 592(588.30) | Grad Norm 2.6726(6.0469) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 55.9959(55.4829) | Bit/dim 3.7377(3.7470) | Xent 0.9317(0.9331) | Loss 4.2036(4.2136) | Error 0.3389(0.3337) Steps 580(587.13) | Grad Norm 5.0238(5.7682) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 59.1689(55.5726) | Bit/dim 3.7715(3.7464) | Xent 0.9190(0.9269) | Loss 4.2310(4.2099) | Error 0.3356(0.3322) Steps 580(586.40) | Grad Norm 4.9888(5.3081) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 56.8600(55.6000) | Bit/dim 3.7504(3.7445) | Xent 0.9410(0.9237) | Loss 4.2209(4.2063) | Error 0.3311(0.3296) Steps 592(587.33) | Grad Norm 6.8955(5.4591) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 109.0563, Epoch Time 3180.6097(2890.9167), Bit/dim 3.7310(best: 3.7433), Xent 0.8774, Loss 4.1697, Error 0.3083(best: 0.3141)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 56.6432(55.6440) | Bit/dim 3.6919(3.7400) | Xent 0.8465(0.9140) | Loss 4.1151(4.1970) | Error 0.2744(0.3245) Steps 580(587.56) | Grad Norm 6.6482(5.5769) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 53.3722(55.5095) | Bit/dim 3.7565(3.7385) | Xent 0.8664(0.9134) | Loss 4.1897(4.1952) | Error 0.2922(0.3242) Steps 586(586.23) | Grad Norm 5.4638(5.5640) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 56.4378(55.4576) | Bit/dim 3.7345(3.7390) | Xent 0.9012(0.9103) | Loss 4.1851(4.1941) | Error 0.3322(0.3235) Steps 580(586.12) | Grad Norm 3.6880(5.5513) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 56.2562(55.5907) | Bit/dim 3.6823(3.7372) | Xent 0.8807(0.9018) | Loss 4.1227(4.1881) | Error 0.3256(0.3225) Steps 586(585.77) | Grad Norm 2.8921(5.4560) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 53.4443(55.6988) | Bit/dim 3.7235(3.7356) | Xent 0.9115(0.8986) | Loss 4.1793(4.1848) | Error 0.3278(0.3218) Steps 592(586.34) | Grad Norm 2.3775(5.4124) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 109.1519, Epoch Time 3192.7310(2899.9711), Bit/dim 3.7333(best: 3.7310), Xent 0.9029, Loss 4.1848, Error 0.3186(best: 0.3083)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 54.1466(55.8124) | Bit/dim 3.7802(3.7363) | Xent 0.8770(0.8970) | Loss 4.2187(4.1848) | Error 0.3222(0.3210) Steps 598(586.83) | Grad Norm 7.5384(5.2157) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 54.2898(55.4070) | Bit/dim 3.7493(3.7375) | Xent 0.9024(0.8928) | Loss 4.2005(4.1839) | Error 0.3189(0.3188) Steps 580(585.96) | Grad Norm 4.3172(5.1173) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 55.3935(55.5216) | Bit/dim 3.7497(3.7312) | Xent 0.8818(0.8933) | Loss 4.1906(4.1779) | Error 0.3278(0.3187) Steps 580(585.95) | Grad Norm 6.2058(5.0911) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 57.3519(55.4944) | Bit/dim 3.7320(3.7322) | Xent 0.8859(0.8878) | Loss 4.1749(4.1761) | Error 0.3233(0.3174) Steps 592(585.84) | Grad Norm 6.6473(4.9602) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 56.9177(55.3480) | Bit/dim 3.7607(3.7304) | Xent 0.8240(0.8885) | Loss 4.1727(4.1746) | Error 0.2922(0.3183) Steps 580(584.68) | Grad Norm 3.9426(4.7961) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 52.7154(55.3502) | Bit/dim 3.7050(3.7308) | Xent 0.9257(0.8965) | Loss 4.1678(4.1791) | Error 0.3478(0.3220) Steps 580(584.71) | Grad Norm 8.1583(5.4955) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 109.0422, Epoch Time 3164.5281(2907.9078), Bit/dim 3.7391(best: 3.7310), Xent 0.8586, Loss 4.1684, Error 0.3074(best: 0.3083)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 55.7707(55.4307) | Bit/dim 3.7267(3.7301) | Xent 0.8534(0.8996) | Loss 4.1534(4.1799) | Error 0.2867(0.3225) Steps 580(585.18) | Grad Norm 2.7045(5.5726) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 53.3269(55.1759) | Bit/dim 3.7224(3.7321) | Xent 0.9052(0.8939) | Loss 4.1750(4.1790) | Error 0.3300(0.3203) Steps 586(585.07) | Grad Norm 3.9872(5.4985) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 53.6069(55.2870) | Bit/dim 3.7381(3.7320) | Xent 0.8436(0.8917) | Loss 4.1599(4.1779) | Error 0.3133(0.3198) Steps 580(585.96) | Grad Norm 3.2432(5.5636) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 57.4459(55.2357) | Bit/dim 3.7171(3.7302) | Xent 0.8830(0.8877) | Loss 4.1586(4.1740) | Error 0.3156(0.3171) Steps 604(586.96) | Grad Norm 7.0295(5.3140) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 56.4725(55.2818) | Bit/dim 3.7350(3.7323) | Xent 0.9064(0.9032) | Loss 4.1881(4.1839) | Error 0.3178(0.3208) Steps 598(586.90) | Grad Norm 6.4346(5.7819) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 108.7413, Epoch Time 3163.2886(2915.5692), Bit/dim 3.7414(best: 3.7310), Xent 0.8585, Loss 4.1706, Error 0.3055(best: 0.3074)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 53.1404(55.1465) | Bit/dim 3.7453(3.7327) | Xent 0.8389(0.8959) | Loss 4.1647(4.1807) | Error 0.2822(0.3171) Steps 592(587.05) | Grad Norm 2.9006(5.3537) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 57.8418(55.4328) | Bit/dim 3.7517(3.7322) | Xent 0.9328(0.8917) | Loss 4.2181(4.1781) | Error 0.3278(0.3155) Steps 586(586.79) | Grad Norm 4.6591(5.4530) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 53.6879(55.3482) | Bit/dim 3.7177(3.7304) | Xent 0.8032(0.8836) | Loss 4.1193(4.1722) | Error 0.2756(0.3141) Steps 586(586.43) | Grad Norm 2.9139(5.0886) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 53.7598(55.3485) | Bit/dim 3.6973(3.7266) | Xent 0.8440(0.8729) | Loss 4.1193(4.1630) | Error 0.3011(0.3107) Steps 592(587.69) | Grad Norm 2.6992(4.9360) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 55.2335(55.2890) | Bit/dim 3.7285(3.7269) | Xent 0.7965(0.8739) | Loss 4.1268(4.1639) | Error 0.2778(0.3107) Steps 604(588.62) | Grad Norm 6.3000(5.2394) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 58.7805(55.4457) | Bit/dim 3.7337(3.7249) | Xent 0.9526(0.8723) | Loss 4.2100(4.1611) | Error 0.3378(0.3102) Steps 598(586.85) | Grad Norm 6.7058(5.1276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 108.0790, Epoch Time 3175.6348(2923.3712), Bit/dim 3.7272(best: 3.7310), Xent 0.9036, Loss 4.1790, Error 0.3186(best: 0.3055)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 56.2696(55.8471) | Bit/dim 3.7239(3.7247) | Xent 0.8094(0.8676) | Loss 4.1286(4.1585) | Error 0.2944(0.3099) Steps 592(586.15) | Grad Norm 5.2972(5.3966) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 55.8427(55.7835) | Bit/dim 3.6884(3.7262) | Xent 0.8482(0.8696) | Loss 4.1125(4.1610) | Error 0.3000(0.3118) Steps 574(586.03) | Grad Norm 4.0957(5.0925) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 57.2738(55.6822) | Bit/dim 3.7023(3.7240) | Xent 0.8790(0.8690) | Loss 4.1419(4.1585) | Error 0.3178(0.3113) Steps 598(584.83) | Grad Norm 3.9853(4.8542) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 56.0646(55.7722) | Bit/dim 3.7315(3.7220) | Xent 0.9265(0.8732) | Loss 4.1947(4.1586) | Error 0.3267(0.3116) Steps 586(585.31) | Grad Norm 6.4599(5.1318) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 56.4435(55.7984) | Bit/dim 3.7357(3.7232) | Xent 0.8211(0.8723) | Loss 4.1462(4.1594) | Error 0.2956(0.3128) Steps 580(585.00) | Grad Norm 6.5914(5.0555) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 110.1893, Epoch Time 3201.2364(2931.7072), Bit/dim 3.7139(best: 3.7272), Xent 0.8606, Loss 4.1442, Error 0.3029(best: 0.3055)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 58.9160(55.8618) | Bit/dim 3.7290(3.7221) | Xent 0.8600(0.8758) | Loss 4.1590(4.1600) | Error 0.3044(0.3136) Steps 592(585.29) | Grad Norm 4.8248(5.3037) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 55.7734(55.6709) | Bit/dim 3.6870(3.7179) | Xent 0.7597(0.8636) | Loss 4.0668(4.1497) | Error 0.2800(0.3104) Steps 580(584.62) | Grad Norm 3.9077(5.3802) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 52.8990(55.2687) | Bit/dim 3.7225(3.7165) | Xent 0.8564(0.8593) | Loss 4.1507(4.1462) | Error 0.3022(0.3076) Steps 574(584.61) | Grad Norm 3.9111(4.9759) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 54.2050(55.2271) | Bit/dim 3.7271(3.7190) | Xent 0.8337(0.8589) | Loss 4.1439(4.1485) | Error 0.2967(0.3077) Steps 592(585.07) | Grad Norm 6.2026(5.0061) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 58.2128(55.3763) | Bit/dim 3.7319(3.7183) | Xent 0.8945(0.8680) | Loss 4.1791(4.1523) | Error 0.3267(0.3120) Steps 598(585.62) | Grad Norm 4.7544(5.3090) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 54.1116(55.3672) | Bit/dim 3.7223(3.7190) | Xent 0.8367(0.8675) | Loss 4.1406(4.1528) | Error 0.2956(0.3126) Steps 592(586.66) | Grad Norm 4.1161(5.2536) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 107.4431, Epoch Time 3164.4294(2938.6888), Bit/dim 3.7221(best: 3.7139), Xent 0.8471, Loss 4.1457, Error 0.3048(best: 0.3029)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 58.0436(55.5338) | Bit/dim 3.6934(3.7176) | Xent 0.8417(0.8597) | Loss 4.1143(4.1474) | Error 0.2922(0.3079) Steps 598(587.58) | Grad Norm 2.8612(5.0251) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 59.1878(55.8551) | Bit/dim 3.7392(3.7175) | Xent 0.8611(0.8504) | Loss 4.1697(4.1427) | Error 0.3133(0.3043) Steps 592(587.97) | Grad Norm 5.2870(4.9419) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 57.8571(55.7009) | Bit/dim 3.7266(3.7158) | Xent 0.8336(0.8496) | Loss 4.1433(4.1406) | Error 0.2856(0.3028) Steps 586(586.98) | Grad Norm 4.7044(4.8588) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 58.4958(55.6645) | Bit/dim 3.7250(3.7150) | Xent 0.8122(0.8508) | Loss 4.1311(4.1404) | Error 0.2811(0.3036) Steps 586(586.56) | Grad Norm 9.3767(5.1322) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 56.9296(55.5457) | Bit/dim 3.7488(3.7139) | Xent 0.9138(0.8554) | Loss 4.2057(4.1416) | Error 0.3289(0.3071) Steps 580(586.29) | Grad Norm 10.0949(5.3566) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 107.4256, Epoch Time 3187.4952(2946.1530), Bit/dim 3.7219(best: 3.7139), Xent 0.9272, Loss 4.1855, Error 0.3279(best: 0.3029)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 53.4932(55.5695) | Bit/dim 3.6912(3.7147) | Xent 0.8461(0.8580) | Loss 4.1142(4.1437) | Error 0.3156(0.3087) Steps 580(586.30) | Grad Norm 5.5329(5.5168) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 54.3199(55.5481) | Bit/dim 3.7180(3.7154) | Xent 0.8681(0.8527) | Loss 4.1521(4.1417) | Error 0.3078(0.3062) Steps 580(586.52) | Grad Norm 4.0210(5.4816) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 53.8004(55.6935) | Bit/dim 3.7190(3.7172) | Xent 0.8151(0.8516) | Loss 4.1266(4.1430) | Error 0.3133(0.3054) Steps 580(585.89) | Grad Norm 3.9604(5.5269) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 53.9964(55.7777) | Bit/dim 3.7384(3.7175) | Xent 0.9021(0.8536) | Loss 4.1895(4.1443) | Error 0.3244(0.3059) Steps 586(586.13) | Grad Norm 5.7184(5.6142) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 56.2469(55.9820) | Bit/dim 3.7349(3.7151) | Xent 0.8239(0.8526) | Loss 4.1468(4.1414) | Error 0.3133(0.3060) Steps 580(586.91) | Grad Norm 7.2231(5.4062) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 54.1815(55.7244) | Bit/dim 3.7041(3.7154) | Xent 0.8882(0.8557) | Loss 4.1483(4.1432) | Error 0.3089(0.3057) Steps 592(587.50) | Grad Norm 8.3479(5.5477) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 110.6890, Epoch Time 3202.0622(2953.8303), Bit/dim 3.7202(best: 3.7139), Xent 0.8876, Loss 4.1640, Error 0.3153(best: 0.3029)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 59.4987(56.0695) | Bit/dim 3.7121(3.7180) | Xent 0.7989(0.8480) | Loss 4.1115(4.1420) | Error 0.2867(0.3022) Steps 580(586.89) | Grad Norm 4.3625(5.3545) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 57.5285(56.0229) | Bit/dim 3.7210(3.7141) | Xent 0.8348(0.8413) | Loss 4.1384(4.1347) | Error 0.3033(0.2999) Steps 580(586.15) | Grad Norm 4.0706(4.8272) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 55.8112(56.0301) | Bit/dim 3.6780(3.7121) | Xent 0.8827(0.8457) | Loss 4.1194(4.1349) | Error 0.3256(0.3018) Steps 598(586.92) | Grad Norm 4.5873(4.8751) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 53.4822(55.8646) | Bit/dim 3.7150(3.7126) | Xent 0.8721(0.8460) | Loss 4.1511(4.1356) | Error 0.3178(0.3010) Steps 586(585.90) | Grad Norm 3.8588(5.0560) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 57.0267(55.9141) | Bit/dim 3.7107(3.7099) | Xent 0.8342(0.8397) | Loss 4.1278(4.1297) | Error 0.2956(0.2992) Steps 574(586.63) | Grad Norm 4.6137(4.9781) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 108.3392, Epoch Time 3222.4590(2961.8892), Bit/dim 3.7051(best: 3.7139), Xent 0.8209, Loss 4.1156, Error 0.2923(best: 0.3029)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 56.9239(56.1525) | Bit/dim 3.7411(3.7104) | Xent 0.8521(0.8401) | Loss 4.1672(4.1305) | Error 0.3067(0.3005) Steps 592(586.75) | Grad Norm 8.3482(5.1067) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 59.8144(56.4028) | Bit/dim 3.7354(3.7125) | Xent 0.8375(0.8435) | Loss 4.1541(4.1343) | Error 0.2889(0.3002) Steps 592(587.96) | Grad Norm 6.8567(5.3595) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 58.0932(56.3049) | Bit/dim 3.6826(3.7095) | Xent 0.8026(0.8380) | Loss 4.0839(4.1285) | Error 0.3056(0.2985) Steps 580(587.50) | Grad Norm 3.8400(5.1959) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 54.5635(56.1410) | Bit/dim 3.7260(3.7085) | Xent 0.9114(0.8401) | Loss 4.1817(4.1286) | Error 0.3256(0.3005) Steps 592(588.70) | Grad Norm 9.1152(5.3176) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 56.1561(55.9944) | Bit/dim 3.7132(3.7102) | Xent 0.8816(0.8419) | Loss 4.1541(4.1312) | Error 0.3056(0.3015) Steps 580(588.22) | Grad Norm 7.8965(5.4228) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run3 --seed 3 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
