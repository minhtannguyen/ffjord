{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_3_run1', scale=1.0, scale_fac=1.0, scale_std=3.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 13.5258(29.8613) | Bit/dim 8.6906(8.9520) | Xent 2.2806(2.3001) | Loss 22.6354(23.4294) | Error 0.7933(0.8603) Steps 0(0.00) | Grad Norm 23.7934(30.8063) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.9651(25.4542) | Bit/dim 8.4863(8.8630) | Xent 2.2266(2.2874) | Loss 22.4628(23.2102) | Error 0.7244(0.8322) Steps 0(0.00) | Grad Norm 10.0006(26.6714) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 12.7697(22.2353) | Bit/dim 8.3928(8.7513) | Xent 2.1745(2.2634) | Loss 21.7136(22.9349) | Error 0.7578(0.8091) Steps 0(0.00) | Grad Norm 9.7509(21.9012) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 13.5242(19.9157) | Bit/dim 8.1926(8.6233) | Xent 2.1132(2.2346) | Loss 21.7182(22.6326) | Error 0.7222(0.7909) Steps 0(0.00) | Grad Norm 6.2093(17.9761) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.5652(18.1832) | Bit/dim 7.9751(8.4738) | Xent 2.1023(2.2021) | Loss 21.0301(22.2657) | Error 0.7100(0.7752) Steps 0(0.00) | Grad Norm 6.0197(14.9300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 78.8873, Epoch Time 846.5525(846.5525), Bit/dim 7.7700(best: inf), Xent 2.0786, Loss 8.8093, Error 0.7011(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.5225(16.9245) | Bit/dim 7.6556(8.2923) | Xent 2.0906(2.1718) | Loss 20.3964(22.5126) | Error 0.7067(0.7585) Steps 0(0.00) | Grad Norm 5.5342(12.5818) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.4368(16.0114) | Bit/dim 7.3662(8.0808) | Xent 2.0777(2.1454) | Loss 19.5783(21.8337) | Error 0.7056(0.7424) Steps 0(0.00) | Grad Norm 4.7528(10.6363) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 13.8896(15.4024) | Bit/dim 7.1816(7.8642) | Xent 2.0847(2.1250) | Loss 19.3669(21.2196) | Error 0.6778(0.7266) Steps 0(0.00) | Grad Norm 3.4745(8.8880) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.7720(14.8949) | Bit/dim 7.0763(7.6684) | Xent 2.0837(2.1142) | Loss 19.1703(20.6769) | Error 0.6811(0.7189) Steps 0(0.00) | Grad Norm 2.6683(7.2988) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 14.0016(14.6021) | Bit/dim 7.0197(7.5057) | Xent 2.0807(2.1035) | Loss 18.8687(20.2364) | Error 0.7111(0.7147) Steps 0(0.00) | Grad Norm 2.8482(6.0149) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.8134(14.4401) | Bit/dim 6.9995(7.3757) | Xent 2.0667(2.0949) | Loss 18.9984(19.9128) | Error 0.7211(0.7136) Steps 0(0.00) | Grad Norm 7.2334(5.4669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 78.0790, Epoch Time 847.8863(846.5925), Bit/dim 6.9918(best: 7.7700), Xent 2.0576, Loss 8.0207, Error 0.6959(best: 0.7011)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.2801(14.3851) | Bit/dim 6.9381(7.2706) | Xent 2.0481(2.0855) | Loss 18.7436(20.2517) | Error 0.6833(0.7116) Steps 0(0.00) | Grad Norm 4.1039(5.3636) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.7455(14.4620) | Bit/dim 6.9080(7.1832) | Xent 2.0638(2.0761) | Loss 18.7295(19.8687) | Error 0.7256(0.7084) Steps 0(0.00) | Grad Norm 5.4860(4.9465) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 14.2654(14.4538) | Bit/dim 6.8745(7.1069) | Xent 2.0528(2.0703) | Loss 18.5682(19.5507) | Error 0.6944(0.7049) Steps 0(0.00) | Grad Norm 16.1503(5.6979) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 14.3772(14.4781) | Bit/dim 6.8472(7.0376) | Xent 1.9945(2.0587) | Loss 18.6817(19.2781) | Error 0.6644(0.6993) Steps 0(0.00) | Grad Norm 2.2593(5.8764) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.7726(14.5542) | Bit/dim 6.7215(6.9635) | Xent 1.9752(2.0478) | Loss 18.2724(19.0223) | Error 0.6689(0.6947) Steps 0(0.00) | Grad Norm 16.2996(6.8160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 80.7793, Epoch Time 897.9116(848.1321), Bit/dim 6.6404(best: 6.9918), Xent 2.0245, Loss 7.6527, Error 0.6910(best: 0.6959)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 14.2771(14.5372) | Bit/dim 6.5825(6.8815) | Xent 2.0171(2.0449) | Loss 17.9351(19.4917) | Error 0.6578(0.6960) Steps 0(0.00) | Grad Norm 16.1967(13.7163) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 14.2369(14.5088) | Bit/dim 6.4499(6.7792) | Xent 2.1656(2.0456) | Loss 17.9282(19.0503) | Error 0.7778(0.6997) Steps 0(0.00) | Grad Norm 108.2801(24.6657) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 14.7760(14.5213) | Bit/dim 6.2463(6.6579) | Xent 2.0855(2.0622) | Loss 17.4051(18.6447) | Error 0.7511(0.7148) Steps 0(0.00) | Grad Norm 53.2528(35.4946) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 14.3931(14.5182) | Bit/dim 6.2302(6.5450) | Xent 2.1761(2.0909) | Loss 17.1482(18.2903) | Error 0.7700(0.7286) Steps 0(0.00) | Grad Norm 111.2584(56.5531) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.5729(14.5421) | Bit/dim 6.0277(6.4267) | Xent 2.0663(2.0852) | Loss 16.6493(17.9258) | Error 0.7022(0.7238) Steps 0(0.00) | Grad Norm 40.0436(53.0072) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 14.5132(14.4493) | Bit/dim 5.8654(6.2944) | Xent 2.0396(2.0756) | Loss 16.4302(17.5586) | Error 0.6889(0.7176) Steps 0(0.00) | Grad Norm 27.4325(45.3006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 80.0089, Epoch Time 893.2055(849.4843), Bit/dim 5.8492(best: 6.6404), Xent 2.0278, Loss 6.8631, Error 0.6834(best: 0.6910)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.9147(14.3714) | Bit/dim 5.8340(6.1711) | Xent 1.9758(2.0698) | Loss 16.1083(17.8633) | Error 0.6522(0.7187) Steps 0(0.00) | Grad Norm 28.1952(48.0915) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.9107(14.3420) | Bit/dim 5.7199(6.0668) | Xent 1.9822(2.0614) | Loss 16.0606(17.4405) | Error 0.6544(0.7169) Steps 0(0.00) | Grad Norm 11.8639(48.2716) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.6522(14.2549) | Bit/dim 5.6651(5.9721) | Xent 1.9559(2.0480) | Loss 15.8104(17.0422) | Error 0.6422(0.7092) Steps 0(0.00) | Grad Norm 15.0311(41.3408) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.8650(14.1695) | Bit/dim 5.6892(5.8922) | Xent 2.0449(2.0353) | Loss 15.7896(16.7457) | Error 0.6700(0.7014) Steps 0(0.00) | Grad Norm 31.0634(37.4511) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 14.1177(14.1060) | Bit/dim 5.6313(5.8296) | Xent 1.9717(2.0211) | Loss 15.8376(16.5145) | Error 0.6833(0.6924) Steps 0(0.00) | Grad Norm 24.1726(33.4721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 78.5927, Epoch Time 869.2354(850.0768), Bit/dim 5.6396(best: 5.8492), Xent 1.9578, Loss 6.6185, Error 0.6865(best: 0.6834)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.9700(14.0457) | Bit/dim 5.6188(5.7778) | Xent 1.9509(2.0065) | Loss 15.7212(17.0404) | Error 0.6644(0.6879) Steps 0(0.00) | Grad Norm 12.6791(32.8719) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.5966(14.0239) | Bit/dim 5.5899(5.7317) | Xent 1.9350(2.0034) | Loss 15.6927(16.6972) | Error 0.6856(0.6907) Steps 0(0.00) | Grad Norm 57.0375(40.9597) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.2244(13.9486) | Bit/dim 5.5976(5.6903) | Xent 1.9329(1.9902) | Loss 15.4153(16.4185) | Error 0.6544(0.6847) Steps 0(0.00) | Grad Norm 28.0936(38.8559) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 14.1823(13.9301) | Bit/dim 5.5673(5.6563) | Xent 1.9308(1.9778) | Loss 15.6125(16.2016) | Error 0.6411(0.6809) Steps 0(0.00) | Grad Norm 15.3125(36.1396) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 14.8272(14.0091) | Bit/dim 5.5401(5.6273) | Xent 1.9935(1.9711) | Loss 15.5467(16.0312) | Error 0.7156(0.6800) Steps 0(0.00) | Grad Norm 57.9447(35.5405) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 14.3841(14.0596) | Bit/dim 5.5473(5.5998) | Xent 1.9635(1.9783) | Loss 15.7168(15.9279) | Error 0.6767(0.6838) Steps 0(0.00) | Grad Norm 42.3888(41.0855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 79.1760, Epoch Time 866.1248(850.5582), Bit/dim 5.4993(best: 5.6396), Xent 1.9752, Loss 6.4869, Error 0.6877(best: 0.6834)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 14.1788(14.1158) | Bit/dim 5.4548(5.5711) | Xent 1.9465(1.9785) | Loss 15.4130(16.4293) | Error 0.6522(0.6863) Steps 0(0.00) | Grad Norm 21.2065(41.4993) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.9649(14.1240) | Bit/dim 5.3875(5.5392) | Xent 1.9543(1.9712) | Loss 15.3465(16.1566) | Error 0.6833(0.6839) Steps 0(0.00) | Grad Norm 24.0716(36.3155) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.9821(14.1574) | Bit/dim 5.3649(5.4997) | Xent 1.9198(1.9574) | Loss 15.2591(15.9207) | Error 0.6489(0.6782) Steps 0(0.00) | Grad Norm 9.7769(33.0594) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.2680(14.2063) | Bit/dim 5.3460(5.4674) | Xent 1.9398(1.9470) | Loss 15.1565(15.7425) | Error 0.6556(0.6735) Steps 0(0.00) | Grad Norm 21.7275(32.2791) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 15.1378(14.3731) | Bit/dim 5.3506(5.4317) | Xent 1.8711(1.9345) | Loss 15.2599(15.6012) | Error 0.6333(0.6687) Steps 0(0.00) | Grad Norm 7.0667(30.3754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 84.0650, Epoch Time 894.3821(851.8729), Bit/dim 5.2790(best: 5.4993), Xent 1.8727, Loss 6.2154, Error 0.6284(best: 0.6834)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.5336(14.4555) | Bit/dim 5.3367(5.3972) | Xent 2.0053(1.9350) | Loss 15.2133(16.2469) | Error 0.7044(0.6726) Steps 0(0.00) | Grad Norm 62.2078(33.7403) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.7899(14.6405) | Bit/dim 5.2579(5.3618) | Xent 1.9090(1.9274) | Loss 14.8849(15.9110) | Error 0.6600(0.6703) Steps 0(0.00) | Grad Norm 17.6484(32.4795) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 14.8922(14.7044) | Bit/dim 5.2105(5.3257) | Xent 1.9218(1.9187) | Loss 14.8435(15.6428) | Error 0.6578(0.6665) Steps 0(0.00) | Grad Norm 13.0032(32.9807) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 14.9527(14.7629) | Bit/dim 5.1654(5.2979) | Xent 1.9346(1.9151) | Loss 14.7514(15.4326) | Error 0.6689(0.6674) Steps 0(0.00) | Grad Norm 27.2870(35.2179) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.7147(14.7148) | Bit/dim 5.1406(5.2647) | Xent 1.9663(1.9200) | Loss 14.8143(15.2774) | Error 0.7200(0.6701) Steps 0(0.00) | Grad Norm 41.2051(34.9173) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 15.4486(14.6840) | Bit/dim 5.1311(5.2335) | Xent 1.8937(1.9168) | Loss 14.6368(15.1173) | Error 0.6678(0.6709) Steps 0(0.00) | Grad Norm 19.0591(31.9350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 83.2669, Epoch Time 915.5727(853.7839), Bit/dim 5.1197(best: 5.2790), Xent 1.8581, Loss 6.0488, Error 0.6288(best: 0.6284)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 15.1742(14.7788) | Bit/dim 5.1067(5.2044) | Xent 1.8619(1.9104) | Loss 14.6896(15.6389) | Error 0.6544(0.6695) Steps 0(0.00) | Grad Norm 25.5628(30.4858) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.9753(14.8591) | Bit/dim 5.0971(5.1789) | Xent 1.8983(1.9204) | Loss 14.5650(15.3959) | Error 0.6611(0.6740) Steps 0(0.00) | Grad Norm 35.9947(34.5113) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 15.4253(15.0222) | Bit/dim 5.0774(5.1488) | Xent 1.9187(1.9183) | Loss 14.5952(15.1683) | Error 0.6589(0.6735) Steps 0(0.00) | Grad Norm 8.0219(32.5521) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 15.2104(15.1266) | Bit/dim 5.0454(5.1182) | Xent 1.9052(1.9087) | Loss 14.5153(14.9656) | Error 0.6800(0.6693) Steps 0(0.00) | Grad Norm 16.2581(29.0616) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.0829(15.2060) | Bit/dim 5.0124(5.0886) | Xent 1.8761(1.8985) | Loss 14.3161(14.8027) | Error 0.6444(0.6639) Steps 0(0.00) | Grad Norm 36.5784(28.1567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 83.5064, Epoch Time 943.3979(856.4724), Bit/dim 4.9581(best: 5.1197), Xent 1.8362, Loss 5.8762, Error 0.6322(best: 0.6284)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.9850(15.3087) | Bit/dim 4.9382(5.0636) | Xent 1.8143(1.8953) | Loss 14.0892(15.4144) | Error 0.6433(0.6638) Steps 0(0.00) | Grad Norm 21.8583(32.6096) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.9284(15.3242) | Bit/dim 4.9153(5.0321) | Xent 1.8081(1.8839) | Loss 13.9941(15.0871) | Error 0.6100(0.6586) Steps 0(0.00) | Grad Norm 13.1668(31.3211) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.9143(15.3072) | Bit/dim 4.8905(5.0012) | Xent 1.8206(1.8725) | Loss 14.0912(14.8234) | Error 0.6422(0.6544) Steps 0(0.00) | Grad Norm 6.8451(29.0702) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.1942(15.3207) | Bit/dim 5.1994(5.0343) | Xent 1.8970(1.9028) | Loss 14.6916(14.8014) | Error 0.6756(0.6642) Steps 0(0.00) | Grad Norm 31.4450(39.9324) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.2102(15.2948) | Bit/dim 5.0352(5.0493) | Xent 2.0127(1.9371) | Loss 14.5858(14.7793) | Error 0.7267(0.6802) Steps 0(0.00) | Grad Norm 18.7813(37.1629) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 15.4216(15.3411) | Bit/dim 4.8972(5.0247) | Xent 1.8779(1.9364) | Loss 14.1005(14.6485) | Error 0.6756(0.6822) Steps 0(0.00) | Grad Norm 9.5628(30.9260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 84.2272, Epoch Time 947.6686(859.2082), Bit/dim 4.9114(best: 4.9581), Xent 1.8800, Loss 5.8514, Error 0.6511(best: 0.6284)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 15.3243(15.3761) | Bit/dim 4.8963(4.9917) | Xent 1.8731(1.9271) | Loss 14.1118(15.1346) | Error 0.6678(0.6782) Steps 0(0.00) | Grad Norm 12.6622(26.0122) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.0867(15.3946) | Bit/dim 4.8019(4.9520) | Xent 1.9369(1.9160) | Loss 13.9669(14.8422) | Error 0.6922(0.6744) Steps 0(0.00) | Grad Norm 9.6249(21.8191) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 15.3968(15.4950) | Bit/dim 4.8232(4.9205) | Xent 1.8116(1.8931) | Loss 13.8428(14.5908) | Error 0.6400(0.6657) Steps 0(0.00) | Grad Norm 16.7783(18.7725) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.9518(15.6049) | Bit/dim 4.7385(4.8859) | Xent 1.8133(1.8700) | Loss 13.5831(14.3836) | Error 0.6478(0.6576) Steps 0(0.00) | Grad Norm 18.9687(17.7221) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 16.3965(15.7153) | Bit/dim 4.7683(4.8637) | Xent 1.8614(1.8596) | Loss 13.9566(14.2502) | Error 0.6544(0.6552) Steps 0(0.00) | Grad Norm 44.8102(22.6969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 86.5661, Epoch Time 970.8790(862.5584), Bit/dim 4.7581(best: 4.9114), Xent 1.7403, Loss 5.6282, Error 0.6120(best: 0.6284)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 16.2455(15.8118) | Bit/dim 4.8135(4.8396) | Xent 1.7767(1.8394) | Loss 13.9839(14.8694) | Error 0.6244(0.6492) Steps 0(0.00) | Grad Norm 43.2027(24.5803) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.9901(15.8744) | Bit/dim 4.6971(4.8174) | Xent 1.7973(1.8248) | Loss 13.7328(14.5854) | Error 0.6300(0.6445) Steps 0(0.00) | Grad Norm 32.0017(26.7525) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 16.6578(15.9833) | Bit/dim 4.7553(4.8082) | Xent 1.7252(1.8148) | Loss 13.8011(14.3837) | Error 0.6056(0.6402) Steps 0(0.00) | Grad Norm 32.4522(31.9595) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.4938(16.0680) | Bit/dim 4.7309(4.7916) | Xent 1.8270(1.8111) | Loss 13.8828(14.2086) | Error 0.6367(0.6387) Steps 0(0.00) | Grad Norm 20.0488(32.5592) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 16.4784(16.0968) | Bit/dim 4.8041(4.7845) | Xent 1.8398(1.8183) | Loss 13.8712(14.1055) | Error 0.6589(0.6425) Steps 0(0.00) | Grad Norm 48.1938(37.1613) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 16.8780(16.1132) | Bit/dim 4.7217(4.7674) | Xent 1.8454(1.8241) | Loss 13.8181(14.0056) | Error 0.6567(0.6470) Steps 0(0.00) | Grad Norm 15.8716(34.9225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 87.1154, Epoch Time 994.9706(866.5307), Bit/dim 4.6873(best: 4.7581), Xent 1.7316, Loss 5.5531, Error 0.6129(best: 0.6120)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 16.1897(16.1407) | Bit/dim 4.7054(4.7465) | Xent 1.7255(1.8026) | Loss 13.5026(14.5151) | Error 0.5967(0.6391) Steps 0(0.00) | Grad Norm 20.8450(29.5045) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 16.3752(16.1937) | Bit/dim 4.6853(4.7313) | Xent 1.8053(1.7913) | Loss 13.5189(14.2624) | Error 0.6456(0.6358) Steps 0(0.00) | Grad Norm 37.8786(30.7052) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 16.8086(16.3100) | Bit/dim 4.6427(4.7125) | Xent 1.7642(1.7796) | Loss 13.3631(14.0600) | Error 0.6167(0.6300) Steps 0(0.00) | Grad Norm 33.4007(29.1315) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 16.4536(16.3628) | Bit/dim 4.6766(4.7007) | Xent 1.8575(1.7774) | Loss 13.8089(13.9322) | Error 0.6700(0.6297) Steps 0(0.00) | Grad Norm 37.1692(31.4870) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 16.2952(16.3905) | Bit/dim 4.5981(4.6824) | Xent 1.7331(1.7697) | Loss 13.4935(13.7928) | Error 0.6267(0.6285) Steps 0(0.00) | Grad Norm 9.5929(29.0139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 90.6481, Epoch Time 1012.2235(870.9015), Bit/dim 4.6613(best: 4.6873), Xent 1.6453, Loss 5.4839, Error 0.5865(best: 0.6120)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.4462(16.4082) | Bit/dim 4.6751(4.6764) | Xent 1.8471(1.7700) | Loss 13.8680(14.4863) | Error 0.6567(0.6289) Steps 0(0.00) | Grad Norm 48.9530(31.5623) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 16.2840(16.3764) | Bit/dim 4.6301(4.6629) | Xent 1.7752(1.7671) | Loss 13.5241(14.2032) | Error 0.6300(0.6300) Steps 0(0.00) | Grad Norm 32.0487(31.4068) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 16.3092(16.3573) | Bit/dim 4.6016(4.6467) | Xent 1.7301(1.7599) | Loss 13.3767(13.9744) | Error 0.6156(0.6273) Steps 0(0.00) | Grad Norm 18.9977(28.3292) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.5828(16.3585) | Bit/dim 4.6115(4.6369) | Xent 1.7755(1.7473) | Loss 13.4339(13.8150) | Error 0.6144(0.6218) Steps 0(0.00) | Grad Norm 40.2315(28.5753) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 16.5498(16.4339) | Bit/dim 4.5718(4.6215) | Xent 1.7078(1.7340) | Loss 13.3944(13.6819) | Error 0.6167(0.6174) Steps 0(0.00) | Grad Norm 27.6103(27.0142) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 16.4179(16.4843) | Bit/dim 4.5953(4.6087) | Xent 1.7279(1.7250) | Loss 13.4007(13.5790) | Error 0.6111(0.6171) Steps 0(0.00) | Grad Norm 36.8477(27.0325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 89.3550, Epoch Time 1011.8499(875.1300), Bit/dim 4.5473(best: 4.6613), Xent 1.5924, Loss 5.3435, Error 0.5616(best: 0.5865)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 15.8664(16.4871) | Bit/dim 4.5259(4.5946) | Xent 1.6543(1.7151) | Loss 12.9247(14.1366) | Error 0.5944(0.6126) Steps 0(0.00) | Grad Norm 11.6864(27.1610) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 16.6515(16.5158) | Bit/dim 4.6025(4.5790) | Xent 1.6238(1.7066) | Loss 13.1800(13.8785) | Error 0.5844(0.6097) Steps 0(0.00) | Grad Norm 45.7713(25.2273) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 16.9461(16.6001) | Bit/dim 4.6879(4.6049) | Xent 1.9393(1.7442) | Loss 13.7105(13.8189) | Error 0.7111(0.6209) Steps 0(0.00) | Grad Norm 26.7499(30.1524) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 16.3933(16.6461) | Bit/dim 4.5733(4.6024) | Xent 1.6993(1.7617) | Loss 13.2316(13.7090) | Error 0.5844(0.6295) Steps 0(0.00) | Grad Norm 11.9088(26.3842) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 16.1124(16.6470) | Bit/dim 4.5524(4.5907) | Xent 1.7294(1.7487) | Loss 13.2048(13.5811) | Error 0.6256(0.6264) Steps 0(0.00) | Grad Norm 17.7340(22.8721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 89.0231, Epoch Time 1024.0389(879.5972), Bit/dim 4.5759(best: 4.5473), Xent 1.6730, Loss 5.4124, Error 0.5945(best: 0.5616)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.2272(16.6271) | Bit/dim 4.5322(4.5749) | Xent 1.6482(1.7315) | Loss 12.9678(14.2173) | Error 0.6178(0.6220) Steps 0(0.00) | Grad Norm 14.8643(24.1064) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.2407(16.6333) | Bit/dim 4.5061(4.5558) | Xent 1.6643(1.7134) | Loss 13.1852(13.9116) | Error 0.5878(0.6162) Steps 0(0.00) | Grad Norm 24.5696(22.8100) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.8389(16.6846) | Bit/dim 4.4681(4.5358) | Xent 1.6258(1.6957) | Loss 13.0087(13.6677) | Error 0.5689(0.6101) Steps 0(0.00) | Grad Norm 13.7298(21.1662) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.7557(16.7150) | Bit/dim 4.4390(4.5168) | Xent 1.5767(1.6752) | Loss 12.8355(13.4690) | Error 0.5344(0.6011) Steps 0(0.00) | Grad Norm 14.5503(20.2364) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.7318(16.7081) | Bit/dim 4.4487(4.5032) | Xent 1.6961(1.6654) | Loss 13.0602(13.3441) | Error 0.6156(0.5986) Steps 0(0.00) | Grad Norm 35.7504(20.8100) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 16.6351(16.7167) | Bit/dim 4.4204(4.4867) | Xent 1.6091(1.6573) | Loss 12.7201(13.2159) | Error 0.5889(0.5962) Steps 0(0.00) | Grad Norm 18.0004(19.4523) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 90.7254, Epoch Time 1028.3309(884.0592), Bit/dim 4.4459(best: 4.5473), Xent 1.5343, Loss 5.2131, Error 0.5479(best: 0.5616)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.7514(16.7669) | Bit/dim 4.4509(4.4721) | Xent 1.6260(1.6416) | Loss 13.0065(13.7811) | Error 0.5933(0.5897) Steps 0(0.00) | Grad Norm 14.1852(18.9972) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 16.8762(16.8091) | Bit/dim 4.3928(4.4593) | Xent 1.6390(1.6389) | Loss 12.6234(13.5414) | Error 0.5722(0.5889) Steps 0(0.00) | Grad Norm 19.1175(20.6538) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 16.9068(16.9193) | Bit/dim 4.3947(4.4462) | Xent 1.5823(1.6278) | Loss 12.7089(13.3475) | Error 0.5589(0.5858) Steps 0(0.00) | Grad Norm 19.8530(20.1061) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 17.1798(16.9550) | Bit/dim 4.4668(4.4431) | Xent 1.6440(1.6320) | Loss 12.9982(13.2257) | Error 0.5722(0.5880) Steps 0(0.00) | Grad Norm 36.2108(22.9760) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.9746(17.0210) | Bit/dim 4.4373(4.4358) | Xent 1.6194(1.6374) | Loss 12.8859(13.1407) | Error 0.5878(0.5885) Steps 0(0.00) | Grad Norm 9.9204(21.8153) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 91.1547, Epoch Time 1045.7803(888.9109), Bit/dim 4.4087(best: 4.4459), Xent 1.5324, Loss 5.1749, Error 0.5453(best: 0.5479)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 17.5292(17.0093) | Bit/dim 4.3925(4.4207) | Xent 1.5447(1.6252) | Loss 12.7112(13.7943) | Error 0.5389(0.5839) Steps 0(0.00) | Grad Norm 22.1958(20.4153) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 18.3425(17.0374) | Bit/dim 4.3625(4.4062) | Xent 1.5369(1.6119) | Loss 12.6804(13.4980) | Error 0.5556(0.5805) Steps 0(0.00) | Grad Norm 11.9213(18.9721) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.9742(17.1387) | Bit/dim 4.3784(4.4009) | Xent 1.5825(1.6103) | Loss 12.7057(13.3018) | Error 0.5578(0.5792) Steps 0(0.00) | Grad Norm 17.3033(21.0508) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.3361(17.0461) | Bit/dim 4.3398(4.4018) | Xent 1.6569(1.6176) | Loss 12.5819(13.1763) | Error 0.5956(0.5837) Steps 0(0.00) | Grad Norm 22.1993(21.4549) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.8354(17.0222) | Bit/dim 4.3388(4.3909) | Xent 1.5621(1.6047) | Loss 12.5796(13.0278) | Error 0.5589(0.5803) Steps 0(0.00) | Grad Norm 6.2205(18.8482) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 16.5523(16.9616) | Bit/dim 4.3251(4.3760) | Xent 1.5532(1.5954) | Loss 12.4489(12.8978) | Error 0.5556(0.5763) Steps 0(0.00) | Grad Norm 11.8483(17.8425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 90.0901, Epoch Time 1045.3904(893.6053), Bit/dim 4.3163(best: 4.4087), Xent 1.4761, Loss 5.0544, Error 0.5315(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.7035(17.0438) | Bit/dim 4.2922(4.3579) | Xent 1.5453(1.5764) | Loss 12.5273(13.4507) | Error 0.5689(0.5704) Steps 0(0.00) | Grad Norm 9.2736(16.2182) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 17.5309(17.0968) | Bit/dim 4.2974(4.3391) | Xent 1.5896(1.5662) | Loss 12.5544(13.1926) | Error 0.5867(0.5686) Steps 0(0.00) | Grad Norm 36.5593(16.9169) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 17.6901(17.1476) | Bit/dim 4.2941(4.3336) | Xent 1.6018(1.5630) | Loss 12.4299(13.0298) | Error 0.5733(0.5673) Steps 0(0.00) | Grad Norm 30.9530(18.5724) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 17.7345(17.1930) | Bit/dim 4.3023(4.3266) | Xent 1.5739(1.5632) | Loss 12.4027(12.8950) | Error 0.5656(0.5669) Steps 0(0.00) | Grad Norm 12.6551(19.2889) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 17.6682(17.1269) | Bit/dim 4.2799(4.3130) | Xent 1.5444(1.5550) | Loss 12.3304(12.7605) | Error 0.5622(0.5650) Steps 0(0.00) | Grad Norm 21.7736(18.8091) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 88.6871, Epoch Time 1050.2199(898.3037), Bit/dim 4.2584(best: 4.3163), Xent 1.4371, Loss 4.9769, Error 0.5152(best: 0.5315)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 17.7319(17.1104) | Bit/dim 4.2256(4.2986) | Xent 1.4499(1.5437) | Loss 12.1489(13.4078) | Error 0.5278(0.5614) Steps 0(0.00) | Grad Norm 6.5057(17.3074) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 17.5013(17.2036) | Bit/dim 4.2396(4.2838) | Xent 1.4955(1.5229) | Loss 12.3596(13.1172) | Error 0.5656(0.5538) Steps 0(0.00) | Grad Norm 12.1967(15.1095) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 17.4928(17.2321) | Bit/dim 4.2190(4.2741) | Xent 1.6928(1.5280) | Loss 12.3978(12.9225) | Error 0.5956(0.5545) Steps 0(0.00) | Grad Norm 39.6402(18.6631) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 17.7000(17.2735) | Bit/dim 4.2641(4.2667) | Xent 1.5103(1.5366) | Loss 12.2796(12.7680) | Error 0.5456(0.5572) Steps 0(0.00) | Grad Norm 10.8723(18.4190) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 17.7520(17.3388) | Bit/dim 4.1791(4.2549) | Xent 1.5081(1.5286) | Loss 12.2943(12.6525) | Error 0.5378(0.5536) Steps 0(0.00) | Grad Norm 14.1425(16.6130) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.5278(17.3027) | Bit/dim 4.1783(4.2419) | Xent 1.4458(1.5113) | Loss 11.9965(12.5219) | Error 0.5256(0.5494) Steps 0(0.00) | Grad Norm 9.8867(15.9495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 88.8652, Epoch Time 1061.9195(903.2122), Bit/dim 4.2009(best: 4.2584), Xent 1.3781, Loss 4.8900, Error 0.5019(best: 0.5152)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 17.1866(17.2309) | Bit/dim 4.2104(4.2309) | Xent 1.4917(1.4991) | Loss 12.3881(13.0842) | Error 0.5411(0.5427) Steps 0(0.00) | Grad Norm 25.9413(16.7740) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 16.3096(17.1542) | Bit/dim 4.2054(4.2226) | Xent 1.4701(1.4892) | Loss 12.1359(12.8381) | Error 0.5278(0.5389) Steps 0(0.00) | Grad Norm 24.9231(17.1037) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 16.5361(17.1561) | Bit/dim 4.1451(4.2143) | Xent 1.4218(1.4785) | Loss 12.0264(12.6620) | Error 0.5333(0.5374) Steps 0(0.00) | Grad Norm 14.7705(17.7339) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 17.1945(17.1242) | Bit/dim 4.1851(4.2040) | Xent 1.3962(1.4645) | Loss 12.1422(12.5092) | Error 0.5111(0.5328) Steps 0(0.00) | Grad Norm 6.0432(16.1313) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.8747(17.1109) | Bit/dim 4.1735(4.1969) | Xent 1.4809(1.4619) | Loss 12.2191(12.4026) | Error 0.5489(0.5320) Steps 0(0.00) | Grad Norm 31.4656(17.0227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 88.5819, Epoch Time 1045.1206(907.4694), Bit/dim 4.2537(best: 4.2009), Xent 1.4983, Loss 5.0028, Error 0.5472(best: 0.5019)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 17.2410(17.0949) | Bit/dim 4.2384(4.2041) | Xent 1.5022(1.4909) | Loss 12.2898(13.1540) | Error 0.5378(0.5383) Steps 0(0.00) | Grad Norm 43.7934(20.7180) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 16.7852(17.0808) | Bit/dim 4.1627(4.2013) | Xent 1.4535(1.4838) | Loss 12.0536(12.8911) | Error 0.5278(0.5352) Steps 0(0.00) | Grad Norm 16.7166(19.4035) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.3866(16.8888) | Bit/dim 4.1691(4.1915) | Xent 1.3459(1.4602) | Loss 12.0033(12.6637) | Error 0.4733(0.5275) Steps 0(0.00) | Grad Norm 6.7812(17.0751) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 17.5353(16.8157) | Bit/dim 4.1626(4.1796) | Xent 1.4495(1.4506) | Loss 12.0709(12.4855) | Error 0.5244(0.5253) Steps 0(0.00) | Grad Norm 12.6449(16.1169) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 16.6254(16.8219) | Bit/dim 4.1101(4.1644) | Xent 1.3841(1.4365) | Loss 11.7869(12.3278) | Error 0.4933(0.5200) Steps 0(0.00) | Grad Norm 10.3588(15.7302) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 16.4847(16.8851) | Bit/dim 4.1591(4.1562) | Xent 1.4061(1.4359) | Loss 11.9169(12.2301) | Error 0.5311(0.5200) Steps 0(0.00) | Grad Norm 17.1149(16.4304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 87.5295, Epoch Time 1029.0330(911.1163), Bit/dim 4.1163(best: 4.2009), Xent 1.3334, Loss 4.7830, Error 0.4834(best: 0.5019)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 17.0024(16.8380) | Bit/dim 4.1453(4.1473) | Xent 1.3549(1.4182) | Loss 11.9869(12.7811) | Error 0.4822(0.5133) Steps 0(0.00) | Grad Norm 8.9238(15.4616) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 16.3286(16.7312) | Bit/dim 4.1271(4.1399) | Xent 1.3376(1.4119) | Loss 11.7844(12.5433) | Error 0.4944(0.5097) Steps 0(0.00) | Grad Norm 11.2306(16.2741) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 16.5801(16.7018) | Bit/dim 4.1005(4.1302) | Xent 1.3058(1.4010) | Loss 11.8892(12.3537) | Error 0.4756(0.5067) Steps 0(0.00) | Grad Norm 7.7457(15.2785) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 16.6849(16.8149) | Bit/dim 4.1653(4.1245) | Xent 1.3723(1.3984) | Loss 11.8456(12.2353) | Error 0.5011(0.5061) Steps 0(0.00) | Grad Norm 23.7873(15.7032) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.9204(16.7830) | Bit/dim 4.1142(4.1223) | Xent 1.3886(1.3929) | Loss 11.9416(12.1577) | Error 0.5111(0.5043) Steps 0(0.00) | Grad Norm 21.2877(16.6976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 85.5551, Epoch Time 1021.9860(914.4424), Bit/dim 4.0883(best: 4.1163), Xent 1.3244, Loss 4.7506, Error 0.4795(best: 0.4834)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 17.5168(16.7700) | Bit/dim 4.0910(4.1134) | Xent 1.4494(1.3884) | Loss 11.9331(12.8355) | Error 0.5178(0.5034) Steps 0(0.00) | Grad Norm 18.4895(15.9877) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 16.1147(16.7625) | Bit/dim 4.0810(4.1064) | Xent 1.3486(1.3828) | Loss 11.6955(12.5683) | Error 0.4867(0.5034) Steps 0(0.00) | Grad Norm 10.8194(15.5700) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 17.1164(16.7867) | Bit/dim 4.1124(4.1027) | Xent 1.3331(1.3719) | Loss 11.8497(12.3766) | Error 0.4733(0.4979) Steps 0(0.00) | Grad Norm 15.4796(14.6987) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 16.3431(16.7623) | Bit/dim 4.0675(4.0946) | Xent 1.3188(1.3712) | Loss 11.7277(12.2097) | Error 0.4844(0.4972) Steps 0(0.00) | Grad Norm 10.0177(14.7286) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 16.6531(16.7124) | Bit/dim 4.0938(4.0857) | Xent 1.4605(1.3658) | Loss 11.9453(12.0728) | Error 0.5156(0.4952) Steps 0(0.00) | Grad Norm 18.2918(13.8681) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.6437(16.6656) | Bit/dim 4.0703(4.0806) | Xent 1.3152(1.3609) | Loss 11.8597(11.9914) | Error 0.4622(0.4942) Steps 0(0.00) | Grad Norm 5.7557(13.8471) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 86.2683, Epoch Time 1022.8549(917.6948), Bit/dim 4.0587(best: 4.0883), Xent 1.2925, Loss 4.7049, Error 0.4698(best: 0.4795)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 16.6124(16.6456) | Bit/dim 4.0596(4.0739) | Xent 1.3758(1.3494) | Loss 11.6697(12.5396) | Error 0.4778(0.4896) Steps 0(0.00) | Grad Norm 15.7956(13.6121) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.6191(16.6427) | Bit/dim 4.0372(4.0656) | Xent 1.3371(1.3419) | Loss 11.6393(12.2951) | Error 0.4822(0.4867) Steps 0(0.00) | Grad Norm 7.8199(12.9334) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 15.9820(16.6620) | Bit/dim 4.0691(4.0665) | Xent 1.3665(1.3333) | Loss 11.6854(12.1376) | Error 0.5011(0.4831) Steps 0(0.00) | Grad Norm 12.1410(13.4130) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 16.6523(16.6861) | Bit/dim 4.0375(4.0590) | Xent 1.3474(1.3363) | Loss 11.6306(12.0101) | Error 0.4656(0.4830) Steps 0(0.00) | Grad Norm 11.5215(14.1544) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 16.4110(16.6101) | Bit/dim 4.0190(4.0557) | Xent 1.3227(1.3294) | Loss 11.5559(11.8998) | Error 0.4833(0.4825) Steps 0(0.00) | Grad Norm 4.8988(13.1451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 85.7096, Epoch Time 1019.2628(920.7418), Bit/dim 4.0249(best: 4.0587), Xent 1.2333, Loss 4.6415, Error 0.4446(best: 0.4698)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 16.8615(16.6265) | Bit/dim 4.0162(4.0454) | Xent 1.2373(1.3185) | Loss 11.4614(12.5471) | Error 0.4389(0.4789) Steps 0(0.00) | Grad Norm 20.4221(12.5155) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 17.1500(16.6498) | Bit/dim 4.0285(4.0449) | Xent 1.2791(1.3205) | Loss 11.5089(12.2955) | Error 0.4644(0.4786) Steps 0(0.00) | Grad Norm 10.7783(13.9766) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 16.5998(16.6356) | Bit/dim 4.0355(4.0425) | Xent 1.3891(1.3230) | Loss 11.7082(12.1163) | Error 0.4800(0.4785) Steps 0(0.00) | Grad Norm 16.3527(14.7326) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 16.0986(16.4992) | Bit/dim 4.0734(4.0392) | Xent 1.3140(1.3224) | Loss 11.6579(11.9715) | Error 0.4778(0.4785) Steps 0(0.00) | Grad Norm 19.6968(14.6242) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.7523(16.5608) | Bit/dim 3.9892(4.0368) | Xent 1.3699(1.3206) | Loss 11.5059(11.8579) | Error 0.5044(0.4774) Steps 0(0.00) | Grad Norm 20.1936(15.3882) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 16.0074(16.4443) | Bit/dim 4.0200(4.0296) | Xent 1.2977(1.3211) | Loss 11.4265(11.7636) | Error 0.4600(0.4768) Steps 0(0.00) | Grad Norm 16.3450(15.4927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 85.4227, Epoch Time 1008.8537(923.3852), Bit/dim 4.0101(best: 4.0249), Xent 1.2172, Loss 4.6187, Error 0.4429(best: 0.4446)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 17.1161(16.4876) | Bit/dim 4.0071(4.0236) | Xent 1.2275(1.3070) | Loss 11.5807(12.3166) | Error 0.4522(0.4716) Steps 0(0.00) | Grad Norm 7.4694(13.7239) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.7613(16.5182) | Bit/dim 4.0212(4.0162) | Xent 1.3071(1.2950) | Loss 11.6008(12.1046) | Error 0.4856(0.4674) Steps 0(0.00) | Grad Norm 5.4111(12.1277) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 16.4234(16.5604) | Bit/dim 3.9779(4.0104) | Xent 1.2605(1.2896) | Loss 11.3411(11.9248) | Error 0.4478(0.4654) Steps 0(0.00) | Grad Norm 17.7773(12.9070) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 16.6649(16.5632) | Bit/dim 3.9845(4.0073) | Xent 1.2812(1.2858) | Loss 11.4659(11.7927) | Error 0.4656(0.4652) Steps 0(0.00) | Grad Norm 9.5225(12.2992) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 16.4528(16.5830) | Bit/dim 4.0090(4.0034) | Xent 1.3653(1.2857) | Loss 11.5346(11.7031) | Error 0.5133(0.4656) Steps 0(0.00) | Grad Norm 9.4540(11.8124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 86.2022, Epoch Time 1019.6557(926.2733), Bit/dim 3.9873(best: 4.0101), Xent 1.2152, Loss 4.5949, Error 0.4394(best: 0.4429)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 15.9378(16.5869) | Bit/dim 3.9439(4.0003) | Xent 1.2035(1.2776) | Loss 11.2052(12.3618) | Error 0.4422(0.4623) Steps 0(0.00) | Grad Norm 12.6418(12.1812) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.3813(16.5706) | Bit/dim 3.9942(3.9964) | Xent 1.2243(1.2698) | Loss 11.3798(12.1084) | Error 0.4344(0.4582) Steps 0(0.00) | Grad Norm 11.6104(12.1391) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 16.9623(16.5526) | Bit/dim 4.0333(3.9948) | Xent 1.2388(1.2632) | Loss 11.4396(11.9218) | Error 0.4511(0.4563) Steps 0(0.00) | Grad Norm 20.9545(12.5759) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 15.8614(16.5681) | Bit/dim 3.9784(3.9926) | Xent 1.3344(1.2657) | Loss 11.4709(11.7902) | Error 0.4700(0.4581) Steps 0(0.00) | Grad Norm 9.4352(13.4644) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.7643(16.5366) | Bit/dim 3.9708(3.9877) | Xent 1.2756(1.2657) | Loss 11.5489(11.6776) | Error 0.4644(0.4582) Steps 0(0.00) | Grad Norm 16.1710(12.9526) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 16.1263(16.5087) | Bit/dim 3.9534(3.9820) | Xent 1.2621(1.2633) | Loss 11.1268(11.5830) | Error 0.4733(0.4571) Steps 0(0.00) | Grad Norm 9.9927(12.8188) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 85.8262, Epoch Time 1011.3097(928.8244), Bit/dim 3.9627(best: 3.9873), Xent 1.1954, Loss 4.5604, Error 0.4309(best: 0.4394)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 16.4701(16.5633) | Bit/dim 3.9672(3.9789) | Xent 1.2170(1.2587) | Loss 11.3246(12.1408) | Error 0.4467(0.4546) Steps 0(0.00) | Grad Norm 8.7520(12.8731) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 16.2712(16.5920) | Bit/dim 3.9416(3.9755) | Xent 1.3011(1.2507) | Loss 11.2573(11.9196) | Error 0.4722(0.4509) Steps 0(0.00) | Grad Norm 19.2618(12.7694) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.5949(16.5872) | Bit/dim 3.9657(3.9697) | Xent 1.2139(1.2482) | Loss 11.1814(11.7606) | Error 0.4456(0.4503) Steps 0(0.00) | Grad Norm 12.5583(13.1815) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 17.4582(16.5264) | Bit/dim 3.9537(3.9671) | Xent 1.2421(1.2473) | Loss 11.3718(11.6551) | Error 0.4522(0.4511) Steps 0(0.00) | Grad Norm 11.2359(13.3357) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.7642(16.3764) | Bit/dim 3.9774(3.9650) | Xent 1.2082(1.2442) | Loss 11.1601(11.5582) | Error 0.4222(0.4502) Steps 0(0.00) | Grad Norm 10.3958(12.5334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 85.1533, Epoch Time 1010.0156(931.2601), Bit/dim 3.9535(best: 3.9627), Xent 1.1561, Loss 4.5315, Error 0.4184(best: 0.4309)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.4368(16.3945) | Bit/dim 3.9665(3.9628) | Xent 1.2342(1.2385) | Loss 11.2933(12.1981) | Error 0.4678(0.4482) Steps 0(0.00) | Grad Norm 11.2199(12.2211) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 16.0391(16.3152) | Bit/dim 3.9621(3.9580) | Xent 1.2290(1.2314) | Loss 11.3016(11.9469) | Error 0.4389(0.4438) Steps 0(0.00) | Grad Norm 18.0075(12.4192) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.8692(16.2983) | Bit/dim 3.9633(3.9545) | Xent 1.1511(1.2246) | Loss 11.1969(11.7629) | Error 0.4367(0.4420) Steps 0(0.00) | Grad Norm 8.8522(12.3431) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.5641(16.2922) | Bit/dim 3.9183(3.9485) | Xent 1.2556(1.2156) | Loss 11.2853(11.6197) | Error 0.4600(0.4395) Steps 0(0.00) | Grad Norm 8.3467(12.0201) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.9447(16.2666) | Bit/dim 3.9516(3.9496) | Xent 1.2412(1.2162) | Loss 11.3493(11.5113) | Error 0.4544(0.4380) Steps 0(0.00) | Grad Norm 11.7264(12.4287) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 16.6158(16.1736) | Bit/dim 3.9442(3.9435) | Xent 1.2690(1.2227) | Loss 11.4333(11.4369) | Error 0.4400(0.4398) Steps 0(0.00) | Grad Norm 16.0676(12.8817) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 84.3645, Epoch Time 990.6812(933.0428), Bit/dim 3.9372(best: 3.9535), Xent 1.1854, Loss 4.5299, Error 0.4311(best: 0.4184)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 15.4205(16.1162) | Bit/dim 3.9618(3.9431) | Xent 1.1901(1.2166) | Loss 11.2281(11.9452) | Error 0.4211(0.4379) Steps 0(0.00) | Grad Norm 24.2797(13.3843) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 16.3499(16.0967) | Bit/dim 3.9484(3.9427) | Xent 1.2184(1.2063) | Loss 11.2624(11.7333) | Error 0.4344(0.4334) Steps 0(0.00) | Grad Norm 11.0689(13.2546) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.6913(16.0844) | Bit/dim 3.9169(3.9401) | Xent 1.1664(1.2028) | Loss 11.0660(11.5645) | Error 0.4111(0.4308) Steps 0(0.00) | Grad Norm 8.0933(13.1838) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.6248(16.1561) | Bit/dim 3.9340(3.9380) | Xent 1.1844(1.2055) | Loss 10.9569(11.4677) | Error 0.4289(0.4331) Steps 0(0.00) | Grad Norm 14.6969(13.7309) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 16.4552(16.1319) | Bit/dim 3.9487(3.9330) | Xent 1.1769(1.2005) | Loss 11.2244(11.3777) | Error 0.4167(0.4317) Steps 0(0.00) | Grad Norm 7.4337(13.2182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 83.8191, Epoch Time 985.4302(934.6144), Bit/dim 3.9202(best: 3.9372), Xent 1.1169, Loss 4.4786, Error 0.3996(best: 0.4184)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 16.2103(16.1009) | Bit/dim 3.9390(3.9288) | Xent 1.2554(1.1977) | Loss 11.3045(12.0272) | Error 0.4422(0.4305) Steps 0(0.00) | Grad Norm 27.3034(12.6410) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.2428(16.0693) | Bit/dim 3.9458(3.9300) | Xent 1.1728(1.1998) | Loss 11.1044(11.7980) | Error 0.4022(0.4300) Steps 0(0.00) | Grad Norm 15.4689(14.4860) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 15.7777(16.0748) | Bit/dim 3.9059(3.9256) | Xent 1.1385(1.1967) | Loss 11.0395(11.6321) | Error 0.3978(0.4312) Steps 0(0.00) | Grad Norm 14.2516(14.5580) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 16.5857(16.0772) | Bit/dim 3.8917(3.9224) | Xent 1.2169(1.1950) | Loss 11.1311(11.5088) | Error 0.4400(0.4282) Steps 0(0.00) | Grad Norm 22.8245(13.9862) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.5817(16.0179) | Bit/dim 3.9125(3.9194) | Xent 1.1719(1.1863) | Loss 11.0224(11.4010) | Error 0.4400(0.4254) Steps 0(0.00) | Grad Norm 19.6264(13.5526) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.1082(15.9833) | Bit/dim 3.9146(3.9180) | Xent 1.1491(1.1815) | Loss 10.9052(11.3134) | Error 0.4056(0.4232) Steps 0(0.00) | Grad Norm 10.9296(13.3142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 84.1705, Epoch Time 981.8808(936.0324), Bit/dim 3.9027(best: 3.9202), Xent 1.1010, Loss 4.4532, Error 0.3925(best: 0.3996)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 16.2384(16.0478) | Bit/dim 3.9621(3.9218) | Xent 1.1802(1.1785) | Loss 11.2469(11.8627) | Error 0.4133(0.4223) Steps 0(0.00) | Grad Norm 19.1847(13.8195) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 16.0026(16.0624) | Bit/dim 3.8899(3.9175) | Xent 1.1877(1.1724) | Loss 11.1246(11.6619) | Error 0.4244(0.4201) Steps 0(0.00) | Grad Norm 9.5751(13.0459) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 16.2616(16.0179) | Bit/dim 3.8790(3.9123) | Xent 1.2179(1.1740) | Loss 11.0971(11.5131) | Error 0.4200(0.4190) Steps 0(0.00) | Grad Norm 16.6667(13.3702) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 16.5434(16.0246) | Bit/dim 3.8650(3.9096) | Xent 1.0957(1.1642) | Loss 10.9671(11.3910) | Error 0.4056(0.4168) Steps 0(0.00) | Grad Norm 13.5270(13.0731) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.5748(16.0631) | Bit/dim 3.9044(3.9063) | Xent 1.1211(1.1548) | Loss 11.0018(11.3026) | Error 0.3811(0.4138) Steps 0(0.00) | Grad Norm 10.6732(11.9529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 82.0510, Epoch Time 985.7998(937.5254), Bit/dim 3.9038(best: 3.9027), Xent 1.1383, Loss 4.4729, Error 0.4036(best: 0.3925)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.7817(16.1157) | Bit/dim 3.9187(3.9064) | Xent 1.0656(1.1512) | Loss 10.9607(11.9209) | Error 0.3622(0.4113) Steps 0(0.00) | Grad Norm 19.3109(12.6588) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.7962(16.0856) | Bit/dim 3.8760(3.9046) | Xent 1.1316(1.1472) | Loss 11.0768(11.6926) | Error 0.3978(0.4110) Steps 0(0.00) | Grad Norm 13.4637(12.3684) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 16.2896(16.0555) | Bit/dim 3.8894(3.9015) | Xent 1.2004(1.1537) | Loss 10.9404(11.5233) | Error 0.4178(0.4127) Steps 0(0.00) | Grad Norm 24.9603(13.4631) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 15.9597(16.0574) | Bit/dim 3.9122(3.9001) | Xent 1.2038(1.1584) | Loss 11.0738(11.4031) | Error 0.4400(0.4151) Steps 0(0.00) | Grad Norm 12.4658(14.1212) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.2595(16.0690) | Bit/dim 3.8882(3.8973) | Xent 1.1711(1.1529) | Loss 11.0531(11.3009) | Error 0.4400(0.4144) Steps 0(0.00) | Grad Norm 18.1557(13.4429) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 16.3791(15.9989) | Bit/dim 3.8762(3.8957) | Xent 1.1435(1.1445) | Loss 11.0963(11.2212) | Error 0.4144(0.4129) Steps 0(0.00) | Grad Norm 8.9084(12.8455) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 81.8173, Epoch Time 980.6212(938.8183), Bit/dim 3.8828(best: 3.9027), Xent 1.0811, Loss 4.4233, Error 0.3885(best: 0.3925)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.5172(15.9548) | Bit/dim 3.8950(3.8936) | Xent 1.1725(1.1375) | Loss 11.1814(11.7279) | Error 0.4200(0.4098) Steps 0(0.00) | Grad Norm 14.9997(13.0690) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.6848(15.9582) | Bit/dim 3.8478(3.8879) | Xent 1.0625(1.1276) | Loss 10.7889(11.5098) | Error 0.3889(0.4050) Steps 0(0.00) | Grad Norm 11.5697(12.5569) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 15.9937(15.9389) | Bit/dim 3.8815(3.8869) | Xent 1.1114(1.1206) | Loss 10.8502(11.3584) | Error 0.4056(0.4021) Steps 0(0.00) | Grad Norm 10.8898(12.2556) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.8016(15.9292) | Bit/dim 3.8830(3.8837) | Xent 1.0729(1.1171) | Loss 10.8442(11.2475) | Error 0.3611(0.4011) Steps 0(0.00) | Grad Norm 15.0627(12.6169) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 15.7027(15.9193) | Bit/dim 3.8841(3.8848) | Xent 1.2667(1.1252) | Loss 11.0846(11.1831) | Error 0.4533(0.4029) Steps 0(0.00) | Grad Norm 17.6411(12.9247) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 82.3789, Epoch Time 973.7497(939.8662), Bit/dim 3.8683(best: 3.8828), Xent 1.0499, Loss 4.3933, Error 0.3742(best: 0.3885)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 15.0529(15.9066) | Bit/dim 3.8563(3.8835) | Xent 1.0818(1.1183) | Loss 10.9116(11.8079) | Error 0.3889(0.3998) Steps 0(0.00) | Grad Norm 9.2708(11.8666) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.8451(15.9019) | Bit/dim 3.8664(3.8825) | Xent 1.1738(1.1088) | Loss 10.9281(11.5645) | Error 0.3933(0.3973) Steps 0(0.00) | Grad Norm 15.1317(12.1963) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.9102(16.0194) | Bit/dim 3.8897(3.8828) | Xent 1.1292(1.1108) | Loss 11.0718(11.4043) | Error 0.4011(0.3975) Steps 0(0.00) | Grad Norm 24.1086(13.3407) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 16.4148(16.0326) | Bit/dim 3.8677(3.8827) | Xent 1.1134(1.1152) | Loss 11.0064(11.3091) | Error 0.4022(0.3993) Steps 0(0.00) | Grad Norm 11.0477(12.8064) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 17.0252(16.1075) | Bit/dim 3.8288(3.8788) | Xent 1.1053(1.0987) | Loss 10.8913(11.2090) | Error 0.3956(0.3943) Steps 0(0.00) | Grad Norm 6.2568(12.3735) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 16.3303(16.1329) | Bit/dim 3.8359(3.8744) | Xent 1.0875(1.0998) | Loss 10.9600(11.1421) | Error 0.3889(0.3932) Steps 0(0.00) | Grad Norm 9.9792(12.0973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 82.3117, Epoch Time 988.2129(941.3166), Bit/dim 3.8622(best: 3.8683), Xent 1.0360, Loss 4.3802, Error 0.3723(best: 0.3742)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.8044(16.1171) | Bit/dim 3.8725(3.8711) | Xent 1.0927(1.0925) | Loss 10.9775(11.6923) | Error 0.3978(0.3920) Steps 0(0.00) | Grad Norm 17.2739(11.9789) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 16.0334(16.0716) | Bit/dim 3.8748(3.8678) | Xent 1.0283(1.0857) | Loss 10.9764(11.4831) | Error 0.3656(0.3897) Steps 0(0.00) | Grad Norm 6.7644(11.7441) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 16.2649(16.0328) | Bit/dim 3.9148(3.8689) | Xent 1.0539(1.0792) | Loss 10.9638(11.3270) | Error 0.3767(0.3868) Steps 0(0.00) | Grad Norm 8.9977(11.2450) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 16.3561(16.0334) | Bit/dim 3.8435(3.8668) | Xent 1.1073(1.0757) | Loss 10.8387(11.2047) | Error 0.4011(0.3857) Steps 0(0.00) | Grad Norm 8.1922(11.5863) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 16.5580(15.9997) | Bit/dim 3.8938(3.8682) | Xent 1.1432(1.0815) | Loss 11.0939(11.1396) | Error 0.4111(0.3884) Steps 0(0.00) | Grad Norm 22.5167(11.6886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 83.4724, Epoch Time 978.7837(942.4406), Bit/dim 3.8622(best: 3.8622), Xent 1.0330, Loss 4.3787, Error 0.3691(best: 0.3723)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 16.2230(15.9819) | Bit/dim 3.8593(3.8683) | Xent 1.1336(1.0821) | Loss 11.0033(11.8089) | Error 0.3833(0.3872) Steps 0(0.00) | Grad Norm 16.5381(12.2508) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 16.1994(16.0105) | Bit/dim 3.8761(3.8649) | Xent 1.0727(1.0781) | Loss 11.1226(11.5706) | Error 0.3678(0.3844) Steps 0(0.00) | Grad Norm 15.3187(12.5127) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 17.1203(16.0190) | Bit/dim 3.8759(3.8658) | Xent 1.0798(1.0708) | Loss 11.0623(11.3963) | Error 0.3833(0.3823) Steps 0(0.00) | Grad Norm 9.8666(12.5397) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 16.1915(16.0308) | Bit/dim 3.8456(3.8623) | Xent 1.0110(1.0672) | Loss 10.7665(11.2564) | Error 0.3578(0.3799) Steps 0(0.00) | Grad Norm 8.2384(12.0916) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 15.9990(16.0167) | Bit/dim 3.8651(3.8598) | Xent 1.0765(1.0743) | Loss 10.7875(11.1470) | Error 0.3856(0.3827) Steps 0(0.00) | Grad Norm 12.9526(12.1456) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 16.0908(16.0941) | Bit/dim 3.8567(3.8554) | Xent 1.1054(1.0720) | Loss 10.9157(11.0767) | Error 0.4033(0.3812) Steps 0(0.00) | Grad Norm 9.9778(11.6310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 82.7163, Epoch Time 985.6333(943.7364), Bit/dim 3.8510(best: 3.8622), Xent 1.0046, Loss 4.3533, Error 0.3570(best: 0.3691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 16.2280(16.0354) | Bit/dim 3.8629(3.8567) | Xent 1.0361(1.0618) | Loss 10.9189(11.6323) | Error 0.3778(0.3776) Steps 0(0.00) | Grad Norm 8.5352(11.3261) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 16.2699(16.0659) | Bit/dim 3.8681(3.8536) | Xent 1.0649(1.0553) | Loss 10.9284(11.4220) | Error 0.3789(0.3743) Steps 0(0.00) | Grad Norm 8.2886(11.6829) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 15.9691(16.1431) | Bit/dim 3.8371(3.8506) | Xent 1.0129(1.0497) | Loss 10.7410(11.2604) | Error 0.3467(0.3719) Steps 0(0.00) | Grad Norm 8.0232(11.2157) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 16.0552(16.1957) | Bit/dim 3.8271(3.8460) | Xent 1.0422(1.0531) | Loss 10.7261(11.1519) | Error 0.3722(0.3734) Steps 0(0.00) | Grad Norm 11.3644(11.5704) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.5219(16.1166) | Bit/dim 3.8354(3.8442) | Xent 1.0329(1.0510) | Loss 10.7046(11.0602) | Error 0.3756(0.3745) Steps 0(0.00) | Grad Norm 9.1727(12.0698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 83.4962, Epoch Time 987.0599(945.0361), Bit/dim 3.8410(best: 3.8510), Xent 0.9937, Loss 4.3378, Error 0.3546(best: 0.3570)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.8032(16.0378) | Bit/dim 3.8319(3.8438) | Xent 1.0528(1.0468) | Loss 10.8310(11.6817) | Error 0.3789(0.3743) Steps 0(0.00) | Grad Norm 13.1043(11.9323) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.8585(16.0085) | Bit/dim 3.8645(3.8478) | Xent 1.0690(1.0532) | Loss 10.7556(11.4691) | Error 0.3911(0.3762) Steps 0(0.00) | Grad Norm 8.7998(13.0007) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 15.8960(16.0063) | Bit/dim 3.8633(3.8464) | Xent 1.0277(1.0526) | Loss 10.9241(11.3094) | Error 0.3611(0.3759) Steps 0(0.00) | Grad Norm 27.6929(13.5404) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.7649(16.0258) | Bit/dim 3.8485(3.8443) | Xent 0.9734(1.0429) | Loss 10.7211(11.1801) | Error 0.3378(0.3729) Steps 0(0.00) | Grad Norm 7.5202(12.2598) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.7977(15.9843) | Bit/dim 3.8396(3.8425) | Xent 1.0376(1.0375) | Loss 10.9516(11.0818) | Error 0.3733(0.3706) Steps 0(0.00) | Grad Norm 12.3416(12.4438) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 16.3669(16.0020) | Bit/dim 3.8029(3.8375) | Xent 1.1443(1.0458) | Loss 10.9736(11.0159) | Error 0.4067(0.3730) Steps 0(0.00) | Grad Norm 17.4031(12.8607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 84.6359, Epoch Time 980.5633(946.1019), Bit/dim 3.8546(best: 3.8410), Xent 1.0142, Loss 4.3617, Error 0.3618(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.2878(15.9979) | Bit/dim 3.8229(3.8403) | Xent 1.0200(1.0513) | Loss 10.7437(11.5944) | Error 0.3744(0.3747) Steps 0(0.00) | Grad Norm 9.8637(13.4411) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.8651(16.0042) | Bit/dim 3.8378(3.8387) | Xent 1.0678(1.0405) | Loss 10.9055(11.3703) | Error 0.3844(0.3722) Steps 0(0.00) | Grad Norm 16.7559(12.6636) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 16.2693(16.0045) | Bit/dim 3.8424(3.8372) | Xent 0.9798(1.0434) | Loss 10.7819(11.2322) | Error 0.3489(0.3729) Steps 0(0.00) | Grad Norm 7.7452(12.2560) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.6552(15.9419) | Bit/dim 3.7951(3.8353) | Xent 1.0298(1.0378) | Loss 10.5691(11.1038) | Error 0.3589(0.3711) Steps 0(0.00) | Grad Norm 11.5603(12.3765) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.6588(15.9576) | Bit/dim 3.8337(3.8322) | Xent 1.0982(1.0379) | Loss 10.9326(11.0271) | Error 0.3878(0.3704) Steps 0(0.00) | Grad Norm 20.2635(12.8050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 83.9269, Epoch Time 980.1998(947.1249), Bit/dim 3.8251(best: 3.8410), Xent 1.0186, Loss 4.3344, Error 0.3661(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.8326(15.9459) | Bit/dim 3.8493(3.8317) | Xent 0.9905(1.0299) | Loss 10.8141(11.6670) | Error 0.3556(0.3674) Steps 0(0.00) | Grad Norm 11.8965(12.4938) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.6311(15.9784) | Bit/dim 3.8441(3.8295) | Xent 0.9973(1.0195) | Loss 10.8376(11.4151) | Error 0.3422(0.3623) Steps 0(0.00) | Grad Norm 8.6693(11.4849) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 16.0483(15.9541) | Bit/dim 3.8416(3.8251) | Xent 1.0537(1.0205) | Loss 10.8946(11.2384) | Error 0.3789(0.3633) Steps 0(0.00) | Grad Norm 17.0551(12.0096) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 16.8361(15.9791) | Bit/dim 3.8308(3.8239) | Xent 1.0307(1.0171) | Loss 10.8760(11.1032) | Error 0.3600(0.3622) Steps 0(0.00) | Grad Norm 13.1198(12.5966) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.8294(15.9794) | Bit/dim 3.8097(3.8231) | Xent 1.0799(1.0308) | Loss 10.8295(11.0270) | Error 0.3822(0.3665) Steps 0(0.00) | Grad Norm 12.9102(12.6175) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.5759(15.9494) | Bit/dim 3.8445(3.8254) | Xent 1.0245(1.0285) | Loss 10.7873(10.9638) | Error 0.3889(0.3677) Steps 0(0.00) | Grad Norm 11.8924(12.0735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 83.3894, Epoch Time 978.9404(948.0793), Bit/dim 3.8236(best: 3.8251), Xent 0.9915, Loss 4.3193, Error 0.3558(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.2683(16.0003) | Bit/dim 3.8118(3.8208) | Xent 1.0049(1.0190) | Loss 10.6388(11.4980) | Error 0.3400(0.3638) Steps 0(0.00) | Grad Norm 6.2337(10.9163) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 16.7835(15.9894) | Bit/dim 3.8150(3.8213) | Xent 1.0106(1.0049) | Loss 10.6304(11.2830) | Error 0.3489(0.3575) Steps 0(0.00) | Grad Norm 8.7005(10.0407) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.8061(15.9723) | Bit/dim 3.7857(3.8202) | Xent 0.9673(0.9979) | Loss 10.7222(11.1380) | Error 0.3411(0.3563) Steps 0(0.00) | Grad Norm 5.2420(10.4397) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 15.7839(15.9565) | Bit/dim 3.8270(3.8209) | Xent 0.9947(1.0055) | Loss 10.6780(11.0421) | Error 0.3478(0.3579) Steps 0(0.00) | Grad Norm 13.4554(11.1331) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.2457(15.9434) | Bit/dim 3.8379(3.8176) | Xent 1.0660(1.0098) | Loss 10.7847(10.9491) | Error 0.3767(0.3585) Steps 0(0.00) | Grad Norm 26.5280(12.5371) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 82.8947, Epoch Time 978.6050(948.9951), Bit/dim 3.8275(best: 3.8236), Xent 0.9842, Loss 4.3196, Error 0.3529(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.9278(15.9486) | Bit/dim 3.8005(3.8199) | Xent 0.9796(1.0111) | Loss 10.6119(11.6169) | Error 0.3511(0.3594) Steps 0(0.00) | Grad Norm 11.8490(12.7787) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 16.9618(16.0217) | Bit/dim 3.8117(3.8169) | Xent 0.9989(1.0026) | Loss 10.7898(11.3746) | Error 0.3711(0.3569) Steps 0(0.00) | Grad Norm 10.3102(12.3000) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 16.6977(16.0777) | Bit/dim 3.8330(3.8159) | Xent 0.9592(0.9958) | Loss 10.8108(11.1974) | Error 0.3378(0.3548) Steps 0(0.00) | Grad Norm 11.4303(11.4838) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 16.0936(16.1107) | Bit/dim 3.8302(3.8179) | Xent 1.0418(0.9940) | Loss 10.8341(11.0782) | Error 0.3678(0.3547) Steps 0(0.00) | Grad Norm 15.5184(11.5771) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.6988(16.0870) | Bit/dim 3.7997(3.8123) | Xent 0.9694(0.9875) | Loss 10.6783(10.9706) | Error 0.3389(0.3526) Steps 0(0.00) | Grad Norm 8.1010(11.0291) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 16.4459(16.0994) | Bit/dim 3.8207(3.8125) | Xent 1.0555(0.9920) | Loss 10.7873(10.9169) | Error 0.3700(0.3541) Steps 0(0.00) | Grad Norm 7.6807(11.4218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 84.1325, Epoch Time 990.0933(950.2281), Bit/dim 3.8113(best: 3.8236), Xent 0.9594, Loss 4.2910, Error 0.3399(best: 0.3529)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.5400(16.0003) | Bit/dim 3.8109(3.8141) | Xent 0.9621(0.9864) | Loss 10.6467(11.4597) | Error 0.3500(0.3521) Steps 0(0.00) | Grad Norm 14.0146(11.0674) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.7492(15.9567) | Bit/dim 3.8390(3.8141) | Xent 0.9920(0.9841) | Loss 10.8130(11.2633) | Error 0.3500(0.3509) Steps 0(0.00) | Grad Norm 12.4409(11.4681) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.5846(15.9719) | Bit/dim 3.8288(3.8121) | Xent 1.0658(0.9916) | Loss 10.7811(11.1334) | Error 0.3822(0.3543) Steps 0(0.00) | Grad Norm 20.3434(12.2145) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 16.3090(16.0159) | Bit/dim 3.8307(3.8078) | Xent 1.0713(0.9963) | Loss 10.8289(11.0133) | Error 0.3967(0.3567) Steps 0(0.00) | Grad Norm 13.4848(11.9935) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.2335(15.9547) | Bit/dim 3.7907(3.8074) | Xent 1.0587(0.9953) | Loss 10.7602(10.9190) | Error 0.3889(0.3553) Steps 0(0.00) | Grad Norm 11.6120(11.5530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 82.7833, Epoch Time 977.0193(951.0318), Bit/dim 3.8002(best: 3.8113), Xent 0.9615, Loss 4.2809, Error 0.3431(best: 0.3399)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 16.4200(16.0654) | Bit/dim 3.7915(3.8056) | Xent 0.9933(0.9911) | Loss 10.7561(11.6006) | Error 0.3600(0.3532) Steps 0(0.00) | Grad Norm 15.0225(11.4616) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.9438(15.9721) | Bit/dim 3.8212(3.8043) | Xent 0.9619(0.9842) | Loss 10.7561(11.3527) | Error 0.3467(0.3518) Steps 0(0.00) | Grad Norm 10.5260(11.2859) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 16.0797(15.9688) | Bit/dim 3.7974(3.8047) | Xent 0.9691(0.9764) | Loss 10.6268(11.1763) | Error 0.3356(0.3496) Steps 0(0.00) | Grad Norm 7.8555(10.8352) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 16.1914(15.9772) | Bit/dim 3.7824(3.8019) | Xent 0.9920(0.9701) | Loss 10.5723(11.0309) | Error 0.3733(0.3464) Steps 0(0.00) | Grad Norm 16.4502(10.2063) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.9487(15.9974) | Bit/dim 3.7249(3.8006) | Xent 1.0113(0.9754) | Loss 10.4928(10.9363) | Error 0.3789(0.3476) Steps 0(0.00) | Grad Norm 7.7477(11.3194) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 15.4945(15.9124) | Bit/dim 3.8090(3.8028) | Xent 1.0498(0.9813) | Loss 10.7047(10.8749) | Error 0.3789(0.3489) Steps 0(0.00) | Grad Norm 17.6866(11.6972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 84.3617, Epoch Time 978.4432(951.8541), Bit/dim 3.8018(best: 3.8002), Xent 1.0020, Loss 4.3028, Error 0.3580(best: 0.3399)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.7840(15.9380) | Bit/dim 3.8124(3.8014) | Xent 0.9437(0.9811) | Loss 10.5353(11.4307) | Error 0.3344(0.3507) Steps 0(0.00) | Grad Norm 10.7389(11.7431) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 16.2397(16.0177) | Bit/dim 3.7727(3.8001) | Xent 0.8741(0.9703) | Loss 10.5011(11.2161) | Error 0.3022(0.3473) Steps 0(0.00) | Grad Norm 8.3694(10.9815) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.7341(16.0388) | Bit/dim 3.8020(3.7990) | Xent 0.9248(0.9646) | Loss 10.5853(11.0628) | Error 0.3289(0.3460) Steps 0(0.00) | Grad Norm 10.9038(11.0505) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 16.4529(16.1118) | Bit/dim 3.7993(3.7979) | Xent 0.8823(0.9637) | Loss 10.7228(10.9590) | Error 0.3233(0.3454) Steps 0(0.00) | Grad Norm 11.9258(11.0016) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 16.9291(16.1361) | Bit/dim 3.7715(3.7963) | Xent 0.8400(0.9584) | Loss 10.5046(10.8601) | Error 0.2989(0.3435) Steps 0(0.00) | Grad Norm 5.8370(11.2332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 83.3736, Epoch Time 990.9527(953.0271), Bit/dim 3.7924(best: 3.8002), Xent 0.9311, Loss 4.2579, Error 0.3299(best: 0.3399)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.9880(16.1181) | Bit/dim 3.7804(3.7945) | Xent 0.9805(0.9534) | Loss 10.5386(11.5118) | Error 0.3678(0.3430) Steps 0(0.00) | Grad Norm 17.0741(10.9667) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.4473(16.0413) | Bit/dim 3.8239(3.7937) | Xent 0.9688(0.9534) | Loss 10.6214(11.2694) | Error 0.3378(0.3417) Steps 0(0.00) | Grad Norm 13.8062(11.0590) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.7139(16.0566) | Bit/dim 3.7694(3.7917) | Xent 0.9262(0.9520) | Loss 10.4664(11.1030) | Error 0.3267(0.3407) Steps 0(0.00) | Grad Norm 6.4697(11.5571) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 16.4258(16.0322) | Bit/dim 3.8166(3.7909) | Xent 0.9327(0.9505) | Loss 10.6410(10.9756) | Error 0.3256(0.3396) Steps 0(0.00) | Grad Norm 5.7741(11.2205) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 16.6862(16.0840) | Bit/dim 3.7846(3.7925) | Xent 1.0267(0.9559) | Loss 10.7195(10.8905) | Error 0.3544(0.3402) Steps 0(0.00) | Grad Norm 9.1927(11.4441) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 16.0670(16.0735) | Bit/dim 3.7618(3.7910) | Xent 0.9219(0.9527) | Loss 10.5062(10.8082) | Error 0.3289(0.3392) Steps 0(0.00) | Grad Norm 7.3798(11.2448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 84.2922, Epoch Time 983.6287(953.9451), Bit/dim 3.7908(best: 3.7924), Xent 0.9213, Loss 4.2514, Error 0.3287(best: 0.3299)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 16.2395(16.0776) | Bit/dim 3.7504(3.7887) | Xent 0.8745(0.9411) | Loss 10.5037(11.3886) | Error 0.2956(0.3343) Steps 0(0.00) | Grad Norm 10.5821(11.3578) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 16.3232(16.0722) | Bit/dim 3.7838(3.7886) | Xent 0.9077(0.9371) | Loss 10.5630(11.1869) | Error 0.3089(0.3321) Steps 0(0.00) | Grad Norm 9.5056(10.8077) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 16.1571(16.0740) | Bit/dim 3.7932(3.7856) | Xent 0.8968(0.9348) | Loss 10.6648(11.0345) | Error 0.3111(0.3320) Steps 0(0.00) | Grad Norm 11.8701(10.8892) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 16.1346(16.0972) | Bit/dim 3.7694(3.7872) | Xent 0.8900(0.9284) | Loss 10.5559(10.9329) | Error 0.3189(0.3315) Steps 0(0.00) | Grad Norm 11.6820(10.6630) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 17.0217(16.2186) | Bit/dim 3.8309(3.7874) | Xent 0.9870(0.9377) | Loss 10.7736(10.8688) | Error 0.3267(0.3340) Steps 0(0.00) | Grad Norm 11.2718(11.2440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 83.5762, Epoch Time 991.9415(955.0850), Bit/dim 3.7892(best: 3.7908), Xent 0.9018, Loss 4.2401, Error 0.3200(best: 0.3287)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.3782(16.1141) | Bit/dim 3.8083(3.7848) | Xent 0.9155(0.9340) | Loss 10.6593(11.5099) | Error 0.3267(0.3328) Steps 0(0.00) | Grad Norm 13.0406(10.9497) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 16.3968(16.1024) | Bit/dim 3.7851(3.7851) | Xent 0.9048(0.9303) | Loss 10.6363(11.2764) | Error 0.3256(0.3306) Steps 0(0.00) | Grad Norm 8.1754(10.7726) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 16.2606(16.0900) | Bit/dim 3.7758(3.7827) | Xent 0.9745(0.9283) | Loss 10.7112(11.0973) | Error 0.3400(0.3305) Steps 0(0.00) | Grad Norm 12.4009(11.2036) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.8867(16.0292) | Bit/dim 3.8066(3.7876) | Xent 0.9893(0.9534) | Loss 10.6458(11.0073) | Error 0.3567(0.3381) Steps 0(0.00) | Grad Norm 11.2977(12.7301) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.7698(16.0352) | Bit/dim 3.7496(3.7864) | Xent 0.9228(0.9530) | Loss 10.5247(10.9081) | Error 0.3333(0.3387) Steps 0(0.00) | Grad Norm 7.9715(12.6660) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.0632(15.9806) | Bit/dim 3.7586(3.7842) | Xent 0.9186(0.9474) | Loss 10.5817(10.8286) | Error 0.3411(0.3364) Steps 0(0.00) | Grad Norm 9.6359(11.8854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 84.5723, Epoch Time 978.3712(955.7836), Bit/dim 3.7844(best: 3.7892), Xent 0.8877, Loss 4.2283, Error 0.3187(best: 0.3200)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.7731(16.0027) | Bit/dim 3.7690(3.7834) | Xent 0.8913(0.9302) | Loss 10.5013(11.3849) | Error 0.3244(0.3300) Steps 0(0.00) | Grad Norm 6.8393(11.3173) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 16.0209(16.0251) | Bit/dim 3.7860(3.7787) | Xent 0.9183(0.9304) | Loss 10.5700(11.1739) | Error 0.3122(0.3300) Steps 0(0.00) | Grad Norm 9.8169(11.6076) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.6061(16.0519) | Bit/dim 3.7888(3.7787) | Xent 0.9151(0.9320) | Loss 10.5475(11.0154) | Error 0.3211(0.3307) Steps 0(0.00) | Grad Norm 8.7840(11.2471) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 16.2757(16.0672) | Bit/dim 3.7667(3.7797) | Xent 0.9209(0.9322) | Loss 10.6119(10.9078) | Error 0.3389(0.3317) Steps 0(0.00) | Grad Norm 8.2311(11.0101) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.2354(16.0492) | Bit/dim 3.7623(3.7793) | Xent 0.8803(0.9317) | Loss 10.6255(10.8310) | Error 0.3067(0.3310) Steps 0(0.00) | Grad Norm 8.0022(11.0973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 84.9003, Epoch Time 987.0865(956.7227), Bit/dim 3.7776(best: 3.7844), Xent 0.8798, Loss 4.2175, Error 0.3094(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.5691(16.0823) | Bit/dim 3.7577(3.7786) | Xent 0.8838(0.9234) | Loss 10.4370(11.4950) | Error 0.3167(0.3284) Steps 0(0.00) | Grad Norm 8.4942(10.9084) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.5318(16.0535) | Bit/dim 3.7690(3.7772) | Xent 0.8664(0.9196) | Loss 10.3828(11.2446) | Error 0.3111(0.3278) Steps 0(0.00) | Grad Norm 13.6314(10.6397) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.6327(15.9608) | Bit/dim 3.7560(3.7759) | Xent 0.9147(0.9172) | Loss 10.3746(11.0578) | Error 0.3189(0.3266) Steps 0(0.00) | Grad Norm 11.9786(10.8422) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 16.0667(15.9636) | Bit/dim 3.7842(3.7779) | Xent 0.8928(0.9147) | Loss 10.6519(10.9445) | Error 0.3233(0.3258) Steps 0(0.00) | Grad Norm 7.0140(10.6728) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 16.5692(15.9786) | Bit/dim 3.7644(3.7780) | Xent 0.9505(0.9272) | Loss 10.5770(10.8640) | Error 0.3422(0.3304) Steps 0(0.00) | Grad Norm 15.8789(12.2796) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 16.9106(16.0072) | Bit/dim 3.7634(3.7746) | Xent 0.8550(0.9299) | Loss 10.4570(10.7884) | Error 0.3067(0.3309) Steps 0(0.00) | Grad Norm 10.7482(12.5147) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 84.7661, Epoch Time 981.1802(957.4564), Bit/dim 3.7715(best: 3.7776), Xent 0.9191, Loss 4.2311, Error 0.3294(best: 0.3094)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 16.2097(16.0030) | Bit/dim 3.7631(3.7736) | Xent 0.9237(0.9247) | Loss 10.5096(11.3429) | Error 0.3311(0.3287) Steps 0(0.00) | Grad Norm 8.8590(12.1255) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 16.3689(16.0423) | Bit/dim 3.7759(3.7737) | Xent 0.8995(0.9180) | Loss 10.3731(11.1277) | Error 0.3278(0.3261) Steps 0(0.00) | Grad Norm 6.9137(10.9916) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 16.0366(16.0616) | Bit/dim 3.7434(3.7711) | Xent 0.9213(0.9144) | Loss 10.4685(10.9614) | Error 0.3322(0.3251) Steps 0(0.00) | Grad Norm 6.3539(10.4877) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 16.0943(16.0644) | Bit/dim 3.7636(3.7698) | Xent 0.8655(0.9057) | Loss 10.4482(10.8485) | Error 0.2922(0.3223) Steps 0(0.00) | Grad Norm 19.3964(10.8004) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.9999(16.0431) | Bit/dim 3.7844(3.7707) | Xent 0.9643(0.9198) | Loss 10.6530(10.7897) | Error 0.3611(0.3285) Steps 0(0.00) | Grad Norm 14.9600(11.7585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 85.6154, Epoch Time 987.2035(958.3488), Bit/dim 3.7717(best: 3.7715), Xent 0.9109, Loss 4.2271, Error 0.3198(best: 0.3094)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.6308(16.0080) | Bit/dim 3.7425(3.7696) | Xent 0.9081(0.9166) | Loss 10.3710(11.4640) | Error 0.3356(0.3270) Steps 0(0.00) | Grad Norm 10.5447(11.6228) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.9825(16.0458) | Bit/dim 3.7284(3.7687) | Xent 0.7933(0.9102) | Loss 10.3836(11.2225) | Error 0.2833(0.3254) Steps 0(0.00) | Grad Norm 7.6203(11.2164) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.4710(15.9833) | Bit/dim 3.7310(3.7688) | Xent 0.7908(0.8994) | Loss 10.4791(11.0353) | Error 0.2811(0.3210) Steps 0(0.00) | Grad Norm 11.0651(10.7519) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 16.2870(16.0159) | Bit/dim 3.7609(3.7683) | Xent 0.8920(0.8968) | Loss 10.5284(10.9001) | Error 0.3144(0.3197) Steps 0(0.00) | Grad Norm 10.3731(10.6027) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 16.6729(16.0020) | Bit/dim 3.7776(3.7684) | Xent 0.9160(0.9057) | Loss 10.6656(10.8300) | Error 0.3267(0.3215) Steps 0(0.00) | Grad Norm 6.7030(11.5325) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.9303(15.9800) | Bit/dim 3.7401(3.7676) | Xent 0.9236(0.9059) | Loss 10.4640(10.7572) | Error 0.3267(0.3220) Steps 0(0.00) | Grad Norm 6.0856(11.1829) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 83.5882, Epoch Time 979.2739(958.9766), Bit/dim 3.7677(best: 3.7715), Xent 0.9246, Loss 4.2301, Error 0.3298(best: 0.3094)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.8638(15.9577) | Bit/dim 3.7949(3.7669) | Xent 0.9215(0.9076) | Loss 10.7083(11.3290) | Error 0.3300(0.3238) Steps 0(0.00) | Grad Norm 10.3358(11.0703) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.7924(15.9862) | Bit/dim 3.7691(3.7671) | Xent 0.8882(0.9028) | Loss 10.4471(11.1165) | Error 0.3056(0.3225) Steps 0(0.00) | Grad Norm 12.3250(10.9205) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.9342(15.9968) | Bit/dim 3.8007(3.7657) | Xent 0.9269(0.9015) | Loss 10.6382(10.9609) | Error 0.3300(0.3224) Steps 0(0.00) | Grad Norm 16.7710(10.9708) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 16.3180(16.0816) | Bit/dim 3.7767(3.7652) | Xent 0.8857(0.8971) | Loss 10.5831(10.8513) | Error 0.3033(0.3204) Steps 0(0.00) | Grad Norm 12.3340(10.7509) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 16.7380(16.0729) | Bit/dim 3.7621(3.7608) | Xent 0.9424(0.8902) | Loss 10.6757(10.7549) | Error 0.3411(0.3175) Steps 0(0.00) | Grad Norm 11.8577(10.4317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 85.3279, Epoch Time 986.7671(959.8103), Bit/dim 3.7633(best: 3.7677), Xent 0.8508, Loss 4.1887, Error 0.3020(best: 0.3094)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.1486(16.0181) | Bit/dim 3.7541(3.7637) | Xent 0.8287(0.8789) | Loss 10.4169(11.4175) | Error 0.3033(0.3139) Steps 0(0.00) | Grad Norm 12.7266(10.0748) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 16.5636(16.0260) | Bit/dim 3.7609(3.7635) | Xent 0.9139(0.8758) | Loss 10.6337(11.1858) | Error 0.3111(0.3118) Steps 0(0.00) | Grad Norm 8.4264(10.1807) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 16.7022(16.0645) | Bit/dim 3.7321(3.7589) | Xent 0.9143(0.8764) | Loss 10.4643(11.0072) | Error 0.3244(0.3110) Steps 0(0.00) | Grad Norm 8.7556(10.6930) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.9882(16.0630) | Bit/dim 3.7643(3.7601) | Xent 0.9071(0.8819) | Loss 10.5509(10.8792) | Error 0.3100(0.3123) Steps 0(0.00) | Grad Norm 10.4120(10.8434) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 16.0097(16.0722) | Bit/dim 3.7576(3.7585) | Xent 0.8648(0.8790) | Loss 10.3946(10.7765) | Error 0.3278(0.3120) Steps 0(0.00) | Grad Norm 6.6984(10.2412) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 16.8080(16.1317) | Bit/dim 3.7481(3.7548) | Xent 0.8213(0.8657) | Loss 10.4219(10.6861) | Error 0.2956(0.3079) Steps 0(0.00) | Grad Norm 8.6528(9.4412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 86.9480, Epoch Time 989.8938(960.7128), Bit/dim 3.7543(best: 3.7633), Xent 0.8490, Loss 4.1788, Error 0.2998(best: 0.3020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 16.2873(16.1836) | Bit/dim 3.7498(3.7579) | Xent 0.7837(0.8541) | Loss 10.2595(11.2605) | Error 0.2856(0.3048) Steps 0(0.00) | Grad Norm 6.7565(9.4790) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.9496(16.1787) | Bit/dim 3.7337(3.7574) | Xent 0.8717(0.8608) | Loss 10.4763(11.0617) | Error 0.3022(0.3067) Steps 0(0.00) | Grad Norm 16.4611(9.9912) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 16.0115(16.1244) | Bit/dim 3.7776(3.7560) | Xent 0.8439(0.8648) | Loss 10.3431(10.8951) | Error 0.3044(0.3076) Steps 0(0.00) | Grad Norm 9.2310(9.8928) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 16.3628(16.1039) | Bit/dim 3.7204(3.7528) | Xent 0.9261(0.8629) | Loss 10.3515(10.7766) | Error 0.3133(0.3059) Steps 0(0.00) | Grad Norm 10.5621(10.2965) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 16.2917(16.1344) | Bit/dim 3.7706(3.7535) | Xent 0.8779(0.8686) | Loss 10.6357(10.7165) | Error 0.3111(0.3087) Steps 0(0.00) | Grad Norm 9.8058(10.8632) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 87.2177, Epoch Time 993.4011(961.6935), Bit/dim 3.7639(best: 3.7543), Xent 0.8479, Loss 4.1878, Error 0.3000(best: 0.2998)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 16.3072(16.1664) | Bit/dim 3.7593(3.7550) | Xent 0.8505(0.8644) | Loss 10.3976(11.4029) | Error 0.3022(0.3075) Steps 0(0.00) | Grad Norm 10.1056(11.0135) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 16.7010(16.1743) | Bit/dim 3.7584(3.7539) | Xent 0.7677(0.8518) | Loss 10.4301(11.1516) | Error 0.2789(0.3035) Steps 0(0.00) | Grad Norm 5.4672(10.0008) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.9992(16.1589) | Bit/dim 3.7576(3.7531) | Xent 0.9013(0.8566) | Loss 10.5626(10.9886) | Error 0.3100(0.3051) Steps 0(0.00) | Grad Norm 10.1196(10.3413) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 16.6481(16.1967) | Bit/dim 3.7388(3.7507) | Xent 0.8515(0.8556) | Loss 10.4605(10.8588) | Error 0.3100(0.3043) Steps 0(0.00) | Grad Norm 8.1013(10.3562) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.9822(16.1681) | Bit/dim 3.7721(3.7513) | Xent 0.8601(0.8579) | Loss 10.4832(10.7606) | Error 0.3133(0.3048) Steps 0(0.00) | Grad Norm 4.8611(10.0484) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 16.5966(16.1714) | Bit/dim 3.7427(3.7478) | Xent 0.8606(0.8619) | Loss 10.4636(10.6747) | Error 0.3133(0.3061) Steps 0(0.00) | Grad Norm 16.1424(10.9956) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 86.6128, Epoch Time 994.6061(962.6808), Bit/dim 3.7515(best: 3.7543), Xent 0.8508, Loss 4.1769, Error 0.3015(best: 0.2998)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 16.2171(16.1175) | Bit/dim 3.7442(3.7492) | Xent 0.8135(0.8566) | Loss 10.3517(11.2682) | Error 0.3133(0.3064) Steps 0(0.00) | Grad Norm 7.7117(11.2544) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.6854(16.0834) | Bit/dim 3.7640(3.7483) | Xent 0.8514(0.8584) | Loss 10.6472(11.0644) | Error 0.2978(0.3052) Steps 0(0.00) | Grad Norm 15.4953(10.8891) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 16.1998(16.1470) | Bit/dim 3.7396(3.7465) | Xent 0.8144(0.8505) | Loss 10.5579(10.8978) | Error 0.2844(0.3025) Steps 0(0.00) | Grad Norm 8.6847(10.1315) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.2760(16.1896) | Bit/dim 3.7440(3.7457) | Xent 0.8764(0.8441) | Loss 10.6193(10.7821) | Error 0.2989(0.3004) Steps 0(0.00) | Grad Norm 16.9606(10.0058) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 16.3350(16.1199) | Bit/dim 3.7609(3.7482) | Xent 0.8849(0.8515) | Loss 10.5041(10.7081) | Error 0.3078(0.3036) Steps 0(0.00) | Grad Norm 10.1243(10.6545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 87.9384, Epoch Time 992.8676(963.5864), Bit/dim 3.7644(best: 3.7515), Xent 0.8860, Loss 4.2074, Error 0.3139(best: 0.2998)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.6674(16.1621) | Bit/dim 3.7555(3.7489) | Xent 0.9199(0.8653) | Loss 10.4588(11.4144) | Error 0.3211(0.3084) Steps 0(0.00) | Grad Norm 17.9067(11.8366) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 16.1824(16.2061) | Bit/dim 3.7484(3.7558) | Xent 0.8326(0.8586) | Loss 10.3844(11.1730) | Error 0.3089(0.3056) Steps 0(0.00) | Grad Norm 8.8730(11.4678) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 16.4157(16.1967) | Bit/dim 3.7264(3.7547) | Xent 0.8262(0.8501) | Loss 10.4053(10.9846) | Error 0.3022(0.3013) Steps 0(0.00) | Grad Norm 8.6708(11.0099) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 16.7341(16.2053) | Bit/dim 3.7680(3.7519) | Xent 0.8431(0.8446) | Loss 10.5239(10.8394) | Error 0.3178(0.3000) Steps 0(0.00) | Grad Norm 11.6027(10.7235) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.1136(16.1861) | Bit/dim 3.6919(3.7459) | Xent 0.8392(0.8392) | Loss 10.4270(10.7279) | Error 0.2756(0.2976) Steps 0(0.00) | Grad Norm 7.3475(10.4037) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.6999(16.1379) | Bit/dim 3.7278(3.7420) | Xent 0.9549(0.8429) | Loss 10.4273(10.6486) | Error 0.3311(0.2976) Steps 0(0.00) | Grad Norm 20.0485(11.0538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 87.2700, Epoch Time 994.8722(964.5250), Bit/dim 3.7564(best: 3.7515), Xent 0.8822, Loss 4.1975, Error 0.3099(best: 0.2998)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.4843(16.0958) | Bit/dim 3.7437(3.7435) | Xent 0.9359(0.8531) | Loss 10.6531(11.2613) | Error 0.3389(0.3020) Steps 0(0.00) | Grad Norm 15.2552(12.6501) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.8273(16.1720) | Bit/dim 3.7358(3.7419) | Xent 0.8049(0.8473) | Loss 10.4348(11.0556) | Error 0.2911(0.3009) Steps 0(0.00) | Grad Norm 6.1395(11.9622) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 16.1107(16.1314) | Bit/dim 3.7858(3.7477) | Xent 0.8073(0.8404) | Loss 10.4983(10.8980) | Error 0.2800(0.2989) Steps 0(0.00) | Grad Norm 8.9332(11.2554) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 16.2646(16.1526) | Bit/dim 3.7447(3.7459) | Xent 0.8444(0.8337) | Loss 10.4629(10.7806) | Error 0.3044(0.2970) Steps 0(0.00) | Grad Norm 7.3779(10.3469) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 16.1040(16.1284) | Bit/dim 3.7219(3.7438) | Xent 0.8350(0.8292) | Loss 10.4838(10.6889) | Error 0.2911(0.2967) Steps 0(0.00) | Grad Norm 6.2094(9.8986) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 88.0343, Epoch Time 994.6077(965.4275), Bit/dim 3.7358(best: 3.7515), Xent 0.8313, Loss 4.1515, Error 0.2943(best: 0.2998)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.3548(16.1243) | Bit/dim 3.7241(3.7398) | Xent 0.7727(0.8208) | Loss 10.3289(11.3581) | Error 0.2678(0.2938) Steps 0(0.00) | Grad Norm 4.4313(9.3931) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 16.5021(16.1399) | Bit/dim 3.7229(3.7393) | Xent 0.8251(0.8149) | Loss 10.4506(11.1202) | Error 0.2922(0.2906) Steps 0(0.00) | Grad Norm 12.1187(9.3289) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 16.0260(16.1256) | Bit/dim 3.7160(3.7377) | Xent 0.8575(0.8180) | Loss 10.5449(10.9514) | Error 0.2944(0.2908) Steps 0(0.00) | Grad Norm 8.4834(9.2583) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.2323(16.1109) | Bit/dim 3.7535(3.7370) | Xent 0.9203(0.8359) | Loss 10.6329(10.8445) | Error 0.3133(0.2951) Steps 0(0.00) | Grad Norm 14.4825(11.3214) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.2723(16.0851) | Bit/dim 3.7135(3.7377) | Xent 0.9262(0.8484) | Loss 10.5572(10.7670) | Error 0.3344(0.3001) Steps 0(0.00) | Grad Norm 8.0293(11.5477) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.5255(16.0592) | Bit/dim 3.7873(3.7401) | Xent 0.7681(0.8407) | Loss 10.5115(10.6959) | Error 0.2756(0.2984) Steps 0(0.00) | Grad Norm 4.9107(10.8734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 87.8999, Epoch Time 988.4510(966.1182), Bit/dim 3.7455(best: 3.7358), Xent 0.8349, Loss 4.1630, Error 0.2954(best: 0.2943)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.3758(16.0358) | Bit/dim 3.7061(3.7379) | Xent 0.8337(0.8365) | Loss 10.3863(11.2679) | Error 0.2889(0.2974) Steps 0(0.00) | Grad Norm 7.9108(9.6626) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.0313(15.9874) | Bit/dim 3.7460(3.7366) | Xent 0.7954(0.8310) | Loss 10.4487(11.0497) | Error 0.2867(0.2955) Steps 0(0.00) | Grad Norm 11.8111(9.6641) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.5900(15.9357) | Bit/dim 3.7443(3.7344) | Xent 0.8289(0.8260) | Loss 10.4049(10.8840) | Error 0.3122(0.2944) Steps 0(0.00) | Grad Norm 8.5160(9.4098) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.2153(15.9162) | Bit/dim 3.6860(3.7331) | Xent 0.8433(0.8186) | Loss 10.4114(10.7607) | Error 0.3033(0.2923) Steps 0(0.00) | Grad Norm 7.3888(9.6622) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.6026(15.9212) | Bit/dim 3.7546(3.7331) | Xent 0.9032(0.8233) | Loss 10.4818(10.6819) | Error 0.3267(0.2946) Steps 0(0.00) | Grad Norm 13.2056(10.1724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 89.0261, Epoch Time 982.0678(966.5967), Bit/dim 3.7300(best: 3.7358), Xent 0.8272, Loss 4.1436, Error 0.2926(best: 0.2943)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.7302(15.9115) | Bit/dim 3.7459(3.7368) | Xent 0.7739(0.8218) | Loss 10.3907(11.3717) | Error 0.2856(0.2943) Steps 0(0.00) | Grad Norm 5.5645(10.4980) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 16.1933(15.8960) | Bit/dim 3.7145(3.7360) | Xent 0.7589(0.8120) | Loss 10.3581(11.1130) | Error 0.2811(0.2907) Steps 0(0.00) | Grad Norm 7.0386(9.6563) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 16.1690(15.9326) | Bit/dim 3.7074(3.7329) | Xent 0.7433(0.8019) | Loss 10.2130(10.9202) | Error 0.2667(0.2878) Steps 0(0.00) | Grad Norm 5.3719(9.1291) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.5519(15.8938) | Bit/dim 3.7639(3.7306) | Xent 0.8745(0.8078) | Loss 10.5564(10.7803) | Error 0.3078(0.2884) Steps 0(0.00) | Grad Norm 12.3179(9.2666) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 16.0447(15.9314) | Bit/dim 3.7462(3.7338) | Xent 0.8200(0.8054) | Loss 10.5044(10.6881) | Error 0.2878(0.2860) Steps 0(0.00) | Grad Norm 13.9628(9.5330) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.7313(15.9629) | Bit/dim 3.7413(3.7337) | Xent 0.8399(0.8052) | Loss 10.5293(10.6180) | Error 0.3056(0.2875) Steps 0(0.00) | Grad Norm 17.6765(10.3928) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 87.7909, Epoch Time 981.6920(967.0495), Bit/dim 3.7305(best: 3.7300), Xent 0.7996, Loss 4.1303, Error 0.2829(best: 0.2926)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.9775(16.0132) | Bit/dim 3.7572(3.7327) | Xent 0.8512(0.7981) | Loss 10.5369(11.2301) | Error 0.3056(0.2856) Steps 0(0.00) | Grad Norm 12.1273(10.4087) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.7589(15.9809) | Bit/dim 3.7534(3.7307) | Xent 0.7466(0.7933) | Loss 10.4003(11.0141) | Error 0.2667(0.2844) Steps 0(0.00) | Grad Norm 6.7183(9.9175) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.2265(15.9504) | Bit/dim 3.7540(3.7302) | Xent 0.8574(0.7907) | Loss 10.3364(10.8443) | Error 0.2967(0.2841) Steps 0(0.00) | Grad Norm 8.2587(9.6920) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.0523(15.9630) | Bit/dim 3.7573(3.7301) | Xent 0.8647(0.7927) | Loss 10.6144(10.7405) | Error 0.3011(0.2845) Steps 0(0.00) | Grad Norm 8.1399(9.7695) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 16.0368(15.9655) | Bit/dim 3.7283(3.7277) | Xent 0.7411(0.7888) | Loss 10.3142(10.6444) | Error 0.2611(0.2833) Steps 0(0.00) | Grad Norm 7.1597(9.5943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 88.1202, Epoch Time 984.2495(967.5655), Bit/dim 3.7236(best: 3.7300), Xent 0.8014, Loss 4.1243, Error 0.2821(best: 0.2829)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.5862(15.9461) | Bit/dim 3.6750(3.7297) | Xent 0.8551(0.7930) | Loss 10.3915(11.3399) | Error 0.2889(0.2832) Steps 0(0.00) | Grad Norm 17.0066(10.6433) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 17.1798(15.9735) | Bit/dim 3.7184(3.7302) | Xent 0.8462(0.8013) | Loss 10.4483(11.1098) | Error 0.2878(0.2858) Steps 0(0.00) | Grad Norm 11.0750(10.8235) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.3918(16.0044) | Bit/dim 3.7366(3.7285) | Xent 0.8127(0.7987) | Loss 10.4316(10.9134) | Error 0.2844(0.2842) Steps 0(0.00) | Grad Norm 8.4172(11.0593) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.8941(15.9848) | Bit/dim 3.7228(3.7286) | Xent 0.7805(0.8061) | Loss 10.2945(10.7834) | Error 0.2656(0.2865) Steps 0(0.00) | Grad Norm 10.3872(11.4873) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.2240(15.9582) | Bit/dim 3.7517(3.7310) | Xent 0.7634(0.7997) | Loss 10.2874(10.6713) | Error 0.2778(0.2846) Steps 0(0.00) | Grad Norm 8.7448(10.7558) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.1178(16.0195) | Bit/dim 3.7322(3.7278) | Xent 0.7344(0.7963) | Loss 10.3759(10.5933) | Error 0.2644(0.2827) Steps 0(0.00) | Grad Norm 8.5279(10.3585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 89.7145, Epoch Time 988.6463(968.1980), Bit/dim 3.7208(best: 3.7236), Xent 0.8025, Loss 4.1221, Error 0.2815(best: 0.2821)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 16.5140(16.0507) | Bit/dim 3.7223(3.7232) | Xent 0.7110(0.7891) | Loss 10.3209(11.1749) | Error 0.2644(0.2814) Steps 0(0.00) | Grad Norm 9.1402(10.0612) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.8354(16.0864) | Bit/dim 3.7336(3.7242) | Xent 0.7111(0.7813) | Loss 10.4252(10.9645) | Error 0.2533(0.2787) Steps 0(0.00) | Grad Norm 10.3741(9.9521) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.8370(16.1048) | Bit/dim 3.7072(3.7224) | Xent 0.8748(0.7885) | Loss 10.4403(10.8032) | Error 0.3222(0.2804) Steps 0(0.00) | Grad Norm 15.9020(10.3142) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.1993(16.0884) | Bit/dim 3.7120(3.7249) | Xent 0.8498(0.7885) | Loss 10.4958(10.6977) | Error 0.2989(0.2814) Steps 0(0.00) | Grad Norm 14.4756(10.6655) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.7521(16.0610) | Bit/dim 3.7385(3.7272) | Xent 0.7765(0.7861) | Loss 10.4179(10.6332) | Error 0.2756(0.2799) Steps 0(0.00) | Grad Norm 12.4913(10.2929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 87.6783, Epoch Time 990.1364(968.8561), Bit/dim 3.7219(best: 3.7208), Xent 0.8025, Loss 4.1231, Error 0.2812(best: 0.2815)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.5651(16.0688) | Bit/dim 3.6862(3.7280) | Xent 0.7103(0.7809) | Loss 10.2641(11.3421) | Error 0.2622(0.2785) Steps 0(0.00) | Grad Norm 8.1233(10.2841) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.8303(16.0609) | Bit/dim 3.7285(3.7268) | Xent 0.8080(0.7731) | Loss 10.4325(11.0858) | Error 0.2878(0.2753) Steps 0(0.00) | Grad Norm 15.3503(10.4016) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 16.6912(16.0658) | Bit/dim 3.7498(3.7242) | Xent 0.7754(0.7781) | Loss 10.4131(10.9073) | Error 0.2700(0.2767) Steps 0(0.00) | Grad Norm 12.8700(10.7108) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 16.8270(16.0874) | Bit/dim 3.7196(3.7234) | Xent 0.7318(0.7775) | Loss 10.3788(10.7675) | Error 0.2589(0.2754) Steps 0(0.00) | Grad Norm 6.4084(10.4200) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.6148(16.1137) | Bit/dim 3.7229(3.7237) | Xent 0.8080(0.7789) | Loss 10.4857(10.6631) | Error 0.2822(0.2755) Steps 0(0.00) | Grad Norm 9.3033(10.3129) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.8811(16.1222) | Bit/dim 3.7303(3.7235) | Xent 0.8585(0.7828) | Loss 10.5223(10.5885) | Error 0.3011(0.2768) Steps 0(0.00) | Grad Norm 10.0867(9.9945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 90.0268, Epoch Time 995.2126(969.6468), Bit/dim 3.7227(best: 3.7208), Xent 0.7684, Loss 4.1068, Error 0.2709(best: 0.2812)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.8980(16.0449) | Bit/dim 3.7307(3.7231) | Xent 0.7289(0.7735) | Loss 10.3239(11.1631) | Error 0.2644(0.2742) Steps 0(0.00) | Grad Norm 8.9918(9.6690) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 15.6884(16.0286) | Bit/dim 3.7277(3.7211) | Xent 0.8131(0.7707) | Loss 10.2631(10.9446) | Error 0.2944(0.2741) Steps 0(0.00) | Grad Norm 8.5662(9.7584) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.7758(16.0034) | Bit/dim 3.7319(3.7214) | Xent 0.8068(0.7670) | Loss 10.3743(10.7915) | Error 0.2811(0.2725) Steps 0(0.00) | Grad Norm 16.3204(9.5071) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.5935(16.0531) | Bit/dim 3.6915(3.7191) | Xent 0.7567(0.7648) | Loss 10.3986(10.6757) | Error 0.2744(0.2712) Steps 0(0.00) | Grad Norm 6.3339(9.5957) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 15.8884(15.9851) | Bit/dim 3.7062(3.7174) | Xent 0.7660(0.7672) | Loss 10.3836(10.5962) | Error 0.2867(0.2737) Steps 0(0.00) | Grad Norm 11.5032(10.3978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 90.9635, Epoch Time 987.3880(970.1791), Bit/dim 3.7192(best: 3.7208), Xent 0.7836, Loss 4.1111, Error 0.2795(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 17.0044(16.0933) | Bit/dim 3.7403(3.7190) | Xent 0.7505(0.7603) | Loss 10.4063(11.3382) | Error 0.2722(0.2704) Steps 0(0.00) | Grad Norm 7.5630(9.7199) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.7819(16.1328) | Bit/dim 3.7059(3.7177) | Xent 0.8126(0.7596) | Loss 10.3621(11.0863) | Error 0.2833(0.2711) Steps 0(0.00) | Grad Norm 13.4878(9.5719) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.3322(16.0657) | Bit/dim 3.7232(3.7161) | Xent 0.7326(0.7600) | Loss 10.4296(10.8988) | Error 0.2667(0.2712) Steps 0(0.00) | Grad Norm 7.8151(9.1996) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.8746(16.0499) | Bit/dim 3.7254(3.7148) | Xent 0.7694(0.7570) | Loss 10.4926(10.7509) | Error 0.2767(0.2696) Steps 0(0.00) | Grad Norm 6.6002(8.5636) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.4598(16.0456) | Bit/dim 3.7109(3.7142) | Xent 0.7916(0.7644) | Loss 10.3992(10.6592) | Error 0.2956(0.2725) Steps 0(0.00) | Grad Norm 11.1641(9.6991) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 17.0876(16.0977) | Bit/dim 3.7322(3.7169) | Xent 0.8142(0.7748) | Loss 10.5595(10.6138) | Error 0.2767(0.2762) Steps 0(0.00) | Grad Norm 10.9935(10.5167) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 88.4705, Epoch Time 992.3916(970.8454), Bit/dim 3.7330(best: 3.7192), Xent 0.7885, Loss 4.1272, Error 0.2837(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 15.9344(16.0477) | Bit/dim 3.6648(3.7192) | Xent 0.7198(0.7728) | Loss 10.2553(11.1993) | Error 0.2522(0.2747) Steps 0(0.00) | Grad Norm 8.8457(10.5113) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.5754(16.0035) | Bit/dim 3.6765(3.7178) | Xent 0.7566(0.7638) | Loss 10.4258(10.9684) | Error 0.2644(0.2721) Steps 0(0.00) | Grad Norm 9.0473(10.1251) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.0047(16.1448) | Bit/dim 3.7377(3.7174) | Xent 0.8009(0.7611) | Loss 10.3869(10.8045) | Error 0.2822(0.2717) Steps 0(0.00) | Grad Norm 12.3736(9.8428) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 16.0013(16.0997) | Bit/dim 3.6896(3.7162) | Xent 0.7664(0.7614) | Loss 10.3228(10.6820) | Error 0.2733(0.2718) Steps 0(0.00) | Grad Norm 6.2879(9.4007) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.9415(16.0715) | Bit/dim 3.7165(3.7114) | Xent 0.7358(0.7566) | Loss 10.4105(10.5833) | Error 0.2633(0.2696) Steps 0(0.00) | Grad Norm 13.5567(9.3408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 89.2979, Epoch Time 992.1052(971.4832), Bit/dim 3.7170(best: 3.7192), Xent 0.7821, Loss 4.1081, Error 0.2783(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.5240(16.0924) | Bit/dim 3.7585(3.7122) | Xent 0.7925(0.7544) | Loss 10.5516(11.2976) | Error 0.2789(0.2688) Steps 0(0.00) | Grad Norm 14.1837(9.5571) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 16.7349(16.0960) | Bit/dim 3.7129(3.7106) | Xent 0.7691(0.7485) | Loss 10.4874(11.0442) | Error 0.2744(0.2666) Steps 0(0.00) | Grad Norm 9.5500(9.6783) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.9896(16.1032) | Bit/dim 3.7065(3.7108) | Xent 0.7334(0.7437) | Loss 10.3246(10.8495) | Error 0.2756(0.2660) Steps 0(0.00) | Grad Norm 7.9897(9.6557) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 16.8841(16.1182) | Bit/dim 3.7242(3.7129) | Xent 0.7194(0.7439) | Loss 10.3063(10.7172) | Error 0.2667(0.2670) Steps 0(0.00) | Grad Norm 10.5129(9.5596) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.7815(16.0927) | Bit/dim 3.7155(3.7120) | Xent 0.7134(0.7453) | Loss 10.2922(10.6159) | Error 0.2678(0.2668) Steps 0(0.00) | Grad Norm 9.8011(9.9684) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.0003(16.0850) | Bit/dim 3.7078(3.7071) | Xent 0.7504(0.7444) | Loss 10.2351(10.5238) | Error 0.2800(0.2653) Steps 0(0.00) | Grad Norm 13.2120(10.0308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 90.0983, Epoch Time 992.1086(972.1020), Bit/dim 3.7128(best: 3.7170), Xent 0.8002, Loss 4.1129, Error 0.2798(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.7258(16.0933) | Bit/dim 3.6951(3.7091) | Xent 0.7192(0.7428) | Loss 10.3017(11.1549) | Error 0.2411(0.2652) Steps 0(0.00) | Grad Norm 6.6906(9.9703) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.5617(16.0561) | Bit/dim 3.7036(3.7074) | Xent 0.7722(0.7393) | Loss 10.2373(10.9359) | Error 0.2778(0.2642) Steps 0(0.00) | Grad Norm 15.8910(9.7546) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.1922(16.0606) | Bit/dim 3.7337(3.7094) | Xent 0.6670(0.7416) | Loss 10.4240(10.7874) | Error 0.2344(0.2651) Steps 0(0.00) | Grad Norm 8.9970(10.2609) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 16.1825(16.0755) | Bit/dim 3.6887(3.7058) | Xent 0.7371(0.7459) | Loss 10.3593(10.6631) | Error 0.2611(0.2652) Steps 0(0.00) | Grad Norm 15.7124(10.3321) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.9048(16.1113) | Bit/dim 3.7245(3.7058) | Xent 0.7091(0.7469) | Loss 10.2734(10.5677) | Error 0.2489(0.2655) Steps 0(0.00) | Grad Norm 9.0068(10.8482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 89.5410, Epoch Time 991.8353(972.6940), Bit/dim 3.7023(best: 3.7128), Xent 0.7825, Loss 4.0935, Error 0.2778(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.3101(16.0914) | Bit/dim 3.7060(3.7081) | Xent 0.6689(0.7427) | Loss 10.3927(11.2667) | Error 0.2567(0.2651) Steps 0(0.00) | Grad Norm 7.3269(10.4033) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.0561(16.0677) | Bit/dim 3.7376(3.7073) | Xent 0.7273(0.7417) | Loss 10.4425(11.0196) | Error 0.2678(0.2647) Steps 0(0.00) | Grad Norm 10.9261(10.1390) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.4994(16.0822) | Bit/dim 3.6829(3.7054) | Xent 0.7531(0.7413) | Loss 10.3939(10.8384) | Error 0.2789(0.2653) Steps 0(0.00) | Grad Norm 14.4504(10.1203) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.8223(16.1089) | Bit/dim 3.7157(3.7071) | Xent 0.6812(0.7338) | Loss 10.3541(10.7059) | Error 0.2378(0.2636) Steps 0(0.00) | Grad Norm 7.7643(9.8943) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.6126(16.1234) | Bit/dim 3.6882(3.7072) | Xent 0.7993(0.7364) | Loss 10.3300(10.6061) | Error 0.2744(0.2635) Steps 0(0.00) | Grad Norm 22.1867(10.1586) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.1406(16.1473) | Bit/dim 3.7118(3.7078) | Xent 0.7491(0.7437) | Loss 10.2804(10.5357) | Error 0.2478(0.2649) Steps 0(0.00) | Grad Norm 7.6578(10.1200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 90.1671, Epoch Time 995.6334(973.3822), Bit/dim 3.7146(best: 3.7023), Xent 0.7604, Loss 4.0948, Error 0.2647(best: 0.2709)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.0378(16.0430) | Bit/dim 3.7275(3.7075) | Xent 0.7193(0.7355) | Loss 10.4666(11.1475) | Error 0.2700(0.2616) Steps 0(0.00) | Grad Norm 5.3109(9.7459) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.7977(16.0093) | Bit/dim 3.7245(3.7097) | Xent 0.7039(0.7320) | Loss 10.2303(10.9350) | Error 0.2500(0.2602) Steps 0(0.00) | Grad Norm 9.7095(9.7711) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.2230(16.0347) | Bit/dim 3.7178(3.7103) | Xent 0.7125(0.7297) | Loss 10.3829(10.7766) | Error 0.2433(0.2594) Steps 0(0.00) | Grad Norm 10.1305(10.1937) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.7888(15.9864) | Bit/dim 3.6596(3.7082) | Xent 0.7062(0.7318) | Loss 10.2280(10.6458) | Error 0.2300(0.2587) Steps 0(0.00) | Grad Norm 8.1307(10.0615) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.4496(16.0110) | Bit/dim 3.6959(3.7063) | Xent 0.7243(0.7226) | Loss 10.3847(10.5459) | Error 0.2544(0.2560) Steps 0(0.00) | Grad Norm 9.9484(9.8485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 89.7658, Epoch Time 985.7194(973.7523), Bit/dim 3.7074(best: 3.7023), Xent 0.7683, Loss 4.0915, Error 0.2694(best: 0.2647)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.4755(16.0806) | Bit/dim 3.6860(3.7063) | Xent 0.7291(0.7235) | Loss 10.2356(11.2556) | Error 0.2489(0.2564) Steps 0(0.00) | Grad Norm 11.5738(9.8469) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 15.7909(16.0722) | Bit/dim 3.7225(3.7078) | Xent 0.6940(0.7211) | Loss 10.2938(11.0004) | Error 0.2622(0.2558) Steps 0(0.00) | Grad Norm 6.7406(10.0167) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.5098(16.1217) | Bit/dim 3.7001(3.7045) | Xent 0.7523(0.7136) | Loss 10.2701(10.8008) | Error 0.2900(0.2545) Steps 0(0.00) | Grad Norm 10.2743(9.8705) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.9172(16.0607) | Bit/dim 3.7314(3.7019) | Xent 0.6950(0.7145) | Loss 10.3063(10.6596) | Error 0.2367(0.2536) Steps 0(0.00) | Grad Norm 9.7707(9.9024) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.0370(16.0628) | Bit/dim 3.7350(3.7031) | Xent 0.7352(0.7115) | Loss 10.3001(10.5669) | Error 0.2544(0.2528) Steps 0(0.00) | Grad Norm 10.0805(9.4333) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 16.3065(16.0496) | Bit/dim 3.7559(3.7051) | Xent 0.7435(0.7226) | Loss 10.4680(10.5039) | Error 0.2556(0.2571) Steps 0(0.00) | Grad Norm 8.8268(10.3215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 89.7921, Epoch Time 992.2270(974.3065), Bit/dim 3.7171(best: 3.7023), Xent 0.7595, Loss 4.0968, Error 0.2689(best: 0.2647)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.9922(16.1002) | Bit/dim 3.6653(3.7027) | Xent 0.6894(0.7179) | Loss 10.1645(11.1048) | Error 0.2433(0.2552) Steps 0(0.00) | Grad Norm 6.8478(10.0358) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 16.0247(16.0977) | Bit/dim 3.6822(3.7007) | Xent 0.6865(0.7151) | Loss 10.1338(10.8773) | Error 0.2400(0.2538) Steps 0(0.00) | Grad Norm 7.0308(9.5445) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.7927(16.1018) | Bit/dim 3.7392(3.7021) | Xent 0.7418(0.7146) | Loss 10.3631(10.7300) | Error 0.2600(0.2526) Steps 0(0.00) | Grad Norm 13.6296(10.3478) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_3_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 3.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
