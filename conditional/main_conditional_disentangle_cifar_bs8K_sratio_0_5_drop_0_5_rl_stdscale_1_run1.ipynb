{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1/epoch_72_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1', scale=1.0, scale_fac=1.0, scale_std=1.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0433 | Time 119.9228(69.9045) | Bit/dim 3.9991(4.0898) | Xent 1.5288(1.5932) | Loss 14.8671(12.9804) | Error 0.5584(0.5732) Steps 0(0.00) | Grad Norm 14.8899(12.0355) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 63.0964(69.7003) | Bit/dim 3.9905(4.0869) | Xent 1.4898(1.5901) | Loss 12.0550(12.9527) | Error 0.5325(0.5720) Steps 0(0.00) | Grad Norm 13.0571(12.0661) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 62.4649(69.4832) | Bit/dim 3.9953(4.0841) | Xent 1.5056(1.5876) | Loss 12.0982(12.9270) | Error 0.5464(0.5713) Steps 0(0.00) | Grad Norm 9.1509(11.9786) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 67.2432(69.4160) | Bit/dim 4.0060(4.0818) | Xent 1.4961(1.5848) | Loss 11.9527(12.8978) | Error 0.5494(0.5706) Steps 0(0.00) | Grad Norm 13.1829(12.0148) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 56.7318(69.0355) | Bit/dim 4.0090(4.0796) | Xent 1.5394(1.5835) | Loss 11.9837(12.8704) | Error 0.5589(0.5702) Steps 0(0.00) | Grad Norm 13.9072(12.0715) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 61.3033(68.8035) | Bit/dim 3.9878(4.0768) | Xent 1.5095(1.5812) | Loss 11.9136(12.8417) | Error 0.5439(0.5695) Steps 0(0.00) | Grad Norm 11.8124(12.0638) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 40.7276, Epoch Time 487.6044(417.1727), Bit/dim 4.0050(best: inf), Xent 1.4758, Loss 4.7429, Error 0.5306(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 73.3378(68.9396) | Bit/dim 4.0116(4.0749) | Xent 1.5242(1.5795) | Loss 16.1932(12.9422) | Error 0.5390(0.5685) Steps 0(0.00) | Grad Norm 16.5830(12.1994) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 58.3206(68.6210) | Bit/dim 3.9818(4.0721) | Xent 1.5678(1.5792) | Loss 12.0891(12.9166) | Error 0.5634(0.5684) Steps 0(0.00) | Grad Norm 21.3493(12.4739) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 65.8919(68.5391) | Bit/dim 3.9973(4.0698) | Xent 1.5770(1.5791) | Loss 11.9782(12.8885) | Error 0.5747(0.5686) Steps 0(0.00) | Grad Norm 17.4948(12.6245) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 60.5510(68.2995) | Bit/dim 3.9806(4.0672) | Xent 1.5148(1.5772) | Loss 11.9075(12.8590) | Error 0.5473(0.5679) Steps 0(0.00) | Grad Norm 10.6170(12.5643) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 60.6138(68.0689) | Bit/dim 3.9899(4.0649) | Xent 1.4975(1.5748) | Loss 11.9633(12.8322) | Error 0.5394(0.5671) Steps 0(0.00) | Grad Norm 8.3539(12.4379) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 57.4120(67.7492) | Bit/dim 3.9788(4.0623) | Xent 1.4839(1.5721) | Loss 11.9260(12.8050) | Error 0.5339(0.5661) Steps 0(0.00) | Grad Norm 8.7807(12.3282) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 23.5030, Epoch Time 416.0379(417.1387), Bit/dim 3.9872(best: 4.0050), Xent 1.4381, Loss 4.7062, Error 0.5238(best: 0.5306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 60.0065(67.5169) | Bit/dim 3.9738(4.0596) | Xent 1.5015(1.5700) | Loss 15.9035(12.8979) | Error 0.5460(0.5655) Steps 0(0.00) | Grad Norm 11.4558(12.3021) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 60.4185(67.3040) | Bit/dim 4.0009(4.0579) | Xent 1.4806(1.5673) | Loss 11.8336(12.8660) | Error 0.5306(0.5644) Steps 0(0.00) | Grad Norm 8.9672(12.2020) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 56.9575(66.9936) | Bit/dim 3.9674(4.0551) | Xent 1.4535(1.5639) | Loss 11.5795(12.8274) | Error 0.5274(0.5633) Steps 0(0.00) | Grad Norm 6.3308(12.0259) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 59.9295(66.7816) | Bit/dim 3.9491(4.0520) | Xent 1.4761(1.5612) | Loss 11.8660(12.7986) | Error 0.5376(0.5626) Steps 0(0.00) | Grad Norm 7.5977(11.8930) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 60.7696(66.6013) | Bit/dim 3.9828(4.0499) | Xent 1.4540(1.5580) | Loss 11.8613(12.7704) | Error 0.5264(0.5615) Steps 0(0.00) | Grad Norm 9.0529(11.8078) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 63.0447(66.4946) | Bit/dim 3.9549(4.0470) | Xent 1.4669(1.5553) | Loss 11.9429(12.7456) | Error 0.5296(0.5605) Steps 0(0.00) | Grad Norm 4.0437(11.5749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.3670, Epoch Time 400.7333(416.6465), Bit/dim 3.9530(best: 3.9872), Xent 1.4111, Loss 4.6586, Error 0.5091(best: 0.5238)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 62.2109(66.3661) | Bit/dim 3.9617(4.0445) | Xent 1.4557(1.5523) | Loss 16.0811(12.8457) | Error 0.5282(0.5595) Steps 0(0.00) | Grad Norm 6.3049(11.4168) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 63.5982(66.2830) | Bit/dim 3.9540(4.0418) | Xent 1.4401(1.5489) | Loss 11.8695(12.8164) | Error 0.5261(0.5585) Steps 0(0.00) | Grad Norm 8.7220(11.3360) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 64.4877(66.2292) | Bit/dim 3.9597(4.0393) | Xent 1.4428(1.5457) | Loss 11.5474(12.7783) | Error 0.5206(0.5574) Steps 0(0.00) | Grad Norm 8.4010(11.2479) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 60.1575(66.0470) | Bit/dim 3.9566(4.0368) | Xent 1.5280(1.5452) | Loss 11.9790(12.7544) | Error 0.5465(0.5571) Steps 0(0.00) | Grad Norm 18.3152(11.4599) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 62.6737(65.9458) | Bit/dim 3.9703(4.0348) | Xent 1.7921(1.5526) | Loss 12.0130(12.7321) | Error 0.6050(0.5585) Steps 0(0.00) | Grad Norm 38.3998(12.2681) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 63.4100(65.8698) | Bit/dim 3.9685(4.0328) | Xent 1.4829(1.5505) | Loss 11.9662(12.7091) | Error 0.5415(0.5580) Steps 0(0.00) | Grad Norm 6.4935(12.0949) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 25.0220, Epoch Time 417.5492(416.6736), Bit/dim 3.9690(best: 3.9530), Xent 1.5171, Loss 4.7276, Error 0.5407(best: 0.5091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 57.5012(65.6187) | Bit/dim 3.9616(4.0307) | Xent 1.5552(1.5507) | Loss 16.1520(12.8124) | Error 0.5624(0.5581) Steps 0(0.00) | Grad Norm 13.1110(12.1254) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 60.7674(65.4732) | Bit/dim 3.9777(4.0291) | Xent 1.5116(1.5495) | Loss 12.0478(12.7895) | Error 0.5477(0.5578) Steps 0(0.00) | Grad Norm 10.8859(12.0882) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 57.8066(65.2432) | Bit/dim 3.9778(4.0276) | Xent 1.4604(1.5468) | Loss 11.9997(12.7658) | Error 0.5297(0.5570) Steps 0(0.00) | Grad Norm 10.2152(12.0320) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 52.3247(64.8556) | Bit/dim 3.9637(4.0256) | Xent 1.4434(1.5437) | Loss 11.9597(12.7416) | Error 0.5132(0.5557) Steps 0(0.00) | Grad Norm 5.9160(11.8485) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 63.0899(64.8026) | Bit/dim 3.9672(4.0239) | Xent 1.4790(1.5418) | Loss 12.0990(12.7223) | Error 0.5406(0.5552) Steps 0(0.00) | Grad Norm 10.5796(11.8104) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 60.2412(64.6658) | Bit/dim 3.9689(4.0222) | Xent 1.5287(1.5414) | Loss 12.0207(12.7013) | Error 0.5521(0.5551) Steps 0(0.00) | Grad Norm 13.8607(11.8720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 24.1660, Epoch Time 391.7968(415.9273), Bit/dim 3.9637(best: 3.9530), Xent 1.4436, Loss 4.6855, Error 0.5222(best: 0.5091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 62.1500(64.5903) | Bit/dim 3.9620(4.0204) | Xent 1.4866(1.5397) | Loss 16.2545(12.8079) | Error 0.5405(0.5547) Steps 0(0.00) | Grad Norm 13.1643(11.9107) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 62.5927(64.5304) | Bit/dim 3.9734(4.0190) | Xent 1.4673(1.5376) | Loss 11.9286(12.7815) | Error 0.5305(0.5540) Steps 0(0.00) | Grad Norm 9.1155(11.8269) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 60.4688(64.4085) | Bit/dim 3.9391(4.0166) | Xent 1.4918(1.5362) | Loss 11.7428(12.7503) | Error 0.5454(0.5537) Steps 0(0.00) | Grad Norm 11.8338(11.8271) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 58.5059(64.2315) | Bit/dim 3.9577(4.0149) | Xent 1.4992(1.5351) | Loss 11.9351(12.7259) | Error 0.5315(0.5530) Steps 0(0.00) | Grad Norm 14.1622(11.8971) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 61.9244(64.1622) | Bit/dim 3.9472(4.0128) | Xent 1.5033(1.5341) | Loss 12.0067(12.7043) | Error 0.5464(0.5528) Steps 0(0.00) | Grad Norm 14.5338(11.9762) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 58.2112(63.9837) | Bit/dim 3.9617(4.0113) | Xent 1.5183(1.5337) | Loss 11.9206(12.6808) | Error 0.5514(0.5528) Steps 0(0.00) | Grad Norm 12.9161(12.0044) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 24.2185, Epoch Time 404.1032(415.5726), Bit/dim 3.9733(best: 3.9530), Xent 1.4163, Loss 4.6815, Error 0.5159(best: 0.5091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 62.8713(63.9503) | Bit/dim 3.9703(4.0101) | Xent 1.4551(1.5313) | Loss 15.1247(12.7541) | Error 0.5171(0.5517) Steps 0(0.00) | Grad Norm 11.4479(11.9877) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 59.4037(63.8139) | Bit/dim 3.9867(4.0094) | Xent 1.4981(1.5303) | Loss 11.8612(12.7273) | Error 0.5369(0.5513) Steps 0(0.00) | Grad Norm 10.7021(11.9492) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 65.1428(63.8538) | Bit/dim 3.9471(4.0075) | Xent 1.4617(1.5282) | Loss 11.5882(12.6931) | Error 0.5306(0.5507) Steps 0(0.00) | Grad Norm 6.3164(11.7802) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 60.9794(63.7676) | Bit/dim 3.9683(4.0063) | Xent 1.4597(1.5262) | Loss 11.8265(12.6671) | Error 0.5331(0.5501) Steps 0(0.00) | Grad Norm 7.8329(11.6618) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 50.3272(63.3644) | Bit/dim 3.9794(4.0055) | Xent 1.4423(1.5237) | Loss 11.9909(12.6469) | Error 0.5215(0.5493) Steps 0(0.00) | Grad Norm 7.4701(11.5360) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 57.9450(63.2018) | Bit/dim 3.9444(4.0037) | Xent 1.4374(1.5211) | Loss 11.7236(12.6192) | Error 0.5164(0.5483) Steps 0(0.00) | Grad Norm 4.9590(11.3387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 25.4114, Epoch Time 398.0524(415.0470), Bit/dim 3.9578(best: 3.9530), Xent 1.3814, Loss 4.6485, Error 0.4972(best: 0.5091)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 58.9602(63.0745) | Bit/dim 3.9605(4.0024) | Xent 1.4266(1.5182) | Loss 15.9961(12.7205) | Error 0.5201(0.5474) Steps 0(0.00) | Grad Norm 7.6660(11.2285) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 68.1198(63.2259) | Bit/dim 3.9351(4.0004) | Xent 1.4424(1.5160) | Loss 11.7272(12.6907) | Error 0.5201(0.5466) Steps 0(0.00) | Grad Norm 11.9085(11.2489) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 56.7336(63.0311) | Bit/dim 3.9613(3.9992) | Xent 1.4755(1.5148) | Loss 12.0371(12.6711) | Error 0.5252(0.5460) Steps 0(0.00) | Grad Norm 17.3506(11.4320) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 57.8955(62.8771) | Bit/dim 3.9792(3.9986) | Xent 1.6163(1.5178) | Loss 12.0070(12.6511) | Error 0.5645(0.5465) Steps 0(0.00) | Grad Norm 27.3212(11.9086) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 61.6585(62.8405) | Bit/dim 3.9511(3.9972) | Xent 1.4929(1.5171) | Loss 11.9238(12.6293) | Error 0.5437(0.5465) Steps 0(0.00) | Grad Norm 10.6827(11.8719) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 63.4308(62.8582) | Bit/dim 3.9327(3.9952) | Xent 1.4744(1.5158) | Loss 11.8986(12.6074) | Error 0.5304(0.5460) Steps 0(0.00) | Grad Norm 7.4956(11.7406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 24.1233, Epoch Time 406.5191(414.7911), Bit/dim 3.9436(best: 3.9530), Xent 1.4294, Loss 4.6583, Error 0.5172(best: 0.4972)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 62.2112(62.8388) | Bit/dim 3.9468(3.9938) | Xent 1.4913(1.5150) | Loss 15.6180(12.6977) | Error 0.5365(0.5457) Steps 0(0.00) | Grad Norm 8.4189(11.6409) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 58.8152(62.7181) | Bit/dim 3.9228(3.9917) | Xent 1.4300(1.5125) | Loss 11.7522(12.6694) | Error 0.5117(0.5447) Steps 0(0.00) | Grad Norm 8.5608(11.5485) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 60.0243(62.6373) | Bit/dim 3.9449(3.9903) | Xent 1.4566(1.5108) | Loss 11.7556(12.6419) | Error 0.5255(0.5441) Steps 0(0.00) | Grad Norm 12.7482(11.5845) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 59.2967(62.5371) | Bit/dim 3.9350(3.9886) | Xent 1.4727(1.5097) | Loss 11.2953(12.6015) | Error 0.5305(0.5437) Steps 0(0.00) | Grad Norm 12.9792(11.6264) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 60.4630(62.4748) | Bit/dim 3.9493(3.9874) | Xent 1.4572(1.5081) | Loss 11.8346(12.5785) | Error 0.5228(0.5431) Steps 0(0.00) | Grad Norm 12.1789(11.6429) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 58.8413(62.3658) | Bit/dim 3.9489(3.9863) | Xent 1.5026(1.5079) | Loss 11.7780(12.5545) | Error 0.5409(0.5430) Steps 0(0.00) | Grad Norm 15.8052(11.7678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 24.6060, Epoch Time 400.1013(414.3504), Bit/dim 3.9795(best: 3.9436), Xent 1.4752, Loss 4.7171, Error 0.5323(best: 0.4972)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 58.1224(62.2385) | Bit/dim 3.9843(3.9862) | Xent 1.5217(1.5083) | Loss 16.1803(12.6633) | Error 0.5511(0.5432) Steps 0(0.00) | Grad Norm 15.8957(11.8916) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 63.4662(62.2754) | Bit/dim 3.9401(3.9848) | Xent 1.4279(1.5059) | Loss 11.8383(12.6385) | Error 0.5200(0.5425) Steps 0(0.00) | Grad Norm 6.2635(11.7228) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 59.5308(62.1930) | Bit/dim 3.9303(3.9832) | Xent 1.4641(1.5047) | Loss 11.5327(12.6054) | Error 0.5400(0.5425) Steps 0(0.00) | Grad Norm 8.9785(11.6405) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 58.8289(62.0921) | Bit/dim 3.9364(3.9818) | Xent 1.4529(1.5031) | Loss 11.7617(12.5801) | Error 0.5199(0.5418) Steps 0(0.00) | Grad Norm 8.9154(11.5587) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 59.9202(62.0269) | Bit/dim 3.9230(3.9800) | Xent 1.4552(1.5017) | Loss 11.8215(12.5573) | Error 0.5189(0.5411) Steps 0(0.00) | Grad Norm 9.8537(11.5076) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 64.6364(62.1052) | Bit/dim 3.9373(3.9787) | Xent 1.4707(1.5008) | Loss 11.8028(12.5347) | Error 0.5340(0.5409) Steps 0(0.00) | Grad Norm 15.3620(11.6232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 23.8884, Epoch Time 404.2200(414.0465), Bit/dim 3.9277(best: 3.9436), Xent 1.4008, Loss 4.6281, Error 0.5036(best: 0.4972)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 58.5939(61.9999) | Bit/dim 3.9240(3.9771) | Xent 1.4242(1.4985) | Loss 15.8853(12.6352) | Error 0.5136(0.5401) Steps 0(0.00) | Grad Norm 10.5524(11.5911) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 54.7526(61.7825) | Bit/dim 3.9237(3.9755) | Xent 1.4337(1.4965) | Loss 11.5912(12.6039) | Error 0.5171(0.5394) Steps 0(0.00) | Grad Norm 7.8509(11.4789) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 59.1784(61.7043) | Bit/dim 3.9438(3.9745) | Xent 1.4302(1.4945) | Loss 11.4598(12.5695) | Error 0.5169(0.5387) Steps 0(0.00) | Grad Norm 10.1759(11.4398) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 62.2399(61.7204) | Bit/dim 3.9247(3.9730) | Xent 1.4237(1.4924) | Loss 11.8154(12.5469) | Error 0.5169(0.5380) Steps 0(0.00) | Grad Norm 8.4990(11.3516) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 60.6701(61.6889) | Bit/dim 3.9163(3.9713) | Xent 1.4273(1.4905) | Loss 11.7325(12.5225) | Error 0.5121(0.5373) Steps 0(0.00) | Grad Norm 7.5300(11.2369) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 57.5690(61.5653) | Bit/dim 3.9247(3.9699) | Xent 1.4204(1.4884) | Loss 11.7141(12.4982) | Error 0.5155(0.5366) Steps 0(0.00) | Grad Norm 9.6576(11.1895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 24.5534, Epoch Time 393.5007(413.4301), Bit/dim 3.9175(best: 3.9277), Xent 1.3707, Loss 4.6029, Error 0.4946(best: 0.4972)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 62.8015(61.6024) | Bit/dim 3.9137(3.9683) | Xent 1.4016(1.4857) | Loss 15.7536(12.5959) | Error 0.5024(0.5356) Steps 0(0.00) | Grad Norm 8.3558(11.1045) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 56.8808(61.4607) | Bit/dim 3.8986(3.9662) | Xent 1.4089(1.4834) | Loss 11.8525(12.5736) | Error 0.5085(0.5348) Steps 0(0.00) | Grad Norm 7.0284(10.9822) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 55.5959(61.2848) | Bit/dim 3.9125(3.9646) | Xent 1.4168(1.4814) | Loss 11.7471(12.5488) | Error 0.5134(0.5341) Steps 0(0.00) | Grad Norm 10.9557(10.9814) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 59.5408(61.2325) | Bit/dim 3.9335(3.9636) | Xent 1.4695(1.4811) | Loss 11.9539(12.5310) | Error 0.5329(0.5341) Steps 0(0.00) | Grad Norm 17.0808(11.1644) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 61.3466(61.2359) | Bit/dim 3.9298(3.9626) | Xent 1.5132(1.4821) | Loss 11.8025(12.5091) | Error 0.5475(0.5345) Steps 0(0.00) | Grad Norm 20.9796(11.4589) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 59.0750(61.1711) | Bit/dim 3.9210(3.9614) | Xent 1.4936(1.4824) | Loss 11.8277(12.4887) | Error 0.5365(0.5346) Steps 0(0.00) | Grad Norm 12.9444(11.5034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 23.8624, Epoch Time 394.6783(412.8676), Bit/dim 3.9187(best: 3.9175), Xent 1.4431, Loss 4.6403, Error 0.5194(best: 0.4946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 61.8301(61.1908) | Bit/dim 3.9109(3.9599) | Xent 1.4802(1.4823) | Loss 15.1521(12.5686) | Error 0.5323(0.5345) Steps 0(0.00) | Grad Norm 9.9215(11.4560) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 61.1812(61.1905) | Bit/dim 3.9185(3.9586) | Xent 1.5354(1.4839) | Loss 11.8749(12.5477) | Error 0.5497(0.5349) Steps 0(0.00) | Grad Norm 13.7945(11.5261) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 61.8501(61.2103) | Bit/dim 3.9155(3.9573) | Xent 1.5051(1.4846) | Loss 11.4434(12.5146) | Error 0.5376(0.5350) Steps 0(0.00) | Grad Norm 11.9067(11.5376) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 60.4244(61.1868) | Bit/dim 3.9279(3.9564) | Xent 1.5745(1.4873) | Loss 11.9432(12.4975) | Error 0.5665(0.5360) Steps 0(0.00) | Grad Norm 17.8797(11.7278) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 58.0120(61.0915) | Bit/dim 3.9732(3.9569) | Xent 1.5670(1.4896) | Loss 11.9500(12.4811) | Error 0.5669(0.5369) Steps 0(0.00) | Grad Norm 12.6470(11.7554) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 62.5284(61.1346) | Bit/dim 3.9560(3.9569) | Xent 1.5196(1.4905) | Loss 11.9483(12.4651) | Error 0.5499(0.5373) Steps 0(0.00) | Grad Norm 9.0064(11.6729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 23.6111, Epoch Time 405.6038(412.6497), Bit/dim 3.9681(best: 3.9175), Xent 1.4386, Loss 4.6874, Error 0.5162(best: 0.4946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 61.8483(61.1560) | Bit/dim 3.9617(3.9571) | Xent 1.4677(1.4899) | Loss 16.0781(12.5735) | Error 0.5268(0.5370) Steps 0(0.00) | Grad Norm 10.4950(11.6376) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 62.3697(61.1924) | Bit/dim 3.9653(3.9573) | Xent 1.5464(1.4916) | Loss 12.0835(12.5588) | Error 0.5563(0.5376) Steps 0(0.00) | Grad Norm 17.3032(11.8076) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 58.9814(61.1261) | Bit/dim 3.9694(3.9577) | Xent 1.7354(1.4989) | Loss 12.1942(12.5478) | Error 0.6105(0.5397) Steps 0(0.00) | Grad Norm 27.6949(12.2842) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 58.0418(61.0336) | Bit/dim 3.9764(3.9582) | Xent 1.4780(1.4982) | Loss 11.8843(12.5279) | Error 0.5323(0.5395) Steps 0(0.00) | Grad Norm 7.8723(12.1518) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 60.1412(61.0068) | Bit/dim 3.9686(3.9585) | Xent 1.5308(1.4992) | Loss 11.8805(12.5085) | Error 0.5485(0.5398) Steps 0(0.00) | Grad Norm 8.2870(12.0359) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 60.9791(61.0060) | Bit/dim 3.9781(3.9591) | Xent 1.4819(1.4987) | Loss 11.9820(12.4927) | Error 0.5371(0.5397) Steps 0(0.00) | Grad Norm 7.5650(11.9018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 24.8597, Epoch Time 403.9234(412.3879), Bit/dim 3.9528(best: 3.9175), Xent 1.4486, Loss 4.6771, Error 0.5271(best: 0.4946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 60.3329(60.9858) | Bit/dim 3.9566(3.9590) | Xent 1.4901(1.4984) | Loss 15.9749(12.5972) | Error 0.5389(0.5397) Steps 0(0.00) | Grad Norm 8.2253(11.7915) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 59.5764(60.9435) | Bit/dim 3.9566(3.9590) | Xent 1.4584(1.4972) | Loss 11.8449(12.5746) | Error 0.5249(0.5392) Steps 0(0.00) | Grad Norm 6.4571(11.6314) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 57.4351(60.8383) | Bit/dim 3.9349(3.9582) | Xent 1.4396(1.4955) | Loss 11.9121(12.5547) | Error 0.5181(0.5386) Steps 0(0.00) | Grad Norm 5.1221(11.4361) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 55.5903(60.6808) | Bit/dim 3.9332(3.9575) | Xent 1.4395(1.4938) | Loss 11.7593(12.5309) | Error 0.5204(0.5381) Steps 0(0.00) | Grad Norm 6.9593(11.3018) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 59.5029(60.6455) | Bit/dim 3.9491(3.9572) | Xent 1.4394(1.4922) | Loss 11.7333(12.5069) | Error 0.5172(0.5374) Steps 0(0.00) | Grad Norm 4.9745(11.1120) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 58.0694(60.5682) | Bit/dim 3.9235(3.9562) | Xent 1.4354(1.4905) | Loss 11.6579(12.4815) | Error 0.5221(0.5370) Steps 0(0.00) | Grad Norm 6.4102(10.9710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 24.2703, Epoch Time 390.6746(411.7365), Bit/dim 3.9166(best: 3.9175), Xent 1.3882, Loss 4.6107, Error 0.5023(best: 0.4946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 63.5494(60.6576) | Bit/dim 3.9152(3.9550) | Xent 1.4469(1.4892) | Loss 15.3512(12.5676) | Error 0.5159(0.5363) Steps 0(0.00) | Grad Norm 6.0637(10.8237) | Total Time 0.00(0.00)\n",
      "Iter 0524 | Time 63.1597(60.7327) | Bit/dim 3.9250(3.9541) | Xent 1.4089(1.4868) | Loss 11.4973(12.5354) | Error 0.5095(0.5355) Steps 0(0.00) | Grad Norm 5.3205(10.6587) | Total Time 0.00(0.00)\n",
      "Iter 0525 | Time 59.9266(60.7085) | Bit/dim 3.9096(3.9528) | Xent 1.4069(1.4844) | Loss 11.7167(12.5109) | Error 0.5076(0.5347) Steps 0(0.00) | Grad Norm 3.9918(10.4586) | Total Time 0.00(0.00)\n",
      "Iter 0526 | Time 58.3500(60.6378) | Bit/dim 3.8968(3.9511) | Xent 1.4086(1.4821) | Loss 11.6492(12.4850) | Error 0.5076(0.5339) Steps 0(0.00) | Grad Norm 5.1308(10.2988) | Total Time 0.00(0.00)\n",
      "Iter 0527 | Time 58.3507(60.5691) | Bit/dim 3.8962(3.9494) | Xent 1.3857(1.4792) | Loss 11.6874(12.4611) | Error 0.4931(0.5327) Steps 0(0.00) | Grad Norm 6.7049(10.1910) | Total Time 0.00(0.00)\n",
      "Iter 0528 | Time 62.1155(60.6155) | Bit/dim 3.9015(3.9480) | Xent 1.3997(1.4768) | Loss 11.6539(12.4369) | Error 0.5092(0.5320) Steps 0(0.00) | Grad Norm 8.1497(10.1298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 24.3714, Epoch Time 406.2748(411.5726), Bit/dim 3.8991(best: 3.9166), Xent 1.3448, Loss 4.5715, Error 0.4859(best: 0.4946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 62.6954(60.6779) | Bit/dim 3.9025(3.9466) | Xent 1.3754(1.4738) | Loss 16.0081(12.5440) | Error 0.4992(0.5310) Steps 0(0.00) | Grad Norm 4.7344(9.9679) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 61.4991(60.7026) | Bit/dim 3.8870(3.9449) | Xent 1.3945(1.4714) | Loss 11.4544(12.5113) | Error 0.5049(0.5302) Steps 0(0.00) | Grad Norm 3.3261(9.7686) | Total Time 0.00(0.00)\n",
      "Iter 0531 | Time 60.7228(60.7032) | Bit/dim 3.8953(3.9434) | Xent 1.3978(1.4692) | Loss 11.6801(12.4864) | Error 0.5026(0.5294) Steps 0(0.00) | Grad Norm 8.5878(9.7332) | Total Time 0.00(0.00)\n",
      "Iter 0532 | Time 57.4543(60.6057) | Bit/dim 3.8983(3.9420) | Xent 1.4010(1.4672) | Loss 11.6270(12.4606) | Error 0.5022(0.5286) Steps 0(0.00) | Grad Norm 15.5369(9.9073) | Total Time 0.00(0.00)\n",
      "Iter 0533 | Time 61.3723(60.6287) | Bit/dim 3.8923(3.9405) | Xent 1.5165(1.4686) | Loss 11.8383(12.4419) | Error 0.5326(0.5287) Steps 0(0.00) | Grad Norm 24.4895(10.3448) | Total Time 0.00(0.00)\n",
      "Iter 0534 | Time 61.5802(60.6572) | Bit/dim 3.9191(3.9399) | Xent 1.7286(1.4764) | Loss 11.8288(12.4236) | Error 0.6006(0.5308) Steps 0(0.00) | Grad Norm 27.9615(10.8733) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 24.7426, Epoch Time 406.1978(411.4114), Bit/dim 3.9486(best: 3.8991), Xent 1.9627, Loss 4.9300, Error 0.6901(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 64.3788(60.7689) | Bit/dim 3.9518(3.9402) | Xent 2.0177(1.4927) | Loss 16.4563(12.5445) | Error 0.6927(0.5357) Steps 0(0.00) | Grad Norm 33.0410(11.5383) | Total Time 0.00(0.00)\n",
      "Iter 0536 | Time 58.3271(60.6956) | Bit/dim 4.0623(3.9439) | Xent 1.7554(1.5006) | Loss 12.2835(12.5367) | Error 0.6329(0.5386) Steps 0(0.00) | Grad Norm 18.0977(11.7351) | Total Time 0.00(0.00)\n",
      "Iter 0537 | Time 57.2682(60.5928) | Bit/dim 3.9459(3.9440) | Xent 1.5752(1.5028) | Loss 11.9709(12.5197) | Error 0.5601(0.5393) Steps 0(0.00) | Grad Norm 9.9619(11.6819) | Total Time 0.00(0.00)\n",
      "Iter 0538 | Time 61.1501(60.6095) | Bit/dim 3.9688(3.9447) | Xent 1.5996(1.5057) | Loss 11.9544(12.5028) | Error 0.5515(0.5396) Steps 0(0.00) | Grad Norm 14.2673(11.7595) | Total Time 0.00(0.00)\n",
      "Iter 0539 | Time 57.0136(60.5017) | Bit/dim 4.0377(3.9475) | Xent 1.8061(1.5147) | Loss 12.4329(12.5007) | Error 0.6200(0.5420) Steps 0(0.00) | Grad Norm 18.3727(11.9579) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 58.8590(60.4524) | Bit/dim 4.0005(3.9491) | Xent 1.5975(1.5172) | Loss 12.1771(12.4910) | Error 0.5586(0.5425) Steps 0(0.00) | Grad Norm 12.7059(11.9803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 23.8560, Epoch Time 396.9162(410.9765), Bit/dim 3.9999(best: 3.8991), Xent 1.6004, Loss 4.8001, Error 0.5776(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 60.0872(60.4414) | Bit/dim 3.9947(3.9505) | Xent 1.6478(1.5211) | Loss 17.0438(12.6275) | Error 0.5934(0.5441) Steps 0(0.00) | Grad Norm 13.8659(12.0369) | Total Time 0.00(0.00)\n",
      "Iter 0542 | Time 58.1143(60.3716) | Bit/dim 3.9900(3.9516) | Xent 1.5602(1.5223) | Loss 11.7354(12.6008) | Error 0.5610(0.5446) Steps 0(0.00) | Grad Norm 7.9780(11.9151) | Total Time 0.00(0.00)\n",
      "Iter 0543 | Time 60.0171(60.3610) | Bit/dim 3.9954(3.9530) | Xent 1.5214(1.5223) | Loss 12.2509(12.5903) | Error 0.5445(0.5446) Steps 0(0.00) | Grad Norm 6.6978(11.7586) | Total Time 0.00(0.00)\n",
      "Iter 0544 | Time 62.6152(60.4286) | Bit/dim 3.9799(3.9538) | Xent 1.5229(1.5223) | Loss 12.2217(12.5792) | Error 0.5520(0.5448) Steps 0(0.00) | Grad Norm 5.2853(11.5644) | Total Time 0.00(0.00)\n",
      "Iter 0545 | Time 63.2253(60.5125) | Bit/dim 3.9764(3.9544) | Xent 1.5934(1.5244) | Loss 12.0575(12.5636) | Error 0.5677(0.5455) Steps 0(0.00) | Grad Norm 10.8279(11.5423) | Total Time 0.00(0.00)\n",
      "Iter 0546 | Time 61.5840(60.5446) | Bit/dim 3.9459(3.9542) | Xent 1.5380(1.5248) | Loss 12.0443(12.5480) | Error 0.5530(0.5457) Steps 0(0.00) | Grad Norm 8.8803(11.4624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 23.9065, Epoch Time 405.9100(410.8245), Bit/dim 3.9423(best: 3.8991), Xent 1.4756, Loss 4.6800, Error 0.5255(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 60.3641(60.5392) | Bit/dim 3.9299(3.9535) | Xent 1.5046(1.5242) | Loss 16.0058(12.6517) | Error 0.5379(0.5455) Steps 0(0.00) | Grad Norm 7.7286(11.3504) | Total Time 0.00(0.00)\n",
      "Iter 0548 | Time 61.8098(60.5773) | Bit/dim 3.9340(3.9529) | Xent 1.5081(1.5237) | Loss 11.7125(12.6236) | Error 0.5344(0.5451) Steps 0(0.00) | Grad Norm 4.6814(11.1504) | Total Time 0.00(0.00)\n",
      "Iter 0549 | Time 58.5609(60.5169) | Bit/dim 3.9518(3.9528) | Xent 1.5201(1.5236) | Loss 11.8202(12.5995) | Error 0.5434(0.5451) Steps 0(0.00) | Grad Norm 8.6608(11.0757) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 66.2404(60.6886) | Bit/dim 3.9281(3.9521) | Xent 1.4862(1.5225) | Loss 11.7051(12.5726) | Error 0.5389(0.5449) Steps 0(0.00) | Grad Norm 6.1408(10.9276) | Total Time 0.00(0.00)\n",
      "Iter 0551 | Time 60.1733(60.6731) | Bit/dim 3.9260(3.9513) | Xent 1.4450(1.5202) | Loss 11.8517(12.5510) | Error 0.5185(0.5441) Steps 0(0.00) | Grad Norm 3.2936(10.6986) | Total Time 0.00(0.00)\n",
      "Iter 0552 | Time 58.3305(60.6028) | Bit/dim 3.9265(3.9506) | Xent 1.4665(1.5186) | Loss 11.8618(12.5303) | Error 0.5275(0.5436) Steps 0(0.00) | Grad Norm 7.9789(10.6170) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 24.7307, Epoch Time 406.3456(410.6902), Bit/dim 3.9160(best: 3.8991), Xent 1.4367, Loss 4.6343, Error 0.5200(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 58.5485(60.5412) | Bit/dim 3.9142(3.9495) | Xent 1.4792(1.5174) | Loss 15.9085(12.6317) | Error 0.5336(0.5433) Steps 0(0.00) | Grad Norm 11.7569(10.6512) | Total Time 0.00(0.00)\n",
      "Iter 0554 | Time 60.5573(60.5417) | Bit/dim 3.9194(3.9486) | Xent 1.5508(1.5184) | Loss 12.0279(12.6136) | Error 0.5537(0.5436) Steps 0(0.00) | Grad Norm 18.4218(10.8843) | Total Time 0.00(0.00)\n",
      "Iter 0555 | Time 54.0798(60.3478) | Bit/dim 3.9359(3.9482) | Xent 1.4963(1.5177) | Loss 11.9160(12.5926) | Error 0.5408(0.5435) Steps 0(0.00) | Grad Norm 17.0192(11.0684) | Total Time 0.00(0.00)\n",
      "Iter 0556 | Time 66.0192(60.5180) | Bit/dim 3.9126(3.9471) | Xent 1.4615(1.5160) | Loss 11.7588(12.5676) | Error 0.5269(0.5430) Steps 0(0.00) | Grad Norm 8.0499(10.9778) | Total Time 0.00(0.00)\n",
      "Iter 0557 | Time 58.7085(60.4637) | Bit/dim 3.9254(3.9465) | Xent 1.5076(1.5158) | Loss 11.4765(12.5349) | Error 0.5356(0.5428) Steps 0(0.00) | Grad Norm 11.9637(11.0074) | Total Time 0.00(0.00)\n",
      "Iter 0558 | Time 59.6819(60.4402) | Bit/dim 3.9201(3.9457) | Xent 1.5408(1.5165) | Loss 11.9042(12.5160) | Error 0.5574(0.5432) Steps 0(0.00) | Grad Norm 15.0559(11.1288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 24.3021, Epoch Time 398.1307(410.3134), Bit/dim 3.9410(best: 3.8991), Xent 1.5176, Loss 4.6998, Error 0.5496(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 58.8032(60.3911) | Bit/dim 3.9405(3.9455) | Xent 1.5729(1.5182) | Loss 16.1391(12.6247) | Error 0.5635(0.5439) Steps 0(0.00) | Grad Norm 12.7052(11.1761) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 59.0236(60.3501) | Bit/dim 3.8929(3.9439) | Xent 1.4901(1.5174) | Loss 11.7460(12.5983) | Error 0.5348(0.5436) Steps 0(0.00) | Grad Norm 6.7173(11.0424) | Total Time 0.00(0.00)\n",
      "Iter 0561 | Time 64.1881(60.4652) | Bit/dim 3.9308(3.9436) | Xent 1.4904(1.5166) | Loss 11.8513(12.5759) | Error 0.5340(0.5433) Steps 0(0.00) | Grad Norm 9.6601(11.0009) | Total Time 0.00(0.00)\n",
      "Iter 0562 | Time 56.0354(60.3323) | Bit/dim 3.8991(3.9422) | Xent 1.4181(1.5136) | Loss 11.5547(12.5453) | Error 0.5129(0.5424) Steps 0(0.00) | Grad Norm 5.0343(10.8219) | Total Time 0.00(0.00)\n",
      "Iter 0563 | Time 59.6528(60.3119) | Bit/dim 3.9045(3.9411) | Xent 1.4583(1.5120) | Loss 11.8418(12.5241) | Error 0.5246(0.5418) Steps 0(0.00) | Grad Norm 8.7533(10.7598) | Total Time 0.00(0.00)\n",
      "Iter 0564 | Time 59.0748(60.2748) | Bit/dim 3.8975(3.9398) | Xent 1.4532(1.5102) | Loss 11.7719(12.5016) | Error 0.5252(0.5414) Steps 0(0.00) | Grad Norm 9.3605(10.7179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 24.1849, Epoch Time 396.9842(409.9135), Bit/dim 3.9075(best: 3.8991), Xent 1.3928, Loss 4.6039, Error 0.4991(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 65.5411(60.4328) | Bit/dim 3.9040(3.9387) | Xent 1.4283(1.5077) | Loss 15.7759(12.5998) | Error 0.5136(0.5405) Steps 0(0.00) | Grad Norm 10.9204(10.7239) | Total Time 0.00(0.00)\n",
      "Iter 0566 | Time 64.0823(60.5423) | Bit/dim 3.8815(3.9370) | Xent 1.4437(1.5058) | Loss 11.6551(12.5715) | Error 0.5189(0.5399) Steps 0(0.00) | Grad Norm 12.2404(10.7694) | Total Time 0.00(0.00)\n",
      "Iter 0567 | Time 62.9483(60.6145) | Bit/dim 3.8768(3.9352) | Xent 1.4354(1.5037) | Loss 11.6971(12.5452) | Error 0.5216(0.5393) Steps 0(0.00) | Grad Norm 10.1139(10.7498) | Total Time 0.00(0.00)\n",
      "Iter 0568 | Time 57.8707(60.5322) | Bit/dim 3.8800(3.9335) | Xent 1.4015(1.5006) | Loss 11.6191(12.5175) | Error 0.5045(0.5383) Steps 0(0.00) | Grad Norm 4.0427(10.5486) | Total Time 0.00(0.00)\n",
      "Iter 0569 | Time 62.7827(60.5997) | Bit/dim 3.8769(3.9318) | Xent 1.4123(1.4980) | Loss 11.5922(12.4897) | Error 0.5134(0.5375) Steps 0(0.00) | Grad Norm 7.7124(10.4635) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 60.3626(60.5926) | Bit/dim 3.8831(3.9304) | Xent 1.4042(1.4952) | Loss 11.6310(12.4639) | Error 0.5046(0.5365) Steps 0(0.00) | Grad Norm 8.7681(10.4126) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 25.0161, Epoch Time 414.1918(410.0419), Bit/dim 3.8685(best: 3.8991), Xent 1.3462, Loss 4.5416, Error 0.4850(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 63.3878(60.6764) | Bit/dim 3.8624(3.9283) | Xent 1.3949(1.4922) | Loss 16.0067(12.5702) | Error 0.5006(0.5355) Steps 0(0.00) | Grad Norm 4.9897(10.2499) | Total Time 0.00(0.00)\n",
      "Iter 0572 | Time 62.0834(60.7186) | Bit/dim 3.8637(3.9264) | Xent 1.3757(1.4887) | Loss 11.7018(12.5442) | Error 0.4974(0.5343) Steps 0(0.00) | Grad Norm 3.5384(10.0486) | Total Time 0.00(0.00)\n",
      "Iter 0573 | Time 64.7761(60.8404) | Bit/dim 3.8585(3.9244) | Xent 1.3780(1.4854) | Loss 11.3656(12.5088) | Error 0.5019(0.5333) Steps 0(0.00) | Grad Norm 6.4252(9.9399) | Total Time 0.00(0.00)\n",
      "Iter 0574 | Time 61.9510(60.8737) | Bit/dim 3.8528(3.9222) | Xent 1.3948(1.4826) | Loss 11.1900(12.4692) | Error 0.5019(0.5324) Steps 0(0.00) | Grad Norm 8.4836(9.8962) | Total Time 0.00(0.00)\n",
      "Iter 0575 | Time 59.7413(60.8397) | Bit/dim 3.8665(3.9205) | Xent 1.4209(1.4808) | Loss 11.7557(12.4478) | Error 0.5090(0.5317) Steps 0(0.00) | Grad Norm 13.0693(9.9914) | Total Time 0.00(0.00)\n",
      "Iter 0576 | Time 54.8416(60.6598) | Bit/dim 3.8888(3.9196) | Xent 1.6554(1.4860) | Loss 11.8630(12.4303) | Error 0.5793(0.5331) Steps 0(0.00) | Grad Norm 26.7541(10.4943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 24.1838, Epoch Time 406.8519(409.9462), Bit/dim 3.9251(best: 3.8685), Xent 1.5767, Loss 4.7134, Error 0.5741(best: 0.4850)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 64.3265(60.7698) | Bit/dim 3.9316(3.9199) | Xent 1.6216(1.4901) | Loss 15.8976(12.5343) | Error 0.5821(0.5346) Steps 0(0.00) | Grad Norm 23.5758(10.8867) | Total Time 0.00(0.00)\n",
      "Iter 0578 | Time 58.0913(60.6894) | Bit/dim 3.8867(3.9189) | Xent 1.5770(1.4927) | Loss 11.6323(12.5073) | Error 0.5726(0.5357) Steps 0(0.00) | Grad Norm 12.8267(10.9449) | Total Time 0.00(0.00)\n",
      "Iter 0579 | Time 57.2453(60.5861) | Bit/dim 3.8992(3.9184) | Xent 1.6346(1.4970) | Loss 11.8222(12.4867) | Error 0.5885(0.5373) Steps 0(0.00) | Grad Norm 13.9149(11.0340) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 56.5357(60.4646) | Bit/dim 3.9537(3.9194) | Xent 1.5120(1.4974) | Loss 12.0941(12.4749) | Error 0.5513(0.5377) Steps 0(0.00) | Grad Norm 13.7200(11.1146) | Total Time 0.00(0.00)\n",
      "Iter 0581 | Time 56.2243(60.3374) | Bit/dim 3.9553(3.9205) | Xent 1.5295(1.4984) | Loss 11.9519(12.4592) | Error 0.5514(0.5381) Steps 0(0.00) | Grad Norm 11.5104(11.1265) | Total Time 0.00(0.00)\n",
      "Iter 0582 | Time 58.9177(60.2948) | Bit/dim 3.8994(3.9199) | Xent 1.4710(1.4975) | Loss 11.6845(12.4360) | Error 0.5396(0.5382) Steps 0(0.00) | Grad Norm 7.2811(11.0111) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 24.7900, Epoch Time 392.1406(409.4120), Bit/dim 3.9274(best: 3.8685), Xent 1.3943, Loss 4.6245, Error 0.5052(best: 0.4850)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 62.7843(60.3695) | Bit/dim 3.9295(3.9201) | Xent 1.4583(1.4964) | Loss 16.1432(12.5472) | Error 0.5211(0.5377) Steps 0(0.00) | Grad Norm 8.9594(10.9495) | Total Time 0.00(0.00)\n",
      "Iter 0584 | Time 59.0065(60.3286) | Bit/dim 3.9086(3.9198) | Xent 1.4243(1.4942) | Loss 11.5452(12.5171) | Error 0.5130(0.5369) Steps 0(0.00) | Grad Norm 8.5128(10.8764) | Total Time 0.00(0.00)\n",
      "Iter 0585 | Time 53.7848(60.1323) | Bit/dim 3.9256(3.9200) | Xent 1.3927(1.4912) | Loss 11.6218(12.4903) | Error 0.4999(0.5358) Steps 0(0.00) | Grad Norm 4.7089(10.6914) | Total Time 0.00(0.00)\n",
      "Iter 0586 | Time 64.1173(60.2518) | Bit/dim 3.9082(3.9196) | Xent 1.4381(1.4896) | Loss 11.5745(12.4628) | Error 0.5226(0.5354) Steps 0(0.00) | Grad Norm 8.2099(10.6170) | Total Time 0.00(0.00)\n",
      "Iter 0587 | Time 64.1511(60.3688) | Bit/dim 3.9008(3.9191) | Xent 1.4079(1.4871) | Loss 11.3857(12.4305) | Error 0.5085(0.5346) Steps 0(0.00) | Grad Norm 8.8255(10.5632) | Total Time 0.00(0.00)\n",
      "Iter 0588 | Time 58.7341(60.3198) | Bit/dim 3.8991(3.9185) | Xent 1.4234(1.4852) | Loss 11.6657(12.4076) | Error 0.5188(0.5341) Steps 0(0.00) | Grad Norm 11.0210(10.5770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 24.3691, Epoch Time 403.0186(409.2202), Bit/dim 3.8895(best: 3.8685), Xent 1.4159, Loss 4.5974, Error 0.5070(best: 0.4850)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 62.9666(60.3992) | Bit/dim 3.9026(3.9180) | Xent 1.4632(1.4845) | Loss 15.8870(12.5119) | Error 0.5202(0.5337) Steps 0(0.00) | Grad Norm 14.6538(10.6993) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 60.1460(60.3916) | Bit/dim 3.8787(3.9168) | Xent 1.4563(1.4837) | Loss 11.7050(12.4877) | Error 0.5311(0.5337) Steps 0(0.00) | Grad Norm 13.5914(10.7860) | Total Time 0.00(0.00)\n",
      "Iter 0591 | Time 60.1629(60.3847) | Bit/dim 3.8793(3.9157) | Xent 1.4010(1.4812) | Loss 11.6723(12.4633) | Error 0.5101(0.5329) Steps 0(0.00) | Grad Norm 4.9144(10.6099) | Total Time 0.00(0.00)\n",
      "Iter 0592 | Time 61.9935(60.4330) | Bit/dim 3.8799(3.9146) | Xent 1.4486(1.4802) | Loss 11.5040(12.4345) | Error 0.5251(0.5327) Steps 0(0.00) | Grad Norm 9.1908(10.5673) | Total Time 0.00(0.00)\n",
      "Iter 0593 | Time 62.9657(60.5089) | Bit/dim 3.8573(3.9129) | Xent 1.4084(1.4781) | Loss 11.8006(12.4155) | Error 0.5066(0.5319) Steps 0(0.00) | Grad Norm 6.0936(10.4331) | Total Time 0.00(0.00)\n",
      "Iter 0594 | Time 60.2480(60.5011) | Bit/dim 3.8645(3.9114) | Xent 1.4014(1.4758) | Loss 11.3329(12.3830) | Error 0.5080(0.5312) Steps 0(0.00) | Grad Norm 6.4546(10.3137) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 25.0172, Epoch Time 409.3807(409.2250), Bit/dim 3.8604(best: 3.8685), Xent 1.3606, Loss 4.5407, Error 0.4971(best: 0.4850)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 61.5671(60.5331) | Bit/dim 3.8438(3.9094) | Xent 1.4089(1.4738) | Loss 16.0363(12.4926) | Error 0.5094(0.5306) Steps 0(0.00) | Grad Norm 7.3351(10.2244) | Total Time 0.00(0.00)\n",
      "Iter 0596 | Time 63.4636(60.6210) | Bit/dim 3.8520(3.9077) | Xent 1.4008(1.4716) | Loss 11.5911(12.4655) | Error 0.5144(0.5301) Steps 0(0.00) | Grad Norm 4.8297(10.0625) | Total Time 0.00(0.00)\n",
      "Iter 0597 | Time 59.7573(60.5951) | Bit/dim 3.8565(3.9061) | Xent 1.3566(1.4681) | Loss 11.3820(12.4330) | Error 0.4920(0.5289) Steps 0(0.00) | Grad Norm 6.5964(9.9586) | Total Time 0.00(0.00)\n",
      "Iter 0598 | Time 59.7042(60.5684) | Bit/dim 3.8599(3.9048) | Xent 1.3706(1.4652) | Loss 11.4100(12.4023) | Error 0.4896(0.5278) Steps 0(0.00) | Grad Norm 5.9725(9.8390) | Total Time 0.00(0.00)\n",
      "Iter 0599 | Time 61.5092(60.5966) | Bit/dim 3.8562(3.9033) | Xent 1.3473(1.4617) | Loss 11.5874(12.3779) | Error 0.4858(0.5265) Steps 0(0.00) | Grad Norm 7.2011(9.7598) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 64.0104(60.6990) | Bit/dim 3.8519(3.9018) | Xent 1.3810(1.4593) | Loss 11.4369(12.3497) | Error 0.4981(0.5256) Steps 0(0.00) | Grad Norm 7.3390(9.6872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 24.2555, Epoch Time 411.1445(409.2826), Bit/dim 3.8478(best: 3.8604), Xent 1.3231, Loss 4.5094, Error 0.4816(best: 0.4850)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 64.0438(60.7994) | Bit/dim 3.8616(3.9006) | Xent 1.3720(1.4566) | Loss 15.6082(12.4474) | Error 0.4951(0.5247) Steps 0(0.00) | Grad Norm 6.7783(9.5999) | Total Time 0.00(0.00)\n",
      "Iter 0602 | Time 56.9307(60.6833) | Bit/dim 3.8320(3.8985) | Xent 1.3326(1.4529) | Loss 11.3542(12.4146) | Error 0.4784(0.5233) Steps 0(0.00) | Grad Norm 4.1973(9.4379) | Total Time 0.00(0.00)\n",
      "Iter 0603 | Time 61.3881(60.7044) | Bit/dim 3.8263(3.8963) | Xent 1.3413(1.4496) | Loss 11.3989(12.3842) | Error 0.4818(0.5221) Steps 0(0.00) | Grad Norm 5.8267(9.3295) | Total Time 0.00(0.00)\n",
      "Iter 0604 | Time 63.9524(60.8019) | Bit/dim 3.8273(3.8943) | Xent 1.3829(1.4476) | Loss 11.6304(12.3615) | Error 0.5009(0.5215) Steps 0(0.00) | Grad Norm 9.2186(9.3262) | Total Time 0.00(0.00)\n",
      "Iter 0605 | Time 64.4233(60.9105) | Bit/dim 3.8253(3.8922) | Xent 1.3811(1.4456) | Loss 11.2904(12.3294) | Error 0.5002(0.5208) Steps 0(0.00) | Grad Norm 9.6094(9.3347) | Total Time 0.00(0.00)\n",
      "Iter 0606 | Time 63.3436(60.9835) | Bit/dim 3.8383(3.8906) | Xent 1.3938(1.4440) | Loss 11.4630(12.3034) | Error 0.5017(0.5202) Steps 0(0.00) | Grad Norm 8.3032(9.3038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 24.7126, Epoch Time 414.8482(409.4496), Bit/dim 3.8305(best: 3.8478), Xent 1.3050, Loss 4.4830, Error 0.4735(best: 0.4816)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 62.3178(61.0235) | Bit/dim 3.8248(3.8886) | Xent 1.3485(1.4412) | Loss 15.5858(12.4019) | Error 0.4831(0.5191) Steps 0(0.00) | Grad Norm 4.7748(9.1679) | Total Time 0.00(0.00)\n",
      "Iter 0608 | Time 63.0776(61.0852) | Bit/dim 3.8189(3.8865) | Xent 1.3360(1.4380) | Loss 11.4304(12.3727) | Error 0.4868(0.5182) Steps 0(0.00) | Grad Norm 3.3400(8.9931) | Total Time 0.00(0.00)\n",
      "Iter 0609 | Time 60.3893(61.0643) | Bit/dim 3.8268(3.8847) | Xent 1.3416(1.4351) | Loss 11.4644(12.3455) | Error 0.4814(0.5171) Steps 0(0.00) | Grad Norm 5.8568(8.8990) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 58.2147(60.9788) | Bit/dim 3.8289(3.8830) | Xent 1.3468(1.4325) | Loss 11.2029(12.3112) | Error 0.4810(0.5160) Steps 0(0.00) | Grad Norm 8.8136(8.8964) | Total Time 0.00(0.00)\n",
      "Iter 0611 | Time 65.0377(61.1006) | Bit/dim 3.8168(3.8811) | Xent 1.3918(1.4312) | Loss 11.2698(12.2800) | Error 0.5014(0.5155) Steps 0(0.00) | Grad Norm 13.4827(9.0340) | Total Time 0.00(0.00)\n",
      "Iter 0612 | Time 60.1305(61.0715) | Bit/dim 3.8321(3.8796) | Xent 1.4532(1.4319) | Loss 11.3902(12.2533) | Error 0.5250(0.5158) Steps 0(0.00) | Grad Norm 17.4787(9.2873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 24.9837, Epoch Time 410.5187(409.4816), Bit/dim 3.8523(best: 3.8305), Xent 1.5103, Loss 4.6074, Error 0.5509(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 57.2873(60.9579) | Bit/dim 3.8470(3.8786) | Xent 1.5420(1.4352) | Loss 15.9725(12.3649) | Error 0.5539(0.5170) Steps 0(0.00) | Grad Norm 18.9397(9.5769) | Total Time 0.00(0.00)\n",
      "Iter 0614 | Time 61.9237(60.9869) | Bit/dim 3.9434(3.8806) | Xent 1.4652(1.4361) | Loss 11.9951(12.3538) | Error 0.5271(0.5173) Steps 0(0.00) | Grad Norm 17.0384(9.8007) | Total Time 0.00(0.00)\n",
      "Iter 0615 | Time 61.9026(61.0144) | Bit/dim 3.8749(3.8804) | Xent 1.5467(1.4394) | Loss 11.6704(12.3333) | Error 0.5641(0.5187) Steps 0(0.00) | Grad Norm 14.8432(9.9520) | Total Time 0.00(0.00)\n",
      "Iter 0616 | Time 55.6774(60.8543) | Bit/dim 3.8716(3.8801) | Xent 1.5457(1.4426) | Loss 11.8403(12.3185) | Error 0.5551(0.5198) Steps 0(0.00) | Grad Norm 10.5453(9.9698) | Total Time 0.00(0.00)\n",
      "Iter 0617 | Time 63.9708(60.9478) | Bit/dim 3.9178(3.8813) | Xent 1.5725(1.4465) | Loss 12.0178(12.3095) | Error 0.5673(0.5212) Steps 0(0.00) | Grad Norm 13.3593(10.0715) | Total Time 0.00(0.00)\n",
      "Iter 0618 | Time 57.8871(60.8559) | Bit/dim 3.8956(3.8817) | Xent 1.4347(1.4461) | Loss 11.8025(12.2943) | Error 0.5229(0.5212) Steps 0(0.00) | Grad Norm 6.7700(9.9725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 24.3553, Epoch Time 399.3626(409.1781), Bit/dim 3.9108(best: 3.8305), Xent 1.3978, Loss 4.6097, Error 0.5121(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 56.4294(60.7232) | Bit/dim 3.9165(3.8827) | Xent 1.4370(1.4459) | Loss 15.9787(12.4048) | Error 0.5256(0.5214) Steps 0(0.00) | Grad Norm 8.2681(9.9213) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 62.9421(60.7897) | Bit/dim 3.8972(3.8832) | Xent 1.4490(1.4460) | Loss 11.7531(12.3852) | Error 0.5259(0.5215) Steps 0(0.00) | Grad Norm 9.0196(9.8943) | Total Time 0.00(0.00)\n",
      "Iter 0621 | Time 62.5395(60.8422) | Bit/dim 3.8903(3.8834) | Xent 1.4196(1.4452) | Loss 11.8392(12.3689) | Error 0.5130(0.5213) Steps 0(0.00) | Grad Norm 10.8746(9.9237) | Total Time 0.00(0.00)\n",
      "Iter 0622 | Time 59.4791(60.8013) | Bit/dim 3.9213(3.8845) | Xent 1.4504(1.4453) | Loss 11.5359(12.3439) | Error 0.5239(0.5213) Steps 0(0.00) | Grad Norm 14.1456(10.0503) | Total Time 0.00(0.00)\n",
      "Iter 0623 | Time 60.9412(60.8055) | Bit/dim 3.8987(3.8849) | Xent 1.4509(1.4455) | Loss 11.5731(12.3207) | Error 0.5329(0.5217) Steps 0(0.00) | Grad Norm 12.0661(10.1108) | Total Time 0.00(0.00)\n",
      "Iter 0624 | Time 59.5146(60.7668) | Bit/dim 3.8536(3.8840) | Xent 1.3860(1.4437) | Loss 11.6580(12.3009) | Error 0.5015(0.5211) Steps 0(0.00) | Grad Norm 7.0083(10.0177) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 25.1302, Epoch Time 403.1268(408.9965), Bit/dim 3.8950(best: 3.8305), Xent 1.3481, Loss 4.5690, Error 0.4905(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 61.3654(60.7847) | Bit/dim 3.8907(3.8842) | Xent 1.3896(1.4421) | Loss 16.0828(12.4143) | Error 0.5115(0.5208) Steps 0(0.00) | Grad Norm 8.7438(9.9795) | Total Time 0.00(0.00)\n",
      "Iter 0626 | Time 60.7256(60.7830) | Bit/dim 3.8474(3.8831) | Xent 1.4184(1.4414) | Loss 11.6641(12.3918) | Error 0.5164(0.5207) Steps 0(0.00) | Grad Norm 8.2949(9.9290) | Total Time 0.00(0.00)\n",
      "Iter 0627 | Time 60.7688(60.7825) | Bit/dim 3.8730(3.8828) | Xent 1.3939(1.4400) | Loss 11.6771(12.3704) | Error 0.5040(0.5202) Steps 0(0.00) | Grad Norm 9.2613(9.9090) | Total Time 0.00(0.00)\n",
      "Iter 0628 | Time 57.7190(60.6906) | Bit/dim 3.8723(3.8825) | Xent 1.3426(1.4370) | Loss 11.5860(12.3468) | Error 0.4865(0.5191) Steps 0(0.00) | Grad Norm 7.8075(9.8459) | Total Time 0.00(0.00)\n",
      "Iter 0629 | Time 59.6992(60.6609) | Bit/dim 3.8473(3.8814) | Xent 1.3619(1.4348) | Loss 11.3808(12.3179) | Error 0.4935(0.5184) Steps 0(0.00) | Grad Norm 4.7200(9.6921) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 61.9827(60.7006) | Bit/dim 3.8616(3.8808) | Xent 1.3615(1.4326) | Loss 11.5652(12.2953) | Error 0.4900(0.5175) Steps 0(0.00) | Grad Norm 6.9418(9.6096) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 24.6778, Epoch Time 402.9544(408.8153), Bit/dim 3.8503(best: 3.8305), Xent 1.3113, Loss 4.5059, Error 0.4755(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 63.5833(60.7870) | Bit/dim 3.8400(3.8796) | Xent 1.3431(1.4299) | Loss 15.5787(12.3938) | Error 0.4770(0.5163) Steps 0(0.00) | Grad Norm 8.3617(9.5722) | Total Time 0.00(0.00)\n",
      "Iter 0632 | Time 62.4205(60.8360) | Bit/dim 3.8474(3.8786) | Xent 1.3651(1.4280) | Loss 11.4072(12.3642) | Error 0.4926(0.5156) Steps 0(0.00) | Grad Norm 8.0739(9.5272) | Total Time 0.00(0.00)\n",
      "Iter 0633 | Time 62.1939(60.8768) | Bit/dim 3.8360(3.8774) | Xent 1.3456(1.4255) | Loss 11.4803(12.3377) | Error 0.4910(0.5149) Steps 0(0.00) | Grad Norm 7.7306(9.4733) | Total Time 0.00(0.00)\n",
      "Iter 0634 | Time 58.7124(60.8118) | Bit/dim 3.8487(3.8765) | Xent 1.3841(1.4242) | Loss 11.7064(12.3187) | Error 0.5011(0.5144) Steps 0(0.00) | Grad Norm 9.0981(9.4621) | Total Time 0.00(0.00)\n",
      "Iter 0635 | Time 60.3359(60.7976) | Bit/dim 3.8285(3.8751) | Xent 1.3641(1.4224) | Loss 11.3556(12.2898) | Error 0.4882(0.5137) Steps 0(0.00) | Grad Norm 9.3501(9.4587) | Total Time 0.00(0.00)\n",
      "Iter 0636 | Time 62.6648(60.8536) | Bit/dim 3.8291(3.8737) | Xent 1.3439(1.4201) | Loss 11.4862(12.2657) | Error 0.4816(0.5127) Steps 0(0.00) | Grad Norm 7.8492(9.4104) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 25.3806, Epoch Time 411.0489(408.8823), Bit/dim 3.8261(best: 3.8305), Xent 1.2992, Loss 4.4757, Error 0.4739(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 65.4382(60.9911) | Bit/dim 3.8228(3.8722) | Xent 1.3332(1.4175) | Loss 15.5886(12.3654) | Error 0.4898(0.5120) Steps 0(0.00) | Grad Norm 6.2649(9.3161) | Total Time 0.00(0.00)\n",
      "Iter 0638 | Time 61.6951(61.0122) | Bit/dim 3.8193(3.8706) | Xent 1.3385(1.4151) | Loss 11.5235(12.3402) | Error 0.4816(0.5111) Steps 0(0.00) | Grad Norm 7.2646(9.2545) | Total Time 0.00(0.00)\n",
      "Iter 0639 | Time 59.9622(60.9807) | Bit/dim 3.8135(3.8689) | Xent 1.3385(1.4128) | Loss 11.4328(12.3129) | Error 0.4811(0.5102) Steps 0(0.00) | Grad Norm 7.2016(9.1929) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 61.7622(61.0042) | Bit/dim 3.8153(3.8672) | Xent 1.3375(1.4105) | Loss 11.4181(12.2861) | Error 0.4846(0.5094) Steps 0(0.00) | Grad Norm 6.0851(9.0997) | Total Time 0.00(0.00)\n",
      "Iter 0641 | Time 67.5466(61.2005) | Bit/dim 3.8203(3.8658) | Xent 1.3359(1.4083) | Loss 11.2517(12.2551) | Error 0.4792(0.5085) Steps 0(0.00) | Grad Norm 6.8309(9.0316) | Total Time 0.00(0.00)\n",
      "Iter 0642 | Time 63.5071(61.2697) | Bit/dim 3.8080(3.8641) | Xent 1.3212(1.4057) | Loss 11.3651(12.2284) | Error 0.4818(0.5077) Steps 0(0.00) | Grad Norm 4.7156(8.9022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 24.8736, Epoch Time 421.1566(409.2505), Bit/dim 3.8114(best: 3.8261), Xent 1.2710, Loss 4.4469, Error 0.4587(best: 0.4735)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 62.4338(61.3046) | Bit/dim 3.8163(3.8627) | Xent 1.3069(1.4027) | Loss 16.1100(12.3448) | Error 0.4759(0.5068) Steps 0(0.00) | Grad Norm 3.3757(8.7364) | Total Time 0.00(0.00)\n",
      "Iter 0644 | Time 62.9983(61.3554) | Bit/dim 3.8143(3.8612) | Xent 1.3246(1.4004) | Loss 11.3221(12.3141) | Error 0.4734(0.5058) Steps 0(0.00) | Grad Norm 6.8721(8.6804) | Total Time 0.00(0.00)\n",
      "Iter 0645 | Time 65.0884(61.4674) | Bit/dim 3.8153(3.8598) | Xent 1.3473(1.3988) | Loss 11.3484(12.2852) | Error 0.4875(0.5052) Steps 0(0.00) | Grad Norm 10.2104(8.7263) | Total Time 0.00(0.00)\n",
      "Iter 0646 | Time 63.1661(61.5183) | Bit/dim 3.7946(3.8579) | Xent 1.3738(1.3980) | Loss 11.4074(12.2588) | Error 0.4908(0.5048) Steps 0(0.00) | Grad Norm 13.2144(8.8610) | Total Time 0.00(0.00)\n",
      "Iter 0647 | Time 60.9616(61.5016) | Bit/dim 3.8170(3.8567) | Xent 1.4340(1.3991) | Loss 11.5283(12.2369) | Error 0.5108(0.5050) Steps 0(0.00) | Grad Norm 16.7243(9.0969) | Total Time 0.00(0.00)\n",
      "Iter 0648 | Time 56.9901(61.3663) | Bit/dim 3.8119(3.8553) | Xent 1.5028(1.4022) | Loss 11.5414(12.2160) | Error 0.5373(0.5059) Steps 0(0.00) | Grad Norm 17.5411(9.3502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 25.6719, Epoch Time 413.6063(409.3812), Bit/dim 3.8673(best: 3.8114), Xent 1.4081, Loss 4.5713, Error 0.5107(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 62.5937(61.4031) | Bit/dim 3.8663(3.8556) | Xent 1.4574(1.4039) | Loss 15.7576(12.3223) | Error 0.5315(0.5067) Steps 0(0.00) | Grad Norm 14.6791(9.5101) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 60.1176(61.3645) | Bit/dim 3.8861(3.8566) | Xent 1.4875(1.4064) | Loss 11.5637(12.2995) | Error 0.5361(0.5076) Steps 0(0.00) | Grad Norm 15.2250(9.6815) | Total Time 0.00(0.00)\n",
      "Iter 0651 | Time 59.2063(61.2998) | Bit/dim 3.8651(3.8568) | Xent 1.4898(1.4089) | Loss 11.8014(12.2846) | Error 0.5404(0.5086) Steps 0(0.00) | Grad Norm 9.0630(9.6630) | Total Time 0.00(0.00)\n",
      "Iter 0652 | Time 63.7375(61.3729) | Bit/dim 3.8895(3.8578) | Xent 1.4847(1.4112) | Loss 11.8610(12.2719) | Error 0.5355(0.5094) Steps 0(0.00) | Grad Norm 9.7190(9.6646) | Total Time 0.00(0.00)\n",
      "Iter 0653 | Time 59.7119(61.3231) | Bit/dim 3.8492(3.8575) | Xent 1.3813(1.4103) | Loss 11.6558(12.2534) | Error 0.5020(0.5092) Steps 0(0.00) | Grad Norm 4.7228(9.5164) | Total Time 0.00(0.00)\n",
      "Iter 0654 | Time 57.0936(61.1962) | Bit/dim 3.8859(3.8584) | Xent 1.4227(1.4106) | Loss 11.4584(12.2295) | Error 0.5148(0.5093) Steps 0(0.00) | Grad Norm 8.9580(9.4996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 24.3371, Epoch Time 403.0229(409.1904), Bit/dim 3.8742(best: 3.8114), Xent 1.3509, Loss 4.5496, Error 0.4881(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 57.2915(61.0791) | Bit/dim 3.8807(3.8591) | Xent 1.3903(1.4100) | Loss 15.6175(12.3312) | Error 0.4945(0.5089) Steps 0(0.00) | Grad Norm 6.5151(9.4101) | Total Time 0.00(0.00)\n",
      "Iter 0656 | Time 61.9332(61.1047) | Bit/dim 3.8755(3.8596) | Xent 1.3700(1.4088) | Loss 11.5322(12.3072) | Error 0.4984(0.5086) Steps 0(0.00) | Grad Norm 7.3135(9.3472) | Total Time 0.00(0.00)\n",
      "Iter 0657 | Time 64.6806(61.2120) | Bit/dim 3.8625(3.8596) | Xent 1.3633(1.4075) | Loss 11.3192(12.2776) | Error 0.4926(0.5081) Steps 0(0.00) | Grad Norm 7.8662(9.3028) | Total Time 0.00(0.00)\n",
      "Iter 0658 | Time 58.2915(61.1244) | Bit/dim 3.8514(3.8594) | Xent 1.4364(1.4083) | Loss 11.6964(12.2601) | Error 0.5195(0.5084) Steps 0(0.00) | Grad Norm 11.5130(9.3691) | Total Time 0.00(0.00)\n",
      "Iter 0659 | Time 61.0021(61.1207) | Bit/dim 3.8460(3.8590) | Xent 1.4008(1.4081) | Loss 11.5747(12.2396) | Error 0.5095(0.5085) Steps 0(0.00) | Grad Norm 16.6518(9.5876) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 61.3150(61.1265) | Bit/dim 3.8744(3.8595) | Xent 1.4236(1.4086) | Loss 11.6415(12.2216) | Error 0.5081(0.5084) Steps 0(0.00) | Grad Norm 16.8626(9.8058) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 26.2268, Epoch Time 406.7353(409.1168), Bit/dim 3.8490(best: 3.8114), Xent 1.3651, Loss 4.5316, Error 0.4958(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 62.1159(61.1562) | Bit/dim 3.8543(3.8593) | Xent 1.4097(1.4086) | Loss 15.8270(12.3298) | Error 0.5076(0.5084) Steps 0(0.00) | Grad Norm 7.2165(9.7281) | Total Time 0.00(0.00)\n",
      "Iter 0662 | Time 58.8377(61.0866) | Bit/dim 3.8292(3.8584) | Xent 1.4315(1.4093) | Loss 11.5403(12.3061) | Error 0.5172(0.5087) Steps 0(0.00) | Grad Norm 6.5913(9.6340) | Total Time 0.00(0.00)\n",
      "Iter 0663 | Time 60.1642(61.0590) | Bit/dim 3.8431(3.8579) | Xent 1.3876(1.4086) | Loss 11.6223(12.2856) | Error 0.4959(0.5083) Steps 0(0.00) | Grad Norm 6.2460(9.5324) | Total Time 0.00(0.00)\n",
      "Iter 0664 | Time 65.9855(61.2068) | Bit/dim 3.8321(3.8572) | Xent 1.3532(1.4070) | Loss 11.5444(12.2634) | Error 0.4904(0.5078) Steps 0(0.00) | Grad Norm 6.6459(9.4458) | Total Time 0.00(0.00)\n",
      "Iter 0665 | Time 62.3533(61.2412) | Bit/dim 3.8454(3.8568) | Xent 1.3370(1.4049) | Loss 11.5735(12.2427) | Error 0.4852(0.5071) Steps 0(0.00) | Grad Norm 5.8817(9.3389) | Total Time 0.00(0.00)\n",
      "Iter 0666 | Time 62.0245(61.2647) | Bit/dim 3.8252(3.8559) | Xent 1.3288(1.4026) | Loss 11.4200(12.2180) | Error 0.4805(0.5063) Steps 0(0.00) | Grad Norm 5.0681(9.2107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 24.6318, Epoch Time 412.5128(409.2186), Bit/dim 3.8320(best: 3.8114), Xent 1.3015, Loss 4.4828, Error 0.4686(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 58.2256(61.1735) | Bit/dim 3.8220(3.8548) | Xent 1.3164(1.4000) | Loss 15.7372(12.3236) | Error 0.4785(0.5055) Steps 0(0.00) | Grad Norm 5.8971(9.1113) | Total Time 0.00(0.00)\n",
      "Iter 0668 | Time 59.2017(61.1143) | Bit/dim 3.8237(3.8539) | Xent 1.3165(1.3975) | Loss 11.4925(12.2986) | Error 0.4730(0.5045) Steps 0(0.00) | Grad Norm 3.6416(8.9472) | Total Time 0.00(0.00)\n",
      "Iter 0669 | Time 61.3241(61.1206) | Bit/dim 3.8112(3.8526) | Xent 1.3139(1.3950) | Loss 11.5024(12.2747) | Error 0.4711(0.5035) Steps 0(0.00) | Grad Norm 3.9176(8.7963) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 58.9372(61.0551) | Bit/dim 3.8103(3.8514) | Xent 1.3410(1.3934) | Loss 11.5478(12.2529) | Error 0.4820(0.5028) Steps 0(0.00) | Grad Norm 7.2840(8.7510) | Total Time 0.00(0.00)\n",
      "Iter 0671 | Time 65.7957(61.1973) | Bit/dim 3.8104(3.8501) | Xent 1.3704(1.3927) | Loss 11.1003(12.2184) | Error 0.4891(0.5024) Steps 0(0.00) | Grad Norm 12.2298(8.8553) | Total Time 0.00(0.00)\n",
      "Iter 0672 | Time 59.1260(61.1352) | Bit/dim 3.8220(3.8493) | Xent 1.4455(1.3943) | Loss 11.2480(12.1892) | Error 0.5142(0.5028) Steps 0(0.00) | Grad Norm 14.2550(9.0173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 24.0965, Epoch Time 402.9816(409.0315), Bit/dim 3.8319(best: 3.8114), Xent 1.3222, Loss 4.4930, Error 0.4792(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 62.3500(61.1717) | Bit/dim 3.8378(3.8489) | Xent 1.3697(1.3935) | Loss 15.4125(12.2859) | Error 0.4960(0.5026) Steps 0(0.00) | Grad Norm 10.0111(9.0471) | Total Time 0.00(0.00)\n",
      "Iter 0674 | Time 64.2649(61.2645) | Bit/dim 3.8161(3.8480) | Xent 1.3105(1.3910) | Loss 11.2522(12.2549) | Error 0.4766(0.5018) Steps 0(0.00) | Grad Norm 4.0044(8.8959) | Total Time 0.00(0.00)\n",
      "Iter 0675 | Time 60.2772(61.2348) | Bit/dim 3.8139(3.8469) | Xent 1.3363(1.3894) | Loss 11.4413(12.2305) | Error 0.4842(0.5013) Steps 0(0.00) | Grad Norm 7.8825(8.8655) | Total Time 0.00(0.00)\n",
      "Iter 0676 | Time 61.2860(61.2364) | Bit/dim 3.8133(3.8459) | Xent 1.3279(1.3876) | Loss 11.1406(12.1978) | Error 0.4804(0.5006) Steps 0(0.00) | Grad Norm 6.3745(8.7907) | Total Time 0.00(0.00)\n",
      "Iter 0677 | Time 60.6287(61.2181) | Bit/dim 3.7971(3.8445) | Xent 1.3357(1.3860) | Loss 11.4622(12.1758) | Error 0.4819(0.5001) Steps 0(0.00) | Grad Norm 8.4570(8.7807) | Total Time 0.00(0.00)\n",
      "Iter 0678 | Time 59.4530(61.1652) | Bit/dim 3.8097(3.8434) | Xent 1.3348(1.3845) | Loss 11.3263(12.1503) | Error 0.4839(0.4996) Steps 0(0.00) | Grad Norm 7.7239(8.7490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 24.2589, Epoch Time 408.3672(409.0116), Bit/dim 3.7943(best: 3.8114), Xent 1.2547, Loss 4.4216, Error 0.4543(best: 0.4587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 58.6632(61.0901) | Bit/dim 3.8049(3.8423) | Xent 1.2822(1.3814) | Loss 15.6380(12.2549) | Error 0.4635(0.4985) Steps 0(0.00) | Grad Norm 2.8403(8.5718) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 60.6290(61.0763) | Bit/dim 3.8078(3.8412) | Xent 1.3060(1.3791) | Loss 11.3886(12.2289) | Error 0.4718(0.4977) Steps 0(0.00) | Grad Norm 6.5386(8.5108) | Total Time 0.00(0.00)\n",
      "Iter 0681 | Time 61.8358(61.0991) | Bit/dim 3.7871(3.8396) | Xent 1.3034(1.3769) | Loss 11.2057(12.1982) | Error 0.4677(0.4968) Steps 0(0.00) | Grad Norm 6.6940(8.4563) | Total Time 0.00(0.00)\n",
      "Iter 0682 | Time 57.5741(60.9933) | Bit/dim 3.8060(3.8386) | Xent 1.2624(1.3734) | Loss 11.1339(12.1663) | Error 0.4546(0.4955) Steps 0(0.00) | Grad Norm 5.8452(8.3779) | Total Time 0.00(0.00)\n",
      "Iter 0683 | Time 62.2513(61.0311) | Bit/dim 3.7991(3.8374) | Xent 1.2667(1.3702) | Loss 11.2430(12.1386) | Error 0.4561(0.4944) Steps 0(0.00) | Grad Norm 6.0020(8.3066) | Total Time 0.00(0.00)\n",
      "Iter 0684 | Time 59.9773(60.9995) | Bit/dim 3.7967(3.8362) | Xent 1.3104(1.3684) | Loss 11.1412(12.1087) | Error 0.4759(0.4938) Steps 0(0.00) | Grad Norm 6.5843(8.2550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 24.3118, Epoch Time 401.6922(408.7920), Bit/dim 3.7929(best: 3.7943), Xent 1.3029, Loss 4.4444, Error 0.4701(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 60.9161(60.9970) | Bit/dim 3.7970(3.8350) | Xent 1.3467(1.3678) | Loss 15.3052(12.2046) | Error 0.4824(0.4935) Steps 0(0.00) | Grad Norm 10.5438(8.3236) | Total Time 0.00(0.00)\n",
      "Iter 0686 | Time 61.0210(60.9977) | Bit/dim 3.8013(3.8340) | Xent 1.3801(1.3682) | Loss 11.3518(12.1790) | Error 0.4898(0.4934) Steps 0(0.00) | Grad Norm 15.4993(8.5389) | Total Time 0.00(0.00)\n",
      "Iter 0687 | Time 62.4926(61.0425) | Bit/dim 3.8058(3.8332) | Xent 1.3584(1.3679) | Loss 11.2592(12.1514) | Error 0.4795(0.4929) Steps 0(0.00) | Grad Norm 11.8188(8.6373) | Total Time 0.00(0.00)\n",
      "Iter 0688 | Time 62.1017(61.0743) | Bit/dim 3.8001(3.8322) | Xent 1.3135(1.3662) | Loss 11.2656(12.1248) | Error 0.4695(0.4922) Steps 0(0.00) | Grad Norm 5.1330(8.5322) | Total Time 0.00(0.00)\n",
      "Iter 0689 | Time 60.8424(61.0673) | Bit/dim 3.8047(3.8313) | Xent 1.3442(1.3656) | Loss 11.1791(12.0964) | Error 0.4815(0.4919) Steps 0(0.00) | Grad Norm 6.5718(8.4734) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 60.3868(61.0469) | Bit/dim 3.7939(3.8302) | Xent 1.3213(1.3642) | Loss 11.5161(12.0790) | Error 0.4735(0.4914) Steps 0(0.00) | Grad Norm 5.1140(8.3726) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 24.7642, Epoch Time 408.6458(408.7876), Bit/dim 3.7890(best: 3.7929), Xent 1.2606, Loss 4.4193, Error 0.4558(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 61.9982(61.0755) | Bit/dim 3.7833(3.8288) | Xent 1.2920(1.3621) | Loss 15.6252(12.1854) | Error 0.4670(0.4906) Steps 0(0.00) | Grad Norm 4.6052(8.2596) | Total Time 0.00(0.00)\n",
      "Iter 0692 | Time 63.1811(61.1386) | Bit/dim 3.7967(3.8279) | Xent 1.2739(1.3594) | Loss 11.3174(12.1594) | Error 0.4600(0.4897) Steps 0(0.00) | Grad Norm 6.4539(8.2054) | Total Time 0.00(0.00)\n",
      "Iter 0693 | Time 61.7300(61.1564) | Bit/dim 3.7896(3.8267) | Xent 1.2733(1.3568) | Loss 11.2618(12.1324) | Error 0.4559(0.4887) Steps 0(0.00) | Grad Norm 3.8532(8.0748) | Total Time 0.00(0.00)\n",
      "Iter 0694 | Time 64.1797(61.2471) | Bit/dim 3.7766(3.8252) | Xent 1.2730(1.3543) | Loss 11.3688(12.1095) | Error 0.4624(0.4879) Steps 0(0.00) | Grad Norm 3.8894(7.9493) | Total Time 0.00(0.00)\n",
      "Iter 0695 | Time 61.9064(61.2669) | Bit/dim 3.7913(3.8242) | Xent 1.2499(1.3512) | Loss 11.3338(12.0863) | Error 0.4473(0.4867) Steps 0(0.00) | Grad Norm 5.1673(7.8658) | Total Time 0.00(0.00)\n",
      "Iter 0696 | Time 62.8659(61.3148) | Bit/dim 3.7932(3.8233) | Xent 1.2900(1.3494) | Loss 11.2945(12.0625) | Error 0.4580(0.4858) Steps 0(0.00) | Grad Norm 9.2661(7.9078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 25.0125, Epoch Time 417.0878(409.0366), Bit/dim 3.7870(best: 3.7890), Xent 1.3977, Loss 4.4859, Error 0.5020(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 62.2788(61.3437) | Bit/dim 3.7897(3.8222) | Xent 1.4499(1.3524) | Loss 15.7981(12.1746) | Error 0.5151(0.4867) Steps 0(0.00) | Grad Norm 19.3364(8.2507) | Total Time 0.00(0.00)\n",
      "Iter 0698 | Time 60.5797(61.3208) | Bit/dim 3.8662(3.8236) | Xent 1.6559(1.3615) | Loss 12.1622(12.1742) | Error 0.5726(0.4893) Steps 0(0.00) | Grad Norm 22.4615(8.6770) | Total Time 0.00(0.00)\n",
      "Iter 0699 | Time 57.2383(61.1983) | Bit/dim 3.8807(3.8253) | Xent 2.1714(1.3858) | Loss 12.5911(12.1867) | Error 0.6996(0.4956) Steps 0(0.00) | Grad Norm 23.2418(9.1139) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 60.7106(61.1837) | Bit/dim 4.1303(3.8344) | Xent 2.0263(1.4050) | Loss 12.9683(12.2102) | Error 0.6355(0.4998) Steps 0(0.00) | Grad Norm 30.6728(9.7607) | Total Time 0.00(0.00)\n",
      "Iter 0701 | Time 62.4092(61.2205) | Bit/dim 4.1387(3.8436) | Xent 1.8691(1.4189) | Loss 12.9258(12.2316) | Error 0.6551(0.5045) Steps 0(0.00) | Grad Norm 16.9291(9.9758) | Total Time 0.00(0.00)\n",
      "Iter 0702 | Time 62.8949(61.2707) | Bit/dim 4.1064(3.8514) | Xent 1.8035(1.4305) | Loss 12.6778(12.2450) | Error 0.6454(0.5087) Steps 0(0.00) | Grad Norm 17.2906(10.1952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 26.0706, Epoch Time 408.3932(409.0173), Bit/dim 4.0116(best: 3.7870), Xent 1.7445, Loss 4.8839, Error 0.6135(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 67.2083(61.4488) | Bit/dim 4.0091(3.8562) | Xent 1.8202(1.4422) | Loss 18.3127(12.4270) | Error 0.6350(0.5125) Steps 0(0.00) | Grad Norm 12.0539(10.2510) | Total Time 0.00(0.00)\n",
      "Iter 0704 | Time 60.3285(61.4152) | Bit/dim 4.1412(3.8647) | Xent 1.6610(1.4487) | Loss 12.8467(12.4396) | Error 0.5922(0.5149) Steps 0(0.00) | Grad Norm 6.4677(10.1375) | Total Time 0.00(0.00)\n",
      "Iter 0705 | Time 63.2605(61.4706) | Bit/dim 4.0872(3.8714) | Xent 1.6487(1.4547) | Loss 12.3489(12.4369) | Error 0.5875(0.5170) Steps 0(0.00) | Grad Norm 7.7620(10.0662) | Total Time 0.00(0.00)\n",
      "Iter 0706 | Time 69.3821(61.7079) | Bit/dim 4.0399(3.8765) | Xent 1.6411(1.4603) | Loss 12.4526(12.4374) | Error 0.5835(0.5190) Steps 0(0.00) | Grad Norm 9.3894(10.0459) | Total Time 0.00(0.00)\n",
      "Iter 0707 | Time 66.1447(61.8410) | Bit/dim 4.0200(3.8808) | Xent 1.5448(1.4628) | Loss 12.3785(12.4356) | Error 0.5517(0.5200) Steps 0(0.00) | Grad Norm 7.1028(9.9576) | Total Time 0.00(0.00)\n",
      "Iter 0708 | Time 69.5075(62.0710) | Bit/dim 4.0183(3.8849) | Xent 1.5312(1.4649) | Loss 12.3333(12.4325) | Error 0.5451(0.5208) Steps 0(0.00) | Grad Norm 5.7393(9.8311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 27.4927, Epoch Time 439.3827(409.9283), Bit/dim 3.9990(best: 3.7870), Xent 1.4754, Loss 4.7367, Error 0.5240(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 66.1069(62.1921) | Bit/dim 4.0070(3.8885) | Xent 1.5283(1.4668) | Loss 18.2961(12.6085) | Error 0.5446(0.5215) Steps 0(0.00) | Grad Norm 8.4896(9.7908) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 66.4835(62.3208) | Bit/dim 4.0200(3.8925) | Xent 1.4983(1.4677) | Loss 12.0352(12.5913) | Error 0.5356(0.5219) Steps 0(0.00) | Grad Norm 8.8984(9.7640) | Total Time 0.00(0.00)\n",
      "Iter 0711 | Time 62.7094(62.3325) | Bit/dim 3.9662(3.8947) | Xent 1.4724(1.4679) | Loss 12.0304(12.5744) | Error 0.5336(0.5223) Steps 0(0.00) | Grad Norm 5.2845(9.6297) | Total Time 0.00(0.00)\n",
      "Iter 0712 | Time 62.5743(62.3398) | Bit/dim 3.9540(3.8965) | Xent 1.4551(1.4675) | Loss 11.8567(12.5529) | Error 0.5185(0.5221) Steps 0(0.00) | Grad Norm 5.8416(9.5160) | Total Time 0.00(0.00)\n",
      "Iter 0713 | Time 62.3991(62.3415) | Bit/dim 3.9456(3.8980) | Xent 1.4941(1.4683) | Loss 11.8811(12.5327) | Error 0.5265(0.5223) Steps 0(0.00) | Grad Norm 9.2837(9.5090) | Total Time 0.00(0.00)\n",
      "Iter 0714 | Time 62.5763(62.3486) | Bit/dim 3.9337(3.8990) | Xent 1.5489(1.4707) | Loss 12.0288(12.5176) | Error 0.5520(0.5232) Steps 0(0.00) | Grad Norm 12.3873(9.5954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 24.7939, Epoch Time 423.3713(410.3316), Bit/dim 3.9733(best: 3.7870), Xent 1.8353, Loss 4.8909, Error 0.6252(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 62.2974(62.3470) | Bit/dim 3.9680(3.9011) | Xent 1.8812(1.4830) | Loss 16.5747(12.6393) | Error 0.6214(0.5261) Steps 0(0.00) | Grad Norm 33.4700(10.3116) | Total Time 0.00(0.00)\n",
      "Iter 0716 | Time 64.1479(62.4011) | Bit/dim 4.0176(3.9046) | Xent 1.5921(1.4863) | Loss 12.1048(12.6233) | Error 0.5724(0.5275) Steps 0(0.00) | Grad Norm 11.2844(10.3408) | Total Time 0.00(0.00)\n",
      "Iter 0717 | Time 63.2504(62.4266) | Bit/dim 3.9542(3.9061) | Xent 1.5793(1.4891) | Loss 12.0285(12.6055) | Error 0.5714(0.5288) Steps 0(0.00) | Grad Norm 8.9349(10.2986) | Total Time 0.00(0.00)\n",
      "Iter 0718 | Time 69.0678(62.6258) | Bit/dim 3.9394(3.9071) | Xent 1.5478(1.4909) | Loss 11.8346(12.5823) | Error 0.5713(0.5301) Steps 0(0.00) | Grad Norm 6.5912(10.1874) | Total Time 0.00(0.00)\n",
      "Iter 0719 | Time 63.6070(62.6552) | Bit/dim 3.9298(3.9078) | Xent 1.5052(1.4913) | Loss 11.8790(12.5612) | Error 0.5397(0.5304) Steps 0(0.00) | Grad Norm 5.1225(10.0355) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 62.4839(62.6501) | Bit/dim 3.9189(3.9081) | Xent 1.4743(1.4908) | Loss 11.5224(12.5301) | Error 0.5356(0.5305) Steps 0(0.00) | Grad Norm 5.1069(9.8876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 25.5111, Epoch Time 426.3321(410.8116), Bit/dim 3.9123(best: 3.7870), Xent 1.4354, Loss 4.6299, Error 0.5119(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 61.5278(62.6164) | Bit/dim 3.9224(3.9085) | Xent 1.4836(1.4906) | Loss 16.4244(12.6469) | Error 0.5335(0.5306) Steps 0(0.00) | Grad Norm 6.6687(9.7910) | Total Time 0.00(0.00)\n",
      "Iter 0722 | Time 66.8472(62.7433) | Bit/dim 3.8898(3.9080) | Xent 1.4738(1.4901) | Loss 11.8811(12.6239) | Error 0.5300(0.5306) Steps 0(0.00) | Grad Norm 6.7593(9.7001) | Total Time 0.00(0.00)\n",
      "Iter 0723 | Time 65.3741(62.8223) | Bit/dim 3.8848(3.9073) | Xent 1.4510(1.4889) | Loss 11.7649(12.5982) | Error 0.5191(0.5303) Steps 0(0.00) | Grad Norm 8.3371(9.6592) | Total Time 0.00(0.00)\n",
      "Iter 0724 | Time 68.1291(62.9815) | Bit/dim 3.8761(3.9063) | Xent 1.4721(1.4884) | Loss 11.7209(12.5718) | Error 0.5252(0.5301) Steps 0(0.00) | Grad Norm 10.9854(9.6990) | Total Time 0.00(0.00)\n",
      "Iter 0725 | Time 68.4994(63.1470) | Bit/dim 3.8892(3.9058) | Xent 1.5012(1.4888) | Loss 11.8229(12.5494) | Error 0.5384(0.5304) Steps 0(0.00) | Grad Norm 14.2565(9.8357) | Total Time 0.00(0.00)\n",
      "Iter 0726 | Time 71.6297(63.4015) | Bit/dim 3.8788(3.9050) | Xent 1.5437(1.4904) | Loss 11.7023(12.5240) | Error 0.5524(0.5310) Steps 0(0.00) | Grad Norm 14.9323(9.9886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 25.6790, Epoch Time 443.3945(411.7891), Bit/dim 3.8664(best: 3.7870), Xent 1.4056, Loss 4.5692, Error 0.5012(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 66.3240(63.4892) | Bit/dim 3.8601(3.9037) | Xent 1.4345(1.4887) | Loss 15.8445(12.6236) | Error 0.5150(0.5305) Steps 0(0.00) | Grad Norm 9.5270(9.9748) | Total Time 0.00(0.00)\n",
      "Iter 0728 | Time 67.7891(63.6182) | Bit/dim 3.8664(3.9025) | Xent 1.4305(1.4870) | Loss 11.7400(12.5971) | Error 0.5212(0.5303) Steps 0(0.00) | Grad Norm 7.0594(9.8873) | Total Time 0.00(0.00)\n",
      "Iter 0729 | Time 67.6295(63.7385) | Bit/dim 3.8534(3.9011) | Xent 1.4269(1.4852) | Loss 11.7372(12.5713) | Error 0.5126(0.5297) Steps 0(0.00) | Grad Norm 5.6520(9.7602) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 69.8518(63.9219) | Bit/dim 3.8497(3.8995) | Xent 1.3991(1.4826) | Loss 11.4042(12.5363) | Error 0.4974(0.5288) Steps 0(0.00) | Grad Norm 3.4670(9.5714) | Total Time 0.00(0.00)\n",
      "Iter 0731 | Time 60.4481(63.8177) | Bit/dim 3.8484(3.8980) | Xent 1.4030(1.4802) | Loss 11.3817(12.5016) | Error 0.5020(0.5280) Steps 0(0.00) | Grad Norm 6.1464(9.4687) | Total Time 0.00(0.00)\n",
      "Iter 0732 | Time 59.4057(63.6853) | Bit/dim 3.8329(3.8960) | Xent 1.4092(1.4781) | Loss 11.5512(12.4731) | Error 0.5035(0.5272) Steps 0(0.00) | Grad Norm 5.9794(9.3640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 25.2074, Epoch Time 432.4586(412.4092), Bit/dim 3.8279(best: 3.7870), Xent 1.3421, Loss 4.4989, Error 0.4780(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 62.7447(63.6571) | Bit/dim 3.8317(3.8941) | Xent 1.3786(1.4751) | Loss 15.0266(12.5497) | Error 0.4912(0.5261) Steps 0(0.00) | Grad Norm 3.5053(9.1883) | Total Time 0.00(0.00)\n",
      "Iter 0734 | Time 67.4721(63.7716) | Bit/dim 3.8215(3.8919) | Xent 1.3812(1.4723) | Loss 11.4965(12.5181) | Error 0.4919(0.5251) Steps 0(0.00) | Grad Norm 4.7803(9.0560) | Total Time 0.00(0.00)\n",
      "Iter 0735 | Time 68.0184(63.8990) | Bit/dim 3.8252(3.8899) | Xent 1.3580(1.4689) | Loss 11.5477(12.4890) | Error 0.4931(0.5242) Steps 0(0.00) | Grad Norm 5.3771(8.9456) | Total Time 0.00(0.00)\n",
      "Iter 0736 | Time 67.8873(64.0186) | Bit/dim 3.8079(3.8875) | Xent 1.3658(1.4658) | Loss 11.4654(12.4583) | Error 0.4885(0.5231) Steps 0(0.00) | Grad Norm 3.5636(8.7842) | Total Time 0.00(0.00)\n",
      "Iter 0737 | Time 66.7740(64.1013) | Bit/dim 3.8115(3.8852) | Xent 1.3525(1.4624) | Loss 11.6107(12.4329) | Error 0.4900(0.5221) Steps 0(0.00) | Grad Norm 3.3219(8.6203) | Total Time 0.00(0.00)\n",
      "Iter 0738 | Time 62.7054(64.0594) | Bit/dim 3.8090(3.8829) | Xent 1.3581(1.4592) | Loss 11.4297(12.4028) | Error 0.4914(0.5212) Steps 0(0.00) | Grad Norm 3.8852(8.4783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 26.1268, Epoch Time 437.9713(413.1760), Bit/dim 3.8068(best: 3.7870), Xent 1.3191, Loss 4.4663, Error 0.4719(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 65.4412(64.1009) | Bit/dim 3.8066(3.8806) | Xent 1.3320(1.4554) | Loss 15.5579(12.4974) | Error 0.4812(0.5200) Steps 0(0.00) | Grad Norm 5.6917(8.3947) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 63.8803(64.0942) | Bit/dim 3.8190(3.8788) | Xent 1.3655(1.4527) | Loss 11.5133(12.4679) | Error 0.4901(0.5191) Steps 0(0.00) | Grad Norm 6.8584(8.3486) | Total Time 0.00(0.00)\n",
      "Iter 0741 | Time 63.2498(64.0689) | Bit/dim 3.7929(3.8762) | Xent 1.3302(1.4490) | Loss 11.2888(12.4325) | Error 0.4730(0.5177) Steps 0(0.00) | Grad Norm 4.8753(8.2444) | Total Time 0.00(0.00)\n",
      "Iter 0742 | Time 65.5681(64.1139) | Bit/dim 3.7827(3.8734) | Xent 1.3217(1.4452) | Loss 11.4230(12.4022) | Error 0.4740(0.5164) Steps 0(0.00) | Grad Norm 2.7355(8.0791) | Total Time 0.00(0.00)\n",
      "Iter 0743 | Time 64.4138(64.1229) | Bit/dim 3.7916(3.8709) | Xent 1.3495(1.4424) | Loss 11.4390(12.3733) | Error 0.4755(0.5152) Steps 0(0.00) | Grad Norm 5.7394(8.0089) | Total Time 0.00(0.00)\n",
      "Iter 0744 | Time 65.9624(64.1781) | Bit/dim 3.7881(3.8684) | Xent 1.3493(1.4396) | Loss 11.3832(12.3436) | Error 0.4869(0.5143) Steps 0(0.00) | Grad Norm 9.5786(8.0560) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 25.6719, Epoch Time 430.4162(413.6933), Bit/dim 3.7903(best: 3.7870), Xent 1.3604, Loss 4.4705, Error 0.4824(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 64.3434(64.1830) | Bit/dim 3.8077(3.8666) | Xent 1.4037(1.4385) | Loss 15.6466(12.4427) | Error 0.5052(0.5140) Steps 0(0.00) | Grad Norm 14.0016(8.2344) | Total Time 0.00(0.00)\n",
      "Iter 0746 | Time 63.3342(64.1576) | Bit/dim 3.8085(3.8649) | Xent 1.5394(1.4415) | Loss 11.3888(12.4111) | Error 0.5380(0.5148) Steps 0(0.00) | Grad Norm 19.4065(8.5695) | Total Time 0.00(0.00)\n",
      "Iter 0747 | Time 63.6601(64.1426) | Bit/dim 3.8087(3.8632) | Xent 1.6718(1.4484) | Loss 11.8613(12.3946) | Error 0.5814(0.5168) Steps 0(0.00) | Grad Norm 16.0401(8.7937) | Total Time 0.00(0.00)\n",
      "Iter 0748 | Time 65.1259(64.1721) | Bit/dim 3.9570(3.8660) | Xent 2.9124(1.4923) | Loss 13.6931(12.4336) | Error 0.7452(0.5236) Steps 0(0.00) | Grad Norm 60.9698(10.3589) | Total Time 0.00(0.00)\n",
      "Iter 0749 | Time 70.3852(64.3585) | Bit/dim 3.9677(3.8691) | Xent 1.5840(1.4951) | Loss 12.2589(12.4283) | Error 0.5704(0.5250) Steps 0(0.00) | Grad Norm 14.7992(10.4922) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 70.8512(64.5533) | Bit/dim 3.9655(3.8720) | Xent 1.9333(1.5082) | Loss 12.4664(12.4295) | Error 0.6735(0.5295) Steps 0(0.00) | Grad Norm 19.2730(10.7556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 25.5930, Epoch Time 439.2663(414.4604), Bit/dim 3.9149(best: 3.7870), Xent 1.6360, Loss 4.7328, Error 0.5876(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 66.1839(64.6022) | Bit/dim 3.9132(3.8732) | Xent 1.6992(1.5140) | Loss 16.7662(12.5596) | Error 0.6098(0.5319) Steps 0(0.00) | Grad Norm 8.7314(10.6949) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 64.3516(64.5947) | Bit/dim 3.9441(3.8753) | Xent 1.5400(1.5147) | Loss 11.9810(12.5422) | Error 0.5495(0.5324) Steps 0(0.00) | Grad Norm 6.4900(10.5687) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 59.2923(64.4356) | Bit/dim 3.9910(3.8788) | Xent 1.5111(1.5146) | Loss 12.1569(12.5307) | Error 0.5486(0.5329) Steps 0(0.00) | Grad Norm 6.5741(10.4489) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 63.8713(64.4187) | Bit/dim 3.9750(3.8817) | Xent 1.5104(1.5145) | Loss 12.1634(12.5196) | Error 0.5460(0.5333) Steps 0(0.00) | Grad Norm 4.7126(10.2768) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 69.9688(64.5852) | Bit/dim 3.9540(3.8838) | Xent 1.4765(1.5134) | Loss 11.8871(12.5007) | Error 0.5291(0.5332) Steps 0(0.00) | Grad Norm 4.7508(10.1110) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 67.4400(64.6708) | Bit/dim 3.9077(3.8846) | Xent 1.4816(1.5124) | Loss 11.8689(12.4817) | Error 0.5319(0.5331) Steps 0(0.00) | Grad Norm 4.6730(9.9479) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 26.5949, Epoch Time 433.8810(415.0431), Bit/dim 3.9166(best: 3.7870), Xent 1.4513, Loss 4.6422, Error 0.5166(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 68.8700(64.7968) | Bit/dim 3.9107(3.8853) | Xent 1.5006(1.5121) | Loss 16.4935(12.6021) | Error 0.5295(0.5330) Steps 0(0.00) | Grad Norm 5.6424(9.8187) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 67.1299(64.8668) | Bit/dim 3.9078(3.8860) | Xent 1.4664(1.5107) | Loss 11.4240(12.5667) | Error 0.5321(0.5330) Steps 0(0.00) | Grad Norm 6.3557(9.7148) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 68.6655(64.9808) | Bit/dim 3.8985(3.8864) | Xent 1.4407(1.5086) | Loss 11.6750(12.5400) | Error 0.5045(0.5321) Steps 0(0.00) | Grad Norm 6.4248(9.6161) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 67.9818(65.0708) | Bit/dim 3.8793(3.8862) | Xent 1.4244(1.5061) | Loss 11.7070(12.5150) | Error 0.5119(0.5315) Steps 0(0.00) | Grad Norm 5.5358(9.4937) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 64.4170(65.0512) | Bit/dim 3.8807(3.8860) | Xent 1.4407(1.5041) | Loss 11.6873(12.4902) | Error 0.5149(0.5310) Steps 0(0.00) | Grad Norm 6.5068(9.4041) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 68.0125(65.1400) | Bit/dim 3.8637(3.8853) | Xent 1.4704(1.5031) | Loss 11.5280(12.4613) | Error 0.5270(0.5309) Steps 0(0.00) | Grad Norm 7.2947(9.3408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 24.8705, Epoch Time 445.8306(415.9667), Bit/dim 3.8690(best: 3.7870), Xent 1.4157, Loss 4.5768, Error 0.5087(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 59.9602(64.9846) | Bit/dim 3.8669(3.8848) | Xent 1.4496(1.5015) | Loss 15.8535(12.5631) | Error 0.5117(0.5303) Steps 0(0.00) | Grad Norm 7.9285(9.2984) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 62.9399(64.9233) | Bit/dim 3.8612(3.8841) | Xent 1.4475(1.4999) | Loss 11.6846(12.5367) | Error 0.5214(0.5301) Steps 0(0.00) | Grad Norm 8.1230(9.2632) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 64.2304(64.9025) | Bit/dim 3.8379(3.8827) | Xent 1.4381(1.4980) | Loss 11.4452(12.5040) | Error 0.5101(0.5295) Steps 0(0.00) | Grad Norm 6.7617(9.1881) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 64.1599(64.8802) | Bit/dim 3.8355(3.8813) | Xent 1.3853(1.4946) | Loss 11.3344(12.4689) | Error 0.4925(0.5284) Steps 0(0.00) | Grad Norm 4.5428(9.0488) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 62.7045(64.8150) | Bit/dim 3.8208(3.8795) | Xent 1.4009(1.4918) | Loss 11.5083(12.4401) | Error 0.4965(0.5274) Steps 0(0.00) | Grad Norm 4.7663(8.9203) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 60.7456(64.6929) | Bit/dim 3.8215(3.8777) | Xent 1.3837(1.4886) | Loss 11.3493(12.4073) | Error 0.4959(0.5265) Steps 0(0.00) | Grad Norm 5.5960(8.8206) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 24.4724, Epoch Time 415.4923(415.9525), Bit/dim 3.8327(best: 3.7870), Xent 1.3487, Loss 4.5071, Error 0.4816(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 65.4374(64.7152) | Bit/dim 3.8254(3.8762) | Xent 1.3782(1.4853) | Loss 14.8554(12.4808) | Error 0.4915(0.5254) Steps 0(0.00) | Grad Norm 6.1011(8.7390) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 66.6280(64.7726) | Bit/dim 3.8091(3.8741) | Xent 1.3694(1.4818) | Loss 11.4130(12.4487) | Error 0.4942(0.5245) Steps 0(0.00) | Grad Norm 3.7402(8.5890) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 67.5061(64.8546) | Bit/dim 3.7979(3.8719) | Xent 1.3591(1.4781) | Loss 11.3093(12.4146) | Error 0.4865(0.5233) Steps 0(0.00) | Grad Norm 3.5316(8.4373) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 66.1100(64.8923) | Bit/dim 3.7981(3.8696) | Xent 1.3542(1.4744) | Loss 11.6001(12.3901) | Error 0.4828(0.5221) Steps 0(0.00) | Grad Norm 5.3396(8.3444) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 63.6240(64.8542) | Bit/dim 3.8108(3.8679) | Xent 1.3938(1.4720) | Loss 11.6461(12.3678) | Error 0.4992(0.5214) Steps 0(0.00) | Grad Norm 7.6001(8.3220) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 67.4639(64.9325) | Bit/dim 3.8151(3.8663) | Xent 1.4034(1.4699) | Loss 10.9086(12.3240) | Error 0.5021(0.5208) Steps 0(0.00) | Grad Norm 10.0163(8.3729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 24.5928, Epoch Time 436.9299(416.5818), Bit/dim 3.8045(best: 3.7870), Xent 1.3626, Loss 4.4857, Error 0.4878(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 69.1135(65.0579) | Bit/dim 3.7959(3.8642) | Xent 1.3766(1.4671) | Loss 15.7098(12.4256) | Error 0.4868(0.5198) Steps 0(0.00) | Grad Norm 10.9132(8.4491) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 60.5626(64.9231) | Bit/dim 3.7970(3.8622) | Xent 1.3947(1.4649) | Loss 11.3995(12.3948) | Error 0.5009(0.5193) Steps 0(0.00) | Grad Norm 10.1954(8.5015) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 68.3873(65.0270) | Bit/dim 3.7904(3.8600) | Xent 1.3742(1.4622) | Loss 11.4271(12.3658) | Error 0.4842(0.5182) Steps 0(0.00) | Grad Norm 8.0347(8.4875) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 61.1437(64.9105) | Bit/dim 3.8111(3.8586) | Xent 1.4038(1.4605) | Loss 11.5557(12.3415) | Error 0.5065(0.5179) Steps 0(0.00) | Grad Norm 8.4889(8.4875) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 63.3345(64.8632) | Bit/dim 3.7919(3.8566) | Xent 1.3637(1.4576) | Loss 11.2998(12.3102) | Error 0.4841(0.5168) Steps 0(0.00) | Grad Norm 6.2243(8.4196) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 61.4620(64.7612) | Bit/dim 3.7782(3.8542) | Xent 1.3723(1.4550) | Loss 11.4989(12.2859) | Error 0.4980(0.5163) Steps 0(0.00) | Grad Norm 6.0833(8.3495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 24.8495, Epoch Time 424.7542(416.8270), Bit/dim 3.7831(best: 3.7870), Xent 1.3466, Loss 4.4564, Error 0.4831(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 60.3170(64.6279) | Bit/dim 3.7966(3.8525) | Xent 1.3904(1.4531) | Loss 16.0637(12.3992) | Error 0.4936(0.5156) Steps 0(0.00) | Grad Norm 8.1347(8.3431) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 62.7289(64.5709) | Bit/dim 3.7771(3.8502) | Xent 1.3586(1.4502) | Loss 11.3229(12.3669) | Error 0.4862(0.5147) Steps 0(0.00) | Grad Norm 8.7284(8.3546) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 62.2817(64.5022) | Bit/dim 3.7798(3.8481) | Xent 1.3407(1.4469) | Loss 11.2988(12.3349) | Error 0.4858(0.5138) Steps 0(0.00) | Grad Norm 7.1449(8.3183) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 63.7544(64.4798) | Bit/dim 3.7694(3.8457) | Xent 1.3405(1.4438) | Loss 11.3147(12.3043) | Error 0.4836(0.5129) Steps 0(0.00) | Grad Norm 4.4602(8.2026) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 67.2194(64.5620) | Bit/dim 3.7712(3.8435) | Xent 1.3378(1.4406) | Loss 11.1614(12.2700) | Error 0.4816(0.5120) Steps 0(0.00) | Grad Norm 7.5315(8.1825) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 63.4295(64.5280) | Bit/dim 3.7825(3.8417) | Xent 1.3680(1.4384) | Loss 11.2628(12.2398) | Error 0.4961(0.5115) Steps 0(0.00) | Grad Norm 8.3122(8.1864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 24.4890, Epoch Time 420.2044(416.9283), Bit/dim 3.7727(best: 3.7831), Xent 1.2928, Loss 4.4191, Error 0.4665(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 59.7514(64.3847) | Bit/dim 3.7738(3.8396) | Xent 1.3340(1.4353) | Loss 15.8022(12.3467) | Error 0.4836(0.5107) Steps 0(0.00) | Grad Norm 6.0124(8.1211) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 64.8612(64.3990) | Bit/dim 3.7806(3.8379) | Xent 1.3442(1.4325) | Loss 11.4659(12.3202) | Error 0.4884(0.5100) Steps 0(0.00) | Grad Norm 7.6195(8.1061) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 60.1514(64.2716) | Bit/dim 3.7647(3.8357) | Xent 1.3413(1.4298) | Loss 11.1534(12.2852) | Error 0.4848(0.5093) Steps 0(0.00) | Grad Norm 6.8188(8.0675) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 62.7230(64.2251) | Bit/dim 3.7651(3.8336) | Xent 1.3221(1.4266) | Loss 11.1307(12.2506) | Error 0.4698(0.5081) Steps 0(0.00) | Grad Norm 6.2983(8.0144) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 63.7397(64.2106) | Bit/dim 3.7649(3.8315) | Xent 1.3505(1.4243) | Loss 11.0861(12.2157) | Error 0.4922(0.5076) Steps 0(0.00) | Grad Norm 6.5005(7.9690) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 70.6686(64.4043) | Bit/dim 3.7751(3.8298) | Xent 1.3417(1.4218) | Loss 11.2741(12.1874) | Error 0.4796(0.5068) Steps 0(0.00) | Grad Norm 6.7876(7.9335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 25.4328, Epoch Time 423.4813(417.1249), Bit/dim 3.7789(best: 3.7727), Xent 1.2974, Loss 4.4276, Error 0.4684(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 62.7873(64.3558) | Bit/dim 3.7753(3.8282) | Xent 1.3397(1.4193) | Loss 15.9456(12.3002) | Error 0.4848(0.5061) Steps 0(0.00) | Grad Norm 7.9250(7.9333) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 58.7743(64.1883) | Bit/dim 3.7635(3.8262) | Xent 1.3058(1.4159) | Loss 11.1190(12.2647) | Error 0.4738(0.5051) Steps 0(0.00) | Grad Norm 4.6846(7.8358) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 57.0713(63.9748) | Bit/dim 3.7822(3.8249) | Xent 1.3149(1.4129) | Loss 11.1855(12.2323) | Error 0.4726(0.5042) Steps 0(0.00) | Grad Norm 6.7272(7.8026) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 63.0120(63.9459) | Bit/dim 3.7756(3.8234) | Xent 1.3122(1.4099) | Loss 10.7664(12.1884) | Error 0.4720(0.5032) Steps 0(0.00) | Grad Norm 6.6200(7.7671) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 62.7282(63.9094) | Bit/dim 3.7758(3.8220) | Xent 1.3049(1.4067) | Loss 11.3168(12.1622) | Error 0.4636(0.5020) Steps 0(0.00) | Grad Norm 4.8850(7.6806) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 69.5109(64.0775) | Bit/dim 3.7688(3.8204) | Xent 1.3148(1.4040) | Loss 11.4229(12.1400) | Error 0.4709(0.5011) Steps 0(0.00) | Grad Norm 6.9981(7.6602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 24.2870, Epoch Time 414.2698(417.0392), Bit/dim 3.7754(best: 3.7727), Xent 1.2983, Loss 4.4245, Error 0.4651(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 57.9005(63.8921) | Bit/dim 3.7665(3.8188) | Xent 1.3611(1.4027) | Loss 15.1835(12.2313) | Error 0.4875(0.5007) Steps 0(0.00) | Grad Norm 11.4119(7.7727) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 65.9154(63.9528) | Bit/dim 3.7991(3.8182) | Xent 1.4377(1.4037) | Loss 11.4788(12.2088) | Error 0.5041(0.5008) Steps 0(0.00) | Grad Norm 13.6454(7.9489) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 64.7949(63.9781) | Bit/dim 3.7882(3.8173) | Xent 1.4746(1.4059) | Loss 11.5203(12.1881) | Error 0.5180(0.5013) Steps 0(0.00) | Grad Norm 13.0508(8.1019) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 64.2089(63.9850) | Bit/dim 3.8713(3.8189) | Xent 1.5766(1.4110) | Loss 11.9284(12.1803) | Error 0.5593(0.5030) Steps 0(0.00) | Grad Norm 11.7139(8.2103) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 64.5040(64.0006) | Bit/dim 3.8311(3.8193) | Xent 1.3591(1.4094) | Loss 11.5373(12.1610) | Error 0.4892(0.5026) Steps 0(0.00) | Grad Norm 9.1317(8.2379) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 66.5900(64.0783) | Bit/dim 3.8263(3.8195) | Xent 1.3964(1.4090) | Loss 11.5713(12.1433) | Error 0.4946(0.5024) Steps 0(0.00) | Grad Norm 9.7980(8.2847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 25.3485, Epoch Time 425.5088(417.2933), Bit/dim 3.8787(best: 3.7727), Xent 1.3423, Loss 4.5499, Error 0.4796(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 65.2304(64.1128) | Bit/dim 3.8683(3.8210) | Xent 1.3819(1.4082) | Loss 15.7346(12.2511) | Error 0.4920(0.5021) Steps 0(0.00) | Grad Norm 9.8205(8.3308) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 59.6895(63.9801) | Bit/dim 3.8338(3.8213) | Xent 1.3905(1.4077) | Loss 11.5494(12.2300) | Error 0.4989(0.5020) Steps 0(0.00) | Grad Norm 8.7793(8.3443) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 63.1103(63.9540) | Bit/dim 3.8705(3.8228) | Xent 1.4823(1.4099) | Loss 11.7632(12.2160) | Error 0.5323(0.5029) Steps 0(0.00) | Grad Norm 11.6883(8.4446) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 62.4210(63.9081) | Bit/dim 3.8447(3.8235) | Xent 1.4263(1.4104) | Loss 11.4785(12.1939) | Error 0.5232(0.5035) Steps 0(0.00) | Grad Norm 7.9051(8.4284) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 67.6196(64.0194) | Bit/dim 3.8409(3.8240) | Xent 1.3581(1.4089) | Loss 11.5081(12.1733) | Error 0.4810(0.5028) Steps 0(0.00) | Grad Norm 4.7350(8.3176) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 69.3777(64.1802) | Bit/dim 3.8167(3.8238) | Xent 1.3821(1.4080) | Loss 11.5769(12.1554) | Error 0.5022(0.5028) Steps 0(0.00) | Grad Norm 5.6876(8.2387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 25.0802, Epoch Time 428.3493(417.6250), Bit/dim 3.8107(best: 3.7727), Xent 1.3280, Loss 4.4747, Error 0.4718(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 74.0032(64.4748) | Bit/dim 3.7984(3.8230) | Xent 1.3686(1.4069) | Loss 16.1826(12.2762) | Error 0.4899(0.5024) Steps 0(0.00) | Grad Norm 7.3847(8.2131) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 75.9041(64.8177) | Bit/dim 3.8409(3.8236) | Xent 1.3739(1.4059) | Loss 11.6272(12.2568) | Error 0.4928(0.5021) Steps 0(0.00) | Grad Norm 10.6167(8.2852) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 74.0871(65.0958) | Bit/dim 3.8236(3.8236) | Xent 1.3519(1.4043) | Loss 11.1417(12.2233) | Error 0.4876(0.5017) Steps 0(0.00) | Grad Norm 8.0937(8.2794) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 78.4249(65.4957) | Bit/dim 3.8150(3.8233) | Xent 1.3403(1.4023) | Loss 11.2731(12.1948) | Error 0.4820(0.5011) Steps 0(0.00) | Grad Norm 4.9614(8.1799) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 74.3457(65.7612) | Bit/dim 3.7910(3.8223) | Xent 1.3176(1.3998) | Loss 11.3493(12.1694) | Error 0.4698(0.5002) Steps 0(0.00) | Grad Norm 4.6093(8.0728) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 70.6780(65.9087) | Bit/dim 3.8074(3.8219) | Xent 1.3224(1.3975) | Loss 11.3405(12.1446) | Error 0.4718(0.4993) Steps 0(0.00) | Grad Norm 6.2085(8.0169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 25.4441, Epoch Time 488.3664(419.7472), Bit/dim 3.8017(best: 3.7727), Xent 1.2977, Loss 4.4505, Error 0.4676(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 69.1854(66.0070) | Bit/dim 3.7916(3.8210) | Xent 1.3363(1.3956) | Loss 15.7726(12.2534) | Error 0.4774(0.4986) Steps 0(0.00) | Grad Norm 7.7212(8.0080) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 73.4417(66.2300) | Bit/dim 3.7788(3.8197) | Xent 1.3078(1.3930) | Loss 11.3522(12.2264) | Error 0.4742(0.4979) Steps 0(0.00) | Grad Norm 7.4691(7.9918) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 70.0317(66.3441) | Bit/dim 3.7975(3.8190) | Xent 1.2848(1.3898) | Loss 11.3086(12.1988) | Error 0.4649(0.4969) Steps 0(0.00) | Grad Norm 6.2303(7.9390) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 76.1845(66.6393) | Bit/dim 3.7850(3.8180) | Xent 1.2960(1.3869) | Loss 11.0403(12.1641) | Error 0.4627(0.4959) Steps 0(0.00) | Grad Norm 2.9723(7.7900) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 65.0332(66.5911) | Bit/dim 3.7913(3.8172) | Xent 1.2763(1.3836) | Loss 11.3734(12.1404) | Error 0.4631(0.4949) Steps 0(0.00) | Grad Norm 3.1813(7.6517) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 66.3915(66.5851) | Bit/dim 3.7673(3.8157) | Xent 1.2904(1.3808) | Loss 11.2758(12.1144) | Error 0.4655(0.4940) Steps 0(0.00) | Grad Norm 5.8459(7.5975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 24.1215, Epoch Time 460.6914(420.9755), Bit/dim 3.7743(best: 3.7727), Xent 1.2802, Loss 4.4144, Error 0.4581(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 73.3340(66.7876) | Bit/dim 3.7727(3.8144) | Xent 1.2930(1.3782) | Loss 15.5463(12.2174) | Error 0.4636(0.4931) Steps 0(0.00) | Grad Norm 10.1770(7.6749) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 61.9701(66.6431) | Bit/dim 3.7851(3.8135) | Xent 1.3670(1.3779) | Loss 11.2940(12.1897) | Error 0.4854(0.4929) Steps 0(0.00) | Grad Norm 11.3090(7.7839) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 71.2669(66.7818) | Bit/dim 3.7807(3.8126) | Xent 1.3807(1.3779) | Loss 11.3093(12.1633) | Error 0.4929(0.4929) Steps 0(0.00) | Grad Norm 13.0471(7.9418) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 72.0362(66.9394) | Bit/dim 3.7804(3.8116) | Xent 1.3740(1.3778) | Loss 11.4035(12.1405) | Error 0.4886(0.4928) Steps 0(0.00) | Grad Norm 10.8014(8.0276) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 75.9511(67.2098) | Bit/dim 3.7686(3.8103) | Xent 1.3051(1.3756) | Loss 11.2314(12.1132) | Error 0.4655(0.4919) Steps 0(0.00) | Grad Norm 7.2370(8.0039) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 74.8759(67.4397) | Bit/dim 3.7775(3.8093) | Xent 1.3535(1.3750) | Loss 11.3871(12.0914) | Error 0.4898(0.4919) Steps 0(0.00) | Grad Norm 11.5901(8.1115) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 24.2288, Epoch Time 469.7279(422.4381), Bit/dim 3.7784(best: 3.7727), Xent 1.3458, Loss 4.4513, Error 0.4772(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 76.1095(67.6998) | Bit/dim 3.7773(3.8084) | Xent 1.3912(1.3755) | Loss 15.7218(12.2003) | Error 0.4930(0.4919) Steps 0(0.00) | Grad Norm 10.3038(8.1773) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 68.1198(67.7124) | Bit/dim 3.7664(3.8071) | Xent 1.3405(1.3744) | Loss 11.1641(12.1693) | Error 0.4849(0.4917) Steps 0(0.00) | Grad Norm 4.6196(8.0705) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 70.6912(67.8018) | Bit/dim 3.7605(3.8057) | Xent 1.3289(1.3731) | Loss 11.3817(12.1456) | Error 0.4821(0.4914) Steps 0(0.00) | Grad Norm 6.5011(8.0235) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 73.0441(67.9591) | Bit/dim 3.7750(3.8048) | Xent 1.3383(1.3720) | Loss 11.1744(12.1165) | Error 0.4821(0.4911) Steps 0(0.00) | Grad Norm 5.5879(7.9504) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 70.1413(68.0245) | Bit/dim 3.7769(3.8039) | Xent 1.2786(1.3692) | Loss 11.1628(12.0879) | Error 0.4579(0.4901) Steps 0(0.00) | Grad Norm 2.9318(7.7998) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 70.9745(68.1130) | Bit/dim 3.7689(3.8029) | Xent 1.2847(1.3667) | Loss 11.1944(12.0611) | Error 0.4607(0.4893) Steps 0(0.00) | Grad Norm 5.0376(7.7170) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 25.3302, Epoch Time 469.9234(423.8627), Bit/dim 3.7541(best: 3.7727), Xent 1.2536, Loss 4.3809, Error 0.4510(best: 0.4543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 73.6764(68.2799) | Bit/dim 3.7506(3.8013) | Xent 1.2687(1.3637) | Loss 15.4468(12.1626) | Error 0.4556(0.4882) Steps 0(0.00) | Grad Norm 3.7047(7.5966) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 79.2710(68.6097) | Bit/dim 3.7709(3.8004) | Xent 1.2832(1.3613) | Loss 11.2756(12.1360) | Error 0.4561(0.4873) Steps 0(0.00) | Grad Norm 4.8269(7.5135) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 68.9906(68.6211) | Bit/dim 3.7722(3.7996) | Xent 1.2622(1.3583) | Loss 11.3326(12.1119) | Error 0.4486(0.4861) Steps 0(0.00) | Grad Norm 4.7963(7.4320) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 76.8974(68.8694) | Bit/dim 3.7670(3.7986) | Xent 1.2498(1.3551) | Loss 11.3027(12.0877) | Error 0.4484(0.4850) Steps 0(0.00) | Grad Norm 5.0712(7.3612) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 75.0051(69.0535) | Bit/dim 3.7420(3.7969) | Xent 1.2987(1.3534) | Loss 11.2737(12.0632) | Error 0.4660(0.4844) Steps 0(0.00) | Grad Norm 6.0636(7.3222) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 71.9925(69.1416) | Bit/dim 3.7689(3.7960) | Xent 1.2659(1.3508) | Loss 11.3847(12.0429) | Error 0.4543(0.4835) Steps 0(0.00) | Grad Norm 6.6234(7.3013) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 24.5318, Epoch Time 486.2242(425.7335), Bit/dim 3.7681(best: 3.7541), Xent 1.2382, Loss 4.3873, Error 0.4393(best: 0.4510)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 75.3726(69.3286) | Bit/dim 3.7778(3.7955) | Xent 1.2618(1.3481) | Loss 15.5564(12.1483) | Error 0.4471(0.4824) Steps 0(0.00) | Grad Norm 7.3824(7.3037) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 75.8474(69.5241) | Bit/dim 3.7425(3.7939) | Xent 1.2614(1.3455) | Loss 10.9957(12.1137) | Error 0.4561(0.4816) Steps 0(0.00) | Grad Norm 7.0189(7.2952) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 75.4049(69.7005) | Bit/dim 3.7401(3.7923) | Xent 1.2428(1.3424) | Loss 11.1351(12.0844) | Error 0.4433(0.4805) Steps 0(0.00) | Grad Norm 5.7613(7.2491) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 72.2869(69.7781) | Bit/dim 3.7512(3.7911) | Xent 1.2842(1.3407) | Loss 11.3031(12.0609) | Error 0.4683(0.4801) Steps 0(0.00) | Grad Norm 4.8337(7.1767) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 78.1689(70.0299) | Bit/dim 3.7446(3.7897) | Xent 1.2447(1.3378) | Loss 11.3122(12.0385) | Error 0.4497(0.4792) Steps 0(0.00) | Grad Norm 3.0517(7.0529) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 68.2573(69.9767) | Bit/dim 3.7364(3.7881) | Xent 1.2478(1.3351) | Loss 11.1830(12.0128) | Error 0.4523(0.4784) Steps 0(0.00) | Grad Norm 2.4227(6.9140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 24.8335, Epoch Time 486.2124(427.5479), Bit/dim 3.7339(best: 3.7541), Xent 1.2177, Loss 4.3427, Error 0.4398(best: 0.4393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 77.4061(70.1996) | Bit/dim 3.7364(3.7865) | Xent 1.2203(1.3316) | Loss 15.5106(12.1177) | Error 0.4383(0.4772) Steps 0(0.00) | Grad Norm 3.5037(6.8117) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 76.5652(70.3905) | Bit/dim 3.7400(3.7851) | Xent 1.2395(1.3289) | Loss 11.1886(12.0898) | Error 0.4429(0.4762) Steps 0(0.00) | Grad Norm 3.9901(6.7271) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 69.6590(70.3686) | Bit/dim 3.7341(3.7836) | Xent 1.2579(1.3268) | Loss 11.3386(12.0673) | Error 0.4554(0.4755) Steps 0(0.00) | Grad Norm 6.1226(6.7089) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 74.8767(70.5038) | Bit/dim 3.7490(3.7826) | Xent 1.2809(1.3254) | Loss 10.9322(12.0333) | Error 0.4630(0.4752) Steps 0(0.00) | Grad Norm 9.7788(6.8010) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 73.6498(70.5982) | Bit/dim 3.7526(3.7817) | Xent 1.3615(1.3265) | Loss 11.2668(12.0103) | Error 0.4822(0.4754) Steps 0(0.00) | Grad Norm 10.8140(6.9214) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 73.9080(70.6975) | Bit/dim 3.7504(3.7807) | Xent 1.3026(1.3257) | Loss 11.2937(11.9888) | Error 0.4616(0.4750) Steps 0(0.00) | Grad Norm 7.3773(6.9351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 25.3737, Epoch Time 487.6129(429.3498), Bit/dim 3.7492(best: 3.7339), Xent 1.2373, Loss 4.3678, Error 0.4389(best: 0.4393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 69.2758(70.6549) | Bit/dim 3.7446(3.7796) | Xent 1.2498(1.3235) | Loss 15.3913(12.0908) | Error 0.4491(0.4742) Steps 0(0.00) | Grad Norm 4.9267(6.8748) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 73.2967(70.7341) | Bit/dim 3.7526(3.7788) | Xent 1.2587(1.3215) | Loss 11.2800(12.0665) | Error 0.4456(0.4733) Steps 0(0.00) | Grad Norm 7.0876(6.8812) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 77.0762(70.9244) | Bit/dim 3.7615(3.7783) | Xent 1.2696(1.3200) | Loss 11.3052(12.0437) | Error 0.4603(0.4729) Steps 0(0.00) | Grad Norm 7.2983(6.8937) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 78.7138(71.1580) | Bit/dim 3.7493(3.7774) | Xent 1.2462(1.3178) | Loss 11.1678(12.0174) | Error 0.4443(0.4721) Steps 0(0.00) | Grad Norm 4.8579(6.8327) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 68.3834(71.0748) | Bit/dim 3.7514(3.7766) | Xent 1.2820(1.3167) | Loss 11.2568(11.9946) | Error 0.4599(0.4717) Steps 0(0.00) | Grad Norm 7.1440(6.8420) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 75.6966(71.2135) | Bit/dim 3.7624(3.7762) | Xent 1.4579(1.3209) | Loss 11.4673(11.9788) | Error 0.5126(0.4729) Steps 0(0.00) | Grad Norm 15.1557(7.0914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 24.5210, Epoch Time 482.9358(430.9574), Bit/dim 3.9795(best: 3.7339), Xent 2.9045, Loss 5.4317, Error 0.7586(best: 0.4389)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 66.8242(71.0818) | Bit/dim 3.9776(3.7823) | Xent 2.9043(1.3684) | Loss 17.5099(12.1447) | Error 0.7551(0.4814) Steps 0(0.00) | Grad Norm 50.3154(8.3881) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 65.7517(70.9219) | Bit/dim 4.1473(3.7932) | Xent 1.9982(1.3873) | Loss 12.8250(12.1651) | Error 0.6618(0.4868) Steps 0(0.00) | Grad Norm 17.7136(8.6679) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 77.8454(71.1296) | Bit/dim 4.0378(3.8005) | Xent 2.1100(1.4090) | Loss 12.9742(12.1894) | Error 0.6427(0.4915) Steps 0(0.00) | Grad Norm 18.5653(8.9648) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 77.4121(71.3181) | Bit/dim 4.1470(3.8109) | Xent 2.9596(1.4555) | Loss 14.0453(12.2451) | Error 0.8239(0.5015) Steps 0(0.00) | Grad Norm 35.4551(9.7595) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 86.5811(71.7760) | Bit/dim 4.5869(3.8342) | Xent 2.1858(1.4774) | Loss 14.3173(12.3072) | Error 0.7691(0.5095) Steps 0(0.00) | Grad Norm 25.9993(10.2467) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 88.3391(72.2728) | Bit/dim 4.3874(3.8508) | Xent 2.2495(1.5006) | Loss 14.0041(12.3581) | Error 0.7853(0.5178) Steps 0(0.00) | Grad Norm 10.0265(10.2401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 29.3575, Epoch Time 508.1457(433.2731), Bit/dim 4.4393(best: 3.7339), Xent 2.3190, Loss 5.5988, Error 0.7967(best: 0.4389)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 89.5048(72.7898) | Bit/dim 4.4434(3.8686) | Xent 2.4230(1.5283) | Loss 20.4806(12.6018) | Error 0.7889(0.5259) Steps 0(0.00) | Grad Norm 31.4851(10.8775) | Total Time 0.00(0.00)\n",
      "Iter 0866 | Time 93.2169(73.4026) | Bit/dim 4.5863(3.8901) | Xent 3.0252(1.5732) | Loss 15.5827(12.6912) | Error 0.8395(0.5353) Steps 0(0.00) | Grad Norm 1010.1185(40.8547) | Total Time 0.00(0.00)\n",
      "Iter 0867 | Time 102.4659(74.2745) | Bit/dim 5.0484(3.9249) | Xent 3.6083(1.6342) | Loss 17.2483(12.8279) | Error 0.8518(0.5448) Steps 0(0.00) | Grad Norm 113015738353.9300(3390472190.2470) | Total Time 0.00(0.00)\n",
      "Iter 0868 | Time 105.5794(75.2137) | Bit/dim 5.8971(3.9840) | Xent 4.2690(1.7133) | Loss 20.4180(13.0556) | Error 0.8709(0.5546) Steps 0(0.00) | Grad Norm 196353158.1418(3294648619.2838) | Total Time 0.00(0.00)\n",
      "Iter 0869 | Time 112.3697(76.3283) | Bit/dim 8.3506(4.1150) | Xent 6.1052(1.8450) | Loss 27.4428(13.4873) | Error 0.8804(0.5644) Steps 0(0.00) | Grad Norm 1894941.7871(3195866008.9589) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 108.0013(77.2785) | Bit/dim 21.0288(4.6224) | Xent 5.8938(1.9665) | Loss 55.8928(14.7594) | Error 0.8675(0.5735) Steps 0(0.00) | Grad Norm 25967975778.1409(3879029302.0344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 41.5596, Epoch Time 668.7704(440.3380), Bit/dim 68.0568(best: 3.7339), Xent 10.1031, Loss 73.1084, Error 0.8938(best: 0.4389)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1/epoch_72_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 1.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
