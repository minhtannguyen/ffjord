{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_noerrtol_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=392, bias=True)\n",
      "  (project_class): LinearZeros(in_features=196, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 807722\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 65.1710(65.1710) | Bit/dim 19.2813(19.2813) | Xent 2.3026(2.3026) | Loss 20.4326(20.4326) | Error 0.9041(0.9041) Steps 410(410.00) | Grad Norm 143.9924(143.9924) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 28.0140(64.0563) | Bit/dim 15.1236(19.1566) | Xent 2.2767(2.3018) | Loss 16.2619(20.3075) | Error 0.6575(0.8967) Steps 410(410.00) | Grad Norm 91.8556(142.4283) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 27.6940(62.9655) | Bit/dim 12.4784(18.9562) | Xent 2.2385(2.2999) | Loss 13.5976(20.1062) | Error 0.7549(0.8925) Steps 410(410.00) | Grad Norm 37.8391(139.2907) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 26.3811(61.8679) | Bit/dim 12.0450(18.7489) | Xent 2.1928(2.2967) | Loss 13.1414(19.8972) | Error 0.3986(0.8777) Steps 410(410.00) | Grad Norm 52.5554(136.6886) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 27.1250(60.8256) | Bit/dim 11.5485(18.5329) | Xent 2.1503(2.2923) | Loss 12.6236(19.6790) | Error 0.4169(0.8638) Steps 410(410.00) | Grad Norm 68.6425(134.6472) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 26.6930(59.8017) | Bit/dim 10.0900(18.2796) | Xent 2.1105(2.2868) | Loss 11.1452(19.4230) | Error 0.5049(0.8531) Steps 410(410.00) | Grad Norm 56.8701(132.3139) | Total Time 10.00(10.00)\n",
      "Iter 0007 | Time 27.3088(58.8269) | Bit/dim 8.7212(17.9928) | Xent 2.0658(2.2802) | Loss 9.7541(19.1329) | Error 0.4481(0.8409) Steps 410(410.00) | Grad Norm 36.4771(129.4388) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 25.2364, Epoch Time 265.9727(265.9727), Bit/dim 8.1838(best: inf), Xent 2.0079, Loss 9.1877, Error 0.3265(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0008 | Time 30.2779(57.9704) | Bit/dim 8.0938(17.6959) | Xent 2.0158(2.2723) | Loss 9.1017(18.8320) | Error 0.3233(0.8254) Steps 410(410.00) | Grad Norm 32.0189(126.5162) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 26.9944(57.0411) | Bit/dim 7.5603(17.3918) | Xent 1.9677(2.2631) | Loss 8.5442(18.5234) | Error 0.2850(0.8092) Steps 410(410.00) | Grad Norm 35.2157(123.7772) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 27.0882(56.1425) | Bit/dim 6.4426(17.0633) | Xent 1.9429(2.2535) | Loss 7.4141(18.1901) | Error 0.3374(0.7950) Steps 410(410.00) | Grad Norm 30.2977(120.9728) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 26.9742(55.2675) | Bit/dim 5.0798(16.7038) | Xent 1.9353(2.2440) | Loss 6.0475(17.8258) | Error 0.3959(0.7830) Steps 410(410.00) | Grad Norm 19.9256(117.9414) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 27.2081(54.4257) | Bit/dim 4.0858(16.3253) | Xent 1.9527(2.2353) | Loss 5.0621(17.4429) | Error 0.4754(0.7738) Steps 410(410.00) | Grad Norm 11.3937(114.7450) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 29.0571(53.6647) | Bit/dim 3.5720(15.9427) | Xent 1.9899(2.2279) | Loss 4.5669(17.0566) | Error 0.5669(0.7676) Steps 428(410.54) | Grad Norm 9.8118(111.5970) | Total Time 10.00(10.00)\n",
      "Iter 0014 | Time 31.3098(52.9940) | Bit/dim 3.3823(15.5659) | Xent 2.0363(2.2221) | Loss 4.4005(16.6769) | Error 0.6826(0.7651) Steps 440(411.42) | Grad Norm 11.5427(108.5953) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 16.9104, Epoch Time 228.2158(264.8400), Bit/dim 3.2494(best: 8.1838), Xent 2.0882, Loss 4.2935, Error 0.7893(best: 0.3265)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0015 | Time 31.5418(52.3504) | Bit/dim 3.2499(15.1964) | Xent 2.0911(2.2182) | Loss 4.2955(16.3055) | Error 0.7957(0.7660) Steps 446(412.46) | Grad Norm 12.4625(105.7113) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 34.5651(51.8169) | Bit/dim 3.1201(14.8341) | Xent 2.1491(2.2161) | Loss 4.1947(15.9422) | Error 0.8516(0.7685) Steps 464(414.01) | Grad Norm 11.8673(102.8960) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 35.5975(51.3303) | Bit/dim 2.9834(14.4786) | Xent 2.1913(2.2154) | Loss 4.0791(15.5863) | Error 0.8544(0.7711) Steps 470(415.69) | Grad Norm 10.0890(100.1118) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 36.7434(50.8927) | Bit/dim 2.9229(14.1319) | Xent 2.2226(2.2156) | Loss 4.0342(15.2397) | Error 0.8301(0.7729) Steps 488(417.86) | Grad Norm 8.4279(97.3613) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 39.9931(50.5657) | Bit/dim 2.9346(13.7960) | Xent 2.2408(2.2164) | Loss 4.0550(14.9042) | Error 0.7837(0.7732) Steps 512(420.68) | Grad Norm 7.9996(94.6804) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 42.1549(50.3134) | Bit/dim 2.9961(13.4720) | Xent 2.2585(2.2176) | Loss 4.1254(14.5808) | Error 0.8229(0.7747) Steps 524(423.78) | Grad Norm 8.2698(92.0881) | Total Time 10.00(10.00)\n",
      "Iter 0021 | Time 42.8948(50.0908) | Bit/dim 3.0060(13.1580) | Xent 2.2729(2.2193) | Loss 4.1424(14.2677) | Error 0.8489(0.7769) Steps 536(427.15) | Grad Norm 7.5642(89.5524) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 19.7886, Epoch Time 295.4063(265.7570), Bit/dim 2.9642(best: 3.2494), Xent 2.2796, Loss 4.1040, Error 0.8555(best: 0.3265)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0022 | Time 43.9203(49.9057) | Bit/dim 2.9726(12.8525) | Xent 2.2795(2.2211) | Loss 4.1124(13.9630) | Error 0.8525(0.7792) Steps 542(430.59) | Grad Norm 5.9094(87.0431) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 44.6798(49.7489) | Bit/dim 2.9557(12.5556) | Xent 2.2809(2.2229) | Loss 4.0961(13.6670) | Error 0.8511(0.7814) Steps 548(434.11) | Grad Norm 5.0299(84.5827) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 45.5409(49.6227) | Bit/dim 2.9580(12.2676) | Xent 2.2746(2.2244) | Loss 4.0953(13.3798) | Error 0.8514(0.7835) Steps 548(437.53) | Grad Norm 5.5903(82.2129) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 45.4467(49.4974) | Bit/dim 2.9404(11.9878) | Xent 2.2689(2.2258) | Loss 4.0749(13.1007) | Error 0.8400(0.7852) Steps 548(440.85) | Grad Norm 6.1410(79.9308) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 46.4909(49.4072) | Bit/dim 2.8861(11.7148) | Xent 2.2592(2.2268) | Loss 4.0157(12.8281) | Error 0.8235(0.7863) Steps 548(444.06) | Grad Norm 6.0068(77.7131) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 44.0360(49.2461) | Bit/dim 2.7972(11.4472) | Xent 2.2451(2.2273) | Loss 3.9197(12.5609) | Error 0.8056(0.7869) Steps 542(447.00) | Grad Norm 5.4736(75.5459) | Total Time 10.00(10.00)\n",
      "Iter 0028 | Time 41.9535(49.0273) | Bit/dim 2.6993(11.1848) | Xent 2.2252(2.2273) | Loss 3.8119(12.2984) | Error 0.7841(0.7868) Steps 524(449.31) | Grad Norm 4.8812(73.4259) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 18.7334, Epoch Time 343.1278(268.0781), Bit/dim 2.5952(best: 2.9642), Xent 2.1906, Loss 3.6905, Error 0.7410(best: 0.3265)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0029 | Time 40.9556(48.7851) | Bit/dim 2.6021(10.9273) | Xent 2.1966(2.2263) | Loss 3.7003(12.0405) | Error 0.7560(0.7859) Steps 512(451.19) | Grad Norm 4.3395(71.3534) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 38.0758(48.4639) | Bit/dim 2.5093(10.6748) | Xent 2.1492(2.2240) | Loss 3.5839(11.7868) | Error 0.7156(0.7838) Steps 494(452.47) | Grad Norm 4.0266(69.3336) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 37.6358(48.1390) | Bit/dim 2.4360(10.4276) | Xent 2.0970(2.2202) | Loss 3.4845(11.5377) | Error 0.6794(0.7806) Steps 482(453.36) | Grad Norm 4.2048(67.3797) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 34.4866(47.7295) | Bit/dim 2.4008(10.1868) | Xent 2.0026(2.2137) | Loss 3.4021(11.2936) | Error 0.6122(0.7756) Steps 464(453.68) | Grad Norm 5.0105(65.5086) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 32.1277(47.2614) | Bit/dim 2.4081(9.9534) | Xent 1.8563(2.2030) | Loss 3.3362(11.0549) | Error 0.5128(0.7677) Steps 452(453.63) | Grad Norm 5.4269(63.7062) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 31.6381(46.7927) | Bit/dim 2.4386(9.7280) | Xent 1.7075(2.1881) | Loss 3.2924(10.8220) | Error 0.4350(0.7577) Steps 446(453.40) | Grad Norm 5.0374(61.9461) | Total Time 10.00(10.00)\n",
      "Iter 0035 | Time 31.4585(46.3327) | Bit/dim 2.4881(9.5108) | Xent 1.5447(2.1688) | Loss 3.2604(10.5952) | Error 0.3642(0.7459) Steps 446(453.18) | Grad Norm 3.7516(60.2003) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 16.9713, Epoch Time 275.8815(268.3122), Bit/dim 2.5669(best: 2.5952), Xent 1.4057, Loss 3.2698, Error 0.3232(best: 0.3265)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0036 | Time 31.5029(45.8878) | Bit/dim 2.5607(9.3023) | Xent 1.4279(2.1466) | Loss 3.2746(10.3756) | Error 0.3275(0.7334) Steps 446(452.96) | Grad Norm 7.0229(58.6049) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 30.6360(45.4302) | Bit/dim 2.5752(9.1005) | Xent 1.3811(2.1236) | Loss 3.2657(10.1623) | Error 0.3217(0.7210) Steps 446(452.75) | Grad Norm 10.3061(57.1560) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 32.9732(45.0565) | Bit/dim 2.4600(8.9013) | Xent 1.4161(2.1024) | Loss 3.1680(9.9525) | Error 0.3253(0.7091) Steps 446(452.55) | Grad Norm 5.9528(55.6199) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 30.7729(44.6280) | Bit/dim 2.3204(8.7038) | Xent 1.4909(2.0840) | Loss 3.0658(9.7459) | Error 0.3279(0.6977) Steps 440(452.17) | Grad Norm 1.9662(54.0103) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 30.7148(44.2106) | Bit/dim 2.2531(8.5103) | Xent 1.5697(2.0686) | Loss 3.0379(9.5446) | Error 0.3519(0.6873) Steps 440(451.81) | Grad Norm 2.8273(52.4748) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 30.9261(43.8121) | Bit/dim 2.2109(8.3213) | Xent 1.6207(2.0552) | Loss 3.0212(9.3489) | Error 0.3879(0.6783) Steps 440(451.45) | Grad Norm 3.6789(51.0109) | Total Time 10.00(10.00)\n",
      "Iter 0042 | Time 30.5441(43.4140) | Bit/dim 2.1845(8.1372) | Xent 1.6358(2.0426) | Loss 3.0024(9.1585) | Error 0.4031(0.6701) Steps 440(451.11) | Grad Norm 4.0579(49.6023) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 16.8198, Epoch Time 247.0021(267.6729), Bit/dim 2.1580(best: 2.5669), Xent 1.6092, Loss 2.9626, Error 0.3904(best: 0.3232)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0043 | Time 31.5388(43.0578) | Bit/dim 2.1575(7.9578) | Xent 1.6256(2.0301) | Loss 2.9703(8.9729) | Error 0.4052(0.6621) Steps 440(450.78) | Grad Norm 4.1067(48.2374) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 31.3850(42.7076) | Bit/dim 2.1409(7.7833) | Xent 1.5582(2.0159) | Loss 2.9200(8.7913) | Error 0.3770(0.6536) Steps 440(450.45) | Grad Norm 4.0559(46.9120) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 31.0741(42.3586) | Bit/dim 2.1371(7.6139) | Xent 1.4065(1.9976) | Loss 2.8404(8.6128) | Error 0.3259(0.6438) Steps 440(450.14) | Grad Norm 3.6418(45.6139) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 29.0705(41.9599) | Bit/dim 2.1662(7.4505) | Xent 1.2156(1.9742) | Loss 2.7740(8.4376) | Error 0.2859(0.6330) Steps 428(449.48) | Grad Norm 2.4655(44.3194) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 31.1348(41.6352) | Bit/dim 2.2235(7.2937) | Xent 0.9854(1.9445) | Loss 2.7162(8.2660) | Error 0.2448(0.6214) Steps 428(448.83) | Grad Norm 2.8590(43.0756) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 29.4406(41.2694) | Bit/dim 2.2956(7.1438) | Xent 0.8668(1.9122) | Loss 2.7290(8.0999) | Error 0.2305(0.6096) Steps 428(448.21) | Grad Norm 6.9080(41.9906) | Total Time 10.00(10.00)\n",
      "Iter 0049 | Time 29.3251(40.9110) | Bit/dim 2.2613(6.9973) | Xent 0.8347(1.8799) | Loss 2.6786(7.9372) | Error 0.2285(0.5982) Steps 428(447.60) | Grad Norm 5.8222(40.9056) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 16.9998, Epoch Time 242.2252(266.9095), Bit/dim 2.1534(best: 2.1580), Xent 0.8657, Loss 2.5863, Error 0.2171(best: 0.3232)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0050 | Time 30.4118(40.5961) | Bit/dim 2.1631(6.8523) | Xent 0.8745(1.8497) | Loss 2.6004(7.7771) | Error 0.2206(0.5869) Steps 434(447.19) | Grad Norm 1.1664(39.7134) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 31.2408(40.3154) | Bit/dim 2.0975(6.7096) | Xent 0.9665(1.8232) | Loss 2.5807(7.6212) | Error 0.2511(0.5768) Steps 452(447.34) | Grad Norm 3.4070(38.6242) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 32.3928(40.0777) | Bit/dim 2.0882(6.5710) | Xent 0.9777(1.7978) | Loss 2.5770(7.4699) | Error 0.2572(0.5672) Steps 464(447.84) | Grad Norm 4.5943(37.6033) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 32.2672(39.8434) | Bit/dim 2.0915(6.4366) | Xent 0.8636(1.7698) | Loss 2.5233(7.3215) | Error 0.2352(0.5573) Steps 464(448.32) | Grad Norm 3.0937(36.5680) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 32.9989(39.6381) | Bit/dim 2.1219(6.3071) | Xent 0.7222(1.7384) | Loss 2.4830(7.1763) | Error 0.2049(0.5467) Steps 470(448.97) | Grad Norm 2.0301(35.5319) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 34.0702(39.4710) | Bit/dim 2.1558(6.1826) | Xent 0.6597(1.7060) | Loss 2.4856(7.0356) | Error 0.1941(0.5361) Steps 464(449.42) | Grad Norm 5.4125(34.6283) | Total Time 10.00(10.00)\n",
      "Iter 0056 | Time 33.6807(39.2973) | Bit/dim 2.1004(6.0601) | Xent 0.6598(1.6746) | Loss 2.4303(6.8975) | Error 0.1921(0.5258) Steps 464(449.86) | Grad Norm 2.5093(33.6647) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 17.0254, Epoch Time 256.5179(266.5978), Bit/dim 2.0417(best: 2.1534), Xent 0.7009, Loss 2.3921, Error 0.2021(best: 0.2171)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0057 | Time 32.5078(39.0936) | Bit/dim 2.0449(5.9397) | Xent 0.7227(1.6461) | Loss 2.4063(6.7627) | Error 0.2053(0.5162) Steps 464(450.28) | Grad Norm 2.6286(32.7336) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 31.5436(38.8671) | Bit/dim 2.0181(5.8220) | Xent 0.7447(1.6190) | Loss 2.3904(6.6316) | Error 0.2166(0.5072) Steps 458(450.52) | Grad Norm 3.9685(31.8707) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 30.6421(38.6204) | Bit/dim 2.0213(5.7080) | Xent 0.6725(1.5906) | Loss 2.3576(6.5033) | Error 0.1970(0.4979) Steps 446(450.38) | Grad Norm 2.0993(30.9775) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 30.3383(38.3719) | Bit/dim 2.0300(5.5977) | Xent 0.6044(1.5611) | Loss 2.3322(6.3782) | Error 0.1863(0.4885) Steps 446(450.25) | Grad Norm 3.8505(30.1637) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 31.0661(38.1527) | Bit/dim 2.0194(5.4903) | Xent 0.5939(1.5320) | Loss 2.3163(6.2563) | Error 0.1781(0.4792) Steps 446(450.12) | Grad Norm 3.1294(29.3527) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 30.3828(37.9196) | Bit/dim 1.9980(5.3856) | Xent 0.6401(1.5053) | Loss 2.3181(6.1382) | Error 0.1913(0.4706) Steps 446(450.00) | Grad Norm 4.0603(28.5939) | Total Time 10.00(10.00)\n",
      "Iter 0063 | Time 32.4927(37.7568) | Bit/dim 1.9764(5.2833) | Xent 0.6040(1.4782) | Loss 2.2784(6.0224) | Error 0.1845(0.4620) Steps 446(449.88) | Grad Norm 1.5521(27.7827) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 17.0736, Epoch Time 248.2774(266.0482), Bit/dim 1.9786(best: 2.0417), Xent 0.5667, Loss 2.2619, Error 0.1700(best: 0.2021)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0064 | Time 30.7193(37.5457) | Bit/dim 1.9828(5.1843) | Xent 0.5854(1.4515) | Loss 2.2756(5.9100) | Error 0.1770(0.4535) Steps 446(449.76) | Grad Norm 5.1733(27.1044) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 31.7453(37.3717) | Bit/dim 1.9434(5.0870) | Xent 0.6277(1.4267) | Loss 2.2572(5.8004) | Error 0.1856(0.4454) Steps 446(449.65) | Grad Norm 5.3322(26.4512) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 30.7641(37.1735) | Bit/dim 1.9711(4.9936) | Xent 0.5609(1.4008) | Loss 2.2516(5.6939) | Error 0.1653(0.4370) Steps 440(449.36) | Grad Norm 3.7589(25.7705) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 30.5752(36.9755) | Bit/dim 1.9609(4.9026) | Xent 0.5199(1.3743) | Loss 2.2208(5.5898) | Error 0.1586(0.4287) Steps 440(449.08) | Grad Norm 3.5234(25.1030) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 34.2226(36.8929) | Bit/dim 1.9250(4.8133) | Xent 0.6340(1.3521) | Loss 2.2420(5.4893) | Error 0.1901(0.4215) Steps 464(449.53) | Grad Norm 7.6072(24.5782) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 30.2409(36.6934) | Bit/dim 2.0055(4.7290) | Xent 0.5318(1.3275) | Loss 2.2714(5.3928) | Error 0.1614(0.4137) Steps 440(449.24) | Grad Norm 11.9371(24.1989) | Total Time 10.00(10.00)\n",
      "Iter 0070 | Time 32.5462(36.5690) | Bit/dim 2.0256(4.6479) | Xent 0.7458(1.3101) | Loss 2.3985(5.3030) | Error 0.2302(0.4082) Steps 458(449.50) | Grad Norm 17.9364(24.0111) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 17.3209, Epoch Time 250.5505(265.5832), Bit/dim 2.1926(best: 1.9786), Xent 0.5060, Loss 2.4456, Error 0.1315(best: 0.1700)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0071 | Time 31.5448(36.4182) | Bit/dim 2.1852(4.5740) | Xent 0.5262(1.2866) | Loss 2.4483(5.2173) | Error 0.1362(0.4000) Steps 458(449.76) | Grad Norm 46.9084(24.6980) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 34.1782(36.3510) | Bit/dim 1.9521(4.4954) | Xent 0.6601(1.2678) | Loss 2.2821(5.1293) | Error 0.2047(0.3942) Steps 470(450.37) | Grad Norm 4.7860(24.1006) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 37.9619(36.3994) | Bit/dim 1.9704(4.4196) | Xent 0.8368(1.2548) | Loss 2.3888(5.0470) | Error 0.2605(0.3902) Steps 506(452.03) | Grad Norm 8.0449(23.6189) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 38.2668(36.4554) | Bit/dim 2.2076(4.3533) | Xent 0.6116(1.2355) | Loss 2.5134(4.9710) | Error 0.1877(0.3841) Steps 494(453.29) | Grad Norm 19.9118(23.5077) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 41.0908(36.5944) | Bit/dim 2.1229(4.2864) | Xent 1.2733(1.2367) | Loss 2.7595(4.9047) | Error 0.4194(0.3852) Steps 536(455.77) | Grad Norm 24.5376(23.5386) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 41.2442(36.7339) | Bit/dim 2.0772(4.2201) | Xent 0.5607(1.2164) | Loss 2.3575(4.8283) | Error 0.1731(0.3788) Steps 518(457.64) | Grad Norm 3.8616(22.9483) | Total Time 10.00(10.00)\n",
      "Iter 0077 | Time 39.1170(36.8054) | Bit/dim 2.2182(4.1600) | Xent 0.5582(1.1966) | Loss 2.4973(4.7583) | Error 0.1651(0.3724) Steps 494(458.73) | Grad Norm 9.3170(22.5394) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 18.5659, Epoch Time 294.2739(266.4439), Bit/dim 2.1330(best: 1.9786), Xent 0.4910, Loss 2.3785, Error 0.1438(best: 0.1315)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0078 | Time 41.4093(36.9435) | Bit/dim 2.1445(4.0996) | Xent 0.5232(1.1764) | Loss 2.4061(4.6878) | Error 0.1518(0.3658) Steps 530(460.87) | Grad Norm 3.1343(21.9572) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 42.9778(37.1246) | Bit/dim 2.1370(4.0407) | Xent 0.5267(1.1569) | Loss 2.4004(4.6192) | Error 0.1580(0.3595) Steps 536(463.12) | Grad Norm 3.7600(21.4113) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 40.8627(37.2367) | Bit/dim 2.1264(3.9833) | Xent 0.5212(1.1379) | Loss 2.3870(4.5522) | Error 0.1630(0.3536) Steps 530(465.13) | Grad Norm 4.5019(20.9040) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 41.4979(37.3645) | Bit/dim 2.0835(3.9263) | Xent 0.5129(1.1191) | Loss 2.3399(4.4858) | Error 0.1566(0.3477) Steps 524(466.90) | Grad Norm 2.8001(20.3609) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 38.6956(37.4045) | Bit/dim 2.0622(3.8703) | Xent 0.4885(1.1002) | Loss 2.3064(4.4204) | Error 0.1552(0.3420) Steps 512(468.25) | Grad Norm 2.4836(19.8246) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 36.7263(37.3841) | Bit/dim 2.0607(3.8160) | Xent 0.4933(1.0820) | Loss 2.3073(4.3570) | Error 0.1486(0.3362) Steps 482(468.66) | Grad Norm 3.2804(19.3283) | Total Time 10.00(10.00)\n",
      "Iter 0084 | Time 37.2000(37.3786) | Bit/dim 2.0353(3.7626) | Xent 0.4652(1.0635) | Loss 2.2679(4.2944) | Error 0.1411(0.3303) Steps 470(468.70) | Grad Norm 1.9023(18.8055) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 17.2692, Epoch Time 308.8558(267.7163), Bit/dim 2.0148(best: 1.9786), Xent 0.4645, Loss 2.2471, Error 0.1434(best: 0.1315)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0085 | Time 35.3247(37.3170) | Bit/dim 2.0155(3.7102) | Xent 0.4675(1.0456) | Loss 2.2492(4.2330) | Error 0.1409(0.3246) Steps 470(468.74) | Grad Norm 1.9207(18.2989) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 33.5592(37.2043) | Bit/dim 2.0050(3.6591) | Xent 0.4752(1.0285) | Loss 2.2426(4.1733) | Error 0.1468(0.3193) Steps 464(468.60) | Grad Norm 1.6655(17.7999) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 33.5996(37.0961) | Bit/dim 1.9953(3.6091) | Xent 0.4799(1.0120) | Loss 2.2353(4.1152) | Error 0.1481(0.3141) Steps 464(468.46) | Grad Norm 1.4261(17.3087) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 33.0942(36.9761) | Bit/dim 1.9819(3.5603) | Xent 0.4692(0.9958) | Loss 2.2165(4.0582) | Error 0.1422(0.3090) Steps 458(468.15) | Grad Norm 1.6625(16.8393) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 32.8806(36.8532) | Bit/dim 1.9670(3.5125) | Xent 0.4566(0.9796) | Loss 2.1953(4.0023) | Error 0.1382(0.3039) Steps 458(467.84) | Grad Norm 1.7623(16.3870) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 32.7204(36.7292) | Bit/dim 1.9646(3.4661) | Xent 0.4394(0.9634) | Loss 2.1843(3.9478) | Error 0.1305(0.2987) Steps 458(467.55) | Grad Norm 1.0829(15.9279) | Total Time 10.00(10.00)\n",
      "Iter 0091 | Time 30.8219(36.5520) | Bit/dim 1.9567(3.4208) | Xent 0.4358(0.9476) | Loss 2.1747(3.8946) | Error 0.1280(0.2935) Steps 446(466.90) | Grad Norm 2.1592(15.5148) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 16.8395, Epoch Time 261.0826(267.5173), Bit/dim 1.9337(best: 1.9786), Xent 0.3993, Loss 2.1334, Error 0.1194(best: 0.1315)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0092 | Time 31.5169(36.4009) | Bit/dim 1.9390(3.3764) | Xent 0.4292(0.9320) | Loss 2.1536(3.8424) | Error 0.1271(0.2886) Steps 446(466.27) | Grad Norm 0.9344(15.0774) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 31.3074(36.2481) | Bit/dim 1.9286(3.3329) | Xent 0.4310(0.9170) | Loss 2.1441(3.7914) | Error 0.1306(0.2838) Steps 446(465.67) | Grad Norm 1.8055(14.6793) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 33.8158(36.1752) | Bit/dim 1.9067(3.2901) | Xent 0.4222(0.9021) | Loss 2.1178(3.7412) | Error 0.1280(0.2791) Steps 446(465.08) | Grad Norm 0.7299(14.2608) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 31.2211(36.0265) | Bit/dim 1.9062(3.2486) | Xent 0.4111(0.8874) | Loss 2.1117(3.6923) | Error 0.1305(0.2747) Steps 446(464.50) | Grad Norm 1.9913(13.8927) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 31.4781(35.8901) | Bit/dim 1.8946(3.2080) | Xent 0.4023(0.8728) | Loss 2.0958(3.6444) | Error 0.1264(0.2702) Steps 446(463.95) | Grad Norm 0.9422(13.5042) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 31.6936(35.7642) | Bit/dim 1.8879(3.1684) | Xent 0.3972(0.8586) | Loss 2.0865(3.5977) | Error 0.1241(0.2659) Steps 446(463.41) | Grad Norm 1.8790(13.1554) | Total Time 10.00(10.00)\n",
      "Iter 0098 | Time 31.1960(35.6272) | Bit/dim 1.8762(3.1296) | Xent 0.4181(0.8454) | Loss 2.0852(3.5523) | Error 0.1270(0.2617) Steps 446(462.89) | Grad Norm 1.4475(12.8042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 17.0802, Epoch Time 251.9544(267.0504), Bit/dim 1.8621(best: 1.9337), Xent 0.3855, Loss 2.0548, Error 0.1176(best: 0.1194)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0099 | Time 31.1267(35.4921) | Bit/dim 1.8661(3.0917) | Xent 0.3959(0.8319) | Loss 2.0641(3.5077) | Error 0.1194(0.2574) Steps 446(462.38) | Grad Norm 0.9118(12.4474) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 31.7130(35.3788) | Bit/dim 1.8666(3.0550) | Xent 0.3868(0.8185) | Loss 2.0600(3.4642) | Error 0.1174(0.2532) Steps 446(461.89) | Grad Norm 2.5121(12.1494) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 32.0225(35.2781) | Bit/dim 1.8636(3.0192) | Xent 0.4135(0.8064) | Loss 2.0704(3.4224) | Error 0.1200(0.2492) Steps 452(461.59) | Grad Norm 4.2355(11.9120) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 30.3110(35.1291) | Bit/dim 1.8612(2.9845) | Xent 0.3905(0.7939) | Loss 2.0565(3.3814) | Error 0.1261(0.2455) Steps 440(460.95) | Grad Norm 5.2645(11.7125) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 31.9366(35.0333) | Bit/dim 1.8787(2.9513) | Xent 0.4005(0.7821) | Loss 2.0790(3.3424) | Error 0.1169(0.2417) Steps 458(460.86) | Grad Norm 7.6459(11.5905) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 30.4136(34.8947) | Bit/dim 1.9498(2.9213) | Xent 0.3841(0.7702) | Loss 2.1418(3.3063) | Error 0.1102(0.2377) Steps 440(460.23) | Grad Norm 11.7945(11.5966) | Total Time 10.00(10.00)\n",
      "Iter 0105 | Time 31.9971(34.8078) | Bit/dim 2.0155(2.8941) | Xent 0.4451(0.7604) | Loss 2.2381(3.2743) | Error 0.1294(0.2345) Steps 464(460.34) | Grad Norm 9.9298(11.5466) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 17.5567, Epoch Time 249.4753(266.5232), Bit/dim 1.8891(best: 1.8621), Xent 0.3732, Loss 2.0757, Error 0.1153(best: 0.1176)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0106 | Time 31.7627(34.7164) | Bit/dim 1.8971(2.8642) | Xent 0.4020(0.7497) | Loss 2.0981(3.2390) | Error 0.1212(0.2311) Steps 458(460.27) | Grad Norm 4.5761(11.3375) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 30.6377(34.5941) | Bit/dim 1.9441(2.8366) | Xent 0.3874(0.7388) | Loss 2.1378(3.2060) | Error 0.1078(0.2274) Steps 452(460.03) | Grad Norm 6.1187(11.1810) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 30.7764(34.4795) | Bit/dim 1.9012(2.8085) | Xent 0.3745(0.7279) | Loss 2.0885(3.1725) | Error 0.1109(0.2239) Steps 446(459.60) | Grad Norm 4.5408(10.9818) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 30.1833(34.3506) | Bit/dim 1.8604(2.7801) | Xent 0.3830(0.7175) | Loss 2.0519(3.1388) | Error 0.1152(0.2206) Steps 440(459.02) | Grad Norm 3.1409(10.7465) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 31.0405(34.2513) | Bit/dim 1.8729(2.7529) | Xent 0.4050(0.7081) | Loss 2.0754(3.1069) | Error 0.1172(0.2175) Steps 446(458.63) | Grad Norm 5.2452(10.5815) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 31.1907(34.1595) | Bit/dim 1.8315(2.7252) | Xent 0.3689(0.6980) | Loss 2.0159(3.0742) | Error 0.1119(0.2144) Steps 452(458.43) | Grad Norm 2.6702(10.3442) | Total Time 10.00(10.00)\n",
      "Iter 0112 | Time 31.6980(34.0857) | Bit/dim 1.8632(2.6994) | Xent 0.3728(0.6882) | Loss 2.0496(3.0435) | Error 0.1092(0.2112) Steps 452(458.23) | Grad Norm 4.1733(10.1590) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 17.0829, Epoch Time 246.4854(265.9220), Bit/dim 1.8281(best: 1.8621), Xent 0.3354, Loss 1.9958, Error 0.1014(best: 0.1153)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0113 | Time 32.9161(34.0506) | Bit/dim 1.8350(2.6734) | Xent 0.3672(0.6786) | Loss 2.0186(3.0127) | Error 0.1099(0.2082) Steps 458(458.23) | Grad Norm 2.6476(9.9337) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 30.5815(33.9465) | Bit/dim 1.8187(2.6478) | Xent 0.3704(0.6693) | Loss 2.0039(2.9824) | Error 0.1132(0.2053) Steps 452(458.04) | Grad Norm 2.4123(9.7080) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 31.0031(33.8582) | Bit/dim 1.8065(2.6225) | Xent 0.3640(0.6602) | Loss 1.9885(2.9526) | Error 0.1075(0.2024) Steps 452(457.86) | Grad Norm 2.4194(9.4894) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 31.6315(33.7914) | Bit/dim 1.8119(2.5982) | Xent 0.3369(0.6505) | Loss 1.9804(2.9235) | Error 0.1012(0.1993) Steps 452(457.68) | Grad Norm 3.1884(9.3004) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 31.2555(33.7153) | Bit/dim 1.7760(2.5736) | Xent 0.3192(0.6405) | Loss 1.9356(2.8938) | Error 0.0945(0.1962) Steps 452(457.51) | Grad Norm 1.3098(9.0606) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 33.1911(33.6996) | Bit/dim 1.7666(2.5494) | Xent 0.3320(0.6313) | Loss 1.9326(2.8650) | Error 0.0978(0.1932) Steps 452(457.35) | Grad Norm 2.0461(8.8502) | Total Time 10.00(10.00)\n",
      "Iter 0119 | Time 30.8758(33.6149) | Bit/dim 1.7512(2.5254) | Xent 0.3701(0.6234) | Loss 1.9362(2.8371) | Error 0.1088(0.1907) Steps 452(457.19) | Grad Norm 1.5646(8.6316) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 17.3374, Epoch Time 251.0827(265.4768), Bit/dim 1.7287(best: 1.8281), Xent 0.3323, Loss 1.8949, Error 0.0944(best: 0.1014)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0120 | Time 31.1319(33.5404) | Bit/dim 1.7327(2.5016) | Xent 0.3540(0.6154) | Loss 1.9097(2.8093) | Error 0.1039(0.1881) Steps 452(457.03) | Grad Norm 1.2601(8.4105) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 30.7933(33.4580) | Bit/dim 1.7122(2.4779) | Xent 0.3472(0.6073) | Loss 1.8858(2.7816) | Error 0.1022(0.1855) Steps 452(456.88) | Grad Norm 1.2483(8.1956) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 32.1549(33.4189) | Bit/dim 1.7133(2.4550) | Xent 0.3388(0.5993) | Loss 1.8827(2.7546) | Error 0.1020(0.1830) Steps 452(456.73) | Grad Norm 1.3346(7.9898) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 30.4724(33.3305) | Bit/dim 1.7058(2.4325) | Xent 0.3248(0.5910) | Loss 1.8682(2.7280) | Error 0.0966(0.1804) Steps 446(456.41) | Grad Norm 2.5447(7.8264) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 31.2940(33.2694) | Bit/dim 1.6885(2.4102) | Xent 0.3442(0.5836) | Loss 1.8606(2.7020) | Error 0.1046(0.1782) Steps 446(456.10) | Grad Norm 3.0484(7.6831) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 31.5674(33.2183) | Bit/dim 1.6717(2.3881) | Xent 0.3486(0.5766) | Loss 1.8460(2.6763) | Error 0.1088(0.1761) Steps 452(455.98) | Grad Norm 3.6554(7.5623) | Total Time 10.00(10.00)\n",
      "Iter 0126 | Time 31.7622(33.1747) | Bit/dim 1.6796(2.3668) | Xent 0.3299(0.5692) | Loss 1.8445(2.6514) | Error 0.0961(0.1737) Steps 452(455.86) | Grad Norm 6.2981(7.5243) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 16.7012, Epoch Time 247.9339(264.9506), Bit/dim 1.7117(best: 1.7287), Xent 0.4873, Loss 1.9553, Error 0.1549(best: 0.0944)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0127 | Time 31.5301(33.1253) | Bit/dim 1.7191(2.3474) | Xent 0.4867(0.5667) | Loss 1.9625(2.6307) | Error 0.1566(0.1732) Steps 446(455.56) | Grad Norm 14.9467(7.7470) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 31.0263(33.0624) | Bit/dim 2.2887(2.3456) | Xent 0.4458(0.5631) | Loss 2.5116(2.6271) | Error 0.1120(0.1713) Steps 446(455.28) | Grad Norm 22.7524(8.1972) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 33.0632(33.0624) | Bit/dim 1.7755(2.3285) | Xent 0.4175(0.5587) | Loss 1.9843(2.6079) | Error 0.1261(0.1700) Steps 464(455.54) | Grad Norm 5.2661(8.1092) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 33.3797(33.0719) | Bit/dim 1.9397(2.3168) | Xent 0.5069(0.5571) | Loss 2.1932(2.5954) | Error 0.1515(0.1694) Steps 482(456.33) | Grad Norm 8.6536(8.1256) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 35.0825(33.1322) | Bit/dim 1.8286(2.3022) | Xent 0.3267(0.5502) | Loss 1.9920(2.5773) | Error 0.0989(0.1673) Steps 482(457.10) | Grad Norm 3.4280(7.9846) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 34.4787(33.1726) | Bit/dim 1.8297(2.2880) | Xent 0.3597(0.5445) | Loss 2.0096(2.5603) | Error 0.1074(0.1655) Steps 488(458.03) | Grad Norm 2.6244(7.8238) | Total Time 10.00(10.00)\n",
      "Iter 0133 | Time 35.7300(33.2493) | Bit/dim 1.8324(2.2744) | Xent 0.3405(0.5384) | Loss 2.0026(2.5436) | Error 0.1074(0.1638) Steps 488(458.93) | Grad Norm 2.8804(7.6755) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 17.7515, Epoch Time 264.3492(264.9325), Bit/dim 1.8206(best: 1.7117), Xent 0.3339, Loss 1.9875, Error 0.0982(best: 0.0944)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0134 | Time 38.2393(33.3990) | Bit/dim 1.8216(2.2608) | Xent 0.3436(0.5325) | Loss 1.9934(2.5270) | Error 0.1028(0.1619) Steps 488(459.80) | Grad Norm 2.2921(7.5140) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 37.5218(33.5227) | Bit/dim 1.7930(2.2467) | Xent 0.3871(0.5282) | Loss 1.9866(2.5108) | Error 0.1108(0.1604) Steps 500(461.01) | Grad Norm 2.8761(7.3749) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 34.7954(33.5609) | Bit/dim 1.7741(2.2326) | Xent 0.4415(0.5256) | Loss 1.9949(2.4954) | Error 0.1344(0.1596) Steps 476(461.45) | Grad Norm 8.0632(7.3955) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 34.7466(33.5965) | Bit/dim 1.8472(2.2210) | Xent 1.6626(0.5597) | Loss 2.6785(2.5008) | Error 0.4116(0.1672) Steps 506(462.79) | Grad Norm 19.3578(7.7544) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 37.5414(33.7148) | Bit/dim 2.1162(2.2179) | Xent 0.5076(0.5581) | Loss 2.3700(2.4969) | Error 0.1155(0.1656) Steps 530(464.81) | Grad Norm 16.5065(8.0170) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 37.2028(33.8195) | Bit/dim 2.0003(2.2113) | Xent 0.3891(0.5531) | Loss 2.1948(2.4879) | Error 0.1146(0.1641) Steps 542(467.12) | Grad Norm 4.3263(7.9062) | Total Time 10.00(10.00)\n",
      "Iter 0140 | Time 37.7522(33.9374) | Bit/dim 1.9400(2.2032) | Xent 0.7266(0.5583) | Loss 2.3033(2.4823) | Error 0.2451(0.1665) Steps 560(469.91) | Grad Norm 5.7548(7.8417) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 18.9920, Epoch Time 289.0042(265.6547), Bit/dim 1.8695(best: 1.7117), Xent 0.5016, Loss 2.1203, Error 0.1614(best: 0.0944)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0141 | Time 37.3576(34.0400) | Bit/dim 1.8736(2.1933) | Xent 0.4982(0.5565) | Loss 2.1227(2.4715) | Error 0.1633(0.1664) Steps 548(472.25) | Grad Norm 2.8570(7.6922) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 34.8921(34.0656) | Bit/dim 1.7892(2.1812) | Xent 0.5607(0.5566) | Loss 2.0695(2.4595) | Error 0.1894(0.1671) Steps 500(473.08) | Grad Norm 3.9669(7.5804) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 34.3057(34.0728) | Bit/dim 1.7254(2.1675) | Xent 0.5584(0.5566) | Loss 2.0046(2.4458) | Error 0.1785(0.1675) Steps 470(472.99) | Grad Norm 6.9318(7.5610) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 36.4032(34.1427) | Bit/dim 1.8023(2.1565) | Xent 1.0513(0.5715) | Loss 2.3279(2.4423) | Error 0.3019(0.1715) Steps 494(473.62) | Grad Norm 25.1884(8.0898) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 34.5099(34.1537) | Bit/dim 2.2902(2.1606) | Xent 10.8871(0.8810) | Loss 7.7337(2.6010) | Error 0.7710(0.1895) Steps 488(474.05) | Grad Norm 98.8597(10.8129) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 35.8941(34.2059) | Bit/dim 2.5600(2.1725) | Xent 1.6875(0.9052) | Loss 3.4037(2.6251) | Error 0.4265(0.1966) Steps 536(475.91) | Grad Norm 13.4966(10.8934) | Total Time 10.00(10.00)\n",
      "Iter 0147 | Time 38.1846(34.3253) | Bit/dim 3.1759(2.2026) | Xent 1.9099(0.9353) | Loss 4.1309(2.6703) | Error 0.4550(0.2043) Steps 572(478.79) | Grad Norm 39.6778(11.7569) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 22.3784, Epoch Time 286.3195(266.2746), Bit/dim 19.7133(best: 1.7117), Xent 18.8157, Loss 29.1211, Error 0.7805(best: 0.0944)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0148 | Time 39.8586(34.4913) | Bit/dim 19.7243(2.7283) | Xent 19.2671(1.4852) | Loss 29.3578(3.4709) | Error 0.7953(0.2221) Steps 644(483.75) | Grad Norm 510.7908(26.7279) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 45.8922(34.8333) | Bit/dim 8.8921(2.9132) | Xent 4.6454(1.5801) | Loss 11.2149(3.7032) | Error 0.4988(0.2304) Steps 746(491.62) | Grad Norm 168.5552(30.9828) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 77.5502(36.1148) | Bit/dim 3.0307(2.9167) | Xent 3.1398(1.6268) | Loss 4.6006(3.7302) | Error 0.7802(0.2469) Steps 1208(513.11) | Grad Norm 715098.6138(21483.0117) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 90.9766(37.7607) | Bit/dim 4.9449(2.9776) | Xent 3.8657(1.6940) | Loss 6.8777(3.8246) | Error 0.8648(0.2654) Steps 1442(540.98) | Grad Norm 455936.0882(34516.6040) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_noerrtol_run1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --lr 0.01 --warmup_iters 113 --atol 1e-5  --rtol 1e-5 --weight_y 0.5 --condition_ratio 0.25\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
