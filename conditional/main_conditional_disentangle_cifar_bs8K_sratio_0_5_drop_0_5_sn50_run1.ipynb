{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn50_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=50, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 98.5589(98.5589) | Bit/dim 11.0259(11.0259) | Xent 2.3026(2.3026) | Loss 12.1772(12.1772) | Error 0.8990(0.8990) Steps 574(574.00) | Grad Norm 68.4517(68.4517) | Total Time 14.00(14.00)\n",
      "Iter 0002 | Time 48.1801(97.0475) | Bit/dim 10.5594(11.0119) | Xent 2.2925(2.3023) | Loss 11.7056(12.1630) | Error 0.7659(0.8950) Steps 574(574.00) | Grad Norm 57.9515(68.1367) | Total Time 14.00(14.00)\n",
      "Iter 0003 | Time 47.8382(95.5712) | Bit/dim 10.0543(10.9832) | Xent 2.2796(2.3016) | Loss 11.1941(12.1340) | Error 0.7691(0.8912) Steps 574(574.00) | Grad Norm 45.0233(67.4433) | Total Time 14.00(14.00)\n",
      "Iter 0004 | Time 46.5479(94.1005) | Bit/dim 9.5915(10.9414) | Xent 2.2610(2.3004) | Loss 10.7220(12.0916) | Error 0.7612(0.8873) Steps 574(574.00) | Grad Norm 31.3339(66.3600) | Total Time 14.00(14.00)\n",
      "Iter 0005 | Time 48.2044(92.7236) | Bit/dim 9.1781(10.8885) | Xent 2.2416(2.2986) | Loss 10.2989(12.0378) | Error 0.7584(0.8835) Steps 574(574.00) | Grad Norm 19.2422(64.9465) | Total Time 14.00(14.00)\n",
      "Iter 0006 | Time 47.3058(91.3611) | Bit/dim 8.9900(10.8316) | Xent 2.2225(2.2963) | Loss 10.1013(11.9797) | Error 0.7681(0.8800) Steps 574(574.00) | Grad Norm 19.1200(63.5717) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 37.3723, Epoch Time 390.9728(390.9728), Bit/dim 8.8737(best: inf), Xent 2.1994, Loss 9.9734, Error 0.7611(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 59.0373(90.3914) | Bit/dim 8.8947(10.7735) | Xent 2.2046(2.2936) | Loss 9.9970(11.9203) | Error 0.7721(0.8768) Steps 574(574.00) | Grad Norm 25.0393(62.4157) | Total Time 14.00(14.00)\n",
      "Iter 0008 | Time 49.9381(89.1778) | Bit/dim 8.8700(10.7164) | Xent 2.1869(2.2904) | Loss 9.9634(11.8616) | Error 0.7679(0.8735) Steps 574(574.00) | Grad Norm 30.2161(61.4497) | Total Time 14.00(14.00)\n",
      "Iter 0009 | Time 48.5722(87.9596) | Bit/dim 8.8946(10.6617) | Xent 2.1633(2.2866) | Loss 9.9763(11.8050) | Error 0.7628(0.8702) Steps 574(574.00) | Grad Norm 31.3614(60.5471) | Total Time 14.00(14.00)\n",
      "Iter 0010 | Time 48.2848(86.7694) | Bit/dim 8.8297(10.6067) | Xent 2.1557(2.2827) | Loss 9.9076(11.7481) | Error 0.7494(0.8666) Steps 574(574.00) | Grad Norm 29.2099(59.6070) | Total Time 14.00(14.00)\n",
      "Iter 0011 | Time 48.7679(85.6293) | Bit/dim 8.7226(10.5502) | Xent 2.1342(2.2782) | Loss 9.7897(11.6893) | Error 0.7345(0.8626) Steps 574(574.00) | Grad Norm 23.9520(58.5373) | Total Time 14.00(14.00)\n",
      "Iter 0012 | Time 47.6014(84.4885) | Bit/dim 8.6129(10.4921) | Xent 2.1258(2.2736) | Loss 9.6758(11.6289) | Error 0.7260(0.8585) Steps 574(574.00) | Grad Norm 19.1282(57.3550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 25.7467, Epoch Time 343.3192(389.5432), Bit/dim 8.4983(best: 8.8737), Xent 2.1094, Loss 9.5530, Error 0.7153(best: 0.7611)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 48.4774(83.4082) | Bit/dim 8.5002(10.4323) | Xent 2.1267(2.2692) | Loss 9.5635(11.5670) | Error 0.7409(0.8550) Steps 574(574.00) | Grad Norm 17.1334(56.1484) | Total Time 14.00(14.00)\n",
      "Iter 0014 | Time 48.3924(82.3577) | Bit/dim 8.3881(10.3710) | Xent 2.1127(2.2645) | Loss 9.4445(11.5033) | Error 0.7363(0.8514) Steps 574(574.00) | Grad Norm 17.5254(54.9897) | Total Time 14.00(14.00)\n",
      "Iter 0015 | Time 47.7285(81.3188) | Bit/dim 8.2982(10.3088) | Xent 2.1015(2.2596) | Loss 9.3489(11.4386) | Error 0.7349(0.8479) Steps 574(574.00) | Grad Norm 17.8576(53.8757) | Total Time 14.00(14.00)\n",
      "Iter 0016 | Time 48.6310(80.3382) | Bit/dim 8.1739(10.2448) | Xent 2.0920(2.2546) | Loss 9.2199(11.3721) | Error 0.7256(0.8442) Steps 574(574.00) | Grad Norm 16.2076(52.7457) | Total Time 14.00(14.00)\n",
      "Iter 0017 | Time 47.7222(79.3597) | Bit/dim 8.0703(10.1796) | Xent 2.0724(2.2491) | Loss 9.1065(11.3041) | Error 0.6992(0.8399) Steps 574(574.00) | Grad Norm 14.1789(51.5887) | Total Time 14.00(14.00)\n",
      "Iter 0018 | Time 48.8428(78.4442) | Bit/dim 7.9475(10.1126) | Xent 2.0439(2.2430) | Loss 8.9694(11.2341) | Error 0.6836(0.8352) Steps 574(574.00) | Grad Norm 12.2458(50.4084) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 25.6684, Epoch Time 330.9808(387.7864), Bit/dim 7.8366(best: 8.4983), Xent 2.0416, Loss 8.8574, Error 0.6803(best: 0.7153)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 47.1250(77.5046) | Bit/dim 7.8421(10.0445) | Xent 2.0486(2.2371) | Loss 8.8664(11.1630) | Error 0.6861(0.8307) Steps 574(574.00) | Grad Norm 11.4739(49.2404) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 47.3197(76.5991) | Bit/dim 7.7414(9.9754) | Xent 2.0497(2.2315) | Loss 8.7662(11.0911) | Error 0.6841(0.8263) Steps 574(574.00) | Grad Norm 10.7769(48.0865) | Total Time 14.00(14.00)\n",
      "Iter 0021 | Time 48.7380(75.7632) | Bit/dim 7.6389(9.9053) | Xent 2.0361(2.2257) | Loss 8.6570(11.0181) | Error 0.6905(0.8223) Steps 574(574.00) | Grad Norm 9.3313(46.9238) | Total Time 14.00(14.00)\n",
      "Iter 0022 | Time 46.7630(74.8932) | Bit/dim 7.5651(9.8351) | Xent 2.0437(2.2202) | Loss 8.5870(10.9452) | Error 0.7040(0.8187) Steps 574(574.00) | Grad Norm 8.1582(45.7608) | Total Time 14.00(14.00)\n",
      "Iter 0023 | Time 48.0559(74.0881) | Bit/dim 7.5100(9.7653) | Xent 2.0423(2.2149) | Loss 8.5312(10.8728) | Error 0.6985(0.8151) Steps 574(574.00) | Grad Norm 8.4409(44.6412) | Total Time 14.00(14.00)\n",
      "Iter 0024 | Time 47.8996(73.3025) | Bit/dim 7.4732(9.6966) | Xent 2.0595(2.2102) | Loss 8.5030(10.8017) | Error 0.7120(0.8120) Steps 574(574.00) | Grad Norm 9.3323(43.5820) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 25.5898, Epoch Time 327.0073(385.9630), Bit/dim 7.4328(best: 7.8366), Xent 2.0491, Loss 8.4574, Error 0.6983(best: 0.6803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 47.8248(72.5381) | Bit/dim 7.4396(9.6289) | Xent 2.0529(2.2055) | Loss 8.4661(10.7316) | Error 0.7081(0.8089) Steps 574(574.00) | Grad Norm 9.1813(42.5500) | Total Time 14.00(14.00)\n",
      "Iter 0026 | Time 47.6180(71.7905) | Bit/dim 7.3748(9.5612) | Xent 2.0559(2.2010) | Loss 8.4027(10.6617) | Error 0.7060(0.8058) Steps 574(574.00) | Grad Norm 8.1098(41.5167) | Total Time 14.00(14.00)\n",
      "Iter 0027 | Time 48.5157(71.0923) | Bit/dim 7.3088(9.4937) | Xent 2.0633(2.1969) | Loss 8.3405(10.5921) | Error 0.7016(0.8027) Steps 574(574.00) | Grad Norm 6.9705(40.4804) | Total Time 14.00(14.00)\n",
      "Iter 0028 | Time 46.9885(70.3692) | Bit/dim 7.2423(9.4261) | Xent 2.0747(2.1932) | Loss 8.2796(10.5227) | Error 0.7120(0.8000) Steps 574(574.00) | Grad Norm 6.3588(39.4567) | Total Time 14.00(14.00)\n",
      "Iter 0029 | Time 47.1009(69.6711) | Bit/dim 7.1817(9.3588) | Xent 2.0667(2.1894) | Loss 8.2150(10.4535) | Error 0.6986(0.7969) Steps 574(574.00) | Grad Norm 5.4251(38.4358) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 48.9669(69.0500) | Bit/dim 7.1455(9.2924) | Xent 2.0643(2.1857) | Loss 8.1777(10.3852) | Error 0.7034(0.7941) Steps 580(574.18) | Grad Norm 4.6876(37.4233) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 25.9153, Epoch Time 328.2122(384.2305), Bit/dim 7.1275(best: 7.4328), Xent 2.0667, Loss 8.1608, Error 0.7101(best: 0.6803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 49.3095(68.4578) | Bit/dim 7.1293(9.2275) | Xent 2.0674(2.1821) | Loss 8.1630(10.3186) | Error 0.7147(0.7917) Steps 580(574.35) | Grad Norm 4.7090(36.4419) | Total Time 14.00(14.00)\n",
      "Iter 0032 | Time 49.4938(67.8889) | Bit/dim 7.1116(9.1640) | Xent 2.0734(2.1788) | Loss 8.1482(10.2534) | Error 0.7315(0.7899) Steps 580(574.52) | Grad Norm 4.8433(35.4939) | Total Time 14.00(14.00)\n",
      "Iter 0033 | Time 49.9737(67.3514) | Bit/dim 7.1042(9.1022) | Xent 2.0768(2.1758) | Loss 8.1427(10.1901) | Error 0.7400(0.7884) Steps 580(574.69) | Grad Norm 5.0413(34.5804) | Total Time 14.00(14.00)\n",
      "Iter 0034 | Time 49.1794(66.8062) | Bit/dim 7.0912(9.0419) | Xent 2.0725(2.1727) | Loss 8.1274(10.1282) | Error 0.7411(0.7870) Steps 580(574.85) | Grad Norm 4.8814(33.6894) | Total Time 14.00(14.00)\n",
      "Iter 0035 | Time 50.8225(66.3267) | Bit/dim 7.0883(8.9833) | Xent 2.0591(2.1693) | Loss 8.1179(10.0679) | Error 0.7212(0.7850) Steps 580(575.00) | Grad Norm 5.2977(32.8376) | Total Time 14.00(14.00)\n",
      "Iter 0036 | Time 50.5775(65.8543) | Bit/dim 7.0592(8.9256) | Xent 2.0411(2.1654) | Loss 8.0798(10.0083) | Error 0.7141(0.7829) Steps 580(575.15) | Grad Norm 5.1664(32.0075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 25.7690, Epoch Time 340.7971(382.9275), Bit/dim 7.0346(best: 7.1275), Xent 2.0361, Loss 8.0527, Error 0.6978(best: 0.6803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 49.8413(65.3739) | Bit/dim 7.0321(8.8688) | Xent 2.0486(2.1619) | Loss 8.0564(9.9497) | Error 0.7087(0.7807) Steps 580(575.30) | Grad Norm 7.7076(31.2785) | Total Time 14.00(14.00)\n",
      "Iter 0038 | Time 49.4940(64.8975) | Bit/dim 7.0095(8.8130) | Xent 2.0706(2.1592) | Loss 8.0449(9.8926) | Error 0.7348(0.7793) Steps 580(575.44) | Grad Norm 13.5665(30.7471) | Total Time 14.00(14.00)\n",
      "Iter 0039 | Time 49.4961(64.4354) | Bit/dim 7.0113(8.7589) | Xent 2.1079(2.1577) | Loss 8.0653(9.8378) | Error 0.7670(0.7789) Steps 580(575.58) | Grad Norm 21.5097(30.4700) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 49.8072(63.9966) | Bit/dim 7.0095(8.7065) | Xent 2.1305(2.1568) | Loss 8.0747(9.7849) | Error 0.7725(0.7787) Steps 580(575.71) | Grad Norm 25.3907(30.3176) | Total Time 14.00(14.00)\n",
      "Iter 0041 | Time 50.3235(63.5864) | Bit/dim 6.9984(8.6552) | Xent 2.0800(2.1545) | Loss 8.0384(9.7325) | Error 0.7492(0.7779) Steps 580(575.84) | Grad Norm 15.8948(29.8850) | Total Time 14.00(14.00)\n",
      "Iter 0042 | Time 50.1870(63.1844) | Bit/dim 6.9820(8.6050) | Xent 2.0321(2.1509) | Loss 7.9981(9.6804) | Error 0.7157(0.7760) Steps 580(575.96) | Grad Norm 4.0575(29.1101) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 26.3333, Epoch Time 340.9043(381.6668), Bit/dim 6.9823(best: 7.0346), Xent 2.0699, Loss 8.0172, Error 0.7555(best: 0.6803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 50.5857(62.8064) | Bit/dim 6.9795(8.5563) | Xent 2.0879(2.1490) | Loss 8.0234(9.6307) | Error 0.7672(0.7757) Steps 586(576.26) | Grad Norm 19.9559(28.8355) | Total Time 14.00(14.00)\n",
      "Iter 0044 | Time 50.8537(62.4479) | Bit/dim 6.9741(8.5088) | Xent 2.1344(2.1485) | Loss 8.0413(9.5831) | Error 0.7714(0.7756) Steps 586(576.56) | Grad Norm 26.2604(28.7583) | Total Time 14.00(14.00)\n",
      "Iter 0045 | Time 50.7018(62.0955) | Bit/dim 6.9582(8.4623) | Xent 2.0557(2.1457) | Loss 7.9861(9.5351) | Error 0.7445(0.7747) Steps 592(577.02) | Grad Norm 16.9499(28.4040) | Total Time 14.00(14.00)\n",
      "Iter 0046 | Time 50.9736(61.7618) | Bit/dim 6.9357(8.4165) | Xent 2.0219(2.1420) | Loss 7.9466(9.4875) | Error 0.7040(0.7725) Steps 592(577.47) | Grad Norm 2.2864(27.6205) | Total Time 14.00(14.00)\n",
      "Iter 0047 | Time 50.8174(61.4335) | Bit/dim 6.9324(8.3719) | Xent 2.0358(2.1388) | Loss 7.9503(9.4414) | Error 0.7105(0.7707) Steps 592(577.90) | Grad Norm 13.0189(27.1824) | Total Time 14.00(14.00)\n",
      "Iter 0048 | Time 52.8942(61.1773) | Bit/dim 6.9130(8.3282) | Xent 2.0323(2.1356) | Loss 7.9291(9.3960) | Error 0.7245(0.7693) Steps 598(578.51) | Grad Norm 12.3974(26.7389) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 26.8842, Epoch Time 349.1687(380.6918), Bit/dim 6.9107(best: 6.9823), Xent 1.9951, Loss 7.9083, Error 0.6768(best: 0.6803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 53.1085(60.9353) | Bit/dim 6.9108(8.2857) | Xent 2.0014(2.1316) | Loss 7.9115(9.3515) | Error 0.6836(0.7667) Steps 598(579.09) | Grad Norm 2.6814(26.0172) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 54.4182(60.7397) | Bit/dim 6.8985(8.2440) | Xent 2.0202(2.1283) | Loss 7.9085(9.3082) | Error 0.6980(0.7647) Steps 598(579.66) | Grad Norm 9.1936(25.5124) | Total Time 14.00(14.00)\n",
      "Iter 0051 | Time 53.0503(60.5091) | Bit/dim 6.8951(8.2036) | Xent 2.0325(2.1254) | Loss 7.9114(9.2663) | Error 0.7230(0.7634) Steps 598(580.21) | Grad Norm 13.0486(25.1385) | Total Time 14.00(14.00)\n",
      "Iter 0052 | Time 53.7719(60.3069) | Bit/dim 6.8797(8.1639) | Xent 2.0030(2.1217) | Loss 7.8812(9.2247) | Error 0.6754(0.7608) Steps 598(580.74) | Grad Norm 6.4635(24.5783) | Total Time 14.00(14.00)\n",
      "Iter 0053 | Time 53.6255(60.1065) | Bit/dim 6.8686(8.1250) | Xent 2.0103(2.1184) | Loss 7.8737(9.1842) | Error 0.6804(0.7584) Steps 598(581.26) | Grad Norm 7.0538(24.0525) | Total Time 14.00(14.00)\n",
      "Iter 0054 | Time 52.4378(59.8764) | Bit/dim 6.8574(8.0870) | Xent 2.0344(2.1159) | Loss 7.8746(9.1449) | Error 0.7229(0.7573) Steps 598(581.76) | Grad Norm 15.8671(23.8070) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 26.6239, Epoch Time 362.5976(380.1490), Bit/dim 6.8438(best: 6.9107), Xent 2.0211, Loss 7.8543, Error 0.6918(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 52.8931(59.6669) | Bit/dim 6.8400(8.0496) | Xent 2.0298(2.1133) | Loss 7.8549(9.1062) | Error 0.7027(0.7557) Steps 598(582.25) | Grad Norm 15.3976(23.5547) | Total Time 14.00(14.00)\n",
      "Iter 0056 | Time 52.2330(59.4439) | Bit/dim 6.8139(8.0125) | Xent 2.0030(2.1100) | Loss 7.8154(9.0675) | Error 0.6996(0.7540) Steps 598(582.72) | Grad Norm 6.9809(23.0575) | Total Time 14.00(14.00)\n",
      "Iter 0057 | Time 53.0635(59.2525) | Bit/dim 6.8115(7.9765) | Xent 1.9960(2.1066) | Loss 7.8094(9.0297) | Error 0.6767(0.7517) Steps 598(583.18) | Grad Norm 5.0697(22.5179) | Total Time 14.00(14.00)\n",
      "Iter 0058 | Time 53.5060(59.0801) | Bit/dim 6.7994(7.9412) | Xent 2.0133(2.1038) | Loss 7.8060(8.9930) | Error 0.6803(0.7495) Steps 598(583.63) | Grad Norm 13.9845(22.2619) | Total Time 14.00(14.00)\n",
      "Iter 0059 | Time 53.4728(58.9119) | Bit/dim 6.7865(7.9065) | Xent 2.0247(2.1014) | Loss 7.7988(8.9572) | Error 0.7224(0.7487) Steps 598(584.06) | Grad Norm 18.9845(22.1635) | Total Time 14.00(14.00)\n",
      "Iter 0060 | Time 52.7206(58.7261) | Bit/dim 6.7643(7.8722) | Xent 2.0131(2.0987) | Loss 7.7708(8.9216) | Error 0.7017(0.7473) Steps 598(584.47) | Grad Norm 18.9144(22.0661) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 26.6957, Epoch Time 360.3846(379.5561), Bit/dim 6.7382(best: 6.8438), Xent 2.0072, Loss 7.7418, Error 0.6970(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 52.5891(58.5420) | Bit/dim 6.7363(7.8382) | Xent 2.0246(2.0965) | Loss 7.7486(8.8864) | Error 0.7076(0.7461) Steps 598(584.88) | Grad Norm 16.6328(21.9031) | Total Time 14.00(14.00)\n",
      "Iter 0062 | Time 52.8629(58.3717) | Bit/dim 6.7038(7.8041) | Xent 2.0268(2.0944) | Loss 7.7173(8.8513) | Error 0.7035(0.7448) Steps 598(585.27) | Grad Norm 15.5491(21.7124) | Total Time 14.00(14.00)\n",
      "Iter 0063 | Time 52.2283(58.1874) | Bit/dim 6.6895(7.7707) | Xent 2.0305(2.0925) | Loss 7.7048(8.8170) | Error 0.7206(0.7441) Steps 598(585.66) | Grad Norm 16.1742(21.5463) | Total Time 14.00(14.00)\n",
      "Iter 0064 | Time 52.8686(58.0278) | Bit/dim 6.6513(7.7371) | Xent 2.0379(2.0909) | Loss 7.6702(8.7825) | Error 0.7165(0.7433) Steps 598(586.03) | Grad Norm 16.0192(21.3805) | Total Time 14.00(14.00)\n",
      "Iter 0065 | Time 53.2095(57.8832) | Bit/dim 6.6290(7.7039) | Xent 2.0138(2.0886) | Loss 7.6358(8.7481) | Error 0.7101(0.7423) Steps 598(586.39) | Grad Norm 14.7375(21.1812) | Total Time 14.00(14.00)\n",
      "Iter 0066 | Time 52.3309(57.7167) | Bit/dim 6.6126(7.6711) | Xent 2.0581(2.0876) | Loss 7.6416(8.7150) | Error 0.7291(0.7419) Steps 598(586.73) | Grad Norm 20.3472(21.1562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 26.2942, Epoch Time 357.7287(378.9012), Bit/dim 6.6580(best: 6.7382), Xent 2.2612, Loss 7.7886, Error 0.7805(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 51.5049(57.5303) | Bit/dim 6.6509(7.6405) | Xent 2.2622(2.0929) | Loss 7.7820(8.6870) | Error 0.7795(0.7430) Steps 586(586.71) | Grad Norm 42.6774(21.8018) | Total Time 14.00(14.00)\n",
      "Iter 0068 | Time 52.0883(57.3671) | Bit/dim 6.7995(7.6153) | Xent 2.6426(2.1094) | Loss 8.1208(8.6700) | Error 0.8377(0.7459) Steps 586(586.69) | Grad Norm 74.1429(23.3720) | Total Time 14.00(14.00)\n",
      "Iter 0069 | Time 51.4040(57.1882) | Bit/dim 6.8411(7.5921) | Xent 2.4936(2.1209) | Loss 8.0879(8.6525) | Error 0.8374(0.7486) Steps 586(586.67) | Grad Norm 74.6039(24.9090) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 51.7433(57.0248) | Bit/dim 6.6993(7.5653) | Xent 3.5503(2.1638) | Loss 8.4745(8.6472) | Error 0.8811(0.7526) Steps 580(586.47) | Grad Norm 86.4407(26.7550) | Total Time 14.00(14.00)\n",
      "Iter 0071 | Time 52.2723(56.8823) | Bit/dim 6.5994(7.5363) | Xent 2.2439(2.1662) | Loss 7.7214(8.6194) | Error 0.7801(0.7534) Steps 586(586.46) | Grad Norm 27.9472(26.7907) | Total Time 14.00(14.00)\n",
      "Iter 0072 | Time 52.2778(56.7441) | Bit/dim 6.7943(7.5141) | Xent 2.5630(2.1781) | Loss 8.0758(8.6031) | Error 0.8267(0.7556) Steps 586(586.44) | Grad Norm 37.5569(27.1137) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 26.1084, Epoch Time 352.8927(378.1210), Bit/dim 6.5611(best: 6.6580), Xent 2.1255, Loss 7.6238, Error 0.7713(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 53.2611(56.6396) | Bit/dim 6.5550(7.4853) | Xent 2.1385(2.1769) | Loss 7.6242(8.5737) | Error 0.7736(0.7561) Steps 586(586.43) | Grad Norm 12.9294(26.6882) | Total Time 14.00(14.00)\n",
      "Iter 0074 | Time 51.0236(56.4711) | Bit/dim 6.5015(7.4558) | Xent 2.1182(2.1751) | Loss 7.5606(8.5433) | Error 0.7440(0.7558) Steps 574(586.06) | Grad Norm 8.4991(26.1425) | Total Time 14.00(14.00)\n",
      "Iter 0075 | Time 49.0779(56.2493) | Bit/dim 6.5814(7.4295) | Xent 2.1425(2.1742) | Loss 7.6527(8.5166) | Error 0.7592(0.7559) Steps 574(585.69) | Grad Norm 10.8228(25.6829) | Total Time 14.00(14.00)\n",
      "Iter 0076 | Time 49.6534(56.0515) | Bit/dim 6.5889(7.4043) | Xent 2.2031(2.1750) | Loss 7.6905(8.4918) | Error 0.8055(0.7574) Steps 574(585.34) | Grad Norm 13.3732(25.3136) | Total Time 14.00(14.00)\n",
      "Iter 0077 | Time 49.4688(55.8540) | Bit/dim 6.5064(7.3774) | Xent 2.2041(2.1759) | Loss 7.6084(8.4653) | Error 0.7908(0.7584) Steps 574(585.00) | Grad Norm 9.7807(24.8476) | Total Time 14.00(14.00)\n",
      "Iter 0078 | Time 50.4056(55.6905) | Bit/dim 6.4148(7.3485) | Xent 2.1670(2.1756) | Loss 7.4983(8.4363) | Error 0.7953(0.7595) Steps 574(584.67) | Grad Norm 8.8376(24.3673) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 25.7666, Epoch Time 344.1701(377.1025), Bit/dim 6.3780(best: 6.5611), Xent 2.1652, Loss 7.4606, Error 0.7664(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 49.8054(55.5140) | Bit/dim 6.3800(7.3194) | Xent 2.1845(2.1759) | Loss 7.4722(8.4074) | Error 0.7871(0.7603) Steps 574(584.35) | Grad Norm 7.0503(23.8478) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 49.9335(55.3466) | Bit/dim 6.3751(7.2911) | Xent 2.2132(2.1770) | Loss 7.4817(8.3796) | Error 0.7722(0.7607) Steps 568(583.86) | Grad Norm 9.3232(23.4121) | Total Time 14.00(14.00)\n",
      "Iter 0081 | Time 49.6768(55.1765) | Bit/dim 6.3545(7.2630) | Xent 2.2241(2.1784) | Loss 7.4666(8.3522) | Error 0.7750(0.7611) Steps 574(583.57) | Grad Norm 12.6543(23.0894) | Total Time 14.00(14.00)\n",
      "Iter 0082 | Time 49.5200(55.0068) | Bit/dim 6.2702(7.2332) | Xent 2.1681(2.1781) | Loss 7.3543(8.3223) | Error 0.7694(0.7613) Steps 574(583.28) | Grad Norm 5.2990(22.5556) | Total Time 14.00(14.00)\n",
      "Iter 0083 | Time 50.3372(54.8667) | Bit/dim 6.2262(7.2030) | Xent 2.1450(2.1771) | Loss 7.2987(8.2916) | Error 0.7825(0.7620) Steps 574(583.00) | Grad Norm 7.9928(22.1188) | Total Time 14.00(14.00)\n",
      "Iter 0084 | Time 49.0561(54.6924) | Bit/dim 6.1860(7.1725) | Xent 2.1777(2.1771) | Loss 7.2749(8.2611) | Error 0.8065(0.7633) Steps 574(582.73) | Grad Norm 8.2356(21.7023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 25.9191, Epoch Time 339.7596(375.9822), Bit/dim 6.1410(best: 6.3780), Xent 2.1258, Loss 7.2039, Error 0.7738(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 48.9382(54.5198) | Bit/dim 6.1431(7.1416) | Xent 2.1259(2.1756) | Loss 7.2061(8.2294) | Error 0.7691(0.7635) Steps 580(582.65) | Grad Norm 4.4447(21.1845) | Total Time 14.00(14.00)\n",
      "Iter 0086 | Time 48.7808(54.3476) | Bit/dim 6.1130(7.1108) | Xent 2.1419(2.1746) | Loss 7.1840(8.1981) | Error 0.7708(0.7637) Steps 574(582.39) | Grad Norm 9.3651(20.8300) | Total Time 14.00(14.00)\n",
      "Iter 0087 | Time 49.8138(54.2116) | Bit/dim 6.0771(7.0798) | Xent 2.1631(2.1742) | Loss 7.1586(8.1669) | Error 0.7984(0.7648) Steps 580(582.32) | Grad Norm 16.2844(20.6936) | Total Time 14.00(14.00)\n",
      "Iter 0088 | Time 49.9120(54.0826) | Bit/dim 5.9952(7.0472) | Xent 2.1254(2.1728) | Loss 7.0579(8.1336) | Error 0.7530(0.7644) Steps 574(582.07) | Grad Norm 11.7620(20.4256) | Total Time 14.00(14.00)\n",
      "Iter 0089 | Time 50.3353(53.9702) | Bit/dim 5.9661(7.0148) | Xent 2.1704(2.1727) | Loss 7.0513(8.1011) | Error 0.7877(0.7651) Steps 580(582.01) | Grad Norm 11.4658(20.1568) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 48.7324(53.8130) | Bit/dim 5.9485(6.9828) | Xent 2.1030(2.1706) | Loss 6.9999(8.0681) | Error 0.7421(0.7644) Steps 568(581.59) | Grad Norm 18.2555(20.0998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 25.1535, Epoch Time 337.2305(374.8196), Bit/dim 5.9545(best: 6.1410), Xent 2.1877, Loss 7.0484, Error 0.8085(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 50.2139(53.7051) | Bit/dim 5.9521(6.9519) | Xent 2.2162(2.1720) | Loss 7.0602(8.0379) | Error 0.8125(0.7659) Steps 574(581.36) | Grad Norm 34.8010(20.5408) | Total Time 14.00(14.00)\n",
      "Iter 0092 | Time 47.2403(53.5111) | Bit/dim 6.0340(6.9243) | Xent 2.9210(2.1945) | Loss 7.4945(8.0216) | Error 0.8558(0.7685) Steps 556(580.60) | Grad Norm 86.7659(22.5276) | Total Time 14.00(14.00)\n",
      "Iter 0093 | Time 52.1649(53.4707) | Bit/dim 6.6628(6.9165) | Xent 3.5835(2.2361) | Loss 8.4546(8.0346) | Error 0.8830(0.7720) Steps 580(580.58) | Grad Norm 81.7707(24.3049) | Total Time 14.00(14.00)\n",
      "Iter 0094 | Time 52.2250(53.4334) | Bit/dim 6.1054(6.8922) | Xent 2.5107(2.2444) | Loss 7.3608(8.0143) | Error 0.8044(0.7730) Steps 586(580.74) | Grad Norm 22.8816(24.2622) | Total Time 14.00(14.00)\n",
      "Iter 0095 | Time 51.8234(53.3851) | Bit/dim 6.0068(6.8656) | Xent 2.2096(2.2433) | Loss 7.1116(7.9873) | Error 0.8077(0.7740) Steps 580(580.72) | Grad Norm 22.9906(24.2240) | Total Time 14.00(14.00)\n",
      "Iter 0096 | Time 51.6517(53.3331) | Bit/dim 6.0236(6.8403) | Xent 2.3125(2.2454) | Loss 7.1798(7.9630) | Error 0.8020(0.7748) Steps 568(580.34) | Grad Norm 10.4743(23.8115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 25.4904, Epoch Time 346.5478(373.9715), Bit/dim 6.0259(best: 5.9545), Xent 2.3194, Loss 7.1856, Error 0.8204(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 47.1089(53.1463) | Bit/dim 6.0270(6.8159) | Xent 2.3372(2.2481) | Loss 7.1956(7.9400) | Error 0.8241(0.7763) Steps 556(579.61) | Grad Norm 15.8568(23.5729) | Total Time 14.00(14.00)\n",
      "Iter 0098 | Time 46.1017(52.9350) | Bit/dim 5.9719(6.7906) | Xent 2.3356(2.2508) | Loss 7.1397(7.9160) | Error 0.8285(0.7779) Steps 550(578.72) | Grad Norm 37.8361(24.0008) | Total Time 14.00(14.00)\n",
      "Iter 0099 | Time 45.8259(52.7217) | Bit/dim 6.0979(6.7698) | Xent 2.8727(2.2694) | Loss 7.5342(7.9046) | Error 0.8416(0.7798) Steps 532(577.32) | Grad Norm 66.4795(25.2752) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 50.0729(52.6423) | Bit/dim 5.9586(6.7455) | Xent 2.9725(2.2905) | Loss 7.4449(7.8908) | Error 0.8822(0.7829) Steps 562(576.86) | Grad Norm 70.7912(26.6406) | Total Time 14.00(14.00)\n",
      "Iter 0101 | Time 48.2596(52.5108) | Bit/dim 5.8888(6.7198) | Xent 2.2717(2.2900) | Loss 7.0247(7.8648) | Error 0.7805(0.7828) Steps 574(576.77) | Grad Norm 14.6257(26.2802) | Total Time 14.00(14.00)\n",
      "Iter 0102 | Time 50.8414(52.4607) | Bit/dim 5.9268(6.6960) | Xent 2.3218(2.2909) | Loss 7.0877(7.8415) | Error 0.7851(0.7829) Steps 586(577.05) | Grad Norm 17.7244(26.0235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 25.3520, Epoch Time 329.5234(372.6380), Bit/dim 5.9014(best: 5.9545), Xent 2.1456, Loss 6.9742, Error 0.7721(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 50.2576(52.3946) | Bit/dim 5.8986(6.6721) | Xent 2.1707(2.2873) | Loss 6.9839(7.8157) | Error 0.7770(0.7827) Steps 586(577.32) | Grad Norm 9.5292(25.5287) | Total Time 14.00(14.00)\n",
      "Iter 0104 | Time 49.8202(52.3174) | Bit/dim 5.8936(6.6487) | Xent 2.1975(2.2846) | Loss 6.9923(7.7910) | Error 0.8136(0.7836) Steps 586(577.58) | Grad Norm 11.7579(25.1156) | Total Time 14.00(14.00)\n",
      "Iter 0105 | Time 51.4247(52.2906) | Bit/dim 5.8798(6.6257) | Xent 2.1537(2.2807) | Loss 6.9567(7.7660) | Error 0.7765(0.7834) Steps 586(577.83) | Grad Norm 7.1542(24.5767) | Total Time 14.00(14.00)\n",
      "Iter 0106 | Time 50.1137(52.2253) | Bit/dim 5.8285(6.6018) | Xent 2.2673(2.2803) | Loss 6.9622(7.7419) | Error 0.7956(0.7838) Steps 586(578.08) | Grad Norm 14.7398(24.2816) | Total Time 14.00(14.00)\n",
      "Iter 0107 | Time 50.8028(52.1826) | Bit/dim 5.7663(6.5767) | Xent 2.1870(2.2775) | Loss 6.8598(7.7154) | Error 0.7755(0.7835) Steps 586(578.31) | Grad Norm 9.5045(23.8383) | Total Time 14.00(14.00)\n",
      "Iter 0108 | Time 50.1265(52.1209) | Bit/dim 5.7486(6.5518) | Xent 2.2496(2.2766) | Loss 6.8734(7.6902) | Error 0.7923(0.7838) Steps 592(578.72) | Grad Norm 12.8690(23.5092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 25.2917, Epoch Time 343.6029(371.7670), Bit/dim 5.7282(best: 5.9014), Xent 2.2087, Loss 6.8326, Error 0.7759(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 50.1294(52.0612) | Bit/dim 5.7344(6.5273) | Xent 2.2255(2.2751) | Loss 6.8471(7.6649) | Error 0.7810(0.7837) Steps 586(578.94) | Grad Norm 11.1551(23.1386) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 48.2974(51.9483) | Bit/dim 5.7281(6.5033) | Xent 2.2145(2.2733) | Loss 6.8354(7.6400) | Error 0.7864(0.7838) Steps 562(578.43) | Grad Norm 16.8608(22.9503) | Total Time 14.00(14.00)\n",
      "Iter 0111 | Time 48.9871(51.8594) | Bit/dim 5.7160(6.4797) | Xent 2.1166(2.2686) | Loss 6.7743(7.6140) | Error 0.7468(0.7827) Steps 568(578.12) | Grad Norm 4.6038(22.3999) | Total Time 14.00(14.00)\n",
      "Iter 0112 | Time 48.9690(51.7727) | Bit/dim 5.6862(6.4559) | Xent 2.2387(2.2677) | Loss 6.8055(7.5898) | Error 0.8020(0.7833) Steps 562(577.64) | Grad Norm 16.1254(22.2116) | Total Time 14.00(14.00)\n",
      "Iter 0113 | Time 47.9820(51.6590) | Bit/dim 5.7008(6.4333) | Xent 2.1509(2.2642) | Loss 6.7762(7.5654) | Error 0.7804(0.7832) Steps 562(577.17) | Grad Norm 8.7311(21.8072) | Total Time 14.00(14.00)\n",
      "Iter 0114 | Time 48.1836(51.5547) | Bit/dim 5.6659(6.4102) | Xent 2.1049(2.2594) | Loss 6.7183(7.5400) | Error 0.7514(0.7822) Steps 568(576.89) | Grad Norm 6.8151(21.3575) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 25.3729, Epoch Time 333.4123(370.6163), Bit/dim 5.6719(best: 5.7282), Xent 2.1232, Loss 6.7334, Error 0.7761(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 48.0421(51.4494) | Bit/dim 5.6554(6.3876) | Xent 2.1343(2.2557) | Loss 6.7226(7.5154) | Error 0.7808(0.7822) Steps 568(576.63) | Grad Norm 15.3788(21.1781) | Total Time 14.00(14.00)\n",
      "Iter 0116 | Time 46.0424(51.2871) | Bit/dim 5.6541(6.3656) | Xent 2.1887(2.2537) | Loss 6.7484(7.4924) | Error 0.7926(0.7825) Steps 550(575.83) | Grad Norm 17.5893(21.0704) | Total Time 14.00(14.00)\n",
      "Iter 0117 | Time 46.0153(51.1290) | Bit/dim 5.6346(6.3437) | Xent 2.1219(2.2497) | Loss 6.6956(7.4685) | Error 0.7705(0.7821) Steps 562(575.41) | Grad Norm 11.5095(20.7836) | Total Time 14.00(14.00)\n",
      "Iter 0118 | Time 47.8355(51.0302) | Bit/dim 5.6120(6.3217) | Xent 2.0722(2.2444) | Loss 6.6481(7.4439) | Error 0.7279(0.7805) Steps 556(574.83) | Grad Norm 2.9436(20.2484) | Total Time 14.00(14.00)\n",
      "Iter 0119 | Time 47.6362(50.9284) | Bit/dim 5.5920(6.2998) | Xent 2.0941(2.2399) | Loss 6.6390(7.4198) | Error 0.7352(0.7791) Steps 550(574.09) | Grad Norm 9.1840(19.9165) | Total Time 14.00(14.00)\n",
      "Iter 0120 | Time 48.0935(50.8433) | Bit/dim 5.5942(6.2787) | Xent 2.1066(2.2359) | Loss 6.6475(7.3966) | Error 0.7702(0.7789) Steps 562(573.72) | Grad Norm 13.7332(19.7310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 23.6654, Epoch Time 322.7075(369.1791), Bit/dim 5.5804(best: 5.6719), Xent 2.0588, Loss 6.6098, Error 0.7280(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 45.5294(50.6839) | Bit/dim 5.5693(6.2574) | Xent 2.0738(2.2310) | Loss 6.6062(7.3729) | Error 0.7339(0.7775) Steps 538(572.65) | Grad Norm 10.7496(19.4615) | Total Time 14.00(14.00)\n",
      "Iter 0122 | Time 46.0829(50.5459) | Bit/dim 5.5699(6.2368) | Xent 2.0503(2.2256) | Loss 6.5950(7.3495) | Error 0.7218(0.7758) Steps 550(571.97) | Grad Norm 3.3977(18.9796) | Total Time 14.00(14.00)\n",
      "Iter 0123 | Time 46.7806(50.4329) | Bit/dim 5.5480(6.2161) | Xent 2.0425(2.2201) | Loss 6.5692(7.3261) | Error 0.7290(0.7744) Steps 556(571.49) | Grad Norm 7.2581(18.6280) | Total Time 14.00(14.00)\n",
      "Iter 0124 | Time 46.7360(50.3220) | Bit/dim 5.5285(6.1955) | Xent 2.1036(2.2166) | Loss 6.5803(7.3038) | Error 0.7403(0.7734) Steps 538(570.49) | Grad Norm 15.5433(18.5354) | Total Time 14.00(14.00)\n",
      "Iter 0125 | Time 46.3382(50.2025) | Bit/dim 5.5427(6.1759) | Xent 2.1549(2.2147) | Loss 6.6201(7.2833) | Error 0.7808(0.7736) Steps 544(569.69) | Grad Norm 21.1467(18.6138) | Total Time 14.00(14.00)\n",
      "Iter 0126 | Time 46.1055(50.0796) | Bit/dim 5.5088(6.1559) | Xent 2.1704(2.2134) | Loss 6.5940(7.2626) | Error 0.7666(0.7734) Steps 538(568.74) | Grad Norm 21.9695(18.7144) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 24.3436, Epoch Time 317.5259(367.6295), Bit/dim 5.4961(best: 5.5804), Xent 2.0639, Loss 6.5281, Error 0.7393(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 46.1091(49.9605) | Bit/dim 5.5022(6.1363) | Xent 2.0891(2.2097) | Loss 6.5467(7.2411) | Error 0.7620(0.7731) Steps 538(567.82) | Grad Norm 14.6790(18.5934) | Total Time 14.00(14.00)\n",
      "Iter 0128 | Time 46.1933(49.8474) | Bit/dim 5.4796(6.1166) | Xent 2.0370(2.2045) | Loss 6.4981(7.2188) | Error 0.7076(0.7711) Steps 544(567.11) | Grad Norm 2.3666(18.1066) | Total Time 14.00(14.00)\n",
      "Iter 0129 | Time 46.8623(49.7579) | Bit/dim 5.4687(6.0971) | Xent 2.0731(2.2006) | Loss 6.5053(7.1974) | Error 0.7303(0.7699) Steps 532(566.05) | Grad Norm 11.5321(17.9093) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 46.2616(49.6530) | Bit/dim 5.4525(6.0778) | Xent 2.0588(2.1963) | Loss 6.4819(7.1759) | Error 0.7385(0.7690) Steps 544(565.39) | Grad Norm 11.4776(17.7164) | Total Time 14.00(14.00)\n",
      "Iter 0131 | Time 47.0999(49.5764) | Bit/dim 5.4353(6.0585) | Xent 2.0025(2.1905) | Loss 6.4365(7.1538) | Error 0.6941(0.7667) Steps 544(564.75) | Grad Norm 2.2973(17.2538) | Total Time 14.00(14.00)\n",
      "Iter 0132 | Time 45.2874(49.4477) | Bit/dim 5.4230(6.0394) | Xent 2.0348(2.1858) | Loss 6.4404(7.1324) | Error 0.7084(0.7650) Steps 538(563.95) | Grad Norm 9.3752(17.0175) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 24.3923, Epoch Time 317.7814(366.1340), Bit/dim 5.4214(best: 5.4961), Xent 2.0132, Loss 6.4280, Error 0.7014(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 45.2382(49.3215) | Bit/dim 5.4225(6.0209) | Xent 2.0237(2.1810) | Loss 6.4344(7.1114) | Error 0.7174(0.7635) Steps 532(562.99) | Grad Norm 9.7390(16.7991) | Total Time 14.00(14.00)\n",
      "Iter 0134 | Time 46.0307(49.2227) | Bit/dim 5.4007(6.0023) | Xent 1.9971(2.1754) | Loss 6.3993(7.0900) | Error 0.6987(0.7616) Steps 538(562.24) | Grad Norm 2.0045(16.3553) | Total Time 14.00(14.00)\n",
      "Iter 0135 | Time 45.9991(49.1260) | Bit/dim 5.3840(5.9838) | Xent 2.0258(2.1710) | Loss 6.3969(7.0693) | Error 0.7167(0.7602) Steps 538(561.51) | Grad Norm 9.2916(16.1434) | Total Time 14.00(14.00)\n",
      "Iter 0136 | Time 45.5682(49.0193) | Bit/dim 5.3791(5.9656) | Xent 2.0185(2.1664) | Loss 6.3883(7.0488) | Error 0.7106(0.7588) Steps 538(560.81) | Grad Norm 10.3365(15.9692) | Total Time 14.00(14.00)\n",
      "Iter 0137 | Time 45.4134(48.9111) | Bit/dim 5.3648(5.9476) | Xent 1.9921(2.1612) | Loss 6.3609(7.0282) | Error 0.6843(0.7565) Steps 538(560.12) | Grad Norm 3.1761(15.5854) | Total Time 14.00(14.00)\n",
      "Iter 0138 | Time 45.9201(48.8214) | Bit/dim 5.3426(5.9295) | Xent 1.9988(2.1563) | Loss 6.3420(7.0076) | Error 0.6861(0.7544) Steps 538(559.46) | Grad Norm 4.9398(15.2660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 24.0126, Epoch Time 313.9620(364.5689), Bit/dim 5.3368(best: 5.4214), Xent 1.9753, Loss 6.3244, Error 0.6663(best: 0.6768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 45.9713(48.7359) | Bit/dim 5.3392(5.9118) | Xent 1.9811(2.1510) | Loss 6.3298(6.9873) | Error 0.6841(0.7523) Steps 538(558.81) | Grad Norm 6.8173(15.0125) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 45.3641(48.6347) | Bit/dim 5.2975(5.8933) | Xent 1.9773(2.1458) | Loss 6.2862(6.9662) | Error 0.6786(0.7501) Steps 538(558.19) | Grad Norm 2.0224(14.6228) | Total Time 14.00(14.00)\n",
      "Iter 0141 | Time 46.8761(48.5820) | Bit/dim 5.3268(5.8763) | Xent 2.0023(2.1415) | Loss 6.3279(6.9471) | Error 0.6970(0.7485) Steps 538(557.58) | Grad Norm 5.7990(14.3581) | Total Time 14.00(14.00)\n",
      "Iter 0142 | Time 46.0707(48.5066) | Bit/dim 5.2954(5.8589) | Xent 2.0147(2.1377) | Loss 6.3027(6.9278) | Error 0.7067(0.7472) Steps 538(557.00) | Grad Norm 9.8965(14.2243) | Total Time 14.00(14.00)\n",
      "Iter 0143 | Time 48.3644(48.5024) | Bit/dim 5.3071(5.8423) | Xent 1.9856(2.1331) | Loss 6.2999(6.9089) | Error 0.6844(0.7454) Steps 544(556.61) | Grad Norm 5.6391(13.9667) | Total Time 14.00(14.00)\n",
      "Iter 0144 | Time 47.8362(48.4824) | Bit/dim 5.2705(5.8252) | Xent 1.9763(2.1284) | Loss 6.2586(6.8894) | Error 0.6825(0.7435) Steps 544(556.23) | Grad Norm 2.3053(13.6169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 24.0097, Epoch Time 320.2076(363.2380), Bit/dim 5.2675(best: 5.3368), Xent 1.9545, Loss 6.2447, Error 0.6559(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 47.5082(48.4532) | Bit/dim 5.2748(5.8087) | Xent 1.9716(2.1237) | Loss 6.2606(6.8705) | Error 0.6716(0.7413) Steps 544(555.86) | Grad Norm 4.0150(13.3288) | Total Time 14.00(14.00)\n",
      "Iter 0146 | Time 47.6887(48.4302) | Bit/dim 5.2410(5.7916) | Xent 1.9674(2.1190) | Loss 6.2247(6.8512) | Error 0.6873(0.7397) Steps 544(555.51) | Grad Norm 4.4081(13.0612) | Total Time 14.00(14.00)\n",
      "Iter 0147 | Time 47.4808(48.4017) | Bit/dim 5.2706(5.7760) | Xent 1.9813(2.1149) | Loss 6.2613(6.8335) | Error 0.6838(0.7380) Steps 544(555.16) | Grad Norm 2.5294(12.7452) | Total Time 14.00(14.00)\n",
      "Iter 0148 | Time 47.5145(48.3751) | Bit/dim 5.2508(5.7603) | Xent 1.9790(2.1108) | Loss 6.2403(6.8157) | Error 0.6867(0.7365) Steps 544(554.83) | Grad Norm 5.7405(12.5351) | Total Time 14.00(14.00)\n",
      "Iter 0149 | Time 48.9392(48.3920) | Bit/dim 5.2358(5.7445) | Xent 2.0202(2.1081) | Loss 6.2459(6.7986) | Error 0.7164(0.7359) Steps 538(554.32) | Grad Norm 14.1663(12.5840) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 48.4177(48.3928) | Bit/dim 5.2886(5.7309) | Xent 2.0260(2.1057) | Loss 6.3016(6.7837) | Error 0.7269(0.7356) Steps 544(554.01) | Grad Norm 18.3015(12.7556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 24.2394, Epoch Time 327.3601(362.1617), Bit/dim 5.2159(best: 5.2675), Xent 1.9634, Loss 6.1976, Error 0.6795(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 47.8834(48.3775) | Bit/dim 5.2287(5.7158) | Xent 1.9808(2.1019) | Loss 6.2191(6.7667) | Error 0.6894(0.7342) Steps 544(553.71) | Grad Norm 7.5220(12.5986) | Total Time 14.00(14.00)\n",
      "Iter 0152 | Time 49.2176(48.4027) | Bit/dim 5.2185(5.7009) | Xent 1.9954(2.0987) | Loss 6.2163(6.7502) | Error 0.7096(0.7335) Steps 544(553.42) | Grad Norm 10.1932(12.5264) | Total Time 14.00(14.00)\n",
      "Iter 0153 | Time 47.7137(48.3821) | Bit/dim 5.2528(5.6874) | Xent 2.0122(2.0961) | Loss 6.2589(6.7355) | Error 0.7119(0.7328) Steps 544(553.14) | Grad Norm 14.1595(12.5754) | Total Time 14.00(14.00)\n",
      "Iter 0154 | Time 47.7090(48.3619) | Bit/dim 5.1918(5.6726) | Xent 1.9541(2.0919) | Loss 6.1688(6.7185) | Error 0.6744(0.7311) Steps 538(552.68) | Grad Norm 2.3872(12.2697) | Total Time 14.00(14.00)\n",
      "Iter 0155 | Time 47.6108(48.3393) | Bit/dim 5.2409(5.6596) | Xent 2.0853(2.0917) | Loss 6.2836(6.7054) | Error 0.7414(0.7314) Steps 544(552.42) | Grad Norm 20.1353(12.5057) | Total Time 14.00(14.00)\n",
      "Iter 0156 | Time 49.3259(48.3689) | Bit/dim 5.2907(5.6485) | Xent 2.0559(2.0906) | Loss 6.3186(6.6938) | Error 0.7275(0.7313) Steps 544(552.17) | Grad Norm 17.4153(12.6530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 23.3964, Epoch Time 328.1599(361.1416), Bit/dim 5.2489(best: 5.2159), Xent 1.9821, Loss 6.2399, Error 0.6864(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 48.1979(48.3638) | Bit/dim 5.2484(5.6365) | Xent 1.9983(2.0878) | Loss 6.2476(6.6804) | Error 0.7097(0.7306) Steps 544(551.93) | Grad Norm 11.5270(12.6192) | Total Time 14.00(14.00)\n",
      "Iter 0158 | Time 49.2214(48.3895) | Bit/dim 5.1960(5.6233) | Xent 2.0822(2.0877) | Loss 6.2371(6.6671) | Error 0.7459(0.7311) Steps 538(551.51) | Grad Norm 16.6158(12.7391) | Total Time 14.00(14.00)\n",
      "Iter 0159 | Time 47.9600(48.3766) | Bit/dim 5.1903(5.6103) | Xent 2.0034(2.0851) | Loss 6.1920(6.6529) | Error 0.7009(0.7302) Steps 538(551.10) | Grad Norm 8.9451(12.6253) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 49.2460(48.4027) | Bit/dim 5.2001(5.5980) | Xent 1.9831(2.0821) | Loss 6.1916(6.6391) | Error 0.6943(0.7291) Steps 544(550.89) | Grad Norm 9.2701(12.5246) | Total Time 14.00(14.00)\n",
      "Iter 0161 | Time 49.4976(48.4356) | Bit/dim 5.2187(5.5866) | Xent 2.0206(2.0802) | Loss 6.2291(6.6268) | Error 0.7311(0.7292) Steps 538(550.50) | Grad Norm 9.5014(12.4339) | Total Time 14.00(14.00)\n",
      "Iter 0162 | Time 47.4639(48.4064) | Bit/dim 5.1696(5.5741) | Xent 1.9657(2.0768) | Loss 6.1524(6.6125) | Error 0.6935(0.7281) Steps 538(550.13) | Grad Norm 5.9589(12.2397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 24.0145, Epoch Time 330.9982(360.2373), Bit/dim 5.1898(best: 5.2159), Xent 1.9705, Loss 6.1750, Error 0.6824(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 47.5649(48.3812) | Bit/dim 5.1864(5.5625) | Xent 1.9858(2.0741) | Loss 6.1793(6.5995) | Error 0.6974(0.7272) Steps 532(549.58) | Grad Norm 11.4800(12.2169) | Total Time 14.00(14.00)\n",
      "Iter 0164 | Time 48.4638(48.3837) | Bit/dim 5.1708(5.5507) | Xent 1.9447(2.0702) | Loss 6.1431(6.5858) | Error 0.6761(0.7256) Steps 538(549.24) | Grad Norm 3.3451(11.9507) | Total Time 14.00(14.00)\n",
      "Iter 0165 | Time 46.7446(48.3345) | Bit/dim 5.1756(5.5395) | Xent 1.9660(2.0671) | Loss 6.1586(6.5730) | Error 0.6907(0.7246) Steps 544(549.08) | Grad Norm 7.5076(11.8174) | Total Time 14.00(14.00)\n",
      "Iter 0166 | Time 47.6771(48.3148) | Bit/dim 5.1490(5.5278) | Xent 1.9619(2.0639) | Loss 6.1299(6.5597) | Error 0.6793(0.7232) Steps 538(548.75) | Grad Norm 1.7908(11.5166) | Total Time 14.00(14.00)\n",
      "Iter 0167 | Time 46.6561(48.2650) | Bit/dim 5.1509(5.5165) | Xent 1.9911(2.0617) | Loss 6.1465(6.5473) | Error 0.7023(0.7226) Steps 532(548.24) | Grad Norm 8.6874(11.4318) | Total Time 14.00(14.00)\n",
      "Iter 0168 | Time 48.5930(48.2748) | Bit/dim 5.1289(5.5048) | Xent 1.9666(2.0589) | Loss 6.1122(6.5343) | Error 0.6854(0.7215) Steps 544(548.12) | Grad Norm 5.6065(11.2570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 24.4701, Epoch Time 325.6365(359.1993), Bit/dim 5.1276(best: 5.1898), Xent 1.9374, Loss 6.0963, Error 0.6632(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 47.8192(48.2612) | Bit/dim 5.1251(5.4935) | Xent 1.9649(2.0560) | Loss 6.1076(6.5215) | Error 0.6795(0.7202) Steps 544(547.99) | Grad Norm 4.8919(11.0661) | Total Time 14.00(14.00)\n",
      "Iter 0170 | Time 47.4697(48.2374) | Bit/dim 5.1297(5.4825) | Xent 1.9802(2.0538) | Loss 6.1198(6.5094) | Error 0.6950(0.7195) Steps 538(547.69) | Grad Norm 7.4033(10.9562) | Total Time 14.00(14.00)\n",
      "Iter 0171 | Time 48.6983(48.2513) | Bit/dim 5.1254(5.4718) | Xent 1.9370(2.0503) | Loss 6.0939(6.4970) | Error 0.6661(0.7179) Steps 538(547.40) | Grad Norm 2.4393(10.7007) | Total Time 14.00(14.00)\n",
      "Iter 0172 | Time 48.0392(48.2449) | Bit/dim 5.1114(5.4610) | Xent 1.9284(2.0466) | Loss 6.0755(6.4843) | Error 0.6665(0.7163) Steps 544(547.30) | Grad Norm 4.5912(10.5174) | Total Time 14.00(14.00)\n",
      "Iter 0173 | Time 47.2121(48.2139) | Bit/dim 5.0940(5.4500) | Xent 1.9452(2.0436) | Loss 6.0666(6.4718) | Error 0.6736(0.7150) Steps 538(547.02) | Grad Norm 2.3402(10.2721) | Total Time 14.00(14.00)\n",
      "Iter 0174 | Time 46.9857(48.1771) | Bit/dim 5.0857(5.4391) | Xent 1.9316(2.0402) | Loss 6.0514(6.4592) | Error 0.6700(0.7137) Steps 538(546.75) | Grad Norm 3.4310(10.0668) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 23.9788, Epoch Time 325.8547(358.1990), Bit/dim 5.0890(best: 5.1276), Xent 1.9129, Loss 6.0455, Error 0.6559(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 47.1767(48.1470) | Bit/dim 5.0842(5.4284) | Xent 1.9282(2.0368) | Loss 6.0483(6.4468) | Error 0.6714(0.7124) Steps 538(546.49) | Grad Norm 4.2948(9.8937) | Total Time 14.00(14.00)\n",
      "Iter 0176 | Time 48.3328(48.1526) | Bit/dim 5.0938(5.4184) | Xent 1.9026(2.0328) | Loss 6.0451(6.4348) | Error 0.6579(0.7108) Steps 544(546.41) | Grad Norm 3.0241(9.6876) | Total Time 14.00(14.00)\n",
      "Iter 0177 | Time 47.9853(48.1476) | Bit/dim 5.0951(5.4087) | Xent 1.9240(2.0295) | Loss 6.0571(6.4235) | Error 0.6613(0.7093) Steps 544(546.34) | Grad Norm 4.6263(9.5358) | Total Time 14.00(14.00)\n",
      "Iter 0178 | Time 48.4434(48.1565) | Bit/dim 5.0712(5.3986) | Xent 1.9289(2.0265) | Loss 6.0356(6.4118) | Error 0.6640(0.7079) Steps 544(546.27) | Grad Norm 5.9695(9.4288) | Total Time 14.00(14.00)\n",
      "Iter 0179 | Time 48.0324(48.1528) | Bit/dim 5.0729(5.3888) | Xent 1.9319(2.0237) | Loss 6.0388(6.4006) | Error 0.6771(0.7070) Steps 544(546.20) | Grad Norm 11.4054(9.4881) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 48.0391(48.1493) | Bit/dim 5.0717(5.3793) | Xent 1.9836(2.0225) | Loss 6.0635(6.3905) | Error 0.7076(0.7070) Steps 538(545.96) | Grad Norm 18.5857(9.7610) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 24.8410, Epoch Time 328.6896(357.3137), Bit/dim 5.1173(best: 5.0890), Xent 2.1157, Loss 6.1751, Error 0.7419(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 48.7816(48.1683) | Bit/dim 5.1205(5.3715) | Xent 2.1280(2.0257) | Loss 6.1845(6.3843) | Error 0.7363(0.7079) Steps 562(546.44) | Grad Norm 33.9493(10.4866) | Total Time 14.00(14.00)\n",
      "Iter 0182 | Time 45.5901(48.0910) | Bit/dim 5.2814(5.3688) | Xent 2.3298(2.0348) | Loss 6.4463(6.3862) | Error 0.8027(0.7108) Steps 544(546.37) | Grad Norm 33.2177(11.1686) | Total Time 14.00(14.00)\n",
      "Iter 0183 | Time 46.9112(48.0556) | Bit/dim 5.1068(5.3610) | Xent 1.9417(2.0320) | Loss 6.0776(6.3769) | Error 0.6716(0.7096) Steps 550(546.47) | Grad Norm 7.5981(11.0615) | Total Time 14.00(14.00)\n",
      "Iter 0184 | Time 49.8556(48.1096) | Bit/dim 5.1793(5.3555) | Xent 2.1066(2.0342) | Loss 6.2326(6.3726) | Error 0.7570(0.7110) Steps 550(546.58) | Grad Norm 22.0817(11.3921) | Total Time 14.00(14.00)\n",
      "Iter 0185 | Time 46.3605(48.0571) | Bit/dim 5.2056(5.3510) | Xent 2.0092(2.0335) | Loss 6.2102(6.3677) | Error 0.7295(0.7116) Steps 544(546.50) | Grad Norm 15.1005(11.5033) | Total Time 14.00(14.00)\n",
      "Iter 0186 | Time 47.6838(48.0459) | Bit/dim 5.1237(5.3442) | Xent 1.9633(2.0314) | Loss 6.1053(6.3599) | Error 0.6890(0.7109) Steps 550(546.61) | Grad Norm 5.8156(11.3327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 24.6408, Epoch Time 325.3597(356.3551), Bit/dim 5.1425(best: 5.0890), Xent 2.0959, Loss 6.1905, Error 0.7519(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 48.2792(48.0529) | Bit/dim 5.1538(5.3385) | Xent 2.1085(2.0337) | Loss 6.2080(6.3553) | Error 0.7436(0.7119) Steps 556(546.89) | Grad Norm 17.5060(11.5179) | Total Time 14.00(14.00)\n",
      "Iter 0188 | Time 47.1480(48.0258) | Bit/dim 5.0750(5.3306) | Xent 1.9923(2.0324) | Loss 6.0712(6.3468) | Error 0.7085(0.7118) Steps 550(546.98) | Grad Norm 4.7254(11.3141) | Total Time 14.00(14.00)\n",
      "Iter 0189 | Time 46.5670(47.9820) | Bit/dim 5.1027(5.3237) | Xent 2.0231(2.0322) | Loss 6.1142(6.3398) | Error 0.7264(0.7122) Steps 550(547.07) | Grad Norm 7.6064(11.2029) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 47.9433(47.9808) | Bit/dim 5.0891(5.3167) | Xent 2.0204(2.0318) | Loss 6.0993(6.3326) | Error 0.7119(0.7122) Steps 550(547.16) | Grad Norm 4.2624(10.9947) | Total Time 14.00(14.00)\n",
      "Iter 0191 | Time 47.9637(47.9803) | Bit/dim 5.0692(5.3093) | Xent 2.0164(2.0313) | Loss 6.0774(6.3249) | Error 0.7111(0.7122) Steps 550(547.25) | Grad Norm 4.4911(10.7996) | Total Time 14.00(14.00)\n",
      "Iter 0192 | Time 47.7021(47.9720) | Bit/dim 5.0680(5.3020) | Xent 1.9635(2.0293) | Loss 6.0498(6.3167) | Error 0.6821(0.7113) Steps 556(547.51) | Grad Norm 2.5428(10.5519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 25.1953, Epoch Time 326.5408(355.4606), Bit/dim 5.0676(best: 5.0890), Xent 1.9920, Loss 6.0636, Error 0.7146(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 49.2065(48.0090) | Bit/dim 5.0768(5.2953) | Xent 2.0015(2.0285) | Loss 6.0776(6.3095) | Error 0.7146(0.7114) Steps 562(547.94) | Grad Norm 4.2169(10.3618) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 49.1499(48.0432) | Bit/dim 5.0506(5.2879) | Xent 1.9654(2.0266) | Loss 6.0333(6.3012) | Error 0.6905(0.7107) Steps 556(548.19) | Grad Norm 2.4848(10.1255) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 48.5959(48.0598) | Bit/dim 5.0230(5.2800) | Xent 1.9925(2.0256) | Loss 6.0192(6.2928) | Error 0.7020(0.7105) Steps 550(548.24) | Grad Norm 2.7920(9.9055) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 48.1995(48.0640) | Bit/dim 5.0271(5.2724) | Xent 1.9853(2.0244) | Loss 6.0198(6.2846) | Error 0.7044(0.7103) Steps 550(548.29) | Grad Norm 3.3144(9.7078) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 49.5902(48.1098) | Bit/dim 5.0448(5.2656) | Xent 1.9552(2.0223) | Loss 6.0224(6.2767) | Error 0.6775(0.7093) Steps 556(548.52) | Grad Norm 4.4562(9.5502) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 47.9221(48.1042) | Bit/dim 5.0389(5.2588) | Xent 1.9635(2.0205) | Loss 6.0206(6.2690) | Error 0.6929(0.7088) Steps 550(548.57) | Grad Norm 3.0164(9.3542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 25.3572, Epoch Time 333.7560(354.8095), Bit/dim 5.0120(best: 5.0676), Xent 1.9386, Loss 5.9813, Error 0.6733(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.4908(48.0858) | Bit/dim 5.0093(5.2513) | Xent 1.9551(2.0186) | Loss 5.9868(6.2606) | Error 0.6931(0.7083) Steps 550(548.61) | Grad Norm 3.0706(9.1657) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 47.9284(48.0810) | Bit/dim 5.0147(5.2442) | Xent 1.9550(2.0166) | Loss 5.9922(6.2525) | Error 0.6798(0.7075) Steps 556(548.83) | Grad Norm 3.2804(8.9891) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 48.0041(48.0787) | Bit/dim 4.9961(5.2367) | Xent 1.9575(2.0149) | Loss 5.9749(6.2442) | Error 0.6836(0.7068) Steps 544(548.69) | Grad Norm 2.2681(8.7875) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 47.1494(48.0509) | Bit/dim 4.9936(5.2295) | Xent 1.9413(2.0127) | Loss 5.9643(6.2358) | Error 0.6793(0.7059) Steps 544(548.55) | Grad Norm 2.3045(8.5930) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 48.1752(48.0546) | Bit/dim 4.9904(5.2223) | Xent 1.9381(2.0104) | Loss 5.9594(6.2275) | Error 0.6804(0.7052) Steps 544(548.41) | Grad Norm 3.2816(8.4337) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 47.8775(48.0493) | Bit/dim 4.9930(5.2154) | Xent 1.9376(2.0082) | Loss 5.9617(6.2195) | Error 0.6829(0.7045) Steps 532(547.92) | Grad Norm 6.2920(8.3694) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 24.3030, Epoch Time 326.7011(353.9663), Bit/dim 5.0051(best: 5.0120), Xent 1.9206, Loss 5.9654, Error 0.6727(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 48.4939(48.0626) | Bit/dim 5.0071(5.2092) | Xent 1.9373(2.0061) | Loss 5.9758(6.2122) | Error 0.6854(0.7039) Steps 544(547.80) | Grad Norm 7.4280(8.3412) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 48.5459(48.0771) | Bit/dim 4.9728(5.2021) | Xent 1.9734(2.0051) | Loss 5.9595(6.2046) | Error 0.6983(0.7038) Steps 532(547.33) | Grad Norm 9.1140(8.3644) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 47.7446(48.0671) | Bit/dim 4.9941(5.1958) | Xent 1.9917(2.0047) | Loss 5.9900(6.1982) | Error 0.7070(0.7039) Steps 538(547.05) | Grad Norm 9.4343(8.3965) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 47.5506(48.0516) | Bit/dim 4.9477(5.1884) | Xent 1.9623(2.0035) | Loss 5.9288(6.1901) | Error 0.6886(0.7034) Steps 532(546.60) | Grad Norm 4.8707(8.2907) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 47.5359(48.0362) | Bit/dim 4.9374(5.1808) | Xent 1.9265(2.0012) | Loss 5.9006(6.1814) | Error 0.6680(0.7023) Steps 526(545.98) | Grad Norm 1.8817(8.0984) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 48.3005(48.0441) | Bit/dim 4.9507(5.1739) | Xent 1.9399(1.9993) | Loss 5.9206(6.1736) | Error 0.6798(0.7017) Steps 532(545.56) | Grad Norm 4.9027(8.0025) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 23.4906, Epoch Time 327.5541(353.1739), Bit/dim 4.9386(best: 5.0051), Xent 1.9154, Loss 5.8962, Error 0.6693(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 47.5428(48.0291) | Bit/dim 4.9527(5.1673) | Xent 1.9415(1.9976) | Loss 5.9235(6.1661) | Error 0.6781(0.7010) Steps 526(544.97) | Grad Norm 6.3528(7.9530) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 47.2433(48.0055) | Bit/dim 4.9683(5.1613) | Xent 1.9292(1.9955) | Loss 5.9329(6.1591) | Error 0.6690(0.7000) Steps 520(544.22) | Grad Norm 7.8507(7.9500) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 48.3344(48.0153) | Bit/dim 4.9943(5.1563) | Xent 1.9909(1.9954) | Loss 5.9898(6.1540) | Error 0.7095(0.7003) Steps 520(543.50) | Grad Norm 17.9160(8.2490) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 45.7181(47.9464) | Bit/dim 5.4066(5.1638) | Xent 2.1697(2.0006) | Loss 6.4914(6.1641) | Error 0.7605(0.7021) Steps 538(543.33) | Grad Norm 18.4880(8.5561) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 46.0011(47.8881) | Bit/dim 5.3612(5.1698) | Xent 1.9601(1.9994) | Loss 6.3413(6.1695) | Error 0.6939(0.7018) Steps 532(542.99) | Grad Norm 7.3490(8.5199) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 44.9408(47.7997) | Bit/dim 5.1221(5.1683) | Xent 2.0656(2.0014) | Loss 6.1549(6.1690) | Error 0.7332(0.7028) Steps 532(542.66) | Grad Norm 12.6514(8.6439) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 23.5217, Epoch Time 318.7037(352.1398), Bit/dim 5.0906(best: 4.9386), Xent 2.0301, Loss 6.1057, Error 0.7122(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 46.8400(47.7709) | Bit/dim 5.1087(5.1665) | Xent 2.0827(2.0038) | Loss 6.1500(6.1684) | Error 0.7232(0.7034) Steps 520(541.98) | Grad Norm 13.8742(8.8008) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 44.7893(47.6814) | Bit/dim 5.0596(5.1633) | Xent 1.9395(2.0019) | Loss 6.0293(6.1643) | Error 0.6825(0.7028) Steps 526(541.50) | Grad Norm 6.4622(8.7306) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 47.6028(47.6791) | Bit/dim 5.1125(5.1618) | Xent 1.9959(2.0017) | Loss 6.1104(6.1627) | Error 0.7050(0.7028) Steps 538(541.40) | Grad Norm 9.1058(8.7419) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 46.6528(47.6483) | Bit/dim 5.0531(5.1585) | Xent 2.0035(2.0018) | Loss 6.0549(6.1594) | Error 0.7027(0.7028) Steps 538(541.29) | Grad Norm 11.2173(8.8161) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 46.6444(47.6182) | Bit/dim 5.0628(5.1557) | Xent 2.0891(2.0044) | Loss 6.1073(6.1579) | Error 0.7325(0.7037) Steps 514(540.48) | Grad Norm 17.9782(9.0910) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 44.1178(47.5131) | Bit/dim 5.0768(5.1533) | Xent 2.2471(2.0117) | Loss 6.2003(6.1591) | Error 0.7853(0.7062) Steps 514(539.68) | Grad Norm 20.2426(9.4255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 23.5431, Epoch Time 315.5795(351.0430), Bit/dim 5.0664(best: 4.9386), Xent 1.9812, Loss 6.0571, Error 0.7136(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 43.8862(47.4043) | Bit/dim 5.0733(5.1509) | Xent 1.9839(2.0108) | Loss 6.0653(6.1563) | Error 0.7096(0.7063) Steps 508(538.73) | Grad Norm 6.9788(9.3521) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 48.1142(47.4256) | Bit/dim 5.0388(5.1475) | Xent 2.1030(2.0136) | Loss 6.0903(6.1543) | Error 0.7358(0.7072) Steps 532(538.53) | Grad Norm 14.4157(9.5040) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 46.9920(47.4126) | Bit/dim 4.9641(5.1420) | Xent 1.9987(2.0132) | Loss 5.9635(6.1486) | Error 0.7047(0.7071) Steps 520(537.97) | Grad Norm 5.8464(9.3943) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 47.1089(47.4035) | Bit/dim 4.9672(5.1368) | Xent 2.0427(2.0140) | Loss 5.9886(6.1438) | Error 0.7278(0.7077) Steps 538(537.97) | Grad Norm 7.3000(9.3315) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 47.9305(47.4193) | Bit/dim 4.9784(5.1320) | Xent 1.9979(2.0136) | Loss 5.9774(6.1388) | Error 0.7070(0.7077) Steps 538(537.98) | Grad Norm 4.3297(9.1814) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 46.4191(47.3893) | Bit/dim 4.9584(5.1268) | Xent 1.9717(2.0123) | Loss 5.9443(6.1330) | Error 0.6992(0.7074) Steps 538(537.98) | Grad Norm 3.7549(9.0186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 24.0733, Epoch Time 319.9693(350.1108), Bit/dim 4.9503(best: 4.9386), Xent 1.9514, Loss 5.9260, Error 0.6997(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 46.1054(47.3508) | Bit/dim 4.9502(5.1215) | Xent 1.9643(2.0109) | Loss 5.9323(6.1270) | Error 0.6959(0.7071) Steps 526(537.62) | Grad Norm 3.1986(8.8440) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 48.7171(47.3918) | Bit/dim 4.9452(5.1162) | Xent 1.9635(2.0094) | Loss 5.9270(6.1210) | Error 0.6895(0.7066) Steps 532(537.45) | Grad Norm 4.2400(8.7059) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 46.1421(47.3543) | Bit/dim 4.9530(5.1114) | Xent 1.9561(2.0078) | Loss 5.9311(6.1153) | Error 0.6825(0.7058) Steps 520(536.92) | Grad Norm 4.9044(8.5919) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 45.7678(47.3067) | Bit/dim 4.9239(5.1057) | Xent 1.9408(2.0058) | Loss 5.8943(6.1086) | Error 0.6909(0.7054) Steps 520(536.42) | Grad Norm 3.8481(8.4496) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 47.7465(47.3199) | Bit/dim 4.9288(5.1004) | Xent 1.9609(2.0045) | Loss 5.9093(6.1027) | Error 0.6910(0.7050) Steps 514(535.74) | Grad Norm 6.9793(8.4054) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 47.7152(47.3318) | Bit/dim 4.9354(5.0955) | Xent 1.9428(2.0026) | Loss 5.9068(6.0968) | Error 0.6773(0.7041) Steps 520(535.27) | Grad Norm 6.8542(8.3589) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 24.0984, Epoch Time 321.9180(349.2650), Bit/dim 4.9082(best: 4.9386), Xent 1.9150, Loss 5.8656, Error 0.6591(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 46.6053(47.3100) | Bit/dim 4.9110(5.0899) | Xent 1.9365(2.0006) | Loss 5.8792(6.0903) | Error 0.6718(0.7032) Steps 520(534.81) | Grad Norm 4.2470(8.2355) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 47.9303(47.3286) | Bit/dim 4.8777(5.0836) | Xent 1.9359(1.9987) | Loss 5.8457(6.0829) | Error 0.6849(0.7026) Steps 514(534.19) | Grad Norm 7.2756(8.2067) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 45.8516(47.2843) | Bit/dim 4.8871(5.0777) | Xent 1.9637(1.9977) | Loss 5.8690(6.0765) | Error 0.6990(0.7025) Steps 508(533.40) | Grad Norm 10.4602(8.2744) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 49.1207(47.3394) | Bit/dim 4.8899(5.0720) | Xent 1.9720(1.9969) | Loss 5.8760(6.0705) | Error 0.7021(0.7025) Steps 514(532.82) | Grad Norm 12.3871(8.3977) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 45.7000(47.2902) | Bit/dim 4.9119(5.0672) | Xent 1.9506(1.9955) | Loss 5.8872(6.0650) | Error 0.6856(0.7020) Steps 502(531.90) | Grad Norm 11.0597(8.4776) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 45.8128(47.2459) | Bit/dim 4.8755(5.0615) | Xent 1.9052(1.9928) | Loss 5.8281(6.0579) | Error 0.6651(0.7009) Steps 502(531.00) | Grad Norm 6.0861(8.4058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 23.5466, Epoch Time 320.1100(348.3903), Bit/dim 4.8812(best: 4.9082), Xent 1.8898, Loss 5.8261, Error 0.6631(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 46.9587(47.2372) | Bit/dim 4.8782(5.0560) | Xent 1.9167(1.9905) | Loss 5.8365(6.0512) | Error 0.6814(0.7003) Steps 514(530.49) | Grad Norm 11.0933(8.4865) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 44.5201(47.1557) | Bit/dim 5.0551(5.0560) | Xent 1.9535(1.9894) | Loss 6.0318(6.0507) | Error 0.7034(0.7004) Steps 514(530.00) | Grad Norm 15.4418(8.6951) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 45.9361(47.1191) | Bit/dim 4.9174(5.0518) | Xent 2.0458(1.9911) | Loss 5.9403(6.0474) | Error 0.7163(0.7009) Steps 508(529.34) | Grad Norm 17.0106(8.9446) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 45.6854(47.0761) | Bit/dim 4.9031(5.0473) | Xent 2.0592(1.9931) | Loss 5.9327(6.0439) | Error 0.7056(0.7010) Steps 508(528.70) | Grad Norm 17.1746(9.1915) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 45.1352(47.0179) | Bit/dim 4.9131(5.0433) | Xent 1.9363(1.9914) | Loss 5.8813(6.0390) | Error 0.6929(0.7008) Steps 508(528.07) | Grad Norm 9.9556(9.2144) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 46.6849(47.0079) | Bit/dim 4.8757(5.0383) | Xent 2.0523(1.9933) | Loss 5.9019(6.0349) | Error 0.7332(0.7017) Steps 508(527.47) | Grad Norm 12.2587(9.3057) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 23.3951, Epoch Time 313.6716(347.3488), Bit/dim 4.8500(best: 4.8812), Xent 1.9056, Loss 5.8028, Error 0.6687(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 46.4353(46.9907) | Bit/dim 4.8324(5.0321) | Xent 1.9201(1.9911) | Loss 5.7924(6.0276) | Error 0.6721(0.7008) Steps 508(526.89) | Grad Norm 4.4877(9.1612) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 45.4675(46.9450) | Bit/dim 4.8522(5.0267) | Xent 1.9199(1.9889) | Loss 5.8122(6.0212) | Error 0.6747(0.7001) Steps 496(525.96) | Grad Norm 4.9952(9.0362) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 44.7973(46.8806) | Bit/dim 4.8523(5.0215) | Xent 1.9459(1.9876) | Loss 5.8252(6.0153) | Error 0.6809(0.6995) Steps 496(525.06) | Grad Norm 4.0960(8.8880) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 45.3105(46.8335) | Bit/dim 4.8272(5.0157) | Xent 1.9327(1.9860) | Loss 5.7936(6.0086) | Error 0.6830(0.6990) Steps 502(524.37) | Grad Norm 3.8106(8.7357) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 48.2751(46.8767) | Bit/dim 4.8293(5.0101) | Xent 1.8969(1.9833) | Loss 5.7778(6.0017) | Error 0.6679(0.6981) Steps 502(523.70) | Grad Norm 3.4438(8.5769) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 47.7613(46.9033) | Bit/dim 4.8352(5.0048) | Xent 1.9077(1.9810) | Loss 5.7890(5.9953) | Error 0.6679(0.6972) Steps 508(523.23) | Grad Norm 7.0938(8.5325) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 23.0703, Epoch Time 316.6027(346.4264), Bit/dim 4.8813(best: 4.8500), Xent 1.8990, Loss 5.8308, Error 0.6621(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 46.9614(46.9050) | Bit/dim 4.8859(5.0012) | Xent 1.9276(1.9794) | Loss 5.8496(5.9910) | Error 0.6743(0.6965) Steps 508(522.77) | Grad Norm 9.4424(8.5597) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 47.8271(46.9327) | Bit/dim 4.8325(4.9962) | Xent 1.9393(1.9782) | Loss 5.8022(5.9853) | Error 0.6897(0.6963) Steps 502(522.15) | Grad Norm 11.4906(8.6477) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 45.5275(46.8905) | Bit/dim 4.8784(4.9927) | Xent 1.9507(1.9774) | Loss 5.8538(5.9814) | Error 0.7004(0.6964) Steps 502(521.54) | Grad Norm 12.3532(8.7588) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 48.8190(46.9484) | Bit/dim 4.8652(4.9888) | Xent 1.9293(1.9760) | Loss 5.8299(5.9768) | Error 0.6746(0.6957) Steps 508(521.14) | Grad Norm 13.9571(8.9148) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 43.6629(46.8498) | Bit/dim 4.9265(4.9870) | Xent 1.9032(1.9738) | Loss 5.8781(5.9739) | Error 0.6729(0.6950) Steps 502(520.56) | Grad Norm 10.0919(8.9501) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 47.4641(46.8683) | Bit/dim 4.8125(4.9817) | Xent 1.9128(1.9720) | Loss 5.7689(5.9677) | Error 0.6786(0.6946) Steps 508(520.19) | Grad Norm 4.9144(8.8290) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 23.5549, Epoch Time 319.1904(345.6093), Bit/dim 4.9094(best: 4.8500), Xent 1.9055, Loss 5.8622, Error 0.6744(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 47.9433(46.9005) | Bit/dim 4.9042(4.9794) | Xent 1.9361(1.9709) | Loss 5.8722(5.9648) | Error 0.6756(0.6940) Steps 508(519.82) | Grad Norm 16.5365(9.0603) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 43.9862(46.8131) | Bit/dim 5.1647(4.9850) | Xent 1.9289(1.9696) | Loss 6.1292(5.9698) | Error 0.6721(0.6933) Steps 502(519.29) | Grad Norm 10.0163(9.0889) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 44.0202(46.7293) | Bit/dim 5.2678(4.9934) | Xent 1.8929(1.9673) | Loss 6.2143(5.9771) | Error 0.6583(0.6923) Steps 502(518.77) | Grad Norm 7.6993(9.0472) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 43.7481(46.6399) | Bit/dim 5.0508(4.9952) | Xent 1.9153(1.9658) | Loss 6.0085(5.9780) | Error 0.6781(0.6919) Steps 490(517.90) | Grad Norm 5.5514(8.9424) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 46.8471(46.6461) | Bit/dim 4.9764(4.9946) | Xent 1.9496(1.9653) | Loss 5.9512(5.9772) | Error 0.6844(0.6916) Steps 508(517.61) | Grad Norm 15.5909(9.1418) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 42.8643(46.5326) | Bit/dim 4.9226(4.9924) | Xent 2.0286(1.9672) | Loss 5.9370(5.9760) | Error 0.7286(0.6927) Steps 502(517.14) | Grad Norm 11.3225(9.2072) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.0248, Epoch Time 307.8125(344.4754), Bit/dim 5.0060(best: 4.8500), Xent 1.8941, Loss 5.9530, Error 0.6581(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 42.4645(46.4106) | Bit/dim 5.0101(4.9930) | Xent 1.9210(1.9658) | Loss 5.9706(5.9759) | Error 0.6739(0.6922) Steps 496(516.51) | Grad Norm 7.0779(9.1434) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 44.2490(46.3457) | Bit/dim 4.9936(4.9930) | Xent 1.9818(1.9663) | Loss 5.9845(5.9761) | Error 0.7065(0.6926) Steps 502(516.07) | Grad Norm 9.8392(9.1642) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 45.5165(46.3208) | Bit/dim 4.8959(4.9901) | Xent 1.9162(1.9648) | Loss 5.8540(5.9725) | Error 0.6706(0.6919) Steps 502(515.65) | Grad Norm 4.2097(9.0156) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 47.3278(46.3511) | Bit/dim 4.8828(4.9869) | Xent 1.9403(1.9640) | Loss 5.8530(5.9689) | Error 0.6917(0.6919) Steps 508(515.42) | Grad Norm 6.2442(8.9325) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 47.5212(46.3862) | Bit/dim 4.8804(4.9837) | Xent 1.9457(1.9635) | Loss 5.8532(5.9654) | Error 0.6941(0.6920) Steps 514(515.38) | Grad Norm 4.9125(8.8119) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 44.7266(46.3364) | Bit/dim 4.8872(4.9808) | Xent 1.8897(1.9613) | Loss 5.8320(5.9614) | Error 0.6651(0.6912) Steps 502(514.97) | Grad Norm 5.9976(8.7274) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 23.4654, Epoch Time 310.8493(343.4666), Bit/dim 4.8564(best: 4.8500), Xent 1.8924, Loss 5.8026, Error 0.6700(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 47.7990(46.3803) | Bit/dim 4.8549(4.9770) | Xent 1.9098(1.9597) | Loss 5.8099(5.9569) | Error 0.6721(0.6906) Steps 514(514.95) | Grad Norm 6.3089(8.6549) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 46.2393(46.3760) | Bit/dim 4.8438(4.9730) | Xent 1.9021(1.9580) | Loss 5.7949(5.9520) | Error 0.6634(0.6898) Steps 502(514.56) | Grad Norm 4.7739(8.5384) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 45.7114(46.3561) | Bit/dim 4.8407(4.9690) | Xent 1.8789(1.9556) | Loss 5.7802(5.9468) | Error 0.6560(0.6888) Steps 502(514.18) | Grad Norm 5.2913(8.4410) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 47.9651(46.4044) | Bit/dim 4.8254(4.9647) | Xent 1.9089(1.9542) | Loss 5.7798(5.9418) | Error 0.6719(0.6883) Steps 508(514.00) | Grad Norm 6.2038(8.3739) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 48.4151(46.4647) | Bit/dim 4.8259(4.9606) | Xent 1.8569(1.9513) | Loss 5.7544(5.9362) | Error 0.6571(0.6874) Steps 508(513.82) | Grad Norm 5.0407(8.2739) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 47.2985(46.4897) | Bit/dim 4.8109(4.9561) | Xent 1.8877(1.9494) | Loss 5.7547(5.9308) | Error 0.6539(0.6863) Steps 514(513.82) | Grad Norm 4.9587(8.1745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 23.3561, Epoch Time 322.6153(342.8411), Bit/dim 4.7908(best: 4.8500), Xent 1.8216, Loss 5.7016, Error 0.6321(best: 0.6559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 48.2815(46.5434) | Bit/dim 4.7946(4.9512) | Xent 1.8420(1.9462) | Loss 5.7156(5.9243) | Error 0.6527(0.6853) Steps 514(513.83) | Grad Norm 3.2825(8.0277) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 47.6576(46.5769) | Bit/dim 4.7929(4.9465) | Xent 1.8543(1.9434) | Loss 5.7200(5.9182) | Error 0.6442(0.6841) Steps 502(513.47) | Grad Norm 4.4402(7.9201) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 47.8547(46.6152) | Bit/dim 4.7887(4.9417) | Xent 1.8567(1.9408) | Loss 5.7171(5.9122) | Error 0.6529(0.6832) Steps 502(513.13) | Grad Norm 5.0285(7.8333) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 48.6160(46.6752) | Bit/dim 4.7705(4.9366) | Xent 1.8634(1.9385) | Loss 5.7022(5.9059) | Error 0.6520(0.6822) Steps 514(513.15) | Grad Norm 6.7660(7.8013) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 46.6165(46.6735) | Bit/dim 4.7888(4.9322) | Xent 1.8349(1.9354) | Loss 5.7063(5.8999) | Error 0.6452(0.6811) Steps 514(513.18) | Grad Norm 8.3658(7.8182) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 47.2548(46.6909) | Bit/dim 4.7855(4.9278) | Xent 1.8934(1.9341) | Loss 5.7322(5.8948) | Error 0.6614(0.6805) Steps 508(513.02) | Grad Norm 18.7710(8.1468) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 23.4689, Epoch Time 325.1651(342.3108), Bit/dim 4.9946(best: 4.7908), Xent 2.0960, Loss 6.0426, Error 0.7459(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 44.5480(46.6266) | Bit/dim 4.9943(4.9298) | Xent 2.1365(1.9402) | Loss 6.0625(5.8999) | Error 0.7518(0.6827) Steps 514(513.05) | Grad Norm 25.0791(8.6548) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 45.4223(46.5905) | Bit/dim 4.9126(4.9293) | Xent 2.0243(1.9427) | Loss 5.9248(5.9006) | Error 0.7080(0.6834) Steps 514(513.08) | Grad Norm 20.5798(9.0125) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 45.3506(46.5533) | Bit/dim 5.1796(4.9368) | Xent 2.5519(1.9610) | Loss 6.4556(5.9173) | Error 0.7654(0.6859) Steps 526(513.47) | Grad Norm 41.5512(9.9887) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 45.8970(46.5336) | Bit/dim 5.3513(4.9492) | Xent 1.9335(1.9602) | Loss 6.3181(5.9293) | Error 0.6949(0.6862) Steps 520(513.66) | Grad Norm 8.3963(9.9409) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 45.3947(46.4994) | Bit/dim 5.6499(4.9702) | Xent 1.9915(1.9611) | Loss 6.6457(5.9508) | Error 0.7056(0.6867) Steps 514(513.67) | Grad Norm 6.6102(9.8410) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 45.7339(46.4765) | Bit/dim 5.6232(4.9898) | Xent 2.0822(1.9647) | Loss 6.6643(5.9722) | Error 0.7569(0.6888) Steps 508(513.50) | Grad Norm 6.7166(9.7473) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 23.4704, Epoch Time 311.4335(341.3845), Bit/dim 5.4379(best: 4.7908), Xent 2.1200, Loss 6.4979, Error 0.7851(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 45.1431(46.4365) | Bit/dim 5.4378(5.0033) | Xent 2.1422(1.9701) | Loss 6.5089(5.9883) | Error 0.7800(0.6916) Steps 514(513.52) | Grad Norm 6.8830(9.6613) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 46.9841(46.4529) | Bit/dim 5.2822(5.0116) | Xent 2.0958(1.9738) | Loss 6.3300(5.9985) | Error 0.7582(0.6936) Steps 538(514.25) | Grad Norm 5.3070(9.5307) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 48.1826(46.5048) | Bit/dim 5.2049(5.0174) | Xent 2.1326(1.9786) | Loss 6.2712(6.0067) | Error 0.7672(0.6958) Steps 544(515.15) | Grad Norm 5.6596(9.4146) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 44.3895(46.4413) | Bit/dim 5.1299(5.0208) | Xent 2.1203(1.9829) | Loss 6.1901(6.0122) | Error 0.7639(0.6978) Steps 514(515.11) | Grad Norm 4.2008(9.2582) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 45.2958(46.4070) | Bit/dim 5.1131(5.0236) | Xent 2.0776(1.9857) | Loss 6.1519(6.0164) | Error 0.7483(0.6993) Steps 514(515.08) | Grad Norm 4.0892(9.1031) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 44.2245(46.3415) | Bit/dim 5.1119(5.0262) | Xent 2.0700(1.9882) | Loss 6.1469(6.0203) | Error 0.7356(0.7004) Steps 520(515.23) | Grad Norm 5.2555(8.9877) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.6953, Epoch Time 313.4501(340.5465), Bit/dim 5.0912(best: 4.7908), Xent 2.0154, Loss 6.0989, Error 0.7202(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 43.7180(46.2628) | Bit/dim 5.0888(5.0281) | Xent 2.0550(1.9902) | Loss 6.1163(6.0232) | Error 0.7369(0.7015) Steps 508(515.01) | Grad Norm 4.3756(8.8493) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 44.3581(46.2057) | Bit/dim 5.0711(5.0294) | Xent 2.0265(1.9913) | Loss 6.0844(6.0250) | Error 0.7211(0.7021) Steps 520(515.16) | Grad Norm 4.0696(8.7059) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 43.7125(46.1309) | Bit/dim 5.0256(5.0293) | Xent 2.0146(1.9920) | Loss 6.0329(6.0253) | Error 0.7069(0.7023) Steps 508(514.94) | Grad Norm 3.4560(8.5484) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 48.5128(46.2023) | Bit/dim 5.0001(5.0284) | Xent 1.9923(1.9920) | Loss 5.9963(6.0244) | Error 0.7083(0.7024) Steps 520(515.10) | Grad Norm 3.7829(8.4055) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 45.4964(46.1811) | Bit/dim 4.9692(5.0266) | Xent 2.0263(1.9931) | Loss 5.9824(6.0231) | Error 0.7294(0.7032) Steps 502(514.70) | Grad Norm 6.6294(8.3522) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 47.2874(46.2143) | Bit/dim 4.9581(5.0246) | Xent 2.0504(1.9948) | Loss 5.9833(6.0219) | Error 0.7306(0.7041) Steps 508(514.50) | Grad Norm 13.0724(8.4938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 23.2230, Epoch Time 311.8774(339.6864), Bit/dim 5.0022(best: 4.7908), Xent 2.0427, Loss 6.0236, Error 0.7378(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 44.3252(46.1576) | Bit/dim 4.9988(5.0238) | Xent 2.0601(1.9967) | Loss 6.0288(6.0222) | Error 0.7390(0.7051) Steps 502(514.13) | Grad Norm 16.4985(8.7339) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 48.3204(46.2225) | Bit/dim 4.9459(5.0215) | Xent 1.9709(1.9960) | Loss 5.9313(6.0194) | Error 0.6961(0.7048) Steps 508(513.94) | Grad Norm 6.0898(8.6546) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 47.0512(46.2474) | Bit/dim 4.9210(5.0184) | Xent 2.0331(1.9971) | Loss 5.9376(6.0170) | Error 0.7126(0.7051) Steps 508(513.76) | Grad Norm 11.2757(8.7332) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 45.0575(46.2117) | Bit/dim 4.9462(5.0163) | Xent 2.0759(1.9994) | Loss 5.9841(6.0160) | Error 0.7370(0.7060) Steps 502(513.41) | Grad Norm 11.1011(8.8043) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 48.5626(46.2822) | Bit/dim 4.8826(5.0123) | Xent 1.9876(1.9991) | Loss 5.8764(6.0118) | Error 0.7150(0.7063) Steps 520(513.61) | Grad Norm 5.7133(8.7115) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 49.4015(46.3758) | Bit/dim 4.9032(5.0090) | Xent 2.0711(2.0012) | Loss 5.9387(6.0096) | Error 0.7310(0.7070) Steps 508(513.44) | Grad Norm 12.2232(8.8169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 23.1022, Epoch Time 321.1806(339.1312), Bit/dim 4.9468(best: 4.7908), Xent 2.0140, Loss 5.9538, Error 0.7175(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 45.3865(46.3461) | Bit/dim 4.9452(5.0071) | Xent 2.0301(2.0021) | Loss 5.9603(6.0081) | Error 0.7176(0.7074) Steps 496(512.92) | Grad Norm 10.8364(8.8775) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 47.1841(46.3713) | Bit/dim 4.8954(5.0037) | Xent 2.0377(2.0032) | Loss 5.9142(6.0053) | Error 0.7258(0.7079) Steps 520(513.13) | Grad Norm 13.0913(9.0039) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 46.2265(46.3669) | Bit/dim 4.8655(4.9996) | Xent 1.9440(2.0014) | Loss 5.8375(6.0003) | Error 0.6891(0.7074) Steps 502(512.80) | Grad Norm 6.2529(8.9214) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 48.0495(46.4174) | Bit/dim 4.8467(4.9950) | Xent 1.9910(2.0011) | Loss 5.8422(5.9955) | Error 0.6959(0.7070) Steps 514(512.83) | Grad Norm 6.6309(8.8527) | Total Time 14.00(14.00)\n",
      "Iter 0311 | Time 45.5118(46.3902) | Bit/dim 4.8374(4.9903) | Xent 1.9436(1.9994) | Loss 5.8092(5.9899) | Error 0.6856(0.7064) Steps 496(512.33) | Grad Norm 3.4159(8.6895) | Total Time 14.00(14.00)\n",
      "Iter 0312 | Time 46.5386(46.3947) | Bit/dim 4.8348(4.9856) | Xent 1.9605(1.9982) | Loss 5.8151(5.9847) | Error 0.6901(0.7059) Steps 502(512.02) | Grad Norm 3.3854(8.5304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 23.5914, Epoch Time 318.4186(338.5098), Bit/dim 4.8002(best: 4.7908), Xent 1.9175, Loss 5.7590, Error 0.6680(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 46.7110(46.4042) | Bit/dim 4.8090(4.9803) | Xent 1.9547(1.9969) | Loss 5.7863(5.9787) | Error 0.6830(0.7052) Steps 502(511.72) | Grad Norm 3.2326(8.3715) | Total Time 14.00(14.00)\n",
      "Iter 0314 | Time 45.8450(46.3874) | Bit/dim 4.7977(4.9748) | Xent 1.9171(1.9945) | Loss 5.7562(5.9721) | Error 0.6743(0.7043) Steps 496(511.25) | Grad Norm 3.4012(8.2224) | Total Time 14.00(14.00)\n",
      "Iter 0315 | Time 46.8481(46.4012) | Bit/dim 4.7855(4.9691) | Xent 1.9173(1.9922) | Loss 5.7442(5.9652) | Error 0.6771(0.7034) Steps 496(510.79) | Grad Norm 5.8956(8.1526) | Total Time 14.00(14.00)\n",
      "Iter 0316 | Time 46.8170(46.4137) | Bit/dim 4.7694(4.9631) | Xent 1.9406(1.9906) | Loss 5.7397(5.9585) | Error 0.6793(0.7027) Steps 502(510.52) | Grad Norm 4.3132(8.0374) | Total Time 14.00(14.00)\n",
      "Iter 0317 | Time 47.4260(46.4441) | Bit/dim 4.7471(4.9567) | Xent 1.9022(1.9880) | Loss 5.6982(5.9507) | Error 0.6661(0.7016) Steps 502(510.27) | Grad Norm 3.7029(7.9074) | Total Time 14.00(14.00)\n",
      "Iter 0318 | Time 46.5359(46.4468) | Bit/dim 4.7412(4.9502) | Xent 1.9127(1.9857) | Loss 5.6975(5.9431) | Error 0.6746(0.7008) Steps 502(510.02) | Grad Norm 3.0984(7.7631) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 23.0997, Epoch Time 318.8281(337.9194), Bit/dim 4.7551(best: 4.7908), Xent 1.8928, Loss 5.7016, Error 0.6694(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 45.9426(46.4317) | Bit/dim 4.7459(4.9441) | Xent 1.9216(1.9838) | Loss 5.7067(5.9360) | Error 0.6761(0.7001) Steps 502(509.78) | Grad Norm 7.2492(7.7477) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 45.8430(46.4140) | Bit/dim 4.7437(4.9381) | Xent 1.9858(1.9839) | Loss 5.7366(5.9300) | Error 0.6959(0.6999) Steps 496(509.37) | Grad Norm 17.7843(8.0488) | Total Time 14.00(14.00)\n",
      "Iter 0321 | Time 45.5691(46.3887) | Bit/dim 4.8934(4.9367) | Xent 2.0965(1.9872) | Loss 5.9416(5.9303) | Error 0.7458(0.7013) Steps 514(509.51) | Grad Norm 16.3575(8.2980) | Total Time 14.00(14.00)\n",
      "Iter 0322 | Time 49.5868(46.4846) | Bit/dim 4.7821(4.9321) | Xent 2.0086(1.9879) | Loss 5.7864(5.9260) | Error 0.7195(0.7019) Steps 520(509.82) | Grad Norm 12.6087(8.4274) | Total Time 14.00(14.00)\n",
      "Iter 0323 | Time 48.0297(46.5310) | Bit/dim 4.8395(4.9293) | Xent 1.9431(1.9865) | Loss 5.8110(5.9226) | Error 0.6803(0.7012) Steps 502(509.59) | Grad Norm 11.1529(8.5091) | Total Time 14.00(14.00)\n",
      "Iter 0324 | Time 42.6811(46.4155) | Bit/dim 4.8925(4.9282) | Xent 1.9175(1.9845) | Loss 5.8513(5.9204) | Error 0.6758(0.7005) Steps 490(509.00) | Grad Norm 7.1281(8.4677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 23.5473, Epoch Time 316.6716(337.2819), Bit/dim 4.8274(best: 4.7551), Xent 1.8892, Loss 5.7721, Error 0.6627(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 45.9887(46.4027) | Bit/dim 4.8329(4.9253) | Xent 1.9079(1.9822) | Loss 5.7868(5.9164) | Error 0.6741(0.6997) Steps 502(508.79) | Grad Norm 5.3842(8.3752) | Total Time 14.00(14.00)\n",
      "Iter 0326 | Time 46.4330(46.4036) | Bit/dim 4.7956(4.9215) | Xent 1.9300(1.9806) | Loss 5.7606(5.9117) | Error 0.6809(0.6991) Steps 502(508.58) | Grad Norm 8.6919(8.3847) | Total Time 14.00(14.00)\n",
      "Iter 0327 | Time 44.4751(46.3457) | Bit/dim 4.8191(4.9184) | Xent 1.9268(1.9790) | Loss 5.7825(5.9079) | Error 0.6850(0.6987) Steps 496(508.21) | Grad Norm 6.0214(8.3138) | Total Time 14.00(14.00)\n",
      "Iter 0328 | Time 45.7057(46.3265) | Bit/dim 4.7705(4.9139) | Xent 1.8837(1.9761) | Loss 5.7123(5.9020) | Error 0.6564(0.6974) Steps 508(508.20) | Grad Norm 3.6485(8.1738) | Total Time 14.00(14.00)\n",
      "Iter 0329 | Time 47.9184(46.3743) | Bit/dim 4.8009(4.9106) | Xent 1.9573(1.9756) | Loss 5.7796(5.8983) | Error 0.6909(0.6972) Steps 514(508.38) | Grad Norm 9.7985(8.2226) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 44.6348(46.3221) | Bit/dim 4.7892(4.9069) | Xent 1.9031(1.9734) | Loss 5.7407(5.8936) | Error 0.6656(0.6963) Steps 496(508.00) | Grad Norm 5.4273(8.1387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 23.1289, Epoch Time 313.7775(336.5768), Bit/dim 4.7641(best: 4.7551), Xent 1.8787, Loss 5.7034, Error 0.6519(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 43.6275(46.2413) | Bit/dim 4.7651(4.9027) | Xent 1.9015(1.9712) | Loss 5.7158(5.8883) | Error 0.6700(0.6955) Steps 490(507.46) | Grad Norm 5.5818(8.0620) | Total Time 14.00(14.00)\n",
      "Iter 0332 | Time 46.7463(46.2564) | Bit/dim 4.7726(4.8988) | Xent 1.8942(1.9689) | Loss 5.7197(5.8832) | Error 0.6695(0.6947) Steps 508(507.48) | Grad Norm 7.1074(8.0334) | Total Time 14.00(14.00)\n",
      "Iter 0333 | Time 46.5262(46.2645) | Bit/dim 4.7292(4.8937) | Xent 1.8639(1.9658) | Loss 5.6612(5.8765) | Error 0.6454(0.6932) Steps 496(507.14) | Grad Norm 2.5927(7.8701) | Total Time 14.00(14.00)\n",
      "Iter 0334 | Time 46.2688(46.2646) | Bit/dim 4.7290(4.8887) | Xent 1.8622(1.9627) | Loss 5.6601(5.8701) | Error 0.6560(0.6921) Steps 496(506.80) | Grad Norm 3.1715(7.7292) | Total Time 14.00(14.00)\n",
      "Iter 0335 | Time 46.7867(46.2803) | Bit/dim 4.7119(4.8834) | Xent 1.8731(1.9600) | Loss 5.6484(5.8634) | Error 0.6554(0.6910) Steps 502(506.66) | Grad Norm 4.4910(7.6320) | Total Time 14.00(14.00)\n",
      "Iter 0336 | Time 45.7603(46.2647) | Bit/dim 4.6957(4.8778) | Xent 1.8604(1.9570) | Loss 5.6259(5.8563) | Error 0.6544(0.6899) Steps 502(506.52) | Grad Norm 5.5811(7.5705) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 23.3227, Epoch Time 314.8491(335.9250), Bit/dim 4.6969(best: 4.7551), Xent 1.8516, Loss 5.6227, Error 0.6591(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 47.9441(46.3151) | Bit/dim 4.6922(4.8722) | Xent 1.8705(1.9544) | Loss 5.6274(5.8494) | Error 0.6693(0.6893) Steps 502(506.38) | Grad Norm 8.3189(7.5930) | Total Time 14.00(14.00)\n",
      "Iter 0338 | Time 48.3067(46.3748) | Bit/dim 4.6942(4.8669) | Xent 1.8982(1.9527) | Loss 5.6433(5.8432) | Error 0.6724(0.6888) Steps 502(506.25) | Grad Norm 11.4548(7.7088) | Total Time 14.00(14.00)\n",
      "Iter 0339 | Time 47.2218(46.4002) | Bit/dim 4.7106(4.8622) | Xent 1.9130(1.9515) | Loss 5.6670(5.8379) | Error 0.6755(0.6884) Steps 496(505.94) | Grad Norm 17.6704(8.0077) | Total Time 14.00(14.00)\n",
      "Iter 0340 | Time 48.5327(46.4642) | Bit/dim 4.7712(4.8595) | Xent 1.9762(1.9523) | Loss 5.7593(5.8356) | Error 0.6970(0.6886) Steps 520(506.36) | Grad Norm 15.6364(8.2365) | Total Time 14.00(14.00)\n",
      "Iter 0341 | Time 49.2836(46.5488) | Bit/dim 4.6888(4.8543) | Xent 1.8563(1.9494) | Loss 5.6169(5.8290) | Error 0.6526(0.6876) Steps 526(506.95) | Grad Norm 6.2322(8.1764) | Total Time 14.00(14.00)\n",
      "Iter 0342 | Time 47.9210(46.5900) | Bit/dim 4.7091(4.8500) | Xent 1.9487(1.9494) | Loss 5.6835(5.8247) | Error 0.6795(0.6873) Steps 502(506.81) | Grad Norm 13.9381(8.3492) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 23.7368, Epoch Time 328.4253(335.7000), Bit/dim 4.7421(best: 4.6969), Xent 1.8531, Loss 5.6687, Error 0.6490(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 47.3139(46.6117) | Bit/dim 4.7427(4.8468) | Xent 1.8714(1.9470) | Loss 5.6784(5.8203) | Error 0.6583(0.6864) Steps 514(507.02) | Grad Norm 8.7962(8.3627) | Total Time 14.00(14.00)\n",
      "Iter 0344 | Time 48.6795(46.6737) | Bit/dim 4.7224(4.8430) | Xent 1.9698(1.9477) | Loss 5.7074(5.8169) | Error 0.7032(0.6869) Steps 520(507.41) | Grad Norm 10.8476(8.4372) | Total Time 14.00(14.00)\n",
      "Iter 0345 | Time 48.0070(46.7137) | Bit/dim 4.6993(4.8387) | Xent 1.8880(1.9459) | Loss 5.6433(5.8117) | Error 0.6623(0.6862) Steps 526(507.97) | Grad Norm 6.0347(8.3651) | Total Time 14.00(14.00)\n",
      "Iter 0346 | Time 46.6970(46.7132) | Bit/dim 4.6937(4.8344) | Xent 1.8832(1.9440) | Loss 5.6352(5.8064) | Error 0.6604(0.6854) Steps 496(507.61) | Grad Norm 4.8274(8.2590) | Total Time 14.00(14.00)\n",
      "Iter 0347 | Time 49.4116(46.7942) | Bit/dim 4.6974(4.8303) | Xent 1.8891(1.9424) | Loss 5.6419(5.8014) | Error 0.6653(0.6848) Steps 502(507.44) | Grad Norm 4.6448(8.1506) | Total Time 14.00(14.00)\n",
      "Iter 0348 | Time 49.0747(46.8626) | Bit/dim 4.6618(4.8252) | Xent 1.9088(1.9414) | Loss 5.6161(5.7959) | Error 0.6770(0.6846) Steps 496(507.10) | Grad Norm 5.8138(8.0805) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.4973, Epoch Time 328.0197(335.4696), Bit/dim 4.6965(best: 4.6969), Xent 1.8620, Loss 5.6275, Error 0.6611(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 48.3183(46.9062) | Bit/dim 4.6850(4.8210) | Xent 1.8946(1.9400) | Loss 5.6323(5.7910) | Error 0.6654(0.6840) Steps 520(507.48) | Grad Norm 10.7225(8.1597) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 50.1048(47.0022) | Bit/dim 4.6819(4.8168) | Xent 1.8733(1.9380) | Loss 5.6186(5.7858) | Error 0.6654(0.6835) Steps 502(507.32) | Grad Norm 8.7042(8.1761) | Total Time 14.00(14.00)\n",
      "Iter 0351 | Time 47.8202(47.0267) | Bit/dim 4.6672(4.8123) | Xent 1.8616(1.9357) | Loss 5.5980(5.7802) | Error 0.6619(0.6828) Steps 502(507.16) | Grad Norm 7.5268(8.1566) | Total Time 14.00(14.00)\n",
      "Iter 0352 | Time 47.4185(47.0385) | Bit/dim 4.6452(4.8073) | Xent 1.8444(1.9329) | Loss 5.5673(5.7738) | Error 0.6481(0.6818) Steps 514(507.37) | Grad Norm 2.9845(8.0014) | Total Time 14.00(14.00)\n",
      "Iter 0353 | Time 48.1636(47.0723) | Bit/dim 4.6347(4.8021) | Xent 1.8574(1.9307) | Loss 5.5634(5.7675) | Error 0.6562(0.6810) Steps 502(507.20) | Grad Norm 3.3745(7.8626) | Total Time 14.00(14.00)\n",
      "Iter 0354 | Time 46.2668(47.0481) | Bit/dim 4.6317(4.7970) | Xent 1.8241(1.9275) | Loss 5.5438(5.7608) | Error 0.6425(0.6798) Steps 502(507.05) | Grad Norm 2.9463(7.7151) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 23.1967, Epoch Time 326.9396(335.2137), Bit/dim 4.6282(best: 4.6965), Xent 1.7748, Loss 5.5156, Error 0.6146(best: 0.6321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 46.9484(47.0451) | Bit/dim 4.6304(4.7920) | Xent 1.8006(1.9237) | Loss 5.5307(5.7539) | Error 0.6330(0.6784) Steps 496(506.72) | Grad Norm 3.0208(7.5743) | Total Time 14.00(14.00)\n",
      "Iter 0356 | Time 47.0607(47.0456) | Bit/dim 4.6421(4.7875) | Xent 1.8140(1.9204) | Loss 5.5491(5.7477) | Error 0.6334(0.6771) Steps 502(506.58) | Grad Norm 6.1356(7.5311) | Total Time 14.00(14.00)\n",
      "Iter 0357 | Time 48.3543(47.0848) | Bit/dim 4.6715(4.7841) | Xent 1.8262(1.9176) | Loss 5.5845(5.7428) | Error 0.6421(0.6760) Steps 508(506.62) | Grad Norm 10.9905(7.6349) | Total Time 14.00(14.00)\n",
      "Iter 0358 | Time 46.7750(47.0755) | Bit/dim 4.7430(4.7828) | Xent 1.9399(1.9182) | Loss 5.7130(5.7419) | Error 0.6779(0.6761) Steps 502(506.48) | Grad Norm 17.6230(7.9346) | Total Time 14.00(14.00)\n",
      "Iter 0359 | Time 46.1493(47.0477) | Bit/dim 4.8950(4.7862) | Xent 1.8505(1.9162) | Loss 5.8203(5.7443) | Error 0.6489(0.6753) Steps 514(506.71) | Grad Norm 11.5562(8.0432) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 47.4326(47.0593) | Bit/dim 4.8273(4.7874) | Xent 1.8770(1.9150) | Loss 5.7658(5.7449) | Error 0.6703(0.6751) Steps 490(506.20) | Grad Norm 9.1067(8.0751) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 22.7087, Epoch Time 320.9798(334.7867), Bit/dim 4.6884(best: 4.6282), Xent 1.8323, Loss 5.6045, Error 0.6462(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 46.5165(47.0430) | Bit/dim 4.6831(4.7843) | Xent 1.8760(1.9138) | Loss 5.6211(5.7412) | Error 0.6601(0.6747) Steps 502(506.08) | Grad Norm 7.7133(8.0643) | Total Time 14.00(14.00)\n",
      "Iter 0362 | Time 46.9926(47.0415) | Bit/dim 4.7030(4.7818) | Xent 1.8848(1.9130) | Loss 5.6454(5.7383) | Error 0.6623(0.6743) Steps 502(505.96) | Grad Norm 6.7237(8.0240) | Total Time 14.00(14.00)\n",
      "Iter 0363 | Time 48.1132(47.0737) | Bit/dim 4.6952(4.7793) | Xent 1.8803(1.9120) | Loss 5.6354(5.7352) | Error 0.6640(0.6740) Steps 502(505.84) | Grad Norm 8.6929(8.0441) | Total Time 14.00(14.00)\n",
      "Iter 0364 | Time 48.4766(47.1157) | Bit/dim 4.7501(4.7784) | Xent 1.9266(1.9124) | Loss 5.7134(5.7346) | Error 0.6790(0.6741) Steps 520(506.26) | Grad Norm 20.9563(8.4315) | Total Time 14.00(14.00)\n",
      "Iter 0365 | Time 48.5509(47.1588) | Bit/dim 4.7931(4.7788) | Xent 2.5401(1.9313) | Loss 6.0632(5.7444) | Error 0.8146(0.6784) Steps 520(506.67) | Grad Norm 26.2322(8.9655) | Total Time 14.00(14.00)\n",
      "Iter 0366 | Time 48.7049(47.2052) | Bit/dim 4.7542(4.7781) | Xent 2.0384(1.9345) | Loss 5.7734(5.7453) | Error 0.7418(0.6803) Steps 526(507.25) | Grad Norm 6.9144(8.9040) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 22.9885, Epoch Time 326.1500(334.5276), Bit/dim 4.7898(best: 4.6282), Xent 2.0017, Loss 5.7907, Error 0.7341(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 48.3392(47.2392) | Bit/dim 4.7910(4.7785) | Xent 2.0377(1.9376) | Loss 5.8099(5.7473) | Error 0.7351(0.6819) Steps 514(507.46) | Grad Norm 11.0500(8.9683) | Total Time 14.00(14.00)\n",
      "Iter 0368 | Time 48.6837(47.2825) | Bit/dim 4.7265(4.7769) | Xent 2.1336(1.9435) | Loss 5.7932(5.7486) | Error 0.7615(0.6843) Steps 520(507.83) | Grad Norm 6.6353(8.8984) | Total Time 14.00(14.00)\n",
      "Iter 0369 | Time 47.2373(47.2812) | Bit/dim 4.7241(4.7753) | Xent 2.0542(1.9468) | Loss 5.7512(5.7487) | Error 0.7286(0.6856) Steps 502(507.66) | Grad Norm 4.8771(8.7777) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 45.7332(47.2347) | Bit/dim 4.7310(4.7740) | Xent 2.0645(1.9503) | Loss 5.7632(5.7491) | Error 0.7232(0.6868) Steps 502(507.49) | Grad Norm 5.6375(8.6835) | Total Time 14.00(14.00)\n",
      "Iter 0371 | Time 46.0399(47.1989) | Bit/dim 4.7135(4.7722) | Xent 2.0845(1.9543) | Loss 5.7558(5.7493) | Error 0.7314(0.6881) Steps 496(507.14) | Grad Norm 4.6147(8.5614) | Total Time 14.00(14.00)\n",
      "Iter 0372 | Time 48.1216(47.2266) | Bit/dim 4.7159(4.7705) | Xent 2.0458(1.9571) | Loss 5.7388(5.7490) | Error 0.7330(0.6894) Steps 514(507.35) | Grad Norm 7.0818(8.5171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 22.8140, Epoch Time 322.2790(334.1601), Bit/dim 4.8287(best: 4.6282), Xent 1.9868, Loss 5.8221, Error 0.7183(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 44.7772(47.1531) | Bit/dim 4.8344(4.7724) | Xent 2.0255(1.9591) | Loss 5.8472(5.7520) | Error 0.7271(0.6906) Steps 496(507.01) | Grad Norm 10.1624(8.5664) | Total Time 14.00(14.00)\n",
      "Iter 0374 | Time 47.5493(47.1650) | Bit/dim 4.7378(4.7714) | Xent 2.0265(1.9612) | Loss 5.7511(5.7519) | Error 0.7169(0.6914) Steps 514(507.22) | Grad Norm 8.6606(8.5692) | Total Time 14.00(14.00)\n",
      "Iter 0375 | Time 47.1998(47.1660) | Bit/dim 4.6926(4.7690) | Xent 1.9126(1.9597) | Loss 5.6489(5.7489) | Error 0.6776(0.6909) Steps 514(507.42) | Grad Norm 3.9485(8.4306) | Total Time 14.00(14.00)\n",
      "Iter 0376 | Time 50.0960(47.2539) | Bit/dim 4.7111(4.7673) | Xent 1.9363(1.9590) | Loss 5.6793(5.7468) | Error 0.6839(0.6907) Steps 526(507.98) | Grad Norm 5.3256(8.3375) | Total Time 14.00(14.00)\n",
      "Iter 0377 | Time 47.9545(47.2749) | Bit/dim 4.7056(4.7654) | Xent 1.8926(1.9570) | Loss 5.6518(5.7439) | Error 0.6658(0.6900) Steps 520(508.34) | Grad Norm 5.4192(8.2499) | Total Time 14.00(14.00)\n",
      "Iter 0378 | Time 47.7179(47.2882) | Bit/dim 4.6593(4.7622) | Xent 1.8925(1.9551) | Loss 5.6055(5.7398) | Error 0.6619(0.6891) Steps 520(508.69) | Grad Norm 5.3298(8.1623) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 23.8696, Epoch Time 324.6160(333.8738), Bit/dim 4.6565(best: 4.6282), Xent 1.8285, Loss 5.5707, Error 0.6433(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 47.6133(47.2980) | Bit/dim 4.6582(4.7591) | Xent 1.8692(1.9525) | Loss 5.5928(5.7354) | Error 0.6599(0.6883) Steps 514(508.85) | Grad Norm 4.5203(8.0531) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 47.8115(47.3134) | Bit/dim 4.6570(4.7560) | Xent 1.9605(1.9527) | Loss 5.6372(5.7324) | Error 0.7016(0.6887) Steps 514(509.00) | Grad Norm 9.6076(8.0997) | Total Time 14.00(14.00)\n",
      "Iter 0381 | Time 46.6233(47.2927) | Bit/dim 4.6699(4.7535) | Xent 1.9590(1.9529) | Loss 5.6494(5.7299) | Error 0.6806(0.6884) Steps 514(509.15) | Grad Norm 10.9804(8.1861) | Total Time 14.00(14.00)\n",
      "Iter 0382 | Time 48.2589(47.3217) | Bit/dim 4.6684(4.7509) | Xent 1.9570(1.9530) | Loss 5.6469(5.7274) | Error 0.6950(0.6886) Steps 514(509.30) | Grad Norm 11.8861(8.2971) | Total Time 14.00(14.00)\n",
      "Iter 0383 | Time 48.2018(47.3481) | Bit/dim 4.7095(4.7497) | Xent 2.0924(1.9572) | Loss 5.7557(5.7283) | Error 0.7066(0.6892) Steps 508(509.26) | Grad Norm 15.1645(8.5031) | Total Time 14.00(14.00)\n",
      "Iter 0384 | Time 45.5198(47.2932) | Bit/dim 4.7510(4.7497) | Xent 1.8747(1.9547) | Loss 5.6883(5.7271) | Error 0.6604(0.6883) Steps 496(508.86) | Grad Norm 7.1297(8.4619) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 22.6839, Epoch Time 322.1370(333.5217), Bit/dim 4.7146(best: 4.6282), Xent 1.8732, Loss 5.6511, Error 0.6587(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 47.7938(47.3082) | Bit/dim 4.7210(4.7488) | Xent 1.9111(1.9534) | Loss 5.6766(5.7256) | Error 0.6800(0.6880) Steps 502(508.66) | Grad Norm 8.2576(8.4558) | Total Time 14.00(14.00)\n",
      "Iter 0386 | Time 47.5619(47.3159) | Bit/dim 4.7089(4.7476) | Xent 1.9085(1.9521) | Loss 5.6631(5.7237) | Error 0.6726(0.6876) Steps 502(508.46) | Grad Norm 8.9948(8.4720) | Total Time 14.00(14.00)\n",
      "Iter 0387 | Time 43.2739(47.1946) | Bit/dim 4.8277(4.7501) | Xent 1.8523(1.9491) | Loss 5.7538(5.7246) | Error 0.6511(0.6865) Steps 490(507.90) | Grad Norm 5.6669(8.3878) | Total Time 14.00(14.00)\n",
      "Iter 0388 | Time 43.1784(47.0741) | Bit/dim 4.8233(4.7522) | Xent 1.8858(1.9472) | Loss 5.7662(5.7258) | Error 0.6678(0.6859) Steps 478(507.01) | Grad Norm 5.6200(8.3048) | Total Time 14.00(14.00)\n",
      "Iter 0389 | Time 46.7341(47.0639) | Bit/dim 4.6851(4.7502) | Xent 1.9014(1.9458) | Loss 5.6358(5.7231) | Error 0.6807(0.6858) Steps 502(506.86) | Grad Norm 5.7086(8.2269) | Total Time 14.00(14.00)\n",
      "Iter 0390 | Time 47.7155(47.0835) | Bit/dim 4.6797(4.7481) | Xent 1.9301(1.9454) | Loss 5.6448(5.7208) | Error 0.6876(0.6858) Steps 508(506.89) | Grad Norm 8.3801(8.2315) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 22.5959, Epoch Time 314.4118(332.9484), Bit/dim 4.6461(best: 4.6282), Xent 1.8491, Loss 5.5707, Error 0.6527(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 47.4580(47.0947) | Bit/dim 4.6348(4.7447) | Xent 1.8856(1.9436) | Loss 5.5776(5.7165) | Error 0.6715(0.6854) Steps 508(506.92) | Grad Norm 4.5709(8.1217) | Total Time 14.00(14.00)\n",
      "Iter 0392 | Time 47.2211(47.0985) | Bit/dim 4.6702(4.7425) | Xent 1.9016(1.9423) | Loss 5.6210(5.7136) | Error 0.6674(0.6849) Steps 508(506.96) | Grad Norm 5.0767(8.0303) | Total Time 14.00(14.00)\n",
      "Iter 0393 | Time 47.6348(47.1146) | Bit/dim 4.6475(4.7396) | Xent 1.8577(1.9398) | Loss 5.5763(5.7095) | Error 0.6545(0.6839) Steps 508(506.99) | Grad Norm 5.8653(7.9654) | Total Time 14.00(14.00)\n",
      "Iter 0394 | Time 48.7235(47.1628) | Bit/dim 4.6242(4.7362) | Xent 1.8413(1.9368) | Loss 5.5449(5.7046) | Error 0.6534(0.6830) Steps 508(507.02) | Grad Norm 3.8881(7.8431) | Total Time 14.00(14.00)\n",
      "Iter 0395 | Time 49.0738(47.2202) | Bit/dim 4.6387(4.7332) | Xent 1.8334(1.9337) | Loss 5.5553(5.7001) | Error 0.6516(0.6821) Steps 514(507.23) | Grad Norm 5.0179(7.7583) | Total Time 14.00(14.00)\n",
      "Iter 0396 | Time 49.0303(47.2745) | Bit/dim 4.5988(4.7292) | Xent 1.8216(1.9303) | Loss 5.5096(5.6944) | Error 0.6545(0.6813) Steps 508(507.25) | Grad Norm 4.1701(7.6507) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 22.6224, Epoch Time 327.5862(332.7875), Bit/dim 4.6077(best: 4.6282), Xent 1.7889, Loss 5.5021, Error 0.6346(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 48.4193(47.3088) | Bit/dim 4.6098(4.7256) | Xent 1.8170(1.9269) | Loss 5.5183(5.6891) | Error 0.6450(0.6802) Steps 514(507.45) | Grad Norm 5.4327(7.5841) | Total Time 14.00(14.00)\n",
      "Iter 0398 | Time 48.0568(47.3313) | Bit/dim 4.6032(4.7220) | Xent 1.7984(1.9231) | Loss 5.5024(5.6835) | Error 0.6327(0.6788) Steps 508(507.47) | Grad Norm 3.1444(7.4509) | Total Time 14.00(14.00)\n",
      "Iter 0399 | Time 48.6160(47.3698) | Bit/dim 4.5709(4.7174) | Xent 1.8180(1.9199) | Loss 5.4799(5.6774) | Error 0.6455(0.6778) Steps 508(507.48) | Grad Norm 4.2002(7.3534) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 48.1570(47.3934) | Bit/dim 4.5724(4.7131) | Xent 1.8084(1.9166) | Loss 5.4766(5.6714) | Error 0.6344(0.6765) Steps 508(507.50) | Grad Norm 5.6060(7.3010) | Total Time 14.00(14.00)\n",
      "Iter 0401 | Time 48.5403(47.4278) | Bit/dim 4.5706(4.7088) | Xent 1.8257(1.9139) | Loss 5.4834(5.6657) | Error 0.6486(0.6756) Steps 508(507.52) | Grad Norm 6.5513(7.2785) | Total Time 14.00(14.00)\n",
      "Iter 0402 | Time 47.5561(47.4317) | Bit/dim 4.5752(4.7048) | Xent 1.7843(1.9100) | Loss 5.4673(5.6598) | Error 0.6322(0.6743) Steps 508(507.53) | Grad Norm 7.3198(7.2797) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 22.7162, Epoch Time 327.5105(332.6292), Bit/dim 4.6320(best: 4.6077), Xent 1.7799, Loss 5.5219, Error 0.6386(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 47.4632(47.4326) | Bit/dim 4.6250(4.7024) | Xent 1.8181(1.9072) | Loss 5.5341(5.6560) | Error 0.6509(0.6736) Steps 514(507.72) | Grad Norm 13.7526(7.4739) | Total Time 14.00(14.00)\n",
      "Iter 0404 | Time 48.2664(47.4576) | Bit/dim 4.9002(4.7083) | Xent 1.9836(1.9095) | Loss 5.8920(5.6631) | Error 0.7116(0.6748) Steps 526(508.27) | Grad Norm 24.1858(7.9753) | Total Time 14.00(14.00)\n",
      "Iter 0405 | Time 47.0302(47.4448) | Bit/dim 4.8604(4.7129) | Xent 2.0716(1.9144) | Loss 5.8962(5.6701) | Error 0.7205(0.6761) Steps 514(508.44) | Grad Norm 14.7393(8.1782) | Total Time 14.00(14.00)\n",
      "Iter 0406 | Time 50.1353(47.5255) | Bit/dim 4.8714(4.7176) | Xent 2.0884(1.9196) | Loss 5.9156(5.6774) | Error 0.7268(0.6776) Steps 538(509.33) | Grad Norm 16.1220(8.4165) | Total Time 14.00(14.00)\n",
      "Iter 0407 | Time 46.4818(47.4942) | Bit/dim 4.7176(4.7176) | Xent 2.0907(1.9247) | Loss 5.7630(5.6800) | Error 0.7539(0.6799) Steps 526(509.83) | Grad Norm 8.7500(8.4265) | Total Time 14.00(14.00)\n",
      "Iter 0408 | Time 47.7655(47.5024) | Bit/dim 4.7277(4.7180) | Xent 2.0733(1.9292) | Loss 5.7644(5.6825) | Error 0.7468(0.6819) Steps 514(509.96) | Grad Norm 6.1651(8.3587) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.6717, Epoch Time 326.1987(332.4363), Bit/dim 4.7750(best: 4.6077), Xent 2.0355, Loss 5.7928, Error 0.7406(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 48.2248(47.5240) | Bit/dim 4.7671(4.7194) | Xent 2.0838(1.9338) | Loss 5.8089(5.6863) | Error 0.7465(0.6839) Steps 532(510.62) | Grad Norm 10.4646(8.4218) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 49.1237(47.5720) | Bit/dim 4.6996(4.7188) | Xent 2.0120(1.9362) | Loss 5.7056(5.6869) | Error 0.7147(0.6848) Steps 520(510.90) | Grad Norm 3.1652(8.2641) | Total Time 14.00(14.00)\n",
      "Iter 0411 | Time 48.3213(47.5945) | Bit/dim 4.6841(4.7178) | Xent 2.0207(1.9387) | Loss 5.6945(5.6871) | Error 0.7176(0.6858) Steps 520(511.17) | Grad Norm 5.6101(8.1845) | Total Time 14.00(14.00)\n",
      "Iter 0412 | Time 50.0364(47.6677) | Bit/dim 4.6817(4.7167) | Xent 1.9794(1.9399) | Loss 5.6714(5.6867) | Error 0.7086(0.6865) Steps 526(511.62) | Grad Norm 3.1195(8.0326) | Total Time 14.00(14.00)\n",
      "Iter 0413 | Time 47.5671(47.6647) | Bit/dim 4.6725(4.7154) | Xent 1.9838(1.9412) | Loss 5.6644(5.6860) | Error 0.7001(0.6869) Steps 526(512.05) | Grad Norm 5.0224(7.9423) | Total Time 14.00(14.00)\n",
      "Iter 0414 | Time 49.2931(47.7136) | Bit/dim 4.6320(4.7129) | Xent 1.9364(1.9411) | Loss 5.6002(5.6834) | Error 0.6897(0.6870) Steps 526(512.47) | Grad Norm 2.4122(7.7764) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 23.6067, Epoch Time 331.7578(332.4159), Bit/dim 4.6298(best: 4.6077), Xent 1.9287, Loss 5.5941, Error 0.6803(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 49.6329(47.7712) | Bit/dim 4.6318(4.7104) | Xent 1.9468(1.9413) | Loss 5.6052(5.6811) | Error 0.6927(0.6871) Steps 544(513.41) | Grad Norm 4.5413(7.6793) | Total Time 14.00(14.00)\n",
      "Iter 0416 | Time 48.8921(47.8048) | Bit/dim 4.6263(4.7079) | Xent 1.9302(1.9409) | Loss 5.5914(5.6784) | Error 0.6784(0.6869) Steps 544(514.33) | Grad Norm 2.4919(7.5237) | Total Time 14.00(14.00)\n",
      "Iter 0417 | Time 49.0914(47.8434) | Bit/dim 4.6153(4.7051) | Xent 1.9196(1.9403) | Loss 5.5751(5.6753) | Error 0.6686(0.6863) Steps 544(515.22) | Grad Norm 4.6572(7.4377) | Total Time 14.00(14.00)\n",
      "Iter 0418 | Time 50.2177(47.9146) | Bit/dim 4.6210(4.7026) | Xent 1.9003(1.9391) | Loss 5.5712(5.6722) | Error 0.6635(0.6856) Steps 538(515.90) | Grad Norm 3.2963(7.3135) | Total Time 14.00(14.00)\n",
      "Iter 0419 | Time 50.7503(47.9997) | Bit/dim 4.6006(4.6996) | Xent 1.8990(1.9379) | Loss 5.5501(5.6685) | Error 0.6741(0.6853) Steps 514(515.85) | Grad Norm 3.9689(7.2131) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 48.6425(48.0190) | Bit/dim 4.5993(4.6966) | Xent 1.8793(1.9361) | Loss 5.5390(5.6646) | Error 0.6630(0.6846) Steps 520(515.97) | Grad Norm 3.6357(7.1058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 22.4938, Epoch Time 335.0738(332.4957), Bit/dim 4.5799(best: 4.6077), Xent 1.8503, Loss 5.5051, Error 0.6507(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 48.4264(48.0312) | Bit/dim 4.5838(4.6932) | Xent 1.8795(1.9344) | Loss 5.5236(5.6604) | Error 0.6691(0.6842) Steps 520(516.09) | Grad Norm 3.6594(7.0024) | Total Time 14.00(14.00)\n",
      "Iter 0422 | Time 47.1731(48.0054) | Bit/dim 4.5734(4.6896) | Xent 1.9181(1.9339) | Loss 5.5324(5.6566) | Error 0.6766(0.6839) Steps 502(515.67) | Grad Norm 7.4214(7.0150) | Total Time 14.00(14.00)\n",
      "Iter 0423 | Time 48.2096(48.0116) | Bit/dim 4.5626(4.6858) | Xent 1.9358(1.9340) | Loss 5.5305(5.6528) | Error 0.6811(0.6839) Steps 514(515.62) | Grad Norm 8.1208(7.0481) | Total Time 14.00(14.00)\n",
      "Iter 0424 | Time 49.0099(48.0415) | Bit/dim 4.5670(4.6822) | Xent 1.8955(1.9329) | Loss 5.5148(5.6486) | Error 0.6719(0.6835) Steps 502(515.21) | Grad Norm 7.7532(7.0693) | Total Time 14.00(14.00)\n",
      "Iter 0425 | Time 46.6039(47.9984) | Bit/dim 4.5585(4.6785) | Xent 1.8726(1.9310) | Loss 5.4948(5.6440) | Error 0.6599(0.6828) Steps 496(514.63) | Grad Norm 7.1495(7.0717) | Total Time 14.00(14.00)\n",
      "Iter 0426 | Time 46.8474(47.9639) | Bit/dim 4.6339(4.6772) | Xent 1.8393(1.9283) | Loss 5.5536(5.6413) | Error 0.6382(0.6814) Steps 496(514.08) | Grad Norm 6.7426(7.0618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 22.6435, Epoch Time 324.5802(332.2582), Bit/dim 4.5440(best: 4.5799), Xent 1.8114, Loss 5.4497, Error 0.6357(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 46.0759(47.9072) | Bit/dim 4.5391(4.6730) | Xent 1.8520(1.9260) | Loss 5.4651(5.6360) | Error 0.6554(0.6807) Steps 502(513.71) | Grad Norm 3.9039(6.9671) | Total Time 14.00(14.00)\n",
      "Iter 0428 | Time 47.1643(47.8849) | Bit/dim 4.5420(4.6691) | Xent 1.8671(1.9242) | Loss 5.4755(5.6312) | Error 0.6550(0.6799) Steps 514(513.72) | Grad Norm 6.4501(6.9516) | Total Time 14.00(14.00)\n",
      "Iter 0429 | Time 49.2963(47.9273) | Bit/dim 4.6271(4.6678) | Xent 1.8455(1.9219) | Loss 5.5498(5.6288) | Error 0.6542(0.6791) Steps 496(513.19) | Grad Norm 6.5676(6.9401) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 49.0568(47.9612) | Bit/dim 4.5391(4.6640) | Xent 1.8390(1.9194) | Loss 5.4586(5.6237) | Error 0.6350(0.6778) Steps 496(512.67) | Grad Norm 3.8695(6.8479) | Total Time 14.00(14.00)\n",
      "Iter 0431 | Time 49.8801(48.0187) | Bit/dim 4.6675(4.6641) | Xent 1.8919(1.9186) | Loss 5.6135(5.6234) | Error 0.6701(0.6776) Steps 502(512.35) | Grad Norm 14.8290(7.0874) | Total Time 14.00(14.00)\n",
      "Iter 0432 | Time 45.5274(47.9440) | Bit/dim 5.0974(4.6771) | Xent 1.8424(1.9163) | Loss 6.0186(5.6352) | Error 0.6422(0.6765) Steps 496(511.86) | Grad Norm 11.6433(7.2241) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 21.8512, Epoch Time 324.7223(332.0321), Bit/dim 5.2181(best: 4.5440), Xent 1.8268, Loss 6.1315, Error 0.6469(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 43.0203(47.7963) | Bit/dim 5.2156(4.6932) | Xent 1.8405(1.9140) | Loss 6.1359(5.6502) | Error 0.6445(0.6756) Steps 490(511.21) | Grad Norm 9.8694(7.3034) | Total Time 14.00(14.00)\n",
      "Iter 0434 | Time 46.7722(47.7656) | Bit/dim 4.9276(4.7003) | Xent 1.8212(1.9112) | Loss 5.8382(5.6559) | Error 0.6436(0.6746) Steps 520(511.47) | Grad Norm 6.1942(7.2701) | Total Time 14.00(14.00)\n",
      "Iter 0435 | Time 49.5483(47.8190) | Bit/dim 4.7484(4.7017) | Xent 1.8629(1.9098) | Loss 5.6799(5.6566) | Error 0.6633(0.6743) Steps 538(512.27) | Grad Norm 8.5189(7.3076) | Total Time 14.00(14.00)\n",
      "Iter 0436 | Time 49.5900(47.8722) | Bit/dim 4.7926(4.7044) | Xent 2.0629(1.9144) | Loss 5.8241(5.6616) | Error 0.6992(0.6750) Steps 538(513.04) | Grad Norm 29.5920(7.9761) | Total Time 14.00(14.00)\n",
      "Iter 0437 | Time 46.5976(47.8339) | Bit/dim 5.2817(4.7217) | Xent 2.5284(1.9328) | Loss 6.5460(5.6881) | Error 0.8344(0.6798) Steps 550(514.15) | Grad Norm 26.2596(8.5246) | Total Time 14.00(14.00)\n",
      "Iter 0438 | Time 46.9672(47.8079) | Bit/dim 5.2543(4.7377) | Xent 2.3056(1.9440) | Loss 6.4071(5.7097) | Error 0.7921(0.6832) Steps 562(515.58) | Grad Norm 9.7669(8.5619) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 23.6322, Epoch Time 321.7821(331.7246), Bit/dim 5.1405(best: 4.5440), Xent 2.0761, Loss 6.1785, Error 0.7427(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 48.1500(47.8182) | Bit/dim 5.1413(4.7498) | Xent 2.0941(1.9485) | Loss 6.1883(5.7241) | Error 0.7472(0.6851) Steps 544(516.44) | Grad Norm 4.5523(8.4416) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 46.8861(47.7902) | Bit/dim 5.0777(4.7597) | Xent 2.1393(1.9542) | Loss 6.1474(5.7368) | Error 0.7478(0.6870) Steps 526(516.72) | Grad Norm 5.8980(8.3653) | Total Time 14.00(14.00)\n",
      "Iter 0441 | Time 48.3028(47.8056) | Bit/dim 5.0492(4.7684) | Xent 2.1723(1.9607) | Loss 6.1353(5.7487) | Error 0.7602(0.6892) Steps 544(517.54) | Grad Norm 7.5073(8.3396) | Total Time 14.00(14.00)\n",
      "Iter 0442 | Time 48.2969(47.8203) | Bit/dim 4.9508(4.7738) | Xent 2.0120(1.9623) | Loss 5.9568(5.7550) | Error 0.7075(0.6897) Steps 532(517.98) | Grad Norm 3.1616(8.1842) | Total Time 14.00(14.00)\n",
      "Iter 0443 | Time 49.1333(47.8597) | Bit/dim 4.8932(4.7774) | Xent 2.0279(1.9643) | Loss 5.9072(5.7595) | Error 0.7205(0.6906) Steps 544(518.76) | Grad Norm 4.7125(8.0801) | Total Time 14.00(14.00)\n",
      "Iter 0444 | Time 47.3843(47.8455) | Bit/dim 4.8783(4.7804) | Xent 1.9854(1.9649) | Loss 5.8710(5.7629) | Error 0.7046(0.6911) Steps 526(518.97) | Grad Norm 3.4693(7.9418) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 23.1090, Epoch Time 326.8730(331.5791), Bit/dim 4.8524(best: 4.5440), Xent 1.9514, Loss 5.8281, Error 0.6881(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 46.6868(47.8107) | Bit/dim 4.8548(4.7827) | Xent 1.9759(1.9652) | Loss 5.8427(5.7653) | Error 0.7037(0.6914) Steps 520(519.00) | Grad Norm 4.0156(7.8240) | Total Time 14.00(14.00)\n",
      "Iter 0446 | Time 46.6841(47.7769) | Bit/dim 4.8426(4.7845) | Xent 1.9685(1.9653) | Loss 5.8268(5.7671) | Error 0.6964(0.6916) Steps 508(518.67) | Grad Norm 3.9168(7.7068) | Total Time 14.00(14.00)\n",
      "Iter 0447 | Time 47.6492(47.7731) | Bit/dim 4.8167(4.7854) | Xent 1.9549(1.9650) | Loss 5.7942(5.7679) | Error 0.6934(0.6916) Steps 514(518.53) | Grad Norm 4.2463(7.6029) | Total Time 14.00(14.00)\n",
      "Iter 0448 | Time 46.8592(47.7457) | Bit/dim 4.7815(4.7853) | Xent 1.9261(1.9638) | Loss 5.7446(5.7672) | Error 0.6813(0.6913) Steps 514(518.40) | Grad Norm 2.5545(7.4515) | Total Time 14.00(14.00)\n",
      "Iter 0449 | Time 48.0654(47.7553) | Bit/dim 4.7283(4.7836) | Xent 1.9608(1.9637) | Loss 5.7087(5.7655) | Error 0.6886(0.6912) Steps 514(518.27) | Grad Norm 3.3784(7.3293) | Total Time 14.00(14.00)\n",
      "Iter 0450 | Time 49.0227(47.7933) | Bit/dim 4.7271(4.7819) | Xent 1.9631(1.9637) | Loss 5.7087(5.7638) | Error 0.6964(0.6914) Steps 520(518.32) | Grad Norm 4.7541(7.2520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.2723, Epoch Time 323.8574(331.3474), Bit/dim 4.7150(best: 4.5440), Xent 1.8807, Loss 5.6553, Error 0.6639(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 47.9457(47.7979) | Bit/dim 4.7102(4.7798) | Xent 1.9213(1.9624) | Loss 5.6709(5.7610) | Error 0.6851(0.6912) Steps 520(518.37) | Grad Norm 3.4946(7.1393) | Total Time 14.00(14.00)\n",
      "Iter 0452 | Time 50.2139(47.8703) | Bit/dim 4.6797(4.7768) | Xent 1.9150(1.9610) | Loss 5.6372(5.7573) | Error 0.6829(0.6910) Steps 526(518.60) | Grad Norm 3.5998(7.0331) | Total Time 14.00(14.00)\n",
      "Iter 0453 | Time 48.9197(47.9018) | Bit/dim 4.6586(4.7732) | Xent 1.9238(1.9599) | Loss 5.6205(5.7532) | Error 0.6780(0.6906) Steps 532(519.00) | Grad Norm 5.2003(6.9781) | Total Time 14.00(14.00)\n",
      "Iter 0454 | Time 50.3526(47.9753) | Bit/dim 4.6628(4.7699) | Xent 1.9279(1.9589) | Loss 5.6268(5.7494) | Error 0.6896(0.6905) Steps 532(519.39) | Grad Norm 8.0843(7.0113) | Total Time 14.00(14.00)\n",
      "Iter 0455 | Time 49.7002(48.0271) | Bit/dim 4.6944(4.7676) | Xent 2.0421(1.9614) | Loss 5.7155(5.7484) | Error 0.7037(0.6909) Steps 526(519.59) | Grad Norm 16.5884(7.2986) | Total Time 14.00(14.00)\n",
      "Iter 0456 | Time 51.7021(48.1373) | Bit/dim 4.7665(4.7676) | Xent 2.5346(1.9786) | Loss 6.0338(5.7569) | Error 0.8014(0.6943) Steps 544(520.32) | Grad Norm 33.3266(8.0795) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 23.1776, Epoch Time 337.5634(331.5339), Bit/dim 5.0808(best: 4.5440), Xent 2.5507, Loss 6.3561, Error 0.8085(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 46.9409(48.1014) | Bit/dim 5.0814(4.7770) | Xent 2.6180(1.9978) | Loss 6.3904(5.7759) | Error 0.8109(0.6977) Steps 508(519.95) | Grad Norm 22.4259(8.5099) | Total Time 14.00(14.00)\n",
      "Iter 0458 | Time 48.9242(48.1261) | Bit/dim 5.1018(4.7868) | Xent 3.5278(2.0437) | Loss 6.8657(5.8086) | Error 0.8960(0.7037) Steps 550(520.85) | Grad Norm 61.6704(10.1047) | Total Time 14.00(14.00)\n",
      "Iter 0459 | Time 48.1680(48.1274) | Bit/dim 5.9313(4.8211) | Xent 9.9367(2.2805) | Loss 10.8997(5.9614) | Error 0.9006(0.7096) Steps 562(522.09) | Grad Norm 85.6824(12.3720) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 43.8699(47.9997) | Bit/dim 6.0343(4.8575) | Xent 6.3008(2.4011) | Loss 9.1847(6.0581) | Error 0.8700(0.7144) Steps 496(521.30) | Grad Norm 48.7921(13.4646) | Total Time 14.00(14.00)\n",
      "Iter 0461 | Time 44.8575(47.9054) | Bit/dim 6.4182(4.9043) | Xent 5.7702(2.5022) | Loss 9.3033(6.1554) | Error 0.9004(0.7200) Steps 520(521.26) | Grad Norm 40.9894(14.2904) | Total Time 14.00(14.00)\n",
      "Iter 0462 | Time 45.4254(47.8310) | Bit/dim 6.4832(4.9517) | Xent 4.2223(2.5538) | Loss 8.5944(6.2286) | Error 0.8177(0.7229) Steps 532(521.59) | Grad Norm 15.4945(14.3265) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 26.6755, Epoch Time 320.1900(331.1936), Bit/dim 6.3963(best: 4.5440), Xent 4.4593, Loss 8.6260, Error 0.7999(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 48.0253(47.8368) | Bit/dim 6.3936(4.9949) | Xent 4.3760(2.6085) | Loss 8.5815(6.2992) | Error 0.7909(0.7250) Steps 586(523.52) | Grad Norm 19.4694(14.4808) | Total Time 14.00(14.00)\n",
      "Iter 0464 | Time 46.5171(47.7972) | Bit/dim 6.3196(5.0347) | Xent 10.0456(2.8316) | Loss 11.3424(6.4505) | Error 0.8811(0.7297) Steps 562(524.67) | Grad Norm 150.9304(18.5743) | Total Time 14.00(14.00)\n",
      "Iter 0465 | Time 48.8076(47.8275) | Bit/dim 7.0323(5.0946) | Xent 11.4858(3.0912) | Loss 12.7752(6.6402) | Error 0.8796(0.7342) Steps 586(526.51) | Grad Norm 40.5758(19.2343) | Total Time 14.00(14.00)\n",
      "Iter 0466 | Time 46.3077(47.7819) | Bit/dim 7.3733(5.1630) | Xent 9.3296(3.2784) | Loss 12.0381(6.8021) | Error 0.8944(0.7390) Steps 550(527.22) | Grad Norm 23.4799(19.3617) | Total Time 14.00(14.00)\n",
      "Iter 0467 | Time 46.6491(47.7480) | Bit/dim 7.3176(5.2276) | Xent 6.7727(3.3832) | Loss 10.7039(6.9192) | Error 0.8618(0.7426) Steps 574(528.62) | Grad Norm 14.4923(19.2156) | Total Time 14.00(14.00)\n",
      "Iter 0468 | Time 49.0435(47.7868) | Bit/dim 7.2676(5.2888) | Xent 6.0867(3.4643) | Loss 10.3109(7.0209) | Error 0.8710(0.7465) Steps 598(530.70) | Grad Norm 25.1015(19.3922) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 26.4611, Epoch Time 327.3175(331.0773), Bit/dim 7.1371(best: 4.5440), Xent 4.2320, Loss 9.2531, Error 0.8449(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 49.9287(47.8511) | Bit/dim 7.1366(5.3442) | Xent 4.2715(3.4885) | Loss 9.2723(7.0885) | Error 0.8395(0.7493) Steps 592(532.54) | Grad Norm 6.5265(19.0062) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 48.5767(47.8729) | Bit/dim 7.1061(5.3971) | Xent 3.9663(3.5028) | Loss 9.0892(7.1485) | Error 0.8611(0.7526) Steps 598(534.51) | Grad Norm 9.9216(18.7337) | Total Time 14.00(14.00)\n",
      "Iter 0471 | Time 47.3693(47.8577) | Bit/dim 7.1099(5.4485) | Xent 2.7534(3.4804) | Loss 8.4866(7.1887) | Error 0.8466(0.7555) Steps 598(536.41) | Grad Norm 10.7427(18.4939) | Total Time 14.00(14.00)\n",
      "Iter 0472 | Time 49.2366(47.8991) | Bit/dim 7.1145(5.4985) | Xent 4.7813(3.5194) | Loss 9.5051(7.2581) | Error 0.8413(0.7580) Steps 586(537.90) | Grad Norm 17.5705(18.4662) | Total Time 14.00(14.00)\n",
      "Iter 0473 | Time 48.3387(47.9123) | Bit/dim 7.1206(5.5471) | Xent 4.2132(3.5402) | Loss 9.2271(7.3172) | Error 0.8810(0.7617) Steps 574(538.98) | Grad Norm 15.2203(18.3688) | Total Time 14.00(14.00)\n",
      "Iter 0474 | Time 49.1417(47.9492) | Bit/dim 7.1366(5.5948) | Xent 4.0009(3.5540) | Loss 9.1371(7.3718) | Error 0.8669(0.7649) Steps 568(539.85) | Grad Norm 11.4262(18.1606) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 26.1545, Epoch Time 334.4327(331.1780), Bit/dim 7.1121(best: 4.5440), Xent 3.6015, Loss 8.9129, Error 0.8564(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 49.4190(47.9933) | Bit/dim 7.1147(5.6404) | Xent 3.6531(3.5570) | Loss 8.9413(7.4189) | Error 0.8559(0.7676) Steps 580(541.06) | Grad Norm 6.9710(17.8249) | Total Time 14.00(14.00)\n",
      "Iter 0476 | Time 48.2736(48.0017) | Bit/dim 7.0744(5.6834) | Xent 3.4433(3.5536) | Loss 8.7961(7.4602) | Error 0.8461(0.7700) Steps 556(541.50) | Grad Norm 6.6284(17.4890) | Total Time 14.00(14.00)\n",
      "Iter 0477 | Time 46.9871(47.9712) | Bit/dim 7.0517(5.7245) | Xent 2.9587(3.5357) | Loss 8.5311(7.4923) | Error 0.8704(0.7730) Steps 538(541.40) | Grad Norm 11.1645(17.2993) | Total Time 14.00(14.00)\n",
      "Iter 0478 | Time 46.3207(47.9217) | Bit/dim 7.0459(5.7641) | Xent 2.4364(3.5028) | Loss 8.2641(7.5155) | Error 0.8263(0.7746) Steps 532(541.12) | Grad Norm 3.8150(16.8947) | Total Time 14.00(14.00)\n",
      "Iter 0479 | Time 46.5157(47.8796) | Bit/dim 7.0247(5.8019) | Xent 2.9672(3.4867) | Loss 8.5083(7.5453) | Error 0.8554(0.7770) Steps 538(541.02) | Grad Norm 9.7560(16.6806) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 45.5297(47.8091) | Bit/dim 6.9873(5.8375) | Xent 2.6906(3.4628) | Loss 8.3326(7.5689) | Error 0.8339(0.7787) Steps 532(540.75) | Grad Norm 5.6961(16.3510) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 23.9536, Epoch Time 322.6574(330.9224), Bit/dim 6.9603(best: 4.5440), Xent 2.3504, Loss 8.1355, Error 0.7771(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 46.3993(47.7668) | Bit/dim 6.9569(5.8711) | Xent 2.4009(3.4309) | Loss 8.1574(7.5866) | Error 0.7901(0.7790) Steps 520(540.13) | Grad Norm 2.5996(15.9385) | Total Time 14.00(14.00)\n",
      "Iter 0482 | Time 46.0502(47.7153) | Bit/dim 6.9321(5.9029) | Xent 2.5569(3.4047) | Loss 8.2106(7.6053) | Error 0.8335(0.7807) Steps 502(538.99) | Grad Norm 6.1448(15.6447) | Total Time 14.00(14.00)\n",
      "Iter 0483 | Time 46.5668(47.6808) | Bit/dim 6.9083(5.9331) | Xent 2.4971(3.3775) | Loss 8.1568(7.6218) | Error 0.8124(0.7816) Steps 508(538.06) | Grad Norm 3.6049(15.2835) | Total Time 14.00(14.00)\n",
      "Iter 0484 | Time 45.4891(47.6151) | Bit/dim 6.8743(5.9613) | Xent 2.5015(3.3512) | Loss 8.1251(7.6369) | Error 0.7928(0.7820) Steps 514(537.34) | Grad Norm 3.3541(14.9256) | Total Time 14.00(14.00)\n",
      "Iter 0485 | Time 45.9270(47.5644) | Bit/dim 6.8440(5.9878) | Xent 2.4137(3.3231) | Loss 8.0509(7.6493) | Error 0.7851(0.7821) Steps 514(536.64) | Grad Norm 2.7977(14.5618) | Total Time 14.00(14.00)\n",
      "Iter 0486 | Time 46.6454(47.5368) | Bit/dim 6.8137(6.0126) | Xent 2.2937(3.2922) | Loss 7.9606(7.6587) | Error 0.7754(0.7819) Steps 526(536.32) | Grad Norm 1.4612(14.1687) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 24.0084, Epoch Time 316.6603(330.4945), Bit/dim 6.7897(best: 4.5440), Xent 2.2465, Loss 7.9129, Error 0.7581(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 45.5761(47.4780) | Bit/dim 6.7909(6.0359) | Xent 2.2991(3.2624) | Loss 7.9404(7.6671) | Error 0.7688(0.7815) Steps 520(535.83) | Grad Norm 2.2007(13.8097) | Total Time 14.00(14.00)\n",
      "Iter 0488 | Time 45.9821(47.4331) | Bit/dim 6.7543(6.0575) | Xent 2.2969(3.2335) | Loss 7.9027(7.6742) | Error 0.7735(0.7812) Steps 520(535.35) | Grad Norm 2.5755(13.4727) | Total Time 14.00(14.00)\n",
      "Iter 0489 | Time 45.6653(47.3801) | Bit/dim 6.7071(6.0770) | Xent 2.2659(3.2044) | Loss 7.8401(7.6792) | Error 0.7848(0.7813) Steps 526(535.07) | Grad Norm 1.5377(13.1146) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 46.1507(47.3432) | Bit/dim 6.6655(6.0946) | Xent 2.2558(3.1760) | Loss 7.7934(7.6826) | Error 0.7899(0.7816) Steps 508(534.26) | Grad Norm 3.3832(12.8227) | Total Time 14.00(14.00)\n",
      "Iter 0491 | Time 45.2545(47.2806) | Bit/dim 6.6112(6.1101) | Xent 2.2648(3.1486) | Loss 7.7436(7.6844) | Error 0.7835(0.7816) Steps 508(533.47) | Grad Norm 3.7434(12.5503) | Total Time 14.00(14.00)\n",
      "Iter 0492 | Time 46.0205(47.2428) | Bit/dim 6.5619(6.1237) | Xent 2.2323(3.1211) | Loss 7.6781(7.6842) | Error 0.7751(0.7814) Steps 508(532.71) | Grad Norm 1.7621(12.2267) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 23.1674, Epoch Time 313.2775(329.9780), Bit/dim 6.5292(best: 4.5440), Xent 2.2326, Loss 7.6455, Error 0.7776(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 45.7948(47.1993) | Bit/dim 6.5243(6.1357) | Xent 2.2672(3.0955) | Loss 7.6579(7.6834) | Error 0.7883(0.7817) Steps 502(531.79) | Grad Norm 5.1045(12.0130) | Total Time 14.00(14.00)\n",
      "Iter 0494 | Time 44.5522(47.1199) | Bit/dim 6.4923(6.1464) | Xent 2.1842(3.0682) | Loss 7.5844(7.6805) | Error 0.7690(0.7813) Steps 496(530.71) | Grad Norm 1.2650(11.6906) | Total Time 14.00(14.00)\n",
      "Iter 0495 | Time 45.0555(47.0580) | Bit/dim 6.4459(6.1554) | Xent 2.1803(3.0416) | Loss 7.5360(7.6761) | Error 0.7631(0.7807) Steps 514(530.21) | Grad Norm 3.2628(11.4377) | Total Time 14.00(14.00)\n",
      "Iter 0496 | Time 45.0611(46.9981) | Bit/dim 6.3765(6.1620) | Xent 2.1506(3.0148) | Loss 7.4518(7.6694) | Error 0.7481(0.7798) Steps 514(529.72) | Grad Norm 2.5765(11.1719) | Total Time 14.00(14.00)\n",
      "Iter 0497 | Time 45.5674(46.9552) | Bit/dim 6.3157(6.1666) | Xent 2.1734(2.9896) | Loss 7.4024(7.6614) | Error 0.7551(0.7790) Steps 514(529.25) | Grad Norm 2.4765(10.9110) | Total Time 14.00(14.00)\n",
      "Iter 0498 | Time 46.4311(46.9394) | Bit/dim 6.2445(6.1689) | Xent 2.1589(2.9647) | Loss 7.3240(7.6513) | Error 0.7596(0.7784) Steps 514(528.80) | Grad Norm 2.7673(10.6667) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 24.1332, Epoch Time 311.9863(329.4382), Bit/dim 6.1906(best: 4.5440), Xent 2.1158, Loss 7.2485, Error 0.7410(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 47.3398(46.9514) | Bit/dim 6.1861(6.1695) | Xent 2.1468(2.9401) | Loss 7.2595(7.6395) | Error 0.7516(0.7776) Steps 520(528.53) | Grad Norm 2.6466(10.4261) | Total Time 14.00(14.00)\n",
      "Iter 0500 | Time 47.6517(46.9724) | Bit/dim 6.1182(6.1679) | Xent 2.1723(2.9171) | Loss 7.2044(7.6265) | Error 0.7611(0.7771) Steps 514(528.10) | Grad Norm 2.5572(10.1900) | Total Time 14.00(14.00)\n",
      "Iter 0501 | Time 48.1758(47.0085) | Bit/dim 6.0575(6.1646) | Xent 2.1488(2.8940) | Loss 7.1319(7.6116) | Error 0.7646(0.7768) Steps 520(527.85) | Grad Norm 3.8785(10.0007) | Total Time 14.00(14.00)\n",
      "Iter 0502 | Time 46.3913(46.9900) | Bit/dim 6.0059(6.1598) | Xent 2.1245(2.8710) | Loss 7.0682(7.5953) | Error 0.7526(0.7760) Steps 502(527.08) | Grad Norm 2.1764(9.7660) | Total Time 14.00(14.00)\n",
      "Iter 0503 | Time 50.4722(47.0945) | Bit/dim 6.0214(6.1557) | Xent 2.1310(2.8488) | Loss 7.0869(7.5801) | Error 0.7521(0.7753) Steps 550(527.76) | Grad Norm 5.3323(9.6330) | Total Time 14.00(14.00)\n",
      "Iter 0504 | Time 47.3310(47.1016) | Bit/dim 6.0974(6.1539) | Xent 2.1365(2.8274) | Loss 7.1657(7.5676) | Error 0.7578(0.7748) Steps 520(527.53) | Grad Norm 9.2505(9.6215) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 24.2081, Epoch Time 327.0487(329.3665), Bit/dim 5.8813(best: 4.5440), Xent 2.2634, Loss 7.0130, Error 0.8219(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 48.0207(47.1292) | Bit/dim 5.8801(6.1457) | Xent 2.3252(2.8123) | Loss 7.0427(7.5519) | Error 0.8191(0.7761) Steps 526(527.49) | Grad Norm 19.4156(9.9153) | Total Time 14.00(14.00)\n",
      "Iter 0506 | Time 52.6887(47.2959) | Bit/dim 5.9403(6.1396) | Xent 2.7462(2.8103) | Loss 7.3133(7.5447) | Error 0.8712(0.7790) Steps 574(528.88) | Grad Norm 31.5400(10.5640) | Total Time 14.00(14.00)\n",
      "Iter 0507 | Time 50.2527(47.3846) | Bit/dim 5.8868(6.1320) | Xent 2.1929(2.7918) | Loss 6.9833(7.5279) | Error 0.8025(0.7797) Steps 562(529.87) | Grad Norm 11.6888(10.5978) | Total Time 14.00(14.00)\n",
      "Iter 0508 | Time 50.6953(47.4840) | Bit/dim 5.8778(6.1244) | Xent 2.2982(2.7770) | Loss 7.0269(7.5129) | Error 0.8171(0.7808) Steps 562(530.84) | Grad Norm 14.2424(10.7071) | Total Time 14.00(14.00)\n",
      "Iter 0509 | Time 51.2545(47.5971) | Bit/dim 5.8094(6.1149) | Xent 2.1790(2.7591) | Loss 6.8989(7.4944) | Error 0.7403(0.7796) Steps 562(531.77) | Grad Norm 3.4551(10.4896) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 54.9730(47.8184) | Bit/dim 5.8006(6.1055) | Xent 2.3441(2.7466) | Loss 6.9726(7.4788) | Error 0.7884(0.7798) Steps 580(533.22) | Grad Norm 7.5314(10.4008) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 26.6808, Epoch Time 350.1294(329.9894), Bit/dim 5.7138(best: 4.5440), Xent 2.2246, Loss 6.8261, Error 0.7816(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 55.9372(48.0619) | Bit/dim 5.7150(6.0938) | Xent 2.2577(2.7320) | Loss 6.8438(7.4597) | Error 0.7844(0.7800) Steps 610(535.52) | Grad Norm 5.0190(10.2394) | Total Time 14.00(14.00)\n",
      "Iter 0512 | Time 52.9431(48.2084) | Bit/dim 5.6308(6.0799) | Xent 2.1384(2.7141) | Loss 6.7000(7.4369) | Error 0.7501(0.7791) Steps 592(537.22) | Grad Norm 1.5816(9.9796) | Total Time 14.00(14.00)\n",
      "Iter 0513 | Time 51.6566(48.3118) | Bit/dim 5.6236(6.0662) | Xent 2.2232(2.6994) | Loss 6.7352(7.4159) | Error 0.8061(0.7799) Steps 580(538.50) | Grad Norm 6.1727(9.8654) | Total Time 14.00(14.00)\n",
      "Iter 0514 | Time 51.0715(48.3946) | Bit/dim 5.5571(6.0509) | Xent 2.2221(2.6851) | Loss 6.6681(7.3935) | Error 0.7970(0.7804) Steps 580(539.75) | Grad Norm 5.2911(9.7282) | Total Time 14.00(14.00)\n",
      "Iter 0515 | Time 52.8036(48.5269) | Bit/dim 5.5348(6.0354) | Xent 2.1248(2.6683) | Loss 6.5972(7.3696) | Error 0.7525(0.7796) Steps 610(541.85) | Grad Norm 1.1937(9.4722) | Total Time 14.00(14.00)\n",
      "Iter 0516 | Time 52.6792(48.6514) | Bit/dim 5.5000(6.0194) | Xent 2.1456(2.6526) | Loss 6.5729(7.3457) | Error 0.7622(0.7791) Steps 610(543.90) | Grad Norm 3.7169(9.2995) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 27.2047, Epoch Time 359.7971(330.8837), Bit/dim 5.4754(best: 4.5440), Xent 2.1393, Loss 6.5450, Error 0.7657(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 52.7411(48.7741) | Bit/dim 5.4659(6.0028) | Xent 2.1760(2.6383) | Loss 6.5539(7.3219) | Error 0.7825(0.7792) Steps 604(545.70) | Grad Norm 4.5890(9.1582) | Total Time 14.00(14.00)\n",
      "Iter 0518 | Time 52.2473(48.8783) | Bit/dim 5.4379(5.9858) | Xent 2.1243(2.6229) | Loss 6.5000(7.2973) | Error 0.7611(0.7786) Steps 586(546.91) | Grad Norm 2.1165(8.9469) | Total Time 14.00(14.00)\n",
      "Iter 0519 | Time 51.2942(48.9508) | Bit/dim 5.4013(5.9683) | Xent 2.1282(2.6081) | Loss 6.4654(7.2723) | Error 0.7739(0.7785) Steps 598(548.44) | Grad Norm 1.3471(8.7189) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 53.3351(49.0823) | Bit/dim 5.3651(5.9502) | Xent 2.1416(2.5941) | Loss 6.4359(7.2472) | Error 0.7789(0.7785) Steps 580(549.39) | Grad Norm 2.1224(8.5210) | Total Time 14.00(14.00)\n",
      "Iter 0521 | Time 52.5038(49.1850) | Bit/dim 5.3313(5.9316) | Xent 2.1482(2.5807) | Loss 6.4054(7.2220) | Error 0.7590(0.7779) Steps 568(549.95) | Grad Norm 2.1068(8.3286) | Total Time 14.00(14.00)\n",
      "Iter 0522 | Time 51.5426(49.2557) | Bit/dim 5.2956(5.9125) | Xent 2.1395(2.5675) | Loss 6.3654(7.1963) | Error 0.7556(0.7772) Steps 562(550.31) | Grad Norm 1.6558(8.1284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 25.7639, Epoch Time 355.1082(331.6104), Bit/dim 5.2577(best: 4.5440), Xent 2.0986, Loss 6.3070, Error 0.7405(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 53.7258(49.3898) | Bit/dim 5.2567(5.8929) | Xent 2.1457(2.5548) | Loss 6.3295(7.1703) | Error 0.7545(0.7766) Steps 574(551.02) | Grad Norm 1.4100(7.9269) | Total Time 14.00(14.00)\n",
      "Iter 0524 | Time 53.2293(49.5050) | Bit/dim 5.2539(5.8737) | Xent 2.1120(2.5415) | Loss 6.3099(7.1444) | Error 0.7474(0.7757) Steps 586(552.07) | Grad Norm 1.2858(7.7276) | Total Time 14.00(14.00)\n",
      "Iter 0525 | Time 53.6951(49.6307) | Bit/dim 5.2284(5.8543) | Xent 2.1160(2.5287) | Loss 6.2864(7.1187) | Error 0.7560(0.7751) Steps 586(553.09) | Grad Norm 1.5886(7.5435) | Total Time 14.00(14.00)\n",
      "Iter 0526 | Time 52.3125(49.7112) | Bit/dim 5.2118(5.8351) | Xent 2.1028(2.5160) | Loss 6.2632(7.0930) | Error 0.7481(0.7743) Steps 574(553.72) | Grad Norm 1.8091(7.3714) | Total Time 14.00(14.00)\n",
      "Iter 0527 | Time 53.2740(49.8180) | Bit/dim 5.1711(5.8151) | Xent 2.1188(2.5041) | Loss 6.2305(7.0672) | Error 0.7496(0.7735) Steps 568(554.14) | Grad Norm 1.4384(7.1935) | Total Time 14.00(14.00)\n",
      "Iter 0528 | Time 50.6977(49.8444) | Bit/dim 5.1466(5.7951) | Xent 2.1032(2.4920) | Loss 6.1982(7.0411) | Error 0.7455(0.7727) Steps 562(554.38) | Grad Norm 1.0643(7.0096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 26.2198, Epoch Time 358.7347(332.4241), Bit/dim 5.1215(best: 4.5440), Xent 2.0672, Loss 6.1551, Error 0.7212(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 51.7814(49.9025) | Bit/dim 5.1167(5.7747) | Xent 2.1085(2.4805) | Loss 6.1710(7.0150) | Error 0.7449(0.7719) Steps 562(554.61) | Grad Norm 1.2331(6.8363) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 51.5454(49.9518) | Bit/dim 5.0886(5.7541) | Xent 2.0913(2.4689) | Loss 6.1343(6.9886) | Error 0.7363(0.7708) Steps 568(555.01) | Grad Norm 1.3805(6.6726) | Total Time 14.00(14.00)\n",
      "Iter 0531 | Time 49.7350(49.9453) | Bit/dim 5.0932(5.7343) | Xent 2.0831(2.4573) | Loss 6.1348(6.9630) | Error 0.7263(0.7695) Steps 556(555.04) | Grad Norm 1.6219(6.5211) | Total Time 14.00(14.00)\n",
      "Iter 0532 | Time 53.7934(50.0608) | Bit/dim 5.0620(5.7142) | Xent 2.0723(2.4457) | Loss 6.0982(6.9370) | Error 0.7390(0.7685) Steps 580(555.79) | Grad Norm 1.1114(6.3588) | Total Time 14.00(14.00)\n",
      "Iter 0533 | Time 54.3464(50.1893) | Bit/dim 5.0501(5.6942) | Xent 2.0757(2.4346) | Loss 6.0879(6.9115) | Error 0.7421(0.7678) Steps 580(556.51) | Grad Norm 1.4610(6.2119) | Total Time 14.00(14.00)\n",
      "Iter 0534 | Time 53.5784(50.2910) | Bit/dim 5.0352(5.6745) | Xent 2.0702(2.4237) | Loss 6.0703(6.8863) | Error 0.7252(0.7665) Steps 580(557.22) | Grad Norm 1.1912(6.0612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 26.1228, Epoch Time 356.7316(333.1533), Bit/dim 5.0113(best: 4.5440), Xent 2.0326, Loss 6.0276, Error 0.7037(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 53.3427(50.3826) | Bit/dim 5.0118(5.6546) | Xent 2.0612(2.4128) | Loss 6.0424(6.8610) | Error 0.7261(0.7653) Steps 580(557.90) | Grad Norm 0.7803(5.9028) | Total Time 14.00(14.00)\n",
      "Iter 0536 | Time 52.1862(50.4367) | Bit/dim 4.9975(5.6349) | Xent 2.0640(2.4024) | Loss 6.0295(6.8360) | Error 0.7326(0.7643) Steps 586(558.75) | Grad Norm 1.3620(5.7666) | Total Time 14.00(14.00)\n",
      "Iter 0537 | Time 51.3886(50.4652) | Bit/dim 5.0003(5.6158) | Xent 2.0469(2.3917) | Loss 6.0238(6.8117) | Error 0.7196(0.7629) Steps 574(559.20) | Grad Norm 1.8155(5.6481) | Total Time 14.00(14.00)\n",
      "Iter 0538 | Time 53.8616(50.5671) | Bit/dim 4.9839(5.5969) | Xent 2.0557(2.3816) | Loss 6.0118(6.7877) | Error 0.7263(0.7618) Steps 592(560.19) | Grad Norm 0.8959(5.5055) | Total Time 14.00(14.00)\n",
      "Iter 0539 | Time 53.6379(50.6592) | Bit/dim 4.9708(5.5781) | Xent 2.0653(2.3721) | Loss 6.0034(6.7642) | Error 0.7285(0.7608) Steps 598(561.32) | Grad Norm 1.6216(5.3890) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 54.4670(50.7735) | Bit/dim 4.9554(5.5594) | Xent 2.0354(2.3620) | Loss 5.9731(6.7404) | Error 0.7225(0.7597) Steps 592(562.24) | Grad Norm 1.3500(5.2678) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 26.3266, Epoch Time 361.0293(333.9896), Bit/dim 4.9363(best: 4.5440), Xent 2.0015, Loss 5.9370, Error 0.6929(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 53.0042(50.8404) | Bit/dim 4.9293(5.5405) | Xent 2.0294(2.3520) | Loss 5.9440(6.7165) | Error 0.7136(0.7583) Steps 592(563.13) | Grad Norm 0.9550(5.1384) | Total Time 14.00(14.00)\n",
      "Iter 0542 | Time 55.3214(50.9748) | Bit/dim 4.9296(5.5222) | Xent 2.0198(2.3421) | Loss 5.9396(6.6932) | Error 0.7164(0.7571) Steps 598(564.18) | Grad Norm 1.4475(5.0277) | Total Time 14.00(14.00)\n",
      "Iter 0543 | Time 53.1981(51.0415) | Bit/dim 4.9264(5.5043) | Xent 2.0149(2.3323) | Loss 5.9339(6.6704) | Error 0.7084(0.7556) Steps 580(564.66) | Grad Norm 1.1814(4.9123) | Total Time 14.00(14.00)\n",
      "Iter 0544 | Time 53.4510(51.1138) | Bit/dim 4.9187(5.4867) | Xent 2.0405(2.3235) | Loss 5.9389(6.6485) | Error 0.7155(0.7544) Steps 580(565.12) | Grad Norm 1.6730(4.8151) | Total Time 14.00(14.00)\n",
      "Iter 0545 | Time 55.2321(51.2374) | Bit/dim 4.8953(5.4690) | Xent 2.0288(2.3147) | Loss 5.9098(6.6263) | Error 0.7096(0.7530) Steps 598(566.10) | Grad Norm 1.5939(4.7185) | Total Time 14.00(14.00)\n",
      "Iter 0546 | Time 54.2502(51.3277) | Bit/dim 4.9010(5.4520) | Xent 2.0199(2.3058) | Loss 5.9110(6.6049) | Error 0.7129(0.7518) Steps 598(567.06) | Grad Norm 2.0286(4.6378) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 25.9916, Epoch Time 366.0983(334.9529), Bit/dim 4.9166(best: 4.5440), Xent 1.9784, Loss 5.9058, Error 0.6813(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 54.1209(51.4115) | Bit/dim 4.9154(5.4359) | Xent 1.9924(2.2964) | Loss 5.9117(6.5841) | Error 0.7025(0.7504) Steps 580(567.45) | Grad Norm 2.4451(4.5720) | Total Time 14.00(14.00)\n",
      "Iter 0548 | Time 53.9321(51.4872) | Bit/dim 4.8770(5.4191) | Xent 2.0120(2.2879) | Loss 5.8830(6.5630) | Error 0.7031(0.7489) Steps 598(568.36) | Grad Norm 1.4612(4.4787) | Total Time 14.00(14.00)\n",
      "Iter 0549 | Time 57.0286(51.6534) | Bit/dim 4.8855(5.4031) | Xent 2.0060(2.2794) | Loss 5.8885(6.5428) | Error 0.6990(0.7474) Steps 604(569.43) | Grad Norm 2.9899(4.4340) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 55.0244(51.7545) | Bit/dim 4.8958(5.3879) | Xent 2.0109(2.2714) | Loss 5.9012(6.5236) | Error 0.7111(0.7464) Steps 580(569.75) | Grad Norm 3.9907(4.4207) | Total Time 14.00(14.00)\n",
      "Iter 0551 | Time 53.8012(51.8159) | Bit/dim 4.8564(5.3719) | Xent 1.9813(2.2627) | Loss 5.8470(6.5033) | Error 0.7003(0.7450) Steps 598(570.60) | Grad Norm 1.2037(4.3242) | Total Time 14.00(14.00)\n",
      "Iter 0552 | Time 57.6151(51.9899) | Bit/dim 4.8701(5.3569) | Xent 2.0245(2.2555) | Loss 5.8823(6.4846) | Error 0.7155(0.7441) Steps 604(571.60) | Grad Norm 4.3390(4.3247) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 26.1107, Epoch Time 373.1447(336.0986), Bit/dim 4.8999(best: 4.5440), Xent 1.9636, Loss 5.8817, Error 0.6790(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 52.6367(52.0093) | Bit/dim 4.8971(5.3431) | Xent 1.9919(2.2476) | Loss 5.8931(6.4669) | Error 0.6992(0.7427) Steps 580(571.85) | Grad Norm 3.6834(4.3054) | Total Time 14.00(14.00)\n",
      "Iter 0554 | Time 54.9147(52.0965) | Bit/dim 4.8347(5.3278) | Xent 2.0012(2.2402) | Loss 5.8353(6.4479) | Error 0.7024(0.7415) Steps 598(572.64) | Grad Norm 2.0608(4.2381) | Total Time 14.00(14.00)\n",
      "Iter 0555 | Time 56.4804(52.2280) | Bit/dim 4.8504(5.3135) | Xent 2.0092(2.2333) | Loss 5.8550(6.4302) | Error 0.7047(0.7404) Steps 604(573.58) | Grad Norm 4.8452(4.2563) | Total Time 14.00(14.00)\n",
      "Iter 0556 | Time 55.0617(52.3130) | Bit/dim 4.8732(5.3003) | Xent 1.9722(2.2255) | Loss 5.8593(6.4130) | Error 0.6904(0.7389) Steps 586(573.95) | Grad Norm 2.6035(4.2067) | Total Time 14.00(14.00)\n",
      "Iter 0557 | Time 56.6703(52.4437) | Bit/dim 4.8132(5.2857) | Xent 1.9918(2.2185) | Loss 5.8091(6.3949) | Error 0.6985(0.7377) Steps 580(574.13) | Grad Norm 3.0536(4.1721) | Total Time 14.00(14.00)\n",
      "Iter 0558 | Time 55.1315(52.5243) | Bit/dim 4.8367(5.2722) | Xent 2.0148(2.2123) | Loss 5.8441(6.3784) | Error 0.6987(0.7365) Steps 604(575.03) | Grad Norm 4.4529(4.1805) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 26.6642, Epoch Time 373.5647(337.2226), Bit/dim 4.8306(best: 4.5440), Xent 1.9418, Loss 5.8014, Error 0.6696(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 53.3159(52.5481) | Bit/dim 4.8306(5.2590) | Xent 1.9723(2.2051) | Loss 5.8167(6.3615) | Error 0.6952(0.7353) Steps 592(575.54) | Grad Norm 1.7711(4.1083) | Total Time 14.00(14.00)\n",
      "Iter 0560 | Time 55.1959(52.6275) | Bit/dim 4.7998(5.2452) | Xent 1.9900(2.1987) | Loss 5.7948(6.3445) | Error 0.7075(0.7345) Steps 592(576.03) | Grad Norm 3.5613(4.0919) | Total Time 14.00(14.00)\n",
      "Iter 0561 | Time 56.1583(52.7335) | Bit/dim 4.8168(5.2323) | Xent 1.9908(2.1925) | Loss 5.8122(6.3286) | Error 0.6964(0.7333) Steps 610(577.05) | Grad Norm 3.9193(4.0867) | Total Time 14.00(14.00)\n",
      "Iter 0562 | Time 56.0262(52.8322) | Bit/dim 4.8100(5.2197) | Xent 1.9665(2.1857) | Loss 5.7932(6.3125) | Error 0.6800(0.7317) Steps 598(577.68) | Grad Norm 1.1559(3.9988) | Total Time 14.00(14.00)\n",
      "Iter 0563 | Time 54.6579(52.8870) | Bit/dim 4.7926(5.2069) | Xent 1.9925(2.1799) | Loss 5.7888(6.2968) | Error 0.7025(0.7309) Steps 592(578.11) | Grad Norm 3.9411(3.9970) | Total Time 14.00(14.00)\n",
      "Iter 0564 | Time 56.1168(52.9839) | Bit/dim 4.7852(5.1942) | Xent 1.9750(2.1737) | Loss 5.7728(6.2811) | Error 0.6915(0.7297) Steps 610(579.06) | Grad Norm 3.1367(3.9712) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 28.1137, Epoch Time 375.4584(338.3697), Bit/dim 4.7849(best: 4.5440), Xent 1.9288, Loss 5.7493, Error 0.6653(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 55.7650(53.0673) | Bit/dim 4.7816(5.1818) | Xent 1.9806(2.1679) | Loss 5.7719(6.2658) | Error 0.6895(0.7285) Steps 610(579.99) | Grad Norm 1.8519(3.9076) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 56.2171(53.1618) | Bit/dim 4.7837(5.1699) | Xent 2.0039(2.1630) | Loss 5.7857(6.2514) | Error 0.7079(0.7279) Steps 604(580.71) | Grad Norm 5.2499(3.9479) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 57.0953(53.2798) | Bit/dim 4.7997(5.1588) | Xent 1.9556(2.1568) | Loss 5.7775(6.2372) | Error 0.6876(0.7266) Steps 604(581.41) | Grad Norm 4.7875(3.9731) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 57.2634(53.3993) | Bit/dim 4.7660(5.1470) | Xent 1.9428(2.1504) | Loss 5.7374(6.2222) | Error 0.6811(0.7253) Steps 610(582.27) | Grad Norm 0.8357(3.8790) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 54.3725(53.4285) | Bit/dim 4.7887(5.1362) | Xent 1.9671(2.1449) | Loss 5.7722(6.2087) | Error 0.6931(0.7243) Steps 592(582.56) | Grad Norm 4.7435(3.9049) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 56.2824(53.5141) | Bit/dim 4.7940(5.1260) | Xent 1.9605(2.1393) | Loss 5.7742(6.1957) | Error 0.6914(0.7233) Steps 586(582.66) | Grad Norm 4.8930(3.9345) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 27.5636, Epoch Time 380.6067(339.6368), Bit/dim 4.7470(best: 4.5440), Xent 1.9097, Loss 5.7019, Error 0.6597(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 56.9918(53.6185) | Bit/dim 4.7486(5.1147) | Xent 1.9501(2.1337) | Loss 5.7237(6.1815) | Error 0.6880(0.7223) Steps 610(583.48) | Grad Norm 1.2433(3.8538) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 54.7035(53.6510) | Bit/dim 4.7512(5.1038) | Xent 1.9834(2.1292) | Loss 5.7429(6.1683) | Error 0.6992(0.7216) Steps 592(583.74) | Grad Norm 5.4995(3.9032) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 56.6133(53.7399) | Bit/dim 4.7602(5.0934) | Xent 1.9652(2.1242) | Loss 5.7428(6.1556) | Error 0.6856(0.7205) Steps 616(584.71) | Grad Norm 5.0060(3.9363) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 55.7555(53.8004) | Bit/dim 4.7358(5.0827) | Xent 1.9351(2.1186) | Loss 5.7034(6.1420) | Error 0.6754(0.7191) Steps 604(585.29) | Grad Norm 0.8791(3.8445) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 57.7727(53.9195) | Bit/dim 4.7353(5.0723) | Xent 1.9533(2.1136) | Loss 5.7120(6.1291) | Error 0.6926(0.7183) Steps 610(586.03) | Grad Norm 4.3107(3.8585) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 57.7357(54.0340) | Bit/dim 4.7366(5.0622) | Xent 1.9489(2.1087) | Loss 5.7110(6.1166) | Error 0.6870(0.7174) Steps 616(586.93) | Grad Norm 4.0728(3.8650) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 28.0688, Epoch Time 383.4761(340.9520), Bit/dim 4.7280(best: 4.5440), Xent 1.8942, Loss 5.6752, Error 0.6531(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 56.9797(54.1224) | Bit/dim 4.7304(5.0523) | Xent 1.9280(2.1032) | Loss 5.6944(6.1039) | Error 0.6781(0.7162) Steps 622(587.98) | Grad Norm 1.0621(3.7809) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 57.5174(54.2242) | Bit/dim 4.7287(5.0426) | Xent 1.9454(2.0985) | Loss 5.7014(6.0918) | Error 0.6904(0.7155) Steps 616(588.82) | Grad Norm 3.2881(3.7661) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 58.8179(54.3621) | Bit/dim 4.7730(5.0345) | Xent 1.9583(2.0943) | Loss 5.7522(6.0816) | Error 0.6883(0.7146) Steps 622(589.81) | Grad Norm 5.6931(3.8239) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 55.8929(54.4080) | Bit/dim 4.8844(5.0300) | Xent 1.9449(2.0898) | Loss 5.8569(6.0749) | Error 0.6955(0.7141) Steps 610(590.42) | Grad Norm 5.7027(3.8803) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 56.9016(54.4828) | Bit/dim 4.7350(5.0211) | Xent 1.9159(2.0846) | Loss 5.6929(6.0634) | Error 0.6639(0.7126) Steps 616(591.19) | Grad Norm 1.6206(3.8125) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 59.1387(54.6225) | Bit/dim 4.9028(5.0176) | Xent 2.0046(2.0822) | Loss 5.9051(6.0587) | Error 0.7035(0.7123) Steps 622(592.11) | Grad Norm 10.7185(4.0197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 26.7268, Epoch Time 387.9234(342.3611), Bit/dim 5.2608(best: 4.5440), Xent 1.9877, Loss 6.2547, Error 0.7159(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 53.9026(54.6009) | Bit/dim 5.2603(5.0249) | Xent 1.9976(2.0797) | Loss 6.2591(6.0647) | Error 0.7219(0.7126) Steps 586(591.93) | Grad Norm 7.3969(4.1210) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 52.7293(54.5447) | Bit/dim 5.4489(5.0376) | Xent 1.9798(2.0767) | Loss 6.4388(6.0759) | Error 0.7055(0.7124) Steps 568(591.21) | Grad Norm 4.7571(4.1401) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 54.0238(54.5291) | Bit/dim 5.2162(5.0429) | Xent 1.9396(2.0726) | Loss 6.1861(6.0792) | Error 0.6740(0.7112) Steps 604(591.59) | Grad Norm 4.8265(4.1606) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 56.2369(54.5803) | Bit/dim 4.8543(5.0373) | Xent 1.9753(2.0696) | Loss 5.8420(6.0721) | Error 0.6997(0.7109) Steps 604(591.97) | Grad Norm 5.2352(4.1929) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 59.4235(54.7256) | Bit/dim 5.1970(5.0421) | Xent 2.2146(2.0740) | Loss 6.3043(6.0791) | Error 0.7620(0.7124) Steps 610(592.51) | Grad Norm 20.7860(4.6907) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 52.7463(54.6662) | Bit/dim 5.2623(5.0487) | Xent 2.3591(2.0825) | Loss 6.4419(6.0900) | Error 0.8145(0.7155) Steps 598(592.67) | Grad Norm 15.8752(5.0262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 26.3991, Epoch Time 371.0260(343.2211), Bit/dim 5.6058(best: 4.5440), Xent 2.0734, Loss 6.6425, Error 0.7373(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 49.7382(54.5184) | Bit/dim 5.6026(5.0653) | Xent 2.0920(2.0828) | Loss 6.6486(6.1067) | Error 0.7401(0.7162) Steps 580(592.29) | Grad Norm 5.6441(5.0448) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 51.3080(54.4221) | Bit/dim 5.7864(5.0869) | Xent 2.0471(2.0818) | Loss 6.8099(6.1278) | Error 0.7479(0.7172) Steps 580(591.92) | Grad Norm 5.3378(5.0535) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 47.1614(54.2043) | Bit/dim 5.8416(5.1096) | Xent 2.0468(2.0807) | Loss 6.8650(6.1499) | Error 0.7408(0.7179) Steps 550(590.67) | Grad Norm 5.1972(5.0579) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 50.7868(54.1017) | Bit/dim 5.8011(5.1303) | Xent 1.9983(2.0782) | Loss 6.8003(6.1694) | Error 0.7067(0.7175) Steps 562(589.81) | Grad Norm 3.5416(5.0124) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 56.4078(54.1709) | Bit/dim 5.6576(5.1461) | Xent 1.9817(2.0753) | Loss 6.6484(6.1838) | Error 0.6943(0.7168) Steps 592(589.87) | Grad Norm 2.9467(4.9504) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 56.2384(54.2329) | Bit/dim 5.4429(5.1550) | Xent 1.9974(2.0730) | Loss 6.4416(6.1915) | Error 0.6954(0.7162) Steps 598(590.12) | Grad Norm 3.4275(4.9047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 26.0328, Epoch Time 353.3596(343.5252), Bit/dim 5.3888(best: 4.5440), Xent 1.9625, Loss 6.3701, Error 0.6751(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 58.8676(54.3720) | Bit/dim 5.3961(5.1623) | Xent 2.0101(2.0711) | Loss 6.4011(6.1978) | Error 0.7144(0.7161) Steps 580(589.81) | Grad Norm 3.3680(4.8586) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 53.5042(54.3460) | Bit/dim 5.3600(5.1682) | Xent 2.0138(2.0694) | Loss 6.3669(6.2029) | Error 0.7230(0.7163) Steps 556(588.80) | Grad Norm 5.2952(4.8717) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 57.0317(54.4265) | Bit/dim 5.2608(5.1710) | Xent 1.9844(2.0668) | Loss 6.2530(6.2044) | Error 0.6976(0.7158) Steps 574(588.35) | Grad Norm 2.2307(4.7925) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 58.6887(54.5544) | Bit/dim 5.2362(5.1729) | Xent 1.9964(2.0647) | Loss 6.2344(6.2053) | Error 0.7126(0.7157) Steps 592(588.46) | Grad Norm 4.5038(4.7838) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 60.1702(54.7229) | Bit/dim 5.2002(5.1738) | Xent 1.9775(2.0621) | Loss 6.1890(6.2048) | Error 0.6957(0.7151) Steps 598(588.75) | Grad Norm 2.1108(4.7036) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 57.5575(54.8079) | Bit/dim 5.1695(5.1736) | Xent 1.9950(2.0601) | Loss 6.1670(6.2037) | Error 0.7137(0.7150) Steps 574(588.31) | Grad Norm 4.7186(4.7041) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 25.9807, Epoch Time 387.3992(344.8414), Bit/dim 5.1205(best: 4.5440), Xent 1.9310, Loss 6.0860, Error 0.6800(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 56.9992(54.8736) | Bit/dim 5.1248(5.1722) | Xent 1.9701(2.0574) | Loss 6.1099(6.2009) | Error 0.6996(0.7146) Steps 586(588.24) | Grad Norm 2.3308(4.6329) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 59.4892(55.0121) | Bit/dim 5.0735(5.1692) | Xent 1.9539(2.0543) | Loss 6.0505(6.1964) | Error 0.6781(0.7135) Steps 586(588.17) | Grad Norm 3.4213(4.5965) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 53.5969(54.9697) | Bit/dim 5.0481(5.1656) | Xent 1.9375(2.0508) | Loss 6.0169(6.1910) | Error 0.6696(0.7122) Steps 580(587.93) | Grad Norm 1.6038(4.5067) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 53.9741(54.9398) | Bit/dim 5.0350(5.1617) | Xent 1.9455(2.0476) | Loss 6.0078(6.1855) | Error 0.6853(0.7114) Steps 580(587.69) | Grad Norm 2.9546(4.4602) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 55.0187(54.9422) | Bit/dim 5.0114(5.1571) | Xent 1.9514(2.0447) | Loss 5.9871(6.1795) | Error 0.6869(0.7106) Steps 574(587.28) | Grad Norm 1.9490(4.3848) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 53.1475(54.8883) | Bit/dim 4.9959(5.1523) | Xent 1.9386(2.0416) | Loss 5.9653(6.1731) | Error 0.6813(0.7097) Steps 574(586.88) | Grad Norm 3.4032(4.3554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 25.9180, Epoch Time 373.6199(345.7048), Bit/dim 4.9632(best: 4.5440), Xent 1.8876, Loss 5.9070, Error 0.6516(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 55.7498(54.9142) | Bit/dim 4.9785(5.1471) | Xent 1.9276(2.0381) | Loss 5.9423(6.1662) | Error 0.6740(0.7087) Steps 580(586.67) | Grad Norm 1.5496(4.2712) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 52.8662(54.8527) | Bit/dim 4.9194(5.1403) | Xent 1.9340(2.0350) | Loss 5.8864(6.1578) | Error 0.6833(0.7079) Steps 574(586.29) | Grad Norm 2.9388(4.2312) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 54.2904(54.8359) | Bit/dim 4.8845(5.1326) | Xent 1.9029(2.0311) | Loss 5.8359(6.1481) | Error 0.6605(0.7065) Steps 562(585.56) | Grad Norm 1.3242(4.1440) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 52.7941(54.7746) | Bit/dim 4.8794(5.1250) | Xent 1.9065(2.0273) | Loss 5.8327(6.1387) | Error 0.6718(0.7054) Steps 568(585.04) | Grad Norm 2.7006(4.1007) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 52.9694(54.7204) | Bit/dim 4.8787(5.1176) | Xent 1.8975(2.0234) | Loss 5.8275(6.1293) | Error 0.6691(0.7044) Steps 556(584.17) | Grad Norm 1.5027(4.0228) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 54.2806(54.7072) | Bit/dim 4.8716(5.1102) | Xent 1.9098(2.0200) | Loss 5.8265(6.1202) | Error 0.6710(0.7034) Steps 580(584.04) | Grad Norm 2.7070(3.9833) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 26.2578, Epoch Time 365.1403(346.2879), Bit/dim 4.8564(best: 4.5440), Xent 1.8783, Loss 5.7955, Error 0.6484(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 58.5569(54.8227) | Bit/dim 4.8599(5.1027) | Xent 1.9213(2.0171) | Loss 5.8206(6.1112) | Error 0.6694(0.7023) Steps 592(584.28) | Grad Norm 3.0752(3.9561) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 58.7333(54.9401) | Bit/dim 4.8336(5.0946) | Xent 1.8989(2.0135) | Loss 5.7830(6.1014) | Error 0.6695(0.7014) Steps 580(584.15) | Grad Norm 1.0625(3.8693) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 58.1964(55.0377) | Bit/dim 4.8167(5.0863) | Xent 1.9033(2.0102) | Loss 5.7684(6.0914) | Error 0.6562(0.7000) Steps 592(584.39) | Grad Norm 2.9689(3.8423) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 58.0507(55.1281) | Bit/dim 4.8054(5.0779) | Xent 1.9325(2.0079) | Loss 5.7717(6.0818) | Error 0.6763(0.6993) Steps 604(584.97) | Grad Norm 4.6617(3.8668) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 58.1638(55.2192) | Bit/dim 4.7995(5.0695) | Xent 1.9110(2.0050) | Loss 5.7550(6.0720) | Error 0.6696(0.6984) Steps 586(585.01) | Grad Norm 4.2307(3.8778) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 58.5696(55.3197) | Bit/dim 4.7883(5.0611) | Xent 1.9069(2.0020) | Loss 5.7418(6.0621) | Error 0.6734(0.6976) Steps 586(585.04) | Grad Norm 3.0221(3.8521) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 26.6803, Epoch Time 392.4519(347.6728), Bit/dim 4.7770(best: 4.5440), Xent 1.8595, Loss 5.7067, Error 0.6414(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 54.4261(55.2929) | Bit/dim 4.7949(5.0531) | Xent 1.9088(1.9992) | Loss 5.7493(6.0527) | Error 0.6667(0.6967) Steps 574(584.70) | Grad Norm 1.3069(3.7757) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 55.9323(55.3121) | Bit/dim 4.7668(5.0445) | Xent 1.8722(1.9954) | Loss 5.7029(6.0422) | Error 0.6551(0.6955) Steps 586(584.74) | Grad Norm 1.1923(3.6982) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 56.7664(55.3557) | Bit/dim 4.7607(5.0360) | Xent 1.8846(1.9921) | Loss 5.7030(6.0320) | Error 0.6527(0.6942) Steps 598(585.14) | Grad Norm 1.4390(3.6305) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 54.6207(55.3337) | Bit/dim 4.7423(5.0272) | Xent 1.9023(1.9894) | Loss 5.6935(6.0219) | Error 0.6684(0.6934) Steps 598(585.53) | Grad Norm 1.7337(3.5735) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 55.8283(55.3485) | Bit/dim 4.7321(5.0183) | Xent 1.8783(1.9861) | Loss 5.6712(6.0114) | Error 0.6581(0.6924) Steps 598(585.90) | Grad Norm 2.8087(3.5506) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 56.1758(55.3733) | Bit/dim 4.7269(5.0096) | Xent 1.8973(1.9834) | Loss 5.6755(6.0013) | Error 0.6743(0.6918) Steps 580(585.72) | Grad Norm 5.1914(3.5998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 26.8948, Epoch Time 376.2601(348.5304), Bit/dim 4.7508(best: 4.5440), Xent 1.8970, Loss 5.6993, Error 0.6600(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 57.0481(55.4236) | Bit/dim 4.7487(5.0018) | Xent 1.9130(1.9813) | Loss 5.7053(5.9924) | Error 0.6680(0.6911) Steps 598(586.09) | Grad Norm 9.4579(3.7756) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 56.1713(55.4460) | Bit/dim 4.7727(4.9949) | Xent 2.0508(1.9834) | Loss 5.7981(5.9866) | Error 0.7112(0.6917) Steps 586(586.09) | Grad Norm 16.3397(4.1525) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 55.7318(55.4546) | Bit/dim 4.8436(4.9904) | Xent 2.1356(1.9879) | Loss 5.9114(5.9843) | Error 0.7545(0.6936) Steps 616(586.99) | Grad Norm 15.7401(4.5001) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 53.3762(55.3922) | Bit/dim 4.7239(4.9824) | Xent 1.8772(1.9846) | Loss 5.6625(5.9747) | Error 0.6578(0.6925) Steps 586(586.96) | Grad Norm 1.5881(4.4128) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 54.5354(55.3665) | Bit/dim 4.7572(4.9756) | Xent 2.0486(1.9865) | Loss 5.7815(5.9689) | Error 0.7201(0.6933) Steps 580(586.75) | Grad Norm 10.2013(4.5864) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 55.0128(55.3559) | Bit/dim 4.7149(4.9678) | Xent 1.9179(1.9845) | Loss 5.6738(5.9600) | Error 0.6844(0.6931) Steps 580(586.55) | Grad Norm 2.0957(4.5117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 26.3660, Epoch Time 373.8577(349.2902), Bit/dim 4.7242(best: 4.5440), Xent 1.9225, Loss 5.6854, Error 0.6739(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 57.3734(55.4164) | Bit/dim 4.7254(4.9605) | Xent 1.9505(1.9835) | Loss 5.7006(5.9522) | Error 0.6949(0.6931) Steps 598(586.89) | Grad Norm 4.4556(4.5100) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 58.6869(55.5145) | Bit/dim 4.7187(4.9533) | Xent 1.9117(1.9813) | Loss 5.6746(5.9439) | Error 0.6756(0.6926) Steps 610(587.58) | Grad Norm 3.8970(4.4916) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 58.6996(55.6101) | Bit/dim 4.7179(4.9462) | Xent 1.9480(1.9803) | Loss 5.6919(5.9364) | Error 0.6889(0.6925) Steps 592(587.72) | Grad Norm 2.2145(4.4233) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 55.3860(55.6034) | Bit/dim 4.7092(4.9391) | Xent 1.9380(1.9790) | Loss 5.6782(5.9286) | Error 0.6864(0.6923) Steps 592(587.84) | Grad Norm 2.2450(4.3580) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 57.3827(55.6567) | Bit/dim 4.7149(4.9324) | Xent 1.9398(1.9779) | Loss 5.6848(5.9213) | Error 0.6951(0.6924) Steps 598(588.15) | Grad Norm 2.5889(4.3049) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 57.5608(55.7139) | Bit/dim 4.6981(4.9253) | Xent 1.9259(1.9763) | Loss 5.6610(5.9135) | Error 0.6844(0.6922) Steps 604(588.62) | Grad Norm 1.9358(4.2338) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 27.9568, Epoch Time 388.6879(350.4722), Bit/dim 4.6967(best: 4.5440), Xent 1.8842, Loss 5.6388, Error 0.6559(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 56.8167(55.7470) | Bit/dim 4.6955(4.9184) | Xent 1.8977(1.9739) | Loss 5.6444(5.9054) | Error 0.6666(0.6914) Steps 610(589.27) | Grad Norm 1.6796(4.1572) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 59.4062(55.8567) | Bit/dim 4.6739(4.9111) | Xent 1.9090(1.9720) | Loss 5.6284(5.8971) | Error 0.6769(0.6909) Steps 604(589.71) | Grad Norm 2.0072(4.0927) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 57.2682(55.8991) | Bit/dim 4.6909(4.9045) | Xent 1.9087(1.9701) | Loss 5.6453(5.8895) | Error 0.6823(0.6907) Steps 604(590.14) | Grad Norm 1.6603(4.0197) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 58.4330(55.9751) | Bit/dim 4.6747(4.8976) | Xent 1.9119(1.9683) | Loss 5.6307(5.8818) | Error 0.6703(0.6901) Steps 598(590.37) | Grad Norm 1.6372(3.9482) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 60.0535(56.0974) | Bit/dim 4.6774(4.8910) | Xent 1.9141(1.9667) | Loss 5.6344(5.8744) | Error 0.6721(0.6895) Steps 610(590.96) | Grad Norm 1.4069(3.8720) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 57.7245(56.1463) | Bit/dim 4.6640(4.8842) | Xent 1.8939(1.9645) | Loss 5.6110(5.8665) | Error 0.6643(0.6888) Steps 604(591.35) | Grad Norm 1.5997(3.8038) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 26.8546, Epoch Time 392.4336(351.7310), Bit/dim 4.6700(best: 4.5440), Xent 1.8588, Loss 5.5994, Error 0.6431(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 57.1164(56.1754) | Bit/dim 4.6791(4.8780) | Xent 1.8787(1.9620) | Loss 5.6184(5.8590) | Error 0.6599(0.6879) Steps 604(591.73) | Grad Norm 1.3354(3.7298) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 59.3843(56.2716) | Bit/dim 4.6543(4.8713) | Xent 1.8816(1.9596) | Loss 5.5951(5.8511) | Error 0.6618(0.6871) Steps 610(592.28) | Grad Norm 1.5086(3.6631) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 61.0571(56.4152) | Bit/dim 4.6480(4.8646) | Xent 1.8973(1.9577) | Loss 5.5966(5.8435) | Error 0.6641(0.6864) Steps 610(592.81) | Grad Norm 1.2031(3.5893) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 58.6349(56.4818) | Bit/dim 4.6603(4.8585) | Xent 1.8905(1.9557) | Loss 5.6055(5.8363) | Error 0.6660(0.6858) Steps 604(593.15) | Grad Norm 1.5972(3.5296) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 60.2960(56.5962) | Bit/dim 4.6451(4.8521) | Xent 1.8754(1.9533) | Loss 5.5828(5.8287) | Error 0.6506(0.6848) Steps 616(593.83) | Grad Norm 1.3300(3.4636) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 60.7190(56.7199) | Bit/dim 4.6462(4.8459) | Xent 1.8719(1.9508) | Loss 5.5821(5.8213) | Error 0.6530(0.6838) Steps 622(594.68) | Grad Norm 1.2757(3.3980) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 28.1261, Epoch Time 400.8287(353.2039), Bit/dim 4.6414(best: 4.5440), Xent 1.8377, Loss 5.5603, Error 0.6338(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 61.1574(56.8530) | Bit/dim 4.6382(4.8397) | Xent 1.8815(1.9487) | Loss 5.5790(5.8141) | Error 0.6719(0.6835) Steps 628(595.68) | Grad Norm 1.7213(3.3477) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 59.2722(56.9256) | Bit/dim 4.6407(4.8337) | Xent 1.8823(1.9467) | Loss 5.5819(5.8071) | Error 0.6504(0.6825) Steps 604(595.93) | Grad Norm 1.4240(3.2899) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 58.1278(56.9617) | Bit/dim 4.6270(4.8275) | Xent 1.8681(1.9444) | Loss 5.5610(5.7997) | Error 0.6529(0.6816) Steps 586(595.63) | Grad Norm 1.0922(3.2240) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 58.4175(57.0053) | Bit/dim 4.6236(4.8214) | Xent 1.8520(1.9416) | Loss 5.5496(5.7922) | Error 0.6484(0.6806) Steps 592(595.52) | Grad Norm 2.2020(3.1933) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 58.8534(57.0608) | Bit/dim 4.6264(4.8155) | Xent 1.8466(1.9388) | Loss 5.5497(5.7849) | Error 0.6485(0.6796) Steps 592(595.41) | Grad Norm 0.9560(3.1262) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 58.5481(57.1054) | Bit/dim 4.6214(4.8097) | Xent 1.8581(1.9363) | Loss 5.5505(5.7779) | Error 0.6527(0.6788) Steps 586(595.13) | Grad Norm 2.0767(3.0947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 27.9453, Epoch Time 398.3676(354.5588), Bit/dim 4.6203(best: 4.5440), Xent 1.8194, Loss 5.5300, Error 0.6309(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 58.8760(57.1585) | Bit/dim 4.6147(4.8039) | Xent 1.8579(1.9340) | Loss 5.5437(5.7709) | Error 0.6504(0.6780) Steps 610(595.58) | Grad Norm 1.9896(3.0616) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 59.3458(57.2241) | Bit/dim 4.6117(4.7981) | Xent 1.8381(1.9311) | Loss 5.5307(5.7637) | Error 0.6420(0.6769) Steps 592(595.47) | Grad Norm 1.2551(3.0074) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 58.6717(57.2676) | Bit/dim 4.6112(4.7925) | Xent 1.8434(1.9285) | Loss 5.5329(5.7567) | Error 0.6479(0.6760) Steps 592(595.37) | Grad Norm 1.0754(2.9494) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 58.4095(57.3018) | Bit/dim 4.6017(4.7868) | Xent 1.8557(1.9263) | Loss 5.5296(5.7499) | Error 0.6498(0.6752) Steps 586(595.09) | Grad Norm 2.0159(2.9214) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 58.0611(57.3246) | Bit/dim 4.5991(4.7811) | Xent 1.8464(1.9239) | Loss 5.5222(5.7431) | Error 0.6445(0.6743) Steps 598(595.17) | Grad Norm 2.4373(2.9069) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 59.8108(57.3992) | Bit/dim 4.5971(4.7756) | Xent 1.8477(1.9216) | Loss 5.5209(5.7364) | Error 0.6506(0.6736) Steps 598(595.26) | Grad Norm 2.3647(2.8906) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 27.7364, Epoch Time 396.8971(355.8290), Bit/dim 4.5919(best: 4.5440), Xent 1.7985, Loss 5.4911, Error 0.6242(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 58.7253(57.4390) | Bit/dim 4.5902(4.7701) | Xent 1.8382(1.9191) | Loss 5.5093(5.7296) | Error 0.6411(0.6726) Steps 610(595.70) | Grad Norm 0.8473(2.8293) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 59.1880(57.4914) | Bit/dim 4.5987(4.7649) | Xent 1.8360(1.9166) | Loss 5.5167(5.7232) | Error 0.6460(0.6718) Steps 592(595.59) | Grad Norm 1.4725(2.7886) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 58.0226(57.5074) | Bit/dim 4.5844(4.7595) | Xent 1.8241(1.9138) | Loss 5.4964(5.7164) | Error 0.6360(0.6707) Steps 610(596.02) | Grad Norm 1.3863(2.7466) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 60.6133(57.6005) | Bit/dim 4.5824(4.7542) | Xent 1.8115(1.9108) | Loss 5.4882(5.7096) | Error 0.6279(0.6695) Steps 598(596.08) | Grad Norm 1.0226(2.6948) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 60.5928(57.6903) | Bit/dim 4.5750(4.7488) | Xent 1.8257(1.9082) | Loss 5.4879(5.7029) | Error 0.6375(0.6685) Steps 604(596.32) | Grad Norm 1.9686(2.6731) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 59.1206(57.7332) | Bit/dim 4.5796(4.7437) | Xent 1.8351(1.9060) | Loss 5.4971(5.6968) | Error 0.6439(0.6678) Steps 598(596.37) | Grad Norm 2.3758(2.6641) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 27.5051, Epoch Time 399.3813(357.1356), Bit/dim 4.5789(best: 4.5440), Xent 1.7846, Loss 5.4712, Error 0.6175(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 58.9884(57.7709) | Bit/dim 4.5729(4.7386) | Xent 1.8192(1.9034) | Loss 5.4825(5.6903) | Error 0.6366(0.6668) Steps 610(596.78) | Grad Norm 2.4217(2.6569) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 59.0868(57.8104) | Bit/dim 4.5685(4.7335) | Xent 1.7948(1.9002) | Loss 5.4659(5.6836) | Error 0.6285(0.6657) Steps 610(597.17) | Grad Norm 1.6199(2.6258) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 60.2778(57.8844) | Bit/dim 4.5776(4.7288) | Xent 1.7968(1.8971) | Loss 5.4760(5.6774) | Error 0.6304(0.6646) Steps 610(597.56) | Grad Norm 1.3506(2.5875) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 57.8571(57.8836) | Bit/dim 4.5547(4.7236) | Xent 1.8333(1.8952) | Loss 5.4713(5.6712) | Error 0.6446(0.6640) Steps 604(597.75) | Grad Norm 2.2192(2.5765) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 58.9150(57.9145) | Bit/dim 4.5669(4.7189) | Xent 1.8174(1.8928) | Loss 5.4756(5.6653) | Error 0.6359(0.6632) Steps 610(598.12) | Grad Norm 4.1404(2.6234) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 58.6525(57.9366) | Bit/dim 4.5838(4.7149) | Xent 1.8798(1.8924) | Loss 5.5236(5.6611) | Error 0.6627(0.6632) Steps 598(598.12) | Grad Norm 9.2726(2.8229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 28.3039, Epoch Time 397.8765(358.3578), Bit/dim 4.6697(best: 4.5440), Xent 2.0135, Loss 5.6764, Error 0.7176(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 56.8015(57.9026) | Bit/dim 4.6719(4.7136) | Xent 2.0348(1.8967) | Loss 5.6893(5.6619) | Error 0.7190(0.6648) Steps 610(598.47) | Grad Norm 15.9189(3.2157) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 58.1541(57.9101) | Bit/dim 4.7173(4.7137) | Xent 2.1738(1.9050) | Loss 5.8042(5.6662) | Error 0.7355(0.6670) Steps 604(598.64) | Grad Norm 19.5989(3.7072) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 58.6713(57.9330) | Bit/dim 4.6088(4.7105) | Xent 1.8808(1.9043) | Loss 5.5492(5.6627) | Error 0.6616(0.6668) Steps 598(598.62) | Grad Norm 3.9252(3.7138) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 61.8312(58.0499) | Bit/dim 4.6654(4.7092) | Xent 1.9993(1.9071) | Loss 5.6650(5.6627) | Error 0.7177(0.6683) Steps 604(598.78) | Grad Norm 8.3840(3.8539) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 56.6149(58.0069) | Bit/dim 4.6569(4.7076) | Xent 1.8757(1.9062) | Loss 5.5947(5.6607) | Error 0.6605(0.6681) Steps 604(598.94) | Grad Norm 3.8561(3.8539) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 58.5445(58.0230) | Bit/dim 4.6274(4.7052) | Xent 1.8775(1.9053) | Loss 5.5662(5.6579) | Error 0.6674(0.6681) Steps 604(599.09) | Grad Norm 2.4329(3.8113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 27.3968, Epoch Time 393.5124(359.4124), Bit/dim 4.6577(best: 4.5440), Xent 1.8535, Loss 5.5845, Error 0.6528(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 57.5600(58.0091) | Bit/dim 4.6629(4.7039) | Xent 1.8779(1.9045) | Loss 5.6018(5.6562) | Error 0.6687(0.6681) Steps 592(598.88) | Grad Norm 3.4625(3.8008) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 59.2349(58.0459) | Bit/dim 4.6442(4.7021) | Xent 1.9104(1.9047) | Loss 5.5994(5.6545) | Error 0.6851(0.6686) Steps 616(599.39) | Grad Norm 4.6081(3.8251) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 59.8454(58.0999) | Bit/dim 4.6140(4.6995) | Xent 1.8684(1.9036) | Loss 5.5481(5.6513) | Error 0.6639(0.6685) Steps 616(599.89) | Grad Norm 1.5838(3.7578) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 57.9337(58.0949) | Bit/dim 4.6264(4.6973) | Xent 1.9010(1.9035) | Loss 5.5769(5.6491) | Error 0.6799(0.6688) Steps 598(599.83) | Grad Norm 2.7014(3.7261) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 55.6034(58.0201) | Bit/dim 4.6903(4.6971) | Xent 1.8723(1.9026) | Loss 5.6265(5.6484) | Error 0.6616(0.6686) Steps 592(599.60) | Grad Norm 3.1019(3.7074) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 58.4546(58.0332) | Bit/dim 4.6228(4.6949) | Xent 1.8788(1.9019) | Loss 5.5622(5.6458) | Error 0.6589(0.6683) Steps 610(599.91) | Grad Norm 3.0839(3.6887) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 28.4193, Epoch Time 392.6624(360.4099), Bit/dim 4.6402(best: 4.5440), Xent 1.8488, Loss 5.5646, Error 0.6459(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 56.3258(57.9820) | Bit/dim 4.6348(4.6931) | Xent 1.8729(1.9010) | Loss 5.5713(5.6436) | Error 0.6555(0.6679) Steps 604(600.03) | Grad Norm 3.1058(3.6712) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 58.0383(57.9836) | Bit/dim 4.5996(4.6903) | Xent 1.8943(1.9008) | Loss 5.5468(5.6407) | Error 0.6815(0.6683) Steps 586(599.61) | Grad Norm 4.4400(3.6943) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 57.0430(57.9554) | Bit/dim 4.6190(4.6881) | Xent 1.9016(1.9008) | Loss 5.5698(5.6385) | Error 0.6753(0.6685) Steps 592(599.38) | Grad Norm 4.0294(3.7043) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 55.9758(57.8960) | Bit/dim 4.6127(4.6859) | Xent 1.8782(1.9001) | Loss 5.5518(5.6359) | Error 0.6689(0.6685) Steps 592(599.16) | Grad Norm 2.4980(3.6681) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 58.3167(57.9087) | Bit/dim 4.5996(4.6833) | Xent 1.8833(1.8996) | Loss 5.5412(5.6331) | Error 0.6601(0.6683) Steps 604(599.31) | Grad Norm 2.8826(3.6446) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 57.7573(57.9041) | Bit/dim 4.6342(4.6818) | Xent 1.8884(1.8993) | Loss 5.5784(5.6314) | Error 0.6645(0.6682) Steps 610(599.63) | Grad Norm 5.5050(3.7004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 26.9391, Epoch Time 386.2750(361.1859), Bit/dim 4.5946(best: 4.5440), Xent 1.8523, Loss 5.5208, Error 0.6580(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 57.6324(57.8960) | Bit/dim 4.5915(4.6791) | Xent 1.8787(1.8987) | Loss 5.5308(5.6284) | Error 0.6643(0.6681) Steps 604(599.76) | Grad Norm 4.8226(3.7341) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 56.8254(57.8639) | Bit/dim 4.5709(4.6758) | Xent 1.8790(1.8981) | Loss 5.5104(5.6249) | Error 0.6646(0.6679) Steps 580(599.17) | Grad Norm 2.8903(3.7087) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 54.4222(57.7606) | Bit/dim 4.5928(4.6733) | Xent 1.8668(1.8972) | Loss 5.5262(5.6219) | Error 0.6526(0.6675) Steps 592(598.95) | Grad Norm 1.6273(3.6463) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 56.2325(57.7148) | Bit/dim 4.5733(4.6703) | Xent 1.8647(1.8962) | Loss 5.5056(5.6184) | Error 0.6644(0.6674) Steps 598(598.92) | Grad Norm 4.1953(3.6628) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 55.8725(57.6595) | Bit/dim 4.5708(4.6674) | Xent 1.8728(1.8955) | Loss 5.5072(5.6151) | Error 0.6601(0.6672) Steps 592(598.71) | Grad Norm 4.7271(3.6947) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 57.6734(57.6599) | Bit/dim 4.5698(4.6644) | Xent 1.8473(1.8940) | Loss 5.4934(5.6114) | Error 0.6536(0.6668) Steps 598(598.69) | Grad Norm 2.5990(3.6618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 27.4617, Epoch Time 381.9112(361.8076), Bit/dim 4.5567(best: 4.5440), Xent 1.8106, Loss 5.4620, Error 0.6344(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 57.1921(57.6459) | Bit/dim 4.5548(4.6611) | Xent 1.8331(1.8922) | Loss 5.4713(5.6072) | Error 0.6502(0.6663) Steps 592(598.49) | Grad Norm 2.0880(3.6146) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 56.2089(57.4835) | Bit/dim 4.5555(4.6525) | Xent 1.8147(1.8887) | Loss 5.4628(5.5968) | Error 0.6436(0.6652) Steps 592(597.41) | Grad Norm 1.8251(3.5683) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 56.2728(57.4472) | Bit/dim 4.5501(4.6494) | Xent 1.8429(1.8873) | Loss 5.4716(5.5931) | Error 0.6495(0.6647) Steps 586(597.07) | Grad Norm 1.5371(3.5073) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 55.8072(57.3980) | Bit/dim 4.5565(4.6466) | Xent 1.8345(1.8857) | Loss 5.4737(5.5895) | Error 0.6449(0.6641) Steps 574(596.38) | Grad Norm 3.5806(3.5095) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 26.7815, Epoch Time 379.1975(362.3293), Bit/dim 4.5432(best: 4.5440), Xent 1.8162, Loss 5.4513, Error 0.6318(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 55.8899(57.3528) | Bit/dim 4.5382(4.6434) | Xent 1.8537(1.8848) | Loss 5.4650(5.5857) | Error 0.6469(0.6636) Steps 586(596.07) | Grad Norm 4.7071(3.5455) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 57.0461(57.3436) | Bit/dim 4.5535(4.6407) | Xent 1.8702(1.8843) | Loss 5.4886(5.5828) | Error 0.6614(0.6635) Steps 592(595.94) | Grad Norm 5.4506(3.6026) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 57.6648(57.3532) | Bit/dim 4.5399(4.6376) | Xent 1.8462(1.8832) | Loss 5.4630(5.5792) | Error 0.6595(0.6634) Steps 598(596.01) | Grad Norm 4.6857(3.6351) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 55.7170(57.3041) | Bit/dim 4.5594(4.6353) | Xent 1.8390(1.8819) | Loss 5.4789(5.5762) | Error 0.6452(0.6629) Steps 592(595.89) | Grad Norm 2.3822(3.5975) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 55.4109(57.2473) | Bit/dim 4.5251(4.6320) | Xent 1.8195(1.8800) | Loss 5.4349(5.5720) | Error 0.6391(0.6621) Steps 592(595.77) | Grad Norm 2.6369(3.5687) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 56.1920(57.2157) | Bit/dim 4.5487(4.6295) | Xent 1.8540(1.8792) | Loss 5.4757(5.5691) | Error 0.6586(0.6620) Steps 598(595.84) | Grad Norm 5.4046(3.6238) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 26.5753, Epoch Time 380.0409(362.8607), Bit/dim 4.5514(best: 4.5432), Xent 1.8099, Loss 5.4563, Error 0.6351(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 56.1434(57.1835) | Bit/dim 4.5527(4.6272) | Xent 1.8672(1.8789) | Loss 5.4863(5.5666) | Error 0.6595(0.6620) Steps 580(595.36) | Grad Norm 5.3627(3.6759) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 56.5332(57.1640) | Bit/dim 4.4992(4.6233) | Xent 1.8348(1.8775) | Loss 5.4166(5.5621) | Error 0.6504(0.6616) Steps 592(595.26) | Grad Norm 2.6643(3.6456) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 57.4895(57.1737) | Bit/dim 4.5332(4.6206) | Xent 1.8143(1.8756) | Loss 5.4403(5.5585) | Error 0.6324(0.6607) Steps 586(594.98) | Grad Norm 3.1806(3.6316) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 57.0393(57.1697) | Bit/dim 4.5339(4.6180) | Xent 1.8654(1.8753) | Loss 5.4665(5.5557) | Error 0.6680(0.6610) Steps 586(594.71) | Grad Norm 7.5402(3.7489) | Total Time 14.00(14.00)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 57.2791(57.0925) | Bit/dim 4.5261(4.6113) | Xent 1.8947(1.8752) | Loss 5.4734(5.5489) | Error 0.6674(0.6608) Steps 580(593.95) | Grad Norm 7.3439(3.8953) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 54.7476(57.0221) | Bit/dim 4.5898(4.6106) | Xent 1.8518(1.8745) | Loss 5.5157(5.5479) | Error 0.6480(0.6604) Steps 586(593.71) | Grad Norm 6.3240(3.9682) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 58.5644(57.0684) | Bit/dim 4.5330(4.6083) | Xent 1.8537(1.8739) | Loss 5.4598(5.5452) | Error 0.6585(0.6603) Steps 586(593.48) | Grad Norm 4.3031(3.9783) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 55.3677(57.0174) | Bit/dim 4.6042(4.6082) | Xent 1.8397(1.8729) | Loss 5.5240(5.5446) | Error 0.6451(0.6599) Steps 604(593.80) | Grad Norm 5.6264(4.0277) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 56.7319(57.0088) | Bit/dim 4.5315(4.6059) | Xent 1.8290(1.8716) | Loss 5.4460(5.5416) | Error 0.6455(0.6594) Steps 586(593.56) | Grad Norm 3.7873(4.0205) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 54.9231(56.9463) | Bit/dim 4.6254(4.6064) | Xent 1.8910(1.8721) | Loss 5.5709(5.5425) | Error 0.6747(0.6599) Steps 574(592.98) | Grad Norm 8.2394(4.1471) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 27.5595, Epoch Time 380.7877(363.9381), Bit/dim 4.6063(best: 4.5312), Xent 1.8995, Loss 5.5560, Error 0.6671(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 54.7622(56.8807) | Bit/dim 4.6115(4.6066) | Xent 1.9198(1.8736) | Loss 5.5714(5.5434) | Error 0.6787(0.6605) Steps 592(592.95) | Grad Norm 9.6937(4.3135) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 56.9440(56.8826) | Bit/dim 4.5458(4.6048) | Xent 1.8591(1.8731) | Loss 5.4753(5.5413) | Error 0.6589(0.6604) Steps 586(592.74) | Grad Norm 6.1588(4.3688) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 55.9148(56.8536) | Bit/dim 4.5485(4.6031) | Xent 1.8602(1.8728) | Loss 5.4786(5.5395) | Error 0.6529(0.6602) Steps 580(592.36) | Grad Norm 5.9567(4.4164) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 56.1093(56.8313) | Bit/dim 4.5576(4.6017) | Xent 1.8784(1.8729) | Loss 5.4968(5.5382) | Error 0.6559(0.6601) Steps 586(592.17) | Grad Norm 6.7715(4.4871) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 54.9941(56.7761) | Bit/dim 4.5848(4.6012) | Xent 1.8135(1.8711) | Loss 5.4916(5.5368) | Error 0.6376(0.6594) Steps 580(591.80) | Grad Norm 3.2820(4.4509) | Total Time 14.00(14.00)\n",
      "Iter 0726 | Time 56.5246(56.7686) | Bit/dim 4.5491(4.5997) | Xent 1.8557(1.8707) | Loss 5.4770(5.5350) | Error 0.6569(0.6593) Steps 556(590.73) | Grad Norm 7.2372(4.5345) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 27.5471, Epoch Time 378.2578(364.3677), Bit/dim 4.6166(best: 4.5312), Xent 1.7964, Loss 5.5148, Error 0.6246(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 54.7200(56.7071) | Bit/dim 4.6223(4.6003) | Xent 1.8109(1.8689) | Loss 5.5278(5.5348) | Error 0.6414(0.6588) Steps 586(590.59) | Grad Norm 5.3297(4.5584) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 57.1871(56.7215) | Bit/dim 4.5993(4.6003) | Xent 1.8619(1.8687) | Loss 5.5302(5.5346) | Error 0.6601(0.6588) Steps 604(590.99) | Grad Norm 6.8557(4.6273) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 53.5755(56.6272) | Bit/dim 4.6202(4.6009) | Xent 1.8774(1.8689) | Loss 5.5589(5.5354) | Error 0.6571(0.6588) Steps 568(590.30) | Grad Norm 6.9343(4.6965) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 56.7165(56.6298) | Bit/dim 4.6162(4.6014) | Xent 1.8344(1.8679) | Loss 5.5334(5.5353) | Error 0.6549(0.6586) Steps 550(589.09) | Grad Norm 7.3281(4.7755) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 53.6373(56.5401) | Bit/dim 4.6340(4.6023) | Xent 1.8997(1.8689) | Loss 5.5838(5.5368) | Error 0.6734(0.6591) Steps 580(588.82) | Grad Norm 8.9368(4.9003) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 53.6083(56.4521) | Bit/dim 4.5788(4.6016) | Xent 1.8375(1.8679) | Loss 5.4976(5.5356) | Error 0.6472(0.6587) Steps 562(588.01) | Grad Norm 4.6898(4.8940) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 26.8810, Epoch Time 371.8459(364.5920), Bit/dim 4.6270(best: 4.5312), Xent 1.8561, Loss 5.5550, Error 0.6518(best: 0.6146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 55.2146(56.4150) | Bit/dim 4.6209(4.6022) | Xent 1.9206(1.8695) | Loss 5.5812(5.5370) | Error 0.6673(0.6590) Steps 574(587.59) | Grad Norm 7.3860(4.9687) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 52.8259(56.3073) | Bit/dim 4.6865(4.6047) | Xent 1.8124(1.8678) | Loss 5.5926(5.5386) | Error 0.6401(0.6584) Steps 538(586.10) | Grad Norm 4.7996(4.9637) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 55.9431(56.2964) | Bit/dim 4.6012(4.6046) | Xent 1.9063(1.8689) | Loss 5.5544(5.5391) | Error 0.6731(0.6589) Steps 580(585.92) | Grad Norm 4.6586(4.9545) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 55.1096(56.2608) | Bit/dim 4.5773(4.6038) | Xent 1.8546(1.8685) | Loss 5.5046(5.5381) | Error 0.6561(0.6588) Steps 586(585.92) | Grad Norm 4.4046(4.9380) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 53.2720(56.1711) | Bit/dim 4.5747(4.6029) | Xent 1.8500(1.8680) | Loss 5.4997(5.5369) | Error 0.6540(0.6586) Steps 568(585.39) | Grad Norm 3.8213(4.9045) | Total Time 14.00(14.00)\n",
      "Iter 0739 | Time 55.3990(56.1599) | Bit/dim 4.5465(4.6004) | Xent 1.7976(1.8662) | Loss 5.4453(5.5335) | Error 0.6305(0.6579) Steps 580(584.54) | Grad Norm 2.1845(4.8047) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn50_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 50 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
