{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_run1_2/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_run1_2', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 23110 | Time 26.1366(26.7877) | Bit/dim 3.4568(3.4693) | Xent 0.0052(0.0143) | Loss 3.4594(3.4765) | Error 0.0011(0.0045) Steps 1102(1104.15) | Grad Norm 0.6608(1.0343) | Total Time 14.00(14.00)\n",
      "Iter 23120 | Time 25.9469(26.7207) | Bit/dim 3.4553(3.4662) | Xent 0.0120(0.0124) | Loss 3.4613(3.4724) | Error 0.0033(0.0039) Steps 1096(1103.59) | Grad Norm 0.5387(0.9131) | Total Time 14.00(14.00)\n",
      "Iter 23130 | Time 27.2214(26.7567) | Bit/dim 3.4657(3.4656) | Xent 0.0133(0.0113) | Loss 3.4724(3.4712) | Error 0.0067(0.0038) Steps 1090(1103.91) | Grad Norm 0.6491(0.8132) | Total Time 14.00(14.00)\n",
      "Iter 23140 | Time 26.7067(26.7436) | Bit/dim 3.4487(3.4615) | Xent 0.0079(0.0104) | Loss 3.4527(3.4667) | Error 0.0033(0.0036) Steps 1102(1102.46) | Grad Norm 0.4963(0.7199) | Total Time 14.00(14.00)\n",
      "Iter 23150 | Time 27.7214(26.6949) | Bit/dim 3.4825(3.4618) | Xent 0.0113(0.0090) | Loss 3.4882(3.4663) | Error 0.0033(0.0030) Steps 1108(1099.46) | Grad Norm 0.6020(0.6235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 136.4615, Epoch Time 1645.5827(1546.2071), Bit/dim 3.4591(best: inf), Xent 2.9198, Loss 4.9190, Error 0.3529(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23160 | Time 25.8815(26.6491) | Bit/dim 3.4939(3.4604) | Xent 0.0114(0.0080) | Loss 3.4996(3.4644) | Error 0.0033(0.0026) Steps 1096(1099.52) | Grad Norm 0.5451(0.5488) | Total Time 14.00(14.00)\n",
      "Iter 23170 | Time 26.0555(26.6689) | Bit/dim 3.4822(3.4584) | Xent 0.0041(0.0069) | Loss 3.4842(3.4618) | Error 0.0022(0.0023) Steps 1102(1098.88) | Grad Norm 0.2890(0.4787) | Total Time 14.00(14.00)\n",
      "Iter 23180 | Time 26.7192(26.7127) | Bit/dim 3.4225(3.4554) | Xent 0.0018(0.0064) | Loss 3.4234(3.4586) | Error 0.0000(0.0020) Steps 1120(1100.60) | Grad Norm 0.1743(0.4304) | Total Time 14.00(14.00)\n",
      "Iter 23190 | Time 26.1254(26.6868) | Bit/dim 3.4552(3.4533) | Xent 0.0027(0.0058) | Loss 3.4566(3.4562) | Error 0.0011(0.0018) Steps 1102(1100.68) | Grad Norm 0.2337(0.4014) | Total Time 14.00(14.00)\n",
      "Iter 23200 | Time 26.2526(26.6470) | Bit/dim 3.4553(3.4533) | Xent 0.0023(0.0053) | Loss 3.4565(3.4560) | Error 0.0000(0.0016) Steps 1066(1098.92) | Grad Norm 0.2392(0.3716) | Total Time 14.00(14.00)\n",
      "Iter 23210 | Time 26.2366(26.5615) | Bit/dim 3.4827(3.4549) | Xent 0.0066(0.0054) | Loss 3.4860(3.4576) | Error 0.0022(0.0016) Steps 1102(1097.75) | Grad Norm 0.3635(0.3625) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 127.1563, Epoch Time 1609.0368(1548.0920), Bit/dim 3.4569(best: 3.4591), Xent 2.8798, Loss 4.8968, Error 0.3530(best: 0.3529)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23220 | Time 25.9401(26.5944) | Bit/dim 3.4413(3.4533) | Xent 0.0025(0.0051) | Loss 3.4426(3.4558) | Error 0.0000(0.0015) Steps 1090(1095.24) | Grad Norm 0.2394(0.3579) | Total Time 14.00(14.00)\n",
      "Iter 23230 | Time 26.0613(26.7021) | Bit/dim 3.4679(3.4546) | Xent 0.0051(0.0046) | Loss 3.4705(3.4569) | Error 0.0022(0.0013) Steps 1114(1095.61) | Grad Norm 0.3259(0.3292) | Total Time 14.00(14.00)\n",
      "Iter 23240 | Time 27.4754(26.6505) | Bit/dim 3.4493(3.4544) | Xent 0.0026(0.0044) | Loss 3.4506(3.4566) | Error 0.0000(0.0012) Steps 1114(1095.18) | Grad Norm 0.1986(0.3199) | Total Time 14.00(14.00)\n",
      "Iter 23250 | Time 26.2305(26.7104) | Bit/dim 3.4169(3.4542) | Xent 0.0059(0.0042) | Loss 3.4199(3.4563) | Error 0.0022(0.0011) Steps 1090(1095.73) | Grad Norm 0.4311(0.3161) | Total Time 14.00(14.00)\n",
      "Iter 23260 | Time 27.1022(26.8109) | Bit/dim 3.4487(3.4536) | Xent 0.0022(0.0043) | Loss 3.4498(3.4558) | Error 0.0000(0.0011) Steps 1096(1096.75) | Grad Norm 0.2154(0.3183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 126.6205, Epoch Time 1620.6567(1550.2689), Bit/dim 3.4556(best: 3.4569), Xent 2.8838, Loss 4.8975, Error 0.3543(best: 0.3529)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23270 | Time 26.1036(26.7775) | Bit/dim 3.4468(3.4512) | Xent 0.0030(0.0043) | Loss 3.4482(3.4533) | Error 0.0011(0.0012) Steps 1102(1096.60) | Grad Norm 0.4612(0.3133) | Total Time 14.00(14.00)\n",
      "Iter 23280 | Time 26.8235(26.7738) | Bit/dim 3.4529(3.4504) | Xent 0.0043(0.0040) | Loss 3.4551(3.4523) | Error 0.0011(0.0011) Steps 1120(1096.72) | Grad Norm 0.2884(0.2964) | Total Time 14.00(14.00)\n",
      "Iter 23290 | Time 25.6254(26.7161) | Bit/dim 3.4571(3.4477) | Xent 0.0074(0.0040) | Loss 3.4608(3.4497) | Error 0.0022(0.0010) Steps 1096(1096.01) | Grad Norm 0.4573(0.2918) | Total Time 14.00(14.00)\n",
      "Iter 23300 | Time 26.4238(26.7544) | Bit/dim 3.4406(3.4509) | Xent 0.0025(0.0037) | Loss 3.4419(3.4528) | Error 0.0000(0.0010) Steps 1096(1095.62) | Grad Norm 0.1840(0.2914) | Total Time 14.00(14.00)\n",
      "Iter 23310 | Time 26.0426(26.7435) | Bit/dim 3.4663(3.4523) | Xent 0.0054(0.0037) | Loss 3.4690(3.4542) | Error 0.0033(0.0010) Steps 1078(1095.48) | Grad Norm 0.3360(0.2832) | Total Time 14.00(14.00)\n",
      "Iter 23320 | Time 26.2791(26.7258) | Bit/dim 3.4576(3.4525) | Xent 0.0023(0.0037) | Loss 3.4587(3.4543) | Error 0.0000(0.0010) Steps 1096(1094.61) | Grad Norm 0.2288(0.2837) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 125.9326, Epoch Time 1613.3533(1552.1615), Bit/dim 3.4545(best: 3.4556), Xent 2.8728, Loss 4.8909, Error 0.3507(best: 0.3529)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23330 | Time 27.4295(26.8099) | Bit/dim 3.4939(3.4523) | Xent 0.0012(0.0035) | Loss 3.4945(3.4541) | Error 0.0000(0.0009) Steps 1078(1093.15) | Grad Norm 0.1161(0.2745) | Total Time 14.00(14.00)\n",
      "Iter 23340 | Time 26.8161(26.7770) | Bit/dim 3.4348(3.4511) | Xent 0.0028(0.0037) | Loss 3.4362(3.4530) | Error 0.0000(0.0008) Steps 1090(1091.66) | Grad Norm 0.2609(0.2751) | Total Time 14.00(14.00)\n",
      "Iter 23350 | Time 26.3007(26.7353) | Bit/dim 3.4263(3.4481) | Xent 0.0059(0.0037) | Loss 3.4292(3.4500) | Error 0.0011(0.0009) Steps 1096(1092.82) | Grad Norm 0.3416(0.2792) | Total Time 14.00(14.00)\n",
      "Iter 23360 | Time 26.3522(26.7750) | Bit/dim 3.4925(3.4525) | Xent 0.0056(0.0036) | Loss 3.4953(3.4543) | Error 0.0022(0.0009) Steps 1078(1093.40) | Grad Norm 0.3207(0.2756) | Total Time 14.00(14.00)\n",
      "Iter 23370 | Time 26.3012(26.7303) | Bit/dim 3.4361(3.4506) | Xent 0.0041(0.0039) | Loss 3.4382(3.4526) | Error 0.0011(0.0010) Steps 1090(1093.27) | Grad Norm 0.2940(0.2904) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 125.8923, Epoch Time 1614.9940(1554.0464), Bit/dim 3.4547(best: 3.4545), Xent 2.8868, Loss 4.8980, Error 0.3562(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23380 | Time 25.6191(26.6597) | Bit/dim 3.4769(3.4520) | Xent 0.0017(0.0038) | Loss 3.4777(3.4539) | Error 0.0000(0.0010) Steps 1090(1093.29) | Grad Norm 0.1547(0.2867) | Total Time 14.00(14.00)\n",
      "Iter 23390 | Time 27.8936(26.6931) | Bit/dim 3.4259(3.4507) | Xent 0.0017(0.0035) | Loss 3.4268(3.4525) | Error 0.0000(0.0008) Steps 1102(1093.62) | Grad Norm 0.1396(0.2661) | Total Time 14.00(14.00)\n",
      "Iter 23400 | Time 27.5659(26.7669) | Bit/dim 3.4478(3.4502) | Xent 0.0036(0.0036) | Loss 3.4496(3.4520) | Error 0.0011(0.0008) Steps 1078(1091.86) | Grad Norm 0.2717(0.2697) | Total Time 14.00(14.00)\n",
      "Iter 23410 | Time 26.7268(26.8789) | Bit/dim 3.4345(3.4498) | Xent 0.0037(0.0033) | Loss 3.4364(3.4515) | Error 0.0011(0.0007) Steps 1114(1094.29) | Grad Norm 0.2702(0.2537) | Total Time 14.00(14.00)\n",
      "Iter 23420 | Time 26.1471(26.7923) | Bit/dim 3.4364(3.4486) | Xent 0.0029(0.0034) | Loss 3.4378(3.4503) | Error 0.0011(0.0007) Steps 1084(1094.22) | Grad Norm 0.1994(0.2523) | Total Time 14.00(14.00)\n",
      "Iter 23430 | Time 26.6600(26.7549) | Bit/dim 3.4433(3.4498) | Xent 0.0046(0.0036) | Loss 3.4456(3.4516) | Error 0.0011(0.0008) Steps 1090(1093.61) | Grad Norm 0.4125(0.2641) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 125.7664, Epoch Time 1617.3279(1555.9449), Bit/dim 3.4530(best: 3.4545), Xent 2.8671, Loss 4.8866, Error 0.3542(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23440 | Time 26.6627(26.6514) | Bit/dim 3.4470(3.4504) | Xent 0.0014(0.0034) | Loss 3.4477(3.4521) | Error 0.0000(0.0007) Steps 1120(1094.22) | Grad Norm 0.1616(0.2567) | Total Time 14.00(14.00)\n",
      "Iter 23450 | Time 27.0501(26.6297) | Bit/dim 3.4362(3.4477) | Xent 0.0015(0.0035) | Loss 3.4369(3.4494) | Error 0.0000(0.0007) Steps 1102(1092.99) | Grad Norm 0.1300(0.2488) | Total Time 14.00(14.00)\n",
      "Iter 23460 | Time 27.2513(26.7187) | Bit/dim 3.4631(3.4494) | Xent 0.0030(0.0032) | Loss 3.4646(3.4510) | Error 0.0011(0.0007) Steps 1084(1093.51) | Grad Norm 0.2347(0.2341) | Total Time 14.00(14.00)\n",
      "Iter 23470 | Time 26.1844(26.6248) | Bit/dim 3.4703(3.4501) | Xent 0.0050(0.0032) | Loss 3.4728(3.4517) | Error 0.0011(0.0007) Steps 1078(1093.04) | Grad Norm 0.4287(0.2389) | Total Time 14.00(14.00)\n",
      "Iter 23480 | Time 27.2684(26.7055) | Bit/dim 3.4498(3.4500) | Xent 0.0025(0.0032) | Loss 3.4510(3.4516) | Error 0.0000(0.0007) Steps 1102(1092.60) | Grad Norm 0.2474(0.2478) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 125.4010, Epoch Time 1607.5708(1557.4937), Bit/dim 3.4530(best: 3.4530), Xent 2.8592, Loss 4.8826, Error 0.3548(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23490 | Time 27.2589(26.6802) | Bit/dim 3.4148(3.4503) | Xent 0.0039(0.0034) | Loss 3.4167(3.4520) | Error 0.0011(0.0009) Steps 1132(1092.57) | Grad Norm 0.2987(0.2674) | Total Time 14.00(14.00)\n",
      "Iter 23500 | Time 27.0480(26.7558) | Bit/dim 3.4433(3.4497) | Xent 0.0012(0.0033) | Loss 3.4439(3.4514) | Error 0.0000(0.0009) Steps 1084(1092.25) | Grad Norm 0.1310(0.2698) | Total Time 14.00(14.00)\n",
      "Iter 23510 | Time 26.2907(26.7763) | Bit/dim 3.4470(3.4497) | Xent 0.0034(0.0031) | Loss 3.4487(3.4513) | Error 0.0011(0.0008) Steps 1108(1092.15) | Grad Norm 0.2802(0.2542) | Total Time 14.00(14.00)\n",
      "Iter 23520 | Time 26.1289(26.7200) | Bit/dim 3.4704(3.4495) | Xent 0.0055(0.0032) | Loss 3.4732(3.4511) | Error 0.0011(0.0008) Steps 1102(1092.16) | Grad Norm 0.3156(0.2542) | Total Time 14.00(14.00)\n",
      "Iter 23530 | Time 26.5006(26.6773) | Bit/dim 3.4487(3.4481) | Xent 0.0022(0.0032) | Loss 3.4498(3.4497) | Error 0.0000(0.0007) Steps 1084(1091.62) | Grad Norm 0.1872(0.2516) | Total Time 14.00(14.00)\n",
      "Iter 23540 | Time 26.6770(26.5707) | Bit/dim 3.4413(3.4493) | Xent 0.0032(0.0032) | Loss 3.4429(3.4509) | Error 0.0011(0.0007) Steps 1096(1090.42) | Grad Norm 0.2717(0.2476) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 125.9700, Epoch Time 1609.5636(1559.0558), Bit/dim 3.4524(best: 3.4530), Xent 2.8518, Loss 4.8783, Error 0.3552(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23550 | Time 27.5929(26.5605) | Bit/dim 3.4473(3.4485) | Xent 0.0022(0.0032) | Loss 3.4484(3.4501) | Error 0.0000(0.0006) Steps 1084(1090.81) | Grad Norm 0.1712(0.2470) | Total Time 14.00(14.00)\n",
      "Iter 23560 | Time 26.8124(26.5749) | Bit/dim 3.4024(3.4481) | Xent 0.0019(0.0034) | Loss 3.4034(3.4498) | Error 0.0000(0.0006) Steps 1108(1092.46) | Grad Norm 0.1837(0.2482) | Total Time 14.00(14.00)\n",
      "Iter 23570 | Time 26.0299(26.6161) | Bit/dim 3.4865(3.4485) | Xent 0.0018(0.0032) | Loss 3.4874(3.4501) | Error 0.0000(0.0005) Steps 1096(1090.94) | Grad Norm 0.1525(0.2385) | Total Time 14.00(14.00)\n",
      "Iter 23580 | Time 26.9528(26.5678) | Bit/dim 3.4564(3.4484) | Xent 0.0044(0.0031) | Loss 3.4586(3.4500) | Error 0.0022(0.0005) Steps 1102(1091.15) | Grad Norm 0.6613(0.2516) | Total Time 14.00(14.00)\n",
      "Iter 23590 | Time 26.2550(26.6097) | Bit/dim 3.4213(3.4488) | Xent 0.0019(0.0029) | Loss 3.4222(3.4503) | Error 0.0000(0.0005) Steps 1084(1092.41) | Grad Norm 0.1706(0.2413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 125.6395, Epoch Time 1607.5070(1560.5093), Bit/dim 3.4519(best: 3.4524), Xent 2.8747, Loss 4.8892, Error 0.3587(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23600 | Time 27.0852(26.6557) | Bit/dim 3.4445(3.4496) | Xent 0.0026(0.0030) | Loss 3.4458(3.4511) | Error 0.0000(0.0005) Steps 1138(1094.71) | Grad Norm 0.2233(0.2465) | Total Time 14.00(14.00)\n",
      "Iter 23610 | Time 25.7458(26.6355) | Bit/dim 3.4418(3.4491) | Xent 0.0009(0.0029) | Loss 3.4422(3.4505) | Error 0.0000(0.0004) Steps 1072(1093.47) | Grad Norm 0.1080(0.2385) | Total Time 14.00(14.00)\n",
      "Iter 23620 | Time 26.5113(26.6140) | Bit/dim 3.4480(3.4519) | Xent 0.0012(0.0027) | Loss 3.4486(3.4532) | Error 0.0000(0.0004) Steps 1072(1091.56) | Grad Norm 0.1282(0.2244) | Total Time 14.00(14.00)\n",
      "Iter 23630 | Time 27.1512(26.6418) | Bit/dim 3.4272(3.4485) | Xent 0.0034(0.0027) | Loss 3.4290(3.4498) | Error 0.0000(0.0004) Steps 1084(1090.23) | Grad Norm 0.2180(0.2185) | Total Time 14.00(14.00)\n",
      "Iter 23640 | Time 26.0349(26.6960) | Bit/dim 3.4044(3.4470) | Xent 0.0024(0.0026) | Loss 3.4056(3.4483) | Error 0.0011(0.0004) Steps 1096(1090.80) | Grad Norm 0.1981(0.2083) | Total Time 14.00(14.00)\n",
      "Iter 23650 | Time 26.4152(26.6175) | Bit/dim 3.4559(3.4476) | Xent 0.0045(0.0027) | Loss 3.4581(3.4490) | Error 0.0011(0.0005) Steps 1054(1090.66) | Grad Norm 0.4027(0.2163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 125.8059, Epoch Time 1608.9385(1561.9622), Bit/dim 3.4523(best: 3.4519), Xent 2.8636, Loss 4.8841, Error 0.3579(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23660 | Time 27.8201(26.6846) | Bit/dim 3.4355(3.4474) | Xent 0.0012(0.0027) | Loss 3.4361(3.4487) | Error 0.0000(0.0005) Steps 1084(1090.69) | Grad Norm 0.1239(0.2093) | Total Time 14.00(14.00)\n",
      "Iter 23670 | Time 25.9310(26.6863) | Bit/dim 3.4844(3.4486) | Xent 0.0042(0.0027) | Loss 3.4865(3.4499) | Error 0.0011(0.0005) Steps 1066(1090.20) | Grad Norm 0.3014(0.2106) | Total Time 14.00(14.00)\n",
      "Iter 23680 | Time 26.9990(26.6319) | Bit/dim 3.4392(3.4483) | Xent 0.0022(0.0026) | Loss 3.4402(3.4496) | Error 0.0000(0.0004) Steps 1108(1091.44) | Grad Norm 0.2181(0.2059) | Total Time 14.00(14.00)\n",
      "Iter 23690 | Time 26.3313(26.5917) | Bit/dim 3.4031(3.4441) | Xent 0.0032(0.0026) | Loss 3.4047(3.4454) | Error 0.0011(0.0004) Steps 1066(1090.86) | Grad Norm 0.2626(0.2171) | Total Time 14.00(14.00)\n",
      "Iter 23700 | Time 26.5304(26.5702) | Bit/dim 3.4213(3.4467) | Xent 0.0020(0.0025) | Loss 3.4223(3.4480) | Error 0.0000(0.0003) Steps 1102(1092.58) | Grad Norm 0.1799(0.2230) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 125.8073, Epoch Time 1605.2530(1563.2609), Bit/dim 3.4514(best: 3.4519), Xent 2.8635, Loss 4.8832, Error 0.3588(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23710 | Time 26.7609(26.5407) | Bit/dim 3.4168(3.4496) | Xent 0.0029(0.0027) | Loss 3.4183(3.4509) | Error 0.0000(0.0005) Steps 1102(1093.28) | Grad Norm 0.2060(0.2280) | Total Time 14.00(14.00)\n",
      "Iter 23720 | Time 26.5148(26.6099) | Bit/dim 3.4341(3.4486) | Xent 0.0033(0.0031) | Loss 3.4357(3.4501) | Error 0.0011(0.0006) Steps 1102(1093.45) | Grad Norm 0.2713(0.2482) | Total Time 14.00(14.00)\n",
      "Iter 23730 | Time 26.7712(26.5756) | Bit/dim 3.4479(3.4495) | Xent 0.0031(0.0028) | Loss 3.4495(3.4509) | Error 0.0011(0.0005) Steps 1096(1092.37) | Grad Norm 0.4058(0.2375) | Total Time 14.00(14.00)\n",
      "Iter 23740 | Time 26.4976(26.6687) | Bit/dim 3.4742(3.4470) | Xent 0.0064(0.0027) | Loss 3.4774(3.4484) | Error 0.0022(0.0005) Steps 1078(1093.56) | Grad Norm 0.4164(0.2284) | Total Time 14.00(14.00)\n",
      "Iter 23750 | Time 26.5745(26.7102) | Bit/dim 3.4428(3.4467) | Xent 0.0021(0.0027) | Loss 3.4439(3.4481) | Error 0.0000(0.0005) Steps 1114(1097.21) | Grad Norm 0.1495(0.2215) | Total Time 14.00(14.00)\n",
      "Iter 23760 | Time 26.7796(26.6945) | Bit/dim 3.4698(3.4476) | Xent 0.0015(0.0026) | Loss 3.4705(3.4490) | Error 0.0000(0.0005) Steps 1078(1092.43) | Grad Norm 0.1551(0.2211) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 125.9206, Epoch Time 1614.1369(1564.7872), Bit/dim 3.4517(best: 3.4514), Xent 2.8664, Loss 4.8849, Error 0.3555(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23770 | Time 26.3621(26.7633) | Bit/dim 3.4345(3.4476) | Xent 0.0020(0.0026) | Loss 3.4354(3.4489) | Error 0.0000(0.0005) Steps 1090(1090.65) | Grad Norm 0.1582(0.2212) | Total Time 14.00(14.00)\n",
      "Iter 23780 | Time 27.6897(26.8076) | Bit/dim 3.4442(3.4468) | Xent 0.0012(0.0025) | Loss 3.4448(3.4480) | Error 0.0000(0.0004) Steps 1072(1089.08) | Grad Norm 0.1360(0.2151) | Total Time 14.00(14.00)\n",
      "Iter 23790 | Time 27.1677(26.7864) | Bit/dim 3.4954(3.4501) | Xent 0.0017(0.0026) | Loss 3.4963(3.4514) | Error 0.0000(0.0005) Steps 1072(1088.80) | Grad Norm 0.1770(0.2143) | Total Time 14.00(14.00)\n",
      "Iter 23800 | Time 25.9606(26.8440) | Bit/dim 3.4603(3.4468) | Xent 0.0017(0.0027) | Loss 3.4611(3.4481) | Error 0.0000(0.0005) Steps 1084(1091.51) | Grad Norm 0.2580(0.2436) | Total Time 14.00(14.00)\n",
      "Iter 23810 | Time 26.5944(26.8022) | Bit/dim 3.4554(3.4467) | Xent 0.0040(0.0025) | Loss 3.4573(3.4479) | Error 0.0011(0.0005) Steps 1084(1090.88) | Grad Norm 0.3738(0.2434) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 126.0313, Epoch Time 1622.4632(1566.5175), Bit/dim 3.4519(best: 3.4514), Xent 2.8537, Loss 4.8787, Error 0.3559(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23820 | Time 26.1813(26.7999) | Bit/dim 3.4335(3.4467) | Xent 0.0017(0.0024) | Loss 3.4344(3.4479) | Error 0.0000(0.0004) Steps 1090(1092.70) | Grad Norm 0.1486(0.2257) | Total Time 14.00(14.00)\n",
      "Iter 23830 | Time 25.9599(26.7743) | Bit/dim 3.4878(3.4489) | Xent 0.0028(0.0025) | Loss 3.4892(3.4501) | Error 0.0000(0.0004) Steps 1060(1092.22) | Grad Norm 0.1797(0.2209) | Total Time 14.00(14.00)\n",
      "Iter 23840 | Time 26.8546(26.7329) | Bit/dim 3.4207(3.4469) | Xent 0.0049(0.0027) | Loss 3.4231(3.4483) | Error 0.0011(0.0005) Steps 1072(1093.29) | Grad Norm 0.4373(0.2413) | Total Time 14.00(14.00)\n",
      "Iter 23850 | Time 26.2690(26.7323) | Bit/dim 3.4067(3.4459) | Xent 0.0024(0.0027) | Loss 3.4079(3.4472) | Error 0.0000(0.0004) Steps 1102(1092.95) | Grad Norm 0.2477(0.2530) | Total Time 14.00(14.00)\n",
      "Iter 23860 | Time 26.8339(26.6953) | Bit/dim 3.4715(3.4479) | Xent 0.0009(0.0026) | Loss 3.4720(3.4492) | Error 0.0000(0.0004) Steps 1072(1091.85) | Grad Norm 0.1272(0.2476) | Total Time 14.00(14.00)\n",
      "Iter 23870 | Time 26.5863(26.6217) | Bit/dim 3.4600(3.4474) | Xent 0.0017(0.0023) | Loss 3.4609(3.4486) | Error 0.0000(0.0003) Steps 1102(1093.34) | Grad Norm 0.1223(0.2311) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 125.4193, Epoch Time 1606.4687(1567.7160), Bit/dim 3.4505(best: 3.4514), Xent 2.8551, Loss 4.8780, Error 0.3582(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23880 | Time 26.5811(26.6282) | Bit/dim 3.4381(3.4455) | Xent 0.0014(0.0022) | Loss 3.4388(3.4466) | Error 0.0000(0.0003) Steps 1096(1091.11) | Grad Norm 0.1658(0.2230) | Total Time 14.00(14.00)\n",
      "Iter 23890 | Time 27.1087(26.5802) | Bit/dim 3.4353(3.4454) | Xent 0.0014(0.0024) | Loss 3.4360(3.4466) | Error 0.0000(0.0004) Steps 1108(1090.08) | Grad Norm 0.2022(0.2413) | Total Time 14.00(14.00)\n",
      "Iter 23900 | Time 26.2333(26.6460) | Bit/dim 3.4035(3.4418) | Xent 0.0040(0.0025) | Loss 3.4055(3.4430) | Error 0.0011(0.0004) Steps 1090(1092.25) | Grad Norm 0.3116(0.2451) | Total Time 14.00(14.00)\n",
      "Iter 23910 | Time 26.0799(26.6364) | Bit/dim 3.4711(3.4471) | Xent 0.0036(0.0027) | Loss 3.4729(3.4484) | Error 0.0011(0.0005) Steps 1084(1090.51) | Grad Norm 0.3740(0.2659) | Total Time 14.00(14.00)\n",
      "Iter 23920 | Time 26.4384(26.6090) | Bit/dim 3.4565(3.4484) | Xent 0.0024(0.0027) | Loss 3.4577(3.4498) | Error 0.0000(0.0004) Steps 1114(1091.37) | Grad Norm 0.2627(0.2628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 124.7246, Epoch Time 1609.2795(1568.9629), Bit/dim 3.4503(best: 3.4505), Xent 2.8693, Loss 4.8849, Error 0.3572(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23930 | Time 26.7182(26.6288) | Bit/dim 3.4415(3.4488) | Xent 0.0027(0.0026) | Loss 3.4429(3.4501) | Error 0.0000(0.0004) Steps 1108(1090.50) | Grad Norm 0.2240(0.2520) | Total Time 14.00(14.00)\n",
      "Iter 23940 | Time 26.3837(26.7232) | Bit/dim 3.4429(3.4484) | Xent 0.0020(0.0025) | Loss 3.4439(3.4496) | Error 0.0000(0.0004) Steps 1096(1092.65) | Grad Norm 0.1702(0.2398) | Total Time 14.00(14.00)\n",
      "Iter 23950 | Time 27.2684(26.7368) | Bit/dim 3.4283(3.4486) | Xent 0.0036(0.0024) | Loss 3.4301(3.4498) | Error 0.0000(0.0004) Steps 1108(1093.45) | Grad Norm 0.3132(0.2330) | Total Time 14.00(14.00)\n",
      "Iter 23960 | Time 26.5968(26.7245) | Bit/dim 3.4824(3.4488) | Xent 0.0023(0.0026) | Loss 3.4836(3.4501) | Error 0.0000(0.0004) Steps 1084(1092.31) | Grad Norm 0.1641(0.2385) | Total Time 14.00(14.00)\n",
      "Iter 23970 | Time 26.5085(26.7181) | Bit/dim 3.4358(3.4485) | Xent 0.0012(0.0026) | Loss 3.4364(3.4498) | Error 0.0000(0.0004) Steps 1108(1091.13) | Grad Norm 0.1276(0.2313) | Total Time 14.00(14.00)\n",
      "Iter 23980 | Time 26.1309(26.7249) | Bit/dim 3.4359(3.4453) | Xent 0.0024(0.0025) | Loss 3.4371(3.4465) | Error 0.0000(0.0003) Steps 1084(1091.89) | Grad Norm 0.2034(0.2286) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 125.7757, Epoch Time 1614.5163(1570.3295), Bit/dim 3.4506(best: 3.4503), Xent 2.8718, Loss 4.8865, Error 0.3620(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 23990 | Time 26.5771(26.6777) | Bit/dim 3.4320(3.4420) | Xent 0.0025(0.0026) | Loss 3.4332(3.4433) | Error 0.0011(0.0005) Steps 1072(1093.84) | Grad Norm 0.2484(0.2311) | Total Time 14.00(14.00)\n",
      "Iter 24000 | Time 26.9052(26.6939) | Bit/dim 3.4518(3.4450) | Xent 0.0017(0.0024) | Loss 3.4526(3.4462) | Error 0.0000(0.0004) Steps 1138(1093.48) | Grad Norm 0.1511(0.2207) | Total Time 14.00(14.00)\n",
      "Iter 24010 | Time 25.9149(26.6483) | Bit/dim 3.4413(3.4461) | Xent 0.0025(0.0023) | Loss 3.4425(3.4472) | Error 0.0000(0.0003) Steps 1084(1092.51) | Grad Norm 0.1747(0.2214) | Total Time 14.00(14.00)\n",
      "Iter 24020 | Time 27.1021(26.6082) | Bit/dim 3.4341(3.4457) | Xent 0.0023(0.0022) | Loss 3.4352(3.4468) | Error 0.0000(0.0003) Steps 1114(1093.37) | Grad Norm 0.1685(0.2125) | Total Time 14.00(14.00)\n",
      "Iter 24030 | Time 26.5767(26.6435) | Bit/dim 3.4459(3.4437) | Xent 0.0019(0.0023) | Loss 3.4469(3.4449) | Error 0.0000(0.0003) Steps 1078(1092.71) | Grad Norm 0.1573(0.2088) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 125.3631, Epoch Time 1605.4131(1571.3820), Bit/dim 3.4499(best: 3.4503), Xent 2.8752, Loss 4.8875, Error 0.3582(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24040 | Time 26.8409(26.5891) | Bit/dim 3.4996(3.4477) | Xent 0.0014(0.0023) | Loss 3.5003(3.4489) | Error 0.0000(0.0003) Steps 1102(1092.40) | Grad Norm 0.1111(0.1979) | Total Time 14.00(14.00)\n",
      "Iter 24050 | Time 27.1454(26.6072) | Bit/dim 3.4356(3.4490) | Xent 0.0018(0.0024) | Loss 3.4365(3.4502) | Error 0.0000(0.0004) Steps 1096(1092.18) | Grad Norm 0.1817(0.2034) | Total Time 14.00(14.00)\n",
      "Iter 24060 | Time 26.9574(26.6071) | Bit/dim 3.4477(3.4491) | Xent 0.0009(0.0024) | Loss 3.4482(3.4503) | Error 0.0000(0.0004) Steps 1054(1089.69) | Grad Norm 0.1339(0.2037) | Total Time 14.00(14.00)\n",
      "Iter 24070 | Time 27.6137(26.6838) | Bit/dim 3.4400(3.4459) | Xent 0.0011(0.0024) | Loss 3.4405(3.4471) | Error 0.0000(0.0004) Steps 1096(1089.55) | Grad Norm 0.1185(0.2115) | Total Time 14.00(14.00)\n",
      "Iter 24080 | Time 28.1259(26.8442) | Bit/dim 3.4632(3.4443) | Xent 0.0016(0.0024) | Loss 3.4640(3.4454) | Error 0.0000(0.0004) Steps 1066(1087.02) | Grad Norm 0.2124(0.2120) | Total Time 14.00(14.00)\n",
      "Iter 24090 | Time 25.5536(26.7880) | Bit/dim 3.4352(3.4447) | Xent 0.0026(0.0025) | Loss 3.4365(3.4460) | Error 0.0011(0.0004) Steps 1078(1088.30) | Grad Norm 0.2446(0.2193) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 125.3299, Epoch Time 1616.1298(1572.7244), Bit/dim 3.4497(best: 3.4499), Xent 2.8734, Loss 4.8864, Error 0.3553(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24100 | Time 26.9701(26.7426) | Bit/dim 3.4411(3.4453) | Xent 0.0028(0.0025) | Loss 3.4425(3.4466) | Error 0.0000(0.0004) Steps 1060(1086.40) | Grad Norm 0.2999(0.2276) | Total Time 14.00(14.00)\n",
      "Iter 24110 | Time 26.4942(26.7419) | Bit/dim 3.4411(3.4466) | Xent 0.0018(0.0024) | Loss 3.4420(3.4478) | Error 0.0000(0.0003) Steps 1072(1086.01) | Grad Norm 0.1598(0.2106) | Total Time 14.00(14.00)\n",
      "Iter 24120 | Time 26.0390(26.7251) | Bit/dim 3.4441(3.4461) | Xent 0.0011(0.0023) | Loss 3.4447(3.4472) | Error 0.0000(0.0003) Steps 1078(1088.88) | Grad Norm 0.1270(0.2045) | Total Time 14.00(14.00)\n",
      "Iter 24130 | Time 26.6697(26.5942) | Bit/dim 3.4323(3.4449) | Xent 0.0033(0.0024) | Loss 3.4339(3.4461) | Error 0.0011(0.0005) Steps 1096(1087.17) | Grad Norm 0.2604(0.2189) | Total Time 14.00(14.00)\n",
      "Iter 24140 | Time 26.4461(26.5732) | Bit/dim 3.4221(3.4446) | Xent 0.0035(0.0025) | Loss 3.4239(3.4459) | Error 0.0011(0.0005) Steps 1066(1087.89) | Grad Norm 0.3002(0.2293) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 124.5933, Epoch Time 1604.4339(1573.6757), Bit/dim 3.4501(best: 3.4497), Xent 2.9022, Loss 4.9012, Error 0.3616(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24150 | Time 27.7659(26.6013) | Bit/dim 3.3824(3.4425) | Xent 0.0017(0.0023) | Loss 3.3832(3.4436) | Error 0.0000(0.0005) Steps 1102(1089.06) | Grad Norm 0.1985(0.2243) | Total Time 14.00(14.00)\n",
      "Iter 24160 | Time 27.7127(26.5928) | Bit/dim 3.4665(3.4447) | Xent 0.0031(0.0023) | Loss 3.4681(3.4459) | Error 0.0011(0.0004) Steps 1060(1085.64) | Grad Norm 0.2916(0.2203) | Total Time 14.00(14.00)\n",
      "Iter 24170 | Time 26.1462(26.5848) | Bit/dim 3.4331(3.4439) | Xent 0.0013(0.0021) | Loss 3.4338(3.4449) | Error 0.0000(0.0003) Steps 1120(1087.42) | Grad Norm 0.1326(0.2037) | Total Time 14.00(14.00)\n",
      "Iter 24180 | Time 27.0250(26.7366) | Bit/dim 3.4768(3.4464) | Xent 0.0017(0.0024) | Loss 3.4777(3.4476) | Error 0.0000(0.0004) Steps 1126(1086.72) | Grad Norm 0.1941(0.2235) | Total Time 14.00(14.00)\n",
      "Iter 24190 | Time 27.8033(26.7250) | Bit/dim 3.4537(3.4468) | Xent 0.0037(0.0026) | Loss 3.4555(3.4482) | Error 0.0011(0.0005) Steps 1096(1086.83) | Grad Norm 0.2476(0.2504) | Total Time 14.00(14.00)\n",
      "Iter 24200 | Time 25.4741(26.7350) | Bit/dim 3.4451(3.4451) | Xent 0.0027(0.0026) | Loss 3.4465(3.4464) | Error 0.0000(0.0005) Steps 1084(1088.27) | Grad Norm 0.2345(0.2451) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 126.2019, Epoch Time 1614.3199(1574.8951), Bit/dim 3.4490(best: 3.4497), Xent 2.8773, Loss 4.8876, Error 0.3634(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24210 | Time 26.6632(26.7231) | Bit/dim 3.4279(3.4463) | Xent 0.0010(0.0028) | Loss 3.4284(3.4476) | Error 0.0000(0.0005) Steps 1090(1090.40) | Grad Norm 0.2134(0.2581) | Total Time 14.00(14.00)\n",
      "Iter 24220 | Time 26.6200(26.7905) | Bit/dim 3.3969(3.4437) | Xent 0.0019(0.0026) | Loss 3.3978(3.4450) | Error 0.0000(0.0004) Steps 1090(1090.50) | Grad Norm 0.2192(0.2445) | Total Time 14.00(14.00)\n",
      "Iter 24230 | Time 26.4025(26.7671) | Bit/dim 3.4063(3.4431) | Xent 0.0015(0.0025) | Loss 3.4070(3.4444) | Error 0.0000(0.0004) Steps 1084(1086.64) | Grad Norm 0.1920(0.2501) | Total Time 14.00(14.00)\n",
      "Iter 24240 | Time 26.4592(26.8673) | Bit/dim 3.4545(3.4457) | Xent 0.0013(0.0024) | Loss 3.4552(3.4469) | Error 0.0000(0.0004) Steps 1078(1086.13) | Grad Norm 0.1467(0.2405) | Total Time 14.00(14.00)\n",
      "Iter 24250 | Time 25.5297(26.9006) | Bit/dim 3.4483(3.4458) | Xent 0.0025(0.0023) | Loss 3.4496(3.4470) | Error 0.0000(0.0003) Steps 1078(1087.24) | Grad Norm 0.1850(0.2255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 125.3317, Epoch Time 1622.7306(1576.3301), Bit/dim 3.4488(best: 3.4490), Xent 2.8928, Loss 4.8952, Error 0.3600(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24260 | Time 26.8450(26.8374) | Bit/dim 3.4392(3.4473) | Xent 0.0023(0.0025) | Loss 3.4403(3.4485) | Error 0.0000(0.0003) Steps 1090(1086.62) | Grad Norm 0.2023(0.2304) | Total Time 14.00(14.00)\n",
      "Iter 24270 | Time 27.0187(26.8133) | Bit/dim 3.4374(3.4466) | Xent 0.0016(0.0025) | Loss 3.4382(3.4478) | Error 0.0000(0.0003) Steps 1084(1086.97) | Grad Norm 0.1413(0.2286) | Total Time 14.00(14.00)\n",
      "Iter 24280 | Time 26.2800(26.7079) | Bit/dim 3.4556(3.4467) | Xent 0.0025(0.0025) | Loss 3.4569(3.4480) | Error 0.0011(0.0004) Steps 1084(1087.82) | Grad Norm 0.2034(0.2240) | Total Time 14.00(14.00)\n",
      "Iter 24290 | Time 26.0758(26.7109) | Bit/dim 3.4300(3.4437) | Xent 0.0014(0.0024) | Loss 3.4307(3.4449) | Error 0.0000(0.0003) Steps 1078(1089.27) | Grad Norm 0.1453(0.2167) | Total Time 14.00(14.00)\n",
      "Iter 24300 | Time 27.0588(26.7868) | Bit/dim 3.4302(3.4439) | Xent 0.0016(0.0023) | Loss 3.4310(3.4451) | Error 0.0000(0.0003) Steps 1078(1089.26) | Grad Norm 0.1305(0.2057) | Total Time 14.00(14.00)\n",
      "Iter 24310 | Time 26.6348(26.7631) | Bit/dim 3.4213(3.4450) | Xent 0.0044(0.0023) | Loss 3.4235(3.4461) | Error 0.0022(0.0003) Steps 1096(1087.20) | Grad Norm 0.3768(0.2014) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 124.1655, Epoch Time 1610.1010(1577.3432), Bit/dim 3.4490(best: 3.4488), Xent 2.8764, Loss 4.8872, Error 0.3598(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24320 | Time 26.9759(26.7974) | Bit/dim 3.4490(3.4453) | Xent 0.0031(0.0025) | Loss 3.4506(3.4465) | Error 0.0011(0.0004) Steps 1078(1087.82) | Grad Norm 0.5059(0.2587) | Total Time 14.00(14.00)\n",
      "Iter 24330 | Time 26.4367(26.8443) | Bit/dim 3.4319(3.4463) | Xent 0.0017(0.0024) | Loss 3.4328(3.4475) | Error 0.0000(0.0004) Steps 1096(1088.56) | Grad Norm 0.3107(0.2730) | Total Time 14.00(14.00)\n",
      "Iter 24340 | Time 26.6227(26.8145) | Bit/dim 3.4877(3.4495) | Xent 0.0018(0.0023) | Loss 3.4886(3.4506) | Error 0.0000(0.0003) Steps 1084(1088.54) | Grad Norm 0.1608(0.2591) | Total Time 14.00(14.00)\n",
      "Iter 24350 | Time 25.6238(26.7862) | Bit/dim 3.4669(3.4456) | Xent 0.0020(0.0023) | Loss 3.4679(3.4468) | Error 0.0000(0.0003) Steps 1090(1089.08) | Grad Norm 0.1593(0.2467) | Total Time 14.00(14.00)\n",
      "Iter 24360 | Time 26.7784(26.8312) | Bit/dim 3.4563(3.4447) | Xent 0.0013(0.0022) | Loss 3.4570(3.4457) | Error 0.0000(0.0003) Steps 1114(1090.94) | Grad Norm 0.1238(0.2310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 126.2550, Epoch Time 1618.1610(1578.5678), Bit/dim 3.4484(best: 3.4488), Xent 2.8720, Loss 4.8844, Error 0.3561(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24370 | Time 26.5196(26.7483) | Bit/dim 3.5009(3.4446) | Xent 0.0012(0.0022) | Loss 3.5015(3.4457) | Error 0.0000(0.0003) Steps 1060(1089.18) | Grad Norm 0.1149(0.2205) | Total Time 14.00(14.00)\n",
      "Iter 24380 | Time 26.9295(26.7035) | Bit/dim 3.4529(3.4435) | Xent 0.0019(0.0024) | Loss 3.4539(3.4447) | Error 0.0000(0.0003) Steps 1114(1088.96) | Grad Norm 0.1878(0.2273) | Total Time 14.00(14.00)\n",
      "Iter 24390 | Time 26.3915(26.7679) | Bit/dim 3.4412(3.4418) | Xent 0.0018(0.0025) | Loss 3.4421(3.4430) | Error 0.0000(0.0004) Steps 1090(1089.90) | Grad Norm 0.2011(0.2522) | Total Time 14.00(14.00)\n",
      "Iter 24400 | Time 26.8737(26.7437) | Bit/dim 3.4754(3.4445) | Xent 0.0009(0.0025) | Loss 3.4758(3.4458) | Error 0.0000(0.0004) Steps 1096(1090.37) | Grad Norm 0.1237(0.2473) | Total Time 14.00(14.00)\n",
      "Iter 24410 | Time 25.5742(26.6279) | Bit/dim 3.4526(3.4419) | Xent 0.0028(0.0024) | Loss 3.4540(3.4431) | Error 0.0000(0.0003) Steps 1096(1088.90) | Grad Norm 0.2141(0.2330) | Total Time 14.00(14.00)\n",
      "Iter 24420 | Time 27.6025(26.6882) | Bit/dim 3.4975(3.4453) | Xent 0.0030(0.0024) | Loss 3.4989(3.4465) | Error 0.0000(0.0003) Steps 1078(1088.38) | Grad Norm 0.2487(0.2368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 125.8775, Epoch Time 1610.7190(1579.5323), Bit/dim 3.4491(best: 3.4484), Xent 2.8906, Loss 4.8944, Error 0.3638(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24430 | Time 27.3380(26.6401) | Bit/dim 3.4306(3.4448) | Xent 0.0011(0.0023) | Loss 3.4311(3.4460) | Error 0.0000(0.0003) Steps 1060(1088.43) | Grad Norm 0.1201(0.2283) | Total Time 14.00(14.00)\n",
      "Iter 24440 | Time 26.6820(26.6684) | Bit/dim 3.4476(3.4441) | Xent 0.0023(0.0022) | Loss 3.4488(3.4452) | Error 0.0000(0.0003) Steps 1078(1087.54) | Grad Norm 0.1700(0.2187) | Total Time 14.00(14.00)\n",
      "Iter 24450 | Time 26.7769(26.5697) | Bit/dim 3.4466(3.4443) | Xent 0.0011(0.0022) | Loss 3.4472(3.4454) | Error 0.0000(0.0003) Steps 1072(1087.14) | Grad Norm 0.1383(0.2132) | Total Time 14.00(14.00)\n",
      "Iter 24460 | Time 26.3434(26.5636) | Bit/dim 3.4249(3.4425) | Xent 0.0049(0.0022) | Loss 3.4273(3.4436) | Error 0.0022(0.0003) Steps 1090(1087.03) | Grad Norm 0.6074(0.2173) | Total Time 14.00(14.00)\n",
      "Iter 24470 | Time 27.1188(26.5935) | Bit/dim 3.4676(3.4443) | Xent 0.0013(0.0022) | Loss 3.4682(3.4454) | Error 0.0000(0.0003) Steps 1108(1088.56) | Grad Norm 0.1551(0.2236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 125.8749, Epoch Time 1604.0157(1580.2668), Bit/dim 3.4495(best: 3.4484), Xent 2.9072, Loss 4.9031, Error 0.3614(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24480 | Time 26.9479(26.5863) | Bit/dim 3.4673(3.4464) | Xent 0.0014(0.0022) | Loss 3.4680(3.4476) | Error 0.0000(0.0003) Steps 1078(1087.44) | Grad Norm 0.1493(0.2233) | Total Time 14.00(14.00)\n",
      "Iter 24490 | Time 26.6936(26.6168) | Bit/dim 3.4247(3.4459) | Xent 0.0020(0.0022) | Loss 3.4256(3.4469) | Error 0.0000(0.0003) Steps 1102(1086.33) | Grad Norm 0.2423(0.2186) | Total Time 14.00(14.00)\n",
      "Iter 24500 | Time 27.3806(26.7365) | Bit/dim 3.4289(3.4428) | Xent 0.0012(0.0021) | Loss 3.4295(3.4439) | Error 0.0000(0.0003) Steps 1096(1087.88) | Grad Norm 0.1441(0.2167) | Total Time 14.00(14.00)\n",
      "Iter 24510 | Time 26.9904(26.6679) | Bit/dim 3.4286(3.4456) | Xent 0.0014(0.0021) | Loss 3.4293(3.4467) | Error 0.0000(0.0003) Steps 1060(1088.17) | Grad Norm 0.1621(0.2110) | Total Time 14.00(14.00)\n",
      "Iter 24520 | Time 26.4531(26.7391) | Bit/dim 3.4361(3.4432) | Xent 0.0025(0.0022) | Loss 3.4373(3.4443) | Error 0.0000(0.0002) Steps 1066(1088.77) | Grad Norm 0.2274(0.2115) | Total Time 14.00(14.00)\n",
      "Iter 24530 | Time 26.4021(26.7033) | Bit/dim 3.4667(3.4442) | Xent 0.0095(0.0023) | Loss 3.4714(3.4453) | Error 0.0011(0.0002) Steps 1090(1089.05) | Grad Norm 0.4326(0.2090) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 125.1474, Epoch Time 1613.8386(1581.2740), Bit/dim 3.4485(best: 3.4484), Xent 2.8925, Loss 4.8947, Error 0.3600(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24540 | Time 26.2791(26.6813) | Bit/dim 3.4726(3.4463) | Xent 0.0018(0.0023) | Loss 3.4735(3.4475) | Error 0.0000(0.0002) Steps 1096(1090.29) | Grad Norm 0.1543(0.2115) | Total Time 14.00(14.00)\n",
      "Iter 24550 | Time 27.2689(26.7604) | Bit/dim 3.4311(3.4453) | Xent 0.0023(0.0023) | Loss 3.4323(3.4465) | Error 0.0011(0.0003) Steps 1096(1092.60) | Grad Norm 0.2431(0.2172) | Total Time 14.00(14.00)\n",
      "Iter 24560 | Time 26.1947(26.6699) | Bit/dim 3.4505(3.4443) | Xent 0.0024(0.0024) | Loss 3.4517(3.4455) | Error 0.0000(0.0004) Steps 1102(1091.52) | Grad Norm 0.2257(0.2380) | Total Time 14.00(14.00)\n",
      "Iter 24570 | Time 25.7250(26.6786) | Bit/dim 3.4295(3.4408) | Xent 0.0016(0.0023) | Loss 3.4303(3.4419) | Error 0.0000(0.0003) Steps 1060(1090.99) | Grad Norm 0.1871(0.2471) | Total Time 14.00(14.00)\n",
      "Iter 24580 | Time 26.5895(26.6989) | Bit/dim 3.4554(3.4431) | Xent 0.0034(0.0022) | Loss 3.4571(3.4441) | Error 0.0011(0.0003) Steps 1096(1090.96) | Grad Norm 0.4088(0.2423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 124.8744, Epoch Time 1609.8874(1582.1324), Bit/dim 3.4476(best: 3.4484), Xent 2.8928, Loss 4.8940, Error 0.3616(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24590 | Time 26.2749(26.6138) | Bit/dim 3.4371(3.4444) | Xent 0.0023(0.0023) | Loss 3.4383(3.4455) | Error 0.0011(0.0003) Steps 1084(1089.26) | Grad Norm 0.2337(0.2477) | Total Time 14.00(14.00)\n",
      "Iter 24600 | Time 27.5025(26.6637) | Bit/dim 3.4603(3.4440) | Xent 0.0026(0.0023) | Loss 3.4616(3.4452) | Error 0.0000(0.0003) Steps 1078(1089.22) | Grad Norm 0.2563(0.2553) | Total Time 14.00(14.00)\n",
      "Iter 24610 | Time 26.7575(26.7069) | Bit/dim 3.4474(3.4424) | Xent 0.0017(0.0022) | Loss 3.4482(3.4435) | Error 0.0000(0.0003) Steps 1084(1088.23) | Grad Norm 0.1676(0.2406) | Total Time 14.00(14.00)\n",
      "Iter 24620 | Time 26.0533(26.7481) | Bit/dim 3.4747(3.4456) | Xent 0.0012(0.0021) | Loss 3.4753(3.4466) | Error 0.0000(0.0003) Steps 1066(1087.85) | Grad Norm 0.1403(0.2268) | Total Time 14.00(14.00)\n",
      "Iter 24630 | Time 26.9317(26.9105) | Bit/dim 3.4354(3.4431) | Xent 0.0015(0.0021) | Loss 3.4362(3.4441) | Error 0.0000(0.0003) Steps 1072(1086.32) | Grad Norm 0.1559(0.2250) | Total Time 14.00(14.00)\n",
      "Iter 24640 | Time 26.2653(26.8094) | Bit/dim 3.4363(3.4445) | Xent 0.0010(0.0020) | Loss 3.4368(3.4456) | Error 0.0000(0.0003) Steps 1078(1085.99) | Grad Norm 0.1093(0.2129) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 126.2563, Epoch Time 1619.1031(1583.2415), Bit/dim 3.4480(best: 3.4476), Xent 2.9237, Loss 4.9099, Error 0.3616(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24650 | Time 26.7608(26.6890) | Bit/dim 3.4573(3.4450) | Xent 0.0014(0.0021) | Loss 3.4580(3.4461) | Error 0.0000(0.0003) Steps 1066(1084.78) | Grad Norm 0.1475(0.2196) | Total Time 14.00(14.00)\n",
      "Iter 24660 | Time 25.6780(26.6985) | Bit/dim 3.4726(3.4459) | Xent 0.0017(0.0023) | Loss 3.4734(3.4470) | Error 0.0000(0.0003) Steps 1072(1083.76) | Grad Norm 0.1473(0.2343) | Total Time 14.00(14.00)\n",
      "Iter 24670 | Time 26.7348(26.6832) | Bit/dim 3.4240(3.4448) | Xent 0.0019(0.0023) | Loss 3.4250(3.4459) | Error 0.0000(0.0003) Steps 1090(1084.72) | Grad Norm 0.1556(0.2310) | Total Time 14.00(14.00)\n",
      "Iter 24680 | Time 26.2452(26.6137) | Bit/dim 3.3938(3.4434) | Xent 0.0012(0.0024) | Loss 3.3944(3.4446) | Error 0.0000(0.0004) Steps 1084(1085.01) | Grad Norm 0.1650(0.2356) | Total Time 14.00(14.00)\n",
      "Iter 24690 | Time 26.6053(26.5753) | Bit/dim 3.4728(3.4444) | Xent 0.0016(0.0024) | Loss 3.4735(3.4456) | Error 0.0000(0.0003) Steps 1078(1083.71) | Grad Norm 0.1392(0.2256) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 125.5993, Epoch Time 1603.0570(1583.8360), Bit/dim 3.4473(best: 3.4476), Xent 2.8975, Loss 4.8960, Error 0.3622(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24700 | Time 26.7180(26.5766) | Bit/dim 3.4575(3.4442) | Xent 0.0017(0.0022) | Loss 3.4584(3.4454) | Error 0.0000(0.0003) Steps 1108(1084.77) | Grad Norm 0.1652(0.2143) | Total Time 14.00(14.00)\n",
      "Iter 24710 | Time 26.2270(26.5268) | Bit/dim 3.4475(3.4446) | Xent 0.0012(0.0023) | Loss 3.4481(3.4458) | Error 0.0000(0.0003) Steps 1084(1085.19) | Grad Norm 0.1301(0.2198) | Total Time 14.00(14.00)\n",
      "Iter 24720 | Time 26.0315(26.4472) | Bit/dim 3.4563(3.4439) | Xent 0.0013(0.0023) | Loss 3.4569(3.4451) | Error 0.0000(0.0004) Steps 1096(1083.75) | Grad Norm 0.1885(0.2216) | Total Time 14.00(14.00)\n",
      "Iter 24730 | Time 27.1955(26.4199) | Bit/dim 3.4416(3.4442) | Xent 0.0020(0.0023) | Loss 3.4426(3.4454) | Error 0.0000(0.0004) Steps 1084(1084.80) | Grad Norm 0.1728(0.2219) | Total Time 14.00(14.00)\n",
      "Iter 24740 | Time 26.7814(26.4808) | Bit/dim 3.4190(3.4428) | Xent 0.0036(0.0023) | Loss 3.4208(3.4439) | Error 0.0000(0.0003) Steps 1072(1085.04) | Grad Norm 0.2492(0.2208) | Total Time 14.00(14.00)\n",
      "Iter 24750 | Time 26.0779(26.5655) | Bit/dim 3.4212(3.4425) | Xent 0.0024(0.0023) | Loss 3.4224(3.4437) | Error 0.0011(0.0005) Steps 1084(1086.94) | Grad Norm 0.2846(0.2357) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 125.2127, Epoch Time 1599.7378(1584.3130), Bit/dim 3.4477(best: 3.4473), Xent 2.9128, Loss 4.9041, Error 0.3625(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24760 | Time 26.9385(26.5906) | Bit/dim 3.4472(3.4415) | Xent 0.0020(0.0022) | Loss 3.4482(3.4426) | Error 0.0000(0.0004) Steps 1096(1088.32) | Grad Norm 0.1975(0.2357) | Total Time 14.00(14.00)\n",
      "Iter 24770 | Time 26.7641(26.6047) | Bit/dim 3.4332(3.4418) | Xent 0.0013(0.0022) | Loss 3.4339(3.4429) | Error 0.0000(0.0004) Steps 1108(1089.01) | Grad Norm 0.1889(0.2342) | Total Time 14.00(14.00)\n",
      "Iter 24780 | Time 27.3068(26.6444) | Bit/dim 3.4342(3.4415) | Xent 0.0028(0.0023) | Loss 3.4356(3.4427) | Error 0.0011(0.0004) Steps 1078(1088.77) | Grad Norm 0.3541(0.2547) | Total Time 14.00(14.00)\n",
      "Iter 24790 | Time 26.9624(26.6987) | Bit/dim 3.4369(3.4426) | Xent 0.0017(0.0025) | Loss 3.4377(3.4439) | Error 0.0000(0.0004) Steps 1084(1088.38) | Grad Norm 0.1942(0.2567) | Total Time 14.00(14.00)\n",
      "Iter 24800 | Time 27.2329(26.7132) | Bit/dim 3.4401(3.4436) | Xent 0.0012(0.0024) | Loss 3.4407(3.4448) | Error 0.0000(0.0004) Steps 1132(1089.13) | Grad Norm 0.1460(0.2425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 125.2181, Epoch Time 1612.0563(1585.1453), Bit/dim 3.4471(best: 3.4473), Xent 2.9107, Loss 4.9025, Error 0.3649(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24810 | Time 26.2743(26.6503) | Bit/dim 3.4409(3.4424) | Xent 0.0014(0.0021) | Loss 3.4416(3.4434) | Error 0.0000(0.0003) Steps 1066(1087.43) | Grad Norm 0.1317(0.2159) | Total Time 14.00(14.00)\n",
      "Iter 24820 | Time 26.5350(26.6378) | Bit/dim 3.4540(3.4436) | Xent 0.0012(0.0022) | Loss 3.4546(3.4447) | Error 0.0000(0.0004) Steps 1096(1086.45) | Grad Norm 0.1235(0.2148) | Total Time 14.00(14.00)\n",
      "Iter 24830 | Time 27.2098(26.6944) | Bit/dim 3.4170(3.4404) | Xent 0.0015(0.0024) | Loss 3.4178(3.4416) | Error 0.0000(0.0004) Steps 1102(1086.79) | Grad Norm 0.2159(0.2373) | Total Time 14.00(14.00)\n",
      "Iter 24840 | Time 26.3292(26.7228) | Bit/dim 3.4753(3.4435) | Xent 0.0039(0.0025) | Loss 3.4772(3.4447) | Error 0.0011(0.0004) Steps 1060(1086.31) | Grad Norm 0.4862(0.2586) | Total Time 14.00(14.00)\n",
      "Iter 24850 | Time 26.6962(26.7222) | Bit/dim 3.4311(3.4422) | Xent 0.0020(0.0025) | Loss 3.4321(3.4434) | Error 0.0000(0.0004) Steps 1108(1086.71) | Grad Norm 0.2638(0.2578) | Total Time 14.00(14.00)\n",
      "Iter 24860 | Time 26.7976(26.6444) | Bit/dim 3.4653(3.4451) | Xent 0.0017(0.0023) | Loss 3.4662(3.4462) | Error 0.0000(0.0003) Steps 1078(1086.68) | Grad Norm 0.1992(0.2401) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 125.8770, Epoch Time 1609.1549(1585.8656), Bit/dim 3.4471(best: 3.4471), Xent 2.9242, Loss 4.9092, Error 0.3634(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24870 | Time 26.2498(26.6512) | Bit/dim 3.4382(3.4468) | Xent 0.0011(0.0022) | Loss 3.4388(3.4479) | Error 0.0000(0.0003) Steps 1078(1085.51) | Grad Norm 0.1271(0.2271) | Total Time 14.00(14.00)\n",
      "Iter 24880 | Time 27.7987(26.6745) | Bit/dim 3.4329(3.4454) | Xent 0.0026(0.0023) | Loss 3.4342(3.4465) | Error 0.0000(0.0003) Steps 1066(1082.38) | Grad Norm 0.2107(0.2287) | Total Time 14.00(14.00)\n",
      "Iter 24890 | Time 25.6394(26.7382) | Bit/dim 3.4401(3.4456) | Xent 0.0023(0.0021) | Loss 3.4413(3.4466) | Error 0.0000(0.0002) Steps 1096(1085.23) | Grad Norm 0.2062(0.2212) | Total Time 14.00(14.00)\n",
      "Iter 24900 | Time 27.2753(26.6670) | Bit/dim 3.4270(3.4437) | Xent 0.0026(0.0024) | Loss 3.4283(3.4449) | Error 0.0000(0.0004) Steps 1102(1086.72) | Grad Norm 0.2595(0.2352) | Total Time 14.00(14.00)\n",
      "Iter 24910 | Time 25.9089(26.6762) | Bit/dim 3.4384(3.4442) | Xent 0.0041(0.0023) | Loss 3.4405(3.4453) | Error 0.0011(0.0003) Steps 1090(1088.55) | Grad Norm 0.3044(0.2213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 126.1724, Epoch Time 1611.4401(1586.6328), Bit/dim 3.4462(best: 3.4471), Xent 2.9230, Loss 4.9077, Error 0.3643(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24920 | Time 26.4676(26.6375) | Bit/dim 3.4496(3.4428) | Xent 0.0012(0.0021) | Loss 3.4502(3.4438) | Error 0.0000(0.0003) Steps 1072(1086.45) | Grad Norm 0.1599(0.2111) | Total Time 14.00(14.00)\n",
      "Iter 24930 | Time 26.5604(26.6208) | Bit/dim 3.4400(3.4447) | Xent 0.0014(0.0022) | Loss 3.4407(3.4459) | Error 0.0000(0.0003) Steps 1108(1084.79) | Grad Norm 0.1206(0.2127) | Total Time 14.00(14.00)\n",
      "Iter 24940 | Time 27.1546(26.6888) | Bit/dim 3.4362(3.4424) | Xent 0.0015(0.0021) | Loss 3.4370(3.4435) | Error 0.0000(0.0003) Steps 1066(1085.76) | Grad Norm 0.1800(0.2014) | Total Time 14.00(14.00)\n",
      "Iter 24950 | Time 26.9718(26.7308) | Bit/dim 3.4165(3.4434) | Xent 0.0018(0.0021) | Loss 3.4174(3.4444) | Error 0.0000(0.0003) Steps 1114(1086.61) | Grad Norm 0.1629(0.2080) | Total Time 14.00(14.00)\n",
      "Iter 24960 | Time 26.6459(26.7767) | Bit/dim 3.4330(3.4412) | Xent 0.0024(0.0023) | Loss 3.4342(3.4424) | Error 0.0011(0.0004) Steps 1072(1087.03) | Grad Norm 0.2388(0.2319) | Total Time 14.00(14.00)\n",
      "Iter 24970 | Time 26.1495(26.7397) | Bit/dim 3.4691(3.4425) | Xent 0.0061(0.0025) | Loss 3.4722(3.4437) | Error 0.0022(0.0005) Steps 1072(1087.86) | Grad Norm 0.3833(0.2365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 124.8677, Epoch Time 1613.6694(1587.4439), Bit/dim 3.4473(best: 3.4462), Xent 2.8904, Loss 4.8925, Error 0.3582(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 24980 | Time 26.2982(26.7047) | Bit/dim 3.4752(3.4454) | Xent 0.0010(0.0025) | Loss 3.4757(3.4466) | Error 0.0000(0.0005) Steps 1096(1089.46) | Grad Norm 0.1365(0.2361) | Total Time 14.00(14.00)\n",
      "Iter 24990 | Time 28.3067(26.8171) | Bit/dim 3.4157(3.4465) | Xent 0.0029(0.0025) | Loss 3.4172(3.4477) | Error 0.0011(0.0005) Steps 1066(1089.51) | Grad Norm 0.4387(0.2529) | Total Time 14.00(14.00)\n",
      "Iter 25000 | Time 27.4662(26.8343) | Bit/dim 3.4162(3.4434) | Xent 0.0012(0.0025) | Loss 3.4168(3.4446) | Error 0.0000(0.0005) Steps 1090(1091.49) | Grad Norm 0.2675(0.2526) | Total Time 14.00(14.00)\n",
      "Iter 25010 | Time 25.7496(26.7466) | Bit/dim 3.4678(3.4447) | Xent 0.0018(0.0024) | Loss 3.4687(3.4459) | Error 0.0000(0.0004) Steps 1078(1090.96) | Grad Norm 0.2100(0.2424) | Total Time 14.00(14.00)\n",
      "Iter 25020 | Time 26.3280(26.6782) | Bit/dim 3.4356(3.4427) | Xent 0.0019(0.0024) | Loss 3.4366(3.4439) | Error 0.0000(0.0005) Steps 1102(1090.75) | Grad Norm 0.1556(0.2484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 125.8060, Epoch Time 1613.5271(1588.2264), Bit/dim 3.4475(best: 3.4462), Xent 2.8965, Loss 4.8958, Error 0.3581(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25030 | Time 26.3202(26.6527) | Bit/dim 3.4314(3.4396) | Xent 0.0017(0.0022) | Loss 3.4323(3.4407) | Error 0.0000(0.0004) Steps 1102(1091.20) | Grad Norm 0.1698(0.2282) | Total Time 14.00(14.00)\n",
      "Iter 25040 | Time 25.8668(26.5384) | Bit/dim 3.4453(3.4416) | Xent 0.0013(0.0022) | Loss 3.4460(3.4427) | Error 0.0000(0.0003) Steps 1054(1089.88) | Grad Norm 0.1415(0.2188) | Total Time 14.00(14.00)\n",
      "Iter 25050 | Time 26.9899(26.5617) | Bit/dim 3.4496(3.4415) | Xent 0.0044(0.0024) | Loss 3.4518(3.4427) | Error 0.0011(0.0003) Steps 1102(1090.07) | Grad Norm 0.4031(0.2216) | Total Time 14.00(14.00)\n",
      "Iter 25060 | Time 25.7931(26.4749) | Bit/dim 3.4591(3.4431) | Xent 0.0016(0.0022) | Loss 3.4599(3.4442) | Error 0.0000(0.0003) Steps 1096(1089.48) | Grad Norm 0.1538(0.2095) | Total Time 14.00(14.00)\n",
      "Iter 25070 | Time 26.1047(26.5328) | Bit/dim 3.4450(3.4421) | Xent 0.0018(0.0022) | Loss 3.4459(3.4432) | Error 0.0000(0.0003) Steps 1090(1088.29) | Grad Norm 0.1510(0.2069) | Total Time 14.00(14.00)\n",
      "Iter 25080 | Time 26.9479(26.5903) | Bit/dim 3.4571(3.4425) | Xent 0.0012(0.0021) | Loss 3.4577(3.4436) | Error 0.0000(0.0003) Steps 1078(1088.30) | Grad Norm 0.1312(0.2026) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 126.8642, Epoch Time 1602.4809(1588.6541), Bit/dim 3.4464(best: 3.4462), Xent 2.9142, Loss 4.9035, Error 0.3648(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25090 | Time 27.9415(26.6987) | Bit/dim 3.4633(3.4428) | Xent 0.0019(0.0021) | Loss 3.4642(3.4439) | Error 0.0000(0.0003) Steps 1132(1091.91) | Grad Norm 0.1629(0.2015) | Total Time 14.00(14.00)\n",
      "Iter 25100 | Time 26.6757(26.6865) | Bit/dim 3.4875(3.4424) | Xent 0.0020(0.0021) | Loss 3.4885(3.4435) | Error 0.0000(0.0002) Steps 1102(1090.35) | Grad Norm 0.2090(0.2060) | Total Time 14.00(14.00)\n",
      "Iter 25110 | Time 26.5657(26.7303) | Bit/dim 3.4263(3.4423) | Xent 0.0017(0.0022) | Loss 3.4272(3.4434) | Error 0.0000(0.0002) Steps 1054(1090.76) | Grad Norm 0.1842(0.2126) | Total Time 14.00(14.00)\n",
      "Iter 25120 | Time 27.0030(26.8116) | Bit/dim 3.4362(3.4436) | Xent 0.0015(0.0023) | Loss 3.4370(3.4447) | Error 0.0000(0.0003) Steps 1072(1091.84) | Grad Norm 0.1411(0.2210) | Total Time 14.00(14.00)\n",
      "Iter 25130 | Time 26.6559(26.7388) | Bit/dim 3.4003(3.4416) | Xent 0.0018(0.0022) | Loss 3.4012(3.4427) | Error 0.0000(0.0003) Steps 1078(1091.75) | Grad Norm 0.1914(0.2253) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 125.9376, Epoch Time 1617.3906(1589.5162), Bit/dim 3.4466(best: 3.4462), Xent 2.9186, Loss 4.9059, Error 0.3603(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25140 | Time 26.5158(26.6424) | Bit/dim 3.4175(3.4424) | Xent 0.0013(0.0022) | Loss 3.4181(3.4434) | Error 0.0000(0.0003) Steps 1072(1090.26) | Grad Norm 0.1544(0.2261) | Total Time 14.00(14.00)\n",
      "Iter 25150 | Time 27.3158(26.7064) | Bit/dim 3.4011(3.4431) | Xent 0.0011(0.0021) | Loss 3.4017(3.4441) | Error 0.0000(0.0003) Steps 1084(1090.86) | Grad Norm 0.1884(0.2186) | Total Time 14.00(14.00)\n",
      "Iter 25160 | Time 26.3857(26.6167) | Bit/dim 3.4190(3.4424) | Xent 0.0040(0.0023) | Loss 3.4210(3.4435) | Error 0.0011(0.0004) Steps 1114(1091.67) | Grad Norm 0.3492(0.2307) | Total Time 14.00(14.00)\n",
      "Iter 25170 | Time 26.5825(26.7150) | Bit/dim 3.4771(3.4424) | Xent 0.0017(0.0022) | Loss 3.4780(3.4434) | Error 0.0000(0.0003) Steps 1072(1091.46) | Grad Norm 0.1749(0.2229) | Total Time 14.00(14.00)\n",
      "Iter 25180 | Time 27.1859(26.7396) | Bit/dim 3.4425(3.4426) | Xent 0.0010(0.0022) | Loss 3.4430(3.4437) | Error 0.0000(0.0003) Steps 1084(1091.29) | Grad Norm 0.2083(0.2186) | Total Time 14.00(14.00)\n",
      "Iter 25190 | Time 26.6387(26.7713) | Bit/dim 3.4260(3.4420) | Xent 0.0010(0.0022) | Loss 3.4265(3.4431) | Error 0.0000(0.0004) Steps 1102(1091.09) | Grad Norm 0.1750(0.2376) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 125.2202, Epoch Time 1612.9659(1590.2197), Bit/dim 3.4462(best: 3.4462), Xent 2.9161, Loss 4.9043, Error 0.3617(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25200 | Time 26.4427(26.7668) | Bit/dim 3.4587(3.4414) | Xent 0.0014(0.0022) | Loss 3.4594(3.4425) | Error 0.0000(0.0003) Steps 1126(1091.60) | Grad Norm 0.2271(0.2432) | Total Time 14.00(14.00)\n",
      "Iter 25210 | Time 26.3130(26.7448) | Bit/dim 3.4973(3.4428) | Xent 0.0025(0.0022) | Loss 3.4985(3.4439) | Error 0.0000(0.0003) Steps 1090(1089.37) | Grad Norm 0.2420(0.2407) | Total Time 14.00(14.00)\n",
      "Iter 25220 | Time 26.7537(26.6681) | Bit/dim 3.4339(3.4417) | Xent 0.0021(0.0020) | Loss 3.4349(3.4427) | Error 0.0000(0.0003) Steps 1108(1090.83) | Grad Norm 0.3013(0.2319) | Total Time 14.00(14.00)\n",
      "Iter 25230 | Time 26.8160(26.7361) | Bit/dim 3.5159(3.4413) | Xent 0.0017(0.0019) | Loss 3.5168(3.4423) | Error 0.0000(0.0002) Steps 1096(1090.42) | Grad Norm 0.2284(0.2266) | Total Time 14.00(14.00)\n",
      "Iter 25240 | Time 26.2700(26.6814) | Bit/dim 3.4308(3.4433) | Xent 0.0011(0.0019) | Loss 3.4313(3.4442) | Error 0.0000(0.0001) Steps 1090(1089.69) | Grad Norm 0.1227(0.2103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 125.7606, Epoch Time 1608.9039(1590.7802), Bit/dim 3.4449(best: 3.4462), Xent 2.9470, Loss 4.9184, Error 0.3639(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25250 | Time 26.5558(26.6273) | Bit/dim 3.4375(3.4449) | Xent 0.0013(0.0019) | Loss 3.4381(3.4458) | Error 0.0000(0.0001) Steps 1102(1089.34) | Grad Norm 0.1317(0.2034) | Total Time 14.00(14.00)\n",
      "Iter 25260 | Time 27.4323(26.6983) | Bit/dim 3.4017(3.4417) | Xent 0.0021(0.0019) | Loss 3.4027(3.4426) | Error 0.0000(0.0001) Steps 1108(1088.68) | Grad Norm 0.1944(0.1959) | Total Time 14.00(14.00)\n",
      "Iter 25270 | Time 27.1632(26.7665) | Bit/dim 3.4450(3.4417) | Xent 0.0014(0.0020) | Loss 3.4457(3.4426) | Error 0.0000(0.0002) Steps 1102(1088.08) | Grad Norm 0.1955(0.2066) | Total Time 14.00(14.00)\n",
      "Iter 25280 | Time 27.6307(26.7184) | Bit/dim 3.4493(3.4416) | Xent 0.0018(0.0019) | Loss 3.4502(3.4426) | Error 0.0000(0.0003) Steps 1108(1088.73) | Grad Norm 0.2551(0.2162) | Total Time 14.00(14.00)\n",
      "Iter 25290 | Time 26.6251(26.7399) | Bit/dim 3.4298(3.4434) | Xent 0.0022(0.0019) | Loss 3.4309(3.4443) | Error 0.0000(0.0003) Steps 1102(1089.07) | Grad Norm 0.2063(0.2134) | Total Time 14.00(14.00)\n",
      "Iter 25300 | Time 27.0510(26.7157) | Bit/dim 3.4444(3.4414) | Xent 0.0020(0.0021) | Loss 3.4454(3.4425) | Error 0.0000(0.0004) Steps 1096(1090.46) | Grad Norm 0.2767(0.2321) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 125.5023, Epoch Time 1614.6058(1591.4950), Bit/dim 3.4459(best: 3.4449), Xent 2.9175, Loss 4.9047, Error 0.3592(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25310 | Time 26.6450(26.6910) | Bit/dim 3.4422(3.4419) | Xent 0.0019(0.0021) | Loss 3.4431(3.4430) | Error 0.0000(0.0003) Steps 1126(1090.03) | Grad Norm 0.1695(0.2315) | Total Time 14.00(14.00)\n",
      "Iter 25320 | Time 26.2081(26.6944) | Bit/dim 3.4699(3.4425) | Xent 0.0011(0.0022) | Loss 3.4705(3.4436) | Error 0.0000(0.0003) Steps 1096(1087.94) | Grad Norm 0.1833(0.2419) | Total Time 14.00(14.00)\n",
      "Iter 25330 | Time 26.0624(26.7109) | Bit/dim 3.4264(3.4441) | Xent 0.0018(0.0021) | Loss 3.4273(3.4452) | Error 0.0000(0.0003) Steps 1096(1088.26) | Grad Norm 0.2663(0.2510) | Total Time 14.00(14.00)\n",
      "Iter 25340 | Time 26.8888(26.6948) | Bit/dim 3.4404(3.4432) | Xent 0.0045(0.0021) | Loss 3.4426(3.4442) | Error 0.0011(0.0003) Steps 1084(1087.82) | Grad Norm 0.7453(0.2578) | Total Time 14.00(14.00)\n",
      "Iter 25350 | Time 27.0099(26.6694) | Bit/dim 3.4478(3.4435) | Xent 0.0015(0.0022) | Loss 3.4485(3.4446) | Error 0.0000(0.0004) Steps 1108(1087.04) | Grad Norm 0.1526(0.2533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 125.3716, Epoch Time 1608.9977(1592.0200), Bit/dim 3.4454(best: 3.4449), Xent 2.9528, Loss 4.9218, Error 0.3666(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25360 | Time 26.2110(26.6039) | Bit/dim 3.4417(3.4411) | Xent 0.0024(0.0022) | Loss 3.4429(3.4422) | Error 0.0000(0.0003) Steps 1060(1087.03) | Grad Norm 0.2862(0.2496) | Total Time 14.00(14.00)\n",
      "Iter 25370 | Time 26.5396(26.6325) | Bit/dim 3.4085(3.4396) | Xent 0.0055(0.0021) | Loss 3.4113(3.4406) | Error 0.0011(0.0003) Steps 1072(1088.18) | Grad Norm 0.4711(0.2575) | Total Time 14.00(14.00)\n",
      "Iter 25380 | Time 26.7618(26.6340) | Bit/dim 3.4467(3.4415) | Xent 0.0014(0.0022) | Loss 3.4474(3.4426) | Error 0.0000(0.0004) Steps 1084(1088.45) | Grad Norm 0.1566(0.2597) | Total Time 14.00(14.00)\n",
      "Iter 25390 | Time 26.8811(26.6333) | Bit/dim 3.4682(3.4432) | Xent 0.0009(0.0020) | Loss 3.4687(3.4442) | Error 0.0000(0.0003) Steps 1096(1090.30) | Grad Norm 0.1218(0.2417) | Total Time 14.00(14.00)\n",
      "Iter 25400 | Time 26.4240(26.6377) | Bit/dim 3.4595(3.4439) | Xent 0.0029(0.0023) | Loss 3.4609(3.4450) | Error 0.0011(0.0003) Steps 1084(1091.09) | Grad Norm 0.2574(0.2455) | Total Time 14.00(14.00)\n",
      "Iter 25410 | Time 25.9756(26.7193) | Bit/dim 3.4431(3.4402) | Xent 0.0025(0.0022) | Loss 3.4444(3.4413) | Error 0.0011(0.0004) Steps 1096(1093.29) | Grad Norm 0.4170(0.2485) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 124.9565, Epoch Time 1611.0729(1592.5916), Bit/dim 3.4460(best: 3.4449), Xent 2.9329, Loss 4.9125, Error 0.3641(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25420 | Time 27.0009(26.6796) | Bit/dim 3.4731(3.4414) | Xent 0.0017(0.0021) | Loss 3.4739(3.4425) | Error 0.0000(0.0003) Steps 1108(1093.59) | Grad Norm 0.1735(0.2307) | Total Time 14.00(14.00)\n",
      "Iter 25430 | Time 26.6728(26.7598) | Bit/dim 3.4198(3.4418) | Xent 0.0012(0.0020) | Loss 3.4204(3.4428) | Error 0.0000(0.0003) Steps 1102(1091.25) | Grad Norm 0.1749(0.2243) | Total Time 14.00(14.00)\n",
      "Iter 25440 | Time 26.9382(26.7448) | Bit/dim 3.4216(3.4408) | Xent 0.0025(0.0020) | Loss 3.4228(3.4418) | Error 0.0000(0.0002) Steps 1090(1089.53) | Grad Norm 0.2046(0.2195) | Total Time 14.00(14.00)\n",
      "Iter 25450 | Time 27.0080(26.7403) | Bit/dim 3.4216(3.4413) | Xent 0.0021(0.0020) | Loss 3.4226(3.4423) | Error 0.0000(0.0002) Steps 1090(1089.96) | Grad Norm 0.2086(0.2135) | Total Time 14.00(14.00)\n",
      "Iter 25460 | Time 26.7190(26.7309) | Bit/dim 3.4229(3.4411) | Xent 0.0013(0.0020) | Loss 3.4236(3.4421) | Error 0.0000(0.0002) Steps 1072(1091.00) | Grad Norm 0.2280(0.2327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 125.4720, Epoch Time 1615.5739(1593.2811), Bit/dim 3.4454(best: 3.4449), Xent 2.9730, Loss 4.9319, Error 0.3678(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25470 | Time 26.7673(26.7325) | Bit/dim 3.3849(3.4404) | Xent 0.0012(0.0021) | Loss 3.3855(3.4415) | Error 0.0000(0.0004) Steps 1108(1091.20) | Grad Norm 0.1574(0.2354) | Total Time 14.00(14.00)\n",
      "Iter 25480 | Time 26.8320(26.6874) | Bit/dim 3.4462(3.4434) | Xent 0.0016(0.0021) | Loss 3.4470(3.4444) | Error 0.0000(0.0003) Steps 1072(1089.01) | Grad Norm 0.1757(0.2316) | Total Time 14.00(14.00)\n",
      "Iter 25490 | Time 26.4430(26.6234) | Bit/dim 3.4391(3.4393) | Xent 0.0068(0.0022) | Loss 3.4425(3.4404) | Error 0.0022(0.0004) Steps 1090(1090.64) | Grad Norm 0.4183(0.2360) | Total Time 14.00(14.00)\n",
      "Iter 25500 | Time 26.9478(26.5432) | Bit/dim 3.4340(3.4401) | Xent 0.0019(0.0021) | Loss 3.4350(3.4411) | Error 0.0000(0.0003) Steps 1078(1088.71) | Grad Norm 0.1911(0.2247) | Total Time 14.00(14.00)\n",
      "Iter 25510 | Time 26.1288(26.5200) | Bit/dim 3.4349(3.4422) | Xent 0.0013(0.0020) | Loss 3.4356(3.4432) | Error 0.0000(0.0003) Steps 1090(1087.37) | Grad Norm 0.2061(0.2248) | Total Time 14.00(14.00)\n",
      "Iter 25520 | Time 26.2701(26.5788) | Bit/dim 3.4359(3.4408) | Xent 0.0013(0.0020) | Loss 3.4366(3.4418) | Error 0.0000(0.0003) Steps 1090(1090.90) | Grad Norm 0.1546(0.2236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 125.4993, Epoch Time 1601.2103(1593.5190), Bit/dim 3.4458(best: 3.4449), Xent 2.9431, Loss 4.9174, Error 0.3581(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25530 | Time 26.3804(26.5959) | Bit/dim 3.4025(3.4388) | Xent 0.0016(0.0020) | Loss 3.4033(3.4398) | Error 0.0000(0.0003) Steps 1096(1088.33) | Grad Norm 0.2568(0.2526) | Total Time 14.00(14.00)\n",
      "Iter 25540 | Time 27.0028(26.6047) | Bit/dim 3.4664(3.4420) | Xent 0.0011(0.0020) | Loss 3.4670(3.4430) | Error 0.0000(0.0003) Steps 1108(1089.47) | Grad Norm 0.1671(0.2652) | Total Time 14.00(14.00)\n",
      "Iter 25550 | Time 26.4363(26.6369) | Bit/dim 3.4103(3.4405) | Xent 0.0012(0.0019) | Loss 3.4109(3.4414) | Error 0.0000(0.0003) Steps 1096(1089.48) | Grad Norm 0.2152(0.2573) | Total Time 14.00(14.00)\n",
      "Iter 25560 | Time 26.8110(26.5074) | Bit/dim 3.4325(3.4418) | Xent 0.0017(0.0019) | Loss 3.4334(3.4428) | Error 0.0000(0.0003) Steps 1102(1088.32) | Grad Norm 0.2692(0.2529) | Total Time 14.00(14.00)\n",
      "Iter 25570 | Time 26.2654(26.5457) | Bit/dim 3.4493(3.4415) | Xent 0.0020(0.0019) | Loss 3.4503(3.4425) | Error 0.0000(0.0003) Steps 1084(1088.56) | Grad Norm 0.1754(0.2556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 125.6944, Epoch Time 1604.7109(1593.8547), Bit/dim 3.4443(best: 3.4449), Xent 2.9440, Loss 4.9163, Error 0.3650(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25580 | Time 26.7868(26.5870) | Bit/dim 3.4747(3.4408) | Xent 0.0011(0.0018) | Loss 3.4752(3.4417) | Error 0.0000(0.0002) Steps 1114(1089.90) | Grad Norm 0.1772(0.2380) | Total Time 14.00(14.00)\n",
      "Iter 25590 | Time 26.0339(26.5480) | Bit/dim 3.4375(3.4393) | Xent 0.0016(0.0019) | Loss 3.4383(3.4402) | Error 0.0000(0.0003) Steps 1072(1087.56) | Grad Norm 0.2197(0.2363) | Total Time 14.00(14.00)\n",
      "Iter 25600 | Time 25.7944(26.5356) | Bit/dim 3.4335(3.4408) | Xent 0.0017(0.0020) | Loss 3.4343(3.4417) | Error 0.0000(0.0003) Steps 1102(1087.08) | Grad Norm 0.1626(0.2343) | Total Time 14.00(14.00)\n",
      "Iter 25610 | Time 26.2735(26.5306) | Bit/dim 3.4270(3.4407) | Xent 0.0018(0.0019) | Loss 3.4279(3.4417) | Error 0.0000(0.0003) Steps 1072(1085.94) | Grad Norm 0.1716(0.2205) | Total Time 14.00(14.00)\n",
      "Iter 25620 | Time 26.7743(26.6184) | Bit/dim 3.4321(3.4421) | Xent 0.0021(0.0018) | Loss 3.4331(3.4430) | Error 0.0000(0.0002) Steps 1072(1086.97) | Grad Norm 0.1842(0.2151) | Total Time 14.00(14.00)\n",
      "Iter 25630 | Time 26.2189(26.6681) | Bit/dim 3.4264(3.4409) | Xent 0.0012(0.0020) | Loss 3.4270(3.4419) | Error 0.0000(0.0002) Steps 1072(1086.13) | Grad Norm 0.2187(0.2263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 126.9793, Epoch Time 1610.6318(1594.3580), Bit/dim 3.4448(best: 3.4443), Xent 2.9826, Loss 4.9361, Error 0.3616(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25640 | Time 26.0466(26.6870) | Bit/dim 3.4175(3.4391) | Xent 0.0010(0.0019) | Loss 3.4180(3.4400) | Error 0.0000(0.0002) Steps 1078(1086.17) | Grad Norm 0.2229(0.2262) | Total Time 14.00(14.00)\n",
      "Iter 25650 | Time 27.9245(26.6810) | Bit/dim 3.4671(3.4404) | Xent 0.0015(0.0017) | Loss 3.4679(3.4413) | Error 0.0000(0.0002) Steps 1078(1087.84) | Grad Norm 0.1271(0.2157) | Total Time 14.00(14.00)\n",
      "Iter 25660 | Time 26.6661(26.6583) | Bit/dim 3.4194(3.4399) | Xent 0.0015(0.0018) | Loss 3.4201(3.4408) | Error 0.0000(0.0002) Steps 1114(1087.86) | Grad Norm 0.2010(0.2191) | Total Time 14.00(14.00)\n",
      "Iter 25670 | Time 25.9795(26.5823) | Bit/dim 3.4477(3.4417) | Xent 0.0047(0.0019) | Loss 3.4500(3.4426) | Error 0.0022(0.0002) Steps 1078(1085.65) | Grad Norm 0.4122(0.2295) | Total Time 14.00(14.00)\n",
      "Iter 25680 | Time 26.4055(26.5585) | Bit/dim 3.4771(3.4418) | Xent 0.0027(0.0019) | Loss 3.4785(3.4427) | Error 0.0000(0.0002) Steps 1072(1085.95) | Grad Norm 0.2641(0.2279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 126.2815, Epoch Time 1604.7731(1594.6705), Bit/dim 3.4451(best: 3.4443), Xent 2.9999, Loss 4.9450, Error 0.3679(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25690 | Time 25.8285(26.4943) | Bit/dim 3.4376(3.4400) | Xent 0.0025(0.0019) | Loss 3.4388(3.4409) | Error 0.0011(0.0002) Steps 1066(1088.22) | Grad Norm 0.2483(0.2183) | Total Time 14.00(14.00)\n",
      "Iter 25700 | Time 26.9714(26.5576) | Bit/dim 3.4470(3.4388) | Xent 0.0019(0.0019) | Loss 3.4480(3.4397) | Error 0.0000(0.0002) Steps 1078(1089.41) | Grad Norm 0.2589(0.2153) | Total Time 14.00(14.00)\n",
      "Iter 25710 | Time 26.7110(26.5751) | Bit/dim 3.4217(3.4391) | Xent 0.0024(0.0018) | Loss 3.4229(3.4400) | Error 0.0011(0.0002) Steps 1084(1090.02) | Grad Norm 0.2577(0.2057) | Total Time 14.00(14.00)\n",
      "Iter 25720 | Time 26.3000(26.5026) | Bit/dim 3.4193(3.4386) | Xent 0.0031(0.0018) | Loss 3.4209(3.4395) | Error 0.0000(0.0001) Steps 1084(1087.19) | Grad Norm 0.4133(0.2012) | Total Time 14.00(14.00)\n",
      "Iter 25730 | Time 27.0635(26.6012) | Bit/dim 3.4829(3.4418) | Xent 0.0026(0.0021) | Loss 3.4842(3.4429) | Error 0.0011(0.0003) Steps 1078(1088.92) | Grad Norm 0.4440(0.2287) | Total Time 14.00(14.00)\n",
      "Iter 25740 | Time 26.3264(26.6114) | Bit/dim 3.4259(3.4423) | Xent 0.0018(0.0022) | Loss 3.4268(3.4434) | Error 0.0000(0.0003) Steps 1084(1089.85) | Grad Norm 0.2027(0.2405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 127.0206, Epoch Time 1608.0549(1595.0720), Bit/dim 3.4453(best: 3.4443), Xent 2.9738, Loss 4.9322, Error 0.3629(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25750 | Time 27.3649(26.7067) | Bit/dim 3.4522(3.4435) | Xent 0.0082(0.0024) | Loss 3.4563(3.4447) | Error 0.0022(0.0004) Steps 1102(1089.16) | Grad Norm 0.4985(0.2691) | Total Time 14.00(14.00)\n",
      "Iter 25760 | Time 26.6827(26.6885) | Bit/dim 3.4544(3.4419) | Xent 0.0018(0.0022) | Loss 3.4553(3.4430) | Error 0.0000(0.0003) Steps 1108(1090.44) | Grad Norm 0.2459(0.2777) | Total Time 14.00(14.00)\n",
      "Iter 25770 | Time 27.6895(26.6144) | Bit/dim 3.4260(3.4421) | Xent 0.0011(0.0022) | Loss 3.4265(3.4432) | Error 0.0000(0.0003) Steps 1114(1088.94) | Grad Norm 0.3710(0.2899) | Total Time 14.00(14.00)\n",
      "Iter 25780 | Time 27.5112(26.6365) | Bit/dim 3.4361(3.4410) | Xent 0.0026(0.0023) | Loss 3.4374(3.4421) | Error 0.0011(0.0004) Steps 1114(1088.07) | Grad Norm 0.2774(0.3049) | Total Time 14.00(14.00)\n",
      "Iter 25790 | Time 27.1962(26.5598) | Bit/dim 3.4215(3.4411) | Xent 0.0012(0.0022) | Loss 3.4221(3.4422) | Error 0.0000(0.0003) Steps 1084(1087.20) | Grad Norm 0.2801(0.3054) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 125.5242, Epoch Time 1606.9930(1595.4296), Bit/dim 3.4444(best: 3.4443), Xent 2.9233, Loss 4.9060, Error 0.3651(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25800 | Time 27.4989(26.6376) | Bit/dim 3.4630(3.4412) | Xent 0.0012(0.0022) | Loss 3.4636(3.4423) | Error 0.0000(0.0003) Steps 1132(1090.14) | Grad Norm 0.2267(0.3261) | Total Time 14.00(14.00)\n",
      "Iter 25810 | Time 26.8911(26.5892) | Bit/dim 3.4657(3.4427) | Xent 0.0015(0.0024) | Loss 3.4665(3.4438) | Error 0.0000(0.0003) Steps 1108(1091.97) | Grad Norm 0.2212(0.3427) | Total Time 14.00(14.00)\n",
      "Iter 25820 | Time 26.2852(26.6015) | Bit/dim 3.4752(3.4405) | Xent 0.0025(0.0023) | Loss 3.4764(3.4417) | Error 0.0011(0.0004) Steps 1108(1092.66) | Grad Norm 0.4596(0.3439) | Total Time 14.00(14.00)\n",
      "Iter 25830 | Time 27.0499(26.5779) | Bit/dim 3.4294(3.4386) | Xent 0.0013(0.0022) | Loss 3.4300(3.4397) | Error 0.0000(0.0004) Steps 1084(1091.33) | Grad Norm 0.2026(0.3279) | Total Time 14.00(14.00)\n",
      "Iter 25840 | Time 26.5108(26.6534) | Bit/dim 3.4635(3.4392) | Xent 0.0020(0.0021) | Loss 3.4645(3.4402) | Error 0.0000(0.0004) Steps 1060(1092.53) | Grad Norm 0.2480(0.3042) | Total Time 14.00(14.00)\n",
      "Iter 25850 | Time 26.2331(26.6136) | Bit/dim 3.4225(3.4401) | Xent 0.0009(0.0020) | Loss 3.4229(3.4411) | Error 0.0000(0.0003) Steps 1096(1089.69) | Grad Norm 0.1348(0.2793) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 126.7510, Epoch Time 1608.7998(1595.8308), Bit/dim 3.4431(best: 3.4443), Xent 2.9764, Loss 4.9313, Error 0.3629(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25860 | Time 26.4896(26.5581) | Bit/dim 3.4474(3.4408) | Xent 0.0009(0.0018) | Loss 3.4479(3.4417) | Error 0.0000(0.0002) Steps 1078(1088.73) | Grad Norm 0.1158(0.2493) | Total Time 14.00(14.00)\n",
      "Iter 25870 | Time 26.1119(26.6699) | Bit/dim 3.4436(3.4406) | Xent 0.0016(0.0018) | Loss 3.4444(3.4415) | Error 0.0000(0.0002) Steps 1060(1088.82) | Grad Norm 0.1713(0.2371) | Total Time 14.00(14.00)\n",
      "Iter 25880 | Time 26.2122(26.6385) | Bit/dim 3.4451(3.4417) | Xent 0.0020(0.0020) | Loss 3.4461(3.4427) | Error 0.0000(0.0003) Steps 1054(1089.14) | Grad Norm 0.2161(0.2416) | Total Time 14.00(14.00)\n",
      "Iter 25890 | Time 26.3614(26.7214) | Bit/dim 3.4724(3.4407) | Xent 0.0024(0.0020) | Loss 3.4736(3.4418) | Error 0.0011(0.0003) Steps 1072(1088.12) | Grad Norm 0.3704(0.2651) | Total Time 14.00(14.00)\n",
      "Iter 25900 | Time 27.0293(26.7806) | Bit/dim 3.4282(3.4390) | Xent 0.0023(0.0020) | Loss 3.4294(3.4400) | Error 0.0000(0.0003) Steps 1102(1088.84) | Grad Norm 0.3367(0.2683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 127.4209, Epoch Time 1615.4970(1596.4207), Bit/dim 3.4444(best: 3.4431), Xent 2.9588, Loss 4.9238, Error 0.3627(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25910 | Time 26.1282(26.7234) | Bit/dim 3.4687(3.4383) | Xent 0.0031(0.0020) | Loss 3.4703(3.4393) | Error 0.0000(0.0003) Steps 1084(1088.94) | Grad Norm 0.2308(0.2548) | Total Time 14.00(14.00)\n",
      "Iter 25920 | Time 26.3991(26.6609) | Bit/dim 3.4201(3.4397) | Xent 0.0022(0.0019) | Loss 3.4213(3.4407) | Error 0.0000(0.0002) Steps 1072(1088.01) | Grad Norm 0.2112(0.2363) | Total Time 14.00(14.00)\n",
      "Iter 25930 | Time 26.4149(26.5003) | Bit/dim 3.4639(3.4414) | Xent 0.0014(0.0019) | Loss 3.4646(3.4424) | Error 0.0000(0.0002) Steps 1090(1087.86) | Grad Norm 0.1569(0.2281) | Total Time 14.00(14.00)\n",
      "Iter 25940 | Time 25.9587(26.4588) | Bit/dim 3.4641(3.4411) | Xent 0.0027(0.0019) | Loss 3.4655(3.4420) | Error 0.0011(0.0003) Steps 1096(1088.41) | Grad Norm 0.2698(0.2250) | Total Time 14.00(14.00)\n",
      "Iter 25950 | Time 26.6642(26.5091) | Bit/dim 3.4197(3.4408) | Xent 0.0013(0.0019) | Loss 3.4204(3.4417) | Error 0.0000(0.0002) Steps 1078(1088.58) | Grad Norm 0.2293(0.2473) | Total Time 14.00(14.00)\n",
      "Iter 25960 | Time 26.6189(26.5000) | Bit/dim 3.4303(3.4402) | Xent 0.0012(0.0018) | Loss 3.4309(3.4412) | Error 0.0000(0.0003) Steps 1084(1088.92) | Grad Norm 0.1595(0.2475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 126.8510, Epoch Time 1598.2209(1596.4747), Bit/dim 3.4438(best: 3.4431), Xent 2.9836, Loss 4.9356, Error 0.3632(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 25970 | Time 26.7750(26.5712) | Bit/dim 3.4614(3.4383) | Xent 0.0016(0.0018) | Loss 3.4622(3.4392) | Error 0.0000(0.0003) Steps 1078(1088.04) | Grad Norm 0.1679(0.2384) | Total Time 14.00(14.00)\n",
      "Iter 25980 | Time 26.2493(26.4843) | Bit/dim 3.4359(3.4387) | Xent 0.0015(0.0018) | Loss 3.4366(3.4396) | Error 0.0000(0.0003) Steps 1096(1088.86) | Grad Norm 0.2074(0.2357) | Total Time 14.00(14.00)\n",
      "Iter 25990 | Time 26.1992(26.4756) | Bit/dim 3.4209(3.4400) | Xent 0.0016(0.0018) | Loss 3.4217(3.4409) | Error 0.0000(0.0002) Steps 1084(1088.75) | Grad Norm 0.2028(0.2235) | Total Time 14.00(14.00)\n",
      "Iter 26000 | Time 26.8799(26.4629) | Bit/dim 3.4699(3.4415) | Xent 0.0010(0.0018) | Loss 3.4704(3.4424) | Error 0.0000(0.0002) Steps 1096(1088.97) | Grad Norm 0.1531(0.2236) | Total Time 14.00(14.00)\n",
      "Iter 26010 | Time 27.0480(26.4508) | Bit/dim 3.4214(3.4409) | Xent 0.0012(0.0018) | Loss 3.4220(3.4418) | Error 0.0000(0.0002) Steps 1102(1089.64) | Grad Norm 0.1540(0.2281) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 127.3617, Epoch Time 1603.9668(1596.6995), Bit/dim 3.4442(best: 3.4431), Xent 2.9765, Loss 4.9325, Error 0.3637(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26020 | Time 26.9485(26.5712) | Bit/dim 3.4439(3.4395) | Xent 0.0017(0.0017) | Loss 3.4448(3.4403) | Error 0.0000(0.0002) Steps 1108(1089.50) | Grad Norm 0.2435(0.2277) | Total Time 14.00(14.00)\n",
      "Iter 26030 | Time 27.2537(26.6282) | Bit/dim 3.4594(3.4416) | Xent 0.0011(0.0017) | Loss 3.4600(3.4424) | Error 0.0000(0.0001) Steps 1102(1089.67) | Grad Norm 0.1727(0.2200) | Total Time 14.00(14.00)\n",
      "Iter 26040 | Time 26.0049(26.5409) | Bit/dim 3.4544(3.4399) | Xent 0.0023(0.0019) | Loss 3.4555(3.4409) | Error 0.0011(0.0003) Steps 1090(1087.51) | Grad Norm 0.3292(0.2457) | Total Time 14.00(14.00)\n",
      "Iter 26050 | Time 27.1615(26.6290) | Bit/dim 3.4637(3.4385) | Xent 0.0014(0.0023) | Loss 3.4643(3.4396) | Error 0.0000(0.0004) Steps 1084(1088.56) | Grad Norm 0.2170(0.2787) | Total Time 14.00(14.00)\n",
      "Iter 26060 | Time 25.7771(26.6427) | Bit/dim 3.4679(3.4377) | Xent 0.0011(0.0023) | Loss 3.4685(3.4388) | Error 0.0000(0.0004) Steps 1078(1085.46) | Grad Norm 0.1758(0.2710) | Total Time 14.00(14.00)\n",
      "Iter 26070 | Time 26.3160(26.5948) | Bit/dim 3.4714(3.4422) | Xent 0.0014(0.0022) | Loss 3.4721(3.4433) | Error 0.0000(0.0004) Steps 1108(1086.42) | Grad Norm 0.2294(0.2664) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 126.7414, Epoch Time 1609.4903(1597.0832), Bit/dim 3.4444(best: 3.4431), Xent 2.9636, Loss 4.9263, Error 0.3682(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26080 | Time 25.6845(26.5090) | Bit/dim 3.4125(3.4433) | Xent 0.0022(0.0021) | Loss 3.4136(3.4444) | Error 0.0011(0.0004) Steps 1084(1087.05) | Grad Norm 0.3877(0.2717) | Total Time 14.00(14.00)\n",
      "Iter 26090 | Time 26.1721(26.4934) | Bit/dim 3.4469(3.4410) | Xent 0.0042(0.0022) | Loss 3.4490(3.4420) | Error 0.0011(0.0004) Steps 1078(1085.90) | Grad Norm 0.6940(0.2832) | Total Time 14.00(14.00)\n",
      "Iter 26100 | Time 26.8857(26.5612) | Bit/dim 3.4494(3.4412) | Xent 0.0027(0.0020) | Loss 3.4508(3.4422) | Error 0.0011(0.0003) Steps 1084(1086.11) | Grad Norm 0.2704(0.2636) | Total Time 14.00(14.00)\n",
      "Iter 26110 | Time 26.4419(26.6843) | Bit/dim 3.4457(3.4406) | Xent 0.0018(0.0020) | Loss 3.4466(3.4416) | Error 0.0000(0.0003) Steps 1084(1085.91) | Grad Norm 0.1911(0.2550) | Total Time 14.00(14.00)\n",
      "Iter 26120 | Time 26.8175(26.6763) | Bit/dim 3.4234(3.4402) | Xent 0.0032(0.0020) | Loss 3.4250(3.4412) | Error 0.0011(0.0004) Steps 1090(1086.17) | Grad Norm 0.3903(0.2475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 125.4164, Epoch Time 1608.6877(1597.4314), Bit/dim 3.4436(best: 3.4431), Xent 2.9901, Loss 4.9386, Error 0.3666(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26130 | Time 26.1393(26.6479) | Bit/dim 3.4324(3.4390) | Xent 0.0017(0.0021) | Loss 3.4333(3.4400) | Error 0.0000(0.0004) Steps 1078(1087.82) | Grad Norm 0.2243(0.2488) | Total Time 14.00(14.00)\n",
      "Iter 26140 | Time 26.5903(26.6161) | Bit/dim 3.4568(3.4363) | Xent 0.0019(0.0022) | Loss 3.4577(3.4374) | Error 0.0000(0.0004) Steps 1096(1088.16) | Grad Norm 0.2700(0.2638) | Total Time 14.00(14.00)\n",
      "Iter 26150 | Time 26.9988(26.6507) | Bit/dim 3.4590(3.4386) | Xent 0.0025(0.0021) | Loss 3.4602(3.4397) | Error 0.0011(0.0004) Steps 1066(1088.75) | Grad Norm 0.3053(0.2621) | Total Time 14.00(14.00)\n",
      "Iter 26160 | Time 26.4084(26.6006) | Bit/dim 3.4600(3.4376) | Xent 0.0008(0.0020) | Loss 3.4605(3.4386) | Error 0.0000(0.0004) Steps 1096(1086.80) | Grad Norm 0.1710(0.2464) | Total Time 14.00(14.00)\n",
      "Iter 26170 | Time 26.7373(26.6969) | Bit/dim 3.4243(3.4377) | Xent 0.0013(0.0019) | Loss 3.4250(3.4386) | Error 0.0000(0.0004) Steps 1096(1087.88) | Grad Norm 0.2283(0.2550) | Total Time 14.00(14.00)\n",
      "Iter 26180 | Time 26.1371(26.6627) | Bit/dim 3.4391(3.4409) | Xent 0.0012(0.0021) | Loss 3.4397(3.4420) | Error 0.0000(0.0003) Steps 1108(1090.11) | Grad Norm 0.3044(0.2555) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 125.1475, Epoch Time 1607.6615(1597.7383), Bit/dim 3.4434(best: 3.4431), Xent 3.0070, Loss 4.9468, Error 0.3665(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26190 | Time 26.8709(26.5768) | Bit/dim 3.4344(3.4398) | Xent 0.0007(0.0020) | Loss 3.4348(3.4408) | Error 0.0000(0.0003) Steps 1120(1090.58) | Grad Norm 0.2066(0.2543) | Total Time 14.00(14.00)\n",
      "Iter 26200 | Time 26.4963(26.6005) | Bit/dim 3.4624(3.4408) | Xent 0.0014(0.0020) | Loss 3.4631(3.4418) | Error 0.0000(0.0003) Steps 1102(1090.21) | Grad Norm 0.1580(0.2554) | Total Time 14.00(14.00)\n",
      "Iter 26210 | Time 27.2257(26.5386) | Bit/dim 3.4099(3.4383) | Xent 0.0014(0.0019) | Loss 3.4106(3.4392) | Error 0.0000(0.0002) Steps 1096(1091.11) | Grad Norm 0.1956(0.2470) | Total Time 14.00(14.00)\n",
      "Iter 26220 | Time 27.1863(26.5454) | Bit/dim 3.4334(3.4379) | Xent 0.0014(0.0017) | Loss 3.4341(3.4388) | Error 0.0000(0.0002) Steps 1078(1089.54) | Grad Norm 0.1244(0.2231) | Total Time 14.00(14.00)\n",
      "Iter 26230 | Time 26.2773(26.5657) | Bit/dim 3.4683(3.4402) | Xent 0.0009(0.0016) | Loss 3.4687(3.4410) | Error 0.0000(0.0001) Steps 1084(1089.23) | Grad Norm 0.1047(0.2123) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 126.4253, Epoch Time 1601.4319(1597.8491), Bit/dim 3.4424(best: 3.4431), Xent 2.9877, Loss 4.9362, Error 0.3667(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26240 | Time 26.9906(26.5242) | Bit/dim 3.4247(3.4388) | Xent 0.0021(0.0016) | Loss 3.4257(3.4396) | Error 0.0000(0.0001) Steps 1096(1090.03) | Grad Norm 0.2046(0.2050) | Total Time 14.00(14.00)\n",
      "Iter 26250 | Time 26.6793(26.4727) | Bit/dim 3.4669(3.4380) | Xent 0.0015(0.0017) | Loss 3.4676(3.4389) | Error 0.0000(0.0002) Steps 1078(1089.72) | Grad Norm 0.2941(0.2159) | Total Time 14.00(14.00)\n",
      "Iter 26260 | Time 25.5826(26.4670) | Bit/dim 3.4471(3.4396) | Xent 0.0020(0.0017) | Loss 3.4481(3.4405) | Error 0.0000(0.0002) Steps 1078(1090.18) | Grad Norm 0.4001(0.2342) | Total Time 14.00(14.00)\n",
      "Iter 26270 | Time 27.0164(26.4429) | Bit/dim 3.4457(3.4394) | Xent 0.0018(0.0018) | Loss 3.4466(3.4403) | Error 0.0000(0.0002) Steps 1096(1088.37) | Grad Norm 0.3361(0.2422) | Total Time 14.00(14.00)\n",
      "Iter 26280 | Time 27.1323(26.5010) | Bit/dim 3.4740(3.4426) | Xent 0.0017(0.0017) | Loss 3.4748(3.4434) | Error 0.0000(0.0002) Steps 1084(1086.41) | Grad Norm 0.1738(0.2341) | Total Time 14.00(14.00)\n",
      "Iter 26290 | Time 27.4059(26.6227) | Bit/dim 3.4397(3.4394) | Xent 0.0023(0.0018) | Loss 3.4409(3.4403) | Error 0.0011(0.0002) Steps 1084(1087.82) | Grad Norm 0.2893(0.2343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 126.0383, Epoch Time 1603.7690(1598.0267), Bit/dim 3.4427(best: 3.4424), Xent 3.0218, Loss 4.9536, Error 0.3650(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26300 | Time 27.0335(26.6162) | Bit/dim 3.4570(3.4379) | Xent 0.0010(0.0018) | Loss 3.4575(3.4387) | Error 0.0000(0.0002) Steps 1072(1087.44) | Grad Norm 0.1638(0.2366) | Total Time 14.00(14.00)\n",
      "Iter 26310 | Time 27.0227(26.5834) | Bit/dim 3.4587(3.4406) | Xent 0.0012(0.0018) | Loss 3.4593(3.4415) | Error 0.0000(0.0002) Steps 1096(1086.98) | Grad Norm 0.2233(0.2500) | Total Time 14.00(14.00)\n",
      "Iter 26320 | Time 27.1877(26.6128) | Bit/dim 3.4335(3.4397) | Xent 0.0032(0.0020) | Loss 3.4351(3.4407) | Error 0.0000(0.0003) Steps 1084(1089.57) | Grad Norm 0.3129(0.2737) | Total Time 14.00(14.00)\n",
      "Iter 26330 | Time 26.2261(26.5959) | Bit/dim 3.4033(3.4383) | Xent 0.0014(0.0019) | Loss 3.4040(3.4393) | Error 0.0000(0.0002) Steps 1072(1089.07) | Grad Norm 0.2583(0.2945) | Total Time 14.00(14.00)\n",
      "Iter 26340 | Time 26.4286(26.5849) | Bit/dim 3.4312(3.4405) | Xent 0.0076(0.0020) | Loss 3.4350(3.4415) | Error 0.0022(0.0003) Steps 1096(1089.52) | Grad Norm 0.4880(0.2887) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 127.0134, Epoch Time 1606.0197(1598.2665), Bit/dim 3.4423(best: 3.4424), Xent 3.0002, Loss 4.9424, Error 0.3644(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26350 | Time 26.1473(26.5437) | Bit/dim 3.4408(3.4378) | Xent 0.0013(0.0019) | Loss 3.4415(3.4387) | Error 0.0000(0.0002) Steps 1102(1089.04) | Grad Norm 0.2425(0.2735) | Total Time 14.00(14.00)\n",
      "Iter 26360 | Time 26.6571(26.5751) | Bit/dim 3.4448(3.4366) | Xent 0.0015(0.0019) | Loss 3.4455(3.4375) | Error 0.0000(0.0002) Steps 1066(1089.51) | Grad Norm 0.1923(0.2635) | Total Time 14.00(14.00)\n",
      "Iter 26370 | Time 25.8286(26.5542) | Bit/dim 3.4449(3.4373) | Xent 0.0033(0.0019) | Loss 3.4466(3.4382) | Error 0.0011(0.0003) Steps 1090(1089.88) | Grad Norm 0.2695(0.2567) | Total Time 14.00(14.00)\n",
      "Iter 26380 | Time 27.1434(26.5657) | Bit/dim 3.4470(3.4402) | Xent 0.0022(0.0019) | Loss 3.4481(3.4411) | Error 0.0011(0.0002) Steps 1102(1090.04) | Grad Norm 0.2104(0.2437) | Total Time 14.00(14.00)\n",
      "Iter 26390 | Time 26.9098(26.5615) | Bit/dim 3.4212(3.4391) | Xent 0.0020(0.0019) | Loss 3.4222(3.4400) | Error 0.0011(0.0003) Steps 1084(1090.98) | Grad Norm 0.3169(0.2383) | Total Time 14.00(14.00)\n",
      "Iter 26400 | Time 27.1865(26.5802) | Bit/dim 3.4180(3.4399) | Xent 0.0015(0.0019) | Loss 3.4187(3.4408) | Error 0.0000(0.0003) Steps 1084(1089.06) | Grad Norm 0.2409(0.2415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 125.2023, Epoch Time 1604.1252(1598.4422), Bit/dim 3.4427(best: 3.4423), Xent 3.0061, Loss 4.9458, Error 0.3602(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26410 | Time 26.3810(26.5317) | Bit/dim 3.4602(3.4417) | Xent 0.0027(0.0019) | Loss 3.4616(3.4426) | Error 0.0022(0.0003) Steps 1102(1088.24) | Grad Norm 0.3961(0.2564) | Total Time 14.00(14.00)\n",
      "Iter 26420 | Time 26.2422(26.5342) | Bit/dim 3.4324(3.4406) | Xent 0.0018(0.0019) | Loss 3.4333(3.4416) | Error 0.0000(0.0003) Steps 1084(1090.84) | Grad Norm 0.2855(0.2702) | Total Time 14.00(14.00)\n",
      "Iter 26430 | Time 26.6425(26.5255) | Bit/dim 3.4187(3.4429) | Xent 0.0016(0.0018) | Loss 3.4195(3.4438) | Error 0.0000(0.0003) Steps 1090(1090.71) | Grad Norm 0.2148(0.2698) | Total Time 14.00(14.00)\n",
      "Iter 26440 | Time 25.9048(26.4072) | Bit/dim 3.4310(3.4404) | Xent 0.0020(0.0020) | Loss 3.4319(3.4414) | Error 0.0000(0.0003) Steps 1084(1091.21) | Grad Norm 0.2505(0.3044) | Total Time 14.00(14.00)\n",
      "Iter 26450 | Time 26.6066(26.3901) | Bit/dim 3.4107(3.4385) | Xent 0.0011(0.0020) | Loss 3.4112(3.4395) | Error 0.0000(0.0004) Steps 1108(1091.59) | Grad Norm 0.3332(0.3016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 124.9169, Epoch Time 1592.8451(1598.2743), Bit/dim 3.4434(best: 3.4423), Xent 3.0379, Loss 4.9623, Error 0.3670(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26460 | Time 26.3804(26.4055) | Bit/dim 3.4387(3.4382) | Xent 0.0012(0.0019) | Loss 3.4393(3.4392) | Error 0.0000(0.0003) Steps 1096(1092.21) | Grad Norm 0.2231(0.3087) | Total Time 14.00(14.00)\n",
      "Iter 26470 | Time 26.6986(26.4051) | Bit/dim 3.4400(3.4379) | Xent 0.0025(0.0020) | Loss 3.4413(3.4389) | Error 0.0011(0.0004) Steps 1084(1090.49) | Grad Norm 0.4102(0.3028) | Total Time 14.00(14.00)\n",
      "Iter 26480 | Time 25.4245(26.4615) | Bit/dim 3.4394(3.4369) | Xent 0.0012(0.0019) | Loss 3.4401(3.4379) | Error 0.0000(0.0003) Steps 1084(1091.25) | Grad Norm 0.2006(0.2902) | Total Time 14.00(14.00)\n",
      "Iter 26490 | Time 25.8146(26.4539) | Bit/dim 3.4417(3.4392) | Xent 0.0013(0.0018) | Loss 3.4424(3.4401) | Error 0.0000(0.0003) Steps 1102(1090.83) | Grad Norm 0.2130(0.2717) | Total Time 14.00(14.00)\n",
      "Iter 26500 | Time 26.6160(26.4381) | Bit/dim 3.4192(3.4381) | Xent 0.0047(0.0018) | Loss 3.4216(3.4390) | Error 0.0011(0.0003) Steps 1090(1091.79) | Grad Norm 0.3367(0.2611) | Total Time 14.00(14.00)\n",
      "Iter 26510 | Time 26.1860(26.4391) | Bit/dim 3.4367(3.4379) | Xent 0.0015(0.0017) | Loss 3.4375(3.4388) | Error 0.0000(0.0003) Steps 1090(1090.22) | Grad Norm 0.2258(0.2513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 126.0476, Epoch Time 1600.4581(1598.3398), Bit/dim 3.4418(best: 3.4423), Xent 3.0458, Loss 4.9647, Error 0.3645(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26520 | Time 27.7028(26.5385) | Bit/dim 3.4224(3.4359) | Xent 0.0014(0.0017) | Loss 3.4230(3.4367) | Error 0.0000(0.0002) Steps 1084(1090.43) | Grad Norm 0.2638(0.2432) | Total Time 14.00(14.00)\n",
      "Iter 26530 | Time 26.1417(26.4944) | Bit/dim 3.4029(3.4369) | Xent 0.0011(0.0017) | Loss 3.4034(3.4378) | Error 0.0000(0.0003) Steps 1102(1089.15) | Grad Norm 0.1507(0.2405) | Total Time 14.00(14.00)\n",
      "Iter 26540 | Time 26.1203(26.4390) | Bit/dim 3.4438(3.4362) | Xent 0.0010(0.0018) | Loss 3.4443(3.4371) | Error 0.0000(0.0003) Steps 1078(1088.27) | Grad Norm 0.2002(0.2673) | Total Time 14.00(14.00)\n",
      "Iter 26550 | Time 25.8149(26.4399) | Bit/dim 3.4582(3.4376) | Xent 0.0008(0.0018) | Loss 3.4586(3.4385) | Error 0.0000(0.0003) Steps 1078(1087.04) | Grad Norm 0.1901(0.2714) | Total Time 14.00(14.00)\n",
      "Iter 26560 | Time 27.0251(26.4290) | Bit/dim 3.4153(3.4368) | Xent 0.0020(0.0018) | Loss 3.4163(3.4377) | Error 0.0000(0.0003) Steps 1102(1087.82) | Grad Norm 0.2889(0.2803) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 128.4045, Epoch Time 1601.3786(1598.4310), Bit/dim 3.4427(best: 3.4418), Xent 3.0151, Loss 4.9503, Error 0.3686(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26570 | Time 26.1835(26.4367) | Bit/dim 3.4312(3.4384) | Xent 0.0012(0.0018) | Loss 3.4318(3.4392) | Error 0.0000(0.0003) Steps 1072(1088.91) | Grad Norm 0.2094(0.2710) | Total Time 14.00(14.00)\n",
      "Iter 26580 | Time 26.2322(26.4487) | Bit/dim 3.4618(3.4401) | Xent 0.0018(0.0018) | Loss 3.4627(3.4410) | Error 0.0000(0.0003) Steps 1078(1088.18) | Grad Norm 0.2917(0.2710) | Total Time 14.00(14.00)\n",
      "Iter 26590 | Time 26.5036(26.4146) | Bit/dim 3.4231(3.4391) | Xent 0.0015(0.0017) | Loss 3.4239(3.4400) | Error 0.0011(0.0003) Steps 1084(1086.85) | Grad Norm 0.4430(0.2719) | Total Time 14.00(14.00)\n",
      "Iter 26600 | Time 25.8191(26.3422) | Bit/dim 3.4677(3.4389) | Xent 0.0023(0.0017) | Loss 3.4688(3.4398) | Error 0.0011(0.0003) Steps 1084(1088.10) | Grad Norm 0.3313(0.2792) | Total Time 14.00(14.00)\n",
      "Iter 26610 | Time 26.7207(26.4228) | Bit/dim 3.4321(3.4398) | Xent 0.0029(0.0017) | Loss 3.4335(3.4406) | Error 0.0000(0.0002) Steps 1096(1089.49) | Grad Norm 0.2530(0.2621) | Total Time 14.00(14.00)\n",
      "Iter 26620 | Time 26.3485(26.3359) | Bit/dim 3.4280(3.4388) | Xent 0.0023(0.0020) | Loss 3.4291(3.4398) | Error 0.0011(0.0004) Steps 1084(1088.65) | Grad Norm 0.3825(0.2939) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 127.9075, Epoch Time 1595.2624(1598.3359), Bit/dim 3.4427(best: 3.4418), Xent 3.0251, Loss 4.9552, Error 0.3637(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26630 | Time 27.1135(26.4009) | Bit/dim 3.4810(3.4399) | Xent 0.0009(0.0019) | Loss 3.4814(3.4409) | Error 0.0000(0.0003) Steps 1096(1089.64) | Grad Norm 0.2512(0.3064) | Total Time 14.00(14.00)\n",
      "Iter 26640 | Time 26.1553(26.4340) | Bit/dim 3.4198(3.4369) | Xent 0.0024(0.0020) | Loss 3.4210(3.4378) | Error 0.0011(0.0004) Steps 1072(1090.57) | Grad Norm 0.3158(0.3012) | Total Time 14.00(14.00)\n",
      "Iter 26650 | Time 26.7466(26.5154) | Bit/dim 3.4558(3.4365) | Xent 0.0014(0.0018) | Loss 3.4565(3.4374) | Error 0.0000(0.0003) Steps 1114(1091.20) | Grad Norm 0.1553(0.2744) | Total Time 14.00(14.00)\n",
      "Iter 26660 | Time 26.3085(26.4458) | Bit/dim 3.4147(3.4366) | Xent 0.0056(0.0018) | Loss 3.4175(3.4375) | Error 0.0022(0.0003) Steps 1090(1091.75) | Grad Norm 0.3517(0.2509) | Total Time 14.00(14.00)\n",
      "Iter 26670 | Time 26.5487(26.4031) | Bit/dim 3.4303(3.4377) | Xent 0.0017(0.0018) | Loss 3.4312(3.4386) | Error 0.0000(0.0002) Steps 1102(1090.89) | Grad Norm 0.1786(0.2368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 125.6315, Epoch Time 1598.5628(1598.3427), Bit/dim 3.4416(best: 3.4418), Xent 3.0382, Loss 4.9607, Error 0.3689(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26680 | Time 27.1065(26.3961) | Bit/dim 3.4180(3.4374) | Xent 0.0015(0.0017) | Loss 3.4188(3.4382) | Error 0.0000(0.0002) Steps 1114(1090.67) | Grad Norm 0.2227(0.2314) | Total Time 14.00(14.00)\n",
      "Iter 26690 | Time 26.9064(26.3731) | Bit/dim 3.4434(3.4413) | Xent 0.0017(0.0016) | Loss 3.4442(3.4422) | Error 0.0000(0.0002) Steps 1096(1091.00) | Grad Norm 0.1991(0.2218) | Total Time 14.00(14.00)\n",
      "Iter 26700 | Time 27.3609(26.3777) | Bit/dim 3.4263(3.4411) | Xent 0.0012(0.0016) | Loss 3.4269(3.4419) | Error 0.0000(0.0002) Steps 1084(1090.01) | Grad Norm 0.1431(0.2218) | Total Time 14.00(14.00)\n",
      "Iter 26710 | Time 25.8514(26.3769) | Bit/dim 3.4314(3.4387) | Xent 0.0012(0.0016) | Loss 3.4320(3.4395) | Error 0.0000(0.0001) Steps 1078(1090.29) | Grad Norm 0.1712(0.2158) | Total Time 14.00(14.00)\n",
      "Iter 26720 | Time 26.2446(26.3747) | Bit/dim 3.4310(3.4362) | Xent 0.0016(0.0015) | Loss 3.4318(3.4370) | Error 0.0000(0.0001) Steps 1090(1088.31) | Grad Norm 0.1822(0.2035) | Total Time 14.00(14.00)\n",
      "Iter 26730 | Time 25.5905(26.3816) | Bit/dim 3.4451(3.4368) | Xent 0.0013(0.0016) | Loss 3.4458(3.4376) | Error 0.0000(0.0001) Steps 1096(1086.78) | Grad Norm 0.1521(0.1996) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 126.7012, Epoch Time 1596.2058(1598.2786), Bit/dim 3.4410(best: 3.4416), Xent 3.0409, Loss 4.9614, Error 0.3712(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26740 | Time 26.4269(26.3098) | Bit/dim 3.4146(3.4366) | Xent 0.0018(0.0016) | Loss 3.4155(3.4373) | Error 0.0000(0.0001) Steps 1078(1086.93) | Grad Norm 0.1578(0.1920) | Total Time 14.00(14.00)\n",
      "Iter 26750 | Time 26.6752(26.2644) | Bit/dim 3.4353(3.4344) | Xent 0.0024(0.0017) | Loss 3.4365(3.4353) | Error 0.0011(0.0002) Steps 1060(1083.73) | Grad Norm 0.3472(0.2211) | Total Time 14.00(14.00)\n",
      "Iter 26760 | Time 26.3699(26.3034) | Bit/dim 3.4324(3.4363) | Xent 0.0026(0.0018) | Loss 3.4338(3.4372) | Error 0.0011(0.0002) Steps 1090(1085.49) | Grad Norm 0.2748(0.2277) | Total Time 14.00(14.00)\n",
      "Iter 26770 | Time 25.6798(26.2997) | Bit/dim 3.4820(3.4370) | Xent 0.0015(0.0017) | Loss 3.4828(3.4379) | Error 0.0000(0.0002) Steps 1078(1084.91) | Grad Norm 0.2214(0.2210) | Total Time 14.00(14.00)\n",
      "Iter 26780 | Time 26.8341(26.4432) | Bit/dim 3.4542(3.4396) | Xent 0.0011(0.0017) | Loss 3.4548(3.4405) | Error 0.0000(0.0002) Steps 1114(1087.59) | Grad Norm 0.2430(0.2415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 126.2235, Epoch Time 1594.9494(1598.1788), Bit/dim 3.4410(best: 3.4410), Xent 3.0296, Loss 4.9558, Error 0.3648(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26790 | Time 26.6259(26.4107) | Bit/dim 3.4750(3.4398) | Xent 0.0019(0.0019) | Loss 3.4760(3.4408) | Error 0.0000(0.0003) Steps 1084(1088.74) | Grad Norm 0.3053(0.2552) | Total Time 14.00(14.00)\n",
      "Iter 26800 | Time 26.4036(26.5049) | Bit/dim 3.4181(3.4393) | Xent 0.0014(0.0018) | Loss 3.4188(3.4402) | Error 0.0000(0.0003) Steps 1066(1089.10) | Grad Norm 0.1783(0.2563) | Total Time 14.00(14.00)\n",
      "Iter 26810 | Time 25.5391(26.4335) | Bit/dim 3.4548(3.4371) | Xent 0.0016(0.0018) | Loss 3.4556(3.4380) | Error 0.0000(0.0002) Steps 1078(1088.35) | Grad Norm 0.1700(0.2435) | Total Time 14.00(14.00)\n",
      "Iter 26820 | Time 27.7651(26.4712) | Bit/dim 3.4339(3.4389) | Xent 0.0026(0.0017) | Loss 3.4353(3.4398) | Error 0.0011(0.0002) Steps 1084(1087.95) | Grad Norm 0.5078(0.2383) | Total Time 14.00(14.00)\n",
      "Iter 26830 | Time 26.4277(26.4434) | Bit/dim 3.4200(3.4388) | Xent 0.0011(0.0017) | Loss 3.4205(3.4397) | Error 0.0000(0.0002) Steps 1114(1089.20) | Grad Norm 0.3899(0.2595) | Total Time 14.00(14.00)\n",
      "Iter 26840 | Time 26.3898(26.4568) | Bit/dim 3.4387(3.4376) | Xent 0.0038(0.0018) | Loss 3.4406(3.4385) | Error 0.0011(0.0002) Steps 1084(1089.35) | Grad Norm 0.4648(0.2705) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 125.8212, Epoch Time 1598.5660(1598.1904), Bit/dim 3.4415(best: 3.4410), Xent 3.0314, Loss 4.9571, Error 0.3673(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26850 | Time 26.3845(26.4324) | Bit/dim 3.4274(3.4354) | Xent 0.0021(0.0018) | Loss 3.4284(3.4362) | Error 0.0000(0.0002) Steps 1060(1086.87) | Grad Norm 0.3186(0.2714) | Total Time 14.00(14.00)\n",
      "Iter 26860 | Time 26.7340(26.4103) | Bit/dim 3.4336(3.4376) | Xent 0.0008(0.0017) | Loss 3.4340(3.4385) | Error 0.0000(0.0002) Steps 1102(1088.72) | Grad Norm 0.1918(0.2585) | Total Time 14.00(14.00)\n",
      "Iter 26870 | Time 26.7277(26.4387) | Bit/dim 3.4469(3.4368) | Xent 0.0076(0.0019) | Loss 3.4507(3.4378) | Error 0.0011(0.0002) Steps 1108(1088.35) | Grad Norm 0.4074(0.2527) | Total Time 14.00(14.00)\n",
      "Iter 26880 | Time 27.1349(26.4950) | Bit/dim 3.4022(3.4356) | Xent 0.0018(0.0018) | Loss 3.4031(3.4365) | Error 0.0000(0.0002) Steps 1114(1090.99) | Grad Norm 0.2319(0.2564) | Total Time 14.00(14.00)\n",
      "Iter 26890 | Time 26.2424(26.4623) | Bit/dim 3.4387(3.4384) | Xent 0.0008(0.0018) | Loss 3.4391(3.4392) | Error 0.0000(0.0002) Steps 1078(1090.51) | Grad Norm 0.1987(0.2803) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 126.1372, Epoch Time 1599.2811(1598.2231), Bit/dim 3.4414(best: 3.4410), Xent 3.0366, Loss 4.9597, Error 0.3667(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26900 | Time 26.3391(26.4735) | Bit/dim 3.4571(3.4372) | Xent 0.0015(0.0017) | Loss 3.4579(3.4380) | Error 0.0000(0.0002) Steps 1090(1091.26) | Grad Norm 0.2665(0.2824) | Total Time 14.00(14.00)\n",
      "Iter 26910 | Time 26.5046(26.4554) | Bit/dim 3.4405(3.4382) | Xent 0.0013(0.0016) | Loss 3.4411(3.4390) | Error 0.0000(0.0002) Steps 1096(1090.42) | Grad Norm 0.1760(0.2638) | Total Time 14.00(14.00)\n",
      "Iter 26920 | Time 26.7098(26.4744) | Bit/dim 3.4362(3.4376) | Xent 0.0013(0.0016) | Loss 3.4368(3.4383) | Error 0.0000(0.0001) Steps 1096(1089.82) | Grad Norm 0.2343(0.2564) | Total Time 14.00(14.00)\n",
      "Iter 26930 | Time 26.7675(26.5346) | Bit/dim 3.4553(3.4368) | Xent 0.0014(0.0016) | Loss 3.4560(3.4376) | Error 0.0000(0.0001) Steps 1078(1090.41) | Grad Norm 0.1848(0.2455) | Total Time 14.00(14.00)\n",
      "Iter 26940 | Time 25.7648(26.6234) | Bit/dim 3.4122(3.4375) | Xent 0.0011(0.0017) | Loss 3.4128(3.4383) | Error 0.0000(0.0001) Steps 1078(1090.55) | Grad Norm 0.2222(0.2544) | Total Time 14.00(14.00)\n",
      "Iter 26950 | Time 26.5645(26.5759) | Bit/dim 3.4380(3.4371) | Xent 0.0011(0.0017) | Loss 3.4385(3.4380) | Error 0.0000(0.0001) Steps 1096(1089.98) | Grad Norm 0.3894(0.2606) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0490 | Time 125.7484, Epoch Time 1605.1349(1598.4304), Bit/dim 3.4408(best: 3.4410), Xent 3.0698, Loss 4.9757, Error 0.3685(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 26960 | Time 26.1442(26.5675) | Bit/dim 3.4292(3.4343) | Xent 0.0012(0.0017) | Loss 3.4298(3.4351) | Error 0.0000(0.0001) Steps 1090(1090.29) | Grad Norm 0.2129(0.2713) | Total Time 14.00(14.00)\n",
      "Iter 26970 | Time 26.5542(26.5966) | Bit/dim 3.4517(3.4345) | Xent 0.0014(0.0018) | Loss 3.4524(3.4353) | Error 0.0000(0.0002) Steps 1102(1090.73) | Grad Norm 0.1964(0.2693) | Total Time 14.00(14.00)\n",
      "Iter 26980 | Time 26.9715(26.6629) | Bit/dim 3.4470(3.4354) | Xent 0.0012(0.0017) | Loss 3.4476(3.4363) | Error 0.0000(0.0002) Steps 1078(1090.34) | Grad Norm 0.3297(0.2719) | Total Time 14.00(14.00)\n",
      "Iter 26990 | Time 27.7989(26.7075) | Bit/dim 3.4766(3.4385) | Xent 0.0028(0.0020) | Loss 3.4780(3.4395) | Error 0.0011(0.0003) Steps 1090(1089.78) | Grad Norm 0.4546(0.3032) | Total Time 14.00(14.00)\n",
      "Iter 27000 | Time 26.4486(26.7345) | Bit/dim 3.4552(3.4399) | Xent 0.0007(0.0019) | Loss 3.4555(3.4409) | Error 0.0000(0.0003) Steps 1096(1091.25) | Grad Norm 0.3023(0.3226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0491 | Time 124.6292, Epoch Time 1610.9138(1598.8049), Bit/dim 3.4418(best: 3.4408), Xent 3.0538, Loss 4.9686, Error 0.3695(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27010 | Time 26.1884(26.6454) | Bit/dim 3.4661(3.4392) | Xent 0.0010(0.0019) | Loss 3.4666(3.4402) | Error 0.0000(0.0003) Steps 1084(1089.51) | Grad Norm 0.1825(0.3067) | Total Time 14.00(14.00)\n",
      "Iter 27020 | Time 25.9464(26.6222) | Bit/dim 3.4287(3.4385) | Xent 0.0021(0.0018) | Loss 3.4298(3.4394) | Error 0.0000(0.0002) Steps 1084(1089.03) | Grad Norm 0.3008(0.2873) | Total Time 14.00(14.00)\n",
      "Iter 27030 | Time 26.1501(26.5418) | Bit/dim 3.4164(3.4373) | Xent 0.0017(0.0017) | Loss 3.4173(3.4382) | Error 0.0011(0.0002) Steps 1090(1089.37) | Grad Norm 0.2905(0.2679) | Total Time 14.00(14.00)\n",
      "Iter 27040 | Time 27.2126(26.5462) | Bit/dim 3.4541(3.4385) | Xent 0.0009(0.0016) | Loss 3.4546(3.4394) | Error 0.0000(0.0002) Steps 1102(1090.65) | Grad Norm 0.1366(0.2494) | Total Time 14.00(14.00)\n",
      "Iter 27050 | Time 26.4783(26.5306) | Bit/dim 3.4453(3.4370) | Xent 0.0011(0.0015) | Loss 3.4459(3.4377) | Error 0.0000(0.0001) Steps 1084(1091.52) | Grad Norm 0.1623(0.2286) | Total Time 14.00(14.00)\n",
      "Iter 27060 | Time 26.2326(26.5039) | Bit/dim 3.4386(3.4377) | Xent 0.0008(0.0015) | Loss 3.4390(3.4385) | Error 0.0000(0.0001) Steps 1096(1090.22) | Grad Norm 0.1374(0.2120) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 126.3170, Epoch Time 1600.1102(1598.8441), Bit/dim 3.4408(best: 3.4408), Xent 3.0606, Loss 4.9711, Error 0.3673(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27070 | Time 26.1007(26.4764) | Bit/dim 3.4254(3.4383) | Xent 0.0012(0.0015) | Loss 3.4260(3.4390) | Error 0.0000(0.0001) Steps 1090(1088.98) | Grad Norm 0.1598(0.2127) | Total Time 14.00(14.00)\n",
      "Iter 27080 | Time 26.1968(26.5120) | Bit/dim 3.4555(3.4388) | Xent 0.0010(0.0015) | Loss 3.4560(3.4395) | Error 0.0000(0.0001) Steps 1084(1087.19) | Grad Norm 0.1850(0.2097) | Total Time 14.00(14.00)\n",
      "Iter 27090 | Time 26.1976(26.4311) | Bit/dim 3.4518(3.4347) | Xent 0.0021(0.0015) | Loss 3.4529(3.4355) | Error 0.0000(0.0001) Steps 1084(1086.24) | Grad Norm 0.2641(0.2093) | Total Time 14.00(14.00)\n",
      "Iter 27100 | Time 25.7251(26.4079) | Bit/dim 3.4303(3.4340) | Xent 0.0016(0.0016) | Loss 3.4311(3.4348) | Error 0.0000(0.0001) Steps 1102(1088.61) | Grad Norm 0.2380(0.2120) | Total Time 14.00(14.00)\n",
      "Iter 27110 | Time 26.5158(26.4307) | Bit/dim 3.4293(3.4355) | Xent 0.0020(0.0017) | Loss 3.4304(3.4363) | Error 0.0011(0.0002) Steps 1078(1085.99) | Grad Norm 0.4293(0.2349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 124.7372, Epoch Time 1594.6378(1598.7179), Bit/dim 3.4409(best: 3.4408), Xent 3.0878, Loss 4.9849, Error 0.3646(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27120 | Time 26.6919(26.3628) | Bit/dim 3.4011(3.4376) | Xent 0.0012(0.0016) | Loss 3.4017(3.4384) | Error 0.0000(0.0002) Steps 1102(1086.08) | Grad Norm 0.2838(0.2404) | Total Time 14.00(14.00)\n",
      "Iter 27130 | Time 26.4367(26.3211) | Bit/dim 3.4485(3.4366) | Xent 0.0022(0.0016) | Loss 3.4496(3.4374) | Error 0.0011(0.0002) Steps 1090(1085.69) | Grad Norm 0.2865(0.2295) | Total Time 14.00(14.00)\n",
      "Iter 27140 | Time 26.5435(26.3743) | Bit/dim 3.4200(3.4373) | Xent 0.0013(0.0015) | Loss 3.4206(3.4381) | Error 0.0000(0.0002) Steps 1096(1088.53) | Grad Norm 0.2389(0.2266) | Total Time 14.00(14.00)\n",
      "Iter 27150 | Time 26.6767(26.3598) | Bit/dim 3.4147(3.4367) | Xent 0.0013(0.0016) | Loss 3.4154(3.4375) | Error 0.0000(0.0003) Steps 1090(1087.51) | Grad Norm 0.1838(0.2211) | Total Time 14.00(14.00)\n",
      "Iter 27160 | Time 26.2299(26.3456) | Bit/dim 3.4390(3.4378) | Xent 0.0016(0.0017) | Loss 3.4398(3.4386) | Error 0.0000(0.0003) Steps 1078(1086.66) | Grad Norm 0.2727(0.2560) | Total Time 14.00(14.00)\n",
      "Iter 27170 | Time 26.6672(26.3418) | Bit/dim 3.4380(3.4369) | Xent 0.0037(0.0018) | Loss 3.4398(3.4378) | Error 0.0011(0.0003) Steps 1084(1086.14) | Grad Norm 0.4930(0.2701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 126.4676, Epoch Time 1592.0388(1598.5175), Bit/dim 3.4414(best: 3.4408), Xent 3.1054, Loss 4.9941, Error 0.3673(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27180 | Time 26.9756(26.3328) | Bit/dim 3.4385(3.4377) | Xent 0.0012(0.0018) | Loss 3.4391(3.4386) | Error 0.0000(0.0003) Steps 1102(1086.61) | Grad Norm 0.2312(0.2765) | Total Time 14.00(14.00)\n",
      "Iter 27190 | Time 26.5293(26.3304) | Bit/dim 3.4270(3.4379) | Xent 0.0013(0.0018) | Loss 3.4277(3.4388) | Error 0.0000(0.0003) Steps 1078(1087.62) | Grad Norm 0.2148(0.2862) | Total Time 14.00(14.00)\n",
      "Iter 27200 | Time 25.9280(26.3403) | Bit/dim 3.4757(3.4402) | Xent 0.0015(0.0019) | Loss 3.4764(3.4411) | Error 0.0000(0.0002) Steps 1078(1087.65) | Grad Norm 0.2233(0.2747) | Total Time 14.00(14.00)\n",
      "Iter 27210 | Time 26.4263(26.3253) | Bit/dim 3.4438(3.4389) | Xent 0.0048(0.0022) | Loss 3.4463(3.4400) | Error 0.0011(0.0004) Steps 1084(1088.70) | Grad Norm 0.6180(0.3476) | Total Time 14.00(14.00)\n",
      "Iter 27220 | Time 26.4414(26.2970) | Bit/dim 3.4468(3.4374) | Xent 0.0015(0.0021) | Loss 3.4475(3.4384) | Error 0.0000(0.0003) Steps 1072(1088.67) | Grad Norm 0.2639(0.3425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 125.4820, Epoch Time 1590.1140(1598.2654), Bit/dim 3.4409(best: 3.4408), Xent 3.0961, Loss 4.9890, Error 0.3688(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27230 | Time 25.5267(26.2779) | Bit/dim 3.4271(3.4354) | Xent 0.0050(0.0020) | Loss 3.4296(3.4364) | Error 0.0011(0.0003) Steps 1078(1088.04) | Grad Norm 0.4204(0.3282) | Total Time 14.00(14.00)\n",
      "Iter 27240 | Time 26.8285(26.3725) | Bit/dim 3.4430(3.4357) | Xent 0.0009(0.0020) | Loss 3.4434(3.4367) | Error 0.0000(0.0003) Steps 1084(1086.95) | Grad Norm 0.2006(0.3190) | Total Time 14.00(14.00)\n",
      "Iter 27250 | Time 26.8157(26.4427) | Bit/dim 3.4467(3.4356) | Xent 0.0011(0.0020) | Loss 3.4473(3.4367) | Error 0.0000(0.0004) Steps 1090(1087.47) | Grad Norm 0.1905(0.3093) | Total Time 14.00(14.00)\n",
      "Iter 27260 | Time 25.4017(26.4177) | Bit/dim 3.4492(3.4325) | Xent 0.0008(0.0018) | Loss 3.4496(3.4334) | Error 0.0000(0.0003) Steps 1078(1088.03) | Grad Norm 0.1917(0.2794) | Total Time 14.00(14.00)\n",
      "Iter 27270 | Time 26.3643(26.3053) | Bit/dim 3.4313(3.4364) | Xent 0.0021(0.0018) | Loss 3.4323(3.4373) | Error 0.0011(0.0003) Steps 1090(1086.89) | Grad Norm 0.2829(0.2860) | Total Time 14.00(14.00)\n",
      "Iter 27280 | Time 26.3144(26.3706) | Bit/dim 3.4472(3.4378) | Xent 0.0012(0.0019) | Loss 3.4478(3.4387) | Error 0.0000(0.0004) Steps 1066(1087.76) | Grad Norm 0.2536(0.2905) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 126.4502, Epoch Time 1596.5699(1598.2146), Bit/dim 3.4403(best: 3.4408), Xent 3.1150, Loss 4.9978, Error 0.3701(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27290 | Time 26.6025(26.4665) | Bit/dim 3.4112(3.4353) | Xent 0.0013(0.0019) | Loss 3.4119(3.4363) | Error 0.0000(0.0004) Steps 1102(1088.58) | Grad Norm 0.3392(0.3083) | Total Time 14.00(14.00)\n",
      "Iter 27300 | Time 25.9583(26.3611) | Bit/dim 3.4906(3.4381) | Xent 0.0012(0.0019) | Loss 3.4912(3.4391) | Error 0.0000(0.0004) Steps 1084(1088.33) | Grad Norm 0.1797(0.3014) | Total Time 14.00(14.00)\n",
      "Iter 27310 | Time 25.8770(26.3394) | Bit/dim 3.4460(3.4361) | Xent 0.0060(0.0020) | Loss 3.4491(3.4371) | Error 0.0011(0.0004) Steps 1078(1088.00) | Grad Norm 0.4320(0.2961) | Total Time 14.00(14.00)\n",
      "Iter 27320 | Time 27.8818(26.4997) | Bit/dim 3.4355(3.4390) | Xent 0.0018(0.0018) | Loss 3.4364(3.4400) | Error 0.0000(0.0003) Steps 1084(1090.06) | Grad Norm 0.2748(0.2775) | Total Time 14.00(14.00)\n",
      "Iter 27330 | Time 26.3976(26.4975) | Bit/dim 3.4048(3.4390) | Xent 0.0012(0.0017) | Loss 3.4054(3.4398) | Error 0.0000(0.0002) Steps 1090(1089.71) | Grad Norm 0.1844(0.2601) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 124.6530, Epoch Time 1597.0306(1598.1791), Bit/dim 3.4400(best: 3.4403), Xent 3.0994, Loss 4.9897, Error 0.3681(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27340 | Time 26.0434(26.4397) | Bit/dim 3.4329(3.4363) | Xent 0.0034(0.0017) | Loss 3.4345(3.4371) | Error 0.0011(0.0002) Steps 1078(1090.82) | Grad Norm 0.3566(0.2429) | Total Time 14.00(14.00)\n",
      "Iter 27350 | Time 26.0466(26.3756) | Bit/dim 3.4071(3.4346) | Xent 0.0022(0.0016) | Loss 3.4082(3.4354) | Error 0.0011(0.0002) Steps 1084(1089.62) | Grad Norm 0.2889(0.2455) | Total Time 14.00(14.00)\n",
      "Iter 27360 | Time 26.5298(26.3879) | Bit/dim 3.4737(3.4367) | Xent 0.0019(0.0016) | Loss 3.4747(3.4375) | Error 0.0000(0.0002) Steps 1084(1088.97) | Grad Norm 0.1918(0.2248) | Total Time 14.00(14.00)\n",
      "Iter 27370 | Time 26.0551(26.4531) | Bit/dim 3.4177(3.4342) | Xent 0.0014(0.0016) | Loss 3.4185(3.4350) | Error 0.0000(0.0002) Steps 1102(1091.45) | Grad Norm 0.2835(0.2339) | Total Time 14.00(14.00)\n",
      "Iter 27380 | Time 25.9044(26.3655) | Bit/dim 3.4250(3.4330) | Xent 0.0006(0.0017) | Loss 3.4253(3.4339) | Error 0.0000(0.0002) Steps 1102(1090.95) | Grad Norm 0.2798(0.2562) | Total Time 14.00(14.00)\n",
      "Iter 27390 | Time 26.9224(26.3748) | Bit/dim 3.4708(3.4376) | Xent 0.0012(0.0017) | Loss 3.4714(3.4384) | Error 0.0000(0.0002) Steps 1096(1090.68) | Grad Norm 0.2390(0.2564) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 126.8150, Epoch Time 1595.5712(1598.1008), Bit/dim 3.4404(best: 3.4400), Xent 3.1048, Loss 4.9928, Error 0.3690(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27400 | Time 26.9428(26.4290) | Bit/dim 3.4517(3.4376) | Xent 0.0014(0.0017) | Loss 3.4524(3.4385) | Error 0.0000(0.0002) Steps 1078(1088.81) | Grad Norm 0.1883(0.2575) | Total Time 14.00(14.00)\n",
      "Iter 27410 | Time 26.3397(26.3740) | Bit/dim 3.4273(3.4358) | Xent 0.0023(0.0016) | Loss 3.4284(3.4366) | Error 0.0011(0.0002) Steps 1090(1088.71) | Grad Norm 0.2683(0.2421) | Total Time 14.00(14.00)\n",
      "Iter 27420 | Time 26.5687(26.4001) | Bit/dim 3.4288(3.4347) | Xent 0.0015(0.0017) | Loss 3.4296(3.4355) | Error 0.0000(0.0002) Steps 1096(1088.10) | Grad Norm 0.1780(0.2561) | Total Time 14.00(14.00)\n",
      "Iter 27430 | Time 26.8236(26.3521) | Bit/dim 3.4218(3.4357) | Xent 0.0012(0.0017) | Loss 3.4224(3.4366) | Error 0.0000(0.0003) Steps 1102(1088.18) | Grad Norm 0.1994(0.2639) | Total Time 14.00(14.00)\n",
      "Iter 27440 | Time 26.8362(26.4053) | Bit/dim 3.4382(3.4366) | Xent 0.0008(0.0017) | Loss 3.4386(3.4374) | Error 0.0000(0.0002) Steps 1090(1087.62) | Grad Norm 0.2044(0.2804) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 125.7488, Epoch Time 1596.8709(1598.0639), Bit/dim 3.4405(best: 3.4400), Xent 3.1004, Loss 4.9906, Error 0.3704(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27450 | Time 26.5452(26.4121) | Bit/dim 3.4184(3.4383) | Xent 0.0016(0.0016) | Loss 3.4192(3.4391) | Error 0.0000(0.0002) Steps 1072(1086.85) | Grad Norm 0.2817(0.2796) | Total Time 14.00(14.00)\n",
      "Iter 27460 | Time 27.1490(26.4358) | Bit/dim 3.4288(3.4377) | Xent 0.0015(0.0016) | Loss 3.4296(3.4386) | Error 0.0000(0.0002) Steps 1078(1086.32) | Grad Norm 0.1776(0.2723) | Total Time 14.00(14.00)\n",
      "Iter 27470 | Time 26.8488(26.4361) | Bit/dim 3.4249(3.4375) | Xent 0.0014(0.0017) | Loss 3.4256(3.4383) | Error 0.0000(0.0003) Steps 1072(1086.88) | Grad Norm 0.2430(0.2662) | Total Time 14.00(14.00)\n",
      "Iter 27480 | Time 26.7442(26.5101) | Bit/dim 3.4496(3.4400) | Xent 0.0043(0.0018) | Loss 3.4518(3.4409) | Error 0.0011(0.0003) Steps 1078(1086.32) | Grad Norm 0.3204(0.2586) | Total Time 14.00(14.00)\n",
      "Iter 27490 | Time 27.1024(26.3899) | Bit/dim 3.4211(3.4379) | Xent 0.0062(0.0020) | Loss 3.4242(3.4389) | Error 0.0011(0.0003) Steps 1108(1086.73) | Grad Norm 0.6601(0.3052) | Total Time 14.00(14.00)\n",
      "Iter 27500 | Time 25.8654(26.3684) | Bit/dim 3.4402(3.4364) | Xent 0.0008(0.0019) | Loss 3.4406(3.4374) | Error 0.0000(0.0002) Steps 1078(1086.67) | Grad Norm 0.3080(0.3104) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0500 | Time 126.1544, Epoch Time 1594.9276(1597.9698), Bit/dim 3.4402(best: 3.4400), Xent 3.1288, Loss 5.0046, Error 0.3669(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27510 | Time 27.7474(26.4136) | Bit/dim 3.3974(3.4357) | Xent 0.0012(0.0019) | Loss 3.3980(3.4366) | Error 0.0000(0.0002) Steps 1120(1086.55) | Grad Norm 0.2116(0.3054) | Total Time 14.00(14.00)\n",
      "Iter 27520 | Time 26.3340(26.3830) | Bit/dim 3.4725(3.4355) | Xent 0.0014(0.0018) | Loss 3.4733(3.4363) | Error 0.0000(0.0002) Steps 1078(1086.71) | Grad Norm 0.2196(0.2973) | Total Time 14.00(14.00)\n",
      "Iter 27530 | Time 26.5640(26.3868) | Bit/dim 3.4479(3.4376) | Xent 0.0010(0.0017) | Loss 3.4484(3.4384) | Error 0.0000(0.0002) Steps 1084(1088.18) | Grad Norm 0.1950(0.2841) | Total Time 14.00(14.00)\n",
      "Iter 27540 | Time 26.0988(26.3898) | Bit/dim 3.4404(3.4370) | Xent 0.0012(0.0018) | Loss 3.4410(3.4379) | Error 0.0000(0.0002) Steps 1090(1088.99) | Grad Norm 0.2157(0.2788) | Total Time 14.00(14.00)\n",
      "Iter 27550 | Time 26.5070(26.3898) | Bit/dim 3.4206(3.4352) | Xent 0.0008(0.0019) | Loss 3.4211(3.4362) | Error 0.0000(0.0003) Steps 1072(1088.66) | Grad Norm 0.3167(0.3039) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0501 | Time 125.1170, Epoch Time 1593.4976(1597.8357), Bit/dim 3.4411(best: 3.4400), Xent 3.1509, Loss 5.0166, Error 0.3709(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27560 | Time 25.6663(26.2752) | Bit/dim 3.4431(3.4369) | Xent 0.0026(0.0019) | Loss 3.4444(3.4379) | Error 0.0011(0.0003) Steps 1060(1086.09) | Grad Norm 0.5169(0.3458) | Total Time 14.00(14.00)\n",
      "Iter 27570 | Time 26.8769(26.2173) | Bit/dim 3.4178(3.4358) | Xent 0.0011(0.0019) | Loss 3.4184(3.4367) | Error 0.0000(0.0003) Steps 1090(1085.49) | Grad Norm 0.2014(0.3591) | Total Time 14.00(14.00)\n",
      "Iter 27580 | Time 25.8474(26.2241) | Bit/dim 3.4498(3.4368) | Xent 0.0038(0.0019) | Loss 3.4517(3.4378) | Error 0.0011(0.0004) Steps 1090(1085.49) | Grad Norm 0.3288(0.3515) | Total Time 14.00(14.00)\n",
      "Iter 27590 | Time 26.1477(26.2134) | Bit/dim 3.4368(3.4374) | Xent 0.0029(0.0020) | Loss 3.4382(3.4384) | Error 0.0000(0.0003) Steps 1084(1085.01) | Grad Norm 0.3025(0.3291) | Total Time 14.00(14.00)\n",
      "Iter 27600 | Time 27.0539(26.2692) | Bit/dim 3.4268(3.4371) | Xent 0.0009(0.0019) | Loss 3.4272(3.4380) | Error 0.0000(0.0003) Steps 1084(1086.06) | Grad Norm 0.2099(0.3058) | Total Time 14.00(14.00)\n",
      "Iter 27610 | Time 26.1785(26.2506) | Bit/dim 3.4418(3.4343) | Xent 0.0009(0.0017) | Loss 3.4423(3.4352) | Error 0.0000(0.0002) Steps 1108(1085.77) | Grad Norm 0.1505(0.2740) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0502 | Time 124.7811, Epoch Time 1582.2436(1597.3679), Bit/dim 3.4392(best: 3.4400), Xent 3.1130, Loss 4.9957, Error 0.3722(best: 0.3507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 27620 | Time 27.4063(26.2709) | Bit/dim 3.4339(3.4351) | Xent 0.0013(0.0016) | Loss 3.4346(3.4359) | Error 0.0000(0.0002) Steps 1120(1086.47) | Grad Norm 0.1739(0.2532) | Total Time 14.00(14.00)\n",
      "Iter 27630 | Time 25.7272(26.2465) | Bit/dim 3.4121(3.4331) | Xent 0.0009(0.0017) | Loss 3.4126(3.4340) | Error 0.0000(0.0002) Steps 1090(1087.56) | Grad Norm 0.2570(0.2888) | Total Time 14.00(14.00)\n",
      "Iter 27640 | Time 26.3365(26.1792) | Bit/dim 3.4237(3.4351) | Xent 0.0023(0.0017) | Loss 3.4249(3.4360) | Error 0.0000(0.0002) Steps 1102(1087.51) | Grad Norm 0.3490(0.2956) | Total Time 14.00(14.00)\n",
      "Iter 27650 | Time 26.1467(26.1451) | Bit/dim 3.4269(3.4369) | Xent 0.0061(0.0021) | Loss 3.4299(3.4379) | Error 0.0011(0.0004) Steps 1066(1087.81) | Grad Norm 0.6003(0.3319) | Total Time 14.00(14.00)\n",
      "Iter 27660 | Time 27.4909(26.2691) | Bit/dim 3.4266(3.4353) | Xent 0.0021(0.0020) | Loss 3.4276(3.4363) | Error 0.0000(0.0004) Steps 1108(1088.62) | Grad Norm 0.2599(0.3285) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_run1_2 --resume ../experiments_published/cnf_conditional_cifar10_bs900_run1_2/current_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
