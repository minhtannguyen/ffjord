{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_run1_2/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_run1_2', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 9910 | Time 21.3171(21.9257) | Bit/dim 3.5439(3.5491) | Xent 0.0195(0.0327) | Loss 3.5537(3.5655) | Error 0.0078(0.0115) Steps 892(893.38) | Grad Norm 1.1303(1.8332) | Total Time 14.00(14.00)\n",
      "Iter 9920 | Time 21.4969(21.8687) | Bit/dim 3.5431(3.5476) | Xent 0.0227(0.0308) | Loss 3.5544(3.5630) | Error 0.0067(0.0106) Steps 868(889.40) | Grad Norm 1.4583(1.7445) | Total Time 14.00(14.00)\n",
      "Iter 9930 | Time 21.1951(21.8611) | Bit/dim 3.5553(3.5490) | Xent 0.0239(0.0303) | Loss 3.5673(3.5641) | Error 0.0067(0.0103) Steps 874(887.39) | Grad Norm 1.2594(1.6978) | Total Time 14.00(14.00)\n",
      "Iter 9940 | Time 21.4576(21.8643) | Bit/dim 3.5424(3.5461) | Xent 0.0299(0.0303) | Loss 3.5573(3.5613) | Error 0.0089(0.0103) Steps 874(886.34) | Grad Norm 1.4555(1.6503) | Total Time 14.00(14.00)\n",
      "Iter 9950 | Time 21.8636(21.8836) | Bit/dim 3.5707(3.5476) | Xent 0.0304(0.0299) | Loss 3.5859(3.5625) | Error 0.0089(0.0098) Steps 898(888.36) | Grad Norm 1.3839(1.6023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 118.0966, Epoch Time 1362.3303(1305.0791), Bit/dim 3.5458(best: inf), Xent 2.7556, Loss 4.9236, Error 0.3642(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9960 | Time 22.3789(21.9993) | Bit/dim 3.5839(3.5472) | Xent 0.0320(0.0287) | Loss 3.5999(3.5616) | Error 0.0111(0.0096) Steps 886(888.83) | Grad Norm 1.6897(1.5955) | Total Time 14.00(14.00)\n",
      "Iter 9970 | Time 21.5073(21.9587) | Bit/dim 3.5706(3.5454) | Xent 0.0211(0.0284) | Loss 3.5811(3.5597) | Error 0.0089(0.0096) Steps 898(892.38) | Grad Norm 1.3454(1.5510) | Total Time 14.00(14.00)\n",
      "Iter 9980 | Time 21.0460(21.8110) | Bit/dim 3.5095(3.5431) | Xent 0.0194(0.0279) | Loss 3.5192(3.5571) | Error 0.0056(0.0094) Steps 910(893.36) | Grad Norm 1.0619(1.5739) | Total Time 14.00(14.00)\n",
      "Iter 9990 | Time 21.1618(21.6159) | Bit/dim 3.5460(3.5420) | Xent 0.0202(0.0287) | Loss 3.5562(3.5564) | Error 0.0078(0.0097) Steps 886(890.64) | Grad Norm 1.7143(1.6533) | Total Time 14.00(14.00)\n",
      "Iter 10000 | Time 21.6795(21.5293) | Bit/dim 3.5518(3.5441) | Xent 0.0354(0.0307) | Loss 3.5695(3.5594) | Error 0.0111(0.0104) Steps 880(892.07) | Grad Norm 1.5825(1.7230) | Total Time 14.00(14.00)\n",
      "Iter 10010 | Time 21.2582(21.4657) | Bit/dim 3.5805(3.5470) | Xent 0.0440(0.0323) | Loss 3.6026(3.5632) | Error 0.0178(0.0114) Steps 910(896.22) | Grad Norm 2.3956(1.7794) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 106.4392, Epoch Time 1306.0499(1305.1082), Bit/dim 3.5533(best: 3.5458), Xent 2.8976, Loss 5.0021, Error 0.3583(best: 0.3642)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10020 | Time 22.8834(21.5169) | Bit/dim 3.5423(3.5467) | Xent 0.0859(0.0346) | Loss 3.5853(3.5640) | Error 0.0222(0.0119) Steps 892(898.06) | Grad Norm 2.9440(1.8319) | Total Time 14.00(14.00)\n",
      "Iter 10030 | Time 21.7120(21.4534) | Bit/dim 3.5611(3.5487) | Xent 0.0386(0.0348) | Loss 3.5804(3.5661) | Error 0.0133(0.0119) Steps 880(896.35) | Grad Norm 1.6722(1.8201) | Total Time 14.00(14.00)\n",
      "Iter 10040 | Time 21.1632(21.3749) | Bit/dim 3.5457(3.5494) | Xent 0.0476(0.0356) | Loss 3.5695(3.5672) | Error 0.0167(0.0122) Steps 898(895.52) | Grad Norm 1.9532(1.8663) | Total Time 14.00(14.00)\n",
      "Iter 10050 | Time 21.7678(21.4995) | Bit/dim 3.5120(3.5500) | Xent 0.0438(0.0352) | Loss 3.5339(3.5676) | Error 0.0178(0.0119) Steps 928(901.38) | Grad Norm 2.0638(1.9114) | Total Time 14.00(14.00)\n",
      "Iter 10060 | Time 21.5276(21.5455) | Bit/dim 3.5542(3.5499) | Xent 0.0428(0.0360) | Loss 3.5756(3.5679) | Error 0.0144(0.0121) Steps 892(901.51) | Grad Norm 2.2706(1.9543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 104.4234, Epoch Time 1305.9271(1305.1328), Bit/dim 3.5489(best: 3.5458), Xent 2.7597, Loss 4.9287, Error 0.3680(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10070 | Time 21.2668(21.5251) | Bit/dim 3.5446(3.5483) | Xent 0.0627(0.0378) | Loss 3.5759(3.5672) | Error 0.0211(0.0127) Steps 904(900.09) | Grad Norm 2.7777(2.0120) | Total Time 14.00(14.00)\n",
      "Iter 10080 | Time 20.9958(21.5828) | Bit/dim 3.5679(3.5502) | Xent 0.0269(0.0383) | Loss 3.5814(3.5694) | Error 0.0100(0.0134) Steps 916(905.61) | Grad Norm 2.5408(2.2424) | Total Time 14.00(14.00)\n",
      "Iter 10090 | Time 21.9107(21.6184) | Bit/dim 3.5594(3.5485) | Xent 0.0642(0.0401) | Loss 3.5915(3.5686) | Error 0.0200(0.0137) Steps 898(905.08) | Grad Norm 2.1756(2.2621) | Total Time 14.00(14.00)\n",
      "Iter 10100 | Time 21.5019(21.5890) | Bit/dim 3.5456(3.5526) | Xent 0.0231(0.0399) | Loss 3.5572(3.5725) | Error 0.0067(0.0135) Steps 910(903.15) | Grad Norm 1.9120(2.2152) | Total Time 14.00(14.00)\n",
      "Iter 10110 | Time 21.5099(21.5885) | Bit/dim 3.5614(3.5529) | Xent 0.0288(0.0370) | Loss 3.5758(3.5714) | Error 0.0067(0.0125) Steps 916(905.93) | Grad Norm 1.4498(2.0335) | Total Time 14.00(14.00)\n",
      "Iter 10120 | Time 21.1133(21.5537) | Bit/dim 3.5523(3.5515) | Xent 0.0412(0.0362) | Loss 3.5729(3.5696) | Error 0.0133(0.0122) Steps 904(906.45) | Grad Norm 1.8229(1.9450) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 106.1634, Epoch Time 1308.2930(1305.2276), Bit/dim 3.5485(best: 3.5458), Xent 2.7556, Loss 4.9263, Error 0.3651(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10130 | Time 21.8958(21.5951) | Bit/dim 3.5879(3.5501) | Xent 0.0420(0.0343) | Loss 3.6089(3.5673) | Error 0.0133(0.0114) Steps 928(907.48) | Grad Norm 1.5768(1.8216) | Total Time 14.00(14.00)\n",
      "Iter 10140 | Time 21.9657(21.5990) | Bit/dim 3.5274(3.5477) | Xent 0.0430(0.0336) | Loss 3.5488(3.5645) | Error 0.0144(0.0113) Steps 904(909.90) | Grad Norm 1.8414(1.7506) | Total Time 14.00(14.00)\n",
      "Iter 10150 | Time 21.3632(21.5885) | Bit/dim 3.5229(3.5444) | Xent 0.0261(0.0314) | Loss 3.5360(3.5601) | Error 0.0100(0.0108) Steps 904(911.07) | Grad Norm 2.0134(1.6913) | Total Time 14.00(14.00)\n",
      "Iter 10160 | Time 21.2440(21.5600) | Bit/dim 3.5792(3.5474) | Xent 0.0206(0.0307) | Loss 3.5895(3.5627) | Error 0.0056(0.0104) Steps 898(909.72) | Grad Norm 0.9165(1.6344) | Total Time 14.00(14.00)\n",
      "Iter 10170 | Time 21.4300(21.4292) | Bit/dim 3.5285(3.5446) | Xent 0.0240(0.0304) | Loss 3.5405(3.5598) | Error 0.0067(0.0101) Steps 904(904.58) | Grad Norm 1.4744(1.6263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 108.4019, Epoch Time 1305.3099(1305.2301), Bit/dim 3.5482(best: 3.5458), Xent 2.9509, Loss 5.0237, Error 0.3707(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10180 | Time 21.3910(21.4377) | Bit/dim 3.5701(3.5453) | Xent 0.0265(0.0312) | Loss 3.5834(3.5609) | Error 0.0089(0.0103) Steps 898(905.46) | Grad Norm 2.4515(1.6843) | Total Time 14.00(14.00)\n",
      "Iter 10190 | Time 22.1279(21.4723) | Bit/dim 3.5247(3.5454) | Xent 0.0164(0.0328) | Loss 3.5329(3.5618) | Error 0.0078(0.0112) Steps 910(905.83) | Grad Norm 1.4087(1.7865) | Total Time 14.00(14.00)\n",
      "Iter 10200 | Time 20.4381(21.3977) | Bit/dim 3.5468(3.5467) | Xent 0.0386(0.0351) | Loss 3.5660(3.5642) | Error 0.0111(0.0120) Steps 898(904.35) | Grad Norm 2.4031(1.8827) | Total Time 14.00(14.00)\n",
      "Iter 10210 | Time 21.6118(21.3618) | Bit/dim 3.5319(3.5470) | Xent 0.0220(0.0348) | Loss 3.5429(3.5644) | Error 0.0100(0.0119) Steps 904(904.18) | Grad Norm 1.2141(1.7955) | Total Time 14.00(14.00)\n",
      "Iter 10220 | Time 20.9274(21.4663) | Bit/dim 3.5382(3.5462) | Xent 0.0421(0.0361) | Loss 3.5592(3.5642) | Error 0.0144(0.0124) Steps 898(904.73) | Grad Norm 2.0267(1.7920) | Total Time 14.00(14.00)\n",
      "Iter 10230 | Time 22.0111(21.4807) | Bit/dim 3.5414(3.5475) | Xent 0.0313(0.0346) | Loss 3.5571(3.5648) | Error 0.0144(0.0121) Steps 928(906.06) | Grad Norm 1.4174(1.7465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 107.7097, Epoch Time 1304.1515(1305.1977), Bit/dim 3.5506(best: 3.5458), Xent 2.8035, Loss 4.9524, Error 0.3640(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10240 | Time 22.4222(21.5423) | Bit/dim 3.5381(3.5468) | Xent 0.0353(0.0327) | Loss 3.5557(3.5632) | Error 0.0111(0.0110) Steps 922(906.61) | Grad Norm 2.1218(1.6942) | Total Time 14.00(14.00)\n",
      "Iter 10250 | Time 21.4531(21.6068) | Bit/dim 3.5280(3.5438) | Xent 0.0243(0.0309) | Loss 3.5402(3.5593) | Error 0.0089(0.0102) Steps 898(906.31) | Grad Norm 1.6231(1.7739) | Total Time 14.00(14.00)\n",
      "Iter 10260 | Time 22.2839(21.6285) | Bit/dim 3.5602(3.5450) | Xent 0.0388(0.0309) | Loss 3.5796(3.5604) | Error 0.0122(0.0103) Steps 898(905.74) | Grad Norm 2.4503(1.8124) | Total Time 14.00(14.00)\n",
      "Iter 10270 | Time 21.4703(21.6243) | Bit/dim 3.5670(3.5465) | Xent 0.0316(0.0305) | Loss 3.5828(3.5617) | Error 0.0100(0.0103) Steps 892(904.43) | Grad Norm 1.9149(1.8848) | Total Time 14.00(14.00)\n",
      "Iter 10280 | Time 21.4812(21.7270) | Bit/dim 3.5517(3.5476) | Xent 0.0300(0.0317) | Loss 3.5667(3.5634) | Error 0.0089(0.0107) Steps 904(906.18) | Grad Norm 2.4575(1.9497) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 105.9445, Epoch Time 1318.1326(1305.5858), Bit/dim 3.5513(best: 3.5458), Xent 2.8694, Loss 4.9860, Error 0.3665(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10290 | Time 22.0574(21.7082) | Bit/dim 3.5106(3.5474) | Xent 0.0225(0.0309) | Loss 3.5219(3.5629) | Error 0.0067(0.0104) Steps 904(905.61) | Grad Norm 1.4358(1.8940) | Total Time 14.00(14.00)\n",
      "Iter 10300 | Time 21.6315(21.6217) | Bit/dim 3.5377(3.5466) | Xent 0.0307(0.0305) | Loss 3.5531(3.5619) | Error 0.0100(0.0102) Steps 910(903.86) | Grad Norm 1.9426(1.8672) | Total Time 14.00(14.00)\n",
      "Iter 10310 | Time 22.3859(21.7037) | Bit/dim 3.5419(3.5465) | Xent 0.0349(0.0298) | Loss 3.5593(3.5614) | Error 0.0122(0.0100) Steps 946(906.09) | Grad Norm 1.8872(1.8262) | Total Time 14.00(14.00)\n",
      "Iter 10320 | Time 21.1742(21.7299) | Bit/dim 3.5735(3.5460) | Xent 0.0269(0.0318) | Loss 3.5870(3.5619) | Error 0.0111(0.0106) Steps 892(906.72) | Grad Norm 3.6000(1.9788) | Total Time 14.00(14.00)\n",
      "Iter 10330 | Time 20.9260(21.7234) | Bit/dim 3.5749(3.5501) | Xent 0.0445(0.0384) | Loss 3.5972(3.5693) | Error 0.0144(0.0124) Steps 880(904.77) | Grad Norm 3.8056(2.4008) | Total Time 14.00(14.00)\n",
      "Iter 10340 | Time 21.3075(21.6639) | Bit/dim 3.5523(3.5545) | Xent 0.0604(0.0396) | Loss 3.5825(3.5743) | Error 0.0222(0.0131) Steps 904(905.29) | Grad Norm 2.6057(2.4527) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 106.9800, Epoch Time 1315.4958(1305.8831), Bit/dim 3.5577(best: 3.5458), Xent 2.8431, Loss 4.9792, Error 0.3596(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10350 | Time 21.0785(21.7496) | Bit/dim 3.5459(3.5529) | Xent 0.0343(0.0384) | Loss 3.5630(3.5720) | Error 0.0100(0.0129) Steps 898(906.12) | Grad Norm 1.6256(2.2917) | Total Time 14.00(14.00)\n",
      "Iter 10360 | Time 21.9352(21.7698) | Bit/dim 3.5015(3.5506) | Xent 0.0208(0.0372) | Loss 3.5119(3.5692) | Error 0.0067(0.0126) Steps 904(907.11) | Grad Norm 1.7427(2.1313) | Total Time 14.00(14.00)\n",
      "Iter 10370 | Time 22.6012(21.8508) | Bit/dim 3.5825(3.5497) | Xent 0.0292(0.0363) | Loss 3.5972(3.5678) | Error 0.0100(0.0124) Steps 892(906.53) | Grad Norm 1.6618(2.0420) | Total Time 14.00(14.00)\n",
      "Iter 10380 | Time 21.0904(21.7624) | Bit/dim 3.5558(3.5480) | Xent 0.0222(0.0346) | Loss 3.5669(3.5653) | Error 0.0056(0.0118) Steps 874(905.50) | Grad Norm 2.0243(2.0323) | Total Time 14.00(14.00)\n",
      "Iter 10390 | Time 22.7455(21.8503) | Bit/dim 3.5182(3.5482) | Xent 0.0302(0.0351) | Loss 3.5333(3.5657) | Error 0.0100(0.0119) Steps 928(906.69) | Grad Norm 1.4882(2.0634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 106.3162, Epoch Time 1326.6808(1306.5070), Bit/dim 3.5487(best: 3.5458), Xent 2.8812, Loss 4.9893, Error 0.3681(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10400 | Time 21.7930(21.8526) | Bit/dim 3.5435(3.5482) | Xent 0.0276(0.0344) | Loss 3.5573(3.5654) | Error 0.0089(0.0119) Steps 880(906.71) | Grad Norm 1.9785(1.9608) | Total Time 14.00(14.00)\n",
      "Iter 10410 | Time 21.3907(21.8547) | Bit/dim 3.5389(3.5474) | Xent 0.0363(0.0340) | Loss 3.5570(3.5644) | Error 0.0133(0.0118) Steps 916(908.41) | Grad Norm 2.1248(1.9145) | Total Time 14.00(14.00)\n",
      "Iter 10420 | Time 21.4868(21.7896) | Bit/dim 3.5452(3.5490) | Xent 0.0424(0.0338) | Loss 3.5664(3.5659) | Error 0.0156(0.0119) Steps 904(908.83) | Grad Norm 2.3097(1.8111) | Total Time 14.00(14.00)\n",
      "Iter 10430 | Time 22.3125(21.6894) | Bit/dim 3.5238(3.5453) | Xent 0.0283(0.0327) | Loss 3.5379(3.5617) | Error 0.0122(0.0116) Steps 910(904.87) | Grad Norm 1.3874(1.7921) | Total Time 14.00(14.00)\n",
      "Iter 10440 | Time 20.7464(21.6190) | Bit/dim 3.5089(3.5445) | Xent 0.0475(0.0348) | Loss 3.5326(3.5619) | Error 0.0133(0.0122) Steps 880(903.31) | Grad Norm 2.5385(1.9397) | Total Time 14.00(14.00)\n",
      "Iter 10450 | Time 21.3206(21.5826) | Bit/dim 3.5615(3.5476) | Xent 0.0577(0.0381) | Loss 3.5904(3.5667) | Error 0.0189(0.0132) Steps 892(901.46) | Grad Norm 5.0315(2.2143) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 108.0720, Epoch Time 1311.8781(1306.6681), Bit/dim 3.5543(best: 3.5458), Xent 2.9172, Loss 5.0129, Error 0.3650(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10460 | Time 21.1056(21.5842) | Bit/dim 3.5404(3.5491) | Xent 0.0228(0.0371) | Loss 3.5518(3.5676) | Error 0.0122(0.0129) Steps 904(901.62) | Grad Norm 1.5674(2.2498) | Total Time 14.00(14.00)\n",
      "Iter 10470 | Time 22.5279(21.6652) | Bit/dim 3.5778(3.5491) | Xent 0.0174(0.0349) | Loss 3.5865(3.5666) | Error 0.0067(0.0121) Steps 910(903.03) | Grad Norm 1.1815(2.1304) | Total Time 14.00(14.00)\n",
      "Iter 10480 | Time 22.5241(21.8791) | Bit/dim 3.5337(3.5474) | Xent 0.0284(0.0327) | Loss 3.5479(3.5638) | Error 0.0100(0.0115) Steps 928(908.39) | Grad Norm 1.7560(1.9767) | Total Time 14.00(14.00)\n",
      "Iter 10490 | Time 21.6310(21.8686) | Bit/dim 3.4958(3.5420) | Xent 0.0361(0.0318) | Loss 3.5138(3.5579) | Error 0.0111(0.0111) Steps 898(908.33) | Grad Norm 1.6632(1.8697) | Total Time 14.00(14.00)\n",
      "Iter 10500 | Time 21.6973(21.7836) | Bit/dim 3.5183(3.5447) | Xent 0.0479(0.0333) | Loss 3.5423(3.5613) | Error 0.0178(0.0116) Steps 934(908.91) | Grad Norm 2.9302(1.8889) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 106.9991, Epoch Time 1325.2794(1307.2265), Bit/dim 3.5510(best: 3.5458), Xent 2.8982, Loss 5.0001, Error 0.3623(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10510 | Time 21.5258(21.6943) | Bit/dim 3.5173(3.5480) | Xent 0.0676(0.0362) | Loss 3.5511(3.5661) | Error 0.0289(0.0125) Steps 922(906.82) | Grad Norm 3.2903(1.9568) | Total Time 14.00(14.00)\n",
      "Iter 10520 | Time 21.3383(21.7331) | Bit/dim 3.5332(3.5478) | Xent 0.0294(0.0365) | Loss 3.5480(3.5661) | Error 0.0133(0.0129) Steps 916(907.57) | Grad Norm 1.7981(1.9553) | Total Time 14.00(14.00)\n",
      "Iter 10530 | Time 22.2808(21.7075) | Bit/dim 3.5461(3.5485) | Xent 0.0349(0.0362) | Loss 3.5635(3.5666) | Error 0.0111(0.0127) Steps 922(906.02) | Grad Norm 1.7477(1.9338) | Total Time 14.00(14.00)\n",
      "Iter 10540 | Time 21.6697(21.7213) | Bit/dim 3.5668(3.5448) | Xent 0.0363(0.0355) | Loss 3.5849(3.5626) | Error 0.0122(0.0124) Steps 910(905.61) | Grad Norm 1.8049(1.8504) | Total Time 14.00(14.00)\n",
      "Iter 10550 | Time 22.5458(21.8040) | Bit/dim 3.5305(3.5435) | Xent 0.0413(0.0334) | Loss 3.5511(3.5602) | Error 0.0100(0.0116) Steps 898(907.59) | Grad Norm 1.9031(1.7696) | Total Time 14.00(14.00)\n",
      "Iter 10560 | Time 22.0078(21.8555) | Bit/dim 3.5611(3.5436) | Xent 0.0220(0.0330) | Loss 3.5720(3.5601) | Error 0.0089(0.0115) Steps 892(905.90) | Grad Norm 1.5904(1.7956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 107.4087, Epoch Time 1322.3006(1307.6787), Bit/dim 3.5454(best: 3.5458), Xent 2.7781, Loss 4.9344, Error 0.3605(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10570 | Time 22.6283(21.8814) | Bit/dim 3.5283(3.5430) | Xent 0.0198(0.0315) | Loss 3.5382(3.5588) | Error 0.0067(0.0111) Steps 886(905.76) | Grad Norm 1.5873(1.8004) | Total Time 14.00(14.00)\n",
      "Iter 10580 | Time 22.2279(21.9217) | Bit/dim 3.5337(3.5412) | Xent 0.0286(0.0297) | Loss 3.5480(3.5561) | Error 0.0089(0.0105) Steps 892(907.05) | Grad Norm 1.2498(1.7061) | Total Time 14.00(14.00)\n",
      "Iter 10590 | Time 22.2845(21.9662) | Bit/dim 3.5879(3.5434) | Xent 0.0183(0.0286) | Loss 3.5971(3.5577) | Error 0.0056(0.0099) Steps 904(907.46) | Grad Norm 1.2747(1.6338) | Total Time 14.00(14.00)\n",
      "Iter 10600 | Time 22.2380(22.0618) | Bit/dim 3.5457(3.5381) | Xent 0.0179(0.0269) | Loss 3.5547(3.5516) | Error 0.0056(0.0094) Steps 934(911.11) | Grad Norm 1.1895(1.5192) | Total Time 14.00(14.00)\n",
      "Iter 10610 | Time 22.0408(22.0294) | Bit/dim 3.5413(3.5364) | Xent 0.0161(0.0261) | Loss 3.5493(3.5494) | Error 0.0067(0.0089) Steps 892(910.56) | Grad Norm 1.5396(1.4825) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 107.8676, Epoch Time 1334.5454(1308.4847), Bit/dim 3.5408(best: 3.5454), Xent 2.8230, Loss 4.9523, Error 0.3673(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10620 | Time 21.4700(21.9441) | Bit/dim 3.5309(3.5366) | Xent 0.0221(0.0268) | Loss 3.5420(3.5500) | Error 0.0056(0.0091) Steps 904(907.28) | Grad Norm 2.0255(1.5810) | Total Time 14.00(14.00)\n",
      "Iter 10630 | Time 21.7953(21.9887) | Bit/dim 3.5794(3.5393) | Xent 0.0263(0.0263) | Loss 3.5926(3.5524) | Error 0.0122(0.0091) Steps 928(910.96) | Grad Norm 1.7730(1.5890) | Total Time 14.00(14.00)\n",
      "Iter 10640 | Time 22.1508(21.9821) | Bit/dim 3.5070(3.5369) | Xent 0.0179(0.0249) | Loss 3.5160(3.5493) | Error 0.0056(0.0086) Steps 892(908.87) | Grad Norm 1.6808(1.5454) | Total Time 14.00(14.00)\n",
      "Iter 10650 | Time 21.5176(21.9594) | Bit/dim 3.5031(3.5362) | Xent 0.0331(0.0269) | Loss 3.5196(3.5497) | Error 0.0111(0.0095) Steps 898(909.31) | Grad Norm 2.1486(1.6364) | Total Time 14.00(14.00)\n",
      "Iter 10660 | Time 21.3686(21.9140) | Bit/dim 3.5594(3.5383) | Xent 0.0229(0.0271) | Loss 3.5708(3.5518) | Error 0.0100(0.0096) Steps 898(909.49) | Grad Norm 1.3763(1.6247) | Total Time 14.00(14.00)\n",
      "Iter 10670 | Time 22.5670(21.9565) | Bit/dim 3.5481(3.5377) | Xent 0.0440(0.0271) | Loss 3.5701(3.5512) | Error 0.0178(0.0100) Steps 898(909.45) | Grad Norm 2.3218(1.6291) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 105.6210, Epoch Time 1330.8424(1309.1554), Bit/dim 3.5401(best: 3.5408), Xent 2.7842, Loss 4.9322, Error 0.3616(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10680 | Time 21.3702(21.8826) | Bit/dim 3.5263(3.5358) | Xent 0.0230(0.0261) | Loss 3.5378(3.5488) | Error 0.0078(0.0095) Steps 904(907.37) | Grad Norm 1.5970(1.5960) | Total Time 14.00(14.00)\n",
      "Iter 10690 | Time 21.6355(21.8656) | Bit/dim 3.5254(3.5357) | Xent 0.0248(0.0259) | Loss 3.5378(3.5487) | Error 0.0078(0.0095) Steps 910(906.10) | Grad Norm 1.3894(1.5860) | Total Time 14.00(14.00)\n",
      "Iter 10700 | Time 22.4972(21.8128) | Bit/dim 3.4980(3.5318) | Xent 0.0299(0.0273) | Loss 3.5129(3.5454) | Error 0.0078(0.0095) Steps 904(904.99) | Grad Norm 1.6352(1.5940) | Total Time 14.00(14.00)\n",
      "Iter 10710 | Time 20.9253(21.7237) | Bit/dim 3.5609(3.5369) | Xent 0.0416(0.0273) | Loss 3.5817(3.5506) | Error 0.0144(0.0093) Steps 886(903.83) | Grad Norm 1.3771(1.5312) | Total Time 14.00(14.00)\n",
      "Iter 10720 | Time 22.7375(21.7683) | Bit/dim 3.5457(3.5380) | Xent 0.0312(0.0267) | Loss 3.5612(3.5513) | Error 0.0089(0.0091) Steps 928(904.80) | Grad Norm 1.5327(1.4893) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 106.3705, Epoch Time 1316.1301(1309.3647), Bit/dim 3.5393(best: 3.5401), Xent 2.8043, Loss 4.9414, Error 0.3719(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10730 | Time 20.9309(21.6795) | Bit/dim 3.5307(3.5381) | Xent 0.0210(0.0260) | Loss 3.5412(3.5511) | Error 0.0067(0.0088) Steps 898(904.02) | Grad Norm 1.4157(1.5104) | Total Time 14.00(14.00)\n",
      "Iter 10740 | Time 21.1527(21.5360) | Bit/dim 3.5282(3.5372) | Xent 0.0255(0.0249) | Loss 3.5409(3.5496) | Error 0.0067(0.0081) Steps 898(902.97) | Grad Norm 1.2835(1.4482) | Total Time 14.00(14.00)\n",
      "Iter 10750 | Time 21.2198(21.4808) | Bit/dim 3.5119(3.5361) | Xent 0.0399(0.0247) | Loss 3.5318(3.5485) | Error 0.0122(0.0079) Steps 904(899.97) | Grad Norm 1.3813(1.4438) | Total Time 14.00(14.00)\n",
      "Iter 10760 | Time 20.6547(21.5329) | Bit/dim 3.5708(3.5362) | Xent 0.0220(0.0251) | Loss 3.5818(3.5488) | Error 0.0067(0.0080) Steps 886(900.72) | Grad Norm 1.6858(1.4583) | Total Time 14.00(14.00)\n",
      "Iter 10770 | Time 20.9259(21.5858) | Bit/dim 3.5306(3.5368) | Xent 0.0278(0.0267) | Loss 3.5445(3.5501) | Error 0.0122(0.0089) Steps 910(903.58) | Grad Norm 2.3304(1.6334) | Total Time 14.00(14.00)\n",
      "Iter 10780 | Time 21.8216(21.5720) | Bit/dim 3.5248(3.5346) | Xent 0.0335(0.0272) | Loss 3.5415(3.5482) | Error 0.0122(0.0092) Steps 940(905.12) | Grad Norm 1.9162(1.6723) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 105.4709, Epoch Time 1302.2986(1309.1527), Bit/dim 3.5390(best: 3.5393), Xent 2.7555, Loss 4.9167, Error 0.3670(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10790 | Time 22.3353(21.6976) | Bit/dim 3.5160(3.5307) | Xent 0.0205(0.0260) | Loss 3.5263(3.5437) | Error 0.0067(0.0089) Steps 898(907.28) | Grad Norm 1.2268(1.5954) | Total Time 14.00(14.00)\n",
      "Iter 10800 | Time 21.6420(21.6803) | Bit/dim 3.5380(3.5333) | Xent 0.0359(0.0265) | Loss 3.5560(3.5465) | Error 0.0111(0.0089) Steps 886(905.50) | Grad Norm 1.3040(1.5464) | Total Time 14.00(14.00)\n",
      "Iter 10810 | Time 22.2409(21.6612) | Bit/dim 3.5269(3.5337) | Xent 0.0299(0.0262) | Loss 3.5419(3.5468) | Error 0.0089(0.0089) Steps 898(903.26) | Grad Norm 1.3211(1.5036) | Total Time 14.00(14.00)\n",
      "Iter 10820 | Time 21.6483(21.7223) | Bit/dim 3.5245(3.5337) | Xent 0.0147(0.0257) | Loss 3.5319(3.5465) | Error 0.0067(0.0090) Steps 886(903.57) | Grad Norm 1.3647(1.5693) | Total Time 14.00(14.00)\n",
      "Iter 10830 | Time 20.7829(21.7106) | Bit/dim 3.5355(3.5325) | Xent 0.0199(0.0264) | Loss 3.5454(3.5457) | Error 0.0089(0.0092) Steps 880(903.04) | Grad Norm 1.3614(1.6451) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 105.8049, Epoch Time 1316.2454(1309.3655), Bit/dim 3.5398(best: 3.5390), Xent 2.7976, Loss 4.9386, Error 0.3633(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10840 | Time 21.7289(21.5986) | Bit/dim 3.5861(3.5369) | Xent 0.0183(0.0253) | Loss 3.5953(3.5495) | Error 0.0067(0.0086) Steps 874(900.01) | Grad Norm 1.5235(1.6695) | Total Time 14.00(14.00)\n",
      "Iter 10850 | Time 21.3164(21.6501) | Bit/dim 3.5206(3.5374) | Xent 0.0305(0.0241) | Loss 3.5359(3.5494) | Error 0.0111(0.0084) Steps 910(900.63) | Grad Norm 2.0640(1.6457) | Total Time 14.00(14.00)\n",
      "Iter 10860 | Time 21.9285(21.7906) | Bit/dim 3.5336(3.5363) | Xent 0.0258(0.0223) | Loss 3.5465(3.5475) | Error 0.0089(0.0076) Steps 928(905.41) | Grad Norm 1.7134(1.5725) | Total Time 14.00(14.00)\n",
      "Iter 10870 | Time 22.4093(21.8773) | Bit/dim 3.5199(3.5318) | Xent 0.0402(0.0229) | Loss 3.5399(3.5432) | Error 0.0133(0.0078) Steps 928(907.90) | Grad Norm 2.1555(1.5414) | Total Time 14.00(14.00)\n",
      "Iter 10880 | Time 21.7284(21.8591) | Bit/dim 3.5555(3.5304) | Xent 0.0327(0.0233) | Loss 3.5719(3.5421) | Error 0.0089(0.0079) Steps 910(909.19) | Grad Norm 2.2375(1.6043) | Total Time 14.00(14.00)\n",
      "Iter 10890 | Time 21.5963(21.7980) | Bit/dim 3.5192(3.5309) | Xent 0.0259(0.0227) | Loss 3.5322(3.5422) | Error 0.0100(0.0078) Steps 910(911.13) | Grad Norm 1.8297(1.6027) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 108.1262, Epoch Time 1327.2074(1309.9007), Bit/dim 3.5380(best: 3.5390), Xent 2.9126, Loss 4.9942, Error 0.3650(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10900 | Time 21.1675(21.8720) | Bit/dim 3.5275(3.5314) | Xent 0.0254(0.0228) | Loss 3.5402(3.5427) | Error 0.0078(0.0078) Steps 898(913.43) | Grad Norm 1.7819(1.6047) | Total Time 14.00(14.00)\n",
      "Iter 10910 | Time 21.8961(21.8884) | Bit/dim 3.5303(3.5334) | Xent 0.0294(0.0244) | Loss 3.5450(3.5456) | Error 0.0111(0.0082) Steps 886(909.68) | Grad Norm 1.4671(1.6943) | Total Time 14.00(14.00)\n",
      "Iter 10920 | Time 21.4604(21.7412) | Bit/dim 3.5336(3.5342) | Xent 0.0145(0.0236) | Loss 3.5409(3.5460) | Error 0.0056(0.0080) Steps 898(906.66) | Grad Norm 1.4546(1.7226) | Total Time 14.00(14.00)\n",
      "Iter 10930 | Time 23.1424(21.8292) | Bit/dim 3.5268(3.5348) | Xent 0.0096(0.0253) | Loss 3.5316(3.5475) | Error 0.0033(0.0085) Steps 970(909.13) | Grad Norm 1.5769(1.8351) | Total Time 14.00(14.00)\n",
      "Iter 10940 | Time 22.4264(21.9968) | Bit/dim 3.5119(3.5352) | Xent 0.0256(0.0258) | Loss 3.5247(3.5482) | Error 0.0133(0.0087) Steps 946(916.43) | Grad Norm 2.1433(1.8495) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 106.9076, Epoch Time 1334.1545(1310.6283), Bit/dim 3.5413(best: 3.5380), Xent 2.8669, Loss 4.9747, Error 0.3677(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10950 | Time 20.8098(21.9277) | Bit/dim 3.4712(3.5329) | Xent 0.0224(0.0267) | Loss 3.4824(3.5463) | Error 0.0089(0.0091) Steps 880(912.01) | Grad Norm 1.2713(1.7932) | Total Time 14.00(14.00)\n",
      "Iter 10960 | Time 21.6006(21.8742) | Bit/dim 3.5544(3.5341) | Xent 0.0274(0.0260) | Loss 3.5681(3.5471) | Error 0.0111(0.0090) Steps 892(909.68) | Grad Norm 1.5811(1.6754) | Total Time 14.00(14.00)\n",
      "Iter 10970 | Time 21.5522(21.8516) | Bit/dim 3.5183(3.5324) | Xent 0.0205(0.0248) | Loss 3.5286(3.5448) | Error 0.0056(0.0085) Steps 940(910.14) | Grad Norm 1.1271(1.6294) | Total Time 14.00(14.00)\n",
      "Iter 10980 | Time 21.2202(21.7730) | Bit/dim 3.5653(3.5345) | Xent 0.0401(0.0253) | Loss 3.5854(3.5472) | Error 0.0122(0.0085) Steps 910(908.06) | Grad Norm 1.9583(1.6305) | Total Time 14.00(14.00)\n",
      "Iter 10990 | Time 22.5841(21.8343) | Bit/dim 3.5441(3.5350) | Xent 0.0295(0.0282) | Loss 3.5588(3.5491) | Error 0.0100(0.0095) Steps 904(907.86) | Grad Norm 1.2294(1.6618) | Total Time 14.00(14.00)\n",
      "Iter 11000 | Time 21.3737(21.8911) | Bit/dim 3.5432(3.5358) | Xent 0.0444(0.0318) | Loss 3.5654(3.5517) | Error 0.0156(0.0106) Steps 922(910.04) | Grad Norm 2.4637(1.7831) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 106.2453, Epoch Time 1319.9190(1310.9071), Bit/dim 3.5530(best: 3.5380), Xent 2.8666, Loss 4.9863, Error 0.3660(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11010 | Time 22.1252(21.9140) | Bit/dim 3.5273(3.5399) | Xent 0.0361(0.0348) | Loss 3.5453(3.5573) | Error 0.0100(0.0116) Steps 922(915.07) | Grad Norm 1.6924(1.9740) | Total Time 14.00(14.00)\n",
      "Iter 11020 | Time 21.5387(21.9128) | Bit/dim 3.4919(3.5391) | Xent 0.0168(0.0351) | Loss 3.5003(3.5566) | Error 0.0056(0.0116) Steps 934(916.78) | Grad Norm 1.2077(1.9523) | Total Time 14.00(14.00)\n",
      "Iter 11030 | Time 21.6306(21.8746) | Bit/dim 3.5086(3.5396) | Xent 0.0368(0.0363) | Loss 3.5270(3.5577) | Error 0.0133(0.0124) Steps 910(915.48) | Grad Norm 2.2414(1.9994) | Total Time 14.00(14.00)\n",
      "Iter 11040 | Time 22.2997(21.9424) | Bit/dim 3.5589(3.5426) | Xent 0.0333(0.0360) | Loss 3.5755(3.5606) | Error 0.0111(0.0123) Steps 958(919.23) | Grad Norm 1.9576(2.0160) | Total Time 14.00(14.00)\n",
      "Iter 11050 | Time 21.8708(21.9490) | Bit/dim 3.5506(3.5445) | Xent 0.0696(0.0382) | Loss 3.5854(3.5636) | Error 0.0222(0.0129) Steps 922(918.94) | Grad Norm 3.7958(2.0995) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 105.7128, Epoch Time 1330.6113(1311.4982), Bit/dim 3.5507(best: 3.5380), Xent 2.9870, Loss 5.0442, Error 0.3698(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11060 | Time 21.5014(21.9388) | Bit/dim 3.5379(3.5461) | Xent 0.0237(0.0370) | Loss 3.5498(3.5646) | Error 0.0100(0.0128) Steps 928(917.97) | Grad Norm 1.4149(2.0762) | Total Time 14.00(14.00)\n",
      "Iter 11070 | Time 21.8060(21.8059) | Bit/dim 3.5319(3.5442) | Xent 0.0282(0.0353) | Loss 3.5459(3.5619) | Error 0.0067(0.0121) Steps 892(913.18) | Grad Norm 1.6029(1.9647) | Total Time 14.00(14.00)\n",
      "Iter 11080 | Time 21.6737(21.8003) | Bit/dim 3.5548(3.5437) | Xent 0.0363(0.0347) | Loss 3.5730(3.5610) | Error 0.0111(0.0117) Steps 910(910.47) | Grad Norm 2.7631(1.9155) | Total Time 14.00(14.00)\n",
      "Iter 11090 | Time 21.3914(21.6959) | Bit/dim 3.5294(3.5412) | Xent 0.0374(0.0364) | Loss 3.5481(3.5594) | Error 0.0144(0.0122) Steps 910(909.72) | Grad Norm 1.4041(1.9112) | Total Time 14.00(14.00)\n",
      "Iter 11100 | Time 22.6458(21.7250) | Bit/dim 3.5340(3.5431) | Xent 0.0303(0.0379) | Loss 3.5491(3.5620) | Error 0.0133(0.0131) Steps 934(911.99) | Grad Norm 1.7847(2.1101) | Total Time 14.00(14.00)\n",
      "Iter 11110 | Time 22.1194(21.8491) | Bit/dim 3.5193(3.5450) | Xent 0.0290(0.0382) | Loss 3.5338(3.5641) | Error 0.0111(0.0133) Steps 904(916.51) | Grad Norm 1.1863(2.0625) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 108.2524, Epoch Time 1318.2619(1311.7011), Bit/dim 3.5496(best: 3.5380), Xent 2.7962, Loss 4.9477, Error 0.3552(best: 0.3583)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11120 | Time 21.5494(21.8987) | Bit/dim 3.5435(3.5444) | Xent 0.0400(0.0380) | Loss 3.5635(3.5634) | Error 0.0133(0.0130) Steps 922(918.51) | Grad Norm 1.5902(1.9781) | Total Time 14.00(14.00)\n",
      "Iter 11130 | Time 21.2205(21.9112) | Bit/dim 3.5245(3.5439) | Xent 0.0545(0.0374) | Loss 3.5518(3.5626) | Error 0.0211(0.0129) Steps 934(921.36) | Grad Norm 2.9840(1.9551) | Total Time 14.00(14.00)\n",
      "Iter 11140 | Time 22.0970(21.9271) | Bit/dim 3.5883(3.5466) | Xent 0.0382(0.0380) | Loss 3.6074(3.5656) | Error 0.0167(0.0132) Steps 904(923.26) | Grad Norm 1.6202(1.9290) | Total Time 14.00(14.00)\n",
      "Iter 11150 | Time 21.6714(21.9831) | Bit/dim 3.5692(3.5435) | Xent 0.0483(0.0394) | Loss 3.5934(3.5632) | Error 0.0244(0.0138) Steps 922(925.24) | Grad Norm 3.4174(2.0632) | Total Time 14.00(14.00)\n",
      "Iter 11160 | Time 22.5004(21.9653) | Bit/dim 3.5595(3.5438) | Xent 0.0357(0.0400) | Loss 3.5773(3.5638) | Error 0.0078(0.0140) Steps 952(924.41) | Grad Norm 1.8334(2.0955) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 106.5972, Epoch Time 1334.5352(1312.3861), Bit/dim 3.5428(best: 3.5380), Xent 2.8027, Loss 4.9441, Error 0.3634(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11170 | Time 22.4685(22.0506) | Bit/dim 3.5949(3.5431) | Xent 0.0522(0.0396) | Loss 3.6210(3.5629) | Error 0.0144(0.0138) Steps 922(924.60) | Grad Norm 1.8999(2.0424) | Total Time 14.00(14.00)\n",
      "Iter 11180 | Time 23.0402(22.1322) | Bit/dim 3.5566(3.5423) | Xent 0.0965(0.0434) | Loss 3.6048(3.5640) | Error 0.0333(0.0148) Steps 970(926.52) | Grad Norm 4.3019(2.1442) | Total Time 14.00(14.00)\n",
      "Iter 11190 | Time 22.1542(22.2741) | Bit/dim 3.5408(3.5425) | Xent 0.0210(0.0430) | Loss 3.5513(3.5640) | Error 0.0067(0.0149) Steps 952(931.94) | Grad Norm 1.3599(2.1410) | Total Time 14.00(14.00)\n",
      "Iter 11200 | Time 21.7882(22.3549) | Bit/dim 3.5814(3.5479) | Xent 0.0525(0.0473) | Loss 3.6077(3.5716) | Error 0.0133(0.0157) Steps 898(934.74) | Grad Norm 2.1832(2.2625) | Total Time 14.00(14.00)\n",
      "Iter 11210 | Time 22.1848(22.3378) | Bit/dim 3.5527(3.5454) | Xent 0.0413(0.0456) | Loss 3.5733(3.5682) | Error 0.0189(0.0156) Steps 940(936.19) | Grad Norm 1.9419(2.1523) | Total Time 14.00(14.00)\n",
      "Iter 11220 | Time 22.3462(22.3420) | Bit/dim 3.5984(3.5476) | Xent 0.0484(0.0450) | Loss 3.6226(3.5701) | Error 0.0178(0.0155) Steps 940(937.16) | Grad Norm 2.4268(2.1138) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 107.8684, Epoch Time 1358.4149(1313.7670), Bit/dim 3.5472(best: 3.5380), Xent 2.7780, Loss 4.9362, Error 0.3628(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11230 | Time 23.2735(22.3267) | Bit/dim 3.5302(3.5461) | Xent 0.0741(0.0452) | Loss 3.5673(3.5686) | Error 0.0233(0.0155) Steps 958(937.89) | Grad Norm 3.8476(2.1296) | Total Time 14.00(14.00)\n",
      "Iter 11240 | Time 22.0942(22.2628) | Bit/dim 3.5411(3.5456) | Xent 0.0489(0.0442) | Loss 3.5655(3.5677) | Error 0.0189(0.0150) Steps 928(933.54) | Grad Norm 1.9791(2.1021) | Total Time 14.00(14.00)\n",
      "Iter 11250 | Time 22.1711(22.2299) | Bit/dim 3.5430(3.5445) | Xent 0.0612(0.0426) | Loss 3.5736(3.5658) | Error 0.0144(0.0144) Steps 892(930.00) | Grad Norm 2.0736(1.9813) | Total Time 14.00(14.00)\n",
      "Iter 11260 | Time 22.3232(22.1943) | Bit/dim 3.5179(3.5407) | Xent 0.0335(0.0402) | Loss 3.5346(3.5607) | Error 0.0111(0.0135) Steps 916(927.74) | Grad Norm 1.5563(1.8567) | Total Time 14.00(14.00)\n",
      "Iter 11270 | Time 23.0356(22.2428) | Bit/dim 3.5614(3.5409) | Xent 0.0504(0.0387) | Loss 3.5866(3.5603) | Error 0.0167(0.0130) Steps 946(930.94) | Grad Norm 2.2691(1.8009) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 108.2520, Epoch Time 1346.3991(1314.7459), Bit/dim 3.5462(best: 3.5380), Xent 2.8134, Loss 4.9529, Error 0.3604(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11280 | Time 22.6446(22.2739) | Bit/dim 3.5604(3.5424) | Xent 0.0352(0.0382) | Loss 3.5780(3.5614) | Error 0.0089(0.0130) Steps 964(935.66) | Grad Norm 1.7050(1.8363) | Total Time 14.00(14.00)\n",
      "Iter 11290 | Time 21.7797(22.2759) | Bit/dim 3.5154(3.5415) | Xent 0.0354(0.0353) | Loss 3.5331(3.5591) | Error 0.0122(0.0120) Steps 952(937.62) | Grad Norm 1.7519(1.7742) | Total Time 14.00(14.00)\n",
      "Iter 11300 | Time 22.9122(22.2650) | Bit/dim 3.5208(3.5374) | Xent 0.0484(0.0337) | Loss 3.5450(3.5543) | Error 0.0100(0.0111) Steps 904(935.43) | Grad Norm 2.1744(1.7660) | Total Time 14.00(14.00)\n",
      "Iter 11310 | Time 21.9050(22.2175) | Bit/dim 3.5281(3.5392) | Xent 0.0203(0.0351) | Loss 3.5383(3.5567) | Error 0.0033(0.0118) Steps 922(932.03) | Grad Norm 1.8405(1.8188) | Total Time 14.00(14.00)\n",
      "Iter 11320 | Time 22.2390(22.2159) | Bit/dim 3.5488(3.5406) | Xent 0.0417(0.0414) | Loss 3.5696(3.5613) | Error 0.0178(0.0139) Steps 970(933.19) | Grad Norm 2.9232(2.2143) | Total Time 14.00(14.00)\n",
      "Iter 11330 | Time 22.4882(22.2273) | Bit/dim 3.5707(3.5435) | Xent 0.0350(0.0411) | Loss 3.5882(3.5640) | Error 0.0122(0.0141) Steps 934(932.02) | Grad Norm 1.9790(2.1546) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 107.9685, Epoch Time 1346.0979(1315.6865), Bit/dim 3.5523(best: 3.5380), Xent 2.7900, Loss 4.9473, Error 0.3684(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11340 | Time 21.7802(22.2053) | Bit/dim 3.5734(3.5458) | Xent 0.0393(0.0407) | Loss 3.5931(3.5661) | Error 0.0111(0.0145) Steps 946(931.92) | Grad Norm 1.6341(2.1019) | Total Time 14.00(14.00)\n",
      "Iter 11350 | Time 23.6340(22.2004) | Bit/dim 3.5291(3.5447) | Xent 0.0251(0.0413) | Loss 3.5416(3.5653) | Error 0.0122(0.0148) Steps 952(931.43) | Grad Norm 1.4350(2.0486) | Total Time 14.00(14.00)\n",
      "Iter 11360 | Time 22.9077(22.3825) | Bit/dim 3.5476(3.5432) | Xent 0.0360(0.0405) | Loss 3.5656(3.5634) | Error 0.0144(0.0143) Steps 946(934.08) | Grad Norm 1.7644(1.9691) | Total Time 14.00(14.00)\n",
      "Iter 11370 | Time 23.1111(22.4079) | Bit/dim 3.5299(3.5389) | Xent 0.0346(0.0396) | Loss 3.5473(3.5587) | Error 0.0133(0.0139) Steps 934(936.81) | Grad Norm 2.0170(1.9323) | Total Time 14.00(14.00)\n",
      "Iter 11380 | Time 21.9946(22.3828) | Bit/dim 3.5497(3.5400) | Xent 0.0256(0.0365) | Loss 3.5625(3.5582) | Error 0.0089(0.0129) Steps 916(937.55) | Grad Norm 1.2865(1.8318) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 107.7309, Epoch Time 1353.6577(1316.8256), Bit/dim 3.5335(best: 3.5380), Xent 2.7941, Loss 4.9306, Error 0.3615(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11390 | Time 21.4663(22.1925) | Bit/dim 3.5231(3.5390) | Xent 0.0182(0.0335) | Loss 3.5322(3.5557) | Error 0.0067(0.0118) Steps 892(932.02) | Grad Norm 1.0336(1.6944) | Total Time 14.00(14.00)\n",
      "Iter 11400 | Time 22.0488(22.1456) | Bit/dim 3.5437(3.5361) | Xent 0.0291(0.0309) | Loss 3.5583(3.5515) | Error 0.0078(0.0108) Steps 892(927.77) | Grad Norm 1.2174(1.5684) | Total Time 14.00(14.00)\n",
      "Iter 11410 | Time 22.1750(22.2016) | Bit/dim 3.5315(3.5324) | Xent 0.0311(0.0301) | Loss 3.5471(3.5474) | Error 0.0089(0.0103) Steps 922(927.42) | Grad Norm 1.1191(1.4985) | Total Time 14.00(14.00)\n",
      "Iter 11420 | Time 22.1597(22.2664) | Bit/dim 3.5620(3.5343) | Xent 0.0244(0.0291) | Loss 3.5742(3.5489) | Error 0.0100(0.0100) Steps 928(926.69) | Grad Norm 2.2272(1.4880) | Total Time 14.00(14.00)\n",
      "Iter 11430 | Time 21.6703(22.1907) | Bit/dim 3.5191(3.5308) | Xent 0.0229(0.0281) | Loss 3.5306(3.5448) | Error 0.0078(0.0096) Steps 934(929.04) | Grad Norm 1.9802(1.5140) | Total Time 14.00(14.00)\n",
      "Iter 11440 | Time 22.2416(22.1909) | Bit/dim 3.5195(3.5312) | Xent 0.0201(0.0269) | Loss 3.5295(3.5446) | Error 0.0056(0.0093) Steps 928(929.67) | Grad Norm 1.2037(1.5105) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 107.2649, Epoch Time 1340.4478(1317.5343), Bit/dim 3.5350(best: 3.5335), Xent 2.8247, Loss 4.9473, Error 0.3626(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11450 | Time 22.1359(22.2525) | Bit/dim 3.5379(3.5311) | Xent 0.0226(0.0251) | Loss 3.5492(3.5437) | Error 0.0056(0.0085) Steps 898(929.03) | Grad Norm 1.4536(1.4594) | Total Time 14.00(14.00)\n",
      "Iter 11460 | Time 23.0612(22.2916) | Bit/dim 3.5591(3.5316) | Xent 0.0367(0.0253) | Loss 3.5774(3.5442) | Error 0.0111(0.0085) Steps 970(930.16) | Grad Norm 1.9420(1.4774) | Total Time 14.00(14.00)\n",
      "Iter 11470 | Time 22.3188(22.2878) | Bit/dim 3.5098(3.5304) | Xent 0.0344(0.0270) | Loss 3.5270(3.5439) | Error 0.0144(0.0091) Steps 934(928.20) | Grad Norm 1.6244(1.5251) | Total Time 14.00(14.00)\n",
      "Iter 11480 | Time 22.1873(22.2958) | Bit/dim 3.4901(3.5315) | Xent 0.0374(0.0288) | Loss 3.5088(3.5459) | Error 0.0111(0.0095) Steps 880(925.56) | Grad Norm 1.8718(1.6500) | Total Time 14.00(14.00)\n",
      "Iter 11490 | Time 22.9556(22.3839) | Bit/dim 3.5556(3.5327) | Xent 0.0461(0.0294) | Loss 3.5787(3.5474) | Error 0.0133(0.0094) Steps 940(929.12) | Grad Norm 1.6650(1.6501) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 108.5903, Epoch Time 1356.2127(1318.6947), Bit/dim 3.5341(best: 3.5335), Xent 2.7351, Loss 4.9017, Error 0.3633(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11500 | Time 20.9770(22.2750) | Bit/dim 3.5419(3.5322) | Xent 0.0284(0.0276) | Loss 3.5561(3.5460) | Error 0.0089(0.0091) Steps 922(931.84) | Grad Norm 1.0922(1.5702) | Total Time 14.00(14.00)\n",
      "Iter 11510 | Time 22.0281(22.2093) | Bit/dim 3.5308(3.5315) | Xent 0.0287(0.0262) | Loss 3.5451(3.5446) | Error 0.0078(0.0086) Steps 910(931.01) | Grad Norm 1.6910(1.5179) | Total Time 14.00(14.00)\n",
      "Iter 11520 | Time 22.0576(22.1693) | Bit/dim 3.5384(3.5298) | Xent 0.0233(0.0249) | Loss 3.5500(3.5423) | Error 0.0111(0.0084) Steps 940(926.73) | Grad Norm 1.2872(1.4827) | Total Time 14.00(14.00)\n",
      "Iter 11530 | Time 21.2017(22.1765) | Bit/dim 3.5218(3.5290) | Xent 0.0310(0.0240) | Loss 3.5373(3.5410) | Error 0.0100(0.0080) Steps 928(927.41) | Grad Norm 1.1033(1.4197) | Total Time 14.00(14.00)\n",
      "Iter 11540 | Time 22.8315(22.2933) | Bit/dim 3.5071(3.5273) | Xent 0.0139(0.0241) | Loss 3.5140(3.5393) | Error 0.0056(0.0081) Steps 952(930.40) | Grad Norm 1.7605(1.4747) | Total Time 14.00(14.00)\n",
      "Iter 11550 | Time 22.2583(22.3552) | Bit/dim 3.5033(3.5265) | Xent 0.0299(0.0242) | Loss 3.5183(3.5386) | Error 0.0111(0.0078) Steps 928(932.28) | Grad Norm 1.3861(1.4527) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 107.8586, Epoch Time 1347.5783(1319.5612), Bit/dim 3.5292(best: 3.5335), Xent 2.7847, Loss 4.9216, Error 0.3650(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11560 | Time 22.8033(22.3854) | Bit/dim 3.5295(3.5248) | Xent 0.0236(0.0231) | Loss 3.5413(3.5364) | Error 0.0089(0.0076) Steps 934(932.60) | Grad Norm 1.6659(1.4221) | Total Time 14.00(14.00)\n",
      "Iter 11570 | Time 21.8714(22.3530) | Bit/dim 3.5094(3.5242) | Xent 0.0135(0.0215) | Loss 3.5161(3.5350) | Error 0.0033(0.0072) Steps 928(932.07) | Grad Norm 0.8980(1.3727) | Total Time 14.00(14.00)\n",
      "Iter 11580 | Time 21.7946(22.2928) | Bit/dim 3.5138(3.5229) | Xent 0.0138(0.0204) | Loss 3.5207(3.5331) | Error 0.0044(0.0067) Steps 886(926.59) | Grad Norm 1.2646(1.3390) | Total Time 14.00(14.00)\n",
      "Iter 11590 | Time 22.5058(22.2829) | Bit/dim 3.5208(3.5245) | Xent 0.0192(0.0230) | Loss 3.5304(3.5360) | Error 0.0078(0.0075) Steps 904(924.56) | Grad Norm 1.4410(1.4694) | Total Time 14.00(14.00)\n",
      "Iter 11600 | Time 21.9512(22.1215) | Bit/dim 3.5274(3.5272) | Xent 0.0136(0.0254) | Loss 3.5342(3.5399) | Error 0.0044(0.0082) Steps 934(921.90) | Grad Norm 1.4982(1.6382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 105.9801, Epoch Time 1339.2549(1320.1520), Bit/dim 3.5363(best: 3.5292), Xent 2.8901, Loss 4.9814, Error 0.3723(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11610 | Time 21.1822(22.0244) | Bit/dim 3.5334(3.5277) | Xent 0.0392(0.0282) | Loss 3.5530(3.5418) | Error 0.0133(0.0093) Steps 904(919.26) | Grad Norm 2.7770(1.7903) | Total Time 14.00(14.00)\n",
      "Iter 11620 | Time 22.0005(21.9511) | Bit/dim 3.5393(3.5292) | Xent 0.0218(0.0265) | Loss 3.5503(3.5425) | Error 0.0089(0.0088) Steps 886(916.81) | Grad Norm 1.5852(1.6894) | Total Time 14.00(14.00)\n",
      "Iter 11630 | Time 22.6860(22.0380) | Bit/dim 3.5033(3.5258) | Xent 0.0245(0.0264) | Loss 3.5155(3.5391) | Error 0.0100(0.0088) Steps 928(918.52) | Grad Norm 1.7624(1.6706) | Total Time 14.00(14.00)\n",
      "Iter 11640 | Time 21.5705(22.1408) | Bit/dim 3.5586(3.5292) | Xent 0.0208(0.0265) | Loss 3.5690(3.5424) | Error 0.0089(0.0090) Steps 898(920.66) | Grad Norm 1.0773(1.6060) | Total Time 14.00(14.00)\n",
      "Iter 11650 | Time 22.7224(22.1790) | Bit/dim 3.5200(3.5275) | Xent 0.0226(0.0263) | Loss 3.5313(3.5407) | Error 0.0067(0.0089) Steps 952(922.33) | Grad Norm 1.6970(1.5734) | Total Time 14.00(14.00)\n",
      "Iter 11660 | Time 22.8865(22.1617) | Bit/dim 3.5500(3.5297) | Xent 0.0273(0.0260) | Loss 3.5637(3.5427) | Error 0.0100(0.0088) Steps 916(919.88) | Grad Norm 1.5568(1.5552) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 106.7956, Epoch Time 1339.9473(1320.7458), Bit/dim 3.5285(best: 3.5292), Xent 2.7971, Loss 4.9271, Error 0.3561(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11670 | Time 21.6814(22.1469) | Bit/dim 3.5163(3.5301) | Xent 0.0286(0.0248) | Loss 3.5306(3.5424) | Error 0.0089(0.0085) Steps 928(920.54) | Grad Norm 1.0658(1.4821) | Total Time 14.00(14.00)\n",
      "Iter 11680 | Time 23.0024(22.2685) | Bit/dim 3.5133(3.5279) | Xent 0.0191(0.0238) | Loss 3.5229(3.5398) | Error 0.0056(0.0078) Steps 922(923.54) | Grad Norm 1.1721(1.4177) | Total Time 14.00(14.00)\n",
      "Iter 11690 | Time 22.4025(22.3634) | Bit/dim 3.5207(3.5277) | Xent 0.0098(0.0219) | Loss 3.5256(3.5387) | Error 0.0033(0.0069) Steps 940(931.17) | Grad Norm 1.0140(1.3634) | Total Time 14.00(14.00)\n",
      "Iter 11700 | Time 21.7310(22.3224) | Bit/dim 3.5066(3.5250) | Xent 0.0098(0.0213) | Loss 3.5115(3.5356) | Error 0.0022(0.0069) Steps 928(927.10) | Grad Norm 1.1177(1.3099) | Total Time 14.00(14.00)\n",
      "Iter 11710 | Time 22.7832(22.2679) | Bit/dim 3.5134(3.5247) | Xent 0.0181(0.0203) | Loss 3.5225(3.5348) | Error 0.0067(0.0066) Steps 934(925.57) | Grad Norm 1.1365(1.3092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 108.9334, Epoch Time 1355.1450(1321.7778), Bit/dim 3.5218(best: 3.5285), Xent 2.7927, Loss 4.9181, Error 0.3565(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11720 | Time 22.5713(22.3815) | Bit/dim 3.5267(3.5219) | Xent 0.0179(0.0195) | Loss 3.5356(3.5317) | Error 0.0056(0.0063) Steps 910(927.33) | Grad Norm 1.2608(1.2592) | Total Time 14.00(14.00)\n",
      "Iter 11730 | Time 22.3066(22.3551) | Bit/dim 3.5144(3.5232) | Xent 0.0090(0.0191) | Loss 3.5189(3.5328) | Error 0.0022(0.0063) Steps 916(926.59) | Grad Norm 0.9182(1.2544) | Total Time 14.00(14.00)\n",
      "Iter 11740 | Time 22.9891(22.3165) | Bit/dim 3.5103(3.5201) | Xent 0.0170(0.0184) | Loss 3.5188(3.5293) | Error 0.0067(0.0061) Steps 940(925.53) | Grad Norm 1.2023(1.2024) | Total Time 14.00(14.00)\n",
      "Iter 11750 | Time 21.9805(22.1813) | Bit/dim 3.4895(3.5198) | Xent 0.0098(0.0176) | Loss 3.4944(3.5286) | Error 0.0022(0.0057) Steps 904(918.69) | Grad Norm 0.7048(1.1261) | Total Time 14.00(14.00)\n",
      "Iter 11760 | Time 21.9952(22.1845) | Bit/dim 3.5105(3.5174) | Xent 0.0167(0.0177) | Loss 3.5188(3.5263) | Error 0.0067(0.0059) Steps 892(916.29) | Grad Norm 1.1847(1.1615) | Total Time 14.00(14.00)\n",
      "Iter 11770 | Time 22.8511(22.1287) | Bit/dim 3.5443(3.5184) | Xent 0.0106(0.0173) | Loss 3.5496(3.5271) | Error 0.0022(0.0056) Steps 886(914.34) | Grad Norm 0.9020(1.1664) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 105.8199, Epoch Time 1340.4392(1322.3377), Bit/dim 3.5219(best: 3.5218), Xent 2.7832, Loss 4.9135, Error 0.3526(best: 0.3552)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11780 | Time 23.4924(22.2658) | Bit/dim 3.5495(3.5212) | Xent 0.0181(0.0182) | Loss 3.5586(3.5303) | Error 0.0078(0.0061) Steps 934(919.14) | Grad Norm 1.0503(1.1973) | Total Time 14.00(14.00)\n",
      "Iter 11790 | Time 22.9139(22.2997) | Bit/dim 3.4910(3.5220) | Xent 0.0279(0.0179) | Loss 3.5049(3.5309) | Error 0.0122(0.0062) Steps 898(919.03) | Grad Norm 1.5609(1.2097) | Total Time 14.00(14.00)\n",
      "Iter 11800 | Time 22.3198(22.2239) | Bit/dim 3.4931(3.5189) | Xent 0.0126(0.0180) | Loss 3.4994(3.5279) | Error 0.0044(0.0062) Steps 898(916.79) | Grad Norm 1.1555(1.2358) | Total Time 14.00(14.00)\n",
      "Iter 11810 | Time 22.4094(22.2114) | Bit/dim 3.5421(3.5203) | Xent 0.0207(0.0175) | Loss 3.5524(3.5290) | Error 0.0078(0.0059) Steps 904(916.32) | Grad Norm 1.2967(1.2520) | Total Time 14.00(14.00)\n",
      "Iter 11820 | Time 22.5988(22.2717) | Bit/dim 3.5113(3.5186) | Xent 0.0150(0.0170) | Loss 3.5188(3.5272) | Error 0.0044(0.0058) Steps 928(916.38) | Grad Norm 1.7321(1.3746) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 104.3964, Epoch Time 1347.6132(1323.0959), Bit/dim 3.5229(best: 3.5218), Xent 2.7735, Loss 4.9096, Error 0.3495(best: 0.3526)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11830 | Time 22.2620(22.2420) | Bit/dim 3.5067(3.5155) | Xent 0.0220(0.0164) | Loss 3.5177(3.5237) | Error 0.0078(0.0056) Steps 910(916.65) | Grad Norm 1.7899(1.4030) | Total Time 14.00(14.00)\n",
      "Iter 11840 | Time 22.3120(22.1976) | Bit/dim 3.5150(3.5168) | Xent 0.0185(0.0158) | Loss 3.5242(3.5247) | Error 0.0078(0.0054) Steps 928(917.50) | Grad Norm 1.6207(1.3801) | Total Time 14.00(14.00)\n",
      "Iter 11850 | Time 21.5997(22.1711) | Bit/dim 3.5243(3.5165) | Xent 0.0067(0.0147) | Loss 3.5276(3.5238) | Error 0.0000(0.0050) Steps 910(919.17) | Grad Norm 0.9208(1.3193) | Total Time 14.00(14.00)\n",
      "Iter 11860 | Time 21.8157(22.1495) | Bit/dim 3.5312(3.5176) | Xent 0.0183(0.0149) | Loss 3.5403(3.5250) | Error 0.0078(0.0049) Steps 898(919.60) | Grad Norm 1.6239(1.2570) | Total Time 14.00(14.00)\n",
      "Iter 11870 | Time 22.5826(22.2196) | Bit/dim 3.5181(3.5164) | Xent 0.0164(0.0158) | Loss 3.5263(3.5243) | Error 0.0078(0.0053) Steps 916(923.17) | Grad Norm 0.9922(1.2206) | Total Time 14.00(14.00)\n",
      "Iter 11880 | Time 22.0076(22.2655) | Bit/dim 3.5370(3.5174) | Xent 0.0212(0.0174) | Loss 3.5475(3.5261) | Error 0.0056(0.0057) Steps 940(927.43) | Grad Norm 1.2170(1.2443) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 107.3909, Epoch Time 1344.6190(1323.7416), Bit/dim 3.5247(best: 3.5218), Xent 2.9006, Loss 4.9750, Error 0.3587(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11890 | Time 22.2909(22.2675) | Bit/dim 3.5394(3.5185) | Xent 0.0179(0.0178) | Loss 3.5483(3.5274) | Error 0.0067(0.0062) Steps 928(928.46) | Grad Norm 1.0494(1.2475) | Total Time 14.00(14.00)\n",
      "Iter 11900 | Time 22.3328(22.2396) | Bit/dim 3.5731(3.5190) | Xent 0.0205(0.0198) | Loss 3.5834(3.5289) | Error 0.0067(0.0066) Steps 922(929.07) | Grad Norm 1.8000(1.3348) | Total Time 14.00(14.00)\n",
      "Iter 11910 | Time 23.0152(22.2931) | Bit/dim 3.5117(3.5208) | Xent 0.0225(0.0213) | Loss 3.5230(3.5314) | Error 0.0056(0.0069) Steps 934(928.88) | Grad Norm 1.7574(1.4534) | Total Time 14.00(14.00)\n",
      "Iter 11920 | Time 22.0997(22.2351) | Bit/dim 3.5276(3.5237) | Xent 0.0298(0.0239) | Loss 3.5424(3.5356) | Error 0.0111(0.0084) Steps 946(930.56) | Grad Norm 1.9875(1.6034) | Total Time 14.00(14.00)\n",
      "Iter 11930 | Time 22.6104(22.2584) | Bit/dim 3.4949(3.5259) | Xent 0.0081(0.0257) | Loss 3.4990(3.5388) | Error 0.0011(0.0086) Steps 958(932.71) | Grad Norm 1.6988(1.7506) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 106.7249, Epoch Time 1348.8516(1324.4949), Bit/dim 3.5352(best: 3.5218), Xent 2.7966, Loss 4.9335, Error 0.3573(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11940 | Time 22.7977(22.3637) | Bit/dim 3.5194(3.5289) | Xent 0.1315(0.0327) | Loss 3.5851(3.5453) | Error 0.0356(0.0107) Steps 904(932.60) | Grad Norm 7.0702(2.1190) | Total Time 14.00(14.00)\n",
      "Iter 11950 | Time 21.5789(22.3367) | Bit/dim 3.5031(3.5353) | Xent 0.0179(0.0341) | Loss 3.5120(3.5523) | Error 0.0067(0.0113) Steps 934(934.90) | Grad Norm 1.5100(2.1662) | Total Time 14.00(14.00)\n",
      "Iter 11960 | Time 22.8463(22.3927) | Bit/dim 3.5097(3.5352) | Xent 0.0409(0.0335) | Loss 3.5301(3.5520) | Error 0.0133(0.0113) Steps 958(935.05) | Grad Norm 1.8713(2.0234) | Total Time 14.00(14.00)\n",
      "Iter 11970 | Time 22.8711(22.4185) | Bit/dim 3.5655(3.5342) | Xent 0.0241(0.0317) | Loss 3.5775(3.5500) | Error 0.0089(0.0105) Steps 970(935.54) | Grad Norm 2.0091(1.9120) | Total Time 14.00(14.00)\n",
      "Iter 11980 | Time 22.1891(22.4673) | Bit/dim 3.5282(3.5330) | Xent 0.0276(0.0313) | Loss 3.5420(3.5486) | Error 0.0078(0.0107) Steps 898(935.94) | Grad Norm 1.2982(1.8115) | Total Time 14.00(14.00)\n",
      "Iter 11990 | Time 22.3272(22.5288) | Bit/dim 3.5155(3.5313) | Xent 0.0183(0.0311) | Loss 3.5247(3.5469) | Error 0.0056(0.0105) Steps 958(937.56) | Grad Norm 1.7111(1.8013) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 109.9723, Epoch Time 1365.5677(1325.7271), Bit/dim 3.5310(best: 3.5218), Xent 2.8827, Loss 4.9724, Error 0.3602(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12000 | Time 22.3171(22.4944) | Bit/dim 3.5410(3.5292) | Xent 0.0164(0.0285) | Loss 3.5492(3.5434) | Error 0.0056(0.0098) Steps 928(936.81) | Grad Norm 1.1466(1.7131) | Total Time 14.00(14.00)\n",
      "Iter 12010 | Time 22.5208(22.4188) | Bit/dim 3.5749(3.5285) | Xent 0.0221(0.0269) | Loss 3.5859(3.5419) | Error 0.0056(0.0090) Steps 940(934.76) | Grad Norm 1.2753(1.6247) | Total Time 14.00(14.00)\n",
      "Iter 12020 | Time 23.4919(22.4673) | Bit/dim 3.5173(3.5262) | Xent 0.0131(0.0265) | Loss 3.5239(3.5394) | Error 0.0044(0.0090) Steps 958(936.10) | Grad Norm 1.2953(1.5701) | Total Time 14.00(14.00)\n",
      "Iter 12030 | Time 21.8076(22.4663) | Bit/dim 3.6036(3.5263) | Xent 0.0385(0.0287) | Loss 3.6229(3.5407) | Error 0.0133(0.0100) Steps 892(932.73) | Grad Norm 1.7917(1.6939) | Total Time 14.00(14.00)\n",
      "Iter 12040 | Time 22.7513(22.4868) | Bit/dim 3.5212(3.5304) | Xent 0.0250(0.0293) | Loss 3.5337(3.5450) | Error 0.0078(0.0102) Steps 892(930.93) | Grad Norm 2.1630(1.8258) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 107.0702, Epoch Time 1355.9471(1326.6337), Bit/dim 3.5303(best: 3.5218), Xent 3.0254, Loss 5.0430, Error 0.3626(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12050 | Time 22.1503(22.4115) | Bit/dim 3.5230(3.5321) | Xent 0.0182(0.0287) | Loss 3.5321(3.5465) | Error 0.0078(0.0100) Steps 946(929.65) | Grad Norm 1.3342(1.8195) | Total Time 14.00(14.00)\n",
      "Iter 12060 | Time 22.7001(22.4396) | Bit/dim 3.4857(3.5285) | Xent 0.0138(0.0269) | Loss 3.4926(3.5420) | Error 0.0033(0.0093) Steps 964(935.03) | Grad Norm 0.9674(1.7114) | Total Time 14.00(14.00)\n",
      "Iter 12070 | Time 22.9202(22.4279) | Bit/dim 3.5240(3.5272) | Xent 0.0249(0.0248) | Loss 3.5364(3.5396) | Error 0.0078(0.0084) Steps 970(937.57) | Grad Norm 1.2366(1.5936) | Total Time 14.00(14.00)\n",
      "Iter 12080 | Time 22.5263(22.4354) | Bit/dim 3.5249(3.5253) | Xent 0.0277(0.0236) | Loss 3.5388(3.5372) | Error 0.0089(0.0079) Steps 946(935.49) | Grad Norm 2.1413(1.5824) | Total Time 14.00(14.00)\n",
      "Iter 12090 | Time 21.8909(22.3715) | Bit/dim 3.5093(3.5260) | Xent 0.0313(0.0231) | Loss 3.5249(3.5375) | Error 0.0133(0.0080) Steps 922(932.72) | Grad Norm 1.6881(1.5947) | Total Time 14.00(14.00)\n",
      "Iter 12100 | Time 22.1087(22.4303) | Bit/dim 3.5224(3.5227) | Xent 0.0155(0.0218) | Loss 3.5301(3.5336) | Error 0.0067(0.0074) Steps 916(932.80) | Grad Norm 1.4831(1.5151) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 107.4973, Epoch Time 1357.0177(1327.5452), Bit/dim 3.5236(best: 3.5218), Xent 2.8709, Loss 4.9591, Error 0.3573(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12110 | Time 22.2815(22.3833) | Bit/dim 3.5196(3.5223) | Xent 0.0194(0.0217) | Loss 3.5293(3.5331) | Error 0.0089(0.0074) Steps 946(931.54) | Grad Norm 1.1482(1.4580) | Total Time 14.00(14.00)\n",
      "Iter 12120 | Time 21.2818(22.3601) | Bit/dim 3.5468(3.5219) | Xent 0.0377(0.0227) | Loss 3.5657(3.5333) | Error 0.0100(0.0076) Steps 904(930.82) | Grad Norm 1.5926(1.4504) | Total Time 14.00(14.00)\n",
      "Iter 12130 | Time 22.8386(22.3472) | Bit/dim 3.5126(3.5244) | Xent 0.0349(0.0248) | Loss 3.5300(3.5368) | Error 0.0111(0.0087) Steps 946(932.05) | Grad Norm 1.7302(1.4845) | Total Time 14.00(14.00)\n",
      "Iter 12140 | Time 23.3748(22.3734) | Bit/dim 3.5274(3.5256) | Xent 0.0251(0.0267) | Loss 3.5399(3.5389) | Error 0.0067(0.0091) Steps 970(938.07) | Grad Norm 1.3326(1.5952) | Total Time 14.00(14.00)\n",
      "Iter 12150 | Time 22.2888(22.3710) | Bit/dim 3.5325(3.5274) | Xent 0.0164(0.0275) | Loss 3.5407(3.5412) | Error 0.0044(0.0097) Steps 946(937.38) | Grad Norm 1.3049(1.5964) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 107.5622, Epoch Time 1352.5468(1328.2953), Bit/dim 3.5308(best: 3.5218), Xent 2.8419, Loss 4.9518, Error 0.3581(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12160 | Time 22.7595(22.4433) | Bit/dim 3.5281(3.5255) | Xent 0.0286(0.0287) | Loss 3.5424(3.5399) | Error 0.0089(0.0099) Steps 976(939.41) | Grad Norm 1.2029(1.5538) | Total Time 14.00(14.00)\n",
      "Iter 12170 | Time 22.2193(22.4283) | Bit/dim 3.4946(3.5237) | Xent 0.0310(0.0278) | Loss 3.5101(3.5376) | Error 0.0111(0.0096) Steps 916(936.74) | Grad Norm 1.7892(1.5260) | Total Time 14.00(14.00)\n",
      "Iter 12180 | Time 23.3529(22.4479) | Bit/dim 3.5306(3.5257) | Xent 0.0198(0.0273) | Loss 3.5405(3.5394) | Error 0.0067(0.0092) Steps 898(931.73) | Grad Norm 1.1165(1.5261) | Total Time 14.00(14.00)\n",
      "Iter 12190 | Time 21.7100(22.3238) | Bit/dim 3.5556(3.5274) | Xent 0.0339(0.0276) | Loss 3.5726(3.5412) | Error 0.0067(0.0094) Steps 934(931.17) | Grad Norm 1.9516(1.5748) | Total Time 14.00(14.00)\n",
      "Iter 12200 | Time 22.6972(22.2573) | Bit/dim 3.5435(3.5282) | Xent 0.0150(0.0270) | Loss 3.5510(3.5417) | Error 0.0044(0.0091) Steps 922(932.22) | Grad Norm 1.6357(1.6180) | Total Time 14.00(14.00)\n",
      "Iter 12210 | Time 22.7523(22.3460) | Bit/dim 3.5273(3.5240) | Xent 0.0159(0.0270) | Loss 3.5353(3.5375) | Error 0.0078(0.0095) Steps 958(935.60) | Grad Norm 1.2312(1.5687) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 109.6787, Epoch Time 1354.8517(1329.0920), Bit/dim 3.5283(best: 3.5218), Xent 2.9857, Loss 5.0212, Error 0.3623(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12220 | Time 22.1269(22.4061) | Bit/dim 3.5522(3.5247) | Xent 0.0265(0.0256) | Loss 3.5655(3.5375) | Error 0.0111(0.0091) Steps 940(937.54) | Grad Norm 1.5331(1.4759) | Total Time 14.00(14.00)\n",
      "Iter 12230 | Time 21.9833(22.3211) | Bit/dim 3.4978(3.5247) | Xent 0.0193(0.0252) | Loss 3.5074(3.5373) | Error 0.0078(0.0093) Steps 910(933.28) | Grad Norm 1.3290(1.4724) | Total Time 14.00(14.00)\n",
      "Iter 12240 | Time 22.5147(22.3725) | Bit/dim 3.5097(3.5239) | Xent 0.0307(0.0259) | Loss 3.5251(3.5368) | Error 0.0144(0.0093) Steps 934(934.54) | Grad Norm 1.4703(1.4954) | Total Time 14.00(14.00)\n",
      "Iter 12250 | Time 22.5534(22.3832) | Bit/dim 3.5226(3.5269) | Xent 0.0630(0.0302) | Loss 3.5541(3.5420) | Error 0.0156(0.0105) Steps 946(938.47) | Grad Norm 4.3279(1.7557) | Total Time 14.00(14.00)\n",
      "Iter 12260 | Time 22.5258(22.3387) | Bit/dim 3.5197(3.5306) | Xent 0.0488(0.0333) | Loss 3.5441(3.5472) | Error 0.0156(0.0115) Steps 934(938.73) | Grad Norm 2.2874(1.9067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 108.2071, Epoch Time 1355.4681(1329.8832), Bit/dim 3.5394(best: 3.5218), Xent 2.9364, Loss 5.0076, Error 0.3648(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12270 | Time 22.7250(22.3872) | Bit/dim 3.4815(3.5318) | Xent 0.0494(0.0339) | Loss 3.5062(3.5488) | Error 0.0156(0.0117) Steps 964(938.73) | Grad Norm 2.2932(1.9478) | Total Time 14.00(14.00)\n",
      "Iter 12280 | Time 21.7880(22.3812) | Bit/dim 3.5380(3.5348) | Xent 0.0431(0.0331) | Loss 3.5596(3.5514) | Error 0.0133(0.0113) Steps 934(938.14) | Grad Norm 2.0857(1.9091) | Total Time 14.00(14.00)\n",
      "Iter 12290 | Time 21.8063(22.3490) | Bit/dim 3.5332(3.5308) | Xent 0.0650(0.0363) | Loss 3.5657(3.5490) | Error 0.0244(0.0126) Steps 916(936.02) | Grad Norm 2.5623(1.9301) | Total Time 14.00(14.00)\n",
      "Iter 12300 | Time 21.6790(22.3283) | Bit/dim 3.5419(3.5339) | Xent 0.0424(0.0379) | Loss 3.5631(3.5529) | Error 0.0156(0.0131) Steps 934(934.71) | Grad Norm 2.4900(2.1475) | Total Time 14.00(14.00)\n",
      "Iter 12310 | Time 22.2249(22.2652) | Bit/dim 3.5277(3.5367) | Xent 0.0472(0.0372) | Loss 3.5513(3.5554) | Error 0.0189(0.0130) Steps 916(936.05) | Grad Norm 1.9583(2.1044) | Total Time 14.00(14.00)\n",
      "Iter 12320 | Time 23.2778(22.2709) | Bit/dim 3.5290(3.5351) | Xent 0.0510(0.0396) | Loss 3.5545(3.5549) | Error 0.0189(0.0140) Steps 952(939.13) | Grad Norm 1.7819(2.0696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 111.9355, Epoch Time 1353.0943(1330.5796), Bit/dim 3.5438(best: 3.5218), Xent 2.9831, Loss 5.0354, Error 0.3739(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12330 | Time 22.1991(22.3866) | Bit/dim 3.5056(3.5351) | Xent 0.0252(0.0415) | Loss 3.5182(3.5558) | Error 0.0089(0.0146) Steps 958(943.91) | Grad Norm 1.5020(2.0581) | Total Time 14.00(14.00)\n",
      "Iter 12340 | Time 22.2180(22.4271) | Bit/dim 3.5618(3.5387) | Xent 0.0536(0.0426) | Loss 3.5886(3.5600) | Error 0.0178(0.0147) Steps 916(944.18) | Grad Norm 1.8193(1.9759) | Total Time 14.00(14.00)\n",
      "Iter 12350 | Time 22.3236(22.3468) | Bit/dim 3.5022(3.5367) | Xent 0.0372(0.0423) | Loss 3.5208(3.5579) | Error 0.0133(0.0145) Steps 958(941.80) | Grad Norm 1.7016(1.9075) | Total Time 14.00(14.00)\n",
      "Iter 12360 | Time 22.2913(22.3689) | Bit/dim 3.5243(3.5362) | Xent 0.0341(0.0405) | Loss 3.5413(3.5564) | Error 0.0111(0.0139) Steps 970(945.51) | Grad Norm 1.6972(1.8041) | Total Time 14.00(14.00)\n",
      "Iter 12370 | Time 22.5587(22.3969) | Bit/dim 3.5354(3.5344) | Xent 0.0484(0.0408) | Loss 3.5596(3.5548) | Error 0.0144(0.0139) Steps 952(945.13) | Grad Norm 1.6028(1.7795) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 110.2882, Epoch Time 1362.3806(1331.5336), Bit/dim 3.5340(best: 3.5218), Xent 2.7625, Loss 4.9152, Error 0.3570(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12380 | Time 23.9184(22.5083) | Bit/dim 3.5609(3.5329) | Xent 0.0590(0.0397) | Loss 3.5904(3.5527) | Error 0.0133(0.0132) Steps 922(945.62) | Grad Norm 2.0411(1.7648) | Total Time 14.00(14.00)\n",
      "Iter 12390 | Time 22.1525(22.6815) | Bit/dim 3.5237(3.5299) | Xent 0.0194(0.0378) | Loss 3.5334(3.5487) | Error 0.0089(0.0126) Steps 916(949.20) | Grad Norm 1.4438(1.7151) | Total Time 14.00(14.00)\n",
      "Iter 12400 | Time 22.1085(22.6801) | Bit/dim 3.5182(3.5298) | Xent 0.0140(0.0359) | Loss 3.5252(3.5477) | Error 0.0022(0.0118) Steps 958(947.18) | Grad Norm 1.2093(1.6827) | Total Time 14.00(14.00)\n",
      "Iter 12410 | Time 21.6548(22.5333) | Bit/dim 3.5457(3.5301) | Xent 0.0964(0.0412) | Loss 3.5939(3.5507) | Error 0.0300(0.0140) Steps 910(944.83) | Grad Norm 6.6797(2.0182) | Total Time 14.00(14.00)\n",
      "Iter 12420 | Time 22.8672(22.5675) | Bit/dim 3.5584(3.5390) | Xent 0.1415(0.0615) | Loss 3.6291(3.5698) | Error 0.0411(0.0202) Steps 964(946.87) | Grad Norm 3.6425(2.5035) | Total Time 14.00(14.00)\n",
      "Iter 12430 | Time 23.0571(22.5773) | Bit/dim 3.5511(3.5473) | Xent 0.0669(0.0737) | Loss 3.5845(3.5841) | Error 0.0189(0.0235) Steps 988(953.89) | Grad Norm 2.4334(2.7672) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 113.1439, Epoch Time 1376.9112(1332.8949), Bit/dim 3.5702(best: 3.5218), Xent 3.0995, Loss 5.1199, Error 0.3677(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12440 | Time 22.6000(22.6446) | Bit/dim 3.5273(3.5486) | Xent 0.0854(0.0730) | Loss 3.5700(3.5851) | Error 0.0289(0.0234) Steps 946(955.32) | Grad Norm 2.2983(2.6000) | Total Time 14.00(14.00)\n",
      "Iter 12450 | Time 23.0034(22.7093) | Bit/dim 3.5856(3.5508) | Xent 0.0500(0.0708) | Loss 3.6106(3.5862) | Error 0.0178(0.0232) Steps 982(960.55) | Grad Norm 2.4279(2.6485) | Total Time 14.00(14.00)\n",
      "Iter 12460 | Time 23.0029(22.8129) | Bit/dim 3.5313(3.5506) | Xent 0.0631(0.0690) | Loss 3.5628(3.5851) | Error 0.0167(0.0226) Steps 982(962.67) | Grad Norm 2.0880(2.5527) | Total Time 14.00(14.00)\n",
      "Iter 12470 | Time 22.4033(22.7572) | Bit/dim 3.5459(3.5498) | Xent 0.0579(0.0633) | Loss 3.5749(3.5815) | Error 0.0167(0.0208) Steps 976(962.90) | Grad Norm 1.8231(2.3365) | Total Time 14.00(14.00)\n",
      "Iter 12480 | Time 22.4562(22.7327) | Bit/dim 3.5681(3.5458) | Xent 0.0474(0.0590) | Loss 3.5918(3.5753) | Error 0.0178(0.0192) Steps 976(962.43) | Grad Norm 1.7382(2.1469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 112.0205, Epoch Time 1382.2833(1334.3766), Bit/dim 3.5387(best: 3.5218), Xent 2.9343, Loss 5.0058, Error 0.3642(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12490 | Time 22.4033(22.6988) | Bit/dim 3.5286(3.5408) | Xent 0.0404(0.0534) | Loss 3.5488(3.5674) | Error 0.0133(0.0177) Steps 952(963.68) | Grad Norm 1.6442(1.9928) | Total Time 14.00(14.00)\n",
      "Iter 12500 | Time 23.0463(22.6809) | Bit/dim 3.5396(3.5368) | Xent 0.0304(0.0481) | Loss 3.5548(3.5608) | Error 0.0111(0.0158) Steps 982(965.17) | Grad Norm 1.5599(1.8653) | Total Time 14.00(14.00)\n",
      "Iter 12510 | Time 22.7342(22.6522) | Bit/dim 3.5089(3.5348) | Xent 0.0505(0.0451) | Loss 3.5342(3.5573) | Error 0.0178(0.0148) Steps 976(962.70) | Grad Norm 1.6926(1.7548) | Total Time 14.00(14.00)\n",
      "Iter 12520 | Time 23.3441(22.6978) | Bit/dim 3.5049(3.5322) | Xent 0.0468(0.0420) | Loss 3.5283(3.5532) | Error 0.0178(0.0141) Steps 988(964.73) | Grad Norm 1.6791(1.7026) | Total Time 14.00(14.00)\n",
      "Iter 12530 | Time 23.8238(22.6852) | Bit/dim 3.5680(3.5337) | Xent 0.0207(0.0390) | Loss 3.5784(3.5532) | Error 0.0078(0.0130) Steps 976(963.03) | Grad Norm 1.2366(1.6360) | Total Time 14.00(14.00)\n",
      "Iter 12540 | Time 22.6765(22.7191) | Bit/dim 3.5125(3.5317) | Xent 0.0252(0.0378) | Loss 3.5251(3.5507) | Error 0.0089(0.0123) Steps 946(961.07) | Grad Norm 1.8906(1.5973) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 111.0097, Epoch Time 1374.2443(1335.5726), Bit/dim 3.5300(best: 3.5218), Xent 2.7568, Loss 4.9085, Error 0.3620(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12550 | Time 23.2079(22.7057) | Bit/dim 3.5332(3.5309) | Xent 0.0254(0.0342) | Loss 3.5459(3.5480) | Error 0.0122(0.0114) Steps 952(959.81) | Grad Norm 1.4311(1.5616) | Total Time 14.00(14.00)\n",
      "Iter 12560 | Time 22.7497(22.7171) | Bit/dim 3.5309(3.5275) | Xent 0.0316(0.0317) | Loss 3.5467(3.5433) | Error 0.0111(0.0107) Steps 970(959.92) | Grad Norm 1.2014(1.4603) | Total Time 14.00(14.00)\n",
      "Iter 12570 | Time 22.8453(22.6704) | Bit/dim 3.5050(3.5264) | Xent 0.0392(0.0313) | Loss 3.5246(3.5420) | Error 0.0144(0.0107) Steps 964(956.53) | Grad Norm 1.6695(1.4439) | Total Time 14.00(14.00)\n",
      "Iter 12580 | Time 22.4601(22.6642) | Bit/dim 3.5176(3.5247) | Xent 0.0144(0.0307) | Loss 3.5248(3.5401) | Error 0.0044(0.0107) Steps 964(954.68) | Grad Norm 0.9971(1.4580) | Total Time 14.00(14.00)\n",
      "Iter 12590 | Time 22.4413(22.6148) | Bit/dim 3.5036(3.5244) | Xent 0.0244(0.0288) | Loss 3.5158(3.5388) | Error 0.0100(0.0098) Steps 958(955.50) | Grad Norm 1.6674(1.4525) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 110.6915, Epoch Time 1372.0351(1336.6665), Bit/dim 3.5248(best: 3.5218), Xent 2.8179, Loss 4.9338, Error 0.3605(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12600 | Time 23.0519(22.6667) | Bit/dim 3.5404(3.5236) | Xent 0.0217(0.0265) | Loss 3.5513(3.5369) | Error 0.0100(0.0091) Steps 982(958.58) | Grad Norm 1.2305(1.4004) | Total Time 14.00(14.00)\n",
      "Iter 12610 | Time 22.4468(22.6765) | Bit/dim 3.5422(3.5234) | Xent 0.0270(0.0263) | Loss 3.5557(3.5365) | Error 0.0078(0.0089) Steps 964(958.71) | Grad Norm 1.6095(1.3690) | Total Time 14.00(14.00)\n",
      "Iter 12620 | Time 22.9377(22.6648) | Bit/dim 3.5528(3.5205) | Xent 0.0177(0.0254) | Loss 3.5616(3.5332) | Error 0.0044(0.0085) Steps 952(955.37) | Grad Norm 0.9802(1.3824) | Total Time 14.00(14.00)\n",
      "Iter 12630 | Time 21.3916(22.5836) | Bit/dim 3.5035(3.5173) | Xent 0.0146(0.0237) | Loss 3.5108(3.5291) | Error 0.0056(0.0079) Steps 952(952.04) | Grad Norm 1.1129(1.3265) | Total Time 14.00(14.00)\n",
      "Iter 12640 | Time 22.0327(22.5654) | Bit/dim 3.5364(3.5166) | Xent 0.0346(0.0227) | Loss 3.5538(3.5279) | Error 0.0133(0.0078) Steps 976(954.65) | Grad Norm 1.2786(1.2585) | Total Time 14.00(14.00)\n",
      "Iter 12650 | Time 22.4344(22.6066) | Bit/dim 3.4963(3.5171) | Xent 0.0251(0.0227) | Loss 3.5088(3.5284) | Error 0.0089(0.0080) Steps 934(954.68) | Grad Norm 1.5049(1.2663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 108.9356, Epoch Time 1368.6029(1337.6246), Bit/dim 3.5193(best: 3.5218), Xent 2.7692, Loss 4.9039, Error 0.3567(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_run1_2 --resume ../experiments_published/cnf_conditional_cifar10_bs900_run1_2/current_checkpt.pth --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
