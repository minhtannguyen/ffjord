{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_bs900_drop_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 7.5671(17.7510) | Bit/dim 23.4589(25.6272) | Xent 2.2733(2.2992) | Loss 24.5956(26.7768) | Error 0.8789(0.8918) Steps 410(410.00) | Grad Norm 185.9120(204.6391) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 7.6087(15.1046) | Bit/dim 18.2993(24.2967) | Xent 2.1963(2.2811) | Loss 19.3975(25.4372) | Error 0.6400(0.8665) Steps 410(410.00) | Grad Norm 133.0143(192.2467) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 8.0470(13.1601) | Bit/dim 13.6085(21.9542) | Xent 2.0629(2.2398) | Loss 14.6399(23.0741) | Error 0.2978(0.7442) Steps 410(410.00) | Grad Norm 71.0609(167.5572) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 7.6500(11.7164) | Bit/dim 11.0788(19.3417) | Xent 1.9434(2.1755) | Loss 12.0505(20.4294) | Error 0.2811(0.6256) Steps 410(410.00) | Grad Norm 27.2136(134.8255) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 7.7106(10.6616) | Bit/dim 9.5240(16.9498) | Xent 1.8083(2.0939) | Loss 10.4281(17.9967) | Error 0.2622(0.5319) Steps 410(410.00) | Grad Norm 16.3388(104.6040) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 7.6224(9.8946) | Bit/dim 8.1703(14.7903) | Xent 1.7097(2.0035) | Loss 9.0252(15.7920) | Error 0.2722(0.4615) Steps 410(410.00) | Grad Norm 14.7714(80.9913) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 41.0262, Epoch Time 578.6728(578.6728), Bit/dim 7.3667(best: inf), Xent 1.6280, Loss 8.1807, Error 0.2335(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 7.6075(9.3362) | Bit/dim 6.8963(12.8692) | Xent 1.6118(1.9117) | Loss 7.7022(13.8250) | Error 0.2478(0.4105) Steps 410(410.00) | Grad Norm 15.4259(63.8082) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 7.5873(8.9043) | Bit/dim 5.6618(11.1147) | Xent 1.6076(1.8325) | Loss 6.4656(12.0309) | Error 0.2700(0.3706) Steps 410(410.00) | Grad Norm 12.0388(50.5983) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 7.7396(8.5949) | Bit/dim 4.5451(9.5136) | Xent 1.6371(1.7737) | Loss 5.3636(10.4005) | Error 0.2900(0.3413) Steps 410(410.00) | Grad Norm 9.8662(40.1203) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 7.7470(8.3565) | Bit/dim 3.6482(8.0746) | Xent 1.6776(1.7415) | Loss 4.4870(8.9453) | Error 0.2633(0.3222) Steps 410(410.00) | Grad Norm 7.8891(31.9107) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 7.6942(8.1811) | Bit/dim 3.0940(6.8216) | Xent 1.7797(1.7375) | Loss 3.9839(7.6903) | Error 0.3122(0.3109) Steps 410(410.00) | Grad Norm 5.5975(25.2722) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 9.1808(8.2739) | Bit/dim 2.7168(5.7788) | Xent 1.8444(1.7557) | Loss 3.6390(6.6566) | Error 0.3344(0.3087) Steps 446(415.74) | Grad Norm 3.6794(19.7883) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 9.1784(8.4844) | Bit/dim 2.5414(4.9461) | Xent 1.8921(1.7852) | Loss 3.4874(5.8387) | Error 0.3567(0.3153) Steps 446(423.69) | Grad Norm 2.4095(15.3523) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 38.9375, Epoch Time 587.4288(578.9355), Bit/dim 2.4734(best: 7.3667), Xent 1.8736, Loss 3.4102, Error 0.2695(best: 0.2335)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.1442(8.6725) | Bit/dim 2.4108(4.2914) | Xent 1.8888(1.8114) | Loss 3.3552(5.1971) | Error 0.3367(0.3202) Steps 446(429.55) | Grad Norm 1.8489(11.8579) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 9.3498(8.7697) | Bit/dim 2.3339(3.7857) | Xent 1.8868(1.8338) | Loss 3.2773(4.7026) | Error 0.3133(0.3265) Steps 446(433.87) | Grad Norm 1.6052(9.1902) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 9.0541(8.8842) | Bit/dim 2.3036(3.3989) | Xent 1.8688(1.8452) | Loss 3.2380(4.3216) | Error 0.3422(0.3296) Steps 446(437.05) | Grad Norm 1.3943(7.1674) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 8.9353(8.9215) | Bit/dim 2.2762(3.1033) | Xent 1.8255(1.8458) | Loss 3.1890(4.0262) | Error 0.3144(0.3290) Steps 446(439.40) | Grad Norm 1.3826(5.6447) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 8.7315(8.9400) | Bit/dim 2.2475(2.8790) | Xent 1.7707(1.8351) | Loss 3.1329(3.7966) | Error 0.3011(0.3245) Steps 434(439.95) | Grad Norm 1.2992(4.5046) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 8.7507(8.9099) | Bit/dim 2.2225(2.7098) | Xent 1.7437(1.8163) | Loss 3.0943(3.6180) | Error 0.3011(0.3211) Steps 434(438.85) | Grad Norm 1.2508(3.6552) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 39.2606, Epoch Time 649.1165(581.0409), Bit/dim 2.2092(best: 2.4734), Xent 1.6604, Loss 3.0394, Error 0.2277(best: 0.2335)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 8.5108(8.8545) | Bit/dim 2.1908(2.5802) | Xent 1.6746(1.7868) | Loss 3.0281(3.4736) | Error 0.2822(0.3132) Steps 434(437.75) | Grad Norm 1.2619(3.0263) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 8.5426(8.8085) | Bit/dim 2.2239(2.4833) | Xent 1.6066(1.7478) | Loss 3.0272(3.3572) | Error 0.2622(0.3045) Steps 434(436.77) | Grad Norm 1.2819(2.5621) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 8.6370(8.7796) | Bit/dim 2.1959(2.4095) | Xent 1.5366(1.7022) | Loss 2.9642(3.2606) | Error 0.2656(0.2977) Steps 434(436.39) | Grad Norm 1.1852(2.2104) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 8.3246(8.7221) | Bit/dim 2.2032(2.3540) | Xent 1.4336(1.6447) | Loss 2.9200(3.1764) | Error 0.2311(0.2864) Steps 428(434.77) | Grad Norm 1.2046(1.9459) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 8.1261(8.6278) | Bit/dim 2.1636(2.3108) | Xent 1.3739(1.5833) | Loss 2.8505(3.1024) | Error 0.2289(0.2770) Steps 428(432.99) | Grad Norm 1.1797(1.7450) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 8.1814(8.5289) | Bit/dim 2.1783(2.2771) | Xent 1.2862(1.5149) | Loss 2.8214(3.0346) | Error 0.2411(0.2655) Steps 428(431.68) | Grad Norm 1.1599(1.6023) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 8.4000(8.4844) | Bit/dim 2.1751(2.2510) | Xent 1.2442(1.4465) | Loss 2.7972(2.9742) | Error 0.2356(0.2555) Steps 428(430.72) | Grad Norm 1.1885(1.5024) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 38.6183, Epoch Time 613.9440(582.0280), Bit/dim 2.1633(best: 2.2092), Xent 1.1419, Loss 2.7343, Error 0.1771(best: 0.2277)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.3093(8.4410) | Bit/dim 2.1509(2.2278) | Xent 1.1503(1.3760) | Loss 2.7260(2.9158) | Error 0.2189(0.2458) Steps 428(430.00) | Grad Norm 1.2442(1.4318) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 8.2485(8.3726) | Bit/dim 2.1379(2.2076) | Xent 1.1068(1.3117) | Loss 2.6913(2.8635) | Error 0.2178(0.2383) Steps 422(428.04) | Grad Norm 1.0713(1.3576) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 8.6109(8.3603) | Bit/dim 2.1402(2.1878) | Xent 1.0289(1.2450) | Loss 2.6546(2.8104) | Error 0.1956(0.2301) Steps 428(427.46) | Grad Norm 1.3107(1.2939) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 8.2919(8.3817) | Bit/dim 2.1073(2.1685) | Xent 0.9640(1.1841) | Loss 2.5893(2.7605) | Error 0.1944(0.2225) Steps 428(427.60) | Grad Norm 0.9640(1.2386) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 8.2456(8.3575) | Bit/dim 2.1255(2.1539) | Xent 0.9319(1.1253) | Loss 2.5914(2.7165) | Error 0.1867(0.2161) Steps 428(427.71) | Grad Norm 1.0967(1.1878) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 8.2353(8.3494) | Bit/dim 2.0792(2.1375) | Xent 0.8762(1.0714) | Loss 2.5173(2.6733) | Error 0.1778(0.2100) Steps 428(427.78) | Grad Norm 1.0211(1.1663) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 8.3151(8.3334) | Bit/dim 2.0823(2.1226) | Xent 0.8443(1.0214) | Loss 2.5044(2.6333) | Error 0.1622(0.2039) Steps 428(427.84) | Grad Norm 1.0179(1.1612) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 38.7391, Epoch Time 603.1354(582.6612), Bit/dim 2.0678(best: 2.1633), Xent 0.7941, Loss 2.4649, Error 0.1449(best: 0.1771)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 8.1334(8.3287) | Bit/dim 2.0722(2.1063) | Xent 0.8188(0.9759) | Loss 2.4816(2.5943) | Error 0.1789(0.2000) Steps 428(427.88) | Grad Norm 1.2704(1.2037) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 8.5008(8.3344) | Bit/dim 2.0466(2.0910) | Xent 0.8241(0.9363) | Loss 2.4586(2.5592) | Error 0.1911(0.1975) Steps 428(427.91) | Grad Norm 1.4167(1.2669) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 8.4951(8.3327) | Bit/dim 2.0265(2.0766) | Xent 0.7651(0.8960) | Loss 2.4090(2.5245) | Error 0.1700(0.1917) Steps 428(427.63) | Grad Norm 1.0343(1.2052) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 8.2866(8.3116) | Bit/dim 1.9997(2.0617) | Xent 0.7329(0.8600) | Loss 2.3661(2.4918) | Error 0.1678(0.1882) Steps 422(426.45) | Grad Norm 1.8421(1.2425) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 8.0815(8.3041) | Bit/dim 2.0113(2.0487) | Xent 0.7621(0.8310) | Loss 2.3923(2.4642) | Error 0.1844(0.1859) Steps 422(426.05) | Grad Norm 0.8784(1.2125) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 8.5881(8.3541) | Bit/dim 1.9998(2.0340) | Xent 0.7180(0.8063) | Loss 2.3588(2.4372) | Error 0.1922(0.1847) Steps 428(426.42) | Grad Norm 1.1217(1.1829) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 39.1580, Epoch Time 607.1898(583.3971), Bit/dim 1.9787(best: 2.0678), Xent 0.6160, Loss 2.2867, Error 0.1271(best: 0.1449)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 8.6660(8.4565) | Bit/dim 1.9818(2.0205) | Xent 0.6930(0.7811) | Loss 2.3283(2.4111) | Error 0.1689(0.1824) Steps 428(427.99) | Grad Norm 0.8508(1.1847) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 8.9400(8.5411) | Bit/dim 1.9510(2.0074) | Xent 0.6920(0.7573) | Loss 2.2970(2.3861) | Error 0.1789(0.1800) Steps 446(430.91) | Grad Norm 0.9030(1.2174) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 8.9240(8.6442) | Bit/dim 1.9824(1.9949) | Xent 0.6352(0.7337) | Loss 2.3000(2.3617) | Error 0.1689(0.1783) Steps 446(434.88) | Grad Norm 1.4210(1.3653) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 8.9037(8.7250) | Bit/dim 1.9427(1.9828) | Xent 0.6572(0.7127) | Loss 2.2713(2.3391) | Error 0.1722(0.1763) Steps 446(437.80) | Grad Norm 1.3565(1.4462) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 8.9599(8.7742) | Bit/dim 1.9235(1.9704) | Xent 0.6436(0.6964) | Loss 2.2453(2.3187) | Error 0.1578(0.1742) Steps 446(439.95) | Grad Norm 2.5964(1.4592) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 9.2355(8.8265) | Bit/dim 1.9149(1.9595) | Xent 0.6692(0.6843) | Loss 2.2495(2.3016) | Error 0.1844(0.1745) Steps 446(441.54) | Grad Norm 1.6794(1.8107) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 8.9781(8.8664) | Bit/dim 1.9119(1.9473) | Xent 0.6278(0.6692) | Loss 2.2258(2.2819) | Error 0.1611(0.1726) Steps 446(442.71) | Grad Norm 3.5102(2.0175) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 41.2006, Epoch Time 644.8030(585.2392), Bit/dim 1.9072(best: 1.9787), Xent 0.5155, Loss 2.1649, Error 0.1186(best: 0.1271)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 8.7933(8.8631) | Bit/dim 1.9121(1.9402) | Xent 0.6215(0.6563) | Loss 2.2228(2.2684) | Error 0.1656(0.1716) Steps 446(443.57) | Grad Norm 2.6895(2.0470) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 8.7722(8.8888) | Bit/dim 1.9025(1.9286) | Xent 0.6193(0.6459) | Loss 2.2121(2.2516) | Error 0.1778(0.1710) Steps 446(444.21) | Grad Norm 2.1776(2.0318) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 9.3400(8.9092) | Bit/dim 1.8805(1.9167) | Xent 0.6375(0.6392) | Loss 2.1993(2.2363) | Error 0.1867(0.1729) Steps 446(444.68) | Grad Norm 2.6772(2.1546) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 9.0359(8.9434) | Bit/dim 1.8892(1.9063) | Xent 0.6032(0.6284) | Loss 2.1909(2.2205) | Error 0.1678(0.1714) Steps 446(445.03) | Grad Norm 2.8307(2.1228) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 8.9474(8.9422) | Bit/dim 1.8610(1.8949) | Xent 0.5756(0.6193) | Loss 2.1488(2.2045) | Error 0.1589(0.1700) Steps 446(445.28) | Grad Norm 1.5769(2.0847) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 8.9913(8.9617) | Bit/dim 1.8595(1.8843) | Xent 0.5532(0.6070) | Loss 2.1361(2.1878) | Error 0.1678(0.1692) Steps 446(445.47) | Grad Norm 2.9135(2.2301) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 40.3668, Epoch Time 647.6961(587.1130), Bit/dim 1.8317(best: 1.9072), Xent 0.4783, Loss 2.0708, Error 0.1154(best: 0.1186)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 8.8056(8.9672) | Bit/dim 1.8350(1.8737) | Xent 0.5915(0.5993) | Loss 2.1308(2.1734) | Error 0.1589(0.1683) Steps 446(445.61) | Grad Norm 4.4201(2.5405) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 8.8625(8.9539) | Bit/dim 1.8301(1.8652) | Xent 0.6001(0.5963) | Loss 2.1302(2.1634) | Error 0.1767(0.1691) Steps 446(445.71) | Grad Norm 2.8517(2.8947) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 8.8980(8.9435) | Bit/dim 1.8332(1.8562) | Xent 0.5534(0.5877) | Loss 2.1099(2.1500) | Error 0.1500(0.1681) Steps 446(445.79) | Grad Norm 3.1987(3.1366) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 8.7650(8.9508) | Bit/dim 1.8282(1.8474) | Xent 0.5968(0.5813) | Loss 2.1266(2.1381) | Error 0.1811(0.1656) Steps 446(445.84) | Grad Norm 5.7431(3.5685) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 9.0898(8.9765) | Bit/dim 1.8220(1.8407) | Xent 0.5670(0.5775) | Loss 2.1055(2.1295) | Error 0.1678(0.1659) Steps 446(445.88) | Grad Norm 6.6079(4.1920) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 9.0628(8.9727) | Bit/dim 1.7888(1.8318) | Xent 0.5111(0.5695) | Loss 2.0444(2.1166) | Error 0.1422(0.1646) Steps 446(445.91) | Grad Norm 5.6888(4.3346) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 8.8158(8.9964) | Bit/dim 1.8049(1.8222) | Xent 0.5280(0.5650) | Loss 2.0689(2.1048) | Error 0.1644(0.1646) Steps 446(445.94) | Grad Norm 3.0314(4.0579) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 40.7270, Epoch Time 648.5159(588.9550), Bit/dim 1.7823(best: 1.8317), Xent 0.4158, Loss 1.9902, Error 0.1104(best: 0.1154)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.0667(8.9628) | Bit/dim 1.7978(1.8131) | Xent 0.5865(0.5619) | Loss 2.0910(2.0941) | Error 0.1744(0.1646) Steps 446(445.95) | Grad Norm 4.5200(3.8164) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 8.8640(8.9733) | Bit/dim 1.7581(1.8039) | Xent 0.5942(0.5596) | Loss 2.0552(2.0837) | Error 0.1867(0.1657) Steps 446(445.97) | Grad Norm 3.1907(3.6781) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 9.0466(8.9770) | Bit/dim 1.7811(1.7959) | Xent 0.6052(0.5550) | Loss 2.0837(2.0734) | Error 0.1844(0.1656) Steps 446(445.97) | Grad Norm 1.4072(3.4379) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 9.2901(8.9711) | Bit/dim 1.7494(1.7854) | Xent 0.5776(0.5514) | Loss 2.0382(2.0611) | Error 0.1633(0.1652) Steps 446(445.98) | Grad Norm 1.8415(3.1195) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 9.0787(8.9591) | Bit/dim 1.7590(1.7737) | Xent 0.5605(0.5462) | Loss 2.0392(2.0468) | Error 0.1756(0.1641) Steps 446(445.99) | Grad Norm 4.7347(3.2556) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 8.9430(8.9429) | Bit/dim 1.7355(1.7652) | Xent 0.5070(0.5373) | Loss 1.9890(2.0339) | Error 0.1511(0.1620) Steps 446(445.99) | Grad Norm 2.8312(4.0725) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 9.1192(8.9580) | Bit/dim 1.7090(1.7537) | Xent 0.5263(0.5353) | Loss 1.9722(2.0214) | Error 0.1589(0.1629) Steps 446(445.99) | Grad Norm 3.4955(3.8036) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 40.7287, Epoch Time 647.2857(590.7050), Bit/dim 1.7231(best: 1.7823), Xent 0.3718, Loss 1.9090, Error 0.1058(best: 0.1104)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.1706(8.9673) | Bit/dim 1.7646(1.7457) | Xent 0.4729(0.5265) | Loss 2.0010(2.0090) | Error 0.1422(0.1598) Steps 440(445.81) | Grad Norm 19.2895(4.6965) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 8.8703(8.9458) | Bit/dim 1.6903(1.7372) | Xent 0.5040(0.5252) | Loss 1.9424(1.9998) | Error 0.1644(0.1600) Steps 446(445.86) | Grad Norm 6.5761(5.7822) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 9.3083(8.9319) | Bit/dim 1.6697(1.7251) | Xent 0.4584(0.5190) | Loss 1.8989(1.9846) | Error 0.1356(0.1586) Steps 452(445.91) | Grad Norm 5.6843(5.4980) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 8.8162(8.9548) | Bit/dim 1.6478(1.7093) | Xent 0.4880(0.5189) | Loss 1.8918(1.9688) | Error 0.1467(0.1585) Steps 446(445.93) | Grad Norm 4.4486(5.2131) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 9.1867(8.9995) | Bit/dim 1.6422(1.6926) | Xent 0.4958(0.5161) | Loss 1.8901(1.9507) | Error 0.1600(0.1580) Steps 452(446.94) | Grad Norm 10.0333(5.9308) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 9.1709(9.0568) | Bit/dim 1.5964(1.6741) | Xent 0.4871(0.5119) | Loss 1.8400(1.9301) | Error 0.1467(0.1573) Steps 452(448.27) | Grad Norm 4.6315(6.5044) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 41.6731, Epoch Time 654.2334(592.6108), Bit/dim 1.5765(best: 1.7231), Xent 0.3836, Loss 1.7684, Error 0.1038(best: 0.1058)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.2816(9.1266) | Bit/dim 1.5958(1.6524) | Xent 0.5269(0.5132) | Loss 1.8592(1.9090) | Error 0.1678(0.1586) Steps 452(449.28) | Grad Norm 3.6719(6.7652) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 9.2148(9.1869) | Bit/dim 1.5466(1.6301) | Xent 0.5125(0.5043) | Loss 1.8028(1.8822) | Error 0.1656(0.1563) Steps 458(450.96) | Grad Norm 2.5252(6.8540) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 9.3583(9.2601) | Bit/dim 1.5918(1.6111) | Xent 0.4366(0.4988) | Loss 1.8101(1.8605) | Error 0.1400(0.1550) Steps 458(453.13) | Grad Norm 18.1867(7.8293) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 9.4399(9.3156) | Bit/dim 1.5365(1.5918) | Xent 0.5012(0.4963) | Loss 1.7871(1.8399) | Error 0.1389(0.1544) Steps 458(455.19) | Grad Norm 8.4692(8.4036) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 9.4081(9.3455) | Bit/dim 1.4877(1.5712) | Xent 0.5262(0.4946) | Loss 1.7508(1.8185) | Error 0.1600(0.1535) Steps 464(456.89) | Grad Norm 8.1114(7.7724) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 9.5050(9.3533) | Bit/dim 1.5049(1.5559) | Xent 0.4237(0.4919) | Loss 1.7168(1.8019) | Error 0.1278(0.1523) Steps 464(457.99) | Grad Norm 3.4192(8.1538) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 9.3554(9.3954) | Bit/dim 1.4895(1.5429) | Xent 0.4910(0.4891) | Loss 1.7350(1.7874) | Error 0.1567(0.1519) Steps 458(458.78) | Grad Norm 5.6475(8.4281) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 42.1388, Epoch Time 680.5038(595.2476), Bit/dim 1.4788(best: 1.5765), Xent 0.3452, Loss 1.6514, Error 0.0994(best: 0.1038)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.4156(9.4146) | Bit/dim 1.4967(1.5297) | Xent 0.4526(0.4846) | Loss 1.7230(1.7720) | Error 0.1522(0.1512) Steps 458(459.51) | Grad Norm 6.6609(7.9727) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 9.3276(9.4486) | Bit/dim 1.5367(1.5371) | Xent 0.3829(0.4888) | Loss 1.7282(1.7815) | Error 0.1200(0.1530) Steps 464(461.21) | Grad Norm 9.0344(11.1270) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 9.2082(9.4390) | Bit/dim 1.5531(1.5374) | Xent 0.3804(0.4742) | Loss 1.7433(1.7745) | Error 0.1089(0.1486) Steps 458(462.39) | Grad Norm 8.2435(10.7601) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 9.3892(9.4436) | Bit/dim 1.5065(1.5281) | Xent 0.4744(0.4728) | Loss 1.7437(1.7645) | Error 0.1500(0.1476) Steps 464(463.28) | Grad Norm 6.8988(9.6543) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 9.6057(9.4507) | Bit/dim 1.4713(1.5143) | Xent 0.5010(0.4742) | Loss 1.7218(1.7514) | Error 0.1522(0.1485) Steps 458(462.67) | Grad Norm 4.6197(8.2061) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 9.4770(9.4403) | Bit/dim 1.4507(1.5030) | Xent 0.4285(0.4700) | Loss 1.6650(1.7380) | Error 0.1200(0.1471) Steps 464(462.10) | Grad Norm 2.7597(6.8197) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 41.8570, Epoch Time 681.6372(597.8393), Bit/dim 1.4476(best: 1.4788), Xent 0.3271, Loss 1.6111, Error 0.0964(best: 0.0994)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.5594(9.4469) | Bit/dim 1.4605(1.4922) | Xent 0.4217(0.4638) | Loss 1.6713(1.7241) | Error 0.1322(0.1456) Steps 464(461.82) | Grad Norm 2.7704(5.9972) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 9.4597(9.4710) | Bit/dim 1.4658(1.4833) | Xent 0.4194(0.4621) | Loss 1.6754(1.7144) | Error 0.1356(0.1464) Steps 458(461.89) | Grad Norm 8.5978(5.7791) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 9.5289(9.4723) | Bit/dim 1.4866(1.4758) | Xent 0.3662(0.4550) | Loss 1.6697(1.7033) | Error 0.1211(0.1442) Steps 458(461.20) | Grad Norm 8.5625(5.7867) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 9.8189(9.4681) | Bit/dim 1.5620(1.4979) | Xent 0.5151(0.4556) | Loss 1.8196(1.7257) | Error 0.1533(0.1435) Steps 470(461.77) | Grad Norm 23.4604(10.2308) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 9.5612(9.4502) | Bit/dim 1.5083(1.5038) | Xent 0.4562(0.4548) | Loss 1.7364(1.7312) | Error 0.1356(0.1433) Steps 470(462.25) | Grad Norm 12.6413(10.1550) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 9.3303(9.4406) | Bit/dim 1.4485(1.4955) | Xent 0.4288(0.4557) | Loss 1.6629(1.7234) | Error 0.1378(0.1436) Steps 458(461.86) | Grad Norm 5.7253(9.2747) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 9.4836(9.4571) | Bit/dim 1.4442(1.4854) | Xent 0.4204(0.4499) | Loss 1.6544(1.7104) | Error 0.1322(0.1419) Steps 458(461.47) | Grad Norm 5.6387(8.1113) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 41.2963, Epoch Time 682.3543(600.3747), Bit/dim 1.4464(best: 1.4476), Xent 0.2924, Loss 1.5926, Error 0.0859(best: 0.0964)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.4706(9.4655) | Bit/dim 1.4248(1.4757) | Xent 0.4237(0.4443) | Loss 1.6366(1.6978) | Error 0.1356(0.1404) Steps 458(461.20) | Grad Norm 3.0977(7.2204) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 9.3471(9.4499) | Bit/dim 1.4286(1.4666) | Xent 0.4254(0.4408) | Loss 1.6413(1.6870) | Error 0.1278(0.1393) Steps 458(460.36) | Grad Norm 7.4798(6.8697) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 9.3077(9.4551) | Bit/dim 1.4605(1.4597) | Xent 0.3834(0.4298) | Loss 1.6522(1.6746) | Error 0.1033(0.1356) Steps 458(459.74) | Grad Norm 10.1027(7.0580) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 9.2364(9.4121) | Bit/dim 1.4220(1.4529) | Xent 0.4442(0.4322) | Loss 1.6441(1.6690) | Error 0.1511(0.1369) Steps 458(459.74) | Grad Norm 8.7357(7.1843) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 9.6420(9.4084) | Bit/dim 1.4376(1.4484) | Xent 0.4701(0.4281) | Loss 1.6727(1.6625) | Error 0.1422(0.1350) Steps 464(459.46) | Grad Norm 16.3534(7.5666) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 9.7554(9.4294) | Bit/dim 1.4163(1.4422) | Xent 0.4188(0.4208) | Loss 1.6257(1.6526) | Error 0.1344(0.1329) Steps 464(460.07) | Grad Norm 3.8765(7.3119) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 9.2950(9.4583) | Bit/dim 1.4625(1.4405) | Xent 0.3767(0.4136) | Loss 1.6509(1.6473) | Error 0.1222(0.1308) Steps 464(461.59) | Grad Norm 16.2680(8.2126) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 42.1460, Epoch Time 680.2782(602.7718), Bit/dim 1.4261(best: 1.4464), Xent 0.2649, Loss 1.5586, Error 0.0780(best: 0.0859)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.3747(9.4410) | Bit/dim 1.4080(1.4373) | Xent 0.4235(0.4083) | Loss 1.6197(1.6414) | Error 0.1189(0.1297) Steps 464(462.22) | Grad Norm 5.5548(8.2586) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 9.4548(9.4761) | Bit/dim 1.4136(1.4366) | Xent 0.3808(0.4007) | Loss 1.6040(1.6370) | Error 0.1022(0.1263) Steps 470(463.21) | Grad Norm 7.7746(9.2223) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 9.5012(9.4625) | Bit/dim 1.4435(1.4380) | Xent 0.4332(0.4015) | Loss 1.6602(1.6387) | Error 0.1367(0.1261) Steps 464(463.75) | Grad Norm 8.6406(9.8323) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 9.3370(9.4770) | Bit/dim 1.4190(1.4343) | Xent 0.3571(0.3979) | Loss 1.5976(1.6333) | Error 0.1144(0.1252) Steps 464(463.97) | Grad Norm 6.3646(9.7232) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 10.0809(9.4938) | Bit/dim 1.4170(1.4293) | Xent 0.4416(0.3914) | Loss 1.6378(1.6250) | Error 0.1467(0.1241) Steps 476(464.83) | Grad Norm 14.8976(9.5477) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 9.4079(9.5428) | Bit/dim 1.4563(1.4344) | Xent 0.3064(0.3880) | Loss 1.6095(1.6284) | Error 0.1044(0.1236) Steps 470(466.83) | Grad Norm 16.0260(11.1500) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 42.3539, Epoch Time 687.3467(605.3091), Bit/dim 1.4928(best: 1.4261), Xent 0.2262, Loss 1.6059, Error 0.0707(best: 0.0780)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.6184(9.5682) | Bit/dim 1.4569(1.4519) | Xent 0.3848(0.3825) | Loss 1.6493(1.6431) | Error 0.1211(0.1219) Steps 470(468.37) | Grad Norm 12.7124(12.3060) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 9.5199(9.5937) | Bit/dim 1.4132(1.4462) | Xent 0.3702(0.3772) | Loss 1.5983(1.6348) | Error 0.1189(0.1203) Steps 470(468.67) | Grad Norm 9.5990(11.1075) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 9.4414(9.5944) | Bit/dim 1.4254(1.4385) | Xent 0.3342(0.3628) | Loss 1.5925(1.6199) | Error 0.1111(0.1164) Steps 470(469.16) | Grad Norm 7.5443(9.8981) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 9.5807(9.5748) | Bit/dim 1.4149(1.4284) | Xent 0.3164(0.3576) | Loss 1.5732(1.6072) | Error 0.1033(0.1148) Steps 470(469.38) | Grad Norm 4.6109(8.4069) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 9.5479(9.5601) | Bit/dim 1.3868(1.4193) | Xent 0.3282(0.3500) | Loss 1.5509(1.5943) | Error 0.0900(0.1115) Steps 470(469.54) | Grad Norm 1.6060(7.3598) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 9.4709(9.5601) | Bit/dim 1.3993(1.4137) | Xent 0.3589(0.3464) | Loss 1.5787(1.5869) | Error 0.1056(0.1104) Steps 470(469.66) | Grad Norm 1.6472(6.9955) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 9.5807(9.5682) | Bit/dim 1.3891(1.4093) | Xent 0.3286(0.3405) | Loss 1.5533(1.5795) | Error 0.0944(0.1086) Steps 470(470.06) | Grad Norm 9.4424(7.6408) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 41.7473, Epoch Time 689.7878(607.8435), Bit/dim 1.3832(best: 1.4261), Xent 0.2045, Loss 1.4855, Error 0.0607(best: 0.0707)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.4619(9.5963) | Bit/dim 1.4480(1.4287) | Xent 0.3114(0.3346) | Loss 1.6037(1.5960) | Error 0.0978(0.1075) Steps 470(470.94) | Grad Norm 10.3282(10.8932) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 9.6109(9.6140) | Bit/dim 1.4348(1.4350) | Xent 0.3028(0.3320) | Loss 1.5862(1.6009) | Error 0.0978(0.1065) Steps 470(471.26) | Grad Norm 10.3430(11.3819) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 9.8101(9.6210) | Bit/dim 1.4014(1.4278) | Xent 0.2703(0.3219) | Loss 1.5366(1.5887) | Error 0.0878(0.1029) Steps 470(470.93) | Grad Norm 5.5266(10.0988) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 9.6116(9.6272) | Bit/dim 1.3840(1.4175) | Xent 0.2863(0.3164) | Loss 1.5272(1.5757) | Error 0.0844(0.1007) Steps 470(470.69) | Grad Norm 2.9367(8.3602) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 9.5359(9.6127) | Bit/dim 1.3846(1.4088) | Xent 0.3032(0.3103) | Loss 1.5362(1.5640) | Error 0.1000(0.0991) Steps 470(470.51) | Grad Norm 1.9383(7.0927) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 9.3963(9.5946) | Bit/dim 1.3807(1.4024) | Xent 0.2854(0.3029) | Loss 1.5234(1.5539) | Error 0.0944(0.0972) Steps 470(470.53) | Grad Norm 4.5799(6.5832) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 42.2166, Epoch Time 692.9762(610.3974), Bit/dim 1.4530(best: 1.3832), Xent 0.1591, Loss 1.5326, Error 0.0519(best: 0.0607)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.7238(9.6086) | Bit/dim 1.3989(1.4002) | Xent 0.2548(0.2983) | Loss 1.5263(1.5494) | Error 0.0822(0.0959) Steps 470(471.01) | Grad Norm 4.8229(7.5234) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 10.0642(9.6523) | Bit/dim 1.4487(1.4124) | Xent 0.2122(0.2942) | Loss 1.5548(1.5595) | Error 0.0744(0.0945) Steps 470(471.64) | Grad Norm 12.4189(10.4006) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 9.8513(9.6561) | Bit/dim 1.3831(1.4109) | Xent 0.2885(0.2907) | Loss 1.5274(1.5563) | Error 0.0989(0.0939) Steps 470(471.51) | Grad Norm 4.2173(10.7522) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 9.5647(9.6562) | Bit/dim 1.3875(1.4039) | Xent 0.2362(0.2878) | Loss 1.5056(1.5478) | Error 0.0744(0.0935) Steps 470(471.29) | Grad Norm 8.3760(10.2860) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 9.5207(9.6418) | Bit/dim 1.3874(1.3979) | Xent 0.2296(0.2825) | Loss 1.5022(1.5392) | Error 0.0822(0.0917) Steps 470(471.09) | Grad Norm 4.2971(9.1988) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 9.3806(9.6221) | Bit/dim 1.3622(1.3903) | Xent 0.2671(0.2765) | Loss 1.4958(1.5286) | Error 0.0789(0.0894) Steps 470(470.94) | Grad Norm 2.9245(8.3141) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 9.7822(9.6474) | Bit/dim 1.3802(1.3852) | Xent 0.2464(0.2657) | Loss 1.5034(1.5180) | Error 0.0722(0.0855) Steps 476(471.86) | Grad Norm 5.3727(8.0789) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 42.6301, Epoch Time 696.3357(612.9756), Bit/dim 1.3575(best: 1.3832), Xent 0.1464, Loss 1.4307, Error 0.0449(best: 0.0519)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.8752(9.6808) | Bit/dim 1.3634(1.3797) | Xent 0.2489(0.2599) | Loss 1.4878(1.5096) | Error 0.0767(0.0829) Steps 476(472.14) | Grad Norm 12.0519(8.1858) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 9.6617(9.7043) | Bit/dim 1.4097(1.3786) | Xent 0.2093(0.2560) | Loss 1.5143(1.5066) | Error 0.0700(0.0822) Steps 470(472.87) | Grad Norm 16.3785(9.3252) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 9.6799(9.6810) | Bit/dim 1.3502(1.3735) | Xent 0.2570(0.2484) | Loss 1.4787(1.4977) | Error 0.0833(0.0797) Steps 470(472.56) | Grad Norm 5.6339(8.9127) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 9.0554(9.6861) | Bit/dim 1.5081(1.3887) | Xent 0.1909(0.2427) | Loss 1.6035(1.5100) | Error 0.0589(0.0777) Steps 464(472.67) | Grad Norm 20.1638(11.7641) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 9.4763(9.6610) | Bit/dim 1.4229(1.4038) | Xent 0.2539(0.2431) | Loss 1.5498(1.5253) | Error 0.0844(0.0773) Steps 470(471.92) | Grad Norm 10.2572(12.6765) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 9.9270(9.6866) | Bit/dim 1.3507(1.4014) | Xent 0.2325(0.2377) | Loss 1.4670(1.5203) | Error 0.0756(0.0752) Steps 470(471.71) | Grad Norm 10.3498(12.0859) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 9.9463(9.6759) | Bit/dim 1.3590(1.3916) | Xent 0.2272(0.2373) | Loss 1.4726(1.5103) | Error 0.0722(0.0757) Steps 470(471.26) | Grad Norm 6.8646(10.5310) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 42.7164, Epoch Time 697.5670(615.5133), Bit/dim 1.3642(best: 1.3575), Xent 0.1175, Loss 1.4229, Error 0.0390(best: 0.0449)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.4834(9.6561) | Bit/dim 1.3424(1.3822) | Xent 0.2607(0.2334) | Loss 1.4728(1.4989) | Error 0.0878(0.0751) Steps 470(470.93) | Grad Norm 4.2485(8.8414) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 9.7706(9.6400) | Bit/dim 1.3163(1.3744) | Xent 0.2257(0.2303) | Loss 1.4291(1.4896) | Error 0.0678(0.0740) Steps 470(470.99) | Grad Norm 4.6227(7.9552) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 9.6284(9.6514) | Bit/dim 1.3443(1.3679) | Xent 0.1707(0.2209) | Loss 1.4297(1.4784) | Error 0.0522(0.0713) Steps 470(471.40) | Grad Norm 4.4105(7.7708) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 9.6796(9.6833) | Bit/dim 1.5074(1.3750) | Xent 0.1913(0.2210) | Loss 1.6030(1.4855) | Error 0.0667(0.0704) Steps 470(472.94) | Grad Norm 25.0492(10.4446) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 9.6831(9.6612) | Bit/dim 1.4193(1.3911) | Xent 0.2205(0.2215) | Loss 1.5296(1.5019) | Error 0.0644(0.0702) Steps 470(472.89) | Grad Norm 11.2598(11.4306) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 9.8790(9.6781) | Bit/dim 1.3517(1.3864) | Xent 0.1972(0.2173) | Loss 1.4503(1.4950) | Error 0.0711(0.0692) Steps 476(474.47) | Grad Norm 7.8950(10.9617) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 41.9783, Epoch Time 695.8647(617.9239), Bit/dim 1.3362(best: 1.3575), Xent 0.1109, Loss 1.3916, Error 0.0345(best: 0.0390)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 10.0809(9.7038) | Bit/dim 1.3364(1.3768) | Xent 0.2040(0.2140) | Loss 1.4383(1.4838) | Error 0.0678(0.0680) Steps 470(474.48) | Grad Norm 3.6166(10.0028) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 9.6504(9.7260) | Bit/dim 1.3548(1.3697) | Xent 0.2079(0.2102) | Loss 1.4587(1.4748) | Error 0.0633(0.0664) Steps 476(475.36) | Grad Norm 12.1337(9.4877) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 9.8007(9.7651) | Bit/dim 1.3899(1.3697) | Xent 0.1523(0.2065) | Loss 1.4660(1.4729) | Error 0.0444(0.0651) Steps 476(476.91) | Grad Norm 11.9535(10.8837) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 9.5638(9.7557) | Bit/dim 1.3461(1.3640) | Xent 0.1883(0.2013) | Loss 1.4402(1.4646) | Error 0.0611(0.0635) Steps 476(477.31) | Grad Norm 8.7025(10.9724) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 9.6346(9.7559) | Bit/dim 1.3207(1.3553) | Xent 0.1675(0.1962) | Loss 1.4045(1.4534) | Error 0.0478(0.0620) Steps 476(477.42) | Grad Norm 2.5694(9.3102) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 9.6045(9.7748) | Bit/dim 1.3701(1.3508) | Xent 0.1521(0.1927) | Loss 1.4462(1.4471) | Error 0.0511(0.0611) Steps 476(477.52) | Grad Norm 16.8361(8.9332) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 9.6531(9.7936) | Bit/dim 1.3707(1.3622) | Xent 0.1579(0.1898) | Loss 1.4497(1.4572) | Error 0.0378(0.0599) Steps 476(478.67) | Grad Norm 11.0305(11.3238) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 44.4588, Epoch Time 707.0514(620.5977), Bit/dim 1.4238(best: 1.3362), Xent 0.1160, Loss 1.4818, Error 0.0364(best: 0.0345)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 9.5060(9.7947) | Bit/dim 1.4066(1.3695) | Xent 0.1221(0.1859) | Loss 1.4677(1.4624) | Error 0.0456(0.0590) Steps 476(478.15) | Grad Norm 11.7351(11.5682) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 9.6067(9.7710) | Bit/dim 1.3363(1.3636) | Xent 0.1694(0.1870) | Loss 1.4210(1.4571) | Error 0.0589(0.0584) Steps 476(477.73) | Grad Norm 3.0139(10.2457) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 9.8874(9.7661) | Bit/dim 1.3314(1.3569) | Xent 0.2267(0.1902) | Loss 1.4447(1.4520) | Error 0.0700(0.0601) Steps 488(478.09) | Grad Norm 6.3100(8.9485) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 9.8243(9.7813) | Bit/dim 1.3099(1.3501) | Xent 0.2058(0.1873) | Loss 1.4129(1.4437) | Error 0.0678(0.0594) Steps 482(478.78) | Grad Norm 10.4425(8.2378) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 9.8785(9.7794) | Bit/dim 1.3132(1.3420) | Xent 0.1880(0.1829) | Loss 1.4072(1.4334) | Error 0.0556(0.0581) Steps 482(478.70) | Grad Norm 7.4481(7.6728) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 10.0498(9.7852) | Bit/dim 1.3060(1.3364) | Xent 0.1910(0.1765) | Loss 1.4015(1.4246) | Error 0.0544(0.0563) Steps 482(478.80) | Grad Norm 5.6233(7.2870) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 43.7163, Epoch Time 705.3803(623.1412), Bit/dim 1.3094(best: 1.3362), Xent 0.0906, Loss 1.3547, Error 0.0276(best: 0.0345)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.9066(9.8022) | Bit/dim 1.3057(1.3326) | Xent 0.1748(0.1775) | Loss 1.3931(1.4214) | Error 0.0567(0.0566) Steps 482(479.14) | Grad Norm 11.7574(8.1719) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 10.0486(9.8287) | Bit/dim 1.3124(1.3343) | Xent 0.1510(0.1728) | Loss 1.3879(1.4207) | Error 0.0489(0.0551) Steps 476(479.74) | Grad Norm 4.1665(9.5329) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 9.6921(9.8428) | Bit/dim 1.4009(1.3594) | Xent 0.1408(0.1735) | Loss 1.4713(1.4461) | Error 0.0444(0.0560) Steps 476(480.28) | Grad Norm 16.8393(11.9208) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 10.0813(9.8173) | Bit/dim 1.3237(1.3563) | Xent 0.1424(0.1717) | Loss 1.3950(1.4422) | Error 0.0444(0.0553) Steps 476(479.93) | Grad Norm 1.4850(11.0220) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 9.7001(9.8260) | Bit/dim 1.3449(1.3475) | Xent 0.1582(0.1706) | Loss 1.4240(1.4328) | Error 0.0467(0.0545) Steps 482(480.12) | Grad Norm 3.3840(9.8625) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 9.6941(9.8270) | Bit/dim 1.3200(1.3384) | Xent 0.1755(0.1674) | Loss 1.4077(1.4221) | Error 0.0556(0.0534) Steps 482(480.94) | Grad Norm 3.5340(8.4769) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 10.3701(9.8491) | Bit/dim 1.3129(1.3315) | Xent 0.1797(0.1635) | Loss 1.4027(1.4132) | Error 0.0578(0.0525) Steps 488(481.40) | Grad Norm 4.3573(7.6855) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 43.7394, Epoch Time 709.6855(625.7375), Bit/dim 1.2970(best: 1.3094), Xent 0.0840, Loss 1.3390, Error 0.0268(best: 0.0276)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 9.9965(9.8945) | Bit/dim 1.2840(1.3229) | Xent 0.1569(0.1614) | Loss 1.3624(1.4036) | Error 0.0489(0.0515) Steps 488(483.13) | Grad Norm 3.0595(6.3746) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 10.2208(9.9145) | Bit/dim 1.2881(1.3188) | Xent 0.1625(0.1610) | Loss 1.3694(1.3993) | Error 0.0511(0.0509) Steps 500(484.31) | Grad Norm 10.0961(6.5445) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 9.6957(9.9464) | Bit/dim 1.3331(1.3160) | Xent 0.1212(0.1558) | Loss 1.3937(1.3939) | Error 0.0456(0.0494) Steps 482(485.41) | Grad Norm 14.4554(7.7468) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 9.9146(9.9504) | Bit/dim 1.3193(1.3140) | Xent 0.1452(0.1558) | Loss 1.3919(1.3919) | Error 0.0444(0.0488) Steps 488(486.26) | Grad Norm 5.8820(8.3056) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 10.3800(9.9878) | Bit/dim 1.4184(1.3442) | Xent 0.2092(0.1599) | Loss 1.5230(1.4241) | Error 0.0656(0.0501) Steps 506(487.50) | Grad Norm 24.3633(11.6221) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 10.1577(9.9780) | Bit/dim 1.3422(1.3496) | Xent 0.1209(0.1593) | Loss 1.4026(1.4293) | Error 0.0378(0.0501) Steps 482(487.14) | Grad Norm 9.6095(11.7747) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 9.7165(9.9559) | Bit/dim 1.3122(1.3427) | Xent 0.1832(0.1587) | Loss 1.4038(1.4221) | Error 0.0567(0.0499) Steps 482(486.42) | Grad Norm 7.0974(10.9940) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 45.5777, Epoch Time 720.8540(628.5910), Bit/dim 1.3055(best: 1.2970), Xent 0.0741, Loss 1.3425, Error 0.0236(best: 0.0268)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.8699(9.9642) | Bit/dim 1.3264(1.3349) | Xent 0.1070(0.1563) | Loss 1.3799(1.4130) | Error 0.0333(0.0494) Steps 482(487.10) | Grad Norm 8.0806(10.4286) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 9.7576(9.9868) | Bit/dim 1.3074(1.3297) | Xent 0.1439(0.1541) | Loss 1.3794(1.4068) | Error 0.0433(0.0494) Steps 482(487.94) | Grad Norm 10.2852(10.3295) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 9.4973(9.9883) | Bit/dim 1.3119(1.3244) | Xent 0.1379(0.1492) | Loss 1.3809(1.3991) | Error 0.0456(0.0473) Steps 482(487.98) | Grad Norm 10.4540(10.8113) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 10.0702(9.9528) | Bit/dim 1.3004(1.3180) | Xent 0.1446(0.1480) | Loss 1.3727(1.3919) | Error 0.0433(0.0468) Steps 494(487.36) | Grad Norm 7.0864(9.9590) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 10.0713(9.9475) | Bit/dim 1.2883(1.3089) | Xent 0.1300(0.1456) | Loss 1.3533(1.3817) | Error 0.0389(0.0462) Steps 488(487.04) | Grad Norm 2.6164(8.5708) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 9.5665(9.9570) | Bit/dim 1.2981(1.3033) | Xent 0.1783(0.1445) | Loss 1.3873(1.3755) | Error 0.0522(0.0463) Steps 482(486.94) | Grad Norm 8.4432(8.0460) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 43.8462, Epoch Time 717.8517(631.2688), Bit/dim 1.2976(best: 1.2970), Xent 0.0681, Loss 1.3317, Error 0.0222(best: 0.0236)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.9740(9.9883) | Bit/dim 1.3009(1.2996) | Xent 0.1648(0.1434) | Loss 1.3832(1.3713) | Error 0.0433(0.0459) Steps 482(487.52) | Grad Norm 12.4422(8.2536) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 9.8052(9.9953) | Bit/dim 1.3291(1.2978) | Xent 0.1472(0.1411) | Loss 1.4027(1.3683) | Error 0.0467(0.0449) Steps 488(487.71) | Grad Norm 17.7021(8.8567) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 9.8948(10.0761) | Bit/dim 1.3560(1.3135) | Xent 0.1343(0.1406) | Loss 1.4232(1.3838) | Error 0.0478(0.0451) Steps 488(489.10) | Grad Norm 15.0599(11.3065) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 10.2635(10.0384) | Bit/dim 1.3726(1.3239) | Xent 0.1204(0.1403) | Loss 1.4328(1.3940) | Error 0.0356(0.0444) Steps 482(489.44) | Grad Norm 13.5476(12.0354) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 10.0927(10.0228) | Bit/dim 1.3168(1.3197) | Xent 0.1450(0.1423) | Loss 1.3893(1.3908) | Error 0.0422(0.0448) Steps 488(489.61) | Grad Norm 13.4550(11.3334) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 10.1132(10.0134) | Bit/dim 1.2763(1.3119) | Xent 0.1152(0.1372) | Loss 1.3339(1.3805) | Error 0.0389(0.0433) Steps 506(489.56) | Grad Norm 7.3070(10.0411) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 10.2500(10.0244) | Bit/dim 1.2749(1.3026) | Xent 0.1388(0.1364) | Loss 1.3444(1.3708) | Error 0.0422(0.0434) Steps 488(489.47) | Grad Norm 3.3236(8.3745) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 43.5861, Epoch Time 722.0230(633.9914), Bit/dim 1.2651(best: 1.2970), Xent 0.0680, Loss 1.2991, Error 0.0220(best: 0.0222)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 10.4724(10.0520) | Bit/dim 1.2799(1.2976) | Xent 0.1217(0.1345) | Loss 1.3407(1.3649) | Error 0.0433(0.0425) Steps 488(489.73) | Grad Norm 5.4672(7.2006) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 9.6629(10.0371) | Bit/dim 1.2910(1.2913) | Xent 0.1156(0.1332) | Loss 1.3488(1.3579) | Error 0.0367(0.0422) Steps 488(489.28) | Grad Norm 12.9243(6.9695) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 10.6721(10.0635) | Bit/dim 1.2950(1.2988) | Xent 0.1524(0.1344) | Loss 1.3712(1.3661) | Error 0.0467(0.0427) Steps 494(490.47) | Grad Norm 6.4034(9.1823) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 9.9762(10.0488) | Bit/dim 1.3266(1.3223) | Xent 0.1451(0.1392) | Loss 1.3991(1.3920) | Error 0.0489(0.0439) Steps 488(491.03) | Grad Norm 4.3808(10.8742) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 10.3416(10.0204) | Bit/dim 1.3175(1.3218) | Xent 0.1457(0.1379) | Loss 1.3904(1.3908) | Error 0.0467(0.0439) Steps 488(491.25) | Grad Norm 10.7564(10.4994) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 10.0151(10.0255) | Bit/dim 1.3016(1.3127) | Xent 0.1026(0.1343) | Loss 1.3529(1.3799) | Error 0.0300(0.0426) Steps 494(491.67) | Grad Norm 2.3690(9.1374) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 44.0348, Epoch Time 721.4397(636.6149), Bit/dim 1.2747(best: 1.2651), Xent 0.0578, Loss 1.3036, Error 0.0195(best: 0.0220)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 10.1180(10.0017) | Bit/dim 1.2787(1.3040) | Xent 0.1358(0.1366) | Loss 1.3466(1.3723) | Error 0.0444(0.0437) Steps 488(490.83) | Grad Norm 9.9923(8.5645) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 10.1331(9.9944) | Bit/dim 1.3162(1.2980) | Xent 0.1119(0.1331) | Loss 1.3722(1.3645) | Error 0.0378(0.0425) Steps 488(490.93) | Grad Norm 13.9780(9.0505) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 9.8865(9.9977) | Bit/dim 1.2922(1.2930) | Xent 0.1157(0.1312) | Loss 1.3500(1.3586) | Error 0.0422(0.0419) Steps 488(490.96) | Grad Norm 10.2579(9.2849) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 9.8221(9.9762) | Bit/dim 1.2768(1.2907) | Xent 0.1267(0.1296) | Loss 1.3402(1.3555) | Error 0.0444(0.0417) Steps 488(490.68) | Grad Norm 10.7976(10.0766) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 10.1046(9.9990) | Bit/dim 1.2559(1.2847) | Xent 0.1158(0.1285) | Loss 1.3138(1.3490) | Error 0.0356(0.0412) Steps 494(490.44) | Grad Norm 2.3628(8.7784) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 10.9948(10.0312) | Bit/dim 1.3081(1.2840) | Xent 0.1656(0.1279) | Loss 1.3909(1.3480) | Error 0.0600(0.0413) Steps 506(490.97) | Grad Norm 23.2753(10.0756) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 10.1939(10.0246) | Bit/dim 1.2681(1.2812) | Xent 0.1063(0.1238) | Loss 1.3213(1.3430) | Error 0.0322(0.0397) Steps 488(491.15) | Grad Norm 12.0649(9.4016) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 43.9563, Epoch Time 720.3151(639.1259), Bit/dim 1.2622(best: 1.2651), Xent 0.0564, Loss 1.2904, Error 0.0176(best: 0.0195)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 10.5461(10.0471) | Bit/dim 1.2915(1.2834) | Xent 0.1176(0.1263) | Loss 1.3503(1.3465) | Error 0.0389(0.0398) Steps 512(493.44) | Grad Norm 14.2876(10.7711) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 10.0281(10.0443) | Bit/dim 1.2879(1.2836) | Xent 0.1160(0.1231) | Loss 1.3459(1.3452) | Error 0.0344(0.0386) Steps 494(493.92) | Grad Norm 10.9095(11.3389) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 9.8706(10.0332) | Bit/dim 1.2438(1.2824) | Xent 0.1149(0.1234) | Loss 1.3012(1.3442) | Error 0.0400(0.0391) Steps 488(494.08) | Grad Norm 2.8976(11.0207) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 10.2717(10.0470) | Bit/dim 1.2662(1.2781) | Xent 0.1077(0.1219) | Loss 1.3201(1.3390) | Error 0.0311(0.0385) Steps 494(494.49) | Grad Norm 3.6428(10.0239) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 10.5987(10.0877) | Bit/dim 1.2776(1.2752) | Xent 0.1262(0.1217) | Loss 1.3407(1.3361) | Error 0.0411(0.0386) Steps 518(495.87) | Grad Norm 20.0563(10.3641) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 10.5956(10.1106) | Bit/dim 1.2501(1.2716) | Xent 0.1473(0.1184) | Loss 1.3238(1.3308) | Error 0.0411(0.0373) Steps 518(497.49) | Grad Norm 18.9227(10.2594) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 10.4454(10.1338) | Bit/dim 1.2517(1.2714) | Xent 0.0963(0.1142) | Loss 1.2999(1.3285) | Error 0.0356(0.0367) Steps 500(497.24) | Grad Norm 2.5274(10.5745) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 44.3029, Epoch Time 727.6571(641.7818), Bit/dim 1.2583(best: 1.2622), Xent 0.0550, Loss 1.2858, Error 0.0188(best: 0.0176)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 10.3198(10.1720) | Bit/dim 1.2936(1.2690) | Xent 0.0794(0.1100) | Loss 1.3333(1.3240) | Error 0.0256(0.0358) Steps 500(499.28) | Grad Norm 14.5440(10.3430) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 9.8094(10.1817) | Bit/dim 1.2513(1.2706) | Xent 0.1130(0.1094) | Loss 1.3078(1.3254) | Error 0.0389(0.0357) Steps 494(499.89) | Grad Norm 8.1665(11.2064) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 10.0301(10.1839) | Bit/dim 1.2363(1.2691) | Xent 0.1330(0.1099) | Loss 1.3028(1.3241) | Error 0.0456(0.0355) Steps 500(500.77) | Grad Norm 5.2623(11.0787) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 10.3713(10.2177) | Bit/dim 1.2430(1.2640) | Xent 0.0917(0.1079) | Loss 1.2889(1.3179) | Error 0.0322(0.0348) Steps 506(500.12) | Grad Norm 6.0967(9.7332) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 9.9140(10.2118) | Bit/dim 1.2803(1.2621) | Xent 0.1189(0.1083) | Loss 1.3397(1.3163) | Error 0.0311(0.0344) Steps 494(500.54) | Grad Norm 8.7925(9.6464) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 9.9195(10.2175) | Bit/dim 1.2491(1.2644) | Xent 0.1169(0.1108) | Loss 1.3076(1.3198) | Error 0.0389(0.0350) Steps 494(500.56) | Grad Norm 7.8444(10.5112) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 45.5860, Epoch Time 736.2636(644.6163), Bit/dim 1.2467(best: 1.2583), Xent 0.0514, Loss 1.2724, Error 0.0176(best: 0.0176)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.1152(10.1962) | Bit/dim 1.2474(1.2621) | Xent 0.0776(0.1073) | Loss 1.2862(1.3158) | Error 0.0222(0.0346) Steps 500(500.32) | Grad Norm 7.4436(10.3539) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 10.3062(10.1978) | Bit/dim 1.2376(1.2573) | Xent 0.1168(0.1080) | Loss 1.2961(1.3113) | Error 0.0433(0.0354) Steps 500(500.41) | Grad Norm 3.2540(9.0072) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 10.2687(10.2040) | Bit/dim 1.2251(1.2531) | Xent 0.0971(0.1076) | Loss 1.2736(1.3069) | Error 0.0356(0.0357) Steps 506(500.63) | Grad Norm 4.4115(7.3850) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 10.1753(10.2123) | Bit/dim 1.2279(1.2481) | Xent 0.1080(0.1036) | Loss 1.2819(1.2999) | Error 0.0378(0.0342) Steps 506(500.96) | Grad Norm 2.8887(6.5947) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 10.3289(10.2189) | Bit/dim 1.2189(1.2439) | Xent 0.1021(0.1011) | Loss 1.2699(1.2944) | Error 0.0322(0.0332) Steps 506(501.03) | Grad Norm 4.6111(6.3665) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 10.0329(10.2671) | Bit/dim 1.3587(1.2593) | Xent 0.1064(0.1030) | Loss 1.4119(1.3108) | Error 0.0400(0.0333) Steps 500(503.06) | Grad Norm 21.2268(9.5831) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 10.2881(10.2502) | Bit/dim 1.3371(1.2750) | Xent 0.1060(0.1105) | Loss 1.3901(1.3303) | Error 0.0300(0.0346) Steps 500(504.02) | Grad Norm 10.5611(10.8550) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 43.5630, Epoch Time 734.8933(647.3246), Bit/dim 1.2480(best: 1.2467), Xent 0.0733, Loss 1.2846, Error 0.0224(best: 0.0176)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 10.5072(10.2092) | Bit/dim 1.2594(1.2737) | Xent 0.0933(0.1131) | Loss 1.3061(1.3303) | Error 0.0311(0.0359) Steps 500(502.32) | Grad Norm 11.9507(10.4363) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 10.6820(10.1980) | Bit/dim 1.2455(1.2682) | Xent 0.1040(0.1109) | Loss 1.2975(1.3237) | Error 0.0400(0.0351) Steps 500(501.59) | Grad Norm 11.8431(9.6819) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 10.1686(10.1930) | Bit/dim 1.2589(1.2648) | Xent 0.1300(0.1068) | Loss 1.3239(1.3182) | Error 0.0489(0.0345) Steps 494(500.52) | Grad Norm 6.8339(9.8828) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 10.3269(10.1968) | Bit/dim 1.2483(1.2593) | Xent 0.0801(0.1061) | Loss 1.2884(1.3123) | Error 0.0222(0.0339) Steps 506(500.79) | Grad Norm 4.7160(9.2480) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 10.6092(10.2079) | Bit/dim 1.2614(1.2575) | Xent 0.1224(0.1050) | Loss 1.3226(1.3100) | Error 0.0322(0.0336) Steps 512(500.90) | Grad Norm 20.6542(10.1944) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 10.3190(10.2161) | Bit/dim 1.2374(1.2519) | Xent 0.1034(0.1039) | Loss 1.2891(1.3039) | Error 0.0300(0.0333) Steps 500(501.60) | Grad Norm 6.8499(9.4861) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 45.9758, Epoch Time 734.1150(649.9283), Bit/dim 1.2203(best: 1.2467), Xent 0.0433, Loss 1.2420, Error 0.0139(best: 0.0176)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 10.1131(10.2391) | Bit/dim 1.2274(1.2447) | Xent 0.1028(0.1025) | Loss 1.2788(1.2960) | Error 0.0389(0.0328) Steps 506(501.38) | Grad Norm 7.3177(8.5463) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 10.1363(10.2587) | Bit/dim 1.2245(1.2397) | Xent 0.1190(0.1008) | Loss 1.2840(1.2901) | Error 0.0344(0.0325) Steps 500(501.49) | Grad Norm 5.5650(7.5515) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 10.1499(10.2603) | Bit/dim 1.2089(1.2364) | Xent 0.1007(0.0997) | Loss 1.2592(1.2862) | Error 0.0344(0.0320) Steps 500(501.26) | Grad Norm 5.4247(6.9343) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 10.2797(10.2884) | Bit/dim 1.2067(1.2318) | Xent 0.0734(0.0977) | Loss 1.2435(1.2807) | Error 0.0211(0.0313) Steps 506(501.26) | Grad Norm 6.3410(6.7464) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 10.2311(10.2680) | Bit/dim 1.2211(1.2323) | Xent 0.1203(0.0974) | Loss 1.2812(1.2810) | Error 0.0356(0.0313) Steps 500(501.29) | Grad Norm 3.6151(7.3890) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 9.8923(10.2766) | Bit/dim 1.3150(1.2369) | Xent 0.0734(0.0973) | Loss 1.3517(1.2856) | Error 0.0222(0.0312) Steps 494(501.26) | Grad Norm 19.3045(9.0446) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 10.5704(10.2992) | Bit/dim 1.3457(1.2504) | Xent 0.1484(0.1018) | Loss 1.4198(1.3013) | Error 0.0533(0.0329) Steps 524(503.17) | Grad Norm 41.3401(11.1962) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 48.6665, Epoch Time 743.7036(652.7416), Bit/dim 1.3201(best: 1.2203), Xent 0.0603, Loss 1.3502, Error 0.0187(best: 0.0139)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 10.2097(10.2537) | Bit/dim 1.2836(1.2681) | Xent 0.1029(0.1026) | Loss 1.3351(1.3194) | Error 0.0322(0.0326) Steps 506(503.06) | Grad Norm 11.8577(11.5374) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 9.9824(10.2182) | Bit/dim 1.2552(1.2656) | Xent 0.1197(0.1053) | Loss 1.3151(1.3182) | Error 0.0367(0.0327) Steps 500(503.26) | Grad Norm 11.5660(11.1999) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 10.2583(10.2237) | Bit/dim 1.2383(1.2588) | Xent 0.0854(0.1035) | Loss 1.2811(1.3106) | Error 0.0267(0.0323) Steps 494(502.80) | Grad Norm 9.4347(10.5390) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 9.8642(10.2269) | Bit/dim 1.2384(1.2525) | Xent 0.0879(0.1002) | Loss 1.2824(1.3026) | Error 0.0244(0.0315) Steps 494(502.24) | Grad Norm 8.5551(9.9941) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 9.7685(10.2002) | Bit/dim 1.2586(1.2504) | Xent 0.0941(0.0994) | Loss 1.3057(1.3002) | Error 0.0222(0.0309) Steps 494(502.23) | Grad Norm 13.1194(10.6778) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 9.7744(10.2130) | Bit/dim 1.2287(1.2461) | Xent 0.0756(0.0985) | Loss 1.2665(1.2953) | Error 0.0256(0.0309) Steps 494(502.21) | Grad Norm 8.9855(10.4418) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 10.5892(10.2211) | Bit/dim 1.2145(1.2390) | Xent 0.1003(0.0958) | Loss 1.2646(1.2869) | Error 0.0344(0.0303) Steps 500(501.77) | Grad Norm 3.1161(9.0913) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 45.4117, Epoch Time 734.2786(655.1877), Bit/dim 1.2091(best: 1.2203), Xent 0.0420, Loss 1.2302, Error 0.0143(best: 0.0139)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.6349(10.2605) | Bit/dim 1.2152(1.2332) | Xent 0.0837(0.0929) | Loss 1.2570(1.2797) | Error 0.0267(0.0296) Steps 506(502.63) | Grad Norm 2.4069(7.4495) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 10.2077(10.2508) | Bit/dim 1.2024(1.2278) | Xent 0.1040(0.0905) | Loss 1.2545(1.2731) | Error 0.0289(0.0287) Steps 500(501.94) | Grad Norm 5.7968(6.2618) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 10.1089(10.2671) | Bit/dim 1.1845(1.2233) | Xent 0.1001(0.0912) | Loss 1.2345(1.2689) | Error 0.0322(0.0286) Steps 500(501.74) | Grad Norm 2.8930(6.1474) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 10.4024(10.3104) | Bit/dim 1.2116(1.2212) | Xent 0.0914(0.0879) | Loss 1.2573(1.2651) | Error 0.0311(0.0275) Steps 506(501.43) | Grad Norm 11.0644(6.1763) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 10.0884(10.2739) | Bit/dim 1.2328(1.2206) | Xent 0.0767(0.0874) | Loss 1.2711(1.2642) | Error 0.0267(0.0276) Steps 500(501.26) | Grad Norm 8.2787(6.6677) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 10.0592(10.2799) | Bit/dim 1.3132(1.2349) | Xent 0.0476(0.0859) | Loss 1.3370(1.2778) | Error 0.0178(0.0277) Steps 500(502.98) | Grad Norm 9.6094(9.0686) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 45.0802, Epoch Time 742.8960(657.8189), Bit/dim 1.2536(best: 1.2091), Xent 0.0486, Loss 1.2779, Error 0.0149(best: 0.0139)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 10.2526(10.3428) | Bit/dim 1.2360(1.2430) | Xent 0.0894(0.0909) | Loss 1.2807(1.2885) | Error 0.0222(0.0288) Steps 506(503.70) | Grad Norm 10.3016(9.8587) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 10.3055(10.3349) | Bit/dim 1.2252(1.2425) | Xent 0.0706(0.0901) | Loss 1.2605(1.2876) | Error 0.0222(0.0286) Steps 494(504.35) | Grad Norm 5.5356(10.3666) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 10.1218(10.2851) | Bit/dim 1.2120(1.2362) | Xent 0.0844(0.0897) | Loss 1.2542(1.2810) | Error 0.0278(0.0282) Steps 500(503.52) | Grad Norm 4.2125(8.8318) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 10.3766(10.2800) | Bit/dim 1.2074(1.2289) | Xent 0.0966(0.0859) | Loss 1.2557(1.2719) | Error 0.0356(0.0273) Steps 500(502.75) | Grad Norm 4.0806(7.5183) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 10.0630(10.2675) | Bit/dim 1.2581(1.2245) | Xent 0.0881(0.0862) | Loss 1.3022(1.2676) | Error 0.0311(0.0274) Steps 500(503.16) | Grad Norm 16.6378(7.5482) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 10.4343(10.3300) | Bit/dim 1.2759(1.2485) | Xent 0.1404(0.0860) | Loss 1.3461(1.2915) | Error 0.0444(0.0273) Steps 500(504.64) | Grad Norm 6.4786(9.3402) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 10.4022(10.3046) | Bit/dim 1.2525(1.2562) | Xent 0.0955(0.0897) | Loss 1.3003(1.3010) | Error 0.0233(0.0280) Steps 512(505.68) | Grad Norm 3.7381(9.1485) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 45.7573, Epoch Time 740.6777(660.3047), Bit/dim 1.2243(best: 1.2091), Xent 0.0438, Loss 1.2462, Error 0.0150(best: 0.0139)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 10.1562(10.3343) | Bit/dim 1.2173(1.2501) | Xent 0.0975(0.0899) | Loss 1.2660(1.2950) | Error 0.0289(0.0279) Steps 506(506.67) | Grad Norm 5.8683(7.9382) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 10.2562(10.3076) | Bit/dim 1.2055(1.2402) | Xent 0.0596(0.0880) | Loss 1.2353(1.2842) | Error 0.0233(0.0274) Steps 506(506.63) | Grad Norm 3.5046(7.4623) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 11.0132(10.3424) | Bit/dim 1.2430(1.2389) | Xent 0.1057(0.0865) | Loss 1.2959(1.2822) | Error 0.0322(0.0265) Steps 530(507.98) | Grad Norm 19.7211(9.5077) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 10.4873(10.3183) | Bit/dim 1.2239(1.2320) | Xent 0.1112(0.0858) | Loss 1.2795(1.2749) | Error 0.0278(0.0260) Steps 512(508.72) | Grad Norm 10.0972(8.9315) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 10.4512(10.3392) | Bit/dim 1.2165(1.2268) | Xent 0.0792(0.0827) | Loss 1.2561(1.2681) | Error 0.0267(0.0255) Steps 506(508.67) | Grad Norm 9.9457(8.5801) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 10.3390(10.3867) | Bit/dim 1.2418(1.2294) | Xent 0.1191(0.0834) | Loss 1.3014(1.2711) | Error 0.0378(0.0261) Steps 500(509.07) | Grad Norm 14.7148(10.0465) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 46.1562, Epoch Time 744.9153(662.8430), Bit/dim 1.2050(best: 1.2091), Xent 0.0386, Loss 1.2243, Error 0.0127(best: 0.0139)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 10.0357(10.3410) | Bit/dim 1.2298(1.2263) | Xent 0.0708(0.0844) | Loss 1.2652(1.2685) | Error 0.0244(0.0268) Steps 506(508.01) | Grad Norm 9.0998(9.7595) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 10.4538(10.3601) | Bit/dim 1.2343(1.2277) | Xent 0.0670(0.0857) | Loss 1.2679(1.2706) | Error 0.0233(0.0271) Steps 506(508.31) | Grad Norm 9.4161(10.6447) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 9.9330(10.3325) | Bit/dim 1.2152(1.2230) | Xent 0.0522(0.0817) | Loss 1.2413(1.2639) | Error 0.0178(0.0258) Steps 506(508.66) | Grad Norm 9.5969(10.4920) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 10.0668(10.3270) | Bit/dim 1.2125(1.2198) | Xent 0.0673(0.0789) | Loss 1.2462(1.2593) | Error 0.0244(0.0251) Steps 506(509.65) | Grad Norm 8.4208(9.6097) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 9.9064(10.3472) | Bit/dim 1.2471(1.2199) | Xent 0.0600(0.0759) | Loss 1.2771(1.2578) | Error 0.0200(0.0240) Steps 506(509.81) | Grad Norm 14.0114(10.0658) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 9.9917(10.3444) | Bit/dim 1.2642(1.2236) | Xent 0.0723(0.0790) | Loss 1.3003(1.2631) | Error 0.0211(0.0246) Steps 506(509.90) | Grad Norm 13.0582(10.8296) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 10.5472(10.3500) | Bit/dim 1.2376(1.2246) | Xent 0.1131(0.0810) | Loss 1.2941(1.2650) | Error 0.0289(0.0250) Steps 524(510.70) | Grad Norm 21.6923(11.3685) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 46.4855, Epoch Time 745.0536(665.3093), Bit/dim 1.2208(best: 1.2050), Xent 0.0382, Loss 1.2399, Error 0.0130(best: 0.0127)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.9948(10.3146) | Bit/dim 1.2109(1.2257) | Xent 0.0690(0.0803) | Loss 1.2454(1.2658) | Error 0.0178(0.0252) Steps 506(509.74) | Grad Norm 2.5690(11.1346) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 10.1421(10.3381) | Bit/dim 1.2092(1.2225) | Xent 0.0743(0.0802) | Loss 1.2463(1.2626) | Error 0.0189(0.0251) Steps 506(510.89) | Grad Norm 7.7027(10.7702) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 10.7807(10.3441) | Bit/dim 1.2075(1.2186) | Xent 0.0851(0.0799) | Loss 1.2501(1.2585) | Error 0.0289(0.0253) Steps 512(510.83) | Grad Norm 4.6866(9.9630) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 10.5503(10.3805) | Bit/dim 1.1876(1.2136) | Xent 0.1006(0.0796) | Loss 1.2379(1.2534) | Error 0.0233(0.0248) Steps 512(511.87) | Grad Norm 3.6429(8.3919) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 10.5938(10.3935) | Bit/dim 1.1862(1.2072) | Xent 0.0970(0.0798) | Loss 1.2347(1.2471) | Error 0.0267(0.0247) Steps 512(512.16) | Grad Norm 5.6539(7.4662) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 10.3356(10.4195) | Bit/dim 1.1916(1.2033) | Xent 0.0947(0.0766) | Loss 1.2390(1.2416) | Error 0.0311(0.0240) Steps 512(512.77) | Grad Norm 6.4040(7.3507) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 10.3399(10.4154) | Bit/dim 1.2364(1.2026) | Xent 0.0470(0.0760) | Loss 1.2599(1.2406) | Error 0.0156(0.0238) Steps 512(512.57) | Grad Norm 16.5034(7.8550) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 46.0221, Epoch Time 747.5708(667.7772), Bit/dim 1.1881(best: 1.2050), Xent 0.0351, Loss 1.2056, Error 0.0113(best: 0.0127)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 9.8802(10.4993) | Bit/dim 1.2112(1.2113) | Xent 0.0628(0.0778) | Loss 1.2426(1.2503) | Error 0.0189(0.0239) Steps 500(514.84) | Grad Norm 9.3149(9.6459) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 10.6267(10.5064) | Bit/dim 1.2223(1.2124) | Xent 0.1017(0.0797) | Loss 1.2731(1.2523) | Error 0.0267(0.0243) Steps 530(514.88) | Grad Norm 18.8990(10.4310) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 10.5320(10.4464) | Bit/dim 1.1894(1.2154) | Xent 0.0852(0.0784) | Loss 1.2320(1.2546) | Error 0.0244(0.0239) Steps 506(513.09) | Grad Norm 3.4926(10.7463) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 9.9822(10.4591) | Bit/dim 1.2013(1.2140) | Xent 0.0621(0.0761) | Loss 1.2324(1.2520) | Error 0.0222(0.0233) Steps 518(515.15) | Grad Norm 9.1904(10.4964) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 10.2297(10.4934) | Bit/dim 1.2008(1.2142) | Xent 0.0738(0.0753) | Loss 1.2377(1.2518) | Error 0.0278(0.0231) Steps 512(516.69) | Grad Norm 4.4431(10.7447) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 10.6684(10.5201) | Bit/dim 1.2002(1.2123) | Xent 0.0615(0.0741) | Loss 1.2310(1.2494) | Error 0.0167(0.0229) Steps 506(515.21) | Grad Norm 8.8343(10.7008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 47.0107, Epoch Time 758.7803(670.5073), Bit/dim 1.1871(best: 1.1881), Xent 0.0381, Loss 1.2061, Error 0.0134(best: 0.0113)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.6637(10.5063) | Bit/dim 1.1871(1.2093) | Xent 0.0932(0.0734) | Loss 1.2337(1.2460) | Error 0.0289(0.0228) Steps 518(513.92) | Grad Norm 9.7188(10.3289) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 10.5157(10.5584) | Bit/dim 1.2634(1.2094) | Xent 0.0539(0.0741) | Loss 1.2904(1.2464) | Error 0.0233(0.0234) Steps 524(516.64) | Grad Norm 16.8759(10.6012) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 9.9539(10.5355) | Bit/dim 1.2201(1.2207) | Xent 0.0781(0.0737) | Loss 1.2592(1.2576) | Error 0.0167(0.0234) Steps 512(516.77) | Grad Norm 6.9005(11.7217) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 9.9921(10.4956) | Bit/dim 1.2057(1.2176) | Xent 0.0500(0.0715) | Loss 1.2307(1.2534) | Error 0.0178(0.0226) Steps 518(517.40) | Grad Norm 8.4060(11.0869) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 10.2768(10.4972) | Bit/dim 1.1915(1.2122) | Xent 0.0813(0.0713) | Loss 1.2321(1.2478) | Error 0.0211(0.0224) Steps 512(518.02) | Grad Norm 6.0927(9.8575) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 10.4443(10.4832) | Bit/dim 1.1794(1.2068) | Xent 0.0660(0.0702) | Loss 1.2124(1.2419) | Error 0.0200(0.0220) Steps 518(517.68) | Grad Norm 4.4613(9.4373) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 11.1935(10.5389) | Bit/dim 1.2063(1.2064) | Xent 0.1065(0.0711) | Loss 1.2596(1.2420) | Error 0.0333(0.0222) Steps 530(519.26) | Grad Norm 17.7978(10.1432) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 50.4697, Epoch Time 760.6303(673.2110), Bit/dim 1.1982(best: 1.1871), Xent 0.0357, Loss 1.2161, Error 0.0118(best: 0.0113)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 10.4502(10.5486) | Bit/dim 1.2109(1.2046) | Xent 0.0491(0.0708) | Loss 1.2355(1.2400) | Error 0.0156(0.0219) Steps 524(520.19) | Grad Norm 3.4392(9.3324) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 10.4152(10.5783) | Bit/dim 1.1731(1.2004) | Xent 0.0547(0.0690) | Loss 1.2004(1.2349) | Error 0.0178(0.0217) Steps 530(521.52) | Grad Norm 2.5002(8.3821) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 11.4842(10.6328) | Bit/dim 1.1944(1.2038) | Xent 0.0906(0.0718) | Loss 1.2397(1.2397) | Error 0.0267(0.0223) Steps 542(523.58) | Grad Norm 15.3522(9.9626) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 10.7754(10.5747) | Bit/dim 1.1868(1.2019) | Xent 0.0759(0.0705) | Loss 1.2247(1.2371) | Error 0.0267(0.0220) Steps 530(522.75) | Grad Norm 10.0513(9.4713) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 10.4953(10.6169) | Bit/dim 1.2130(1.2029) | Xent 0.0649(0.0696) | Loss 1.2455(1.2376) | Error 0.0178(0.0216) Steps 524(523.49) | Grad Norm 14.1168(10.3335) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 11.4571(10.6343) | Bit/dim 1.1831(1.2039) | Xent 0.0653(0.0700) | Loss 1.2158(1.2389) | Error 0.0200(0.0220) Steps 518(523.71) | Grad Norm 18.8192(10.6905) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 47.9865, Epoch Time 767.2624(676.0325), Bit/dim 1.2443(best: 1.1871), Xent 0.0331, Loss 1.2608, Error 0.0117(best: 0.0113)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 10.5805(10.6976) | Bit/dim 1.1896(1.2061) | Xent 0.0808(0.0699) | Loss 1.2300(1.2410) | Error 0.0322(0.0222) Steps 518(523.89) | Grad Norm 4.5009(10.9962) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 10.0725(10.6831) | Bit/dim 1.1986(1.2094) | Xent 0.0704(0.0712) | Loss 1.2338(1.2450) | Error 0.0300(0.0223) Steps 518(524.71) | Grad Norm 7.6680(11.6888) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 10.6698(10.6750) | Bit/dim 1.1795(1.2098) | Xent 0.0582(0.0691) | Loss 1.2085(1.2443) | Error 0.0144(0.0218) Steps 530(524.39) | Grad Norm 3.8033(11.1521) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 10.2054(10.6642) | Bit/dim 1.2031(1.2112) | Xent 0.0574(0.0663) | Loss 1.2318(1.2444) | Error 0.0211(0.0210) Steps 518(524.69) | Grad Norm 10.4149(11.6016) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 10.3563(10.6371) | Bit/dim 1.1980(1.2113) | Xent 0.0835(0.0664) | Loss 1.2398(1.2445) | Error 0.0267(0.0208) Steps 518(524.91) | Grad Norm 2.8135(11.3184) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 10.4344(10.6460) | Bit/dim 1.1982(1.2081) | Xent 0.0556(0.0677) | Loss 1.2260(1.2420) | Error 0.0144(0.0213) Steps 518(524.65) | Grad Norm 9.9578(11.0973) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 10.7471(10.6287) | Bit/dim 1.1930(1.2039) | Xent 0.0666(0.0670) | Loss 1.2263(1.2374) | Error 0.0233(0.0210) Steps 530(523.94) | Grad Norm 12.0733(10.3170) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 48.1675, Epoch Time 765.6078(678.7198), Bit/dim 1.1814(best: 1.1871), Xent 0.0275, Loss 1.1951, Error 0.0108(best: 0.0113)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.2726(10.6975) | Bit/dim 1.1717(1.1989) | Xent 0.0519(0.0676) | Loss 1.1976(1.2327) | Error 0.0233(0.0211) Steps 524(524.77) | Grad Norm 3.0509(9.0125) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 10.5959(10.7398) | Bit/dim 1.1876(1.1944) | Xent 0.0387(0.0620) | Loss 1.2070(1.2254) | Error 0.0111(0.0194) Steps 524(525.52) | Grad Norm 4.5795(7.6258) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 10.6778(10.8242) | Bit/dim 1.1777(1.1897) | Xent 0.0409(0.0614) | Loss 1.1981(1.2204) | Error 0.0111(0.0189) Steps 530(526.18) | Grad Norm 2.3337(6.4592) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 11.5293(10.8865) | Bit/dim 1.1824(1.1859) | Xent 0.0567(0.0608) | Loss 1.2108(1.2163) | Error 0.0222(0.0192) Steps 536(527.10) | Grad Norm 2.4606(6.0141) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 11.1195(10.9745) | Bit/dim 1.2272(1.1948) | Xent 0.0954(0.0633) | Loss 1.2749(1.2265) | Error 0.0300(0.0197) Steps 524(529.38) | Grad Norm 9.3508(7.7616) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 10.2869(10.8950) | Bit/dim 1.2231(1.2042) | Xent 0.0509(0.0655) | Loss 1.2486(1.2370) | Error 0.0144(0.0204) Steps 518(529.83) | Grad Norm 11.9363(9.7309) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 10.4312(10.8217) | Bit/dim 1.1901(1.2082) | Xent 0.0605(0.0662) | Loss 1.2204(1.2413) | Error 0.0189(0.0208) Steps 518(529.14) | Grad Norm 3.9717(10.3853) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 50.1295, Epoch Time 787.2695(681.9763), Bit/dim 1.2474(best: 1.1814), Xent 0.0382, Loss 1.2664, Error 0.0116(best: 0.0108)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 11.1067(10.7634) | Bit/dim 1.1994(1.2091) | Xent 0.0778(0.0657) | Loss 1.2384(1.2420) | Error 0.0256(0.0206) Steps 542(528.28) | Grad Norm 10.0946(10.6159) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 10.5031(10.6758) | Bit/dim 1.1861(1.2048) | Xent 0.0515(0.0627) | Loss 1.2119(1.2361) | Error 0.0156(0.0198) Steps 518(526.37) | Grad Norm 10.1796(10.3281) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 11.2114(10.7465) | Bit/dim 1.1927(1.2016) | Xent 0.0484(0.0642) | Loss 1.2169(1.2337) | Error 0.0156(0.0203) Steps 530(526.68) | Grad Norm 8.7021(9.8588) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 10.9800(10.7501) | Bit/dim 1.1986(1.1981) | Xent 0.0579(0.0637) | Loss 1.2276(1.2300) | Error 0.0167(0.0199) Steps 530(525.87) | Grad Norm 9.7916(9.6280) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 11.3339(10.7436) | Bit/dim 1.1911(1.1947) | Xent 0.0681(0.0627) | Loss 1.2252(1.2260) | Error 0.0233(0.0196) Steps 536(525.34) | Grad Norm 5.0110(9.0920) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 11.4936(10.8780) | Bit/dim 1.1703(1.1905) | Xent 0.0708(0.0612) | Loss 1.2057(1.2211) | Error 0.0189(0.0191) Steps 542(527.75) | Grad Norm 2.1104(8.0740) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 48.5840, Epoch Time 777.2659(684.8349), Bit/dim 1.1660(best: 1.1814), Xent 0.0294, Loss 1.1807, Error 0.0102(best: 0.0108)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 11.6008(10.9465) | Bit/dim 1.1871(1.1862) | Xent 0.0478(0.0600) | Loss 1.2110(1.2162) | Error 0.0133(0.0186) Steps 536(528.19) | Grad Norm 8.0334(7.4277) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 10.3149(10.9354) | Bit/dim 1.2583(1.1864) | Xent 0.0705(0.0592) | Loss 1.2935(1.2160) | Error 0.0200(0.0184) Steps 518(528.51) | Grad Norm 17.3397(8.3378) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 10.4040(10.9183) | Bit/dim 1.2252(1.1994) | Xent 0.0656(0.0646) | Loss 1.2580(1.2317) | Error 0.0156(0.0195) Steps 536(530.44) | Grad Norm 7.6119(9.7198) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 10.6872(10.9060) | Bit/dim 1.1978(1.1997) | Xent 0.0459(0.0652) | Loss 1.2207(1.2322) | Error 0.0156(0.0196) Steps 518(530.21) | Grad Norm 8.5567(9.4057) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 11.0141(10.9567) | Bit/dim 1.1889(1.1957) | Xent 0.0492(0.0660) | Loss 1.2135(1.2287) | Error 0.0144(0.0199) Steps 530(530.35) | Grad Norm 6.2233(8.6675) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 11.6368(10.9644) | Bit/dim 1.1842(1.1930) | Xent 0.0632(0.0652) | Loss 1.2158(1.2256) | Error 0.0189(0.0196) Steps 542(529.34) | Grad Norm 5.9728(8.6832) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 11.0712(10.9760) | Bit/dim 1.2194(1.1978) | Xent 0.0440(0.0629) | Loss 1.2414(1.2293) | Error 0.0178(0.0191) Steps 530(529.62) | Grad Norm 12.5246(10.2997) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_bs900_drop_0_5_run3 --seed 3 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
