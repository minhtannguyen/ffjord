{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_02_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_02_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.02, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 2540 | Time 14.1966(15.2228) | Bit/dim 3.7952(3.8037) | Xent 0.9920(0.9813) | Loss 9.3579(9.9757) | Error 0.3500(0.3484) Steps 562(563.45) | Grad Norm 7.2417(10.1154) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.9637(15.1400) | Bit/dim 3.8081(3.8029) | Xent 1.0052(0.9746) | Loss 9.5303(9.8272) | Error 0.3511(0.3463) Steps 574(565.05) | Grad Norm 14.4969(10.1190) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 13.9838(15.0153) | Bit/dim 3.7910(3.8025) | Xent 0.9480(0.9769) | Loss 9.2425(9.7242) | Error 0.3389(0.3472) Steps 562(563.05) | Grad Norm 11.2813(10.7325) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.1244(14.9411) | Bit/dim 3.7844(3.8009) | Xent 0.9550(0.9810) | Loss 9.4599(9.6589) | Error 0.3322(0.3486) Steps 556(565.27) | Grad Norm 13.9357(11.1248) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 14.0889(14.9027) | Bit/dim 3.8138(3.8002) | Xent 1.0262(0.9761) | Loss 9.4931(9.5986) | Error 0.3489(0.3468) Steps 556(564.85) | Grad Norm 15.9971(11.2205) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 79.6933, Epoch Time 935.6229(849.6531), Bit/dim 3.7976(best: inf), Xent 0.9554, Loss 4.2754, Error 0.3404(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 14.5337(14.9259) | Bit/dim 3.7794(3.7990) | Xent 0.9227(0.9756) | Loss 9.3546(10.0730) | Error 0.3289(0.3461) Steps 550(565.33) | Grad Norm 6.5120(10.5805) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 13.6438(14.8828) | Bit/dim 3.7972(3.7975) | Xent 0.9686(0.9669) | Loss 9.3305(9.8910) | Error 0.3478(0.3433) Steps 556(566.74) | Grad Norm 11.8351(10.1471) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.4415(14.8371) | Bit/dim 3.7581(3.7933) | Xent 0.9638(0.9620) | Loss 9.3869(9.7602) | Error 0.3411(0.3424) Steps 580(565.75) | Grad Norm 7.0295(9.9600) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 13.9683(14.7840) | Bit/dim 3.8135(3.7931) | Xent 0.8286(0.9567) | Loss 9.3421(9.6613) | Error 0.2933(0.3410) Steps 568(565.84) | Grad Norm 6.5767(9.6410) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.4742(14.8578) | Bit/dim 3.7876(3.7939) | Xent 0.9534(0.9535) | Loss 9.3354(9.5904) | Error 0.3378(0.3402) Steps 550(566.59) | Grad Norm 8.6465(9.2411) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.1396(14.9336) | Bit/dim 3.7792(3.7933) | Xent 0.9285(0.9525) | Loss 9.4265(9.5545) | Error 0.3311(0.3396) Steps 556(568.84) | Grad Norm 9.8970(9.4590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 76.1830, Epoch Time 917.1686(851.6786), Bit/dim 3.8088(best: 3.7976), Xent 0.9807, Loss 4.2991, Error 0.3437(best: 0.3404)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 14.2402(14.9302) | Bit/dim 3.7671(3.7917) | Xent 0.8832(0.9554) | Loss 9.3244(9.9691) | Error 0.3178(0.3426) Steps 562(570.03) | Grad Norm 9.9047(9.8255) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.7943(14.9226) | Bit/dim 3.8133(3.7943) | Xent 0.9505(0.9632) | Loss 9.4189(9.8406) | Error 0.3500(0.3460) Steps 550(569.92) | Grad Norm 13.8895(10.8624) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 14.1314(14.9275) | Bit/dim 3.7850(3.7938) | Xent 0.9028(0.9635) | Loss 9.3103(9.7289) | Error 0.3111(0.3437) Steps 562(569.03) | Grad Norm 10.9823(11.2241) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 13.7023(14.8649) | Bit/dim 3.8198(3.7929) | Xent 0.9428(0.9615) | Loss 9.5770(9.6421) | Error 0.3189(0.3431) Steps 538(568.06) | Grad Norm 11.1246(10.5072) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.9327(14.8499) | Bit/dim 3.7804(3.7929) | Xent 0.9081(0.9548) | Loss 9.3850(9.5710) | Error 0.3278(0.3408) Steps 568(565.20) | Grad Norm 10.3560(10.3414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 77.0224, Epoch Time 910.6703(853.4483), Bit/dim 3.7801(best: 3.7976), Xent 0.8964, Loss 4.2284, Error 0.3195(best: 0.3404)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 14.2790(14.8347) | Bit/dim 3.7813(3.7906) | Xent 0.8890(0.9414) | Loss 9.2995(10.0319) | Error 0.3122(0.3351) Steps 556(565.49) | Grad Norm 5.0600(9.2879) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 14.4937(14.8463) | Bit/dim 3.8279(3.7887) | Xent 0.9759(0.9355) | Loss 9.5841(9.8621) | Error 0.3333(0.3340) Steps 586(567.02) | Grad Norm 18.4667(10.0091) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.2320(14.8886) | Bit/dim 3.7939(3.7856) | Xent 0.9219(0.9469) | Loss 9.4424(9.7532) | Error 0.3311(0.3373) Steps 568(568.99) | Grad Norm 9.2529(10.6383) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.1238(14.8955) | Bit/dim 3.7546(3.7850) | Xent 0.9722(0.9494) | Loss 9.2696(9.6560) | Error 0.3433(0.3378) Steps 544(571.10) | Grad Norm 6.1318(10.9930) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.0768(14.9395) | Bit/dim 3.7882(3.7851) | Xent 1.0048(0.9456) | Loss 9.3503(9.5782) | Error 0.3722(0.3360) Steps 532(569.62) | Grad Norm 12.2734(10.7827) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 15.0009(15.0062) | Bit/dim 3.8285(3.7864) | Xent 0.9454(0.9464) | Loss 9.5067(9.5319) | Error 0.3144(0.3372) Steps 592(569.31) | Grad Norm 11.9362(10.3154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 77.4266, Epoch Time 920.6604(855.4647), Bit/dim 3.7888(best: 3.7801), Xent 0.8955, Loss 4.2366, Error 0.3197(best: 0.3195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 14.7811(14.9953) | Bit/dim 3.7547(3.7843) | Xent 0.8607(0.9347) | Loss 9.1547(9.9408) | Error 0.3144(0.3329) Steps 580(569.94) | Grad Norm 5.5590(9.8839) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 14.8167(14.9287) | Bit/dim 3.7858(3.7853) | Xent 0.8790(0.9306) | Loss 9.3129(9.7970) | Error 0.3200(0.3314) Steps 574(569.75) | Grad Norm 8.9827(9.0257) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.6327(14.8657) | Bit/dim 3.7778(3.7838) | Xent 0.8637(0.9258) | Loss 9.2399(9.6634) | Error 0.2989(0.3293) Steps 562(567.18) | Grad Norm 6.6670(9.0949) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 13.6428(14.8652) | Bit/dim 3.7944(3.7823) | Xent 0.9492(0.9329) | Loss 9.2525(9.5921) | Error 0.3400(0.3313) Steps 550(566.89) | Grad Norm 8.7955(9.7088) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.0066(14.8253) | Bit/dim 3.7834(3.7808) | Xent 1.0102(0.9338) | Loss 9.4896(9.5339) | Error 0.3767(0.3328) Steps 586(566.76) | Grad Norm 16.0239(10.3033) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 77.7603, Epoch Time 910.5283(857.1166), Bit/dim 3.7789(best: 3.7801), Xent 0.9327, Loss 4.2453, Error 0.3296(best: 0.3195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 14.7428(14.8475) | Bit/dim 3.7678(3.7794) | Xent 0.9099(0.9334) | Loss 9.2862(10.0330) | Error 0.3289(0.3324) Steps 544(566.15) | Grad Norm 5.0782(10.4464) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.1840(14.8052) | Bit/dim 3.7498(3.7766) | Xent 0.9007(0.9279) | Loss 9.2878(9.8382) | Error 0.3222(0.3299) Steps 544(564.62) | Grad Norm 10.5128(10.1825) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.1819(14.7946) | Bit/dim 3.7612(3.7773) | Xent 0.8920(0.9251) | Loss 9.1139(9.7055) | Error 0.3356(0.3297) Steps 568(565.31) | Grad Norm 5.9433(10.0534) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.8361(14.8402) | Bit/dim 3.7623(3.7764) | Xent 0.9802(0.9256) | Loss 9.4047(9.6086) | Error 0.3489(0.3299) Steps 586(565.42) | Grad Norm 8.2674(10.1517) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.6364(14.8526) | Bit/dim 3.7957(3.7782) | Xent 0.9408(0.9294) | Loss 9.3857(9.5429) | Error 0.3289(0.3313) Steps 556(568.98) | Grad Norm 8.4283(10.0755) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.2671(14.9000) | Bit/dim 3.7938(3.7790) | Xent 0.8507(0.9247) | Loss 9.3779(9.5011) | Error 0.3144(0.3317) Steps 598(571.39) | Grad Norm 11.8390(9.5948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 78.6388, Epoch Time 914.3244(858.8328), Bit/dim 3.7779(best: 3.7789), Xent 0.8870, Loss 4.2214, Error 0.3125(best: 0.3195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 14.4567(14.8649) | Bit/dim 3.7498(3.7774) | Xent 1.0062(0.9185) | Loss 9.4185(9.9189) | Error 0.3444(0.3280) Steps 556(568.03) | Grad Norm 16.8163(9.6835) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.8248(14.8968) | Bit/dim 3.7479(3.7762) | Xent 0.9831(0.9239) | Loss 9.3941(9.7713) | Error 0.3378(0.3288) Steps 580(569.96) | Grad Norm 8.3542(9.8436) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 13.5740(14.8242) | Bit/dim 3.7871(3.7740) | Xent 0.9525(0.9188) | Loss 9.4037(9.6498) | Error 0.3356(0.3270) Steps 562(568.93) | Grad Norm 14.8875(9.2430) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.1796(14.9157) | Bit/dim 3.7990(3.7743) | Xent 0.8636(0.9148) | Loss 9.3139(9.5666) | Error 0.3244(0.3261) Steps 562(568.31) | Grad Norm 6.6997(8.9877) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 16.0108(14.9807) | Bit/dim 3.7389(3.7742) | Xent 0.8671(0.9074) | Loss 9.2921(9.5125) | Error 0.3167(0.3248) Steps 544(569.92) | Grad Norm 8.2077(8.6931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 78.2109, Epoch Time 919.2104(860.6442), Bit/dim 3.7641(best: 3.7779), Xent 0.8854, Loss 4.2068, Error 0.3132(best: 0.3125)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.0481(15.0131) | Bit/dim 3.7654(3.7712) | Xent 0.9739(0.9015) | Loss 9.3516(9.9978) | Error 0.3422(0.3216) Steps 592(573.46) | Grad Norm 13.6834(8.7952) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 14.3355(14.9770) | Bit/dim 3.7885(3.7715) | Xent 0.8587(0.8968) | Loss 9.2416(9.8091) | Error 0.3056(0.3203) Steps 562(572.64) | Grad Norm 7.4693(9.0090) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 14.8077(14.9909) | Bit/dim 3.7626(3.7712) | Xent 0.9456(0.9062) | Loss 9.2965(9.6888) | Error 0.3389(0.3234) Steps 550(569.59) | Grad Norm 12.0556(9.8591) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 14.4381(14.9818) | Bit/dim 3.7600(3.7702) | Xent 0.9215(0.9072) | Loss 9.3557(9.5892) | Error 0.3333(0.3243) Steps 562(571.73) | Grad Norm 12.4518(9.8518) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.8418(15.0642) | Bit/dim 3.7628(3.7688) | Xent 0.9694(0.9131) | Loss 9.4215(9.5261) | Error 0.3444(0.3264) Steps 574(568.51) | Grad Norm 14.9299(9.9997) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.0537(14.9193) | Bit/dim 3.8174(3.7712) | Xent 0.9108(0.9098) | Loss 9.4051(9.4675) | Error 0.3044(0.3250) Steps 592(571.04) | Grad Norm 6.4393(9.7120) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 79.3388, Epoch Time 918.9809(862.3943), Bit/dim 3.7666(best: 3.7641), Xent 0.8705, Loss 4.2019, Error 0.3096(best: 0.3125)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.9791(14.9141) | Bit/dim 3.7408(3.7673) | Xent 0.8933(0.8988) | Loss 9.3330(9.8765) | Error 0.2867(0.3199) Steps 550(569.33) | Grad Norm 9.8783(9.6848) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.7984(14.8913) | Bit/dim 3.7719(3.7675) | Xent 0.9077(0.8952) | Loss 9.2961(9.7340) | Error 0.3333(0.3177) Steps 562(569.53) | Grad Norm 10.1792(9.8557) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.7029(14.9302) | Bit/dim 3.7926(3.7689) | Xent 0.8739(0.8947) | Loss 9.3981(9.6295) | Error 0.3167(0.3173) Steps 580(570.80) | Grad Norm 12.0036(10.8339) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 14.7466(14.9030) | Bit/dim 3.7603(3.7687) | Xent 0.8896(0.9019) | Loss 9.3274(9.5487) | Error 0.3289(0.3195) Steps 592(570.04) | Grad Norm 10.6631(10.8183) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.1927(14.9099) | Bit/dim 3.7573(3.7698) | Xent 0.9278(0.9009) | Loss 9.2442(9.4874) | Error 0.3322(0.3197) Steps 562(568.78) | Grad Norm 11.3704(10.5920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 78.1441, Epoch Time 917.6346(864.0515), Bit/dim 3.7759(best: 3.7641), Xent 0.8930, Loss 4.2224, Error 0.3146(best: 0.3096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 14.4268(14.8913) | Bit/dim 3.7434(3.7692) | Xent 0.8420(0.8936) | Loss 9.1474(9.9696) | Error 0.3078(0.3175) Steps 568(569.49) | Grad Norm 12.9799(10.0135) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 14.9606(14.9361) | Bit/dim 3.7545(3.7682) | Xent 0.8544(0.8920) | Loss 9.1753(9.7884) | Error 0.3033(0.3183) Steps 568(570.13) | Grad Norm 8.3758(9.7421) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.4684(14.9361) | Bit/dim 3.7621(3.7652) | Xent 0.8069(0.8892) | Loss 9.2675(9.6475) | Error 0.2922(0.3187) Steps 574(570.70) | Grad Norm 9.0550(9.6445) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.0870(15.0338) | Bit/dim 3.7445(3.7632) | Xent 0.8755(0.8850) | Loss 9.1687(9.5500) | Error 0.3178(0.3161) Steps 580(571.11) | Grad Norm 4.9980(9.2723) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.8577(15.1313) | Bit/dim 3.7700(3.7615) | Xent 0.9743(0.8876) | Loss 9.4672(9.4870) | Error 0.3400(0.3169) Steps 592(574.45) | Grad Norm 9.1886(9.6936) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 14.7468(14.9981) | Bit/dim 3.7493(3.7628) | Xent 0.8865(0.8861) | Loss 9.2595(9.4317) | Error 0.3378(0.3165) Steps 574(571.98) | Grad Norm 6.8998(9.6477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 77.8945, Epoch Time 921.4916(865.7747), Bit/dim 3.7633(best: 3.7641), Xent 0.8939, Loss 4.2103, Error 0.3168(best: 0.3096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 14.0904(14.8983) | Bit/dim 3.7930(3.7643) | Xent 0.8095(0.8753) | Loss 9.2888(9.8340) | Error 0.2933(0.3129) Steps 574(570.86) | Grad Norm 10.1146(10.0267) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.0888(14.8922) | Bit/dim 3.7438(3.7620) | Xent 0.9082(0.8738) | Loss 9.2664(9.6925) | Error 0.3289(0.3113) Steps 568(571.34) | Grad Norm 7.3076(9.6695) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 14.4026(14.9031) | Bit/dim 3.7619(3.7625) | Xent 0.8831(0.8738) | Loss 9.2655(9.5849) | Error 0.3122(0.3121) Steps 592(572.10) | Grad Norm 11.6409(9.9735) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 14.6785(14.9356) | Bit/dim 3.7397(3.7666) | Xent 0.8924(0.8944) | Loss 9.1902(9.5366) | Error 0.3256(0.3171) Steps 562(571.78) | Grad Norm 8.9979(11.6210) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.5590(14.9792) | Bit/dim 3.7576(3.7669) | Xent 0.9269(0.8988) | Loss 9.4279(9.4884) | Error 0.3178(0.3185) Steps 568(574.53) | Grad Norm 10.8812(11.1386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 78.0354, Epoch Time 917.7385(867.3336), Bit/dim 3.7624(best: 3.7633), Xent 0.8639, Loss 4.1944, Error 0.3102(best: 0.3096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.4161(15.0357) | Bit/dim 3.7869(3.7628) | Xent 0.8309(0.8908) | Loss 9.3709(9.9474) | Error 0.2989(0.3151) Steps 604(575.21) | Grad Norm 8.7689(10.5853) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 14.8497(15.0400) | Bit/dim 3.7053(3.7617) | Xent 0.9407(0.8854) | Loss 9.3217(9.7846) | Error 0.3256(0.3139) Steps 538(574.89) | Grad Norm 7.4052(10.1971) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 14.3833(15.0258) | Bit/dim 3.7781(3.7599) | Xent 0.8622(0.8830) | Loss 9.4077(9.6584) | Error 0.3200(0.3139) Steps 580(578.82) | Grad Norm 6.0736(10.1363) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 16.1309(15.0918) | Bit/dim 3.7677(3.7610) | Xent 0.8448(0.8768) | Loss 9.4423(9.5656) | Error 0.2944(0.3112) Steps 592(577.84) | Grad Norm 10.0967(9.6336) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.9609(15.0591) | Bit/dim 3.7616(3.7598) | Xent 0.9148(0.8726) | Loss 9.3029(9.4862) | Error 0.3167(0.3106) Steps 550(577.84) | Grad Norm 12.3141(9.2784) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.4585(14.9974) | Bit/dim 3.7592(3.7577) | Xent 0.9107(0.8782) | Loss 9.4482(9.4419) | Error 0.3211(0.3126) Steps 598(577.99) | Grad Norm 10.2733(9.5589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 81.4479, Epoch Time 925.7298(869.0855), Bit/dim 3.7492(best: 3.7624), Xent 0.8734, Loss 4.1859, Error 0.3121(best: 0.3096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.0892(15.0231) | Bit/dim 3.7634(3.7562) | Xent 0.8114(0.8657) | Loss 9.2276(9.8505) | Error 0.2833(0.3087) Steps 586(575.88) | Grad Norm 6.4337(9.6242) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 14.6793(15.0261) | Bit/dim 3.7485(3.7550) | Xent 0.9010(0.8658) | Loss 9.1954(9.6926) | Error 0.3122(0.3088) Steps 556(577.98) | Grad Norm 8.6485(9.5645) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 14.3145(15.0736) | Bit/dim 3.7462(3.7518) | Xent 0.8433(0.8671) | Loss 9.1733(9.5770) | Error 0.3000(0.3090) Steps 532(576.63) | Grad Norm 9.3935(8.9315) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.2259(15.1233) | Bit/dim 3.7603(3.7516) | Xent 0.8256(0.8619) | Loss 9.3852(9.4958) | Error 0.2856(0.3073) Steps 568(574.66) | Grad Norm 6.4249(8.6985) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.4584(15.0444) | Bit/dim 3.7313(3.7508) | Xent 0.9639(0.8682) | Loss 9.4686(9.4306) | Error 0.3500(0.3090) Steps 598(574.48) | Grad Norm 13.2804(9.7143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 79.9043, Epoch Time 925.2161(870.7694), Bit/dim 3.7531(best: 3.7492), Xent 0.8704, Loss 4.1883, Error 0.3085(best: 0.3096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.7429(15.0059) | Bit/dim 3.7327(3.7511) | Xent 0.8667(0.8672) | Loss 9.3729(9.9358) | Error 0.3056(0.3083) Steps 610(575.98) | Grad Norm 6.9307(9.8034) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.3618(15.0546) | Bit/dim 3.7582(3.7511) | Xent 0.9914(0.8659) | Loss 9.3697(9.7531) | Error 0.3778(0.3087) Steps 598(576.96) | Grad Norm 23.2288(9.8129) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 14.6564(15.0795) | Bit/dim 3.7574(3.7547) | Xent 0.9472(0.8940) | Loss 9.3879(9.6562) | Error 0.3378(0.3188) Steps 574(580.35) | Grad Norm 15.7051(10.9396) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.3999(15.1252) | Bit/dim 3.7354(3.7567) | Xent 0.8570(0.8903) | Loss 9.2385(9.5630) | Error 0.2933(0.3178) Steps 616(583.39) | Grad Norm 5.0004(9.9420) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 14.5534(15.0791) | Bit/dim 3.7286(3.7543) | Xent 0.9000(0.8811) | Loss 9.3149(9.4805) | Error 0.3233(0.3138) Steps 580(580.96) | Grad Norm 9.4684(9.1738) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.1326(15.0865) | Bit/dim 3.7884(3.7554) | Xent 0.8310(0.8761) | Loss 9.3500(9.4285) | Error 0.3078(0.3124) Steps 580(577.84) | Grad Norm 9.5911(9.7970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 78.4378, Epoch Time 928.5183(872.5019), Bit/dim 3.7495(best: 3.7492), Xent 0.8795, Loss 4.1893, Error 0.3133(best: 0.3085)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 14.4887(15.0765) | Bit/dim 3.7326(3.7541) | Xent 0.8249(0.8649) | Loss 9.0898(9.8401) | Error 0.3078(0.3089) Steps 556(578.21) | Grad Norm 5.6778(9.6897) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.2331(15.0598) | Bit/dim 3.7512(3.7520) | Xent 0.8516(0.8603) | Loss 9.1876(9.6797) | Error 0.3056(0.3070) Steps 568(577.67) | Grad Norm 11.1090(9.7874) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 14.7527(15.0364) | Bit/dim 3.7358(3.7500) | Xent 0.8585(0.8569) | Loss 9.1562(9.5633) | Error 0.3156(0.3067) Steps 580(578.54) | Grad Norm 7.6754(9.8111) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 14.7001(15.0537) | Bit/dim 3.7452(3.7469) | Xent 0.8473(0.8584) | Loss 9.2560(9.4766) | Error 0.3211(0.3068) Steps 550(577.84) | Grad Norm 12.0940(9.5847) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 16.3211(15.0760) | Bit/dim 3.7665(3.7505) | Xent 0.8828(0.8630) | Loss 9.3223(9.4310) | Error 0.3211(0.3075) Steps 586(578.17) | Grad Norm 8.8761(9.7718) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 80.0276, Epoch Time 925.7874(874.1004), Bit/dim 3.7487(best: 3.7492), Xent 0.8447, Loss 4.1710, Error 0.3000(best: 0.3085)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 16.1563(15.1262) | Bit/dim 3.7761(3.7501) | Xent 0.7444(0.8508) | Loss 9.1203(9.9281) | Error 0.2722(0.3036) Steps 574(577.29) | Grad Norm 9.8256(9.6608) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.3143(15.1317) | Bit/dim 3.7312(3.7483) | Xent 0.8005(0.8435) | Loss 9.2905(9.7438) | Error 0.2844(0.3022) Steps 574(576.41) | Grad Norm 6.1192(9.3592) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.3043(15.1735) | Bit/dim 3.7110(3.7455) | Xent 0.8356(0.8515) | Loss 9.2253(9.6163) | Error 0.2844(0.3038) Steps 586(580.07) | Grad Norm 5.3942(9.3611) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.4915(15.1499) | Bit/dim 3.7235(3.7435) | Xent 0.8148(0.8460) | Loss 9.1962(9.5110) | Error 0.2889(0.3029) Steps 574(582.35) | Grad Norm 7.3261(9.1185) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.6673(15.1645) | Bit/dim 3.7462(3.7469) | Xent 0.8758(0.8525) | Loss 9.3658(9.4593) | Error 0.3144(0.3054) Steps 610(581.18) | Grad Norm 9.8478(9.6327) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 14.8094(15.2540) | Bit/dim 3.7259(3.7451) | Xent 0.8608(0.8489) | Loss 9.1573(9.3909) | Error 0.3289(0.3049) Steps 592(581.71) | Grad Norm 10.2244(9.5298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.8997, Epoch Time 938.5122(876.0328), Bit/dim 3.7435(best: 3.7487), Xent 0.8373, Loss 4.1621, Error 0.2992(best: 0.3000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.1657(15.2751) | Bit/dim 3.7874(3.7442) | Xent 0.7550(0.8431) | Loss 9.3192(9.8144) | Error 0.2844(0.3016) Steps 586(582.01) | Grad Norm 9.7230(9.3041) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 14.6303(15.1620) | Bit/dim 3.7110(3.7406) | Xent 0.8314(0.8438) | Loss 8.9885(9.6594) | Error 0.2978(0.3006) Steps 592(586.62) | Grad Norm 8.8344(9.1673) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 14.8256(15.1566) | Bit/dim 3.7345(3.7391) | Xent 0.8241(0.8396) | Loss 9.1831(9.5420) | Error 0.2767(0.2983) Steps 586(586.41) | Grad Norm 9.0195(9.1095) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 14.6725(15.1870) | Bit/dim 3.7495(3.7401) | Xent 0.8198(0.8376) | Loss 9.2409(9.4544) | Error 0.2822(0.2993) Steps 574(586.54) | Grad Norm 8.8344(9.8525) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.0207(15.2218) | Bit/dim 3.7849(3.7449) | Xent 0.8483(0.8334) | Loss 9.3198(9.4090) | Error 0.2978(0.2971) Steps 592(586.60) | Grad Norm 12.1844(9.6768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 82.6290, Epoch Time 937.9746(877.8911), Bit/dim 3.7343(best: 3.7435), Xent 0.8316, Loss 4.1501, Error 0.2997(best: 0.2992)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.2323(15.2603) | Bit/dim 3.7557(3.7427) | Xent 0.7556(0.8308) | Loss 9.1529(9.9103) | Error 0.2644(0.2964) Steps 574(586.13) | Grad Norm 5.6702(9.3086) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.6324(15.2732) | Bit/dim 3.7448(3.7419) | Xent 0.8405(0.8284) | Loss 9.3209(9.7259) | Error 0.3011(0.2956) Steps 592(587.45) | Grad Norm 15.3587(9.1110) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.2294(15.3507) | Bit/dim 3.7447(3.7396) | Xent 0.8135(0.8246) | Loss 9.2340(9.5885) | Error 0.2889(0.2947) Steps 580(590.03) | Grad Norm 5.7101(8.7261) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.2727(15.3565) | Bit/dim 3.7332(3.7370) | Xent 0.8392(0.8339) | Loss 9.0989(9.4955) | Error 0.2889(0.2977) Steps 574(590.06) | Grad Norm 9.8806(9.4318) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.8948(15.4071) | Bit/dim 3.7402(3.7384) | Xent 0.8649(0.8339) | Loss 9.2805(9.4233) | Error 0.3067(0.2989) Steps 592(590.36) | Grad Norm 11.8009(9.7701) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.7211(15.3767) | Bit/dim 3.7593(3.7409) | Xent 0.7665(0.8287) | Loss 9.1121(9.3668) | Error 0.2622(0.2987) Steps 598(592.62) | Grad Norm 8.5304(9.6461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 81.7557, Epoch Time 946.5038(879.9494), Bit/dim 3.7400(best: 3.7343), Xent 0.8418, Loss 4.1609, Error 0.2972(best: 0.2992)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.1018(15.4275) | Bit/dim 3.7353(3.7398) | Xent 0.7320(0.8333) | Loss 9.0855(9.8121) | Error 0.2578(0.2995) Steps 580(595.15) | Grad Norm 7.2757(9.8433) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.5563(15.4320) | Bit/dim 3.7225(3.7365) | Xent 0.8141(0.8282) | Loss 9.1499(9.6441) | Error 0.2911(0.2979) Steps 598(593.50) | Grad Norm 4.5542(9.0720) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.5387(15.4150) | Bit/dim 3.7198(3.7374) | Xent 0.8134(0.8241) | Loss 9.0960(9.5305) | Error 0.3033(0.2966) Steps 598(594.28) | Grad Norm 5.6475(8.5166) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 14.8459(15.4509) | Bit/dim 3.7378(3.7383) | Xent 0.8418(0.8143) | Loss 9.1367(9.4394) | Error 0.2811(0.2912) Steps 568(594.60) | Grad Norm 8.7558(8.5342) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.3536(15.4065) | Bit/dim 3.7480(3.7365) | Xent 0.7924(0.8162) | Loss 9.1576(9.3749) | Error 0.2833(0.2914) Steps 610(595.27) | Grad Norm 8.2767(8.5015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 82.1538, Epoch Time 947.2107(881.9673), Bit/dim 3.7412(best: 3.7343), Xent 0.8510, Loss 4.1667, Error 0.3004(best: 0.2972)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 14.7051(15.3436) | Bit/dim 3.7030(3.7348) | Xent 0.7941(0.8085) | Loss 9.1156(9.8738) | Error 0.2856(0.2897) Steps 568(595.73) | Grad Norm 11.9843(8.6565) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 14.6379(15.2838) | Bit/dim 3.7452(3.7345) | Xent 0.8024(0.8081) | Loss 9.2217(9.6994) | Error 0.2811(0.2897) Steps 586(593.81) | Grad Norm 11.2994(8.6930) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.2664(15.2661) | Bit/dim 3.7153(3.7323) | Xent 0.8365(0.8107) | Loss 9.1286(9.5691) | Error 0.2967(0.2898) Steps 574(594.64) | Grad Norm 15.0539(9.5539) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 14.7484(15.2344) | Bit/dim 3.7587(3.7343) | Xent 0.9110(0.8259) | Loss 9.2371(9.4781) | Error 0.3244(0.2943) Steps 592(595.68) | Grad Norm 12.8503(10.3966) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.7852(15.3153) | Bit/dim 3.7139(3.7359) | Xent 0.7881(0.8326) | Loss 9.2497(9.4361) | Error 0.2644(0.2973) Steps 562(597.57) | Grad Norm 12.9579(10.3848) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.2439(15.2753) | Bit/dim 3.7049(3.7353) | Xent 0.8006(0.8208) | Loss 9.0393(9.3651) | Error 0.2911(0.2932) Steps 604(595.98) | Grad Norm 9.8643(9.8202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 81.8985, Epoch Time 938.3557(883.6589), Bit/dim 3.7275(best: 3.7343), Xent 0.8077, Loss 4.1313, Error 0.2886(best: 0.2972)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.3113(15.2942) | Bit/dim 3.7247(3.7329) | Xent 0.7728(0.8166) | Loss 9.2444(9.7877) | Error 0.2700(0.2920) Steps 586(596.51) | Grad Norm 7.4980(9.3180) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 14.1624(15.2328) | Bit/dim 3.7451(3.7340) | Xent 0.8711(0.8173) | Loss 9.2091(9.6296) | Error 0.3011(0.2911) Steps 568(595.61) | Grad Norm 17.6035(9.8893) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 14.8472(15.2781) | Bit/dim 3.6992(3.7337) | Xent 0.7648(0.8186) | Loss 9.1063(9.5233) | Error 0.2756(0.2930) Steps 568(595.34) | Grad Norm 9.0259(10.2975) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 15.0624(15.3663) | Bit/dim 3.7372(3.7319) | Xent 0.7792(0.8159) | Loss 9.2073(9.4357) | Error 0.2700(0.2908) Steps 592(597.45) | Grad Norm 5.9443(9.8175) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.6666(15.4211) | Bit/dim 3.7086(3.7297) | Xent 0.7769(0.8167) | Loss 9.1951(9.3669) | Error 0.2678(0.2918) Steps 616(598.48) | Grad Norm 12.1152(9.8230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 82.9782, Epoch Time 948.8537(885.6148), Bit/dim 3.7353(best: 3.7275), Xent 0.8411, Loss 4.1559, Error 0.2975(best: 0.2886)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.3395(15.4242) | Bit/dim 3.7314(3.7307) | Xent 0.7350(0.8124) | Loss 9.1230(9.8788) | Error 0.2644(0.2899) Steps 622(599.99) | Grad Norm 6.4821(9.6132) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.2243(15.4201) | Bit/dim 3.7134(3.7323) | Xent 0.8084(0.8070) | Loss 9.1139(9.6929) | Error 0.2833(0.2888) Steps 610(599.64) | Grad Norm 10.6730(9.2777) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.3821(15.4585) | Bit/dim 3.7329(3.7331) | Xent 0.7869(0.8053) | Loss 9.2153(9.5680) | Error 0.2667(0.2884) Steps 574(598.96) | Grad Norm 9.5560(9.8068) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.8141(15.4901) | Bit/dim 3.7372(3.7313) | Xent 0.7544(0.8053) | Loss 9.2024(9.4685) | Error 0.2800(0.2887) Steps 598(598.52) | Grad Norm 7.0712(9.5102) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 15.0621(15.3776) | Bit/dim 3.7068(3.7252) | Xent 0.8321(0.7939) | Loss 9.1016(9.3640) | Error 0.2922(0.2834) Steps 598(595.99) | Grad Norm 13.7372(9.0354) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.7489(15.4351) | Bit/dim 3.7514(3.7255) | Xent 0.8023(0.7972) | Loss 9.2057(9.3090) | Error 0.3033(0.2860) Steps 586(596.00) | Grad Norm 10.0842(8.9552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 82.1064, Epoch Time 951.3444(887.5867), Bit/dim 3.7279(best: 3.7275), Xent 0.8041, Loss 4.1300, Error 0.2863(best: 0.2886)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.6836(15.4927) | Bit/dim 3.7590(3.7259) | Xent 0.7913(0.7911) | Loss 9.2249(9.7337) | Error 0.2567(0.2831) Steps 604(598.05) | Grad Norm 10.1439(8.7300) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 15.1373(15.4939) | Bit/dim 3.7341(3.7248) | Xent 0.8221(0.7972) | Loss 9.1758(9.5826) | Error 0.2844(0.2849) Steps 598(600.40) | Grad Norm 12.1443(9.5479) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.8260(15.4618) | Bit/dim 3.7170(3.7241) | Xent 0.7756(0.7989) | Loss 9.2922(9.4721) | Error 0.2678(0.2844) Steps 622(600.21) | Grad Norm 5.4157(9.6258) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 15.5216(15.5031) | Bit/dim 3.7607(3.7249) | Xent 0.8257(0.7984) | Loss 9.1553(9.3980) | Error 0.3022(0.2867) Steps 610(602.34) | Grad Norm 10.9206(9.7704) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.1779(15.5754) | Bit/dim 3.7456(3.7258) | Xent 0.7398(0.8010) | Loss 9.2426(9.3570) | Error 0.2644(0.2871) Steps 616(605.86) | Grad Norm 8.1818(9.8599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 81.1889, Epoch Time 958.4414(889.7123), Bit/dim 3.7256(best: 3.7275), Xent 0.8321, Loss 4.1417, Error 0.2969(best: 0.2863)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 17.0122(15.6591) | Bit/dim 3.7241(3.7258) | Xent 0.8294(0.7957) | Loss 9.2485(9.8725) | Error 0.2922(0.2848) Steps 616(606.44) | Grad Norm 7.0339(9.5807) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.2666(15.6343) | Bit/dim 3.7223(3.7240) | Xent 0.8181(0.7962) | Loss 9.0723(9.6858) | Error 0.2933(0.2846) Steps 604(606.66) | Grad Norm 11.9451(9.9069) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.1782(15.6941) | Bit/dim 3.7707(3.7277) | Xent 0.8301(0.7985) | Loss 9.3318(9.5705) | Error 0.2922(0.2850) Steps 646(610.78) | Grad Norm 11.0262(10.4884) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.5342(15.6970) | Bit/dim 3.7291(3.7272) | Xent 0.7502(0.7921) | Loss 9.2444(9.4607) | Error 0.2722(0.2832) Steps 622(610.89) | Grad Norm 6.1395(9.6745) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 14.8312(15.6392) | Bit/dim 3.7361(3.7234) | Xent 0.8332(0.7840) | Loss 9.3180(9.3708) | Error 0.3100(0.2801) Steps 598(612.21) | Grad Norm 10.7452(8.9212) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 16.0250(15.6334) | Bit/dim 3.7434(3.7236) | Xent 0.7899(0.7891) | Loss 9.3296(9.3293) | Error 0.2911(0.2822) Steps 610(612.31) | Grad Norm 9.4047(9.2859) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 82.9881, Epoch Time 962.3838(891.8924), Bit/dim 3.7257(best: 3.7256), Xent 0.8225, Loss 4.1369, Error 0.2930(best: 0.2863)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 15.7991(15.6344) | Bit/dim 3.7075(3.7212) | Xent 0.7368(0.7835) | Loss 9.0705(9.7550) | Error 0.2656(0.2798) Steps 616(611.31) | Grad Norm 5.4383(8.9890) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.7428(15.6986) | Bit/dim 3.7022(3.7170) | Xent 0.8058(0.7766) | Loss 9.1785(9.5827) | Error 0.2978(0.2776) Steps 646(614.78) | Grad Norm 11.3995(8.8054) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.4030(15.7376) | Bit/dim 3.7281(3.7217) | Xent 0.8381(0.7754) | Loss 9.1742(9.4785) | Error 0.3067(0.2775) Steps 610(615.13) | Grad Norm 8.7763(8.6185) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.0370(15.7303) | Bit/dim 3.7138(3.7201) | Xent 0.7839(0.7829) | Loss 9.0738(9.3976) | Error 0.2867(0.2807) Steps 586(615.25) | Grad Norm 6.6111(9.1969) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.2097(15.7190) | Bit/dim 3.7099(3.7236) | Xent 0.8041(0.7929) | Loss 9.1599(9.3501) | Error 0.3044(0.2839) Steps 610(617.16) | Grad Norm 7.8047(9.9068) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 82.4327, Epoch Time 966.6008(894.1337), Bit/dim 3.7233(best: 3.7256), Xent 0.7911, Loss 4.1188, Error 0.2768(best: 0.2863)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.4372(15.7061) | Bit/dim 3.7091(3.7205) | Xent 0.7074(0.7861) | Loss 9.0212(9.8556) | Error 0.2511(0.2812) Steps 592(617.26) | Grad Norm 7.2755(9.7347) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 14.8327(15.7279) | Bit/dim 3.7052(3.7185) | Xent 0.7824(0.7756) | Loss 9.0920(9.6525) | Error 0.2700(0.2767) Steps 598(616.42) | Grad Norm 10.7749(8.9690) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.0975(15.6980) | Bit/dim 3.7140(3.7194) | Xent 0.7480(0.7729) | Loss 9.1504(9.5219) | Error 0.2667(0.2753) Steps 586(614.22) | Grad Norm 6.3655(8.6116) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.0231(15.6870) | Bit/dim 3.7491(3.7199) | Xent 0.7954(0.7792) | Loss 9.2584(9.4308) | Error 0.2833(0.2763) Steps 628(616.02) | Grad Norm 9.5618(8.8656) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.4647(15.7300) | Bit/dim 3.6817(3.7188) | Xent 0.8383(0.7784) | Loss 9.1276(9.3506) | Error 0.3011(0.2775) Steps 616(615.39) | Grad Norm 11.2990(8.8963) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.6942(15.7419) | Bit/dim 3.7232(3.7147) | Xent 0.7197(0.7758) | Loss 9.0189(9.2864) | Error 0.2500(0.2768) Steps 604(617.76) | Grad Norm 5.0595(8.3324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 82.7145, Epoch Time 965.9384(896.2878), Bit/dim 3.7122(best: 3.7233), Xent 0.7955, Loss 4.1099, Error 0.2808(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.9271(15.7867) | Bit/dim 3.7165(3.7139) | Xent 0.7696(0.7750) | Loss 9.2924(9.7515) | Error 0.2800(0.2766) Steps 628(617.69) | Grad Norm 17.8034(9.0630) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.9508(15.8924) | Bit/dim 3.7531(3.7143) | Xent 0.7710(0.7738) | Loss 9.3048(9.6059) | Error 0.3011(0.2763) Steps 646(619.80) | Grad Norm 7.9971(9.4536) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.2289(15.8266) | Bit/dim 3.6877(3.7122) | Xent 0.7322(0.7674) | Loss 9.0212(9.4717) | Error 0.2667(0.2739) Steps 610(619.05) | Grad Norm 6.8060(8.7382) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.7241(15.8924) | Bit/dim 3.7009(3.7121) | Xent 0.7491(0.7605) | Loss 9.1203(9.3778) | Error 0.2700(0.2718) Steps 628(619.58) | Grad Norm 9.0726(8.5126) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.2792(15.8829) | Bit/dim 3.7427(3.7135) | Xent 0.9163(0.7732) | Loss 9.3162(9.3328) | Error 0.3156(0.2752) Steps 592(620.68) | Grad Norm 10.0039(9.4900) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 84.5714, Epoch Time 977.9240(898.7369), Bit/dim 3.7144(best: 3.7122), Xent 0.7952, Loss 4.1119, Error 0.2812(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 15.4750(15.8506) | Bit/dim 3.6713(3.7149) | Xent 0.7428(0.7734) | Loss 9.0250(9.8586) | Error 0.2678(0.2761) Steps 616(620.76) | Grad Norm 16.6972(9.8352) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.5862(15.8956) | Bit/dim 3.7265(3.7146) | Xent 0.7266(0.7646) | Loss 9.1863(9.6772) | Error 0.2511(0.2720) Steps 604(617.63) | Grad Norm 11.2975(9.5370) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.3022(15.9354) | Bit/dim 3.7438(3.7140) | Xent 0.6959(0.7588) | Loss 9.1021(9.5330) | Error 0.2556(0.2702) Steps 646(618.15) | Grad Norm 4.5106(9.2000) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 14.9447(15.9033) | Bit/dim 3.7151(3.7137) | Xent 0.8610(0.7635) | Loss 9.2846(9.4436) | Error 0.3022(0.2721) Steps 628(618.73) | Grad Norm 11.4099(9.4584) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.5084(15.9243) | Bit/dim 3.7125(3.7147) | Xent 0.7490(0.7681) | Loss 9.2463(9.3788) | Error 0.2989(0.2755) Steps 628(620.04) | Grad Norm 8.7740(9.6162) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.0010(15.9272) | Bit/dim 3.6833(3.7110) | Xent 0.8005(0.7703) | Loss 8.9895(9.3146) | Error 0.2833(0.2747) Steps 604(616.78) | Grad Norm 7.4984(9.3794) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 85.4584, Epoch Time 980.4657(901.1888), Bit/dim 3.7054(best: 3.7122), Xent 0.8273, Loss 4.1191, Error 0.2922(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.8116(15.9171) | Bit/dim 3.7264(3.7139) | Xent 0.7390(0.7730) | Loss 9.2226(9.7893) | Error 0.2511(0.2753) Steps 598(618.71) | Grad Norm 13.1347(10.1306) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 16.8123(16.0337) | Bit/dim 3.7003(3.7143) | Xent 0.7165(0.7641) | Loss 9.0323(9.6222) | Error 0.2489(0.2717) Steps 676(623.23) | Grad Norm 11.8420(9.9138) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.0349(16.0386) | Bit/dim 3.7157(3.7111) | Xent 0.7338(0.7567) | Loss 9.0611(9.4948) | Error 0.2611(0.2699) Steps 628(624.31) | Grad Norm 8.5413(9.2647) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.8089(15.9709) | Bit/dim 3.7034(3.7097) | Xent 0.7427(0.7581) | Loss 9.2023(9.4136) | Error 0.2778(0.2719) Steps 622(624.05) | Grad Norm 5.6432(9.0111) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 15.2367(16.0084) | Bit/dim 3.7243(3.7102) | Xent 0.7019(0.7538) | Loss 9.0202(9.3419) | Error 0.2456(0.2697) Steps 634(622.52) | Grad Norm 8.0793(8.4676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 85.5218, Epoch Time 988.5623(903.8100), Bit/dim 3.7088(best: 3.7054), Xent 0.8257, Loss 4.1217, Error 0.2958(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.7738(16.0683) | Bit/dim 3.7353(3.7116) | Xent 0.7893(0.7586) | Loss 9.2787(9.8730) | Error 0.2878(0.2721) Steps 652(626.41) | Grad Norm 11.7097(9.6179) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.4798(16.0886) | Bit/dim 3.7106(3.7098) | Xent 0.7716(0.7625) | Loss 9.1480(9.6897) | Error 0.2622(0.2730) Steps 604(624.53) | Grad Norm 10.8529(9.6778) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.1863(16.1000) | Bit/dim 3.7265(3.7084) | Xent 0.7336(0.7533) | Loss 9.1755(9.5457) | Error 0.2567(0.2696) Steps 628(627.21) | Grad Norm 13.3973(9.4767) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 16.0852(16.0908) | Bit/dim 3.6929(3.7071) | Xent 0.7754(0.7537) | Loss 9.2371(9.4515) | Error 0.2900(0.2686) Steps 622(627.16) | Grad Norm 10.6385(9.9117) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.9784(16.1638) | Bit/dim 3.7169(3.7117) | Xent 0.7448(0.7512) | Loss 9.2236(9.3717) | Error 0.2711(0.2679) Steps 628(627.23) | Grad Norm 9.2216(9.8865) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.7039(16.1873) | Bit/dim 3.7307(3.7099) | Xent 0.8398(0.7593) | Loss 9.3480(9.3250) | Error 0.2944(0.2719) Steps 616(628.94) | Grad Norm 13.7782(10.0238) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 85.6510, Epoch Time 992.4324(906.4687), Bit/dim 3.7166(best: 3.7054), Xent 0.7736, Loss 4.1034, Error 0.2757(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 17.1486(16.1814) | Bit/dim 3.7180(3.7112) | Xent 0.6877(0.7464) | Loss 9.1883(9.7603) | Error 0.2522(0.2684) Steps 616(627.23) | Grad Norm 9.7468(9.6376) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 17.1461(16.1724) | Bit/dim 3.7237(3.7110) | Xent 0.6778(0.7418) | Loss 9.1942(9.5954) | Error 0.2256(0.2656) Steps 604(628.35) | Grad Norm 8.8484(9.8865) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 16.2240(16.1767) | Bit/dim 3.7019(3.7096) | Xent 0.7727(0.7454) | Loss 9.0991(9.4625) | Error 0.2756(0.2663) Steps 622(628.47) | Grad Norm 7.8243(9.4460) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.4518(16.1401) | Bit/dim 3.6977(3.7086) | Xent 0.7812(0.7457) | Loss 9.0283(9.3833) | Error 0.2778(0.2669) Steps 604(631.20) | Grad Norm 6.4803(9.1740) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 16.3890(16.2245) | Bit/dim 3.7310(3.7052) | Xent 0.7163(0.7489) | Loss 9.2275(9.3235) | Error 0.2611(0.2664) Steps 622(632.09) | Grad Norm 7.7949(8.9704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 85.4686, Epoch Time 995.1401(909.1288), Bit/dim 3.7024(best: 3.7054), Xent 0.7599, Loss 4.0823, Error 0.2668(best: 0.2757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 16.8105(16.3182) | Bit/dim 3.7186(3.7039) | Xent 0.7590(0.7483) | Loss 9.2729(9.8917) | Error 0.2767(0.2666) Steps 652(634.40) | Grad Norm 8.4383(8.6642) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 16.6661(16.3140) | Bit/dim 3.7274(3.7046) | Xent 0.6621(0.7444) | Loss 9.0382(9.6895) | Error 0.2289(0.2654) Steps 652(635.36) | Grad Norm 5.2269(8.6265) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.8434(16.3402) | Bit/dim 3.6985(3.7032) | Xent 0.6912(0.7332) | Loss 9.0918(9.5461) | Error 0.2444(0.2619) Steps 628(637.83) | Grad Norm 10.1390(8.6406) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 16.4838(16.3512) | Bit/dim 3.6613(3.7012) | Xent 0.7246(0.7301) | Loss 9.0258(9.4426) | Error 0.2600(0.2611) Steps 634(636.12) | Grad Norm 6.5212(8.1415) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.5969(16.3463) | Bit/dim 3.6975(3.7009) | Xent 0.7183(0.7236) | Loss 9.0716(9.3598) | Error 0.2500(0.2580) Steps 628(635.30) | Grad Norm 5.5467(7.5062) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.4058(16.2641) | Bit/dim 3.7025(3.7003) | Xent 0.7337(0.7229) | Loss 8.9832(9.2868) | Error 0.2600(0.2576) Steps 640(633.51) | Grad Norm 9.8881(7.6092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 86.3516, Epoch Time 1004.7739(911.9982), Bit/dim 3.6956(best: 3.7024), Xent 0.7586, Loss 4.0750, Error 0.2655(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 17.0156(16.3726) | Bit/dim 3.7163(3.7038) | Xent 0.7195(0.7193) | Loss 9.2061(9.7670) | Error 0.2511(0.2561) Steps 640(639.79) | Grad Norm 3.9365(7.4262) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 17.0437(16.4972) | Bit/dim 3.6886(3.7000) | Xent 0.7820(0.7196) | Loss 9.2530(9.6020) | Error 0.2789(0.2566) Steps 670(646.65) | Grad Norm 11.8020(7.9295) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.6388(16.4863) | Bit/dim 3.6683(3.6962) | Xent 0.7287(0.7220) | Loss 9.0631(9.4789) | Error 0.2689(0.2574) Steps 634(647.05) | Grad Norm 6.7384(7.9227) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.1397(16.4459) | Bit/dim 3.6617(3.6964) | Xent 0.7221(0.7223) | Loss 9.0631(9.3830) | Error 0.2711(0.2587) Steps 664(644.83) | Grad Norm 6.8859(8.4971) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 16.3009(16.4832) | Bit/dim 3.7011(3.6960) | Xent 0.6955(0.7199) | Loss 9.0429(9.3162) | Error 0.2311(0.2580) Steps 670(645.56) | Grad Norm 5.6316(7.9232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 85.3003, Epoch Time 1014.6536(915.0778), Bit/dim 3.7012(best: 3.6956), Xent 0.7691, Loss 4.0857, Error 0.2751(best: 0.2655)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 15.5445(16.4506) | Bit/dim 3.7122(3.6985) | Xent 0.6473(0.7173) | Loss 9.0382(9.8481) | Error 0.2322(0.2569) Steps 604(639.32) | Grad Norm 5.1409(8.0482) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 17.2986(16.5664) | Bit/dim 3.6713(3.6988) | Xent 0.6462(0.7125) | Loss 9.0815(9.6620) | Error 0.2356(0.2557) Steps 652(643.09) | Grad Norm 8.4119(7.8868) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.7691(16.5718) | Bit/dim 3.6900(3.6974) | Xent 0.6675(0.7142) | Loss 8.9248(9.5195) | Error 0.2400(0.2553) Steps 682(645.93) | Grad Norm 6.8693(7.7997) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 15.9877(16.5239) | Bit/dim 3.7178(3.6981) | Xent 0.7231(0.7221) | Loss 9.0774(9.4245) | Error 0.2611(0.2582) Steps 664(646.40) | Grad Norm 5.8413(8.5389) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 16.8933(16.6209) | Bit/dim 3.6782(3.6968) | Xent 0.6947(0.7185) | Loss 9.0247(9.3413) | Error 0.2589(0.2570) Steps 646(647.32) | Grad Norm 8.0807(8.3047) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 16.6324(16.5892) | Bit/dim 3.6695(3.6956) | Xent 0.7242(0.7232) | Loss 9.1471(9.2856) | Error 0.2567(0.2585) Steps 646(644.20) | Grad Norm 8.9395(8.7728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 85.9279, Epoch Time 1017.6310(918.1544), Bit/dim 3.6989(best: 3.6956), Xent 0.8061, Loss 4.1019, Error 0.2813(best: 0.2655)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.4046(16.5530) | Bit/dim 3.7133(3.6963) | Xent 0.8032(0.7244) | Loss 9.3438(9.7471) | Error 0.2900(0.2595) Steps 670(642.84) | Grad Norm 15.5731(9.2978) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 15.8647(16.4694) | Bit/dim 3.6589(3.6933) | Xent 0.7023(0.7250) | Loss 8.9225(9.5682) | Error 0.2589(0.2581) Steps 616(643.89) | Grad Norm 5.4673(9.1833) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 16.9537(16.5822) | Bit/dim 3.7012(3.6947) | Xent 0.6825(0.7142) | Loss 8.9489(9.4322) | Error 0.2400(0.2544) Steps 646(645.53) | Grad Norm 6.5324(8.5862) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.1494(16.5659) | Bit/dim 3.6994(3.6926) | Xent 0.7448(0.7127) | Loss 9.0935(9.3412) | Error 0.2600(0.2542) Steps 652(648.58) | Grad Norm 9.7260(8.9572) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.7753(16.5594) | Bit/dim 3.7117(3.6989) | Xent 0.7704(0.7276) | Loss 9.1421(9.3075) | Error 0.2867(0.2604) Steps 634(648.32) | Grad Norm 7.0210(9.8009) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 87.1421, Epoch Time 1015.6215(921.0784), Bit/dim 3.7072(best: 3.6956), Xent 0.7525, Loss 4.0835, Error 0.2648(best: 0.2655)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.7023(16.5328) | Bit/dim 3.6762(3.7018) | Xent 0.7077(0.7262) | Loss 9.1694(9.8794) | Error 0.2444(0.2592) Steps 640(648.98) | Grad Norm 7.3196(9.8425) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 16.7771(16.5243) | Bit/dim 3.7027(3.7026) | Xent 0.8150(0.7290) | Loss 9.1943(9.6788) | Error 0.2878(0.2595) Steps 694(650.72) | Grad Norm 10.1593(9.6096) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 16.7067(16.5481) | Bit/dim 3.7125(3.7021) | Xent 0.7051(0.7291) | Loss 9.2173(9.5356) | Error 0.2533(0.2600) Steps 664(652.30) | Grad Norm 6.4601(9.1883) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.1540(16.6144) | Bit/dim 3.6934(3.7016) | Xent 0.7177(0.7263) | Loss 9.1732(9.4404) | Error 0.2633(0.2612) Steps 658(652.87) | Grad Norm 10.9339(8.7953) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.6424(16.7267) | Bit/dim 3.6566(3.6984) | Xent 0.7659(0.7184) | Loss 9.1359(9.3624) | Error 0.2678(0.2590) Steps 664(653.22) | Grad Norm 8.1198(8.5347) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 17.3239(16.7267) | Bit/dim 3.6599(3.6952) | Xent 0.7387(0.7194) | Loss 9.1843(9.3043) | Error 0.2600(0.2581) Steps 652(655.11) | Grad Norm 7.3323(8.6324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 87.3375, Epoch Time 1023.8902(924.1628), Bit/dim 3.6884(best: 3.6956), Xent 0.7628, Loss 4.0699, Error 0.2672(best: 0.2648)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 16.3348(16.6921) | Bit/dim 3.6967(3.6924) | Xent 0.6978(0.7090) | Loss 9.1738(9.7778) | Error 0.2322(0.2547) Steps 640(656.76) | Grad Norm 9.6557(8.1787) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.0845(16.7430) | Bit/dim 3.7035(3.6897) | Xent 0.6835(0.7060) | Loss 9.2050(9.5966) | Error 0.2400(0.2532) Steps 646(655.47) | Grad Norm 12.2817(8.2871) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.1530(16.7934) | Bit/dim 3.7372(3.6923) | Xent 0.7469(0.7038) | Loss 9.1972(9.4695) | Error 0.2511(0.2516) Steps 682(658.64) | Grad Norm 8.3030(8.6111) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 16.5912(16.8444) | Bit/dim 3.6707(3.6922) | Xent 0.7233(0.7044) | Loss 9.0408(9.3727) | Error 0.2567(0.2516) Steps 670(655.50) | Grad Norm 7.2565(8.4516) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.1069(16.8061) | Bit/dim 3.7084(3.6935) | Xent 0.7155(0.7049) | Loss 9.1679(9.3105) | Error 0.2622(0.2518) Steps 652(655.16) | Grad Norm 7.1693(8.4053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 87.0715, Epoch Time 1029.5980(927.3258), Bit/dim 3.6922(best: 3.6884), Xent 0.7561, Loss 4.0702, Error 0.2647(best: 0.2648)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 16.9886(16.8207) | Bit/dim 3.6890(3.6939) | Xent 0.7446(0.7056) | Loss 9.1695(9.8788) | Error 0.2567(0.2511) Steps 646(656.03) | Grad Norm 6.3505(8.4150) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 17.9805(16.8285) | Bit/dim 3.6929(3.6914) | Xent 0.7100(0.7015) | Loss 9.3006(9.6756) | Error 0.2467(0.2498) Steps 646(656.38) | Grad Norm 6.5283(8.2731) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 17.7038(16.8481) | Bit/dim 3.6970(3.6932) | Xent 0.7066(0.6998) | Loss 9.2778(9.5367) | Error 0.2567(0.2493) Steps 670(656.69) | Grad Norm 8.1314(9.0069) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 16.3926(16.8114) | Bit/dim 3.6780(3.6905) | Xent 0.6424(0.6973) | Loss 8.9768(9.4153) | Error 0.2300(0.2490) Steps 652(656.52) | Grad Norm 9.3011(8.7106) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 16.8710(16.7549) | Bit/dim 3.6969(3.6897) | Xent 0.6687(0.6967) | Loss 8.9790(9.3169) | Error 0.2478(0.2495) Steps 676(655.53) | Grad Norm 7.5410(8.8724) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.1119(16.7725) | Bit/dim 3.6978(3.6874) | Xent 0.7726(0.7087) | Loss 9.1578(9.2701) | Error 0.2800(0.2537) Steps 604(658.45) | Grad Norm 11.0906(9.0863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 87.2700, Epoch Time 1028.3172(930.3556), Bit/dim 3.6893(best: 3.6884), Xent 0.7561, Loss 4.0674, Error 0.2669(best: 0.2647)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 16.5816(16.7578) | Bit/dim 3.7114(3.6926) | Xent 0.7015(0.7040) | Loss 9.1976(9.7473) | Error 0.2578(0.2512) Steps 664(658.54) | Grad Norm 9.1580(9.0851) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.2507(16.7884) | Bit/dim 3.7025(3.6903) | Xent 0.6677(0.6912) | Loss 9.1364(9.5670) | Error 0.2311(0.2455) Steps 664(656.54) | Grad Norm 5.7494(8.2687) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.1249(16.8102) | Bit/dim 3.6774(3.6888) | Xent 0.6585(0.6848) | Loss 9.0117(9.4297) | Error 0.2389(0.2422) Steps 652(655.50) | Grad Norm 4.9077(7.8679) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 17.2795(16.8551) | Bit/dim 3.6772(3.6857) | Xent 0.6587(0.6866) | Loss 8.9547(9.3417) | Error 0.2322(0.2428) Steps 646(658.40) | Grad Norm 9.1791(7.7925) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 16.9507(16.8407) | Bit/dim 3.6934(3.6855) | Xent 0.6754(0.6896) | Loss 8.9583(9.2678) | Error 0.2211(0.2445) Steps 652(659.26) | Grad Norm 11.5960(7.9285) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 86.1825, Epoch Time 1029.2741(933.3231), Bit/dim 3.6885(best: 3.6884), Xent 0.7481, Loss 4.0626, Error 0.2600(best: 0.2647)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.2505(16.8194) | Bit/dim 3.6822(3.6859) | Xent 0.7035(0.6857) | Loss 9.1301(9.8066) | Error 0.2556(0.2435) Steps 688(659.50) | Grad Norm 12.9875(8.2128) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 16.8620(16.9200) | Bit/dim 3.7191(3.6907) | Xent 0.6678(0.6805) | Loss 8.9539(9.6157) | Error 0.2444(0.2424) Steps 640(661.58) | Grad Norm 6.3089(7.8418) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 17.1881(16.8994) | Bit/dim 3.7047(3.6906) | Xent 0.6807(0.6775) | Loss 9.2441(9.4828) | Error 0.2489(0.2413) Steps 676(660.92) | Grad Norm 9.8083(8.1150) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 16.2008(16.9490) | Bit/dim 3.7012(3.6906) | Xent 0.6703(0.6809) | Loss 9.0884(9.3900) | Error 0.2556(0.2435) Steps 640(661.91) | Grad Norm 9.1052(8.6262) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 17.5287(16.9528) | Bit/dim 3.6785(3.6900) | Xent 0.6519(0.6817) | Loss 9.1287(9.3089) | Error 0.2311(0.2429) Steps 652(661.98) | Grad Norm 3.9601(8.7469) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.3516(16.8520) | Bit/dim 3.6531(3.6864) | Xent 0.6723(0.6790) | Loss 9.1038(9.2465) | Error 0.2489(0.2440) Steps 652(660.05) | Grad Norm 8.9718(8.3275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 88.1271, Epoch Time 1037.4131(936.4458), Bit/dim 3.6885(best: 3.6884), Xent 0.7245, Loss 4.0507, Error 0.2583(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 16.6647(16.8515) | Bit/dim 3.6753(3.6870) | Xent 0.6299(0.6802) | Loss 9.0207(9.7200) | Error 0.2144(0.2447) Steps 664(660.25) | Grad Norm 8.3868(8.3745) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 17.0559(16.8449) | Bit/dim 3.6793(3.6851) | Xent 0.6659(0.6747) | Loss 9.1431(9.5350) | Error 0.2600(0.2429) Steps 646(659.58) | Grad Norm 8.1260(7.9107) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 16.6304(16.8114) | Bit/dim 3.7059(3.6853) | Xent 0.7177(0.6749) | Loss 9.1614(9.4182) | Error 0.2378(0.2425) Steps 682(662.25) | Grad Norm 10.3864(8.1299) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_02_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_02_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0 --trust_coefficient 0.02 --clip True\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
