{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=1e-05, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_0001/best_nll_checkpt.pth', rtol=1e-05, save='experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_00001', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 42.3879(42.3879) | Bit/dim 1.0779(1.0779) | Xent 0.0109(0.0109) | Loss 1.0834(1.0834) | Error 0.0033(0.0033) Steps 554(554.00) | Grad Norm 2.8835(2.8835) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 16.6844(35.6985) | Bit/dim 1.0843(1.0779) | Xent 0.0068(0.0101) | Loss 1.0877(1.0830) | Error 0.0033(0.0031) Steps 554(554.10) | Grad Norm 2.1814(2.8278) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 17.1272(30.7794) | Bit/dim 1.0739(1.0795) | Xent 0.0070(0.0097) | Loss 1.0774(1.0843) | Error 0.0033(0.0029) Steps 554(554.23) | Grad Norm 1.5352(2.6067) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 16.8986(27.1175) | Bit/dim 1.0776(1.0789) | Xent 0.0044(0.0097) | Loss 1.0798(1.0838) | Error 0.0011(0.0029) Steps 554(553.70) | Grad Norm 0.4378(2.2398) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 16.7330(24.4444) | Bit/dim 1.0844(1.0803) | Xent 0.0111(0.0097) | Loss 1.0900(1.0851) | Error 0.0044(0.0031) Steps 554(553.64) | Grad Norm 1.3952(1.8878) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 16.7491(22.4461) | Bit/dim 1.0811(1.0809) | Xent 0.0064(0.0097) | Loss 1.0843(1.0857) | Error 0.0022(0.0030) Steps 548(553.80) | Grad Norm 0.9542(1.6172) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 16.8449(20.9749) | Bit/dim 1.0763(1.0816) | Xent 0.0100(0.0094) | Loss 1.0813(1.0863) | Error 0.0033(0.0029) Steps 554(554.06) | Grad Norm 0.6614(1.3835) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 66.9045, Epoch Time 1220.2601(1220.2601), Bit/dim 1.0754, Xent 0.0302, Loss 1.0905, Error 0.0191\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 16.8922(19.9106) | Bit/dim 1.0781(1.0822) | Xent 0.0098(0.0093) | Loss 1.0830(1.0868) | Error 0.0033(0.0030) Steps 554(554.36) | Grad Norm 0.9358(1.2092) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 16.8273(19.1004) | Bit/dim 1.0667(1.0805) | Xent 0.0066(0.0087) | Loss 1.0700(1.0849) | Error 0.0011(0.0028) Steps 548(553.67) | Grad Norm 0.6908(1.0869) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 17.0873(18.5479) | Bit/dim 1.0772(1.0810) | Xent 0.0074(0.0089) | Loss 1.0809(1.0855) | Error 0.0022(0.0028) Steps 548(552.78) | Grad Norm 0.4621(0.9861) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 16.8518(18.1380) | Bit/dim 1.0843(1.0811) | Xent 0.0159(0.0091) | Loss 1.0923(1.0857) | Error 0.0022(0.0027) Steps 548(552.70) | Grad Norm 0.6022(0.9013) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 16.9424(17.8183) | Bit/dim 1.0789(1.0820) | Xent 0.0194(0.0095) | Loss 1.0886(1.0867) | Error 0.0067(0.0028) Steps 554(552.30) | Grad Norm 1.1758(0.8550) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 16.8852(17.5643) | Bit/dim 1.0754(1.0815) | Xent 0.0043(0.0090) | Loss 1.0776(1.0860) | Error 0.0000(0.0026) Steps 548(552.88) | Grad Norm 0.5100(0.8261) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 16.7485(17.4028) | Bit/dim 1.0955(1.0819) | Xent 0.0046(0.0085) | Loss 1.0978(1.0861) | Error 0.0022(0.0026) Steps 554(553.51) | Grad Norm 0.4109(0.7857) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 68.0763, Epoch Time 1199.0418(1219.6235), Bit/dim 1.0752, Xent 0.0300, Loss 1.0903, Error 0.0187\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 16.8270(17.2543) | Bit/dim 1.0750(1.0819) | Xent 0.0076(0.0086) | Loss 1.0788(1.0862) | Error 0.0033(0.0027) Steps 560(553.69) | Grad Norm 0.5395(0.7758) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 16.5236(17.1349) | Bit/dim 1.0856(1.0818) | Xent 0.0075(0.0081) | Loss 1.0893(1.0858) | Error 0.0033(0.0025) Steps 554(554.04) | Grad Norm 0.5593(0.7311) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 16.7390(17.0469) | Bit/dim 1.0653(1.0816) | Xent 0.0113(0.0085) | Loss 1.0710(1.0859) | Error 0.0022(0.0026) Steps 554(553.37) | Grad Norm 0.6836(0.7304) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 16.7172(16.9861) | Bit/dim 1.0860(1.0815) | Xent 0.0098(0.0081) | Loss 1.0909(1.0855) | Error 0.0022(0.0024) Steps 548(553.33) | Grad Norm 0.6276(0.7089) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 16.8474(16.9572) | Bit/dim 1.0866(1.0813) | Xent 0.0074(0.0082) | Loss 1.0903(1.0854) | Error 0.0000(0.0024) Steps 548(553.54) | Grad Norm 1.3804(0.6998) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 16.6862(16.9546) | Bit/dim 1.0791(1.0811) | Xent 0.0174(0.0082) | Loss 1.0878(1.0852) | Error 0.0056(0.0024) Steps 548(553.50) | Grad Norm 1.1776(0.7058) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 67.2556, Epoch Time 1193.1133(1218.8282), Bit/dim 1.0753, Xent 0.0307, Loss 1.0907, Error 0.0199\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 16.5984(16.9167) | Bit/dim 1.0850(1.0811) | Xent 0.0079(0.0078) | Loss 1.0889(1.0850) | Error 0.0022(0.0022) Steps 554(553.13) | Grad Norm 0.6755(0.6935) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 16.4895(16.8975) | Bit/dim 1.0846(1.0815) | Xent 0.0076(0.0083) | Loss 1.0884(1.0856) | Error 0.0011(0.0026) Steps 548(552.87) | Grad Norm 0.8067(0.6934) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 16.8996(16.8803) | Bit/dim 1.0721(1.0809) | Xent 0.0055(0.0087) | Loss 1.0749(1.0852) | Error 0.0000(0.0027) Steps 554(552.67) | Grad Norm 0.4969(0.6895) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 16.7278(16.8845) | Bit/dim 1.0770(1.0812) | Xent 0.0102(0.0082) | Loss 1.0820(1.0852) | Error 0.0033(0.0025) Steps 554(553.05) | Grad Norm 0.5586(0.6928) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 16.7894(16.8718) | Bit/dim 1.0736(1.0806) | Xent 0.0082(0.0086) | Loss 1.0777(1.0849) | Error 0.0022(0.0028) Steps 554(553.42) | Grad Norm 0.7890(0.6972) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 17.0704(16.8586) | Bit/dim 1.0815(1.0807) | Xent 0.0074(0.0085) | Loss 1.0852(1.0849) | Error 0.0033(0.0026) Steps 566(553.48) | Grad Norm 0.9824(0.6976) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 16.9700(16.8348) | Bit/dim 1.0787(1.0804) | Xent 0.0113(0.0083) | Loss 1.0843(1.0846) | Error 0.0056(0.0026) Steps 554(553.89) | Grad Norm 0.5949(0.6892) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 67.2755, Epoch Time 1191.7981(1218.0173), Bit/dim 1.0756, Xent 0.0318, Loss 1.0915, Error 0.0183\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 16.7813(16.8222) | Bit/dim 1.0789(1.0808) | Xent 0.0072(0.0081) | Loss 1.0826(1.0848) | Error 0.0011(0.0026) Steps 554(553.44) | Grad Norm 0.4512(0.6671) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 16.7763(16.8290) | Bit/dim 1.0710(1.0804) | Xent 0.0101(0.0081) | Loss 1.0761(1.0844) | Error 0.0033(0.0025) Steps 560(553.12) | Grad Norm 0.5148(0.6607) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 16.8019(16.8318) | Bit/dim 1.0854(1.0808) | Xent 0.0178(0.0082) | Loss 1.0943(1.0848) | Error 0.0022(0.0025) Steps 548(553.50) | Grad Norm 0.5970(0.6446) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 16.9316(16.8257) | Bit/dim 1.0806(1.0809) | Xent 0.0224(0.0087) | Loss 1.0918(1.0852) | Error 0.0033(0.0027) Steps 554(553.66) | Grad Norm 0.6353(0.6230) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 16.7430(16.8307) | Bit/dim 1.0969(1.0810) | Xent 0.0065(0.0089) | Loss 1.1002(1.0854) | Error 0.0000(0.0029) Steps 548(553.60) | Grad Norm 0.6403(0.6483) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 16.9701(16.8452) | Bit/dim 1.0813(1.0815) | Xent 0.0102(0.0086) | Loss 1.0864(1.0858) | Error 0.0056(0.0030) Steps 554(554.05) | Grad Norm 0.9441(0.6720) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 67.4572, Epoch Time 1192.1167(1217.2403), Bit/dim 1.0761, Xent 0.0299, Loss 1.0910, Error 0.0189\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 16.7117(16.8368) | Bit/dim 1.0768(1.0809) | Xent 0.0134(0.0084) | Loss 1.0835(1.0851) | Error 0.0044(0.0028) Steps 554(553.24) | Grad Norm 1.0543(0.6734) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 17.1081(16.8572) | Bit/dim 1.0871(1.0812) | Xent 0.0087(0.0080) | Loss 1.0914(1.0852) | Error 0.0033(0.0026) Steps 560(554.21) | Grad Norm 0.5211(0.6596) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 16.9487(16.8507) | Bit/dim 1.0872(1.0822) | Xent 0.0052(0.0077) | Loss 1.0898(1.0861) | Error 0.0011(0.0025) Steps 554(554.13) | Grad Norm 0.6266(0.6946) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 16.7095(16.8590) | Bit/dim 1.0916(1.0828) | Xent 0.0172(0.0082) | Loss 1.1002(1.0869) | Error 0.0044(0.0025) Steps 548(553.30) | Grad Norm 0.9058(0.7129) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 16.4388(16.8533) | Bit/dim 1.0792(1.0830) | Xent 0.0100(0.0082) | Loss 1.0842(1.0871) | Error 0.0033(0.0026) Steps 548(553.76) | Grad Norm 1.2243(0.7484) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 16.7938(16.8535) | Bit/dim 1.0900(1.0823) | Xent 0.0097(0.0091) | Loss 1.0949(1.0869) | Error 0.0044(0.0028) Steps 566(553.53) | Grad Norm 0.6573(0.7621) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 16.6629(16.8319) | Bit/dim 1.0765(1.0812) | Xent 0.0064(0.0091) | Loss 1.0797(1.0858) | Error 0.0022(0.0029) Steps 548(552.82) | Grad Norm 1.4957(0.8483) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 67.4006, Epoch Time 1193.3305(1216.5230), Bit/dim 1.0755, Xent 0.0306, Loss 1.0908, Error 0.0186\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 16.5686(16.8259) | Bit/dim 1.0781(1.0796) | Xent 0.0062(0.0087) | Loss 1.0812(1.0840) | Error 0.0022(0.0026) Steps 548(552.46) | Grad Norm 0.6160(0.8481) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 17.1870(16.8602) | Bit/dim 1.0736(1.0798) | Xent 0.0067(0.0088) | Loss 1.0770(1.0842) | Error 0.0022(0.0026) Steps 560(552.77) | Grad Norm 0.9734(0.8282) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 16.7380(16.8328) | Bit/dim 1.0780(1.0803) | Xent 0.0080(0.0082) | Loss 1.0821(1.0844) | Error 0.0033(0.0024) Steps 560(553.37) | Grad Norm 0.8864(0.7911) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 16.8559(16.8095) | Bit/dim 1.0957(1.0813) | Xent 0.0063(0.0078) | Loss 1.0988(1.0852) | Error 0.0011(0.0024) Steps 554(553.39) | Grad Norm 0.7026(0.7669) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 16.6219(16.8093) | Bit/dim 1.0822(1.0809) | Xent 0.0093(0.0077) | Loss 1.0868(1.0848) | Error 0.0067(0.0026) Steps 554(552.92) | Grad Norm 0.7390(0.7963) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 16.8837(16.8219) | Bit/dim 1.1011(1.0813) | Xent 0.0081(0.0077) | Loss 1.1051(1.0851) | Error 0.0022(0.0025) Steps 554(552.92) | Grad Norm 0.6393(0.7823) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 16.9161(16.8189) | Bit/dim 1.0751(1.0813) | Xent 0.0071(0.0081) | Loss 1.0787(1.0853) | Error 0.0033(0.0027) Steps 560(552.92) | Grad Norm 0.8062(0.7526) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 67.9275, Epoch Time 1191.5684(1215.7743), Bit/dim 1.0754, Xent 0.0293, Loss 1.0901, Error 0.0186\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 16.6661(16.8063) | Bit/dim 1.0746(1.0807) | Xent 0.0082(0.0085) | Loss 1.0788(1.0849) | Error 0.0033(0.0030) Steps 548(552.56) | Grad Norm 0.5616(0.8012) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 17.1035(16.8290) | Bit/dim 1.0798(1.0808) | Xent 0.0078(0.0079) | Loss 1.0836(1.0848) | Error 0.0022(0.0027) Steps 554(552.79) | Grad Norm 0.4870(0.7674) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 16.5254(16.8573) | Bit/dim 1.0793(1.0818) | Xent 0.0104(0.0090) | Loss 1.0844(1.0863) | Error 0.0033(0.0029) Steps 554(553.71) | Grad Norm 1.3838(0.7720) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 16.7259(16.8443) | Bit/dim 1.0788(1.0819) | Xent 0.0074(0.0093) | Loss 1.0825(1.0866) | Error 0.0033(0.0029) Steps 548(553.44) | Grad Norm 0.8413(0.7824) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 16.7490(16.8299) | Bit/dim 1.0811(1.0820) | Xent 0.0051(0.0090) | Loss 1.0837(1.0865) | Error 0.0022(0.0029) Steps 548(552.79) | Grad Norm 0.9370(0.8146) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 16.7989(16.8407) | Bit/dim 1.0726(1.0818) | Xent 0.0077(0.0087) | Loss 1.0765(1.0862) | Error 0.0044(0.0029) Steps 554(553.45) | Grad Norm 1.1386(0.7892) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 67.0366, Epoch Time 1191.8750(1215.0574), Bit/dim 1.0757, Xent 0.0294, Loss 1.0904, Error 0.0184\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 16.5514(16.7993) | Bit/dim 1.0887(1.0808) | Xent 0.0040(0.0084) | Loss 1.0907(1.0850) | Error 0.0011(0.0025) Steps 560(553.81) | Grad Norm 1.0446(0.7846) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 16.7865(16.7831) | Bit/dim 1.0725(1.0809) | Xent 0.0104(0.0085) | Loss 1.0777(1.0851) | Error 0.0022(0.0026) Steps 548(552.91) | Grad Norm 0.7535(0.7797) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 16.9110(16.7903) | Bit/dim 1.0724(1.0807) | Xent 0.0073(0.0086) | Loss 1.0761(1.0850) | Error 0.0022(0.0025) Steps 548(552.53) | Grad Norm 0.7023(0.7408) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 16.9757(16.8185) | Bit/dim 1.0858(1.0798) | Xent 0.0152(0.0084) | Loss 1.0934(1.0840) | Error 0.0067(0.0027) Steps 548(552.62) | Grad Norm 1.9977(0.8036) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 16.7832(16.8119) | Bit/dim 1.0767(1.0795) | Xent 0.0033(0.0078) | Loss 1.0783(1.0835) | Error 0.0011(0.0026) Steps 548(552.81) | Grad Norm 0.3324(0.8077) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 16.5670(16.7958) | Bit/dim 1.0795(1.0794) | Xent 0.0120(0.0083) | Loss 1.0855(1.0836) | Error 0.0056(0.0029) Steps 554(553.31) | Grad Norm 1.5916(0.8654) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 16.6689(16.7904) | Bit/dim 1.0841(1.0816) | Xent 0.0085(0.0080) | Loss 1.0883(1.0856) | Error 0.0022(0.0027) Steps 554(553.92) | Grad Norm 0.8921(0.8397) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 67.6446, Epoch Time 1189.4919(1214.2904), Bit/dim 1.0757, Xent 0.0316, Loss 1.0915, Error 0.0198\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 16.8731(16.8127) | Bit/dim 1.0861(1.0822) | Xent 0.0060(0.0077) | Loss 1.0891(1.0861) | Error 0.0022(0.0026) Steps 560(553.63) | Grad Norm 0.6691(0.8710) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 16.8104(16.8284) | Bit/dim 1.0880(1.0819) | Xent 0.0101(0.0083) | Loss 1.0931(1.0860) | Error 0.0033(0.0027) Steps 554(553.73) | Grad Norm 0.7374(0.8251) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 16.8312(16.8330) | Bit/dim 1.0878(1.0812) | Xent 0.0065(0.0077) | Loss 1.0910(1.0851) | Error 0.0011(0.0023) Steps 548(553.79) | Grad Norm 0.7104(0.8119) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 16.7393(16.8600) | Bit/dim 1.0596(1.0800) | Xent 0.0103(0.0078) | Loss 1.0648(1.0839) | Error 0.0033(0.0025) Steps 560(553.57) | Grad Norm 0.7282(0.7853) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 16.9380(16.8552) | Bit/dim 1.0831(1.0821) | Xent 0.0087(0.0082) | Loss 1.0874(1.0862) | Error 0.0033(0.0027) Steps 554(553.66) | Grad Norm 1.1773(0.7938) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 16.7841(16.8491) | Bit/dim 1.0751(1.0814) | Xent 0.0074(0.0084) | Loss 1.0788(1.0856) | Error 0.0022(0.0028) Steps 554(553.77) | Grad Norm 0.6043(0.8273) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 67.6059, Epoch Time 1194.8881(1213.7083), Bit/dim 1.0749, Xent 0.0306, Loss 1.0902, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 17.0018(16.8742) | Bit/dim 1.0852(1.0811) | Xent 0.0081(0.0083) | Loss 1.0893(1.0852) | Error 0.0011(0.0028) Steps 554(553.66) | Grad Norm 0.7574(0.8208) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 17.1401(16.8832) | Bit/dim 1.0833(1.0818) | Xent 0.0020(0.0084) | Loss 1.0843(1.0860) | Error 0.0000(0.0027) Steps 554(553.24) | Grad Norm 0.6569(0.8792) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 16.9372(16.8962) | Bit/dim 1.0785(1.0818) | Xent 0.0059(0.0086) | Loss 1.0814(1.0861) | Error 0.0011(0.0027) Steps 554(553.59) | Grad Norm 0.9465(0.8699) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 16.9709(16.8789) | Bit/dim 1.0879(1.0804) | Xent 0.0096(0.0089) | Loss 1.0927(1.0849) | Error 0.0033(0.0030) Steps 554(553.20) | Grad Norm 0.9879(0.8775) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 16.6680(16.8903) | Bit/dim 1.0771(1.0811) | Xent 0.0053(0.0083) | Loss 1.0797(1.0852) | Error 0.0011(0.0027) Steps 554(553.40) | Grad Norm 0.6527(0.8357) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 16.8263(16.8880) | Bit/dim 1.0644(1.0803) | Xent 0.0068(0.0079) | Loss 1.0678(1.0842) | Error 0.0033(0.0027) Steps 560(553.27) | Grad Norm 1.6820(0.8404) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 16.9370(16.8825) | Bit/dim 1.0773(1.0803) | Xent 0.0039(0.0078) | Loss 1.0792(1.0842) | Error 0.0011(0.0024) Steps 548(552.99) | Grad Norm 0.8114(0.8683) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 67.8186, Epoch Time 1196.0632(1213.1790), Bit/dim 1.0758, Xent 0.0296, Loss 1.0906, Error 0.0183\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 16.7441(16.8807) | Bit/dim 1.0921(1.0817) | Xent 0.0054(0.0081) | Loss 1.0948(1.0858) | Error 0.0022(0.0025) Steps 554(553.71) | Grad Norm 0.5471(0.8826) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 16.9422(16.8614) | Bit/dim 1.0710(1.0814) | Xent 0.0050(0.0079) | Loss 1.0735(1.0854) | Error 0.0022(0.0025) Steps 560(553.62) | Grad Norm 1.1129(0.8684) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 16.9087(16.8890) | Bit/dim 1.0889(1.0820) | Xent 0.0070(0.0079) | Loss 1.0925(1.0859) | Error 0.0022(0.0026) Steps 548(554.46) | Grad Norm 0.7764(0.8846) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 16.8399(16.8712) | Bit/dim 1.0656(1.0808) | Xent 0.0082(0.0080) | Loss 1.0697(1.0848) | Error 0.0033(0.0026) Steps 548(553.74) | Grad Norm 0.5383(0.8343) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 16.6532(16.8753) | Bit/dim 1.0841(1.0803) | Xent 0.0068(0.0083) | Loss 1.0875(1.0844) | Error 0.0022(0.0027) Steps 560(553.67) | Grad Norm 0.9279(0.8228) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 17.1036(16.8774) | Bit/dim 1.0850(1.0805) | Xent 0.0060(0.0084) | Loss 1.0880(1.0847) | Error 0.0022(0.0027) Steps 548(553.87) | Grad Norm 0.5581(0.8011) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 16.7353(16.9579) | Bit/dim 1.0787(1.0810) | Xent 0.0080(0.0088) | Loss 1.0828(1.0854) | Error 0.0022(0.0028) Steps 554(554.10) | Grad Norm 0.7152(0.7866) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 67.9860, Epoch Time 1198.4556(1212.7373), Bit/dim 1.0754, Xent 0.0336, Loss 1.0922, Error 0.0189\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 17.0065(16.9574) | Bit/dim 1.0766(1.0814) | Xent 0.0051(0.0084) | Loss 1.0791(1.0856) | Error 0.0033(0.0027) Steps 554(553.75) | Grad Norm 0.9737(0.7597) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 16.7506(16.9321) | Bit/dim 1.0780(1.0810) | Xent 0.0039(0.0087) | Loss 1.0799(1.0854) | Error 0.0000(0.0029) Steps 554(553.64) | Grad Norm 0.5402(0.7562) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 16.8653(16.9571) | Bit/dim 1.0765(1.0800) | Xent 0.0095(0.0083) | Loss 1.0813(1.0842) | Error 0.0044(0.0028) Steps 554(554.23) | Grad Norm 0.7961(0.7604) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 17.1184(16.9277) | Bit/dim 1.0792(1.0805) | Xent 0.0127(0.0085) | Loss 1.0855(1.0848) | Error 0.0056(0.0029) Steps 554(554.27) | Grad Norm 0.7667(0.7666) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 16.9566(16.9699) | Bit/dim 1.0783(1.0801) | Xent 0.0191(0.0088) | Loss 1.0879(1.0845) | Error 0.0056(0.0030) Steps 554(553.75) | Grad Norm 1.6810(0.9002) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 17.0675(16.9828) | Bit/dim 1.0682(1.0815) | Xent 0.0065(0.0084) | Loss 1.0714(1.0857) | Error 0.0033(0.0028) Steps 554(554.70) | Grad Norm 0.7995(0.9589) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 67.5312, Epoch Time 1201.7066(1212.4064), Bit/dim 1.0753, Xent 0.0271, Loss 1.0889, Error 0.0179\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 16.9071(16.9773) | Bit/dim 1.0918(1.0818) | Xent 0.0070(0.0080) | Loss 1.0953(1.0858) | Error 0.0022(0.0026) Steps 548(554.94) | Grad Norm 0.7622(0.9336) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 16.8749(17.0192) | Bit/dim 1.0804(1.0808) | Xent 0.0139(0.0088) | Loss 1.0873(1.0852) | Error 0.0033(0.0028) Steps 548(554.18) | Grad Norm 1.5579(0.9641) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 16.9781(17.0060) | Bit/dim 1.0796(1.0810) | Xent 0.0045(0.0085) | Loss 1.0818(1.0852) | Error 0.0011(0.0027) Steps 566(554.67) | Grad Norm 0.9183(0.8881) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 16.8121(16.9674) | Bit/dim 1.0847(1.0810) | Xent 0.0155(0.0086) | Loss 1.0924(1.0853) | Error 0.0022(0.0026) Steps 548(554.30) | Grad Norm 1.6240(0.9307) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 17.0135(16.9871) | Bit/dim 1.0786(1.0810) | Xent 0.0087(0.0088) | Loss 1.0830(1.0854) | Error 0.0033(0.0028) Steps 548(554.36) | Grad Norm 0.7785(0.8984) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 16.7207(16.9619) | Bit/dim 1.0691(1.0794) | Xent 0.0113(0.0084) | Loss 1.0747(1.0836) | Error 0.0044(0.0027) Steps 560(553.98) | Grad Norm 1.0919(0.8759) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 16.8300(16.9450) | Bit/dim 1.0970(1.0805) | Xent 0.0028(0.0083) | Loss 1.0984(1.0847) | Error 0.0000(0.0026) Steps 554(553.96) | Grad Norm 0.4926(0.8297) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 68.1385, Epoch Time 1201.3726(1212.0753), Bit/dim 1.0750, Xent 0.0276, Loss 1.0888, Error 0.0174\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 16.8449(16.9581) | Bit/dim 1.0927(1.0817) | Xent 0.0077(0.0085) | Loss 1.0966(1.0859) | Error 0.0033(0.0027) Steps 554(554.40) | Grad Norm 0.6148(0.8359) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 16.9858(16.9263) | Bit/dim 1.0786(1.0819) | Xent 0.0086(0.0081) | Loss 1.0829(1.0860) | Error 0.0033(0.0027) Steps 554(554.48) | Grad Norm 0.9506(0.8614) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 16.7786(16.9409) | Bit/dim 1.0735(1.0816) | Xent 0.0053(0.0082) | Loss 1.0762(1.0857) | Error 0.0011(0.0028) Steps 560(554.35) | Grad Norm 0.7378(0.8474) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 16.9111(16.9557) | Bit/dim 1.0829(1.0800) | Xent 0.0074(0.0081) | Loss 1.0866(1.0840) | Error 0.0022(0.0026) Steps 554(554.48) | Grad Norm 1.4596(0.8758) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 16.9692(16.9369) | Bit/dim 1.0804(1.0806) | Xent 0.0079(0.0081) | Loss 1.0844(1.0846) | Error 0.0011(0.0024) Steps 548(554.20) | Grad Norm 0.8966(0.8628) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 16.9089(16.9364) | Bit/dim 1.0789(1.0799) | Xent 0.0073(0.0076) | Loss 1.0826(1.0838) | Error 0.0011(0.0023) Steps 554(553.84) | Grad Norm 0.4694(0.7861) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 67.9166, Epoch Time 1198.7418(1211.6753), Bit/dim 1.0758, Xent 0.0299, Loss 1.0908, Error 0.0186\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 16.8696(16.9139) | Bit/dim 1.0804(1.0812) | Xent 0.0072(0.0073) | Loss 1.0840(1.0848) | Error 0.0022(0.0021) Steps 548(553.69) | Grad Norm 0.6905(0.7694) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 17.0424(16.9353) | Bit/dim 1.0573(1.0797) | Xent 0.0102(0.0080) | Loss 1.0624(1.0838) | Error 0.0022(0.0025) Steps 554(554.58) | Grad Norm 0.5989(0.7991) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 16.9667(16.9457) | Bit/dim 1.0969(1.0808) | Xent 0.0081(0.0089) | Loss 1.1009(1.0852) | Error 0.0022(0.0025) Steps 554(554.87) | Grad Norm 1.7594(0.8953) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 17.1864(16.9807) | Bit/dim 1.0796(1.0807) | Xent 0.0049(0.0085) | Loss 1.0820(1.0849) | Error 0.0011(0.0026) Steps 554(554.81) | Grad Norm 0.5199(0.8701) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 16.6847(16.9585) | Bit/dim 1.0724(1.0807) | Xent 0.0095(0.0080) | Loss 1.0771(1.0847) | Error 0.0022(0.0023) Steps 560(555.13) | Grad Norm 0.7763(0.8198) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 16.8548(16.9711) | Bit/dim 1.0821(1.0814) | Xent 0.0022(0.0080) | Loss 1.0832(1.0854) | Error 0.0000(0.0023) Steps 548(554.79) | Grad Norm 0.9658(0.8587) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 17.0080(16.9539) | Bit/dim 1.0884(1.0816) | Xent 0.0188(0.0078) | Loss 1.0977(1.0855) | Error 0.0056(0.0023) Steps 554(555.03) | Grad Norm 1.1788(0.8307) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 67.2681, Epoch Time 1201.5462(1211.3715), Bit/dim 1.0754, Xent 0.0304, Loss 1.0906, Error 0.0183\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 16.9035(16.9542) | Bit/dim 1.0696(1.0807) | Xent 0.0076(0.0076) | Loss 1.0734(1.0845) | Error 0.0011(0.0022) Steps 554(555.13) | Grad Norm 1.3984(0.8998) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 16.6201(16.9557) | Bit/dim 1.0748(1.0805) | Xent 0.0073(0.0076) | Loss 1.0785(1.0843) | Error 0.0022(0.0021) Steps 560(554.86) | Grad Norm 0.6143(0.9452) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 16.7755(16.9318) | Bit/dim 1.0625(1.0803) | Xent 0.0102(0.0083) | Loss 1.0676(1.0845) | Error 0.0033(0.0024) Steps 554(554.49) | Grad Norm 0.8680(0.9156) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 16.7281(16.9445) | Bit/dim 1.0847(1.0805) | Xent 0.0050(0.0078) | Loss 1.0872(1.0844) | Error 0.0011(0.0022) Steps 554(554.07) | Grad Norm 0.4819(0.8402) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 16.9636(16.9565) | Bit/dim 1.0829(1.0816) | Xent 0.0040(0.0074) | Loss 1.0848(1.0853) | Error 0.0000(0.0022) Steps 554(554.05) | Grad Norm 0.6058(0.8239) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 16.9753(16.9676) | Bit/dim 1.0839(1.0812) | Xent 0.0054(0.0078) | Loss 1.0866(1.0851) | Error 0.0011(0.0026) Steps 560(554.06) | Grad Norm 0.5345(0.8363) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 16.9652(16.9882) | Bit/dim 1.0751(1.0807) | Xent 0.0072(0.0075) | Loss 1.0787(1.0845) | Error 0.0022(0.0025) Steps 548(553.45) | Grad Norm 1.4574(0.8611) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 67.4852, Epoch Time 1201.1941(1211.0661), Bit/dim 1.0754, Xent 0.0320, Loss 1.0914, Error 0.0181\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 17.1017(16.9609) | Bit/dim 1.0881(1.0813) | Xent 0.0075(0.0074) | Loss 1.0919(1.0850) | Error 0.0022(0.0025) Steps 548(553.45) | Grad Norm 0.4595(0.8333) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 16.5673(16.9398) | Bit/dim 1.0789(1.0806) | Xent 0.0032(0.0077) | Loss 1.0805(1.0845) | Error 0.0000(0.0024) Steps 554(553.90) | Grad Norm 0.7063(0.8487) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 17.1412(16.9861) | Bit/dim 1.0854(1.0817) | Xent 0.0047(0.0077) | Loss 1.0878(1.0856) | Error 0.0011(0.0024) Steps 560(553.92) | Grad Norm 0.3947(0.9248) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 16.8387(16.9560) | Bit/dim 1.0756(1.0811) | Xent 0.0087(0.0081) | Loss 1.0799(1.0852) | Error 0.0033(0.0028) Steps 554(554.28) | Grad Norm 1.2037(0.9071) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 16.7360(16.9458) | Bit/dim 1.0733(1.0805) | Xent 0.0048(0.0079) | Loss 1.0757(1.0844) | Error 0.0011(0.0027) Steps 554(553.77) | Grad Norm 1.2020(0.9142) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 17.0117(16.9461) | Bit/dim 1.0845(1.0804) | Xent 0.0070(0.0081) | Loss 1.0879(1.0844) | Error 0.0022(0.0028) Steps 554(553.87) | Grad Norm 0.9827(0.9217) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 66.8949, Epoch Time 1198.9604(1210.7030), Bit/dim 1.0754, Xent 0.0319, Loss 1.0913, Error 0.0193\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 17.0071(16.9650) | Bit/dim 1.0677(1.0801) | Xent 0.0064(0.0080) | Loss 1.0709(1.0841) | Error 0.0022(0.0027) Steps 554(552.81) | Grad Norm 0.8463(0.9359) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 16.9613(16.9487) | Bit/dim 1.0713(1.0810) | Xent 0.0043(0.0080) | Loss 1.0734(1.0850) | Error 0.0000(0.0026) Steps 554(553.13) | Grad Norm 0.7926(0.9542) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 17.1283(16.9473) | Bit/dim 1.0752(1.0791) | Xent 0.0108(0.0080) | Loss 1.0806(1.0831) | Error 0.0056(0.0027) Steps 560(553.79) | Grad Norm 0.9727(0.9787) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 16.8074(16.9333) | Bit/dim 1.0871(1.0796) | Xent 0.0095(0.0083) | Loss 1.0919(1.0837) | Error 0.0033(0.0028) Steps 548(553.84) | Grad Norm 1.7264(1.0616) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 16.8075(16.9438) | Bit/dim 1.0785(1.0804) | Xent 0.0060(0.0085) | Loss 1.0815(1.0847) | Error 0.0011(0.0027) Steps 554(553.37) | Grad Norm 0.5745(1.0200) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 17.2078(16.9293) | Bit/dim 1.0819(1.0811) | Xent 0.0064(0.0092) | Loss 1.0851(1.0857) | Error 0.0011(0.0028) Steps 560(554.45) | Grad Norm 1.3413(1.1614) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 16.9093(16.9713) | Bit/dim 1.0788(1.0807) | Xent 0.0125(0.0091) | Loss 1.0850(1.0852) | Error 0.0033(0.0026) Steps 548(553.39) | Grad Norm 0.7191(1.2257) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 67.5483, Epoch Time 1199.4864(1210.3665), Bit/dim 1.0748, Xent 0.0285, Loss 1.0891, Error 0.0179\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 16.6836(16.9553) | Bit/dim 1.0766(1.0811) | Xent 0.0063(0.0088) | Loss 1.0797(1.0855) | Error 0.0011(0.0026) Steps 548(553.67) | Grad Norm 1.5180(1.2888) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 17.0654(16.9596) | Bit/dim 1.1003(1.0824) | Xent 0.0055(0.0087) | Loss 1.1031(1.0868) | Error 0.0011(0.0027) Steps 548(553.42) | Grad Norm 0.9920(1.2579) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 16.9997(16.9685) | Bit/dim 1.0770(1.0821) | Xent 0.0067(0.0086) | Loss 1.0803(1.0864) | Error 0.0022(0.0027) Steps 560(553.63) | Grad Norm 0.8863(1.1778) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 17.1728(16.9521) | Bit/dim 1.0839(1.0818) | Xent 0.0044(0.0082) | Loss 1.0860(1.0859) | Error 0.0011(0.0026) Steps 554(553.62) | Grad Norm 0.5303(1.0642) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 16.4622(16.9264) | Bit/dim 1.0858(1.0810) | Xent 0.0051(0.0080) | Loss 1.0884(1.0850) | Error 0.0011(0.0027) Steps 560(553.78) | Grad Norm 0.5939(0.9598) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 16.8650(16.9085) | Bit/dim 1.0858(1.0809) | Xent 0.0059(0.0087) | Loss 1.0887(1.0853) | Error 0.0044(0.0029) Steps 554(553.97) | Grad Norm 0.5582(0.9373) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 67.5852, Epoch Time 1198.0062(1209.9957), Bit/dim 1.0751, Xent 0.0305, Loss 1.0903, Error 0.0191\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 16.8561(16.9039) | Bit/dim 1.0802(1.0801) | Xent 0.0149(0.0090) | Loss 1.0876(1.0846) | Error 0.0033(0.0029) Steps 554(553.55) | Grad Norm 2.2262(1.0617) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 16.9820(16.8953) | Bit/dim 1.0955(1.0799) | Xent 0.0084(0.0089) | Loss 1.0997(1.0843) | Error 0.0022(0.0029) Steps 554(553.64) | Grad Norm 0.5890(1.0597) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 16.9905(16.8990) | Bit/dim 1.0824(1.0805) | Xent 0.0061(0.0084) | Loss 1.0855(1.0847) | Error 0.0022(0.0026) Steps 548(553.99) | Grad Norm 0.5834(1.0075) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 16.9149(16.9114) | Bit/dim 1.0721(1.0812) | Xent 0.0076(0.0084) | Loss 1.0759(1.0854) | Error 0.0022(0.0025) Steps 554(554.69) | Grad Norm 0.9489(0.9514) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 16.9887(16.9166) | Bit/dim 1.0912(1.0810) | Xent 0.0048(0.0080) | Loss 1.0936(1.0850) | Error 0.0011(0.0023) Steps 554(554.37) | Grad Norm 0.5311(0.9297) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 16.7983(16.9015) | Bit/dim 1.0727(1.0797) | Xent 0.0057(0.0078) | Loss 1.0756(1.0836) | Error 0.0011(0.0023) Steps 554(553.80) | Grad Norm 1.0499(0.9207) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 16.9841(16.9067) | Bit/dim 1.0805(1.0803) | Xent 0.0198(0.0081) | Loss 1.0904(1.0844) | Error 0.0056(0.0024) Steps 560(554.38) | Grad Norm 0.8075(0.8902) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 67.7665, Epoch Time 1196.4970(1209.5907), Bit/dim 1.0751, Xent 0.0305, Loss 1.0904, Error 0.0181\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 17.0360(16.8861) | Bit/dim 1.0804(1.0803) | Xent 0.0062(0.0078) | Loss 1.0835(1.0842) | Error 0.0033(0.0024) Steps 548(554.40) | Grad Norm 1.2868(0.9813) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 16.9878(16.9074) | Bit/dim 1.0806(1.0803) | Xent 0.0057(0.0080) | Loss 1.0834(1.0843) | Error 0.0022(0.0024) Steps 554(554.64) | Grad Norm 1.3842(1.0611) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 16.9945(16.9123) | Bit/dim 1.0773(1.0804) | Xent 0.0092(0.0078) | Loss 1.0819(1.0843) | Error 0.0022(0.0025) Steps 560(555.77) | Grad Norm 0.8428(1.0684) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 16.7294(16.8892) | Bit/dim 1.0923(1.0818) | Xent 0.0082(0.0079) | Loss 1.0964(1.0857) | Error 0.0022(0.0024) Steps 548(555.29) | Grad Norm 1.1907(1.0754) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 16.8348(16.8859) | Bit/dim 1.0833(1.0813) | Xent 0.0078(0.0078) | Loss 1.0872(1.0852) | Error 0.0033(0.0024) Steps 554(555.41) | Grad Norm 0.9691(1.0685) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 17.0670(16.8741) | Bit/dim 1.0770(1.0808) | Xent 0.0050(0.0080) | Loss 1.0795(1.0848) | Error 0.0011(0.0024) Steps 554(555.37) | Grad Norm 1.9043(1.1372) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 16.8454(16.8509) | Bit/dim 1.0851(1.0802) | Xent 0.0160(0.0081) | Loss 1.0931(1.0843) | Error 0.0056(0.0025) Steps 548(555.29) | Grad Norm 1.2349(1.1287) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 67.8854, Epoch Time 1195.1407(1209.1572), Bit/dim 1.0748, Xent 0.0292, Loss 1.0893, Error 0.0184\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 16.7424(16.8609) | Bit/dim 1.0796(1.0801) | Xent 0.0104(0.0081) | Loss 1.0848(1.0842) | Error 0.0022(0.0023) Steps 554(554.81) | Grad Norm 1.6575(1.1170) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 16.8254(16.8533) | Bit/dim 1.0589(1.0795) | Xent 0.0050(0.0082) | Loss 1.0614(1.0835) | Error 0.0011(0.0023) Steps 548(554.41) | Grad Norm 1.0585(1.1597) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 16.8134(16.8764) | Bit/dim 1.0955(1.0806) | Xent 0.0058(0.0082) | Loss 1.0985(1.0847) | Error 0.0011(0.0024) Steps 566(554.83) | Grad Norm 0.4435(1.0942) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 17.1147(16.9036) | Bit/dim 1.0772(1.0809) | Xent 0.0062(0.0085) | Loss 1.0802(1.0851) | Error 0.0011(0.0025) Steps 560(554.48) | Grad Norm 1.1861(1.1343) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 16.6911(16.8897) | Bit/dim 1.0811(1.0807) | Xent 0.0025(0.0079) | Loss 1.0824(1.0846) | Error 0.0000(0.0024) Steps 554(554.35) | Grad Norm 0.5433(1.0335) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 16.9721(16.9008) | Bit/dim 1.0750(1.0803) | Xent 0.0081(0.0085) | Loss 1.0790(1.0845) | Error 0.0044(0.0028) Steps 554(554.65) | Grad Norm 1.2762(1.0393) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 67.5517, Epoch Time 1197.9632(1208.8214), Bit/dim 1.0748, Xent 0.0309, Loss 1.0902, Error 0.0194\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 16.9834(16.9260) | Bit/dim 1.0725(1.0805) | Xent 0.0049(0.0085) | Loss 1.0750(1.0848) | Error 0.0011(0.0027) Steps 560(555.22) | Grad Norm 0.7108(1.0168) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 17.1100(16.9327) | Bit/dim 1.0648(1.0801) | Xent 0.0154(0.0083) | Loss 1.0725(1.0843) | Error 0.0044(0.0027) Steps 560(555.30) | Grad Norm 0.6247(0.9583) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 16.5652(16.8969) | Bit/dim 1.0899(1.0812) | Xent 0.0048(0.0082) | Loss 1.0922(1.0853) | Error 0.0011(0.0027) Steps 548(554.56) | Grad Norm 1.1163(0.9713) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 16.8354(16.8746) | Bit/dim 1.0848(1.0814) | Xent 0.0129(0.0087) | Loss 1.0912(1.0857) | Error 0.0044(0.0029) Steps 554(554.75) | Grad Norm 1.0477(1.1341) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 16.9669(16.8800) | Bit/dim 1.0902(1.0808) | Xent 0.0039(0.0083) | Loss 1.0921(1.0849) | Error 0.0011(0.0027) Steps 560(555.02) | Grad Norm 0.5857(1.1043) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 16.8903(16.8799) | Bit/dim 1.0921(1.0802) | Xent 0.0069(0.0083) | Loss 1.0955(1.0844) | Error 0.0022(0.0027) Steps 554(554.40) | Grad Norm 0.8267(1.1139) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 16.8332(16.8811) | Bit/dim 1.0829(1.0808) | Xent 0.0025(0.0084) | Loss 1.0842(1.0850) | Error 0.0000(0.0028) Steps 554(554.25) | Grad Norm 0.3683(1.0889) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 67.7176, Epoch Time 1195.0586(1208.4085), Bit/dim 1.0746, Xent 0.0280, Loss 1.0886, Error 0.0179\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 16.7274(16.8774) | Bit/dim 1.0803(1.0813) | Xent 0.0162(0.0088) | Loss 1.0884(1.0857) | Error 0.0022(0.0029) Steps 554(553.87) | Grad Norm 0.6962(1.1397) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 16.8969(16.8316) | Bit/dim 1.0839(1.0804) | Xent 0.0066(0.0087) | Loss 1.0872(1.0847) | Error 0.0044(0.0029) Steps 554(553.81) | Grad Norm 1.5951(1.1385) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 16.8788(16.8386) | Bit/dim 1.0943(1.0814) | Xent 0.0074(0.0088) | Loss 1.0980(1.0858) | Error 0.0011(0.0029) Steps 560(553.58) | Grad Norm 1.4894(1.1466) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 16.6782(16.8298) | Bit/dim 1.0822(1.0812) | Xent 0.0036(0.0084) | Loss 1.0840(1.0854) | Error 0.0000(0.0026) Steps 554(554.47) | Grad Norm 1.4994(1.1001) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 16.8665(16.8341) | Bit/dim 1.0690(1.0783) | Xent 0.0090(0.0079) | Loss 1.0735(1.0823) | Error 0.0011(0.0025) Steps 560(554.43) | Grad Norm 1.1779(1.1821) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 17.0163(16.8536) | Bit/dim 1.0793(1.0799) | Xent 0.0332(0.0081) | Loss 1.0959(1.0839) | Error 0.0067(0.0025) Steps 554(554.74) | Grad Norm 0.8844(1.1496) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 67.2353, Epoch Time 1191.9529(1207.9148), Bit/dim 1.0751, Xent 0.0285, Loss 1.0893, Error 0.0187\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 16.8372(16.8605) | Bit/dim 1.0684(1.0799) | Xent 0.0062(0.0086) | Loss 1.0715(1.0842) | Error 0.0022(0.0027) Steps 554(554.72) | Grad Norm 0.5836(1.1395) | Total Time 10.00(10.00)\n",
      "Iter 1660 | Time 16.8762(16.8587) | Bit/dim 1.0920(1.0806) | Xent 0.0061(0.0082) | Loss 1.0951(1.0847) | Error 0.0033(0.0025) Steps 560(554.68) | Grad Norm 1.0344(1.0534) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 16.7979(16.8616) | Bit/dim 1.0716(1.0796) | Xent 0.0128(0.0079) | Loss 1.0780(1.0835) | Error 0.0033(0.0024) Steps 548(554.02) | Grad Norm 0.8978(1.0065) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 17.0089(16.8581) | Bit/dim 1.0955(1.0795) | Xent 0.0101(0.0079) | Loss 1.1006(1.0834) | Error 0.0033(0.0024) Steps 554(553.88) | Grad Norm 1.3630(0.9952) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 16.9482(16.8708) | Bit/dim 1.0802(1.0801) | Xent 0.0040(0.0078) | Loss 1.0822(1.0840) | Error 0.0011(0.0022) Steps 560(553.64) | Grad Norm 0.5000(0.9616) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 16.8096(16.8684) | Bit/dim 1.0764(1.0804) | Xent 0.0068(0.0076) | Loss 1.0798(1.0842) | Error 0.0022(0.0022) Steps 554(554.36) | Grad Norm 1.8459(0.9995) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 16.8786(16.8546) | Bit/dim 1.0836(1.0801) | Xent 0.0099(0.0080) | Loss 1.0885(1.0841) | Error 0.0011(0.0024) Steps 560(554.95) | Grad Norm 1.0709(1.0998) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 67.4319, Epoch Time 1193.6817(1207.4878), Bit/dim 1.0746, Xent 0.0294, Loss 1.0893, Error 0.0188\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 16.8396(16.8517) | Bit/dim 1.0838(1.0808) | Xent 0.0095(0.0082) | Loss 1.0886(1.0849) | Error 0.0033(0.0024) Steps 554(554.22) | Grad Norm 0.5017(1.0920) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 17.0826(16.8568) | Bit/dim 1.0951(1.0802) | Xent 0.0062(0.0081) | Loss 1.0982(1.0842) | Error 0.0022(0.0025) Steps 548(553.66) | Grad Norm 1.1352(1.1273) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 16.8278(16.8652) | Bit/dim 1.0776(1.0805) | Xent 0.0100(0.0082) | Loss 1.0826(1.0846) | Error 0.0022(0.0026) Steps 560(553.77) | Grad Norm 0.4958(1.1530) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 17.0655(16.8878) | Bit/dim 1.0819(1.0804) | Xent 0.0211(0.0085) | Loss 1.0924(1.0847) | Error 0.0044(0.0026) Steps 554(553.37) | Grad Norm 1.2419(1.1265) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 16.8247(16.8842) | Bit/dim 1.0788(1.0808) | Xent 0.0072(0.0084) | Loss 1.0824(1.0850) | Error 0.0022(0.0028) Steps 548(553.35) | Grad Norm 1.0410(1.0945) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 16.8476(16.8846) | Bit/dim 1.0897(1.0801) | Xent 0.0101(0.0080) | Loss 1.0948(1.0841) | Error 0.0033(0.0025) Steps 554(553.65) | Grad Norm 1.2642(1.1258) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 16.8967(16.8961) | Bit/dim 1.0848(1.0807) | Xent 0.0056(0.0085) | Loss 1.0876(1.0850) | Error 0.0011(0.0027) Steps 560(553.48) | Grad Norm 1.5947(1.1706) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 67.4985, Epoch Time 1196.1717(1207.1484), Bit/dim 1.0749, Xent 0.0306, Loss 1.0902, Error 0.0189\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 16.6692(16.8642) | Bit/dim 1.0759(1.0796) | Xent 0.0112(0.0085) | Loss 1.0815(1.0839) | Error 0.0056(0.0028) Steps 554(553.75) | Grad Norm 1.4006(1.1733) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 16.9641(16.8704) | Bit/dim 1.0915(1.0799) | Xent 0.0099(0.0087) | Loss 1.0964(1.0842) | Error 0.0033(0.0027) Steps 554(553.66) | Grad Norm 1.1611(1.1814) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 16.9354(16.8790) | Bit/dim 1.0752(1.0788) | Xent 0.0052(0.0085) | Loss 1.0778(1.0830) | Error 0.0000(0.0028) Steps 554(554.02) | Grad Norm 1.3472(1.2261) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 16.7470(16.8857) | Bit/dim 1.0811(1.0801) | Xent 0.0034(0.0083) | Loss 1.0828(1.0842) | Error 0.0011(0.0026) Steps 548(553.55) | Grad Norm 0.5244(1.2445) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 16.9518(16.9106) | Bit/dim 1.1005(1.0806) | Xent 0.0032(0.0077) | Loss 1.1021(1.0845) | Error 0.0000(0.0024) Steps 554(553.18) | Grad Norm 0.3519(1.1154) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 16.8341(16.9308) | Bit/dim 1.0903(1.0810) | Xent 0.0054(0.0075) | Loss 1.0930(1.0847) | Error 0.0033(0.0024) Steps 554(553.82) | Grad Norm 0.5857(1.0343) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 67.2850, Epoch Time 1195.9834(1206.8134), Bit/dim 1.0745, Xent 0.0271, Loss 1.0881, Error 0.0182\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 17.0734(16.9132) | Bit/dim 1.0803(1.0810) | Xent 0.0067(0.0073) | Loss 1.0837(1.0846) | Error 0.0011(0.0023) Steps 554(553.71) | Grad Norm 0.6343(1.0441) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 16.7903(16.8782) | Bit/dim 1.0797(1.0811) | Xent 0.0110(0.0073) | Loss 1.0852(1.0848) | Error 0.0033(0.0023) Steps 548(553.62) | Grad Norm 1.3936(1.0401) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 16.8483(16.8844) | Bit/dim 1.0839(1.0805) | Xent 0.0066(0.0071) | Loss 1.0872(1.0840) | Error 0.0022(0.0023) Steps 554(553.74) | Grad Norm 0.8653(0.9671) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 16.6441(16.8677) | Bit/dim 1.0730(1.0800) | Xent 0.0053(0.0076) | Loss 1.0756(1.0838) | Error 0.0011(0.0025) Steps 554(553.50) | Grad Norm 1.4561(1.0468) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 17.2451(16.8771) | Bit/dim 1.0823(1.0796) | Xent 0.0140(0.0079) | Loss 1.0893(1.0836) | Error 0.0056(0.0026) Steps 548(553.49) | Grad Norm 0.7462(0.9818) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 16.8627(16.8651) | Bit/dim 1.0848(1.0795) | Xent 0.0104(0.0079) | Loss 1.0900(1.0835) | Error 0.0022(0.0025) Steps 548(553.32) | Grad Norm 0.8557(0.9392) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 16.8133(16.8725) | Bit/dim 1.0713(1.0808) | Xent 0.0147(0.0085) | Loss 1.0786(1.0850) | Error 0.0033(0.0026) Steps 554(553.47) | Grad Norm 1.4087(1.0229) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 67.4687, Epoch Time 1194.4455(1206.4424), Bit/dim 1.0752, Xent 0.0362, Loss 1.0933, Error 0.0193\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 16.7678(16.8629) | Bit/dim 1.0831(1.0809) | Xent 0.0058(0.0080) | Loss 1.0860(1.0849) | Error 0.0033(0.0025) Steps 554(553.44) | Grad Norm 0.9924(1.0486) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 16.8907(16.8654) | Bit/dim 1.0817(1.0801) | Xent 0.0031(0.0086) | Loss 1.0832(1.0844) | Error 0.0011(0.0027) Steps 554(553.77) | Grad Norm 2.0205(1.0655) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 16.6931(16.8374) | Bit/dim 1.0861(1.0792) | Xent 0.0065(0.0081) | Loss 1.0894(1.0833) | Error 0.0011(0.0024) Steps 560(554.28) | Grad Norm 0.9310(1.0575) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 16.7354(16.8149) | Bit/dim 1.0821(1.0793) | Xent 0.0084(0.0081) | Loss 1.0863(1.0834) | Error 0.0011(0.0025) Steps 554(554.49) | Grad Norm 0.5938(1.0327) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 16.8740(16.8186) | Bit/dim 1.0708(1.0796) | Xent 0.0078(0.0078) | Loss 1.0747(1.0835) | Error 0.0022(0.0022) Steps 554(553.87) | Grad Norm 1.9331(1.0378) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 16.7927(16.8234) | Bit/dim 1.0747(1.0803) | Xent 0.0043(0.0082) | Loss 1.0769(1.0843) | Error 0.0022(0.0024) Steps 554(554.26) | Grad Norm 2.2802(1.1076) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 67.4300, Epoch Time 1190.4946(1205.9639), Bit/dim 1.0741, Xent 0.0295, Loss 1.0888, Error 0.0186\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1980 | Time 16.5327(16.8113) | Bit/dim 1.0710(1.0803) | Xent 0.0063(0.0083) | Loss 1.0742(1.0845) | Error 0.0022(0.0025) Steps 554(554.20) | Grad Norm 0.8932(1.0736) | Total Time 10.00(10.00)\n",
      "Iter 1990 | Time 16.8648(16.8201) | Bit/dim 1.0850(1.0811) | Xent 0.0052(0.0081) | Loss 1.0876(1.0851) | Error 0.0011(0.0025) Steps 548(554.29) | Grad Norm 0.5403(1.0097) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 16.9986(16.8100) | Bit/dim 1.0827(1.0809) | Xent 0.0052(0.0082) | Loss 1.0853(1.0850) | Error 0.0000(0.0026) Steps 566(554.11) | Grad Norm 0.4430(0.9784) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 16.7496(16.7987) | Bit/dim 1.0813(1.0809) | Xent 0.0080(0.0081) | Loss 1.0853(1.0849) | Error 0.0033(0.0025) Steps 560(555.02) | Grad Norm 1.5680(0.9936) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 17.0354(16.8044) | Bit/dim 1.0750(1.0797) | Xent 0.0120(0.0081) | Loss 1.0811(1.0838) | Error 0.0033(0.0024) Steps 554(554.27) | Grad Norm 1.8938(1.0869) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 16.8617(16.7962) | Bit/dim 1.0856(1.0798) | Xent 0.0041(0.0078) | Loss 1.0876(1.0837) | Error 0.0000(0.0025) Steps 554(554.32) | Grad Norm 1.1845(1.1494) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 16.5312(16.8123) | Bit/dim 1.0734(1.0798) | Xent 0.0070(0.0078) | Loss 1.0769(1.0837) | Error 0.0033(0.0027) Steps 554(554.39) | Grad Norm 2.0859(1.3538) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 67.4146, Epoch Time 1190.1420(1205.4893), Bit/dim 1.0742, Xent 0.0307, Loss 1.0895, Error 0.0179\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 16.9189(16.8261) | Bit/dim 1.0813(1.0803) | Xent 0.0095(0.0081) | Loss 1.0861(1.0843) | Error 0.0033(0.0027) Steps 554(554.45) | Grad Norm 0.6921(1.4488) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 16.8082(16.8175) | Bit/dim 1.0825(1.0800) | Xent 0.0067(0.0083) | Loss 1.0859(1.0842) | Error 0.0033(0.0027) Steps 548(553.98) | Grad Norm 1.5269(1.5326) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 17.1016(16.8450) | Bit/dim 1.0760(1.0800) | Xent 0.0056(0.0079) | Loss 1.0788(1.0840) | Error 0.0011(0.0027) Steps 560(554.61) | Grad Norm 1.0033(1.5593) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 16.8966(16.8241) | Bit/dim 1.0741(1.0803) | Xent 0.0046(0.0080) | Loss 1.0765(1.0843) | Error 0.0000(0.0028) Steps 566(554.79) | Grad Norm 0.7813(1.4654) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 17.0608(16.8617) | Bit/dim 1.0774(1.0811) | Xent 0.0060(0.0079) | Loss 1.0803(1.0850) | Error 0.0033(0.0028) Steps 548(554.74) | Grad Norm 0.7251(1.4232) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 16.8152(16.8589) | Bit/dim 1.0686(1.0801) | Xent 0.0050(0.0074) | Loss 1.0711(1.0838) | Error 0.0011(0.0026) Steps 548(554.28) | Grad Norm 0.7110(1.2963) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 16.6870(16.8698) | Bit/dim 1.0863(1.0796) | Xent 0.0092(0.0081) | Loss 1.0909(1.0836) | Error 0.0033(0.0026) Steps 548(554.30) | Grad Norm 1.9843(1.2692) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 67.6038, Epoch Time 1194.7308(1205.1665), Bit/dim 1.0745, Xent 0.0273, Loss 1.0882, Error 0.0178\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 16.8152(16.8444) | Bit/dim 1.0819(1.0791) | Xent 0.0127(0.0085) | Loss 1.0883(1.0833) | Error 0.0033(0.0026) Steps 554(554.21) | Grad Norm 1.7236(1.2030) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 16.6611(16.8322) | Bit/dim 1.0706(1.0793) | Xent 0.0100(0.0081) | Loss 1.0756(1.0834) | Error 0.0033(0.0026) Steps 560(554.05) | Grad Norm 2.3972(1.1858) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 16.8514(16.8119) | Bit/dim 1.0673(1.0783) | Xent 0.0059(0.0075) | Loss 1.0702(1.0821) | Error 0.0022(0.0024) Steps 560(554.68) | Grad Norm 0.8355(1.1321) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 16.9474(16.8059) | Bit/dim 1.0743(1.0787) | Xent 0.0248(0.0080) | Loss 1.0867(1.0827) | Error 0.0056(0.0026) Steps 554(554.50) | Grad Norm 1.1058(1.0588) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 16.7007(16.7990) | Bit/dim 1.0830(1.0796) | Xent 0.0074(0.0079) | Loss 1.0867(1.0835) | Error 0.0044(0.0026) Steps 554(554.05) | Grad Norm 1.0710(1.0501) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 17.0823(16.8033) | Bit/dim 1.0907(1.0807) | Xent 0.0080(0.0078) | Loss 1.0947(1.0846) | Error 0.0022(0.0026) Steps 560(554.20) | Grad Norm 0.4802(0.9744) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 67.8284, Epoch Time 1188.8005(1204.6755), Bit/dim 1.0741, Xent 0.0320, Loss 1.0901, Error 0.0192\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 17.0622(16.7954) | Bit/dim 1.0762(1.0802) | Xent 0.0159(0.0080) | Loss 1.0841(1.0842) | Error 0.0067(0.0026) Steps 560(554.35) | Grad Norm 1.0339(0.9591) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 16.6055(16.7634) | Bit/dim 1.0710(1.0804) | Xent 0.0060(0.0080) | Loss 1.0740(1.0844) | Error 0.0022(0.0028) Steps 560(554.24) | Grad Norm 0.6896(0.9474) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 16.8113(16.7924) | Bit/dim 1.0776(1.0793) | Xent 0.0036(0.0080) | Loss 1.0794(1.0833) | Error 0.0011(0.0027) Steps 560(553.92) | Grad Norm 0.4548(0.9168) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 16.8159(16.8331) | Bit/dim 1.0842(1.0795) | Xent 0.0046(0.0079) | Loss 1.0865(1.0834) | Error 0.0011(0.0026) Steps 548(553.92) | Grad Norm 0.7048(0.9470) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 16.5933(16.8184) | Bit/dim 1.0785(1.0791) | Xent 0.0084(0.0077) | Loss 1.0827(1.0829) | Error 0.0011(0.0026) Steps 554(554.60) | Grad Norm 0.7282(0.9543) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 16.7937(16.8241) | Bit/dim 1.0613(1.0788) | Xent 0.0105(0.0078) | Loss 1.0665(1.0827) | Error 0.0033(0.0027) Steps 554(554.38) | Grad Norm 0.6814(0.9508) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 16.7331(16.8244) | Bit/dim 1.0814(1.0804) | Xent 0.0083(0.0080) | Loss 1.0855(1.0844) | Error 0.0022(0.0028) Steps 554(554.32) | Grad Norm 1.0146(1.0216) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 67.3751, Epoch Time 1191.4913(1204.2800), Bit/dim 1.0743, Xent 0.0309, Loss 1.0897, Error 0.0188\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 16.7059(16.7952) | Bit/dim 1.0961(1.0813) | Xent 0.0153(0.0080) | Loss 1.1037(1.0853) | Error 0.0056(0.0027) Steps 554(554.26) | Grad Norm 0.6791(0.9914) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 16.5832(16.7945) | Bit/dim 1.0858(1.0809) | Xent 0.0087(0.0086) | Loss 1.0902(1.0852) | Error 0.0033(0.0029) Steps 548(554.13) | Grad Norm 0.9244(0.9741) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 16.8891(16.7739) | Bit/dim 1.0798(1.0809) | Xent 0.0065(0.0081) | Loss 1.0830(1.0849) | Error 0.0011(0.0026) Steps 560(554.73) | Grad Norm 1.9435(1.0423) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 16.8326(16.7847) | Bit/dim 1.0676(1.0801) | Xent 0.0039(0.0083) | Loss 1.0696(1.0842) | Error 0.0011(0.0027) Steps 560(554.09) | Grad Norm 0.8124(1.0448) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 16.8661(16.8102) | Bit/dim 1.0681(1.0804) | Xent 0.0135(0.0084) | Loss 1.0749(1.0846) | Error 0.0056(0.0028) Steps 554(553.60) | Grad Norm 1.7801(1.1078) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 16.7394(16.8010) | Bit/dim 1.0820(1.0798) | Xent 0.0044(0.0084) | Loss 1.0841(1.0840) | Error 0.0011(0.0027) Steps 560(554.24) | Grad Norm 0.9478(1.1213) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 68.0428, Epoch Time 1189.4783(1203.8360), Bit/dim 1.0747, Xent 0.0289, Loss 1.0892, Error 0.0185\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2310 | Time 16.7294(16.7957) | Bit/dim 1.0768(1.0793) | Xent 0.0052(0.0081) | Loss 1.0794(1.0833) | Error 0.0011(0.0027) Steps 554(554.32) | Grad Norm 0.6844(1.0309) | Total Time 10.00(10.00)\n",
      "Iter 2320 | Time 16.9620(16.7981) | Bit/dim 1.0714(1.0791) | Xent 0.0114(0.0085) | Loss 1.0771(1.0833) | Error 0.0044(0.0028) Steps 560(554.69) | Grad Norm 1.7560(1.1859) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 16.8264(16.7981) | Bit/dim 1.0802(1.0786) | Xent 0.0084(0.0084) | Loss 1.0844(1.0828) | Error 0.0033(0.0027) Steps 554(554.36) | Grad Norm 1.8588(1.2630) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 16.7291(16.7955) | Bit/dim 1.0816(1.0786) | Xent 0.0161(0.0079) | Loss 1.0896(1.0825) | Error 0.0056(0.0025) Steps 554(553.95) | Grad Norm 0.7476(1.2043) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 16.8369(16.7918) | Bit/dim 1.0827(1.0797) | Xent 0.0057(0.0084) | Loss 1.0855(1.0839) | Error 0.0022(0.0026) Steps 560(554.14) | Grad Norm 0.8693(1.1546) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 17.1910(16.8519) | Bit/dim 1.0741(1.0802) | Xent 0.0064(0.0082) | Loss 1.0774(1.0843) | Error 0.0022(0.0025) Steps 554(554.74) | Grad Norm 1.2536(1.1280) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 16.6721(16.8350) | Bit/dim 1.0847(1.0804) | Xent 0.0043(0.0079) | Loss 1.0869(1.0844) | Error 0.0011(0.0025) Steps 554(554.25) | Grad Norm 0.5519(1.1048) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 67.6037, Epoch Time 1191.9697(1203.4800), Bit/dim 1.0735, Xent 0.0299, Loss 1.0884, Error 0.0182\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 16.8987(16.8432) | Bit/dim 1.0614(1.0794) | Xent 0.0057(0.0079) | Loss 1.0642(1.0833) | Error 0.0011(0.0024) Steps 554(554.46) | Grad Norm 0.8875(1.1460) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 16.9705(16.8567) | Bit/dim 1.0659(1.0795) | Xent 0.0106(0.0079) | Loss 1.0712(1.0835) | Error 0.0056(0.0024) Steps 554(554.21) | Grad Norm 2.8079(1.3512) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 16.8466(16.8364) | Bit/dim 1.0988(1.0793) | Xent 0.0059(0.0080) | Loss 1.1018(1.0833) | Error 0.0022(0.0024) Steps 560(553.89) | Grad Norm 1.6556(1.4497) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 16.6778(16.8259) | Bit/dim 1.0750(1.0797) | Xent 0.0094(0.0079) | Loss 1.0797(1.0837) | Error 0.0044(0.0024) Steps 560(554.27) | Grad Norm 1.0008(1.4221) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 16.7704(16.8258) | Bit/dim 1.0650(1.0793) | Xent 0.0094(0.0080) | Loss 1.0696(1.0833) | Error 0.0033(0.0024) Steps 554(553.73) | Grad Norm 1.3611(1.3704) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 16.7493(16.8129) | Bit/dim 1.0786(1.0799) | Xent 0.0113(0.0080) | Loss 1.0843(1.0839) | Error 0.0044(0.0025) Steps 554(554.00) | Grad Norm 1.1385(1.3584) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 16.7529(16.8088) | Bit/dim 1.0836(1.0804) | Xent 0.0100(0.0082) | Loss 1.0886(1.0845) | Error 0.0033(0.0026) Steps 554(554.79) | Grad Norm 1.0107(1.3008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 67.5796, Epoch Time 1191.1902(1203.1113), Bit/dim 1.0740, Xent 0.0305, Loss 1.0892, Error 0.0183\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 16.9470(16.7957) | Bit/dim 1.0853(1.0803) | Xent 0.0066(0.0087) | Loss 1.0886(1.0847) | Error 0.0022(0.0029) Steps 554(554.89) | Grad Norm 0.5359(1.2526) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 16.9296(16.7949) | Bit/dim 1.0731(1.0798) | Xent 0.0144(0.0090) | Loss 1.0803(1.0843) | Error 0.0044(0.0029) Steps 554(555.02) | Grad Norm 1.3826(1.1775) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 16.8462(16.8052) | Bit/dim 1.0750(1.0799) | Xent 0.0191(0.0088) | Loss 1.0845(1.0843) | Error 0.0044(0.0029) Steps 560(555.23) | Grad Norm 1.5068(1.1434) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 16.9055(16.8372) | Bit/dim 1.0755(1.0796) | Xent 0.0059(0.0087) | Loss 1.0785(1.0839) | Error 0.0022(0.0028) Steps 554(554.86) | Grad Norm 0.5611(1.0950) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 16.5187(16.8065) | Bit/dim 1.0898(1.0797) | Xent 0.0054(0.0083) | Loss 1.0925(1.0839) | Error 0.0022(0.0027) Steps 554(555.12) | Grad Norm 0.5596(1.1287) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 16.7292(16.8056) | Bit/dim 1.0806(1.0798) | Xent 0.0051(0.0080) | Loss 1.0831(1.0838) | Error 0.0011(0.0025) Steps 560(555.05) | Grad Norm 0.9107(1.1177) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 67.8877, Epoch Time 1190.6332(1202.7369), Bit/dim 1.0739, Xent 0.0312, Loss 1.0895, Error 0.0197\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 16.6486(16.7913) | Bit/dim 1.0716(1.0795) | Xent 0.0057(0.0077) | Loss 1.0744(1.0833) | Error 0.0022(0.0025) Steps 548(554.71) | Grad Norm 0.6060(1.0747) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 16.8889(16.7810) | Bit/dim 1.0868(1.0793) | Xent 0.0040(0.0073) | Loss 1.0888(1.0830) | Error 0.0011(0.0024) Steps 554(554.67) | Grad Norm 0.6940(1.0682) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 16.8424(16.7918) | Bit/dim 1.0874(1.0798) | Xent 0.0248(0.0081) | Loss 1.0999(1.0839) | Error 0.0033(0.0024) Steps 554(554.68) | Grad Norm 1.2701(1.0086) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 16.8084(16.7865) | Bit/dim 1.0909(1.0794) | Xent 0.0109(0.0088) | Loss 1.0963(1.0838) | Error 0.0033(0.0027) Steps 548(554.16) | Grad Norm 0.6716(1.0277) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 16.8214(16.7661) | Bit/dim 1.0722(1.0788) | Xent 0.0195(0.0084) | Loss 1.0820(1.0830) | Error 0.0033(0.0025) Steps 560(554.44) | Grad Norm 0.7781(0.9679) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 16.9087(16.7769) | Bit/dim 1.0727(1.0790) | Xent 0.0137(0.0087) | Loss 1.0795(1.0834) | Error 0.0033(0.0026) Steps 554(554.44) | Grad Norm 0.8254(0.9416) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 16.7621(16.7673) | Bit/dim 1.0878(1.0805) | Xent 0.0114(0.0086) | Loss 1.0935(1.0848) | Error 0.0011(0.0025) Steps 554(554.37) | Grad Norm 0.8658(1.0064) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 67.5787, Epoch Time 1187.4416(1202.2781), Bit/dim 1.0740, Xent 0.0296, Loss 1.0888, Error 0.0183\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 16.6474(16.7725) | Bit/dim 1.0838(1.0808) | Xent 0.0058(0.0086) | Loss 1.0866(1.0851) | Error 0.0022(0.0027) Steps 548(554.87) | Grad Norm 1.1097(1.0282) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 16.6503(16.8056) | Bit/dim 1.0749(1.0793) | Xent 0.0084(0.0082) | Loss 1.0791(1.0834) | Error 0.0044(0.0026) Steps 548(554.91) | Grad Norm 1.0915(1.2202) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 16.9201(16.8295) | Bit/dim 1.0699(1.0795) | Xent 0.0050(0.0083) | Loss 1.0724(1.0836) | Error 0.0011(0.0026) Steps 560(555.06) | Grad Norm 0.7612(1.3325) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 16.9204(16.8349) | Bit/dim 1.0621(1.0786) | Xent 0.0040(0.0081) | Loss 1.0640(1.0827) | Error 0.0011(0.0027) Steps 554(554.77) | Grad Norm 0.5114(1.1585) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 16.8258(16.8562) | Bit/dim 1.0750(1.0785) | Xent 0.0128(0.0087) | Loss 1.0814(1.0829) | Error 0.0056(0.0028) Steps 554(554.71) | Grad Norm 0.5429(1.1996) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 17.0673(16.8521) | Bit/dim 1.0814(1.0789) | Xent 0.0058(0.0085) | Loss 1.0843(1.0831) | Error 0.0022(0.0029) Steps 554(554.51) | Grad Norm 1.3049(1.1481) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 67.6825, Epoch Time 1195.1815(1202.0652), Bit/dim 1.0738, Xent 0.0317, Loss 1.0897, Error 0.0191\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2640 | Time 16.8066(16.8705) | Bit/dim 1.0605(1.0795) | Xent 0.0085(0.0086) | Loss 1.0648(1.0838) | Error 0.0044(0.0027) Steps 548(554.07) | Grad Norm 0.7117(1.0633) | Total Time 10.00(10.00)\n",
      "Iter 2650 | Time 16.8189(16.8535) | Bit/dim 1.0781(1.0798) | Xent 0.0157(0.0081) | Loss 1.0860(1.0839) | Error 0.0033(0.0024) Steps 554(554.54) | Grad Norm 0.7085(1.0063) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 16.6701(16.8395) | Bit/dim 1.0921(1.0800) | Xent 0.0071(0.0076) | Loss 1.0957(1.0837) | Error 0.0022(0.0022) Steps 554(554.87) | Grad Norm 2.2426(1.1887) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 17.0493(16.8237) | Bit/dim 1.0855(1.0792) | Xent 0.0110(0.0081) | Loss 1.0910(1.0833) | Error 0.0044(0.0024) Steps 548(554.75) | Grad Norm 0.6478(1.2128) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 16.3010(16.8132) | Bit/dim 1.0793(1.0794) | Xent 0.0116(0.0086) | Loss 1.0851(1.0837) | Error 0.0044(0.0026) Steps 548(554.82) | Grad Norm 1.8087(1.1645) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 16.8565(16.8137) | Bit/dim 1.0794(1.0789) | Xent 0.0132(0.0086) | Loss 1.0860(1.0832) | Error 0.0056(0.0026) Steps 554(554.15) | Grad Norm 1.1455(1.1112) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 16.8065(16.8193) | Bit/dim 1.0576(1.0789) | Xent 0.0087(0.0085) | Loss 1.0620(1.0832) | Error 0.0022(0.0027) Steps 554(554.57) | Grad Norm 1.3265(1.0988) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 67.3914, Epoch Time 1190.2399(1201.7104), Bit/dim 1.0739, Xent 0.0292, Loss 1.0885, Error 0.0182\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 16.7641(16.8280) | Bit/dim 1.0772(1.0781) | Xent 0.0057(0.0082) | Loss 1.0801(1.0822) | Error 0.0011(0.0026) Steps 560(555.21) | Grad Norm 0.8042(1.0617) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 16.9622(16.8367) | Bit/dim 1.0788(1.0785) | Xent 0.0116(0.0087) | Loss 1.0846(1.0829) | Error 0.0067(0.0027) Steps 554(555.37) | Grad Norm 1.1496(1.0684) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 17.0046(16.8496) | Bit/dim 1.0822(1.0789) | Xent 0.0080(0.0085) | Loss 1.0862(1.0831) | Error 0.0022(0.0027) Steps 548(554.70) | Grad Norm 0.7489(1.0306) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 16.9853(16.8598) | Bit/dim 1.0770(1.0803) | Xent 0.0087(0.0080) | Loss 1.0814(1.0843) | Error 0.0033(0.0025) Steps 554(554.79) | Grad Norm 0.8689(0.9969) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_00001 --resume experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_0001/best_nll_checkpt.pth --conditional True --log_freq 10 --weight_y 0.5 --lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
