{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn500_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=500, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 19.3422(38.5078) | Bit/dim 9.9841(10.8668) | Xent 2.2822(2.3001) | Loss 11.1252(12.0169) | Error 0.7544(0.8647) Steps 574(574.00) | Grad Norm 43.0930(65.6775) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 19.0403(33.3999) | Bit/dim 8.9208(10.4483) | Xent 2.2382(2.2874) | Loss 10.0399(11.5921) | Error 0.7711(0.8372) Steps 574(574.00) | Grad Norm 13.4920(54.5694) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 19.0326(29.6444) | Bit/dim 8.4745(9.9732) | Xent 2.1725(2.2652) | Loss 9.5607(11.1058) | Error 0.7411(0.8179) Steps 574(574.00) | Grad Norm 11.0646(43.6695) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 18.6731(26.8352) | Bit/dim 8.1480(9.5353) | Xent 2.1390(2.2364) | Loss 9.2175(10.6535) | Error 0.7344(0.7967) Steps 574(574.00) | Grad Norm 7.4052(34.4375) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 19.1617(24.8288) | Bit/dim 7.8444(9.1268) | Xent 2.1342(2.2085) | Loss 8.9115(10.2311) | Error 0.7167(0.7775) Steps 574(574.00) | Grad Norm 5.0415(27.0108) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 94.8053, Epoch Time 1187.4498(1187.4498), Bit/dim 7.6721(best: inf), Xent 2.0964, Loss 8.7203, Error 0.6904(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 19.4687(23.3585) | Bit/dim 7.5852(8.7489) | Xent 2.1005(2.1809) | Loss 8.6354(9.8393) | Error 0.7122(0.7585) Steps 574(574.00) | Grad Norm 4.3291(21.1314) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 19.0155(22.2446) | Bit/dim 7.3210(8.4010) | Xent 2.1132(2.1598) | Loss 8.3776(9.4808) | Error 0.6911(0.7399) Steps 574(574.00) | Grad Norm 2.8777(16.4840) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 19.3163(21.3950) | Bit/dim 7.1843(8.0948) | Xent 2.0971(2.1432) | Loss 8.2328(9.1664) | Error 0.6833(0.7274) Steps 574(574.00) | Grad Norm 2.1532(12.8669) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 19.8787(20.8969) | Bit/dim 7.0908(7.8399) | Xent 2.1223(2.1324) | Loss 8.1519(8.9061) | Error 0.7367(0.7223) Steps 592(576.34) | Grad Norm 1.9471(10.0653) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 19.7388(20.5532) | Bit/dim 6.9965(7.6298) | Xent 2.0784(2.1193) | Loss 8.0357(8.6895) | Error 0.7000(0.7148) Steps 592(580.45) | Grad Norm 3.6416(7.9968) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 20.9581(20.6070) | Bit/dim 6.9601(7.4584) | Xent 2.0761(2.1083) | Loss 7.9981(8.5125) | Error 0.7011(0.7114) Steps 610(586.33) | Grad Norm 1.4189(6.4126) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 94.4180, Epoch Time 1188.5986(1187.4842), Bit/dim 6.9513(best: 7.6721), Xent 2.0648, Loss 7.9837, Error 0.6921(best: 0.6904)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 21.1468(20.7124) | Bit/dim 6.8760(7.3170) | Xent 2.0525(2.0966) | Loss 7.9023(8.3653) | Error 0.7089(0.7085) Steps 610(592.55) | Grad Norm 3.6950(5.4403) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 21.6790(20.9053) | Bit/dim 6.8234(7.1959) | Xent 2.0547(2.0888) | Loss 7.8508(8.2403) | Error 0.6989(0.7073) Steps 622(599.27) | Grad Norm 1.2239(5.1942) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 21.6897(21.0966) | Bit/dim 6.7405(7.0866) | Xent 2.0470(2.0827) | Loss 7.7640(8.1279) | Error 0.7044(0.7094) Steps 622(605.24) | Grad Norm 4.6825(6.2873) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 23.3801(21.2588) | Bit/dim 6.6513(6.9847) | Xent 2.1114(2.0799) | Loss 7.7070(8.0247) | Error 0.7700(0.7139) Steps 628(610.93) | Grad Norm 31.8352(9.2244) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 21.5972(21.4017) | Bit/dim 6.5317(6.8784) | Xent 2.0860(2.0791) | Loss 7.5748(7.9179) | Error 0.7533(0.7200) Steps 634(615.77) | Grad Norm 27.9555(13.6115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 99.1292, Epoch Time 1302.3466(1190.9301), Bit/dim 6.4308(best: 6.9513), Xent 2.0326, Loss 7.4471, Error 0.6929(best: 0.6904)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 22.0952(21.5530) | Bit/dim 6.3748(6.7625) | Xent 2.0721(2.0781) | Loss 7.4108(7.8015) | Error 0.7200(0.7223) Steps 628(620.38) | Grad Norm 24.2075(16.5989) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 21.8695(21.6862) | Bit/dim 6.1739(6.6277) | Xent 2.0758(2.0713) | Loss 7.2118(7.6633) | Error 0.7500(0.7206) Steps 628(622.51) | Grad Norm 30.9520(17.2644) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 22.5117(21.8482) | Bit/dim 6.0958(6.4924) | Xent 2.1340(2.0853) | Loss 7.1628(7.5350) | Error 0.7733(0.7286) Steps 634(624.96) | Grad Norm 64.8580(26.5554) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 22.4522(21.9516) | Bit/dim 5.8924(6.3529) | Xent 2.0412(2.0786) | Loss 6.9130(7.3922) | Error 0.7067(0.7260) Steps 634(627.33) | Grad Norm 25.3159(26.9618) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 21.9144(21.9782) | Bit/dim 5.8017(6.2221) | Xent 2.0959(2.0853) | Loss 6.8496(7.2648) | Error 0.7856(0.7346) Steps 634(628.77) | Grad Norm 38.5685(29.0272) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 21.4236(21.9983) | Bit/dim 5.7290(6.1058) | Xent 2.0301(2.0775) | Loss 6.7441(7.1445) | Error 0.7122(0.7289) Steps 634(629.82) | Grad Norm 21.8763(25.4687) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 98.5693, Epoch Time 1333.1623(1195.1971), Bit/dim 5.7403(best: 6.4308), Xent 2.0328, Loss 6.7566, Error 0.6805(best: 0.6904)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 21.7970(21.9419) | Bit/dim 5.7340(6.0020) | Xent 2.0222(2.0627) | Loss 6.7451(7.0333) | Error 0.6822(0.7147) Steps 622(629.89) | Grad Norm 4.5968(20.5197) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 21.3666(21.8808) | Bit/dim 5.7288(5.9402) | Xent 2.0724(2.1094) | Loss 6.7650(6.9949) | Error 0.7356(0.7330) Steps 616(629.07) | Grad Norm 9.7317(30.6432) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 21.4212(21.8048) | Bit/dim 5.6256(5.8733) | Xent 2.0616(2.1037) | Loss 6.6564(6.9251) | Error 0.6989(0.7299) Steps 628(628.21) | Grad Norm 3.4087(25.5719) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 21.1756(21.6931) | Bit/dim 5.6485(5.8164) | Xent 2.0665(2.0957) | Loss 6.6818(6.8643) | Error 0.7167(0.7283) Steps 622(626.58) | Grad Norm 11.3240(21.0315) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 21.2013(21.5640) | Bit/dim 5.5364(5.7674) | Xent 2.0836(2.0890) | Loss 6.5782(6.8119) | Error 0.7533(0.7258) Steps 622(625.38) | Grad Norm 9.5525(19.4610) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 98.4847, Epoch Time 1299.1704(1198.3163), Bit/dim 5.5935(best: 5.7403), Xent 2.0194, Loss 6.6032, Error 0.6659(best: 0.6805)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 21.3421(21.4853) | Bit/dim 5.5932(5.7212) | Xent 2.0435(2.0775) | Loss 6.6150(6.7599) | Error 0.7122(0.7194) Steps 622(624.49) | Grad Norm 13.1205(16.9697) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 21.1839(21.4547) | Bit/dim 5.5983(5.6855) | Xent 1.9807(2.0607) | Loss 6.5887(6.7159) | Error 0.6800(0.7108) Steps 622(623.84) | Grad Norm 20.6724(16.3138) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 21.1449(21.4130) | Bit/dim 5.5591(5.6608) | Xent 2.0269(2.0639) | Loss 6.5726(6.6928) | Error 0.6978(0.7162) Steps 622(623.35) | Grad Norm 7.6960(21.1450) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 21.4540(21.3956) | Bit/dim 5.5531(5.6282) | Xent 2.0117(2.0544) | Loss 6.5590(6.6554) | Error 0.6889(0.7136) Steps 622(623.00) | Grad Norm 8.4456(17.9311) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 22.0152(21.4274) | Bit/dim 5.5135(5.5988) | Xent 2.0133(2.0427) | Loss 6.5202(6.6201) | Error 0.6867(0.7058) Steps 640(624.12) | Grad Norm 6.9812(14.6550) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 21.7638(21.4975) | Bit/dim 5.4417(5.5634) | Xent 1.9861(2.0291) | Loss 6.4347(6.5779) | Error 0.6811(0.6984) Steps 640(627.39) | Grad Norm 3.6545(12.0556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 99.0120, Epoch Time 1295.7908(1201.2405), Bit/dim 5.4540(best: 5.5935), Xent 1.9561, Loss 6.4321, Error 0.6523(best: 0.6659)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 21.3444(21.4667) | Bit/dim 5.4111(5.5320) | Xent 1.9394(2.0107) | Loss 6.3808(6.5373) | Error 0.6578(0.6890) Steps 646(631.38) | Grad Norm 4.6129(10.1662) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 22.1923(21.5831) | Bit/dim 5.3722(5.4986) | Xent 1.9550(1.9948) | Loss 6.3497(6.4960) | Error 0.6733(0.6835) Steps 664(636.58) | Grad Norm 10.9449(10.0096) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 22.5945(21.9058) | Bit/dim 5.3328(5.4635) | Xent 1.9776(1.9805) | Loss 6.3216(6.4537) | Error 0.7011(0.6799) Steps 676(646.38) | Grad Norm 9.7573(9.8724) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 23.0225(22.1464) | Bit/dim 5.3248(5.4295) | Xent 1.9511(1.9692) | Loss 6.3004(6.4141) | Error 0.6589(0.6759) Steps 694(655.86) | Grad Norm 24.0122(11.9956) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 23.1070(22.3754) | Bit/dim 5.2390(5.3930) | Xent 1.9818(1.9610) | Loss 6.2299(6.3735) | Error 0.7000(0.6743) Steps 700(666.04) | Grad Norm 12.6013(12.8734) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 108.9283, Epoch Time 1361.0512(1206.0348), Bit/dim 5.2556(best: 5.4540), Xent 1.9032, Loss 6.2072, Error 0.6533(best: 0.6523)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 23.2112(22.5664) | Bit/dim 5.2851(5.3677) | Xent 2.0460(1.9604) | Loss 6.3081(6.3479) | Error 0.7322(0.6755) Steps 694(674.30) | Grad Norm 29.3866(14.0514) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 22.8747(22.7150) | Bit/dim 5.2699(5.3429) | Xent 1.9190(1.9659) | Loss 6.2294(6.3258) | Error 0.6511(0.6806) Steps 700(681.07) | Grad Norm 16.3827(16.5648) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 22.6456(22.7790) | Bit/dim 5.2370(5.3124) | Xent 1.9314(1.9507) | Loss 6.2027(6.2878) | Error 0.6522(0.6737) Steps 694(684.76) | Grad Norm 5.5049(14.1176) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 22.7592(22.8135) | Bit/dim 5.1494(5.2791) | Xent 1.8815(1.9399) | Loss 6.0902(6.2491) | Error 0.6478(0.6705) Steps 694(686.73) | Grad Norm 24.3540(13.2861) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 23.1816(22.8326) | Bit/dim 5.1461(5.2524) | Xent 1.9274(1.9337) | Loss 6.1098(6.2192) | Error 0.6889(0.6718) Steps 700(689.11) | Grad Norm 14.9527(14.3063) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 23.1987(22.8629) | Bit/dim 5.1627(5.2285) | Xent 1.8790(1.9237) | Loss 6.1023(6.1903) | Error 0.6467(0.6685) Steps 700(691.04) | Grad Norm 7.7800(12.7446) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 109.5826, Epoch Time 1391.2308(1211.5907), Bit/dim 5.1217(best: 5.2556), Xent 1.8632, Loss 6.0534, Error 0.6375(best: 0.6523)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 22.8875(22.8741) | Bit/dim 5.1023(5.2039) | Xent 1.8638(1.9122) | Loss 6.0342(6.1600) | Error 0.6522(0.6645) Steps 688(692.02) | Grad Norm 8.1954(11.3432) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 23.1060(22.9300) | Bit/dim 5.2583(5.2008) | Xent 2.1719(1.9491) | Loss 6.3443(6.1754) | Error 0.7411(0.6771) Steps 700(693.61) | Grad Norm 27.8400(17.9956) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 22.7445(22.9443) | Bit/dim 5.0933(5.1860) | Xent 1.9601(1.9629) | Loss 6.0734(6.1674) | Error 0.6933(0.6878) Steps 682(691.11) | Grad Norm 6.4872(17.1970) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 22.0090(22.8270) | Bit/dim 5.0390(5.1586) | Xent 1.9537(1.9684) | Loss 6.0158(6.1427) | Error 0.6944(0.6908) Steps 676(688.36) | Grad Norm 6.1971(14.1765) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 22.6945(22.7680) | Bit/dim 5.0753(5.1284) | Xent 1.9522(1.9633) | Loss 6.0514(6.1100) | Error 0.6522(0.6872) Steps 682(686.04) | Grad Norm 12.0328(11.9089) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 108.9049, Epoch Time 1379.5446(1216.6293), Bit/dim 5.1802(best: 5.1217), Xent 1.9216, Loss 6.1409, Error 0.6796(best: 0.6375)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 23.4394(22.7382) | Bit/dim 5.4049(5.1386) | Xent 2.0844(1.9594) | Loss 6.4471(6.1183) | Error 0.7344(0.6849) Steps 688(684.70) | Grad Norm 66.0340(15.7243) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 22.7753(22.7594) | Bit/dim 5.1987(5.1570) | Xent 2.0020(1.9730) | Loss 6.1996(6.1435) | Error 0.7278(0.6932) Steps 682(684.64) | Grad Norm 8.1671(15.3367) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 23.0672(22.7674) | Bit/dim 5.0670(5.1383) | Xent 1.9488(1.9661) | Loss 6.0414(6.1214) | Error 0.6811(0.6902) Steps 670(683.02) | Grad Norm 6.6108(13.0417) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 23.1501(22.7791) | Bit/dim 4.9749(5.1058) | Xent 1.8768(1.9499) | Loss 5.9133(6.0807) | Error 0.6689(0.6852) Steps 682(683.55) | Grad Norm 6.1112(11.2698) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 22.9855(22.8170) | Bit/dim 4.9613(5.0696) | Xent 1.8283(1.9332) | Loss 5.8755(6.0362) | Error 0.6367(0.6779) Steps 682(683.16) | Grad Norm 10.4712(10.5712) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 22.8174(22.9281) | Bit/dim 4.9469(5.0371) | Xent 1.9439(1.9229) | Loss 5.9188(5.9986) | Error 0.6922(0.6758) Steps 694(683.90) | Grad Norm 17.0687(11.6004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 107.8353, Epoch Time 1385.2035(1221.6865), Bit/dim 4.9189(best: 5.1217), Xent 1.8331, Loss 5.8355, Error 0.6332(best: 0.6375)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 22.9454(23.1142) | Bit/dim 4.9766(5.0054) | Xent 1.8394(1.9029) | Loss 5.8963(5.9568) | Error 0.6544(0.6680) Steps 700(687.34) | Grad Norm 20.4641(11.8116) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 23.7848(23.2358) | Bit/dim 4.8582(4.9782) | Xent 1.8989(1.8893) | Loss 5.8077(5.9229) | Error 0.6778(0.6617) Steps 724(691.29) | Grad Norm 10.4176(12.6352) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 23.5101(23.2929) | Bit/dim 4.9065(4.9760) | Xent 1.8819(1.8853) | Loss 5.8475(5.9186) | Error 0.6644(0.6604) Steps 700(694.29) | Grad Norm 16.5420(15.1067) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 24.1576(23.4293) | Bit/dim 4.8478(4.9541) | Xent 1.8032(1.8810) | Loss 5.7494(5.8945) | Error 0.6411(0.6586) Steps 706(697.26) | Grad Norm 14.8541(14.5722) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 24.8529(23.5882) | Bit/dim 4.9566(4.9291) | Xent 1.9121(1.8693) | Loss 5.9126(5.8637) | Error 0.6589(0.6556) Steps 730(702.66) | Grad Norm 40.5755(14.8206) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 106.5289, Epoch Time 1424.5554(1227.7726), Bit/dim 5.3609(best: 4.9189), Xent 1.9117, Loss 6.3167, Error 0.6968(best: 0.6332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 23.1411(23.5011) | Bit/dim 4.9776(4.9941) | Xent 1.8625(1.8807) | Loss 5.9088(5.9344) | Error 0.6467(0.6595) Steps 688(702.36) | Grad Norm 7.1682(17.8327) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 23.3033(23.4692) | Bit/dim 4.9204(4.9888) | Xent 1.8126(1.8757) | Loss 5.8267(5.9267) | Error 0.6344(0.6574) Steps 700(700.93) | Grad Norm 6.2586(15.1982) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 22.9051(23.4908) | Bit/dim 4.8079(4.9581) | Xent 1.8212(1.8694) | Loss 5.7185(5.8928) | Error 0.6256(0.6556) Steps 706(701.88) | Grad Norm 11.2558(13.9456) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 23.4571(23.4939) | Bit/dim 4.7628(4.9160) | Xent 1.7792(1.8482) | Loss 5.6524(5.8401) | Error 0.6267(0.6496) Steps 718(704.61) | Grad Norm 4.8973(12.7552) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 24.2172(23.5811) | Bit/dim 4.8260(4.8970) | Xent 1.9279(1.8380) | Loss 5.7899(5.8160) | Error 0.6711(0.6460) Steps 736(708.89) | Grad Norm 25.8381(15.3576) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 24.2311(23.7200) | Bit/dim 4.7595(4.8716) | Xent 1.8335(1.8302) | Loss 5.6762(5.7868) | Error 0.6433(0.6420) Steps 736(715.19) | Grad Norm 12.5822(14.7785) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 113.6659, Epoch Time 1431.7566(1233.8921), Bit/dim 4.7774(best: 4.9189), Xent 1.7247, Loss 5.6398, Error 0.5987(best: 0.6332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 23.7388(23.8726) | Bit/dim 4.7655(4.8444) | Xent 1.7833(1.8128) | Loss 5.6571(5.7507) | Error 0.6467(0.6380) Steps 730(721.42) | Grad Norm 11.9351(14.6550) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 23.9833(24.0342) | Bit/dim 4.7424(4.8159) | Xent 1.7496(1.7970) | Loss 5.6171(5.7144) | Error 0.6122(0.6331) Steps 742(726.77) | Grad Norm 34.0336(14.6613) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 24.1918(24.1004) | Bit/dim 4.7698(4.8191) | Xent 1.8188(1.8152) | Loss 5.6792(5.7267) | Error 0.6222(0.6407) Steps 736(731.91) | Grad Norm 9.6352(17.1534) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 23.8620(24.1106) | Bit/dim 4.7182(4.8021) | Xent 1.7798(1.8133) | Loss 5.6081(5.7087) | Error 0.6400(0.6400) Steps 730(732.11) | Grad Norm 16.6034(15.7523) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 24.7510(24.2140) | Bit/dim 4.6765(4.7726) | Xent 1.7500(1.7952) | Loss 5.5515(5.6701) | Error 0.6267(0.6349) Steps 754(736.32) | Grad Norm 2.6287(13.8198) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 115.5310, Epoch Time 1472.8375(1241.0605), Bit/dim 4.9843(best: 4.7774), Xent 1.8734, Loss 5.9210, Error 0.6542(best: 0.5987)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 24.6869(24.2395) | Bit/dim 4.8859(4.8118) | Xent 1.8523(1.8170) | Loss 5.8120(5.7203) | Error 0.6667(0.6434) Steps 766(740.03) | Grad Norm 11.5551(17.6697) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 23.9950(24.2935) | Bit/dim 4.7596(4.8202) | Xent 1.7772(1.8347) | Loss 5.6482(5.7375) | Error 0.6367(0.6513) Steps 736(745.39) | Grad Norm 4.3356(15.3408) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 24.4095(24.3693) | Bit/dim 4.7080(4.7959) | Xent 1.7649(1.8206) | Loss 5.5905(5.7062) | Error 0.6278(0.6465) Steps 742(746.55) | Grad Norm 5.5728(12.6126) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 24.3251(24.3902) | Bit/dim 4.6397(4.7605) | Xent 1.7829(1.8111) | Loss 5.5312(5.6661) | Error 0.6300(0.6442) Steps 742(744.01) | Grad Norm 6.8112(11.0493) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 24.6384(24.4005) | Bit/dim 4.6330(4.7294) | Xent 1.7395(1.7887) | Loss 5.5027(5.6238) | Error 0.6344(0.6360) Steps 760(745.63) | Grad Norm 6.3514(10.0464) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 25.5867(24.5561) | Bit/dim 4.6124(4.7003) | Xent 1.6520(1.7685) | Loss 5.4384(5.5846) | Error 0.6000(0.6300) Steps 784(752.33) | Grad Norm 6.6782(8.8942) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 119.9247, Epoch Time 1487.1193(1248.4423), Bit/dim 4.6030(best: 4.7774), Xent 1.6318, Loss 5.4189, Error 0.5725(best: 0.5987)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 25.2828(24.6947) | Bit/dim 4.6082(4.6845) | Xent 1.7352(1.7591) | Loss 5.4758(5.5641) | Error 0.6511(0.6290) Steps 790(759.58) | Grad Norm 9.0866(11.4299) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 25.1357(24.8360) | Bit/dim 4.5818(4.6607) | Xent 1.7336(1.7463) | Loss 5.4486(5.5338) | Error 0.6211(0.6237) Steps 778(766.57) | Grad Norm 13.4496(11.3460) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 25.2629(24.9355) | Bit/dim 4.5768(4.6354) | Xent 1.6835(1.7313) | Loss 5.4186(5.5010) | Error 0.5822(0.6179) Steps 784(771.77) | Grad Norm 11.7642(11.2209) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 25.4033(25.0897) | Bit/dim 4.6166(4.6146) | Xent 1.7974(1.7222) | Loss 5.5153(5.4757) | Error 0.6600(0.6151) Steps 796(778.28) | Grad Norm 27.9494(12.0056) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 25.3760(25.1761) | Bit/dim 4.5552(4.6085) | Xent 1.7313(1.7198) | Loss 5.4208(5.4684) | Error 0.6200(0.6132) Steps 784(782.27) | Grad Norm 8.6737(13.0173) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 130.3646, Epoch Time 1540.3157(1257.1985), Bit/dim 4.5581(best: 4.6030), Xent 1.5955, Loss 5.3559, Error 0.5658(best: 0.5725)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 26.0898(25.3077) | Bit/dim 4.5450(4.5898) | Xent 1.6604(1.7068) | Loss 5.3751(5.4432) | Error 0.6089(0.6113) Steps 814(787.77) | Grad Norm 10.7030(12.5115) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 25.2271(25.3066) | Bit/dim 4.5268(4.5701) | Xent 1.6485(1.6888) | Loss 5.3511(5.4145) | Error 0.5656(0.6041) Steps 796(788.83) | Grad Norm 11.1812(11.7108) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 25.1564(25.3509) | Bit/dim 4.5241(4.5531) | Xent 1.6667(1.6833) | Loss 5.3575(5.3947) | Error 0.5944(0.6023) Steps 790(791.25) | Grad Norm 12.9584(12.3166) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 24.9863(25.4030) | Bit/dim 4.5259(4.5383) | Xent 1.6846(1.6872) | Loss 5.3682(5.3819) | Error 0.6100(0.6025) Steps 784(792.57) | Grad Norm 11.3321(12.9612) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 25.8611(25.4959) | Bit/dim 4.5150(4.5347) | Xent 1.7246(1.6866) | Loss 5.3772(5.3780) | Error 0.6344(0.6027) Steps 796(795.48) | Grad Norm 12.2869(13.2699) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 25.2767(25.5748) | Bit/dim 4.4908(4.5196) | Xent 1.6681(1.6826) | Loss 5.3248(5.3609) | Error 0.5911(0.6028) Steps 796(797.35) | Grad Norm 11.1405(13.0006) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 122.7691, Epoch Time 1548.8193(1265.9471), Bit/dim 4.4588(best: 4.5581), Xent 1.5484, Loss 5.2330, Error 0.5495(best: 0.5658)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 25.3395(25.5938) | Bit/dim 4.4523(4.5042) | Xent 1.5761(1.6692) | Loss 5.2403(5.3388) | Error 0.5689(0.5976) Steps 784(795.27) | Grad Norm 14.8373(12.7198) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 25.5402(25.6164) | Bit/dim 4.5139(4.4963) | Xent 1.8080(1.6698) | Loss 5.4179(5.3312) | Error 0.6367(0.5974) Steps 766(793.85) | Grad Norm 17.8449(13.6008) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 25.4723(25.6354) | Bit/dim 4.4912(4.4927) | Xent 1.5760(1.6711) | Loss 5.2792(5.3283) | Error 0.5633(0.5986) Steps 784(796.09) | Grad Norm 10.0906(13.7126) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 26.1695(25.7369) | Bit/dim 4.4426(4.4796) | Xent 1.5304(1.6498) | Loss 5.2078(5.3045) | Error 0.5578(0.5908) Steps 832(800.17) | Grad Norm 12.7177(12.1061) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 25.8212(25.7507) | Bit/dim 4.3855(4.4625) | Xent 1.6058(1.6410) | Loss 5.1884(5.2830) | Error 0.5767(0.5879) Steps 802(799.39) | Grad Norm 8.7069(12.3935) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 124.8162, Epoch Time 1559.5638(1274.7556), Bit/dim 4.4086(best: 4.4588), Xent 1.5476, Loss 5.1824, Error 0.5548(best: 0.5495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 26.2454(25.7684) | Bit/dim 4.4111(4.4502) | Xent 1.6574(1.6321) | Loss 5.2398(5.2662) | Error 0.5811(0.5843) Steps 808(799.91) | Grad Norm 25.2615(12.9697) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 25.5151(25.8533) | Bit/dim 4.4312(4.4503) | Xent 1.5444(1.6323) | Loss 5.2034(5.2665) | Error 0.5656(0.5837) Steps 790(800.58) | Grad Norm 12.7484(13.1624) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 25.5624(25.9199) | Bit/dim 4.4078(4.4463) | Xent 1.6680(1.6306) | Loss 5.2419(5.2616) | Error 0.5933(0.5840) Steps 790(801.00) | Grad Norm 15.8427(13.8237) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 25.8814(25.9147) | Bit/dim 4.4459(4.4413) | Xent 1.6837(1.6403) | Loss 5.2878(5.2614) | Error 0.6111(0.5887) Steps 796(801.15) | Grad Norm 14.0713(13.7687) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 25.2568(25.9142) | Bit/dim 4.3628(4.4293) | Xent 1.5945(1.6339) | Loss 5.1601(5.2462) | Error 0.5756(0.5882) Steps 802(802.69) | Grad Norm 6.4309(12.5053) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 26.3164(25.9561) | Bit/dim 4.3816(4.4138) | Xent 1.5121(1.6183) | Loss 5.1377(5.2229) | Error 0.5467(0.5831) Steps 808(806.21) | Grad Norm 7.5979(11.7445) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 127.7833, Epoch Time 1576.4819(1283.8074), Bit/dim 4.3643(best: 4.4086), Xent 1.4605, Loss 5.0945, Error 0.5233(best: 0.5495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 25.4282(26.0184) | Bit/dim 4.4728(4.4107) | Xent 1.7326(1.6241) | Loss 5.3391(5.2228) | Error 0.6189(0.5834) Steps 784(808.00) | Grad Norm 18.8590(13.7130) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 25.5190(25.9659) | Bit/dim 4.3921(4.4171) | Xent 1.5938(1.6285) | Loss 5.1890(5.2314) | Error 0.5756(0.5844) Steps 814(809.76) | Grad Norm 6.8864(13.2603) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 26.1162(25.9754) | Bit/dim 4.3730(4.4031) | Xent 1.6689(1.6237) | Loss 5.2074(5.2150) | Error 0.6100(0.5826) Steps 820(811.44) | Grad Norm 15.9761(12.3178) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 25.4107(25.9061) | Bit/dim 4.3685(4.3916) | Xent 1.5922(1.6192) | Loss 5.1646(5.2012) | Error 0.5656(0.5803) Steps 808(810.74) | Grad Norm 13.9797(11.9890) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 26.2609(25.9311) | Bit/dim 4.3197(4.3795) | Xent 1.5067(1.6018) | Loss 5.0731(5.1804) | Error 0.5456(0.5742) Steps 814(810.84) | Grad Norm 6.6517(11.1596) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 127.5648, Epoch Time 1575.4913(1292.5579), Bit/dim 4.3245(best: 4.3643), Xent 1.4285, Loss 5.0387, Error 0.5146(best: 0.5233)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 26.3887(26.0302) | Bit/dim 4.3253(4.3623) | Xent 1.5538(1.5790) | Loss 5.1022(5.1518) | Error 0.5478(0.5659) Steps 820(812.91) | Grad Norm 15.7902(10.8480) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 26.4117(26.0651) | Bit/dim 4.3226(4.3574) | Xent 1.4813(1.5716) | Loss 5.0632(5.1432) | Error 0.5400(0.5617) Steps 832(815.11) | Grad Norm 10.1057(11.6364) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 26.3099(26.1100) | Bit/dim 4.3148(4.3536) | Xent 1.4913(1.5724) | Loss 5.0605(5.1398) | Error 0.5344(0.5606) Steps 844(819.86) | Grad Norm 8.2277(12.0139) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 26.4316(26.1392) | Bit/dim 4.3569(4.3482) | Xent 1.6233(1.5693) | Loss 5.1685(5.1329) | Error 0.5911(0.5614) Steps 820(822.98) | Grad Norm 20.7575(12.0658) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 26.8526(26.2670) | Bit/dim 4.3070(4.3404) | Xent 1.4727(1.5656) | Loss 5.0433(5.1232) | Error 0.5189(0.5612) Steps 856(826.53) | Grad Norm 8.4279(11.6340) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 26.6473(26.3499) | Bit/dim 4.2938(4.3279) | Xent 1.4598(1.5467) | Loss 5.0238(5.1012) | Error 0.5222(0.5561) Steps 820(828.45) | Grad Norm 8.6975(10.4609) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 131.3232, Epoch Time 1598.1993(1301.7271), Bit/dim 4.2910(best: 4.3245), Xent 1.3965, Loss 4.9893, Error 0.5020(best: 0.5146)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 26.2102(26.4480) | Bit/dim 4.2700(4.3158) | Xent 1.4296(1.5202) | Loss 4.9848(5.0758) | Error 0.5200(0.5457) Steps 820(828.87) | Grad Norm 9.2136(10.1591) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 26.2380(26.4541) | Bit/dim 4.3955(4.3314) | Xent 1.7425(1.5361) | Loss 5.2667(5.0995) | Error 0.6144(0.5491) Steps 826(830.69) | Grad Norm 22.2624(12.8180) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 26.6704(26.3695) | Bit/dim 4.3484(4.3417) | Xent 1.5117(1.5725) | Loss 5.1042(5.1279) | Error 0.5467(0.5635) Steps 838(828.18) | Grad Norm 7.1729(13.2771) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 26.3155(26.3109) | Bit/dim 4.2920(4.3343) | Xent 1.5270(1.5706) | Loss 5.0555(5.1197) | Error 0.5589(0.5632) Steps 826(825.84) | Grad Norm 3.9140(11.5044) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 26.1334(26.2311) | Bit/dim 4.2359(4.3179) | Xent 1.4788(1.5536) | Loss 4.9752(5.0947) | Error 0.5400(0.5581) Steps 826(823.43) | Grad Norm 3.9125(9.7534) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 129.1319, Epoch Time 1592.1634(1310.4402), Bit/dim 4.2499(best: 4.2910), Xent 1.4342, Loss 4.9670, Error 0.5156(best: 0.5020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 26.0792(26.2519) | Bit/dim 4.1904(4.3006) | Xent 1.4879(1.5335) | Loss 4.9344(5.0673) | Error 0.5278(0.5510) Steps 832(823.47) | Grad Norm 6.0488(8.8865) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 26.5132(26.3455) | Bit/dim 4.2212(4.2860) | Xent 1.4593(1.5170) | Loss 4.9509(5.0445) | Error 0.5278(0.5450) Steps 838(826.17) | Grad Norm 6.1651(8.9418) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 27.0156(26.4783) | Bit/dim 4.2679(4.2784) | Xent 1.4313(1.5240) | Loss 4.9836(5.0404) | Error 0.5222(0.5467) Steps 844(832.81) | Grad Norm 11.1857(9.8868) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 26.7092(26.5166) | Bit/dim 4.2448(4.2742) | Xent 1.4254(1.5119) | Loss 4.9575(5.0302) | Error 0.4956(0.5431) Steps 850(835.52) | Grad Norm 6.5068(9.3401) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 26.7991(26.6878) | Bit/dim 4.2382(4.2687) | Xent 1.4365(1.5080) | Loss 4.9564(5.0227) | Error 0.5322(0.5398) Steps 826(837.30) | Grad Norm 12.0121(10.3132) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 27.3647(26.7624) | Bit/dim 4.2307(4.2577) | Xent 1.4291(1.4957) | Loss 4.9452(5.0055) | Error 0.5144(0.5361) Steps 868(840.52) | Grad Norm 6.6349(9.2169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 130.3179, Epoch Time 1622.8158(1319.8115), Bit/dim 4.2201(best: 4.2499), Xent 1.3320, Loss 4.8862, Error 0.4771(best: 0.5020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 26.9387(26.7764) | Bit/dim 4.2201(4.2460) | Xent 1.4152(1.4682) | Loss 4.9277(4.9801) | Error 0.4944(0.5266) Steps 838(840.84) | Grad Norm 6.9452(9.0836) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 26.7845(26.7579) | Bit/dim 4.2307(4.2416) | Xent 1.5198(1.4614) | Loss 4.9906(4.9723) | Error 0.5478(0.5233) Steps 850(841.12) | Grad Norm 8.8207(9.7294) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 26.4020(26.7809) | Bit/dim 4.2041(4.2322) | Xent 1.4547(1.4528) | Loss 4.9315(4.9586) | Error 0.5156(0.5218) Steps 838(842.89) | Grad Norm 5.7893(9.3972) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 26.4836(26.7184) | Bit/dim 4.1927(4.2204) | Xent 1.3678(1.4375) | Loss 4.8766(4.9391) | Error 0.4944(0.5169) Steps 832(841.27) | Grad Norm 9.7322(8.6586) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 26.8131(26.7152) | Bit/dim 4.2246(4.2218) | Xent 1.3825(1.4359) | Loss 4.9158(4.9398) | Error 0.4900(0.5155) Steps 856(840.73) | Grad Norm 11.9007(9.7496) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 128.9725, Epoch Time 1617.7806(1328.7506), Bit/dim 4.3001(best: 4.2201), Xent 1.4611, Loss 5.0307, Error 0.5292(best: 0.4771)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 26.1009(26.7205) | Bit/dim 4.2689(4.2502) | Xent 1.5418(1.4872) | Loss 5.0398(4.9939) | Error 0.5611(0.5318) Steps 838(842.20) | Grad Norm 11.1905(11.7268) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 26.7541(26.6814) | Bit/dim 4.2206(4.2505) | Xent 1.4108(1.4791) | Loss 4.9260(4.9900) | Error 0.5300(0.5314) Steps 862(841.41) | Grad Norm 5.4051(10.6377) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 27.6060(26.7059) | Bit/dim 4.1821(4.2387) | Xent 1.4336(1.4621) | Loss 4.8989(4.9698) | Error 0.5056(0.5255) Steps 850(841.64) | Grad Norm 10.9797(9.7111) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 26.9095(26.7541) | Bit/dim 4.1815(4.2218) | Xent 1.3629(1.4404) | Loss 4.8629(4.9420) | Error 0.4911(0.5188) Steps 844(842.46) | Grad Norm 4.9611(9.1351) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 26.9477(26.8254) | Bit/dim 4.1618(4.2069) | Xent 1.3837(1.4255) | Loss 4.8536(4.9196) | Error 0.4978(0.5129) Steps 850(844.15) | Grad Norm 9.2725(8.5338) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 26.7553(26.7994) | Bit/dim 4.2094(4.1967) | Xent 1.5939(1.4128) | Loss 5.0064(4.9030) | Error 0.5511(0.5068) Steps 868(847.08) | Grad Norm 20.2883(8.6893) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 126.9609, Epoch Time 1617.6633(1337.4179), Bit/dim 4.1834(best: 4.2201), Xent 1.2751, Loss 4.8209, Error 0.4591(best: 0.4771)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 26.9871(26.7935) | Bit/dim 4.1765(4.1912) | Xent 1.4928(1.4121) | Loss 4.9229(4.8973) | Error 0.5033(0.5064) Steps 856(846.57) | Grad Norm 12.1655(9.5720) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 27.3095(26.8821) | Bit/dim 4.1538(4.1878) | Xent 1.3884(1.4085) | Loss 4.8480(4.8921) | Error 0.4967(0.5047) Steps 862(848.07) | Grad Norm 9.8639(9.9876) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn500_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 500\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
