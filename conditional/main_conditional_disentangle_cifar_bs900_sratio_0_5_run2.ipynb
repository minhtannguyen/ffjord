{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run2/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13430 | Time 27.3337(27.4390) | Bit/dim 3.5914(3.5630) | Xent 0.0603(0.1017) | Loss 3.6215(3.6139) | Error 0.0222(0.0370) Steps 1102(1107.25) | Grad Norm 3.1126(4.2050) | Total Time 14.00(14.00)\n",
      "Iter 13440 | Time 27.4205(27.3034) | Bit/dim 3.5827(3.5615) | Xent 0.0306(0.0885) | Loss 3.5980(3.6057) | Error 0.0122(0.0321) Steps 1102(1105.93) | Grad Norm 1.1486(3.6692) | Total Time 14.00(14.00)\n",
      "Iter 13450 | Time 27.1059(27.2208) | Bit/dim 3.5416(3.5568) | Xent 0.0449(0.0762) | Loss 3.5641(3.5949) | Error 0.0133(0.0272) Steps 1090(1104.55) | Grad Norm 1.2547(3.1124) | Total Time 14.00(14.00)\n",
      "Iter 13460 | Time 26.5454(27.0717) | Bit/dim 3.5213(3.5544) | Xent 0.0352(0.0658) | Loss 3.5389(3.5873) | Error 0.0122(0.0231) Steps 1102(1104.16) | Grad Norm 0.9645(2.5924) | Total Time 14.00(14.00)\n",
      "Iter 13470 | Time 26.5920(26.9575) | Bit/dim 3.5419(3.5504) | Xent 0.0388(0.0577) | Loss 3.5613(3.5793) | Error 0.0167(0.0199) Steps 1090(1103.37) | Grad Norm 0.9903(2.1882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 131.4441, Epoch Time 1642.4362(1571.2784), Bit/dim 3.5458(best: inf), Xent 1.7517, Loss 4.4216, Error 0.2727(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13480 | Time 26.4313(26.9559) | Bit/dim 3.5442(3.5493) | Xent 0.0375(0.0508) | Loss 3.5630(3.5747) | Error 0.0156(0.0170) Steps 1102(1105.01) | Grad Norm 0.9749(1.8784) | Total Time 14.00(14.00)\n",
      "Iter 13490 | Time 26.6763(26.9458) | Bit/dim 3.5714(3.5458) | Xent 0.0318(0.0457) | Loss 3.5873(3.5686) | Error 0.0100(0.0148) Steps 1120(1105.16) | Grad Norm 0.8705(1.6335) | Total Time 14.00(14.00)\n",
      "Iter 13500 | Time 26.9166(26.9298) | Bit/dim 3.5485(3.5429) | Xent 0.0258(0.0420) | Loss 3.5614(3.5639) | Error 0.0078(0.0134) Steps 1126(1104.40) | Grad Norm 0.9239(1.4598) | Total Time 14.00(14.00)\n",
      "Iter 13510 | Time 26.4481(26.9540) | Bit/dim 3.5641(3.5423) | Xent 0.0341(0.0391) | Loss 3.5812(3.5619) | Error 0.0111(0.0125) Steps 1084(1104.58) | Grad Norm 1.0428(1.3337) | Total Time 14.00(14.00)\n",
      "Iter 13520 | Time 26.6693(26.9298) | Bit/dim 3.5115(3.5406) | Xent 0.0242(0.0364) | Loss 3.5237(3.5588) | Error 0.0067(0.0114) Steps 1096(1103.43) | Grad Norm 0.7791(1.2014) | Total Time 14.00(14.00)\n",
      "Iter 13530 | Time 26.8821(26.9543) | Bit/dim 3.5194(3.5393) | Xent 0.0425(0.0351) | Loss 3.5406(3.5568) | Error 0.0144(0.0108) Steps 1102(1104.75) | Grad Norm 1.1221(1.1266) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 126.8062, Epoch Time 1627.7060(1572.9713), Bit/dim 3.5400(best: 3.5458), Xent 1.7846, Loss 4.4324, Error 0.2743(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13540 | Time 26.2740(26.9376) | Bit/dim 3.5474(3.5387) | Xent 0.0230(0.0328) | Loss 3.5589(3.5551) | Error 0.0056(0.0098) Steps 1108(1104.41) | Grad Norm 0.8281(1.0685) | Total Time 14.00(14.00)\n",
      "Iter 13550 | Time 27.1999(26.9222) | Bit/dim 3.5438(3.5351) | Xent 0.0281(0.0316) | Loss 3.5578(3.5509) | Error 0.0056(0.0091) Steps 1096(1103.31) | Grad Norm 0.9117(1.0285) | Total Time 14.00(14.00)\n",
      "Iter 13560 | Time 26.6240(26.8460) | Bit/dim 3.5387(3.5354) | Xent 0.0289(0.0311) | Loss 3.5531(3.5510) | Error 0.0089(0.0088) Steps 1126(1103.01) | Grad Norm 0.8772(1.0015) | Total Time 14.00(14.00)\n",
      "Iter 13570 | Time 27.3647(26.7871) | Bit/dim 3.5515(3.5357) | Xent 0.0242(0.0311) | Loss 3.5636(3.5512) | Error 0.0067(0.0088) Steps 1120(1101.35) | Grad Norm 0.8123(0.9734) | Total Time 14.00(14.00)\n",
      "Iter 13580 | Time 26.4338(26.8184) | Bit/dim 3.5279(3.5360) | Xent 0.0213(0.0296) | Loss 3.5385(3.5509) | Error 0.0044(0.0081) Steps 1096(1102.20) | Grad Norm 0.8077(0.9514) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 125.8767, Epoch Time 1618.1762(1574.3274), Bit/dim 3.5374(best: 3.5400), Xent 1.7954, Loss 4.4351, Error 0.2744(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13590 | Time 26.6234(26.8314) | Bit/dim 3.5293(3.5356) | Xent 0.0223(0.0288) | Loss 3.5404(3.5500) | Error 0.0067(0.0078) Steps 1090(1102.90) | Grad Norm 0.7107(0.9238) | Total Time 14.00(14.00)\n",
      "Iter 13600 | Time 26.5347(26.8149) | Bit/dim 3.5297(3.5348) | Xent 0.0178(0.0280) | Loss 3.5386(3.5488) | Error 0.0022(0.0075) Steps 1090(1101.83) | Grad Norm 0.6102(0.9102) | Total Time 14.00(14.00)\n",
      "Iter 13610 | Time 26.8497(26.8632) | Bit/dim 3.5348(3.5350) | Xent 0.0300(0.0282) | Loss 3.5498(3.5491) | Error 0.0100(0.0078) Steps 1114(1102.45) | Grad Norm 1.3448(0.9300) | Total Time 14.00(14.00)\n",
      "Iter 13620 | Time 27.1702(26.9301) | Bit/dim 3.5390(3.5362) | Xent 0.0332(0.0285) | Loss 3.5556(3.5505) | Error 0.0089(0.0079) Steps 1096(1100.07) | Grad Norm 1.0035(0.9248) | Total Time 14.00(14.00)\n",
      "Iter 13630 | Time 27.4126(26.9450) | Bit/dim 3.5527(3.5353) | Xent 0.0262(0.0280) | Loss 3.5659(3.5493) | Error 0.0089(0.0079) Steps 1108(1102.39) | Grad Norm 0.9329(0.9053) | Total Time 14.00(14.00)\n",
      "Iter 13640 | Time 26.8624(26.8858) | Bit/dim 3.5214(3.5314) | Xent 0.0318(0.0284) | Loss 3.5373(3.5456) | Error 0.0100(0.0079) Steps 1090(1101.93) | Grad Norm 0.8775(0.9023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 126.2792, Epoch Time 1623.8163(1575.8121), Bit/dim 3.5371(best: 3.5374), Xent 1.8002, Loss 4.4373, Error 0.2730(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 27.3328(26.8596) | Bit/dim 3.5270(3.5308) | Xent 0.0241(0.0276) | Loss 3.5391(3.5446) | Error 0.0044(0.0074) Steps 1120(1103.69) | Grad Norm 0.8075(0.8966) | Total Time 14.00(14.00)\n",
      "Iter 13660 | Time 27.1103(26.8416) | Bit/dim 3.5288(3.5303) | Xent 0.0251(0.0268) | Loss 3.5414(3.5437) | Error 0.0078(0.0071) Steps 1108(1106.25) | Grad Norm 0.8721(0.8982) | Total Time 14.00(14.00)\n",
      "Iter 13670 | Time 26.8575(26.9486) | Bit/dim 3.5521(3.5349) | Xent 0.0239(0.0266) | Loss 3.5641(3.5482) | Error 0.0067(0.0072) Steps 1108(1106.56) | Grad Norm 0.8637(0.9028) | Total Time 14.00(14.00)\n",
      "Iter 13680 | Time 26.0990(26.9086) | Bit/dim 3.5438(3.5359) | Xent 0.0232(0.0265) | Loss 3.5554(3.5492) | Error 0.0067(0.0072) Steps 1096(1107.44) | Grad Norm 0.7428(0.8926) | Total Time 14.00(14.00)\n",
      "Iter 13690 | Time 26.8996(26.9308) | Bit/dim 3.4956(3.5297) | Xent 0.0199(0.0268) | Loss 3.5055(3.5431) | Error 0.0022(0.0074) Steps 1108(1106.27) | Grad Norm 0.7531(0.8879) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 127.2718, Epoch Time 1627.7421(1577.3700), Bit/dim 3.5348(best: 3.5371), Xent 1.8346, Loss 4.4521, Error 0.2773(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 27.4128(27.0147) | Bit/dim 3.5444(3.5306) | Xent 0.0294(0.0272) | Loss 3.5591(3.5442) | Error 0.0100(0.0072) Steps 1132(1108.23) | Grad Norm 0.9680(0.9106) | Total Time 14.00(14.00)\n",
      "Iter 13710 | Time 26.9587(27.0036) | Bit/dim 3.5249(3.5294) | Xent 0.0274(0.0275) | Loss 3.5386(3.5432) | Error 0.0067(0.0075) Steps 1126(1108.67) | Grad Norm 0.7385(0.9258) | Total Time 14.00(14.00)\n",
      "Iter 13720 | Time 27.5615(27.0716) | Bit/dim 3.5238(3.5304) | Xent 0.0270(0.0271) | Loss 3.5373(3.5440) | Error 0.0056(0.0070) Steps 1114(1108.61) | Grad Norm 0.9390(0.9033) | Total Time 14.00(14.00)\n",
      "Iter 13730 | Time 27.8776(27.0975) | Bit/dim 3.5448(3.5302) | Xent 0.0301(0.0265) | Loss 3.5598(3.5435) | Error 0.0089(0.0069) Steps 1102(1107.44) | Grad Norm 0.8617(0.9108) | Total Time 14.00(14.00)\n",
      "Iter 13740 | Time 27.0242(26.9998) | Bit/dim 3.5537(3.5310) | Xent 0.0329(0.0269) | Loss 3.5702(3.5444) | Error 0.0100(0.0075) Steps 1102(1107.14) | Grad Norm 0.9231(0.9204) | Total Time 14.00(14.00)\n",
      "Iter 13750 | Time 27.6741(26.9866) | Bit/dim 3.5544(3.5301) | Xent 0.0196(0.0260) | Loss 3.5642(3.5431) | Error 0.0067(0.0070) Steps 1108(1106.25) | Grad Norm 0.8489(0.9034) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 125.7267, Epoch Time 1631.0644(1578.9808), Bit/dim 3.5347(best: 3.5348), Xent 1.8260, Loss 4.4477, Error 0.2751(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 27.0743(26.9844) | Bit/dim 3.5412(3.5306) | Xent 0.0240(0.0249) | Loss 3.5532(3.5430) | Error 0.0056(0.0066) Steps 1108(1105.67) | Grad Norm 0.7005(0.8733) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 27.0874(27.0571) | Bit/dim 3.5539(3.5297) | Xent 0.0188(0.0246) | Loss 3.5633(3.5420) | Error 0.0044(0.0065) Steps 1114(1107.22) | Grad Norm 0.7804(0.8587) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 27.4224(27.1093) | Bit/dim 3.5496(3.5319) | Xent 0.0239(0.0250) | Loss 3.5615(3.5444) | Error 0.0078(0.0067) Steps 1102(1107.59) | Grad Norm 0.7264(0.8688) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 27.6548(27.1357) | Bit/dim 3.5111(3.5298) | Xent 0.0375(0.0249) | Loss 3.5299(3.5422) | Error 0.0089(0.0064) Steps 1090(1107.14) | Grad Norm 1.1497(0.8672) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 26.7992(27.1724) | Bit/dim 3.5163(3.5301) | Xent 0.0264(0.0251) | Loss 3.5295(3.5426) | Error 0.0100(0.0067) Steps 1102(1107.24) | Grad Norm 0.7804(0.8818) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 127.0725, Epoch Time 1638.7874(1580.7750), Bit/dim 3.5333(best: 3.5347), Xent 1.8391, Loss 4.4528, Error 0.2755(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 27.0624(27.1343) | Bit/dim 3.5190(3.5283) | Xent 0.0205(0.0247) | Loss 3.5293(3.5406) | Error 0.0078(0.0068) Steps 1102(1106.06) | Grad Norm 0.8098(0.8847) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 26.5796(27.1868) | Bit/dim 3.5400(3.5307) | Xent 0.0230(0.0244) | Loss 3.5515(3.5429) | Error 0.0078(0.0067) Steps 1132(1107.03) | Grad Norm 0.8954(0.8785) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 27.6259(27.2314) | Bit/dim 3.5464(3.5298) | Xent 0.0217(0.0242) | Loss 3.5572(3.5418) | Error 0.0044(0.0065) Steps 1120(1108.09) | Grad Norm 0.7698(0.8625) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 27.1689(27.3549) | Bit/dim 3.5261(3.5288) | Xent 0.0253(0.0243) | Loss 3.5387(3.5410) | Error 0.0056(0.0064) Steps 1084(1106.67) | Grad Norm 0.8624(0.8694) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 27.4861(27.3031) | Bit/dim 3.4842(3.5273) | Xent 0.0289(0.0247) | Loss 3.4986(3.5397) | Error 0.0100(0.0065) Steps 1102(1105.55) | Grad Norm 0.8874(0.8615) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 27.3521(27.2599) | Bit/dim 3.5232(3.5270) | Xent 0.0318(0.0254) | Loss 3.5391(3.5397) | Error 0.0089(0.0068) Steps 1108(1106.47) | Grad Norm 1.0263(0.8808) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 126.5454, Epoch Time 1647.5758(1582.7790), Bit/dim 3.5325(best: 3.5333), Xent 1.8254, Loss 4.4452, Error 0.2752(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.8839(27.1993) | Bit/dim 3.5188(3.5275) | Xent 0.0204(0.0247) | Loss 3.5290(3.5398) | Error 0.0056(0.0064) Steps 1102(1106.83) | Grad Norm 0.6635(0.8458) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 27.0685(27.2068) | Bit/dim 3.5137(3.5256) | Xent 0.0180(0.0243) | Loss 3.5227(3.5378) | Error 0.0033(0.0062) Steps 1096(1108.35) | Grad Norm 0.5802(0.8397) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 25.9392(27.1200) | Bit/dim 3.5445(3.5268) | Xent 0.0172(0.0239) | Loss 3.5531(3.5387) | Error 0.0022(0.0062) Steps 1102(1109.51) | Grad Norm 0.8472(0.8474) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 27.4107(27.0643) | Bit/dim 3.5363(3.5290) | Xent 0.0297(0.0243) | Loss 3.5512(3.5412) | Error 0.0067(0.0063) Steps 1126(1110.24) | Grad Norm 0.9502(0.8522) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 28.0504(27.0793) | Bit/dim 3.5351(3.5274) | Xent 0.0202(0.0237) | Loss 3.5452(3.5393) | Error 0.0056(0.0062) Steps 1126(1110.01) | Grad Norm 0.7021(0.8380) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 126.2922, Epoch Time 1632.2110(1584.2620), Bit/dim 3.5318(best: 3.5325), Xent 1.8461, Loss 4.4548, Error 0.2733(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 26.4925(27.1094) | Bit/dim 3.5246(3.5276) | Xent 0.0159(0.0235) | Loss 3.5326(3.5394) | Error 0.0011(0.0060) Steps 1120(1107.59) | Grad Norm 0.6785(0.8415) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 27.0623(27.1331) | Bit/dim 3.5083(3.5275) | Xent 0.0189(0.0231) | Loss 3.5177(3.5390) | Error 0.0056(0.0060) Steps 1090(1107.80) | Grad Norm 0.6752(0.8394) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 27.4693(27.1743) | Bit/dim 3.5195(3.5264) | Xent 0.0168(0.0225) | Loss 3.5280(3.5377) | Error 0.0033(0.0058) Steps 1132(1108.79) | Grad Norm 0.6011(0.8086) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 26.4200(27.1339) | Bit/dim 3.5049(3.5256) | Xent 0.0253(0.0235) | Loss 3.5176(3.5373) | Error 0.0067(0.0060) Steps 1096(1110.36) | Grad Norm 0.7530(0.8301) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 26.3684(27.0806) | Bit/dim 3.5431(3.5270) | Xent 0.0241(0.0243) | Loss 3.5552(3.5391) | Error 0.0044(0.0064) Steps 1132(1111.94) | Grad Norm 0.8056(0.8407) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 27.8643(27.1385) | Bit/dim 3.5273(3.5265) | Xent 0.0302(0.0247) | Loss 3.5424(3.5389) | Error 0.0067(0.0064) Steps 1108(1111.00) | Grad Norm 1.1016(0.8520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 125.8637, Epoch Time 1637.2212(1585.8508), Bit/dim 3.5309(best: 3.5318), Xent 1.8646, Loss 4.4632, Error 0.2717(best: 0.2727)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 26.8260(27.1073) | Bit/dim 3.5229(3.5271) | Xent 0.0169(0.0234) | Loss 3.5313(3.5388) | Error 0.0033(0.0057) Steps 1114(1110.21) | Grad Norm 0.7233(0.8386) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 27.7431(27.1290) | Bit/dim 3.5157(3.5258) | Xent 0.0239(0.0237) | Loss 3.5277(3.5377) | Error 0.0056(0.0058) Steps 1108(1109.19) | Grad Norm 0.7491(0.8550) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 27.6512(27.1465) | Bit/dim 3.5156(3.5250) | Xent 0.0284(0.0242) | Loss 3.5298(3.5371) | Error 0.0078(0.0060) Steps 1120(1110.54) | Grad Norm 0.9216(0.8685) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 27.2212(27.0749) | Bit/dim 3.5506(3.5277) | Xent 0.0262(0.0240) | Loss 3.5637(3.5397) | Error 0.0078(0.0060) Steps 1114(1109.11) | Grad Norm 0.8259(0.8696) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 27.8131(27.1348) | Bit/dim 3.5426(3.5263) | Xent 0.0224(0.0239) | Loss 3.5538(3.5382) | Error 0.0056(0.0059) Steps 1120(1108.53) | Grad Norm 0.9519(0.8719) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 126.6491, Epoch Time 1635.7612(1587.3481), Bit/dim 3.5308(best: 3.5309), Xent 1.8496, Loss 4.4556, Error 0.2745(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 26.3757(27.1195) | Bit/dim 3.5227(3.5263) | Xent 0.0225(0.0236) | Loss 3.5340(3.5381) | Error 0.0067(0.0059) Steps 1114(1107.78) | Grad Norm 0.8741(0.8640) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 27.0983(27.0702) | Bit/dim 3.5300(3.5272) | Xent 0.0177(0.0235) | Loss 3.5388(3.5389) | Error 0.0033(0.0060) Steps 1132(1109.22) | Grad Norm 0.7802(0.8549) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 28.2038(27.0854) | Bit/dim 3.5586(3.5268) | Xent 0.0290(0.0234) | Loss 3.5731(3.5384) | Error 0.0100(0.0058) Steps 1108(1107.97) | Grad Norm 0.8892(0.8552) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 27.5654(27.2066) | Bit/dim 3.5153(3.5262) | Xent 0.0306(0.0234) | Loss 3.5306(3.5378) | Error 0.0122(0.0061) Steps 1084(1106.22) | Grad Norm 1.0788(0.8795) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 26.9463(27.1491) | Bit/dim 3.5506(3.5273) | Xent 0.0240(0.0230) | Loss 3.5626(3.5388) | Error 0.0044(0.0058) Steps 1126(1107.17) | Grad Norm 0.9451(0.8699) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 26.7898(27.2458) | Bit/dim 3.4998(3.5251) | Xent 0.0181(0.0224) | Loss 3.5088(3.5363) | Error 0.0022(0.0054) Steps 1126(1109.19) | Grad Norm 0.8037(0.8595) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 126.7854, Epoch Time 1641.0617(1588.9595), Bit/dim 3.5319(best: 3.5308), Xent 1.8713, Loss 4.4675, Error 0.2765(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 27.6690(27.2530) | Bit/dim 3.5350(3.5259) | Xent 0.0297(0.0229) | Loss 3.5499(3.5373) | Error 0.0056(0.0056) Steps 1102(1110.01) | Grad Norm 0.8731(0.8532) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 27.9621(27.2503) | Bit/dim 3.5101(3.5259) | Xent 0.0205(0.0232) | Loss 3.5204(3.5375) | Error 0.0067(0.0058) Steps 1108(1108.91) | Grad Norm 0.7910(0.8686) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 26.6768(27.1884) | Bit/dim 3.5118(3.5273) | Xent 0.0185(0.0233) | Loss 3.5211(3.5389) | Error 0.0044(0.0058) Steps 1114(1112.27) | Grad Norm 0.6513(0.8610) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 27.7647(27.2500) | Bit/dim 3.5215(3.5271) | Xent 0.0243(0.0234) | Loss 3.5337(3.5388) | Error 0.0078(0.0059) Steps 1114(1113.56) | Grad Norm 0.8694(0.8942) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 27.2146(27.2551) | Bit/dim 3.4937(3.5226) | Xent 0.0247(0.0228) | Loss 3.5060(3.5340) | Error 0.0056(0.0055) Steps 1108(1113.68) | Grad Norm 1.2998(0.9071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 127.3509, Epoch Time 1642.8716(1590.5769), Bit/dim 3.5305(best: 3.5308), Xent 1.8728, Loss 4.4669, Error 0.2760(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 27.2068(27.2193) | Bit/dim 3.5388(3.5252) | Xent 0.0252(0.0229) | Loss 3.5514(3.5367) | Error 0.0056(0.0055) Steps 1120(1112.47) | Grad Norm 1.2138(0.9236) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 26.6927(27.1451) | Bit/dim 3.5093(3.5247) | Xent 0.0258(0.0224) | Loss 3.5222(3.5359) | Error 0.0089(0.0055) Steps 1108(1111.38) | Grad Norm 1.2318(0.9070) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 26.9002(27.1477) | Bit/dim 3.5287(3.5263) | Xent 0.0232(0.0227) | Loss 3.5403(3.5376) | Error 0.0089(0.0057) Steps 1114(1111.73) | Grad Norm 1.1594(0.9109) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 28.0738(27.1187) | Bit/dim 3.5623(3.5262) | Xent 0.0258(0.0235) | Loss 3.5752(3.5380) | Error 0.0078(0.0061) Steps 1144(1114.80) | Grad Norm 1.0093(0.9185) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 28.4544(27.1652) | Bit/dim 3.5327(3.5237) | Xent 0.0223(0.0227) | Loss 3.5439(3.5351) | Error 0.0056(0.0057) Steps 1114(1115.77) | Grad Norm 0.7643(0.8799) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 27.0757(27.1869) | Bit/dim 3.5465(3.5232) | Xent 0.0182(0.0222) | Loss 3.5556(3.5343) | Error 0.0022(0.0052) Steps 1108(1114.55) | Grad Norm 0.7439(0.8646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 127.0408, Epoch Time 1636.9386(1591.9677), Bit/dim 3.5284(best: 3.5305), Xent 1.8723, Loss 4.4646, Error 0.2778(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 27.4746(27.2585) | Bit/dim 3.5000(3.5215) | Xent 0.0282(0.0225) | Loss 3.5141(3.5328) | Error 0.0056(0.0053) Steps 1108(1114.74) | Grad Norm 0.8255(0.8691) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 26.6572(27.2588) | Bit/dim 3.4922(3.5212) | Xent 0.0255(0.0219) | Loss 3.5049(3.5321) | Error 0.0067(0.0049) Steps 1096(1113.85) | Grad Norm 1.0059(0.8540) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 26.7038(27.1912) | Bit/dim 3.5270(3.5226) | Xent 0.0215(0.0218) | Loss 3.5378(3.5335) | Error 0.0056(0.0049) Steps 1114(1112.89) | Grad Norm 0.6900(0.8299) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 27.7941(27.1982) | Bit/dim 3.5377(3.5242) | Xent 0.0175(0.0220) | Loss 3.5465(3.5352) | Error 0.0033(0.0052) Steps 1108(1113.61) | Grad Norm 0.6660(0.8307) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 27.3129(27.1247) | Bit/dim 3.5517(3.5235) | Xent 0.0220(0.0224) | Loss 3.5627(3.5347) | Error 0.0044(0.0055) Steps 1132(1113.55) | Grad Norm 0.7579(0.8498) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 126.4486, Epoch Time 1640.2915(1593.4174), Bit/dim 3.5290(best: 3.5284), Xent 1.8915, Loss 4.4748, Error 0.2783(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 27.1109(27.1514) | Bit/dim 3.5041(3.5236) | Xent 0.0217(0.0225) | Loss 3.5150(3.5348) | Error 0.0033(0.0055) Steps 1126(1113.13) | Grad Norm 0.7088(0.8339) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 26.6341(27.2066) | Bit/dim 3.5329(3.5244) | Xent 0.0231(0.0226) | Loss 3.5445(3.5356) | Error 0.0056(0.0055) Steps 1114(1114.71) | Grad Norm 0.8444(0.8571) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 27.2047(27.1694) | Bit/dim 3.5029(3.5237) | Xent 0.0184(0.0222) | Loss 3.5120(3.5348) | Error 0.0011(0.0052) Steps 1114(1114.34) | Grad Norm 0.7293(0.8543) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 27.5886(27.2189) | Bit/dim 3.5371(3.5247) | Xent 0.0231(0.0224) | Loss 3.5487(3.5359) | Error 0.0033(0.0052) Steps 1120(1115.86) | Grad Norm 0.8670(0.8725) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 26.7146(27.1992) | Bit/dim 3.5100(3.5254) | Xent 0.0202(0.0226) | Loss 3.5201(3.5367) | Error 0.0044(0.0053) Steps 1102(1114.97) | Grad Norm 0.7930(0.8759) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 27.5312(27.2132) | Bit/dim 3.5264(3.5240) | Xent 0.0168(0.0223) | Loss 3.5347(3.5352) | Error 0.0033(0.0053) Steps 1120(1113.52) | Grad Norm 0.7216(0.8765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 126.8825, Epoch Time 1642.4783(1594.8893), Bit/dim 3.5293(best: 3.5284), Xent 1.9013, Loss 4.4799, Error 0.2766(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 26.5177(27.1964) | Bit/dim 3.5086(3.5243) | Xent 0.0199(0.0223) | Loss 3.5186(3.5355) | Error 0.0022(0.0053) Steps 1114(1113.39) | Grad Norm 0.5716(0.8610) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 27.5270(27.1943) | Bit/dim 3.5406(3.5241) | Xent 0.0207(0.0223) | Loss 3.5510(3.5353) | Error 0.0056(0.0053) Steps 1132(1114.79) | Grad Norm 0.9031(0.8862) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 26.7674(27.1896) | Bit/dim 3.5182(3.5256) | Xent 0.0183(0.0216) | Loss 3.5273(3.5364) | Error 0.0022(0.0049) Steps 1108(1114.40) | Grad Norm 0.6187(0.8622) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 27.8748(27.2461) | Bit/dim 3.5205(3.5255) | Xent 0.0198(0.0212) | Loss 3.5304(3.5360) | Error 0.0033(0.0047) Steps 1084(1112.81) | Grad Norm 0.7507(0.8402) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 26.7221(27.2092) | Bit/dim 3.5327(3.5229) | Xent 0.0214(0.0211) | Loss 3.5434(3.5335) | Error 0.0056(0.0048) Steps 1108(1112.36) | Grad Norm 0.8415(0.8370) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 127.0740, Epoch Time 1640.4041(1596.2547), Bit/dim 3.5275(best: 3.5284), Xent 1.9202, Loss 4.4875, Error 0.2828(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 27.2573(27.1519) | Bit/dim 3.5116(3.5232) | Xent 0.0229(0.0211) | Loss 3.5231(3.5338) | Error 0.0067(0.0047) Steps 1090(1112.16) | Grad Norm 1.0158(0.8506) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 27.3483(27.1424) | Bit/dim 3.5086(3.5239) | Xent 0.0185(0.0208) | Loss 3.5179(3.5343) | Error 0.0022(0.0044) Steps 1096(1111.92) | Grad Norm 0.7215(0.8241) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 26.9890(27.0978) | Bit/dim 3.5457(3.5270) | Xent 0.0224(0.0216) | Loss 3.5569(3.5378) | Error 0.0056(0.0048) Steps 1102(1111.84) | Grad Norm 0.9867(0.8794) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 27.1168(27.1457) | Bit/dim 3.5194(3.5234) | Xent 0.0274(0.0222) | Loss 3.5332(3.5344) | Error 0.0067(0.0050) Steps 1096(1112.50) | Grad Norm 0.9945(0.8977) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 27.0184(27.1928) | Bit/dim 3.4975(3.5217) | Xent 0.0193(0.0221) | Loss 3.5071(3.5327) | Error 0.0044(0.0051) Steps 1108(1113.70) | Grad Norm 0.6540(0.8967) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 27.3078(27.1292) | Bit/dim 3.5357(3.5213) | Xent 0.0234(0.0224) | Loss 3.5474(3.5325) | Error 0.0044(0.0054) Steps 1114(1114.42) | Grad Norm 1.1631(0.9159) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 127.5531, Epoch Time 1636.7556(1597.4697), Bit/dim 3.5273(best: 3.5275), Xent 1.8912, Loss 4.4729, Error 0.2774(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 27.4457(27.1635) | Bit/dim 3.4963(3.5216) | Xent 0.0208(0.0218) | Loss 3.5067(3.5325) | Error 0.0044(0.0053) Steps 1102(1114.21) | Grad Norm 0.7479(0.8981) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 27.7672(27.1744) | Bit/dim 3.5226(3.5224) | Xent 0.0202(0.0222) | Loss 3.5327(3.5335) | Error 0.0089(0.0055) Steps 1096(1112.34) | Grad Norm 0.6721(0.8900) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 27.3399(27.2166) | Bit/dim 3.5343(3.5214) | Xent 0.0214(0.0223) | Loss 3.5450(3.5326) | Error 0.0056(0.0057) Steps 1114(1113.38) | Grad Norm 0.8397(0.8912) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 27.1887(27.2471) | Bit/dim 3.5360(3.5245) | Xent 0.0327(0.0222) | Loss 3.5524(3.5356) | Error 0.0089(0.0054) Steps 1120(1113.35) | Grad Norm 0.9514(0.8722) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 26.7643(27.1805) | Bit/dim 3.5286(3.5216) | Xent 0.0146(0.0218) | Loss 3.5359(3.5324) | Error 0.0033(0.0056) Steps 1120(1113.37) | Grad Norm 0.6881(0.8649) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 127.5001, Epoch Time 1640.2731(1598.7538), Bit/dim 3.5279(best: 3.5273), Xent 1.8931, Loss 4.4745, Error 0.2757(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 27.7912(27.1690) | Bit/dim 3.5398(3.5213) | Xent 0.0206(0.0216) | Loss 3.5501(3.5321) | Error 0.0067(0.0055) Steps 1114(1113.34) | Grad Norm 0.8765(0.8881) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 26.5842(27.0871) | Bit/dim 3.5258(3.5201) | Xent 0.0236(0.0214) | Loss 3.5376(3.5308) | Error 0.0067(0.0052) Steps 1108(1111.30) | Grad Norm 1.0599(0.8851) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 26.4995(27.1078) | Bit/dim 3.5144(3.5223) | Xent 0.0235(0.0210) | Loss 3.5261(3.5328) | Error 0.0056(0.0048) Steps 1102(1111.56) | Grad Norm 0.9621(0.9022) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 27.1103(27.1037) | Bit/dim 3.5283(3.5247) | Xent 0.0193(0.0209) | Loss 3.5380(3.5351) | Error 0.0033(0.0048) Steps 1120(1113.20) | Grad Norm 0.8785(0.8972) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 27.3078(27.1205) | Bit/dim 3.5297(3.5241) | Xent 0.0193(0.0204) | Loss 3.5394(3.5343) | Error 0.0044(0.0047) Steps 1108(1113.56) | Grad Norm 0.9515(0.8795) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 27.5546(27.1373) | Bit/dim 3.5219(3.5212) | Xent 0.0357(0.0211) | Loss 3.5398(3.5317) | Error 0.0133(0.0049) Steps 1108(1113.77) | Grad Norm 1.6884(0.8927) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 125.8785, Epoch Time 1635.3526(1599.8518), Bit/dim 3.5258(best: 3.5273), Xent 1.9272, Loss 4.4894, Error 0.2780(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 26.4309(27.1121) | Bit/dim 3.5332(3.5214) | Xent 0.0221(0.0209) | Loss 3.5442(3.5318) | Error 0.0056(0.0050) Steps 1096(1112.45) | Grad Norm 0.6956(0.8728) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 27.4606(27.2303) | Bit/dim 3.5036(3.5197) | Xent 0.0180(0.0209) | Loss 3.5126(3.5301) | Error 0.0044(0.0048) Steps 1126(1112.58) | Grad Norm 0.7104(0.8685) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 27.5272(27.2590) | Bit/dim 3.5089(3.5184) | Xent 0.0140(0.0206) | Loss 3.5159(3.5287) | Error 0.0033(0.0047) Steps 1114(1112.92) | Grad Norm 0.7251(0.8827) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 26.8559(27.2270) | Bit/dim 3.5135(3.5208) | Xent 0.0232(0.0208) | Loss 3.5251(3.5312) | Error 0.0056(0.0048) Steps 1108(1111.41) | Grad Norm 1.3864(0.9004) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 27.2027(27.2310) | Bit/dim 3.5584(3.5239) | Xent 0.0200(0.0204) | Loss 3.5683(3.5340) | Error 0.0044(0.0048) Steps 1114(1112.15) | Grad Norm 0.9552(0.8925) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 126.5532, Epoch Time 1642.7702(1601.1393), Bit/dim 3.5256(best: 3.5258), Xent 1.9410, Loss 4.4961, Error 0.2799(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 27.3425(27.1879) | Bit/dim 3.5411(3.5230) | Xent 0.0159(0.0204) | Loss 3.5490(3.5332) | Error 0.0011(0.0047) Steps 1114(1111.81) | Grad Norm 0.6942(0.8909) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 27.3315(27.1518) | Bit/dim 3.5311(3.5215) | Xent 0.0272(0.0206) | Loss 3.5447(3.5318) | Error 0.0067(0.0046) Steps 1132(1111.83) | Grad Norm 1.2084(0.8813) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 27.5813(27.2152) | Bit/dim 3.5448(3.5237) | Xent 0.0166(0.0206) | Loss 3.5531(3.5340) | Error 0.0022(0.0045) Steps 1120(1112.06) | Grad Norm 1.0825(0.9149) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 27.6662(27.2369) | Bit/dim 3.5190(3.5236) | Xent 0.0212(0.0209) | Loss 3.5296(3.5340) | Error 0.0033(0.0046) Steps 1150(1113.20) | Grad Norm 0.7739(0.9031) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 27.8470(27.2645) | Bit/dim 3.5055(3.5190) | Xent 0.0253(0.0207) | Loss 3.5181(3.5294) | Error 0.0056(0.0047) Steps 1120(1112.32) | Grad Norm 0.9719(0.8865) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 27.4617(27.2837) | Bit/dim 3.5391(3.5200) | Xent 0.0224(0.0205) | Loss 3.5504(3.5303) | Error 0.0033(0.0046) Steps 1108(1111.92) | Grad Norm 0.9814(0.8784) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 125.7428, Epoch Time 1643.3878(1602.4068), Bit/dim 3.5256(best: 3.5256), Xent 1.9255, Loss 4.4883, Error 0.2808(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 28.1995(27.2670) | Bit/dim 3.5402(3.5179) | Xent 0.0140(0.0203) | Loss 3.5472(3.5280) | Error 0.0044(0.0046) Steps 1144(1113.37) | Grad Norm 0.7020(0.8593) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 26.7416(27.2150) | Bit/dim 3.5349(3.5201) | Xent 0.0225(0.0210) | Loss 3.5461(3.5306) | Error 0.0078(0.0049) Steps 1108(1113.87) | Grad Norm 0.8282(0.8690) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 26.9586(27.1820) | Bit/dim 3.5429(3.5200) | Xent 0.0136(0.0208) | Loss 3.5497(3.5304) | Error 0.0011(0.0047) Steps 1102(1112.61) | Grad Norm 0.8169(0.8683) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 26.6484(27.1773) | Bit/dim 3.5089(3.5195) | Xent 0.0198(0.0210) | Loss 3.5188(3.5300) | Error 0.0067(0.0049) Steps 1114(1112.89) | Grad Norm 0.8741(0.8787) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 27.3556(27.2226) | Bit/dim 3.5319(3.5216) | Xent 0.0199(0.0211) | Loss 3.5419(3.5321) | Error 0.0056(0.0051) Steps 1126(1112.17) | Grad Norm 0.7584(0.9095) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 125.6544, Epoch Time 1638.5684(1603.4916), Bit/dim 3.5265(best: 3.5256), Xent 1.9116, Loss 4.4823, Error 0.2795(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 26.4981(27.1434) | Bit/dim 3.5351(3.5215) | Xent 0.0216(0.0207) | Loss 3.5459(3.5318) | Error 0.0067(0.0049) Steps 1120(1112.04) | Grad Norm 1.0428(0.9066) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 26.6147(27.1388) | Bit/dim 3.4915(3.5183) | Xent 0.0169(0.0207) | Loss 3.5000(3.5287) | Error 0.0033(0.0050) Steps 1108(1111.10) | Grad Norm 0.8466(0.9011) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 26.3919(27.1802) | Bit/dim 3.5349(3.5181) | Xent 0.0294(0.0221) | Loss 3.5496(3.5292) | Error 0.0078(0.0055) Steps 1120(1114.16) | Grad Norm 1.3921(0.9660) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 27.2343(27.1148) | Bit/dim 3.5171(3.5191) | Xent 0.0185(0.0214) | Loss 3.5264(3.5298) | Error 0.0056(0.0053) Steps 1120(1113.67) | Grad Norm 0.9824(0.9425) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 26.7642(27.0675) | Bit/dim 3.5311(3.5210) | Xent 0.0267(0.0213) | Loss 3.5445(3.5316) | Error 0.0089(0.0052) Steps 1108(1112.42) | Grad Norm 1.0851(0.9475) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 27.7663(27.0718) | Bit/dim 3.5439(3.5222) | Xent 0.0186(0.0208) | Loss 3.5532(3.5326) | Error 0.0022(0.0048) Steps 1126(1112.10) | Grad Norm 0.7848(0.9171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 127.6828, Epoch Time 1633.2710(1604.3850), Bit/dim 3.5254(best: 3.5256), Xent 1.9315, Loss 4.4912, Error 0.2809(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 27.3775(27.0982) | Bit/dim 3.5260(3.5202) | Xent 0.0164(0.0206) | Loss 3.5342(3.5305) | Error 0.0022(0.0048) Steps 1108(1113.01) | Grad Norm 0.6873(0.8978) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 27.0768(27.0929) | Bit/dim 3.5541(3.5199) | Xent 0.0171(0.0194) | Loss 3.5627(3.5296) | Error 0.0022(0.0043) Steps 1108(1113.12) | Grad Norm 0.7942(0.8481) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 27.5865(27.1523) | Bit/dim 3.5129(3.5227) | Xent 0.0193(0.0202) | Loss 3.5226(3.5328) | Error 0.0022(0.0046) Steps 1102(1114.52) | Grad Norm 0.7643(0.9040) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 26.6162(27.1475) | Bit/dim 3.5302(3.5208) | Xent 0.0240(0.0205) | Loss 3.5422(3.5310) | Error 0.0056(0.0047) Steps 1120(1113.98) | Grad Norm 1.4114(0.9658) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 27.1701(27.1437) | Bit/dim 3.5388(3.5208) | Xent 0.0215(0.0199) | Loss 3.5495(3.5308) | Error 0.0067(0.0045) Steps 1096(1113.06) | Grad Norm 1.0220(0.9881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 125.9974, Epoch Time 1638.4643(1605.4074), Bit/dim 3.5245(best: 3.5254), Xent 1.9527, Loss 4.5009, Error 0.2800(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 26.8089(27.1363) | Bit/dim 3.5255(3.5209) | Xent 0.0208(0.0200) | Loss 3.5359(3.5309) | Error 0.0033(0.0043) Steps 1108(1112.49) | Grad Norm 0.8396(1.0136) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 26.6954(27.1487) | Bit/dim 3.5273(3.5205) | Xent 0.0237(0.0204) | Loss 3.5391(3.5307) | Error 0.0078(0.0044) Steps 1126(1112.03) | Grad Norm 0.8832(0.9914) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 26.8531(27.1299) | Bit/dim 3.4889(3.5186) | Xent 0.0181(0.0200) | Loss 3.4980(3.5286) | Error 0.0044(0.0042) Steps 1102(1111.77) | Grad Norm 0.7947(0.9689) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 27.5260(27.1810) | Bit/dim 3.5076(3.5188) | Xent 0.0172(0.0201) | Loss 3.5162(3.5288) | Error 0.0033(0.0045) Steps 1108(1113.24) | Grad Norm 0.6863(0.9697) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 27.6360(27.1748) | Bit/dim 3.5384(3.5207) | Xent 0.0180(0.0196) | Loss 3.5474(3.5305) | Error 0.0033(0.0044) Steps 1108(1112.06) | Grad Norm 0.8067(0.9299) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 26.3629(27.1857) | Bit/dim 3.4972(3.5201) | Xent 0.0153(0.0205) | Loss 3.5049(3.5304) | Error 0.0022(0.0048) Steps 1096(1109.99) | Grad Norm 0.8562(0.9657) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 126.0546, Epoch Time 1638.8345(1606.4102), Bit/dim 3.5247(best: 3.5245), Xent 1.9538, Loss 4.5016, Error 0.2799(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 27.6437(27.1747) | Bit/dim 3.5112(3.5216) | Xent 0.0167(0.0201) | Loss 3.5195(3.5317) | Error 0.0011(0.0047) Steps 1114(1112.53) | Grad Norm 0.8120(0.9200) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 26.2862(27.0882) | Bit/dim 3.5206(3.5212) | Xent 0.0156(0.0197) | Loss 3.5284(3.5311) | Error 0.0000(0.0044) Steps 1114(1112.74) | Grad Norm 0.6372(0.9113) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 27.0177(27.1164) | Bit/dim 3.5440(3.5208) | Xent 0.0154(0.0198) | Loss 3.5517(3.5307) | Error 0.0033(0.0046) Steps 1132(1115.62) | Grad Norm 0.8839(0.8999) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 27.0739(27.1401) | Bit/dim 3.4490(3.5145) | Xent 0.0263(0.0202) | Loss 3.4622(3.5246) | Error 0.0067(0.0047) Steps 1102(1113.85) | Grad Norm 1.1520(0.8979) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 27.1824(27.1587) | Bit/dim 3.5629(3.5191) | Xent 0.0286(0.0205) | Loss 3.5772(3.5293) | Error 0.0089(0.0047) Steps 1114(1113.46) | Grad Norm 1.1297(0.9220) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 126.2953, Epoch Time 1638.1942(1607.3637), Bit/dim 3.5241(best: 3.5245), Xent 1.9752, Loss 4.5117, Error 0.2799(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 27.0269(27.1545) | Bit/dim 3.5565(3.5201) | Xent 0.0216(0.0204) | Loss 3.5673(3.5303) | Error 0.0056(0.0045) Steps 1120(1114.17) | Grad Norm 1.1043(0.9648) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 27.4754(27.0911) | Bit/dim 3.5051(3.5198) | Xent 0.0217(0.0206) | Loss 3.5160(3.5301) | Error 0.0044(0.0046) Steps 1132(1113.34) | Grad Norm 0.8601(1.0150) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 27.3000(27.1408) | Bit/dim 3.5334(3.5193) | Xent 0.0206(0.0206) | Loss 3.5437(3.5296) | Error 0.0044(0.0047) Steps 1114(1113.37) | Grad Norm 0.7479(1.0101) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 27.6910(27.1967) | Bit/dim 3.5055(3.5192) | Xent 0.0155(0.0200) | Loss 3.5132(3.5293) | Error 0.0044(0.0046) Steps 1108(1112.11) | Grad Norm 0.7810(0.9607) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 26.5564(27.2148) | Bit/dim 3.5297(3.5168) | Xent 0.0145(0.0210) | Loss 3.5370(3.5273) | Error 0.0011(0.0051) Steps 1114(1112.82) | Grad Norm 0.6787(0.9724) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 27.1048(27.1963) | Bit/dim 3.5254(3.5195) | Xent 0.0214(0.0201) | Loss 3.5361(3.5296) | Error 0.0044(0.0046) Steps 1114(1114.07) | Grad Norm 0.7072(0.9235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 125.8296, Epoch Time 1637.1261(1608.2566), Bit/dim 3.5232(best: 3.5241), Xent 1.9768, Loss 4.5116, Error 0.2821(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 27.1900(27.1608) | Bit/dim 3.5500(3.5214) | Xent 0.0160(0.0204) | Loss 3.5580(3.5316) | Error 0.0033(0.0047) Steps 1126(1115.82) | Grad Norm 0.7264(0.9204) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 27.5400(27.1330) | Bit/dim 3.5073(3.5198) | Xent 0.0159(0.0206) | Loss 3.5153(3.5301) | Error 0.0044(0.0050) Steps 1132(1114.99) | Grad Norm 0.8797(0.9144) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 27.5344(27.1061) | Bit/dim 3.5287(3.5196) | Xent 0.0210(0.0209) | Loss 3.5393(3.5300) | Error 0.0056(0.0051) Steps 1126(1114.72) | Grad Norm 1.1469(0.9552) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 28.0621(27.1433) | Bit/dim 3.5167(3.5198) | Xent 0.0279(0.0213) | Loss 3.5306(3.5304) | Error 0.0078(0.0051) Steps 1114(1115.14) | Grad Norm 1.9655(1.0162) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 27.0376(27.1302) | Bit/dim 3.5306(3.5180) | Xent 0.0217(0.0216) | Loss 3.5414(3.5288) | Error 0.0044(0.0054) Steps 1108(1113.58) | Grad Norm 1.1008(1.0128) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 127.2917, Epoch Time 1635.8136(1609.0833), Bit/dim 3.5235(best: 3.5232), Xent 1.9506, Loss 4.4988, Error 0.2785(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 27.6176(27.1453) | Bit/dim 3.5532(3.5188) | Xent 0.0234(0.0211) | Loss 3.5648(3.5294) | Error 0.0056(0.0052) Steps 1108(1112.54) | Grad Norm 0.9901(1.0057) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 26.8684(27.1153) | Bit/dim 3.5196(3.5188) | Xent 0.0202(0.0206) | Loss 3.5297(3.5291) | Error 0.0033(0.0049) Steps 1108(1112.16) | Grad Norm 0.6594(0.9446) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 27.2707(27.1089) | Bit/dim 3.5300(3.5164) | Xent 0.0176(0.0209) | Loss 3.5387(3.5268) | Error 0.0044(0.0049) Steps 1132(1113.99) | Grad Norm 0.8180(0.9705) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 27.9133(27.1088) | Bit/dim 3.5186(3.5171) | Xent 0.0153(0.0209) | Loss 3.5262(3.5275) | Error 0.0011(0.0049) Steps 1114(1113.57) | Grad Norm 0.6273(0.9596) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 26.7686(27.1563) | Bit/dim 3.5106(3.5182) | Xent 0.0146(0.0205) | Loss 3.5179(3.5285) | Error 0.0033(0.0048) Steps 1114(1113.03) | Grad Norm 0.8856(0.9396) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 27.6394(27.1859) | Bit/dim 3.5304(3.5192) | Xent 0.0230(0.0205) | Loss 3.5420(3.5294) | Error 0.0067(0.0050) Steps 1096(1111.90) | Grad Norm 1.2963(0.9484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 128.1596, Epoch Time 1640.2351(1610.0179), Bit/dim 3.5232(best: 3.5232), Xent 1.9736, Loss 4.5100, Error 0.2846(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 27.6401(27.1482) | Bit/dim 3.4812(3.5171) | Xent 0.0199(0.0204) | Loss 3.4911(3.5273) | Error 0.0044(0.0048) Steps 1084(1110.80) | Grad Norm 0.9936(0.9445) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 26.5325(27.2094) | Bit/dim 3.5159(3.5182) | Xent 0.0267(0.0201) | Loss 3.5292(3.5282) | Error 0.0078(0.0045) Steps 1102(1112.20) | Grad Norm 1.0032(0.9481) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 27.7461(27.2715) | Bit/dim 3.5139(3.5196) | Xent 0.0185(0.0202) | Loss 3.5232(3.5298) | Error 0.0044(0.0048) Steps 1138(1114.83) | Grad Norm 0.7854(0.9466) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 26.5413(27.2182) | Bit/dim 3.5219(3.5184) | Xent 0.0211(0.0203) | Loss 3.5325(3.5285) | Error 0.0056(0.0050) Steps 1120(1114.73) | Grad Norm 1.2405(0.9521) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 27.2038(27.1857) | Bit/dim 3.5065(3.5178) | Xent 0.0185(0.0199) | Loss 3.5157(3.5277) | Error 0.0044(0.0047) Steps 1126(1114.64) | Grad Norm 1.0273(0.9385) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 128.2326, Epoch Time 1643.0220(1611.0080), Bit/dim 3.5220(best: 3.5232), Xent 1.9844, Loss 4.5141, Error 0.2796(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 27.1675(27.1672) | Bit/dim 3.5223(3.5164) | Xent 0.0210(0.0202) | Loss 3.5328(3.5265) | Error 0.0033(0.0047) Steps 1114(1115.71) | Grad Norm 0.8982(0.9598) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 27.3560(27.0391) | Bit/dim 3.5045(3.5167) | Xent 0.0168(0.0199) | Loss 3.5129(3.5267) | Error 0.0011(0.0044) Steps 1132(1113.77) | Grad Norm 0.6577(0.9444) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 26.6878(27.0330) | Bit/dim 3.4919(3.5192) | Xent 0.0176(0.0196) | Loss 3.5007(3.5290) | Error 0.0056(0.0045) Steps 1108(1114.06) | Grad Norm 0.7646(0.9181) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 26.1626(26.9727) | Bit/dim 3.5013(3.5182) | Xent 0.0169(0.0195) | Loss 3.5097(3.5280) | Error 0.0033(0.0046) Steps 1102(1113.03) | Grad Norm 0.7970(0.9336) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 27.1945(27.0164) | Bit/dim 3.5210(3.5187) | Xent 0.0191(0.0196) | Loss 3.5306(3.5285) | Error 0.0033(0.0046) Steps 1120(1112.34) | Grad Norm 0.8522(0.9298) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 27.1265(27.0609) | Bit/dim 3.5049(3.5175) | Xent 0.0188(0.0211) | Loss 3.5143(3.5280) | Error 0.0022(0.0049) Steps 1120(1112.40) | Grad Norm 1.0930(0.9808) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 127.3780, Epoch Time 1628.8842(1611.5443), Bit/dim 3.5230(best: 3.5220), Xent 1.9809, Loss 4.5134, Error 0.2721(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 26.8640(27.0429) | Bit/dim 3.5517(3.5185) | Xent 0.0233(0.0209) | Loss 3.5634(3.5289) | Error 0.0067(0.0051) Steps 1114(1112.03) | Grad Norm 0.8924(0.9891) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 27.4165(27.1264) | Bit/dim 3.5014(3.5180) | Xent 0.0252(0.0214) | Loss 3.5140(3.5288) | Error 0.0067(0.0053) Steps 1108(1112.04) | Grad Norm 0.9530(1.0378) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 27.6491(27.2167) | Bit/dim 3.5215(3.5178) | Xent 0.0157(0.0209) | Loss 3.5294(3.5283) | Error 0.0033(0.0050) Steps 1102(1111.69) | Grad Norm 0.9169(1.0384) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 27.8233(27.2071) | Bit/dim 3.5381(3.5190) | Xent 0.0256(0.0216) | Loss 3.5509(3.5298) | Error 0.0044(0.0052) Steps 1126(1112.04) | Grad Norm 1.4453(1.0723) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 27.1163(27.1866) | Bit/dim 3.5058(3.5174) | Xent 0.0225(0.0210) | Loss 3.5170(3.5279) | Error 0.0056(0.0049) Steps 1114(1112.10) | Grad Norm 0.9117(1.0363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 126.2611, Epoch Time 1641.4795(1612.4423), Bit/dim 3.5217(best: 3.5220), Xent 1.9678, Loss 4.5056, Error 0.2809(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.7650(27.2179) | Bit/dim 3.5018(3.5178) | Xent 0.0129(0.0208) | Loss 3.5083(3.5282) | Error 0.0022(0.0047) Steps 1102(1112.25) | Grad Norm 0.6854(0.9998) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 26.9117(27.2641) | Bit/dim 3.5349(3.5197) | Xent 0.0149(0.0197) | Loss 3.5423(3.5296) | Error 0.0033(0.0042) Steps 1108(1113.18) | Grad Norm 0.6365(0.9448) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 26.9597(27.2510) | Bit/dim 3.5090(3.5192) | Xent 0.0261(0.0199) | Loss 3.5221(3.5292) | Error 0.0067(0.0043) Steps 1126(1112.87) | Grad Norm 1.0525(0.9322) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 27.9078(27.2662) | Bit/dim 3.5622(3.5201) | Xent 0.0131(0.0193) | Loss 3.5687(3.5297) | Error 0.0022(0.0041) Steps 1120(1113.42) | Grad Norm 0.6380(0.8983) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 27.2295(27.2581) | Bit/dim 3.5007(3.5166) | Xent 0.0250(0.0198) | Loss 3.5132(3.5265) | Error 0.0100(0.0044) Steps 1120(1112.97) | Grad Norm 1.0533(0.9016) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 27.8591(27.2952) | Bit/dim 3.5367(3.5161) | Xent 0.0260(0.0204) | Loss 3.5497(3.5263) | Error 0.0078(0.0048) Steps 1108(1113.92) | Grad Norm 1.0243(0.9251) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 125.7665, Epoch Time 1646.0646(1613.4510), Bit/dim 3.5238(best: 3.5217), Xent 2.0120, Loss 4.5298, Error 0.2794(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 27.6913(27.2742) | Bit/dim 3.5206(3.5160) | Xent 0.0179(0.0202) | Loss 3.5296(3.5261) | Error 0.0033(0.0046) Steps 1114(1114.94) | Grad Norm 0.9837(0.9467) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 27.0419(27.1838) | Bit/dim 3.5106(3.5153) | Xent 0.0188(0.0202) | Loss 3.5200(3.5254) | Error 0.0033(0.0046) Steps 1102(1113.07) | Grad Norm 1.0320(0.9447) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 28.3681(27.2509) | Bit/dim 3.5169(3.5169) | Xent 0.0203(0.0201) | Loss 3.5270(3.5270) | Error 0.0033(0.0046) Steps 1120(1112.97) | Grad Norm 1.0198(0.9607) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 26.8624(27.2242) | Bit/dim 3.4861(3.5167) | Xent 0.0173(0.0198) | Loss 3.4947(3.5266) | Error 0.0022(0.0045) Steps 1096(1112.22) | Grad Norm 1.0410(0.9504) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 26.6991(27.1828) | Bit/dim 3.5354(3.5172) | Xent 0.0254(0.0197) | Loss 3.5481(3.5271) | Error 0.0067(0.0044) Steps 1126(1112.97) | Grad Norm 1.2257(0.9521) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 126.7918, Epoch Time 1638.2798(1614.1959), Bit/dim 3.5211(best: 3.5217), Xent 1.9753, Loss 4.5088, Error 0.2803(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 27.4786(27.1963) | Bit/dim 3.5194(3.5188) | Xent 0.0252(0.0202) | Loss 3.5321(3.5289) | Error 0.0067(0.0047) Steps 1096(1112.29) | Grad Norm 1.0339(0.9756) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 28.0660(27.2848) | Bit/dim 3.5175(3.5181) | Xent 0.0290(0.0205) | Loss 3.5321(3.5283) | Error 0.0056(0.0050) Steps 1126(1112.89) | Grad Norm 1.0920(0.9997) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 26.6283(27.2739) | Bit/dim 3.4973(3.5191) | Xent 0.0149(0.0201) | Loss 3.5048(3.5292) | Error 0.0022(0.0048) Steps 1108(1113.44) | Grad Norm 0.7467(0.9736) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 26.6759(27.1231) | Bit/dim 3.5260(3.5166) | Xent 0.0180(0.0200) | Loss 3.5350(3.5266) | Error 0.0044(0.0048) Steps 1120(1112.72) | Grad Norm 0.9540(0.9924) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 26.6811(27.0919) | Bit/dim 3.5065(3.5166) | Xent 0.0155(0.0198) | Loss 3.5143(3.5265) | Error 0.0033(0.0046) Steps 1108(1114.08) | Grad Norm 0.9136(1.0045) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 27.0548(27.0557) | Bit/dim 3.5263(3.5168) | Xent 0.0165(0.0205) | Loss 3.5345(3.5270) | Error 0.0033(0.0048) Steps 1114(1112.25) | Grad Norm 0.7336(1.0153) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 126.9817, Epoch Time 1635.8097(1614.8443), Bit/dim 3.5224(best: 3.5211), Xent 1.9930, Loss 4.5189, Error 0.2781(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 27.4225(27.0728) | Bit/dim 3.5051(3.5151) | Xent 0.0169(0.0206) | Loss 3.5135(3.5253) | Error 0.0033(0.0047) Steps 1108(1111.61) | Grad Norm 1.1396(1.0349) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 27.3434(27.1249) | Bit/dim 3.5411(3.5154) | Xent 0.0248(0.0209) | Loss 3.5535(3.5259) | Error 0.0067(0.0051) Steps 1114(1110.95) | Grad Norm 1.2038(1.0535) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 27.2834(27.2181) | Bit/dim 3.5169(3.5132) | Xent 0.0141(0.0212) | Loss 3.5239(3.5237) | Error 0.0022(0.0053) Steps 1102(1110.15) | Grad Norm 0.7549(1.0895) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 27.7541(27.3251) | Bit/dim 3.5311(3.5163) | Xent 0.0172(0.0205) | Loss 3.5397(3.5265) | Error 0.0067(0.0050) Steps 1114(1109.45) | Grad Norm 1.2295(1.0472) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 27.1523(27.3430) | Bit/dim 3.5554(3.5176) | Xent 0.0154(0.0196) | Loss 3.5632(3.5274) | Error 0.0044(0.0046) Steps 1108(1109.99) | Grad Norm 0.7982(1.0109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 127.1941, Epoch Time 1650.0768(1615.9013), Bit/dim 3.5215(best: 3.5211), Xent 2.0223, Loss 4.5326, Error 0.2811(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 26.5659(27.2124) | Bit/dim 3.5158(3.5181) | Xent 0.0203(0.0198) | Loss 3.5259(3.5280) | Error 0.0056(0.0047) Steps 1108(1111.01) | Grad Norm 0.9778(1.0529) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 27.4290(27.1945) | Bit/dim 3.5234(3.5189) | Xent 0.0196(0.0199) | Loss 3.5332(3.5288) | Error 0.0033(0.0047) Steps 1102(1111.02) | Grad Norm 0.8594(1.0466) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 27.5194(27.2562) | Bit/dim 3.5305(3.5181) | Xent 0.0176(0.0201) | Loss 3.5393(3.5282) | Error 0.0033(0.0048) Steps 1126(1113.31) | Grad Norm 1.3006(1.1116) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 27.7050(27.2781) | Bit/dim 3.5189(3.5174) | Xent 0.0224(0.0206) | Loss 3.5301(3.5277) | Error 0.0044(0.0051) Steps 1114(1111.86) | Grad Norm 1.6338(1.1806) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 27.1981(27.2461) | Bit/dim 3.5411(3.5173) | Xent 0.0130(0.0203) | Loss 3.5476(3.5275) | Error 0.0022(0.0051) Steps 1114(1111.66) | Grad Norm 1.2001(1.2237) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 27.0023(27.2614) | Bit/dim 3.4983(3.5168) | Xent 0.0228(0.0205) | Loss 3.5097(3.5271) | Error 0.0044(0.0050) Steps 1108(1112.61) | Grad Norm 1.0766(1.2692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 128.2466, Epoch Time 1642.5032(1616.6993), Bit/dim 3.5218(best: 3.5211), Xent 2.0298, Loss 4.5367, Error 0.2831(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 27.6169(27.2466) | Bit/dim 3.5171(3.5148) | Xent 0.0161(0.0201) | Loss 3.5252(3.5248) | Error 0.0033(0.0048) Steps 1108(1112.47) | Grad Norm 0.8225(1.2068) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 26.8743(27.1478) | Bit/dim 3.5241(3.5133) | Xent 0.0223(0.0199) | Loss 3.5353(3.5233) | Error 0.0044(0.0047) Steps 1108(1111.55) | Grad Norm 1.0558(1.1604) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 27.3684(27.2217) | Bit/dim 3.4874(3.5156) | Xent 0.0199(0.0199) | Loss 3.4973(3.5255) | Error 0.0044(0.0047) Steps 1108(1111.74) | Grad Norm 1.1214(1.1748) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 27.6519(27.1829) | Bit/dim 3.5402(3.5158) | Xent 0.0228(0.0204) | Loss 3.5516(3.5260) | Error 0.0111(0.0048) Steps 1120(1111.22) | Grad Norm 1.1677(1.1591) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 26.1575(27.1651) | Bit/dim 3.5262(3.5166) | Xent 0.0215(0.0200) | Loss 3.5369(3.5266) | Error 0.0078(0.0050) Steps 1108(1111.06) | Grad Norm 1.0319(1.1303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 128.7923, Epoch Time 1638.6223(1617.3570), Bit/dim 3.5199(best: 3.5211), Xent 2.0096, Loss 4.5247, Error 0.2785(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 27.4476(27.1554) | Bit/dim 3.5259(3.5178) | Xent 0.0209(0.0199) | Loss 3.5363(3.5278) | Error 0.0044(0.0050) Steps 1144(1112.02) | Grad Norm 0.9281(1.0956) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 27.3725(27.2076) | Bit/dim 3.5099(3.5169) | Xent 0.0229(0.0198) | Loss 3.5213(3.5268) | Error 0.0067(0.0051) Steps 1108(1113.04) | Grad Norm 1.3462(1.0699) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 27.1563(27.2715) | Bit/dim 3.5322(3.5153) | Xent 0.0186(0.0194) | Loss 3.5414(3.5250) | Error 0.0044(0.0047) Steps 1114(1112.12) | Grad Norm 1.0265(1.0331) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 27.4153(27.3220) | Bit/dim 3.5159(3.5160) | Xent 0.0196(0.0192) | Loss 3.5257(3.5256) | Error 0.0056(0.0047) Steps 1132(1112.50) | Grad Norm 0.9576(1.0066) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 26.7208(27.3189) | Bit/dim 3.5046(3.5151) | Xent 0.0187(0.0195) | Loss 3.5139(3.5249) | Error 0.0044(0.0046) Steps 1108(1112.26) | Grad Norm 1.3116(1.0237) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 27.6967(27.3449) | Bit/dim 3.5326(3.5152) | Xent 0.0153(0.0189) | Loss 3.5402(3.5247) | Error 0.0011(0.0042) Steps 1096(1112.06) | Grad Norm 0.7680(0.9838) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 128.0590, Epoch Time 1652.3571(1618.4070), Bit/dim 3.5206(best: 3.5199), Xent 2.0175, Loss 4.5293, Error 0.2820(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 27.5142(27.3003) | Bit/dim 3.5326(3.5159) | Xent 0.0213(0.0193) | Loss 3.5432(3.5255) | Error 0.0067(0.0046) Steps 1126(1112.30) | Grad Norm 0.9226(0.9896) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 27.8944(27.3543) | Bit/dim 3.4943(3.5170) | Xent 0.0191(0.0188) | Loss 3.5039(3.5265) | Error 0.0033(0.0043) Steps 1114(1111.88) | Grad Norm 0.8168(0.9657) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 26.8404(27.3417) | Bit/dim 3.4593(3.5156) | Xent 0.0190(0.0186) | Loss 3.4688(3.5249) | Error 0.0044(0.0042) Steps 1096(1110.79) | Grad Norm 0.9830(0.9690) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 26.9749(27.3875) | Bit/dim 3.4942(3.5139) | Xent 0.0152(0.0190) | Loss 3.5018(3.5234) | Error 0.0022(0.0043) Steps 1114(1111.73) | Grad Norm 0.7329(0.9497) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 27.6237(27.4042) | Bit/dim 3.5024(3.5125) | Xent 0.0185(0.0195) | Loss 3.5117(3.5222) | Error 0.0033(0.0046) Steps 1102(1112.34) | Grad Norm 0.9052(0.9676) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 127.2813, Epoch Time 1652.0956(1619.4177), Bit/dim 3.5191(best: 3.5199), Xent 2.0362, Loss 4.5372, Error 0.2818(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 26.9754(27.3564) | Bit/dim 3.5341(3.5132) | Xent 0.0208(0.0196) | Loss 3.5445(3.5230) | Error 0.0067(0.0047) Steps 1120(1112.41) | Grad Norm 0.8698(0.9545) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 26.7923(27.3259) | Bit/dim 3.4878(3.5159) | Xent 0.0224(0.0193) | Loss 3.4990(3.5255) | Error 0.0056(0.0046) Steps 1108(1113.21) | Grad Norm 1.3800(0.9636) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 27.6709(27.3852) | Bit/dim 3.5109(3.5162) | Xent 0.0149(0.0190) | Loss 3.5184(3.5257) | Error 0.0022(0.0045) Steps 1096(1113.79) | Grad Norm 0.7693(0.9679) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 27.1700(27.3325) | Bit/dim 3.5281(3.5164) | Xent 0.0170(0.0188) | Loss 3.5366(3.5258) | Error 0.0022(0.0046) Steps 1108(1111.85) | Grad Norm 0.8994(0.9415) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 27.4028(27.2853) | Bit/dim 3.5488(3.5149) | Xent 0.0147(0.0191) | Loss 3.5561(3.5244) | Error 0.0011(0.0046) Steps 1108(1111.74) | Grad Norm 0.6693(0.9802) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 27.1657(27.1819) | Bit/dim 3.5031(3.5145) | Xent 0.0162(0.0193) | Loss 3.5112(3.5242) | Error 0.0033(0.0047) Steps 1102(1111.11) | Grad Norm 0.9025(0.9981) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 128.2933, Epoch Time 1641.9740(1620.0944), Bit/dim 3.5195(best: 3.5191), Xent 2.0572, Loss 4.5481, Error 0.2815(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 27.7298(27.1291) | Bit/dim 3.5490(3.5150) | Xent 0.0118(0.0196) | Loss 3.5549(3.5248) | Error 0.0011(0.0048) Steps 1108(1111.48) | Grad Norm 0.9287(1.0261) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 26.8488(27.1945) | Bit/dim 3.5041(3.5137) | Xent 0.0159(0.0194) | Loss 3.5120(3.5234) | Error 0.0044(0.0045) Steps 1114(1111.57) | Grad Norm 1.2995(1.0446) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 26.7170(27.1933) | Bit/dim 3.5043(3.5158) | Xent 0.0136(0.0191) | Loss 3.5110(3.5253) | Error 0.0011(0.0044) Steps 1102(1111.94) | Grad Norm 0.9844(1.0933) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 27.9203(27.1907) | Bit/dim 3.5344(3.5161) | Xent 0.0177(0.0194) | Loss 3.5432(3.5258) | Error 0.0033(0.0045) Steps 1114(1111.24) | Grad Norm 1.2241(1.0988) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 27.7243(27.2318) | Bit/dim 3.5209(3.5145) | Xent 0.0159(0.0205) | Loss 3.5288(3.5248) | Error 0.0011(0.0048) Steps 1126(1112.21) | Grad Norm 1.0763(1.1358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 126.8949, Epoch Time 1641.9625(1620.7504), Bit/dim 3.5221(best: 3.5191), Xent 2.0450, Loss 4.5446, Error 0.2798(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.9950(27.2273) | Bit/dim 3.4918(3.5128) | Xent 0.0232(0.0204) | Loss 3.5034(3.5230) | Error 0.0089(0.0050) Steps 1108(1112.41) | Grad Norm 1.3253(1.1582) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 27.4645(27.2245) | Bit/dim 3.5035(3.5136) | Xent 0.0233(0.0204) | Loss 3.5152(3.5238) | Error 0.0044(0.0051) Steps 1096(1112.77) | Grad Norm 0.8723(1.1190) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 26.6374(27.2529) | Bit/dim 3.4760(3.5127) | Xent 0.0285(0.0197) | Loss 3.4902(3.5226) | Error 0.0100(0.0048) Steps 1108(1112.75) | Grad Norm 1.2542(1.0745) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 27.5524(27.3186) | Bit/dim 3.5021(3.5126) | Xent 0.0243(0.0200) | Loss 3.5143(3.5226) | Error 0.0078(0.0048) Steps 1108(1112.61) | Grad Norm 1.5516(1.0746) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 26.8313(27.2836) | Bit/dim 3.5305(3.5146) | Xent 0.0245(0.0202) | Loss 3.5427(3.5247) | Error 0.0089(0.0050) Steps 1126(1113.11) | Grad Norm 1.3801(1.1223) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 27.0429(27.2170) | Bit/dim 3.5012(3.5142) | Xent 0.0169(0.0199) | Loss 3.5096(3.5242) | Error 0.0022(0.0049) Steps 1114(1113.23) | Grad Norm 1.3837(1.1143) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 127.2992, Epoch Time 1644.1187(1621.4515), Bit/dim 3.5205(best: 3.5191), Xent 2.0951, Loss 4.5680, Error 0.2849(best: 0.2717)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 26.8728(27.1217) | Bit/dim 3.5056(3.5149) | Xent 0.0205(0.0194) | Loss 3.5159(3.5246) | Error 0.0033(0.0047) Steps 1108(1112.77) | Grad Norm 1.1233(1.0829) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 26.8377(27.1267) | Bit/dim 3.4977(3.5117) | Xent 0.0126(0.0191) | Loss 3.5040(3.5212) | Error 0.0011(0.0045) Steps 1096(1111.88) | Grad Norm 0.9684(1.0523) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run2/current_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
