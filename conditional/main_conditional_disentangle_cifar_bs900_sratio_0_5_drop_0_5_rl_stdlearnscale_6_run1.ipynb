{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1/epoch_40_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 2210 | Time 19.0223(18.4722) | Bit/dim 3.8769(3.8444) | Xent 1.0121(1.0418) | Loss 10.5089(11.1431) | Error 0.3544(0.3705) Steps 0(0.00) | Grad Norm 7.7362(11.9841) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 18.2160(18.2957) | Bit/dim 3.7865(3.8411) | Xent 1.0093(1.0373) | Loss 10.3527(10.9300) | Error 0.3400(0.3697) Steps 0(0.00) | Grad Norm 9.8485(11.3724) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 18.5891(18.2476) | Bit/dim 3.7999(3.8368) | Xent 1.0688(1.0307) | Loss 10.5619(10.7736) | Error 0.3944(0.3669) Steps 0(0.00) | Grad Norm 11.3965(10.5693) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 20.0461(18.0883) | Bit/dim 3.8494(3.8348) | Xent 1.0634(1.0340) | Loss 10.6091(10.6786) | Error 0.3822(0.3678) Steps 0(0.00) | Grad Norm 15.6653(11.6144) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 16.6925(17.9737) | Bit/dim 3.8121(3.8333) | Xent 1.0613(1.0309) | Loss 10.3454(10.5991) | Error 0.3744(0.3666) Steps 0(0.00) | Grad Norm 9.9185(11.2287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 98.8668, Epoch Time 1123.8608(1014.0341), Bit/dim 3.8283(best: inf), Xent 1.0072, Loss 4.3319, Error 0.3546(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 16.6243(17.8963) | Bit/dim 3.8495(3.8328) | Xent 0.9879(1.0245) | Loss 10.2585(11.2203) | Error 0.3656(0.3650) Steps 0(0.00) | Grad Norm 9.8419(11.0135) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 16.8120(17.8561) | Bit/dim 3.8093(3.8326) | Xent 0.9978(1.0180) | Loss 10.3736(10.9981) | Error 0.3644(0.3623) Steps 0(0.00) | Grad Norm 11.2262(11.0085) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 17.7715(17.8753) | Bit/dim 3.8515(3.8294) | Xent 0.9854(1.0166) | Loss 10.3967(10.8179) | Error 0.3633(0.3623) Steps 0(0.00) | Grad Norm 18.5312(10.8066) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 17.0102(17.8600) | Bit/dim 3.8142(3.8270) | Xent 1.0331(1.0256) | Loss 10.4244(10.6982) | Error 0.3633(0.3652) Steps 0(0.00) | Grad Norm 8.8873(11.4795) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 20.7800(17.8672) | Bit/dim 3.8879(3.8291) | Xent 1.0617(1.0333) | Loss 10.6239(10.6265) | Error 0.3867(0.3673) Steps 0(0.00) | Grad Norm 8.5860(12.0401) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 19.5449(17.7769) | Bit/dim 3.7994(3.8251) | Xent 0.9547(1.0247) | Loss 10.3748(10.5380) | Error 0.3211(0.3633) Steps 0(0.00) | Grad Norm 7.2424(10.9352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 91.4743, Epoch Time 1085.7266(1016.1849), Bit/dim 3.8175(best: 3.8283), Xent 0.9723, Loss 4.3036, Error 0.3415(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 17.3488(17.7718) | Bit/dim 3.8166(3.8216) | Xent 0.9305(1.0160) | Loss 10.1858(11.1102) | Error 0.3444(0.3613) Steps 0(0.00) | Grad Norm 5.2189(10.9624) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 18.7255(17.7289) | Bit/dim 3.7954(3.8226) | Xent 1.0278(1.0076) | Loss 10.4199(10.8986) | Error 0.3611(0.3575) Steps 0(0.00) | Grad Norm 11.0617(11.3234) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 18.4516(17.7528) | Bit/dim 3.8081(3.8196) | Xent 1.0266(1.0095) | Loss 10.3156(10.7367) | Error 0.3522(0.3586) Steps 0(0.00) | Grad Norm 18.2026(11.5184) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 18.1483(17.8303) | Bit/dim 3.8422(3.8216) | Xent 1.0549(1.0179) | Loss 10.2552(10.6306) | Error 0.3844(0.3615) Steps 0(0.00) | Grad Norm 16.5100(12.5901) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 17.6903(17.7314) | Bit/dim 3.8091(3.8214) | Xent 0.9764(1.0290) | Loss 10.3672(10.5727) | Error 0.3411(0.3647) Steps 0(0.00) | Grad Norm 10.4557(12.8209) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 90.4259, Epoch Time 1086.2793(1018.2877), Bit/dim 3.8197(best: 3.8175), Xent 0.9854, Loss 4.3124, Error 0.3531(best: 0.3415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 18.6917(17.8396) | Bit/dim 3.8341(3.8238) | Xent 1.0727(1.0305) | Loss 10.5859(11.2568) | Error 0.3689(0.3654) Steps 0(0.00) | Grad Norm 14.3188(12.8316) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 18.5363(17.9827) | Bit/dim 3.8075(3.8236) | Xent 1.0088(1.0202) | Loss 10.2992(11.0109) | Error 0.3422(0.3632) Steps 0(0.00) | Grad Norm 6.7180(12.0859) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 17.1056(17.9444) | Bit/dim 3.7875(3.8168) | Xent 0.9650(1.0111) | Loss 10.1985(10.8290) | Error 0.3500(0.3606) Steps 0(0.00) | Grad Norm 7.6688(11.2964) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 18.9348(17.8993) | Bit/dim 3.7979(3.8165) | Xent 0.9979(1.0048) | Loss 10.4065(10.7006) | Error 0.3533(0.3589) Steps 0(0.00) | Grad Norm 15.5745(11.6245) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 18.1499(17.8043) | Bit/dim 3.8192(3.8168) | Xent 0.9734(1.0013) | Loss 10.3648(10.5919) | Error 0.3378(0.3584) Steps 0(0.00) | Grad Norm 12.4922(11.3029) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 17.6962(17.8592) | Bit/dim 3.8019(3.8158) | Xent 0.9661(1.0038) | Loss 10.3855(10.5194) | Error 0.3278(0.3582) Steps 0(0.00) | Grad Norm 9.9800(11.9516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 89.6990, Epoch Time 1094.0177(1020.5596), Bit/dim 3.8084(best: 3.8175), Xent 0.9869, Loss 4.3018, Error 0.3504(best: 0.3415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 17.6850(17.9348) | Bit/dim 3.8078(3.8138) | Xent 0.9659(0.9958) | Loss 10.1733(11.0611) | Error 0.3433(0.3557) Steps 0(0.00) | Grad Norm 11.6771(11.7931) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 17.8927(17.9125) | Bit/dim 3.8198(3.8131) | Xent 1.0761(0.9972) | Loss 10.4750(10.8421) | Error 0.3733(0.3543) Steps 0(0.00) | Grad Norm 27.1742(12.6026) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 17.4968(17.9777) | Bit/dim 3.8274(3.8124) | Xent 0.9660(1.0041) | Loss 10.1864(10.7112) | Error 0.3356(0.3570) Steps 0(0.00) | Grad Norm 7.6853(11.9157) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 19.7863(17.8994) | Bit/dim 3.8078(3.8117) | Xent 0.9882(1.0037) | Loss 10.2622(10.6150) | Error 0.3422(0.3576) Steps 0(0.00) | Grad Norm 17.9631(12.0629) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.3589(17.8785) | Bit/dim 3.8503(3.8118) | Xent 0.9391(0.9988) | Loss 9.9513(10.5320) | Error 0.3244(0.3548) Steps 0(0.00) | Grad Norm 5.3111(11.9544) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 91.8651, Epoch Time 1094.6146(1022.7813), Bit/dim 3.8062(best: 3.8084), Xent 0.9322, Loss 4.2723, Error 0.3270(best: 0.3415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 18.4914(17.8862) | Bit/dim 3.8143(3.8061) | Xent 0.9322(0.9899) | Loss 10.3747(11.1275) | Error 0.3289(0.3515) Steps 0(0.00) | Grad Norm 5.9254(11.3293) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 17.3417(17.8264) | Bit/dim 3.8484(3.8090) | Xent 0.9040(0.9732) | Loss 10.3484(10.8958) | Error 0.3200(0.3453) Steps 0(0.00) | Grad Norm 12.0535(10.4984) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 17.9883(17.7063) | Bit/dim 3.8161(3.8039) | Xent 0.9815(0.9739) | Loss 10.2912(10.7208) | Error 0.3444(0.3461) Steps 0(0.00) | Grad Norm 7.3040(10.3292) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 17.9988(17.6596) | Bit/dim 3.7693(3.7999) | Xent 0.9147(0.9732) | Loss 9.8115(10.5890) | Error 0.3322(0.3458) Steps 0(0.00) | Grad Norm 11.2467(10.5516) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 18.3835(17.7446) | Bit/dim 3.7940(3.8000) | Xent 1.0402(0.9780) | Loss 10.2732(10.5024) | Error 0.3689(0.3475) Steps 0(0.00) | Grad Norm 6.1935(10.2576) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 18.4499(17.7757) | Bit/dim 3.7669(3.7964) | Xent 0.9564(0.9786) | Loss 10.2160(10.4282) | Error 0.3567(0.3501) Steps 0(0.00) | Grad Norm 10.7630(10.4350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 90.2051, Epoch Time 1082.4469(1024.5712), Bit/dim 3.7949(best: 3.8062), Xent 0.9580, Loss 4.2738, Error 0.3380(best: 0.3270)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 18.5255(17.7567) | Bit/dim 3.7741(3.7956) | Xent 0.9611(0.9793) | Loss 10.2836(11.0199) | Error 0.3300(0.3509) Steps 0(0.00) | Grad Norm 7.1572(10.6125) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 20.4176(17.7516) | Bit/dim 3.7890(3.7938) | Xent 0.8639(0.9730) | Loss 10.1851(10.8155) | Error 0.3189(0.3484) Steps 0(0.00) | Grad Norm 8.0117(10.4306) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 18.0234(17.7469) | Bit/dim 3.8113(3.7943) | Xent 0.9466(0.9667) | Loss 10.1514(10.6519) | Error 0.3478(0.3466) Steps 0(0.00) | Grad Norm 7.6622(10.5074) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 16.5063(17.7742) | Bit/dim 3.7832(3.7950) | Xent 0.9049(0.9622) | Loss 10.0583(10.5383) | Error 0.3133(0.3448) Steps 0(0.00) | Grad Norm 13.7125(10.1859) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 16.1741(17.7666) | Bit/dim 3.8057(3.7949) | Xent 1.1061(0.9689) | Loss 10.2321(10.4630) | Error 0.3833(0.3462) Steps 0(0.00) | Grad Norm 27.6267(10.5070) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 91.9390, Epoch Time 1088.1650(1026.4791), Bit/dim 3.7864(best: 3.7949), Xent 0.9533, Loss 4.2630, Error 0.3362(best: 0.3270)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 16.5396(17.8093) | Bit/dim 3.8045(3.7918) | Xent 0.9337(0.9700) | Loss 10.2606(11.1040) | Error 0.3289(0.3454) Steps 0(0.00) | Grad Norm 11.1461(11.3506) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 17.8751(17.7758) | Bit/dim 3.8068(3.7914) | Xent 0.9395(0.9682) | Loss 10.4028(10.8807) | Error 0.3322(0.3458) Steps 0(0.00) | Grad Norm 11.8789(11.9504) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 17.1758(17.6590) | Bit/dim 3.7893(3.7906) | Xent 0.9571(0.9663) | Loss 10.1282(10.7152) | Error 0.3422(0.3455) Steps 0(0.00) | Grad Norm 13.7758(12.1114) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 17.6913(17.5978) | Bit/dim 3.7632(3.7903) | Xent 0.9862(0.9659) | Loss 10.2646(10.5946) | Error 0.3478(0.3438) Steps 0(0.00) | Grad Norm 9.0736(11.8887) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 19.6737(17.6740) | Bit/dim 3.8024(3.7896) | Xent 0.9719(0.9648) | Loss 10.3776(10.4951) | Error 0.3622(0.3439) Steps 0(0.00) | Grad Norm 10.1310(11.6624) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 16.2215(17.7335) | Bit/dim 3.8192(3.7896) | Xent 1.0341(0.9717) | Loss 10.3478(10.4308) | Error 0.3722(0.3461) Steps 0(0.00) | Grad Norm 20.1768(12.4939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 90.1295, Epoch Time 1079.9021(1028.0817), Bit/dim 3.7768(best: 3.7864), Xent 0.9266, Loss 4.2401, Error 0.3250(best: 0.3270)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 17.3349(17.6963) | Bit/dim 3.8024(3.7888) | Xent 0.9712(0.9629) | Loss 10.0783(11.0032) | Error 0.3478(0.3429) Steps 0(0.00) | Grad Norm 16.3079(12.0424) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 15.9471(17.6798) | Bit/dim 3.7645(3.7841) | Xent 1.0343(0.9626) | Loss 10.2846(10.7971) | Error 0.3700(0.3427) Steps 0(0.00) | Grad Norm 15.1664(12.0960) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 18.8122(17.6886) | Bit/dim 3.7848(3.7868) | Xent 0.9351(0.9601) | Loss 10.1431(10.6557) | Error 0.3311(0.3434) Steps 0(0.00) | Grad Norm 12.0259(11.7091) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 18.7637(17.8249) | Bit/dim 3.7963(3.7866) | Xent 0.9420(0.9623) | Loss 10.2996(10.5510) | Error 0.3378(0.3435) Steps 0(0.00) | Grad Norm 7.9942(11.0826) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 18.3472(17.8371) | Bit/dim 3.7912(3.7856) | Xent 0.9398(0.9609) | Loss 10.3813(10.4710) | Error 0.3300(0.3435) Steps 0(0.00) | Grad Norm 11.1198(10.7275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 91.9414, Epoch Time 1089.7983(1029.9332), Bit/dim 3.7907(best: 3.7768), Xent 0.9452, Loss 4.2633, Error 0.3313(best: 0.3250)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 17.7162(18.1882) | Bit/dim 3.7403(3.7807) | Xent 0.9316(0.9580) | Loss 10.0327(11.0630) | Error 0.3333(0.3426) Steps 0(0.00) | Grad Norm 7.3293(10.8555) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 18.7384(18.0148) | Bit/dim 3.7781(3.7832) | Xent 0.9579(0.9664) | Loss 10.2772(10.8539) | Error 0.3389(0.3455) Steps 0(0.00) | Grad Norm 14.2810(11.7632) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 17.2521(17.9429) | Bit/dim 3.7233(3.7810) | Xent 0.9342(0.9606) | Loss 10.1869(10.6854) | Error 0.3122(0.3420) Steps 0(0.00) | Grad Norm 5.7161(11.2222) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 16.8524(17.8302) | Bit/dim 3.7738(3.7781) | Xent 0.9495(0.9511) | Loss 10.0389(10.5586) | Error 0.3544(0.3385) Steps 0(0.00) | Grad Norm 5.6069(10.1067) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 17.2524(17.9010) | Bit/dim 3.7615(3.7764) | Xent 0.9848(0.9476) | Loss 10.3395(10.4560) | Error 0.3522(0.3363) Steps 0(0.00) | Grad Norm 8.3040(9.5551) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 17.6678(17.8223) | Bit/dim 3.7784(3.7766) | Xent 0.9119(0.9398) | Loss 10.2016(10.3849) | Error 0.3378(0.3344) Steps 0(0.00) | Grad Norm 5.3361(9.2513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 92.5206, Epoch Time 1096.9034(1031.9423), Bit/dim 3.7729(best: 3.7768), Xent 0.9080, Loss 4.2269, Error 0.3177(best: 0.3250)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 18.3403(17.7163) | Bit/dim 3.8017(3.7749) | Xent 0.8731(0.9342) | Loss 10.1571(10.9649) | Error 0.3011(0.3332) Steps 0(0.00) | Grad Norm 7.4480(10.3806) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 18.4713(17.8597) | Bit/dim 3.7705(3.7762) | Xent 0.9019(0.9378) | Loss 10.0903(10.7586) | Error 0.3344(0.3346) Steps 0(0.00) | Grad Norm 8.4543(10.6653) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 18.2549(17.8165) | Bit/dim 3.7721(3.7751) | Xent 0.9927(0.9417) | Loss 10.2253(10.6311) | Error 0.3500(0.3351) Steps 0(0.00) | Grad Norm 18.0128(11.2772) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 17.5327(17.8336) | Bit/dim 3.7885(3.7745) | Xent 0.9545(0.9436) | Loss 10.0562(10.5151) | Error 0.3244(0.3354) Steps 0(0.00) | Grad Norm 17.4434(11.7466) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 24.1921(18.2181) | Bit/dim 3.8043(3.7756) | Xent 0.9016(0.9403) | Loss 10.3351(10.4431) | Error 0.3267(0.3351) Steps 0(0.00) | Grad Norm 12.5113(11.0102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 91.9663, Epoch Time 1103.2057(1034.0802), Bit/dim 3.7667(best: 3.7729), Xent 0.9145, Loss 4.2240, Error 0.3250(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 16.6420(18.0973) | Bit/dim 3.8175(3.7740) | Xent 0.9414(0.9361) | Loss 9.9647(11.1151) | Error 0.3344(0.3344) Steps 0(0.00) | Grad Norm 8.6028(11.0340) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 17.8615(18.1005) | Bit/dim 3.7838(3.7709) | Xent 0.9111(0.9266) | Loss 10.2576(10.8611) | Error 0.3267(0.3307) Steps 0(0.00) | Grad Norm 8.1565(10.2462) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 16.9890(17.9945) | Bit/dim 3.7620(3.7710) | Xent 0.9515(0.9222) | Loss 10.0714(10.6689) | Error 0.3367(0.3281) Steps 0(0.00) | Grad Norm 10.1350(10.0860) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 17.4093(17.8513) | Bit/dim 3.7913(3.7702) | Xent 0.9489(0.9214) | Loss 10.3364(10.5357) | Error 0.3300(0.3278) Steps 0(0.00) | Grad Norm 18.3092(9.8673) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 17.9260(17.6808) | Bit/dim 3.7507(3.7696) | Xent 0.9008(0.9266) | Loss 9.9819(10.4336) | Error 0.3178(0.3307) Steps 0(0.00) | Grad Norm 9.1613(10.8875) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 17.7630(17.7209) | Bit/dim 3.7692(3.7703) | Xent 0.9784(0.9353) | Loss 10.1961(10.3764) | Error 0.3633(0.3339) Steps 0(0.00) | Grad Norm 12.3531(10.7807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 90.7557, Epoch Time 1080.9382(1035.4860), Bit/dim 3.7773(best: 3.7667), Xent 0.9033, Loss 4.2290, Error 0.3179(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 16.8397(17.7624) | Bit/dim 3.7654(3.7708) | Xent 0.9094(0.9346) | Loss 10.1333(10.9084) | Error 0.3233(0.3337) Steps 0(0.00) | Grad Norm 11.0740(11.2355) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 16.3441(17.8347) | Bit/dim 3.7725(3.7687) | Xent 0.8838(0.9266) | Loss 10.0641(10.7017) | Error 0.3144(0.3305) Steps 0(0.00) | Grad Norm 10.2514(11.0811) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 17.9289(17.8090) | Bit/dim 3.7644(3.7651) | Xent 0.9733(0.9209) | Loss 10.0819(10.5468) | Error 0.3589(0.3300) Steps 0(0.00) | Grad Norm 9.7785(10.4558) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 17.9931(17.7483) | Bit/dim 3.7455(3.7608) | Xent 0.9025(0.9193) | Loss 10.0273(10.4250) | Error 0.3067(0.3281) Steps 0(0.00) | Grad Norm 8.1263(10.4377) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 16.9756(17.7001) | Bit/dim 3.7822(3.7633) | Xent 0.8680(0.9135) | Loss 10.2582(10.3688) | Error 0.3067(0.3265) Steps 0(0.00) | Grad Norm 10.9555(10.2792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 92.6175, Epoch Time 1089.3840(1037.1029), Bit/dim 3.7575(best: 3.7667), Xent 0.9229, Loss 4.2190, Error 0.3238(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 18.8732(17.7703) | Bit/dim 3.7683(3.7609) | Xent 0.9681(0.9220) | Loss 10.1860(11.0450) | Error 0.3467(0.3287) Steps 0(0.00) | Grad Norm 25.6876(11.6653) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 17.1245(17.7387) | Bit/dim 3.7710(3.7625) | Xent 0.8922(0.9248) | Loss 9.8614(10.8048) | Error 0.3378(0.3297) Steps 0(0.00) | Grad Norm 9.6765(12.1751) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 17.2152(17.7392) | Bit/dim 3.7202(3.7614) | Xent 1.0231(0.9277) | Loss 10.2810(10.6421) | Error 0.3578(0.3310) Steps 0(0.00) | Grad Norm 10.0890(11.0810) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 18.0189(17.7745) | Bit/dim 3.7786(3.7637) | Xent 0.8862(0.9267) | Loss 10.1575(10.5039) | Error 0.3222(0.3315) Steps 0(0.00) | Grad Norm 10.0970(11.2094) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 18.8465(17.8417) | Bit/dim 3.7597(3.7633) | Xent 0.9456(0.9263) | Loss 10.3029(10.4208) | Error 0.3478(0.3325) Steps 0(0.00) | Grad Norm 10.7012(10.9301) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 17.4564(17.8123) | Bit/dim 3.7512(3.7604) | Xent 0.9372(0.9131) | Loss 10.2032(10.3272) | Error 0.3411(0.3286) Steps 0(0.00) | Grad Norm 9.4351(10.3628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 90.7083, Epoch Time 1088.1017(1038.6329), Bit/dim 3.7576(best: 3.7575), Xent 0.9111, Loss 4.2131, Error 0.3197(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 17.9314(17.7492) | Bit/dim 3.7600(3.7578) | Xent 0.8857(0.9039) | Loss 9.8533(10.8742) | Error 0.3222(0.3257) Steps 0(0.00) | Grad Norm 8.5246(10.0457) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 18.0615(17.7436) | Bit/dim 3.7843(3.7570) | Xent 0.8646(0.9008) | Loss 10.2804(10.6729) | Error 0.3189(0.3237) Steps 0(0.00) | Grad Norm 7.3663(10.0267) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 17.5215(17.8190) | Bit/dim 3.7595(3.7564) | Xent 0.9642(0.9111) | Loss 10.1102(10.5363) | Error 0.3433(0.3263) Steps 0(0.00) | Grad Norm 13.4396(11.3552) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 17.3251(17.8850) | Bit/dim 3.7804(3.7598) | Xent 0.9008(0.9125) | Loss 10.1524(10.4474) | Error 0.3122(0.3256) Steps 0(0.00) | Grad Norm 7.8219(11.4819) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 17.5118(17.8043) | Bit/dim 3.7476(3.7589) | Xent 0.8386(0.9107) | Loss 9.9462(10.3555) | Error 0.3178(0.3254) Steps 0(0.00) | Grad Norm 6.9647(11.1481) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 92.1845, Epoch Time 1091.0502(1040.2054), Bit/dim 3.7575(best: 3.7575), Xent 0.9015, Loss 4.2083, Error 0.3182(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 17.8744(17.8624) | Bit/dim 3.7111(3.7559) | Xent 0.9460(0.9093) | Loss 10.0633(11.0416) | Error 0.3522(0.3244) Steps 0(0.00) | Grad Norm 10.0636(10.8476) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 18.5686(17.8568) | Bit/dim 3.7510(3.7567) | Xent 0.8739(0.9025) | Loss 10.0687(10.7871) | Error 0.3289(0.3236) Steps 0(0.00) | Grad Norm 13.9531(10.7276) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 16.2576(17.6763) | Bit/dim 3.7523(3.7558) | Xent 0.8571(0.9066) | Loss 10.0230(10.6139) | Error 0.2956(0.3245) Steps 0(0.00) | Grad Norm 5.1464(10.3596) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 16.6356(17.7532) | Bit/dim 3.7432(3.7545) | Xent 0.9117(0.9090) | Loss 9.9625(10.4753) | Error 0.3189(0.3242) Steps 0(0.00) | Grad Norm 12.9120(11.0107) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 18.5314(17.8535) | Bit/dim 3.7461(3.7549) | Xent 0.8422(0.8989) | Loss 10.1101(10.3989) | Error 0.3167(0.3208) Steps 0(0.00) | Grad Norm 14.3170(10.9433) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 18.1309(17.9927) | Bit/dim 3.7400(3.7520) | Xent 0.9207(0.8917) | Loss 10.0618(10.3250) | Error 0.3400(0.3189) Steps 0(0.00) | Grad Norm 12.6687(10.8440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 90.7678, Epoch Time 1092.9493(1041.7877), Bit/dim 3.7518(best: 3.7575), Xent 0.8989, Loss 4.2013, Error 0.3134(best: 0.3177)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 20.3312(17.9228) | Bit/dim 3.7654(3.7529) | Xent 0.8717(0.8899) | Loss 10.1694(10.8903) | Error 0.3200(0.3190) Steps 0(0.00) | Grad Norm 8.4710(11.1255) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 17.6338(17.6985) | Bit/dim 3.7411(3.7529) | Xent 0.9302(0.8856) | Loss 9.9189(10.6729) | Error 0.3456(0.3186) Steps 0(0.00) | Grad Norm 12.3160(10.8027) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 17.4312(17.7580) | Bit/dim 3.7368(3.7479) | Xent 0.9245(0.8799) | Loss 10.2001(10.5255) | Error 0.3222(0.3161) Steps 0(0.00) | Grad Norm 9.1315(10.0896) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 16.0058(17.5099) | Bit/dim 3.7425(3.7481) | Xent 0.9754(0.8825) | Loss 9.9190(10.3878) | Error 0.3567(0.3155) Steps 0(0.00) | Grad Norm 16.9314(9.9876) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 18.3271(17.4306) | Bit/dim 3.7269(3.7479) | Xent 0.8965(0.8894) | Loss 9.9799(10.3207) | Error 0.3111(0.3175) Steps 0(0.00) | Grad Norm 9.7497(10.6797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 86.9663, Epoch Time 1068.8162(1042.5986), Bit/dim 3.7533(best: 3.7518), Xent 0.8767, Loss 4.1916, Error 0.3089(best: 0.3134)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 16.3178(17.5937) | Bit/dim 3.7381(3.7494) | Xent 0.9168(0.8865) | Loss 10.1608(10.9669) | Error 0.3378(0.3174) Steps 0(0.00) | Grad Norm 13.5601(10.6738) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.8688(17.4840) | Bit/dim 3.7189(3.7472) | Xent 0.8605(0.8850) | Loss 10.1097(10.7259) | Error 0.3100(0.3151) Steps 0(0.00) | Grad Norm 6.8234(10.1853) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 17.3950(17.3844) | Bit/dim 3.7713(3.7475) | Xent 0.9099(0.8788) | Loss 9.9406(10.5277) | Error 0.3333(0.3139) Steps 0(0.00) | Grad Norm 9.6328(9.6471) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 16.3294(17.4970) | Bit/dim 3.7159(3.7460) | Xent 0.9155(0.8851) | Loss 10.0320(10.4174) | Error 0.3289(0.3148) Steps 0(0.00) | Grad Norm 12.0270(9.9224) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 17.0156(17.4316) | Bit/dim 3.7389(3.7466) | Xent 0.8583(0.8783) | Loss 9.9979(10.3250) | Error 0.2967(0.3113) Steps 0(0.00) | Grad Norm 8.7908(9.5536) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 17.7142(17.5123) | Bit/dim 3.7378(3.7441) | Xent 0.8332(0.8760) | Loss 10.0629(10.2541) | Error 0.3044(0.3114) Steps 0(0.00) | Grad Norm 9.0832(9.2216) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 89.1235, Epoch Time 1058.2162(1043.0671), Bit/dim 3.7403(best: 3.7518), Xent 0.8538, Loss 4.1672, Error 0.2999(best: 0.3089)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 17.6757(17.5093) | Bit/dim 3.7190(3.7430) | Xent 0.8884(0.8719) | Loss 10.1576(10.8431) | Error 0.3222(0.3101) Steps 0(0.00) | Grad Norm 9.2197(9.3573) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 17.3260(17.4989) | Bit/dim 3.7691(3.7431) | Xent 0.9282(0.8722) | Loss 10.0186(10.6335) | Error 0.3100(0.3091) Steps 0(0.00) | Grad Norm 12.0652(9.7357) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 17.3338(17.4459) | Bit/dim 3.7240(3.7419) | Xent 0.8816(0.8758) | Loss 10.0985(10.4970) | Error 0.3122(0.3095) Steps 0(0.00) | Grad Norm 12.5332(10.0763) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.7649(17.2688) | Bit/dim 3.7436(3.7429) | Xent 0.8061(0.8684) | Loss 9.9989(10.3877) | Error 0.2833(0.3063) Steps 0(0.00) | Grad Norm 8.4027(9.8717) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 17.1417(17.2085) | Bit/dim 3.7509(3.7388) | Xent 0.9287(0.8760) | Loss 10.0220(10.2995) | Error 0.3289(0.3093) Steps 0(0.00) | Grad Norm 11.5678(9.8753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 87.7670, Epoch Time 1053.1394(1043.3693), Bit/dim 3.7461(best: 3.7403), Xent 0.8946, Loss 4.1934, Error 0.3158(best: 0.2999)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 16.9013(17.2794) | Bit/dim 3.6906(3.7397) | Xent 0.8296(0.8704) | Loss 9.8695(10.9459) | Error 0.2922(0.3098) Steps 0(0.00) | Grad Norm 8.7971(9.4830) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 17.1605(17.1846) | Bit/dim 3.7649(3.7397) | Xent 0.8955(0.8670) | Loss 10.2396(10.7177) | Error 0.3111(0.3087) Steps 0(0.00) | Grad Norm 10.4980(9.9044) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 17.1145(17.1854) | Bit/dim 3.7206(3.7422) | Xent 0.9363(0.8668) | Loss 10.0726(10.5510) | Error 0.3278(0.3086) Steps 0(0.00) | Grad Norm 12.6158(9.7453) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 17.5076(17.2236) | Bit/dim 3.7486(3.7417) | Xent 0.9326(0.8751) | Loss 10.3202(10.4420) | Error 0.3322(0.3112) Steps 0(0.00) | Grad Norm 10.4587(10.6493) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 17.3240(17.2588) | Bit/dim 3.7191(3.7427) | Xent 0.8675(0.8785) | Loss 10.1100(10.3399) | Error 0.3111(0.3144) Steps 0(0.00) | Grad Norm 16.2898(10.9816) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 16.1042(17.2764) | Bit/dim 3.7384(3.7429) | Xent 0.8977(0.8807) | Loss 10.0292(10.2643) | Error 0.3267(0.3152) Steps 0(0.00) | Grad Norm 9.2159(11.1402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 88.6183, Epoch Time 1052.8329(1043.6532), Bit/dim 3.7463(best: 3.7403), Xent 0.8870, Loss 4.1898, Error 0.3110(best: 0.2999)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 16.4553(17.3556) | Bit/dim 3.7478(3.7433) | Xent 0.8056(0.8702) | Loss 9.9291(10.8226) | Error 0.3100(0.3118) Steps 0(0.00) | Grad Norm 5.6603(10.4072) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 17.8585(17.4405) | Bit/dim 3.7392(3.7419) | Xent 0.8372(0.8624) | Loss 10.0659(10.6251) | Error 0.3144(0.3096) Steps 0(0.00) | Grad Norm 7.2856(10.0528) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 18.9769(17.5404) | Bit/dim 3.7199(3.7351) | Xent 0.8554(0.8584) | Loss 9.9893(10.4547) | Error 0.3033(0.3074) Steps 0(0.00) | Grad Norm 10.8479(9.9161) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 19.3636(17.5847) | Bit/dim 3.6998(3.7334) | Xent 0.9920(0.8640) | Loss 9.9967(10.3515) | Error 0.3767(0.3101) Steps 0(0.00) | Grad Norm 8.7978(10.5541) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 17.2552(17.4164) | Bit/dim 3.7114(3.7339) | Xent 0.8457(0.8643) | Loss 9.9866(10.2413) | Error 0.3000(0.3097) Steps 0(0.00) | Grad Norm 11.4879(9.7951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 85.6895, Epoch Time 1066.3316(1044.3335), Bit/dim 3.7306(best: 3.7403), Xent 0.8575, Loss 4.1593, Error 0.3014(best: 0.2999)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 17.4757(17.4118) | Bit/dim 3.7455(3.7316) | Xent 0.7942(0.8583) | Loss 10.0408(10.8775) | Error 0.2722(0.3063) Steps 0(0.00) | Grad Norm 6.5380(9.2005) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 17.6101(17.4009) | Bit/dim 3.7638(3.7341) | Xent 0.9150(0.8547) | Loss 10.2081(10.6656) | Error 0.3189(0.3049) Steps 0(0.00) | Grad Norm 11.0499(9.7545) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 17.4453(17.4383) | Bit/dim 3.7213(3.7315) | Xent 0.8277(0.8711) | Loss 9.7830(10.5077) | Error 0.2800(0.3119) Steps 0(0.00) | Grad Norm 9.5494(10.4023) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.5208(17.3164) | Bit/dim 3.7289(3.7329) | Xent 0.8366(0.8768) | Loss 9.9503(10.3908) | Error 0.3111(0.3146) Steps 0(0.00) | Grad Norm 5.3377(10.8615) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 17.5686(17.2807) | Bit/dim 3.7518(3.7323) | Xent 0.7927(0.8670) | Loss 10.0842(10.3074) | Error 0.2667(0.3093) Steps 0(0.00) | Grad Norm 7.4349(10.3183) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 18.8890(17.3813) | Bit/dim 3.7314(3.7298) | Xent 0.8990(0.8634) | Loss 10.1483(10.2252) | Error 0.3222(0.3074) Steps 0(0.00) | Grad Norm 9.2177(9.7008) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 85.9916, Epoch Time 1055.4153(1044.6660), Bit/dim 3.7279(best: 3.7306), Xent 0.8403, Loss 4.1481, Error 0.2959(best: 0.2999)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 17.2162(17.4242) | Bit/dim 3.7334(3.7273) | Xent 0.8361(0.8544) | Loss 10.1516(10.7924) | Error 0.2911(0.3041) Steps 0(0.00) | Grad Norm 9.1823(9.4237) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.6978(17.3547) | Bit/dim 3.7354(3.7258) | Xent 0.7662(0.8462) | Loss 9.7296(10.5612) | Error 0.2611(0.3015) Steps 0(0.00) | Grad Norm 8.3147(9.3103) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 17.0639(17.2977) | Bit/dim 3.7276(3.7282) | Xent 0.8795(0.8622) | Loss 10.0289(10.4092) | Error 0.3200(0.3062) Steps 0(0.00) | Grad Norm 11.1725(10.8680) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.9630(17.1993) | Bit/dim 3.7382(3.7303) | Xent 0.9272(0.8725) | Loss 10.2022(10.3315) | Error 0.3200(0.3095) Steps 0(0.00) | Grad Norm 16.1493(10.8446) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 16.8389(17.2648) | Bit/dim 3.7375(3.7320) | Xent 0.8035(0.8664) | Loss 9.8836(10.2440) | Error 0.2922(0.3076) Steps 0(0.00) | Grad Norm 7.9483(10.5569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 87.6312, Epoch Time 1055.4660(1044.9900), Bit/dim 3.7329(best: 3.7279), Xent 0.8347, Loss 4.1502, Error 0.2933(best: 0.2959)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 16.9019(17.4177) | Bit/dim 3.7310(3.7346) | Xent 0.8293(0.8512) | Loss 10.1743(10.9375) | Error 0.3022(0.3034) Steps 0(0.00) | Grad Norm 6.5800(9.5943) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.7554(17.4925) | Bit/dim 3.6661(3.7306) | Xent 0.8138(0.8463) | Loss 9.5518(10.6958) | Error 0.3100(0.3019) Steps 0(0.00) | Grad Norm 5.4314(9.4896) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 17.0901(17.4156) | Bit/dim 3.7080(3.7275) | Xent 0.8191(0.8391) | Loss 9.9867(10.5200) | Error 0.3122(0.3006) Steps 0(0.00) | Grad Norm 7.5511(9.0435) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.6797(17.2239) | Bit/dim 3.7111(3.7241) | Xent 0.8256(0.8364) | Loss 9.8997(10.3707) | Error 0.3200(0.2997) Steps 0(0.00) | Grad Norm 9.5882(9.3446) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 16.2896(17.2640) | Bit/dim 3.7588(3.7256) | Xent 0.8689(0.8417) | Loss 10.3321(10.2870) | Error 0.3267(0.3016) Steps 0(0.00) | Grad Norm 11.7176(9.8540) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 17.6380(17.3104) | Bit/dim 3.7189(3.7261) | Xent 0.9487(0.8451) | Loss 10.1857(10.2088) | Error 0.3244(0.3020) Steps 0(0.00) | Grad Norm 10.0932(9.7018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 85.0971, Epoch Time 1056.1206(1045.3239), Bit/dim 3.7175(best: 3.7279), Xent 0.8370, Loss 4.1360, Error 0.2961(best: 0.2933)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 16.6007(17.2400) | Bit/dim 3.6757(3.7233) | Xent 0.7470(0.8390) | Loss 9.8832(10.7737) | Error 0.2733(0.3000) Steps 0(0.00) | Grad Norm 4.9262(9.1927) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 16.8728(17.3152) | Bit/dim 3.7254(3.7213) | Xent 0.8192(0.8346) | Loss 9.6830(10.5565) | Error 0.2733(0.2961) Steps 0(0.00) | Grad Norm 13.6387(9.5558) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.9593(17.2798) | Bit/dim 3.7142(3.7220) | Xent 0.8900(0.8385) | Loss 10.1341(10.4274) | Error 0.3178(0.2991) Steps 0(0.00) | Grad Norm 14.4425(10.2314) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 17.5552(17.3126) | Bit/dim 3.7482(3.7221) | Xent 0.7767(0.8333) | Loss 10.0781(10.3177) | Error 0.2589(0.2977) Steps 0(0.00) | Grad Norm 11.5959(9.6055) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 17.4216(17.1828) | Bit/dim 3.7060(3.7222) | Xent 0.8276(0.8363) | Loss 9.9994(10.2467) | Error 0.3000(0.2982) Steps 0(0.00) | Grad Norm 10.8698(9.8488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 87.1671, Epoch Time 1045.4284(1045.3270), Bit/dim 3.7300(best: 3.7175), Xent 0.8577, Loss 4.1589, Error 0.3050(best: 0.2933)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 17.7143(17.1192) | Bit/dim 3.7195(3.7207) | Xent 0.8363(0.8377) | Loss 10.1664(10.9094) | Error 0.3156(0.2990) Steps 0(0.00) | Grad Norm 10.8530(10.2089) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.9397(17.1271) | Bit/dim 3.7314(3.7220) | Xent 0.7765(0.8304) | Loss 10.0986(10.6639) | Error 0.2789(0.2968) Steps 0(0.00) | Grad Norm 8.1441(10.3668) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 17.8492(17.2259) | Bit/dim 3.7232(3.7188) | Xent 0.8027(0.8226) | Loss 10.0498(10.4782) | Error 0.3033(0.2934) Steps 0(0.00) | Grad Norm 11.8603(10.0199) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 17.5097(17.1981) | Bit/dim 3.6884(3.7172) | Xent 0.8733(0.8207) | Loss 9.8888(10.3323) | Error 0.3122(0.2935) Steps 0(0.00) | Grad Norm 10.9456(9.7900) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 17.9026(17.2473) | Bit/dim 3.7191(3.7200) | Xent 0.7993(0.8192) | Loss 10.0745(10.2525) | Error 0.2833(0.2923) Steps 0(0.00) | Grad Norm 8.2380(10.0111) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 17.7024(17.2513) | Bit/dim 3.7009(3.7169) | Xent 0.8539(0.8251) | Loss 10.0005(10.1874) | Error 0.3078(0.2948) Steps 0(0.00) | Grad Norm 6.7450(9.7450) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 86.4208, Epoch Time 1053.1242(1045.5610), Bit/dim 3.7166(best: 3.7175), Xent 0.8213, Loss 4.1273, Error 0.2892(best: 0.2933)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 18.3572(17.3056) | Bit/dim 3.6953(3.7154) | Xent 0.8024(0.8199) | Loss 9.8475(10.7295) | Error 0.2900(0.2936) Steps 0(0.00) | Grad Norm 7.8700(9.4291) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 17.9010(17.2486) | Bit/dim 3.7581(3.7159) | Xent 0.8776(0.8178) | Loss 9.9719(10.5217) | Error 0.3200(0.2921) Steps 0(0.00) | Grad Norm 11.2730(9.9958) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 19.2278(17.4339) | Bit/dim 3.7127(3.7152) | Xent 0.8067(0.8197) | Loss 9.9572(10.3932) | Error 0.2722(0.2924) Steps 0(0.00) | Grad Norm 6.9499(10.1992) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 18.3028(17.4394) | Bit/dim 3.7028(3.7120) | Xent 0.9405(0.8239) | Loss 10.3114(10.3099) | Error 0.3244(0.2928) Steps 0(0.00) | Grad Norm 12.9260(10.0878) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.5452(17.4307) | Bit/dim 3.7396(3.7166) | Xent 0.8420(0.8273) | Loss 9.6932(10.2247) | Error 0.3056(0.2957) Steps 0(0.00) | Grad Norm 12.6286(10.0459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 87.5596, Epoch Time 1061.3006(1046.0332), Bit/dim 3.7137(best: 3.7166), Xent 0.8102, Loss 4.1188, Error 0.2861(best: 0.2892)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 17.8146(17.4223) | Bit/dim 3.7425(3.7176) | Xent 0.7773(0.8207) | Loss 10.0818(10.8963) | Error 0.2656(0.2934) Steps 0(0.00) | Grad Norm 7.1130(9.8210) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 17.3338(17.3603) | Bit/dim 3.6987(3.7179) | Xent 0.8522(0.8237) | Loss 9.8793(10.6585) | Error 0.3022(0.2946) Steps 0(0.00) | Grad Norm 11.0531(9.6460) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 19.4717(17.6610) | Bit/dim 3.7101(3.7136) | Xent 0.8845(0.8200) | Loss 10.1997(10.4897) | Error 0.3056(0.2935) Steps 0(0.00) | Grad Norm 8.7842(9.3395) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 17.9995(17.6314) | Bit/dim 3.7148(3.7132) | Xent 0.7559(0.8168) | Loss 9.9188(10.3508) | Error 0.2778(0.2930) Steps 0(0.00) | Grad Norm 6.1405(9.2473) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 18.0748(17.6409) | Bit/dim 3.7223(3.7142) | Xent 0.8968(0.8122) | Loss 10.1795(10.2504) | Error 0.3078(0.2912) Steps 0(0.00) | Grad Norm 10.1275(9.7921) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 17.1906(17.4230) | Bit/dim 3.7419(3.7117) | Xent 0.8605(0.8146) | Loss 9.8916(10.1516) | Error 0.3111(0.2919) Steps 0(0.00) | Grad Norm 9.6408(9.9746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 87.2008, Epoch Time 1067.8652(1046.6881), Bit/dim 3.7141(best: 3.7137), Xent 0.8299, Loss 4.1290, Error 0.2887(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 20.1560(17.4824) | Bit/dim 3.7130(3.7115) | Xent 0.8060(0.8057) | Loss 10.0173(10.7568) | Error 0.2944(0.2880) Steps 0(0.00) | Grad Norm 6.7609(9.7261) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.6690(17.5825) | Bit/dim 3.7260(3.7125) | Xent 0.8590(0.8056) | Loss 9.8773(10.5314) | Error 0.3033(0.2874) Steps 0(0.00) | Grad Norm 11.7544(10.0992) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 17.6057(17.4697) | Bit/dim 3.6604(3.7109) | Xent 0.7945(0.8067) | Loss 9.9071(10.3874) | Error 0.2844(0.2871) Steps 0(0.00) | Grad Norm 10.5465(10.3062) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.5538(17.5512) | Bit/dim 3.7094(3.7112) | Xent 0.8239(0.8039) | Loss 9.8957(10.2808) | Error 0.2956(0.2883) Steps 0(0.00) | Grad Norm 13.2165(10.6354) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.4635(17.4116) | Bit/dim 3.7245(3.7103) | Xent 0.8639(0.8046) | Loss 9.9881(10.1772) | Error 0.3056(0.2876) Steps 0(0.00) | Grad Norm 7.4848(10.5126) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 85.0200, Epoch Time 1060.0853(1047.0900), Bit/dim 3.7144(best: 3.7137), Xent 0.8465, Loss 4.1377, Error 0.2948(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.6019(17.2776) | Bit/dim 3.7393(3.7137) | Xent 0.8495(0.8122) | Loss 9.9005(10.8729) | Error 0.3033(0.2911) Steps 0(0.00) | Grad Norm 12.7217(10.5870) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 16.3926(17.2141) | Bit/dim 3.7174(3.7111) | Xent 0.7697(0.8069) | Loss 9.6736(10.6106) | Error 0.2611(0.2886) Steps 0(0.00) | Grad Norm 7.0756(9.9258) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 17.0547(17.2709) | Bit/dim 3.6884(3.7086) | Xent 0.8268(0.8021) | Loss 9.9144(10.4308) | Error 0.2900(0.2870) Steps 0(0.00) | Grad Norm 13.9932(9.7281) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 17.1814(17.2440) | Bit/dim 3.6938(3.7066) | Xent 0.7989(0.8018) | Loss 9.8157(10.2947) | Error 0.2778(0.2868) Steps 0(0.00) | Grad Norm 8.3169(10.1879) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 16.7702(17.1415) | Bit/dim 3.6972(3.7082) | Xent 0.8118(0.8063) | Loss 9.7902(10.1957) | Error 0.2956(0.2883) Steps 0(0.00) | Grad Norm 14.2352(10.3487) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 19.1235(17.4869) | Bit/dim 3.7136(3.7069) | Xent 0.8383(0.8089) | Loss 10.0140(10.1308) | Error 0.2889(0.2901) Steps 0(0.00) | Grad Norm 11.7884(10.3119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 86.4325, Epoch Time 1055.3003(1047.3363), Bit/dim 3.7102(best: 3.7137), Xent 0.8345, Loss 4.1274, Error 0.2911(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 17.0487(17.3829) | Bit/dim 3.7089(3.7070) | Xent 0.7304(0.7991) | Loss 9.8646(10.6858) | Error 0.2644(0.2869) Steps 0(0.00) | Grad Norm 11.9690(10.3204) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 21.2818(17.4224) | Bit/dim 3.6922(3.7054) | Xent 0.7824(0.7874) | Loss 9.9502(10.4528) | Error 0.2878(0.2823) Steps 0(0.00) | Grad Norm 5.4538(9.6674) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.5161(17.4097) | Bit/dim 3.7156(3.7054) | Xent 0.8214(0.7922) | Loss 9.7989(10.3217) | Error 0.2989(0.2836) Steps 0(0.00) | Grad Norm 8.3096(9.3158) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 16.9527(17.2893) | Bit/dim 3.6757(3.7037) | Xent 0.8332(0.7905) | Loss 9.9011(10.2126) | Error 0.3000(0.2838) Steps 0(0.00) | Grad Norm 15.4892(9.4255) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.7879(17.3618) | Bit/dim 3.6852(3.7064) | Xent 0.6957(0.7871) | Loss 9.7074(10.1243) | Error 0.2433(0.2822) Steps 0(0.00) | Grad Norm 7.8187(9.5419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 87.3274, Epoch Time 1053.8319(1047.5312), Bit/dim 3.7163(best: 3.7102), Xent 0.8473, Loss 4.1399, Error 0.2972(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 18.0547(17.5077) | Bit/dim 3.7556(3.7079) | Xent 0.8577(0.7966) | Loss 10.1785(10.7949) | Error 0.3189(0.2853) Steps 0(0.00) | Grad Norm 16.0116(10.5815) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 16.3745(17.3478) | Bit/dim 3.7009(3.7082) | Xent 0.7611(0.7970) | Loss 9.9126(10.5634) | Error 0.2533(0.2849) Steps 0(0.00) | Grad Norm 12.4495(10.3934) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.7919(17.3505) | Bit/dim 3.6847(3.7070) | Xent 0.7959(0.7926) | Loss 10.0227(10.3939) | Error 0.2889(0.2843) Steps 0(0.00) | Grad Norm 9.1789(9.9449) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 17.7654(17.4044) | Bit/dim 3.7025(3.7064) | Xent 0.7712(0.7921) | Loss 9.7412(10.2692) | Error 0.2800(0.2839) Steps 0(0.00) | Grad Norm 7.5319(9.6204) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.7687(17.3409) | Bit/dim 3.7229(3.7069) | Xent 0.8059(0.7920) | Loss 10.0347(10.1849) | Error 0.3067(0.2845) Steps 0(0.00) | Grad Norm 12.4722(9.5551) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 17.1701(17.2567) | Bit/dim 3.6874(3.7024) | Xent 0.7538(0.7862) | Loss 9.6725(10.1032) | Error 0.2633(0.2818) Steps 0(0.00) | Grad Norm 8.9233(9.3830) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 87.5983, Epoch Time 1057.6663(1047.8353), Bit/dim 3.6987(best: 3.7102), Xent 0.8193, Loss 4.1084, Error 0.2889(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 16.7138(17.4635) | Bit/dim 3.6943(3.7022) | Xent 0.7907(0.7797) | Loss 9.8475(10.6727) | Error 0.2733(0.2794) Steps 0(0.00) | Grad Norm 8.2851(9.0340) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 17.7798(17.4851) | Bit/dim 3.7177(3.7022) | Xent 0.8161(0.7805) | Loss 9.8874(10.4674) | Error 0.2800(0.2789) Steps 0(0.00) | Grad Norm 11.3139(9.1687) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.5040(17.5212) | Bit/dim 3.7198(3.7026) | Xent 0.8466(0.7774) | Loss 9.7627(10.3097) | Error 0.2956(0.2790) Steps 0(0.00) | Grad Norm 8.1887(8.7688) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 17.6797(17.4282) | Bit/dim 3.6912(3.7028) | Xent 0.7220(0.7716) | Loss 9.7286(10.1939) | Error 0.2500(0.2764) Steps 0(0.00) | Grad Norm 9.5442(8.7158) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 17.1665(17.4750) | Bit/dim 3.6670(3.6984) | Xent 0.8839(0.7784) | Loss 9.9804(10.0864) | Error 0.3189(0.2788) Steps 0(0.00) | Grad Norm 19.8752(9.4885) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 88.5519, Epoch Time 1079.3562(1048.7809), Bit/dim 3.6994(best: 3.6987), Xent 0.8377, Loss 4.1183, Error 0.2973(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.4037(17.5870) | Bit/dim 3.7128(3.6969) | Xent 0.8075(0.7838) | Loss 10.1275(10.7659) | Error 0.2811(0.2796) Steps 0(0.00) | Grad Norm 18.6670(9.7510) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 17.5362(17.5548) | Bit/dim 3.7011(3.6995) | Xent 0.8436(0.7768) | Loss 9.9382(10.5401) | Error 0.2933(0.2774) Steps 0(0.00) | Grad Norm 22.6769(10.3389) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.3098(17.5740) | Bit/dim 3.7206(3.7033) | Xent 0.8224(0.7853) | Loss 9.5737(10.3767) | Error 0.3022(0.2813) Steps 0(0.00) | Grad Norm 10.3495(10.7554) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 17.7990(17.3785) | Bit/dim 3.6739(3.7022) | Xent 0.8236(0.7880) | Loss 9.9628(10.2479) | Error 0.2900(0.2819) Steps 0(0.00) | Grad Norm 7.8309(10.1994) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.5144(17.3976) | Bit/dim 3.6692(3.7006) | Xent 0.7884(0.7859) | Loss 9.6514(10.1496) | Error 0.2911(0.2818) Steps 0(0.00) | Grad Norm 8.6683(9.7729) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.0454(17.2550) | Bit/dim 3.7237(3.6993) | Xent 0.6813(0.7785) | Loss 9.6908(10.0735) | Error 0.2322(0.2782) Steps 0(0.00) | Grad Norm 8.8496(9.1439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 87.1674, Epoch Time 1051.0434(1048.8488), Bit/dim 3.6901(best: 3.6987), Xent 0.7935, Loss 4.0869, Error 0.2808(best: 0.2861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 18.9486(17.3142) | Bit/dim 3.7000(3.6959) | Xent 0.7780(0.7706) | Loss 9.8876(10.6198) | Error 0.2767(0.2765) Steps 0(0.00) | Grad Norm 8.3079(9.1921) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 18.4439(17.4646) | Bit/dim 3.6861(3.6927) | Xent 0.7250(0.7639) | Loss 9.6160(10.3958) | Error 0.2489(0.2735) Steps 0(0.00) | Grad Norm 12.3130(9.2166) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 18.6632(17.5249) | Bit/dim 3.6600(3.6928) | Xent 0.7414(0.7645) | Loss 9.6431(10.2582) | Error 0.2556(0.2744) Steps 0(0.00) | Grad Norm 11.5922(8.8977) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 16.1330(17.3881) | Bit/dim 3.7044(3.6936) | Xent 0.7934(0.7646) | Loss 10.0353(10.1567) | Error 0.2900(0.2743) Steps 0(0.00) | Grad Norm 11.6022(9.2923) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 20.8115(17.5384) | Bit/dim 3.7017(3.6960) | Xent 0.6875(0.7570) | Loss 9.7948(10.0772) | Error 0.2478(0.2720) Steps 0(0.00) | Grad Norm 5.7829(8.7694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 86.5362, Epoch Time 1074.8795(1049.6297), Bit/dim 3.6997(best: 3.6901), Xent 0.8512, Loss 4.1253, Error 0.3033(best: 0.2808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 18.9820(17.7225) | Bit/dim 3.7100(3.6973) | Xent 0.7565(0.7549) | Loss 9.9037(10.7738) | Error 0.2800(0.2716) Steps 0(0.00) | Grad Norm 7.4808(9.0949) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 17.8813(17.5436) | Bit/dim 3.6801(3.6958) | Xent 0.7187(0.7515) | Loss 9.6466(10.5148) | Error 0.2589(0.2699) Steps 0(0.00) | Grad Norm 6.1923(9.1727) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 19.1326(17.5096) | Bit/dim 3.6916(3.6972) | Xent 0.8044(0.7557) | Loss 10.0383(10.3425) | Error 0.2911(0.2700) Steps 0(0.00) | Grad Norm 9.9686(9.8622) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 17.3938(17.3883) | Bit/dim 3.6599(3.6948) | Xent 0.7693(0.7639) | Loss 9.7391(10.2082) | Error 0.2600(0.2723) Steps 0(0.00) | Grad Norm 8.8519(10.3302) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.1540(17.3378) | Bit/dim 3.6788(3.6929) | Xent 0.7808(0.7639) | Loss 9.7754(10.1001) | Error 0.2800(0.2719) Steps 0(0.00) | Grad Norm 7.5688(10.2386) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.9875(17.3145) | Bit/dim 3.6695(3.6930) | Xent 0.7830(0.7633) | Loss 9.8304(10.0126) | Error 0.2644(0.2720) Steps 0(0.00) | Grad Norm 8.5585(9.8835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 86.0061, Epoch Time 1051.2869(1049.6794), Bit/dim 3.6912(best: 3.6901), Xent 0.8157, Loss 4.0991, Error 0.2862(best: 0.2808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 17.6453(17.3922) | Bit/dim 3.6769(3.6882) | Xent 0.7702(0.7565) | Loss 10.0146(10.5722) | Error 0.2644(0.2704) Steps 0(0.00) | Grad Norm 12.3596(9.8582) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 18.1016(17.5577) | Bit/dim 3.7033(3.6918) | Xent 0.7335(0.7519) | Loss 9.8958(10.3926) | Error 0.2767(0.2692) Steps 0(0.00) | Grad Norm 6.6571(9.6012) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.6478(17.3529) | Bit/dim 3.7321(3.6943) | Xent 0.7776(0.7596) | Loss 9.8113(10.2582) | Error 0.2811(0.2715) Steps 0(0.00) | Grad Norm 11.7025(9.9819) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 17.8573(17.4329) | Bit/dim 3.6511(3.6934) | Xent 0.7093(0.7595) | Loss 9.7281(10.1441) | Error 0.2500(0.2705) Steps 0(0.00) | Grad Norm 12.5296(10.1461) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 16.5269(17.4021) | Bit/dim 3.7301(3.6954) | Xent 0.8007(0.7664) | Loss 9.7309(10.0751) | Error 0.2956(0.2734) Steps 0(0.00) | Grad Norm 8.3828(9.9592) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 87.8209, Epoch Time 1069.9551(1050.2877), Bit/dim 3.6925(best: 3.6901), Xent 0.7940, Loss 4.0895, Error 0.2849(best: 0.2808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 16.3438(17.3999) | Bit/dim 3.7305(3.6941) | Xent 0.7394(0.7705) | Loss 9.9508(10.7513) | Error 0.2556(0.2753) Steps 0(0.00) | Grad Norm 15.2719(10.0243) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 16.2918(17.4103) | Bit/dim 3.6677(3.6921) | Xent 0.7071(0.7624) | Loss 9.7724(10.5009) | Error 0.2544(0.2718) Steps 0(0.00) | Grad Norm 9.1330(9.9399) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 17.0527(17.4198) | Bit/dim 3.6943(3.6947) | Xent 0.7335(0.7577) | Loss 9.8214(10.3419) | Error 0.2667(0.2708) Steps 0(0.00) | Grad Norm 8.1596(9.6977) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.6915(17.5768) | Bit/dim 3.6927(3.6927) | Xent 0.7411(0.7498) | Loss 9.8648(10.2047) | Error 0.2700(0.2672) Steps 0(0.00) | Grad Norm 10.4322(9.3359) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 18.5931(17.6342) | Bit/dim 3.7187(3.6935) | Xent 0.8663(0.7505) | Loss 10.0079(10.1156) | Error 0.2989(0.2681) Steps 0(0.00) | Grad Norm 15.8164(9.6822) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 17.0824(17.8706) | Bit/dim 3.7004(3.6918) | Xent 0.7082(0.7447) | Loss 9.9452(10.0542) | Error 0.2600(0.2666) Steps 0(0.00) | Grad Norm 7.0152(8.8399) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 87.9437, Epoch Time 1079.9220(1051.1767), Bit/dim 3.6926(best: 3.6901), Xent 0.7931, Loss 4.0891, Error 0.2795(best: 0.2808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 18.2702(17.8134) | Bit/dim 3.7198(3.6915) | Xent 0.8697(0.7478) | Loss 10.1075(10.6326) | Error 0.3000(0.2680) Steps 0(0.00) | Grad Norm 13.6392(8.9702) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.7020(17.7035) | Bit/dim 3.6717(3.6903) | Xent 0.7184(0.7486) | Loss 9.7505(10.3999) | Error 0.2667(0.2676) Steps 0(0.00) | Grad Norm 10.0751(8.7465) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.8874(17.7196) | Bit/dim 3.6257(3.6891) | Xent 0.7992(0.7432) | Loss 9.8089(10.2556) | Error 0.2800(0.2659) Steps 0(0.00) | Grad Norm 19.7144(8.7293) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.9498(17.4572) | Bit/dim 3.6526(3.6880) | Xent 0.7257(0.7422) | Loss 9.7352(10.1406) | Error 0.2533(0.2658) Steps 0(0.00) | Grad Norm 3.9828(8.7062) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 16.9878(17.3818) | Bit/dim 3.6870(3.6871) | Xent 0.7329(0.7389) | Loss 9.7029(10.0473) | Error 0.2611(0.2640) Steps 0(0.00) | Grad Norm 6.7443(8.6308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 87.4126, Epoch Time 1056.4839(1051.3359), Bit/dim 3.6987(best: 3.6901), Xent 0.8227, Loss 4.1101, Error 0.2855(best: 0.2795)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 19.7573(17.4409) | Bit/dim 3.6933(3.6872) | Xent 0.8205(0.7563) | Loss 9.9818(10.7260) | Error 0.2900(0.2693) Steps 0(0.00) | Grad Norm 6.6947(9.2663) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 17.3105(17.4215) | Bit/dim 3.7006(3.6882) | Xent 0.7011(0.7449) | Loss 9.8892(10.4985) | Error 0.2478(0.2640) Steps 0(0.00) | Grad Norm 8.4673(8.9300) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 18.2716(17.4619) | Bit/dim 3.6796(3.6903) | Xent 0.7503(0.7451) | Loss 9.8618(10.3237) | Error 0.2856(0.2667) Steps 0(0.00) | Grad Norm 5.0290(8.6528) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 17.2368(17.4170) | Bit/dim 3.6756(3.6887) | Xent 0.7150(0.7399) | Loss 9.8021(10.1825) | Error 0.2511(0.2646) Steps 0(0.00) | Grad Norm 6.9052(8.2772) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 18.1299(17.5185) | Bit/dim 3.6985(3.6887) | Xent 0.7657(0.7393) | Loss 10.0368(10.1165) | Error 0.2733(0.2646) Steps 0(0.00) | Grad Norm 7.2475(8.7406) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 17.5704(17.5822) | Bit/dim 3.6866(3.6875) | Xent 0.7547(0.7399) | Loss 9.7743(10.0492) | Error 0.2733(0.2648) Steps 0(0.00) | Grad Norm 10.9365(8.9290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 87.9490, Epoch Time 1071.5922(1051.9436), Bit/dim 3.6871(best: 3.6901), Xent 0.7687, Loss 4.0714, Error 0.2706(best: 0.2795)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 17.4460(17.6070) | Bit/dim 3.6819(3.6858) | Xent 0.7514(0.7360) | Loss 9.7785(10.5790) | Error 0.2644(0.2636) Steps 0(0.00) | Grad Norm 9.1527(8.5327) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 16.4850(17.6802) | Bit/dim 3.7127(3.6846) | Xent 0.8299(0.7341) | Loss 9.8482(10.3692) | Error 0.2944(0.2633) Steps 0(0.00) | Grad Norm 15.4673(9.4354) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 15.7786(17.5324) | Bit/dim 3.7024(3.6869) | Xent 0.7816(0.7382) | Loss 9.8498(10.2415) | Error 0.2822(0.2636) Steps 0(0.00) | Grad Norm 12.0154(9.6255) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.9661(17.4682) | Bit/dim 3.6686(3.6850) | Xent 0.7176(0.7308) | Loss 9.4927(10.1001) | Error 0.2578(0.2608) Steps 0(0.00) | Grad Norm 8.9101(9.2642) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 17.1149(17.2626) | Bit/dim 3.7121(3.6844) | Xent 0.7555(0.7239) | Loss 9.8953(10.0306) | Error 0.2611(0.2579) Steps 0(0.00) | Grad Norm 6.8285(8.4006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 88.4572, Epoch Time 1057.4170(1052.1078), Bit/dim 3.6843(best: 3.6871), Xent 0.7961, Loss 4.0824, Error 0.2774(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 18.0223(17.2494) | Bit/dim 3.6804(3.6834) | Xent 0.7299(0.7223) | Loss 9.7169(10.6833) | Error 0.2578(0.2568) Steps 0(0.00) | Grad Norm 5.6317(8.2607) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 18.3348(17.4293) | Bit/dim 3.6636(3.6821) | Xent 0.6916(0.7191) | Loss 9.6076(10.4396) | Error 0.2544(0.2563) Steps 0(0.00) | Grad Norm 10.7489(8.3820) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 17.0950(17.5445) | Bit/dim 3.7069(3.6840) | Xent 0.7689(0.7336) | Loss 9.9662(10.2919) | Error 0.2756(0.2629) Steps 0(0.00) | Grad Norm 10.9891(10.0916) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.7898(17.5775) | Bit/dim 3.7034(3.6838) | Xent 0.7592(0.7370) | Loss 9.7888(10.1675) | Error 0.2700(0.2633) Steps 0(0.00) | Grad Norm 7.7205(9.5327) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.7243(17.6606) | Bit/dim 3.6568(3.6808) | Xent 0.7030(0.7306) | Loss 9.6590(10.0628) | Error 0.2500(0.2595) Steps 0(0.00) | Grad Norm 11.0785(8.9059) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 17.6843(17.7388) | Bit/dim 3.6853(3.6779) | Xent 0.7682(0.7237) | Loss 9.9832(9.9997) | Error 0.2778(0.2558) Steps 0(0.00) | Grad Norm 9.0241(8.5750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 88.5855, Epoch Time 1084.7089(1053.0858), Bit/dim 3.6772(best: 3.6843), Xent 0.7684, Loss 4.0614, Error 0.2691(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 19.2573(17.8595) | Bit/dim 3.6848(3.6759) | Xent 0.7083(0.7132) | Loss 9.8449(10.5787) | Error 0.2689(0.2520) Steps 0(0.00) | Grad Norm 6.7838(8.5205) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.6707(17.7461) | Bit/dim 3.6572(3.6738) | Xent 0.7561(0.7143) | Loss 9.9739(10.3549) | Error 0.2867(0.2546) Steps 0(0.00) | Grad Norm 5.6024(8.3995) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.2024(17.7309) | Bit/dim 3.6667(3.6766) | Xent 0.8022(0.7146) | Loss 9.9560(10.2223) | Error 0.2867(0.2558) Steps 0(0.00) | Grad Norm 8.5129(8.3117) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 19.4619(17.7571) | Bit/dim 3.7077(3.6790) | Xent 0.7035(0.7134) | Loss 9.9519(10.1147) | Error 0.2633(0.2551) Steps 0(0.00) | Grad Norm 11.1482(8.4081) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.8878(17.7712) | Bit/dim 3.6824(3.6783) | Xent 0.7485(0.7141) | Loss 9.7650(10.0380) | Error 0.2589(0.2545) Steps 0(0.00) | Grad Norm 9.4458(8.4529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 88.3858, Epoch Time 1085.5709(1054.0604), Bit/dim 3.6773(best: 3.6772), Xent 0.7955, Loss 4.0751, Error 0.2797(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.5589(17.8022) | Bit/dim 3.6738(3.6793) | Xent 0.7434(0.7212) | Loss 9.9172(10.7165) | Error 0.2389(0.2552) Steps 0(0.00) | Grad Norm 16.6514(8.8087) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 17.1756(17.6502) | Bit/dim 3.6920(3.6804) | Xent 0.7297(0.7168) | Loss 9.9783(10.4695) | Error 0.2756(0.2547) Steps 0(0.00) | Grad Norm 14.3839(9.0430) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.0256(17.5361) | Bit/dim 3.6856(3.6807) | Xent 0.6714(0.7123) | Loss 9.7402(10.2926) | Error 0.2333(0.2532) Steps 0(0.00) | Grad Norm 6.8523(9.3194) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.0797(17.5212) | Bit/dim 3.6768(3.6815) | Xent 0.6478(0.7118) | Loss 9.7935(10.1536) | Error 0.2267(0.2529) Steps 0(0.00) | Grad Norm 12.7152(9.5822) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 19.1397(17.5233) | Bit/dim 3.6734(3.6795) | Xent 0.7274(0.7162) | Loss 9.7895(10.0382) | Error 0.2644(0.2542) Steps 0(0.00) | Grad Norm 7.2811(9.5241) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 16.5586(17.5154) | Bit/dim 3.6740(3.6782) | Xent 0.7324(0.7207) | Loss 9.8919(9.9858) | Error 0.2633(0.2566) Steps 0(0.00) | Grad Norm 8.7600(9.5866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 88.0437, Epoch Time 1060.5546(1054.2552), Bit/dim 3.6900(best: 3.6772), Xent 0.7798, Loss 4.0799, Error 0.2745(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 18.2644(17.5277) | Bit/dim 3.6693(3.6789) | Xent 0.7156(0.7191) | Loss 9.8743(10.5719) | Error 0.2700(0.2565) Steps 0(0.00) | Grad Norm 13.8771(10.0781) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 18.6865(17.5738) | Bit/dim 3.6609(3.6784) | Xent 0.7069(0.7212) | Loss 9.8412(10.3742) | Error 0.2589(0.2576) Steps 0(0.00) | Grad Norm 6.6587(9.9090) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.4615(17.7306) | Bit/dim 3.6415(3.6754) | Xent 0.6692(0.7176) | Loss 9.7249(10.2197) | Error 0.2322(0.2560) Steps 0(0.00) | Grad Norm 6.0221(9.2601) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 17.6596(17.7481) | Bit/dim 3.6900(3.6766) | Xent 0.6811(0.7107) | Loss 9.8829(10.1082) | Error 0.2389(0.2544) Steps 0(0.00) | Grad Norm 7.6230(8.7412) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 18.5255(17.8614) | Bit/dim 3.6717(3.6757) | Xent 0.7209(0.7103) | Loss 9.7545(10.0107) | Error 0.2689(0.2535) Steps 0(0.00) | Grad Norm 10.7021(8.4068) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 87.6970, Epoch Time 1081.6898(1055.0783), Bit/dim 3.6699(best: 3.6772), Xent 0.7766, Loss 4.0582, Error 0.2756(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 16.3241(17.6789) | Bit/dim 3.6855(3.6775) | Xent 0.7415(0.7070) | Loss 9.5219(10.6756) | Error 0.2644(0.2531) Steps 0(0.00) | Grad Norm 9.5262(8.2414) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 20.3119(17.6977) | Bit/dim 3.6801(3.6780) | Xent 0.8090(0.7048) | Loss 9.9856(10.4436) | Error 0.2822(0.2518) Steps 0(0.00) | Grad Norm 17.4427(9.0958) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 17.6229(17.6507) | Bit/dim 3.6694(3.6770) | Xent 0.6828(0.6997) | Loss 9.7648(10.2542) | Error 0.2378(0.2500) Steps 0(0.00) | Grad Norm 6.5970(8.8073) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.0142(17.6915) | Bit/dim 3.6989(3.6791) | Xent 0.6858(0.7013) | Loss 9.9454(10.1416) | Error 0.2322(0.2493) Steps 0(0.00) | Grad Norm 8.0799(9.2352) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 18.7013(17.8258) | Bit/dim 3.6727(3.6762) | Xent 0.7132(0.6983) | Loss 9.7223(10.0354) | Error 0.2467(0.2483) Steps 0(0.00) | Grad Norm 4.9854(8.2523) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 17.7616(17.8860) | Bit/dim 3.6000(3.6717) | Xent 0.8283(0.7032) | Loss 9.8017(9.9582) | Error 0.2767(0.2513) Steps 0(0.00) | Grad Norm 20.4961(8.8914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 88.4152, Epoch Time 1084.3281(1055.9557), Bit/dim 3.6703(best: 3.6699), Xent 0.7626, Loss 4.0516, Error 0.2715(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 16.8598(17.7663) | Bit/dim 3.6913(3.6716) | Xent 0.6790(0.7040) | Loss 9.5380(10.5210) | Error 0.2378(0.2521) Steps 0(0.00) | Grad Norm 8.3278(8.8035) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 19.1703(17.8909) | Bit/dim 3.6904(3.6739) | Xent 0.6574(0.7005) | Loss 9.8524(10.3302) | Error 0.2244(0.2507) Steps 0(0.00) | Grad Norm 8.4289(9.0200) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 17.4810(17.8536) | Bit/dim 3.7046(3.6716) | Xent 0.6981(0.6997) | Loss 9.8288(10.1602) | Error 0.2367(0.2491) Steps 0(0.00) | Grad Norm 10.6042(8.9176) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.4922(17.9093) | Bit/dim 3.7287(3.6736) | Xent 0.7288(0.7080) | Loss 9.7443(10.0773) | Error 0.2678(0.2524) Steps 0(0.00) | Grad Norm 13.6282(9.8933) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 17.1782(17.7332) | Bit/dim 3.6748(3.6722) | Xent 0.7027(0.7078) | Loss 9.8660(10.0031) | Error 0.2533(0.2528) Steps 0(0.00) | Grad Norm 8.4625(9.3329) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 86.7970, Epoch Time 1079.4544(1056.6607), Bit/dim 3.6729(best: 3.6699), Xent 0.7769, Loss 4.0614, Error 0.2725(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 18.7969(17.6488) | Bit/dim 3.6160(3.6673) | Xent 0.7206(0.7030) | Loss 9.7087(10.6829) | Error 0.2433(0.2513) Steps 0(0.00) | Grad Norm 6.6348(8.8092) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.8824(17.8657) | Bit/dim 3.6645(3.6673) | Xent 0.6719(0.6945) | Loss 9.5076(10.4083) | Error 0.2422(0.2475) Steps 0(0.00) | Grad Norm 5.3829(8.4526) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.9322(17.8273) | Bit/dim 3.6520(3.6664) | Xent 0.6554(0.6879) | Loss 9.8364(10.2286) | Error 0.2311(0.2461) Steps 0(0.00) | Grad Norm 10.2877(8.1908) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 17.6844(17.9458) | Bit/dim 3.7111(3.6685) | Xent 0.6348(0.6825) | Loss 9.7526(10.1116) | Error 0.2289(0.2442) Steps 0(0.00) | Grad Norm 6.9310(8.3561) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 17.9945(17.9078) | Bit/dim 3.6635(3.6701) | Xent 0.6621(0.6871) | Loss 9.6832(10.0067) | Error 0.2456(0.2447) Steps 0(0.00) | Grad Norm 8.3821(8.3180) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 18.4508(17.9872) | Bit/dim 3.6221(3.6662) | Xent 0.6821(0.6866) | Loss 9.4752(9.9148) | Error 0.2411(0.2457) Steps 0(0.00) | Grad Norm 8.9971(8.3965) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 88.0161, Epoch Time 1096.0667(1057.8429), Bit/dim 3.6657(best: 3.6699), Xent 0.7401, Loss 4.0358, Error 0.2615(best: 0.2691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 17.3744(17.8615) | Bit/dim 3.6494(3.6678) | Xent 0.6830(0.6778) | Loss 9.9051(10.4825) | Error 0.2433(0.2423) Steps 0(0.00) | Grad Norm 7.9501(8.1125) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 18.3321(17.9553) | Bit/dim 3.6589(3.6708) | Xent 0.6722(0.6812) | Loss 9.8623(10.3003) | Error 0.2322(0.2436) Steps 0(0.00) | Grad Norm 13.0308(8.7152) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 17.2145(17.9850) | Bit/dim 3.6617(3.6690) | Xent 0.6218(0.6765) | Loss 9.5850(10.1453) | Error 0.2256(0.2417) Steps 0(0.00) | Grad Norm 7.0133(8.9137) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 19.5877(18.1608) | Bit/dim 3.6786(3.6675) | Xent 0.6618(0.6762) | Loss 9.7710(10.0460) | Error 0.2278(0.2419) Steps 0(0.00) | Grad Norm 4.7128(8.2100) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.6921(18.1023) | Bit/dim 3.6477(3.6655) | Xent 0.7051(0.6799) | Loss 9.8260(9.9628) | Error 0.2622(0.2435) Steps 0(0.00) | Grad Norm 16.2526(8.2686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 90.1365, Epoch Time 1096.1666(1058.9926), Bit/dim 3.6684(best: 3.6657), Xent 0.7663, Loss 4.0515, Error 0.2656(best: 0.2615)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.0795(17.9505) | Bit/dim 3.6672(3.6673) | Xent 0.6926(0.6831) | Loss 9.7235(10.6701) | Error 0.2467(0.2445) Steps 0(0.00) | Grad Norm 11.7188(8.9151) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 19.5623(17.8913) | Bit/dim 3.6618(3.6635) | Xent 0.6551(0.6770) | Loss 9.8127(10.4043) | Error 0.2444(0.2432) Steps 0(0.00) | Grad Norm 7.4762(8.5836) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 17.9706(18.0234) | Bit/dim 3.7029(3.6671) | Xent 0.7058(0.6763) | Loss 9.9263(10.2276) | Error 0.2344(0.2420) Steps 0(0.00) | Grad Norm 9.4400(8.5757) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 16.6064(17.9296) | Bit/dim 3.6523(3.6671) | Xent 0.6970(0.6800) | Loss 9.6328(10.0980) | Error 0.2422(0.2437) Steps 0(0.00) | Grad Norm 8.1469(8.4892) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 16.7898(17.8764) | Bit/dim 3.6638(3.6679) | Xent 0.6961(0.6895) | Loss 9.6826(10.0189) | Error 0.2522(0.2468) Steps 0(0.00) | Grad Norm 13.8916(9.1207) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.2047(17.8047) | Bit/dim 3.6963(3.6657) | Xent 0.6460(0.6860) | Loss 9.5506(9.9261) | Error 0.2300(0.2453) Steps 0(0.00) | Grad Norm 5.5977(9.0373) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 90.6868, Epoch Time 1086.2787(1059.8112), Bit/dim 3.6717(best: 3.6657), Xent 0.7672, Loss 4.0554, Error 0.2645(best: 0.2615)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 18.3712(17.9374) | Bit/dim 3.6507(3.6667) | Xent 0.6152(0.6781) | Loss 9.7294(10.5192) | Error 0.2267(0.2424) Steps 0(0.00) | Grad Norm 8.1040(9.0154) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.7312(17.9474) | Bit/dim 3.6644(3.6672) | Xent 0.6631(0.6857) | Loss 9.6955(10.2979) | Error 0.2256(0.2451) Steps 0(0.00) | Grad Norm 9.6334(9.0404) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.1348(17.8390) | Bit/dim 3.6409(3.6683) | Xent 0.7014(0.6853) | Loss 9.5625(10.1554) | Error 0.2400(0.2432) Steps 0(0.00) | Grad Norm 6.3670(8.8442) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 19.4193(17.8725) | Bit/dim 3.6400(3.6677) | Xent 0.6696(0.6814) | Loss 9.7556(10.0506) | Error 0.2178(0.2416) Steps 0(0.00) | Grad Norm 5.5404(8.4206) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.9729(17.9171) | Bit/dim 3.6574(3.6630) | Xent 0.6991(0.6764) | Loss 9.6603(9.9629) | Error 0.2500(0.2407) Steps 0(0.00) | Grad Norm 4.5544(8.2171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 90.0080, Epoch Time 1093.3877(1060.8185), Bit/dim 3.6663(best: 3.6657), Xent 0.7509, Loss 4.0418, Error 0.2625(best: 0.2615)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 16.3723(17.8955) | Bit/dim 3.6496(3.6651) | Xent 0.6748(0.6753) | Loss 9.8136(10.6368) | Error 0.2456(0.2408) Steps 0(0.00) | Grad Norm 14.4335(8.5885) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 17.5375(17.9091) | Bit/dim 3.6412(3.6635) | Xent 0.6813(0.6687) | Loss 9.6395(10.3731) | Error 0.2644(0.2396) Steps 0(0.00) | Grad Norm 11.1657(9.0421) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 18.0417(17.8241) | Bit/dim 3.6632(3.6651) | Xent 0.7171(0.6723) | Loss 9.7760(10.1965) | Error 0.2622(0.2409) Steps 0(0.00) | Grad Norm 5.7053(9.1191) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.9525(17.9030) | Bit/dim 3.6678(3.6659) | Xent 0.7325(0.6799) | Loss 9.8338(10.0819) | Error 0.2644(0.2421) Steps 0(0.00) | Grad Norm 9.0611(9.2590) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 16.9878(17.9413) | Bit/dim 3.6911(3.6697) | Xent 0.6708(0.6833) | Loss 9.7231(9.9953) | Error 0.2278(0.2431) Steps 0(0.00) | Grad Norm 5.9671(9.5101) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.8706(17.9011) | Bit/dim 3.6345(3.6662) | Xent 0.7491(0.6808) | Loss 9.7099(9.9113) | Error 0.2733(0.2415) Steps 0(0.00) | Grad Norm 5.6936(8.9189) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 89.7923, Epoch Time 1089.9210(1061.6916), Bit/dim 3.6676(best: 3.6657), Xent 0.7390, Loss 4.0371, Error 0.2614(best: 0.2615)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 19.2613(18.0104) | Bit/dim 3.6453(3.6660) | Xent 0.7098(0.6697) | Loss 9.8805(10.4937) | Error 0.2511(0.2381) Steps 0(0.00) | Grad Norm 13.3257(9.0829) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.9217(18.0229) | Bit/dim 3.7043(3.6679) | Xent 0.7466(0.6708) | Loss 9.9995(10.2927) | Error 0.2556(0.2380) Steps 0(0.00) | Grad Norm 7.8415(8.7565) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 18.0541(17.9597) | Bit/dim 3.6324(3.6653) | Xent 0.6212(0.6738) | Loss 9.6766(10.1412) | Error 0.2244(0.2385) Steps 0(0.00) | Grad Norm 10.3247(8.8065) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 18.1187(17.9349) | Bit/dim 3.6518(3.6631) | Xent 0.7584(0.6791) | Loss 9.7451(10.0293) | Error 0.2578(0.2407) Steps 0(0.00) | Grad Norm 15.3291(8.6809) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 17.2886(17.8574) | Bit/dim 3.6197(3.6630) | Xent 0.6706(0.6724) | Loss 9.7233(9.9224) | Error 0.2433(0.2392) Steps 0(0.00) | Grad Norm 6.1862(8.9388) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 88.3524, Epoch Time 1092.8239(1062.6255), Bit/dim 3.6605(best: 3.6657), Xent 0.7353, Loss 4.0282, Error 0.2576(best: 0.2614)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 20.3010(18.1338) | Bit/dim 3.6764(3.6636) | Xent 0.6643(0.6696) | Loss 9.8175(10.5836) | Error 0.2333(0.2377) Steps 0(0.00) | Grad Norm 7.5077(8.6307) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 16.7337(18.2173) | Bit/dim 3.6470(3.6620) | Xent 0.6749(0.6646) | Loss 9.6494(10.3628) | Error 0.2444(0.2372) Steps 0(0.00) | Grad Norm 10.5664(8.9174) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 18.7651(18.1524) | Bit/dim 3.6485(3.6595) | Xent 0.6627(0.6618) | Loss 9.7386(10.1865) | Error 0.2400(0.2357) Steps 0(0.00) | Grad Norm 10.8637(9.0320) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 16.8466(18.2135) | Bit/dim 3.6932(3.6598) | Xent 0.6873(0.6657) | Loss 9.4880(10.0503) | Error 0.2422(0.2366) Steps 0(0.00) | Grad Norm 8.9247(8.8699) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 17.2757(18.1032) | Bit/dim 3.6610(3.6605) | Xent 0.6603(0.6688) | Loss 9.7484(9.9550) | Error 0.2444(0.2375) Steps 0(0.00) | Grad Norm 7.4736(9.0359) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 18.2131(18.1145) | Bit/dim 3.6399(3.6597) | Xent 0.6703(0.6626) | Loss 9.7766(9.8688) | Error 0.2244(0.2354) Steps 0(0.00) | Grad Norm 7.2727(8.5521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 90.0650, Epoch Time 1111.2055(1064.0829), Bit/dim 3.6584(best: 3.6605), Xent 0.7389, Loss 4.0279, Error 0.2569(best: 0.2576)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 16.8637(18.2689) | Bit/dim 3.6799(3.6587) | Xent 0.6616(0.6525) | Loss 9.7701(10.4677) | Error 0.2267(0.2319) Steps 0(0.00) | Grad Norm 11.4325(8.1460) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 16.3096(18.1496) | Bit/dim 3.6762(3.6591) | Xent 0.6566(0.6508) | Loss 9.7383(10.2551) | Error 0.2378(0.2320) Steps 0(0.00) | Grad Norm 5.6731(7.7594) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 18.1533(18.2660) | Bit/dim 3.6756(3.6579) | Xent 0.6708(0.6537) | Loss 9.6070(10.0885) | Error 0.2378(0.2331) Steps 0(0.00) | Grad Norm 13.6651(8.5823) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 17.3537(18.1121) | Bit/dim 3.6672(3.6584) | Xent 0.6539(0.6574) | Loss 9.8755(10.0030) | Error 0.2244(0.2324) Steps 0(0.00) | Grad Norm 9.5774(8.8464) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 18.4029(17.9657) | Bit/dim 3.6695(3.6595) | Xent 0.6501(0.6600) | Loss 9.7847(9.9231) | Error 0.2267(0.2344) Steps 0(0.00) | Grad Norm 7.9953(9.0989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 88.7386, Epoch Time 1100.2347(1065.1675), Bit/dim 3.6675(best: 3.6584), Xent 0.7532, Loss 4.0441, Error 0.2633(best: 0.2569)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 17.1666(17.9062) | Bit/dim 3.6625(3.6590) | Xent 0.6334(0.6537) | Loss 9.7641(10.5648) | Error 0.2267(0.2325) Steps 0(0.00) | Grad Norm 13.8972(8.9778) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1/epoch_40_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 6.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
