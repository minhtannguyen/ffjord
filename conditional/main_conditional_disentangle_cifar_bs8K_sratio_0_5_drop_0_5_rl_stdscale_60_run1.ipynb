{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1/epoch_28_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1', scale=1.0, scale_fac=1.0, scale_std=60.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0169 | Time 105.4705(43.5508) | Bit/dim 4.9220(5.3335) | Xent 1.9729(2.0100) | Loss 12.1159(11.7582) | Error 0.6905(0.7025) Steps 0(0.00) | Grad Norm 5.6038(12.1588) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 49.9746(43.7435) | Bit/dim 4.8975(5.3204) | Xent 1.9323(2.0076) | Loss 10.7572(11.7282) | Error 0.6778(0.7018) Steps 0(0.00) | Grad Norm 2.6889(11.8747) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 44.3269(43.7610) | Bit/dim 4.9001(5.3078) | Xent 1.9598(2.0062) | Loss 10.8167(11.7009) | Error 0.6935(0.7015) Steps 0(0.00) | Grad Norm 4.9587(11.6672) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 40.9381(43.6763) | Bit/dim 4.8954(5.2954) | Xent 1.9358(2.0041) | Loss 10.4659(11.6638) | Error 0.6800(0.7009) Steps 0(0.00) | Grad Norm 2.9904(11.4069) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 44.7461(43.7084) | Bit/dim 4.8926(5.2834) | Xent 1.9266(2.0018) | Loss 10.6835(11.6344) | Error 0.6679(0.6999) Steps 0(0.00) | Grad Norm 2.7479(11.1472) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 43.3778(43.6985) | Bit/dim 4.8866(5.2715) | Xent 1.9136(1.9991) | Loss 10.5726(11.6026) | Error 0.6655(0.6988) Steps 0(0.00) | Grad Norm 2.8707(10.8989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 38.3023, Epoch Time 384.7641(302.0043), Bit/dim 4.8782(best: inf), Xent 1.8872, Loss 5.8218, Error 0.6488(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 53.3409(43.9878) | Bit/dim 4.8829(5.2598) | Xent 1.9193(1.9967) | Loss 12.6536(11.6341) | Error 0.6730(0.6981) Steps 0(0.00) | Grad Norm 4.5372(10.7080) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 43.0408(43.9594) | Bit/dim 4.8536(5.2476) | Xent 1.8900(1.9935) | Loss 10.6249(11.6038) | Error 0.6713(0.6973) Steps 0(0.00) | Grad Norm 5.4668(10.5508) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 45.2638(43.9985) | Bit/dim 4.8482(5.2356) | Xent 1.9126(1.9911) | Loss 10.3754(11.5670) | Error 0.6741(0.6966) Steps 0(0.00) | Grad Norm 4.7021(10.3753) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 46.4042(44.0707) | Bit/dim 4.8381(5.2237) | Xent 1.9037(1.9885) | Loss 10.5475(11.5364) | Error 0.6691(0.6957) Steps 0(0.00) | Grad Norm 3.3944(10.1659) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 42.2286(44.0154) | Bit/dim 4.8410(5.2122) | Xent 1.8702(1.9849) | Loss 10.5257(11.5060) | Error 0.6565(0.6946) Steps 0(0.00) | Grad Norm 2.8750(9.9472) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 46.0155(44.0754) | Bit/dim 4.8302(5.2008) | Xent 1.8914(1.9821) | Loss 10.6428(11.4801) | Error 0.6559(0.6934) Steps 0(0.00) | Grad Norm 5.3328(9.8087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 20.8718, Epoch Time 313.2560(302.3419), Bit/dim 4.8345(best: 4.8782), Xent 1.8672, Loss 5.7681, Error 0.6575(best: 0.6488)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 48.8472(44.2186) | Bit/dim 4.8146(5.1892) | Xent 1.9084(1.9799) | Loss 12.3325(11.5057) | Error 0.6720(0.6928) Steps 0(0.00) | Grad Norm 11.2539(9.8521) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 44.7907(44.2357) | Bit/dim 4.9131(5.1809) | Xent 1.9643(1.9794) | Loss 10.6517(11.4801) | Error 0.6940(0.6928) Steps 0(0.00) | Grad Norm 17.3929(10.0783) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 44.2089(44.2349) | Bit/dim 4.8453(5.1708) | Xent 1.9398(1.9783) | Loss 10.2439(11.4430) | Error 0.6870(0.6926) Steps 0(0.00) | Grad Norm 20.2991(10.3849) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 43.9556(44.2265) | Bit/dim 4.8505(5.1612) | Xent 1.9523(1.9775) | Loss 10.6961(11.4206) | Error 0.6992(0.6928) Steps 0(0.00) | Grad Norm 14.8973(10.5203) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 46.3315(44.2897) | Bit/dim 4.8032(5.1505) | Xent 1.8817(1.9746) | Loss 10.4108(11.3903) | Error 0.6500(0.6915) Steps 0(0.00) | Grad Norm 2.7452(10.2870) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 47.8547(44.3966) | Bit/dim 4.8257(5.1407) | Xent 1.9662(1.9744) | Loss 10.7760(11.3719) | Error 0.6914(0.6915) Steps 0(0.00) | Grad Norm 16.6170(10.4769) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 19.7641, Epoch Time 311.4143(302.6141), Bit/dim 4.8650(best: 4.8345), Xent 1.8873, Loss 5.8086, Error 0.6532(best: 0.6488)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 51.5998(44.6127) | Bit/dim 4.8733(5.1327) | Xent 1.8971(1.9720) | Loss 12.8284(11.4156) | Error 0.6679(0.6908) Steps 0(0.00) | Grad Norm 10.9151(10.4901) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 50.1988(44.7803) | Bit/dim 4.8224(5.1234) | Xent 2.0592(1.9746) | Loss 10.7155(11.3946) | Error 0.7280(0.6919) Steps 0(0.00) | Grad Norm 22.3671(10.8464) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 43.2271(44.7337) | Bit/dim 4.9159(5.1172) | Xent 1.9939(1.9752) | Loss 10.4633(11.3666) | Error 0.7112(0.6925) Steps 0(0.00) | Grad Norm 13.1380(10.9151) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 44.3950(44.7236) | Bit/dim 4.8530(5.1093) | Xent 1.9884(1.9756) | Loss 10.7112(11.3470) | Error 0.7084(0.6930) Steps 0(0.00) | Grad Norm 12.2225(10.9544) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 42.5303(44.6578) | Bit/dim 4.8746(5.1022) | Xent 1.9834(1.9759) | Loss 10.3569(11.3173) | Error 0.7059(0.6934) Steps 0(0.00) | Grad Norm 14.1305(11.0497) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 44.2194(44.6446) | Bit/dim 4.8467(5.0946) | Xent 1.9931(1.9764) | Loss 10.7359(11.2998) | Error 0.7230(0.6943) Steps 0(0.00) | Grad Norm 9.3499(10.9987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 20.3933, Epoch Time 312.2938(302.9044), Bit/dim 4.8508(best: 4.8345), Xent 1.9374, Loss 5.8195, Error 0.6798(best: 0.6488)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 44.0854(44.6278) | Bit/dim 4.8465(5.0871) | Xent 1.9615(1.9759) | Loss 12.8977(11.3478) | Error 0.7020(0.6945) Steps 0(0.00) | Grad Norm 9.4748(10.9529) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 44.5688(44.6261) | Bit/dim 4.8297(5.0794) | Xent 1.9359(1.9747) | Loss 10.6881(11.3280) | Error 0.6904(0.6944) Steps 0(0.00) | Grad Norm 10.7077(10.9456) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 41.7981(44.5412) | Bit/dim 4.7981(5.0710) | Xent 1.8915(1.9722) | Loss 10.6234(11.3068) | Error 0.6650(0.6935) Steps 0(0.00) | Grad Norm 7.7286(10.8491) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 44.5174(44.5405) | Bit/dim 4.8132(5.0632) | Xent 1.8683(1.9691) | Loss 10.5531(11.2842) | Error 0.6566(0.6924) Steps 0(0.00) | Grad Norm 5.0185(10.6742) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 52.5236(44.7800) | Bit/dim 4.7896(5.0550) | Xent 1.8962(1.9669) | Loss 10.6543(11.2653) | Error 0.6658(0.6916) Steps 0(0.00) | Grad Norm 7.5701(10.5810) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 46.9882(44.8463) | Bit/dim 4.7726(5.0465) | Xent 1.8733(1.9641) | Loss 10.4235(11.2401) | Error 0.6705(0.6910) Steps 0(0.00) | Grad Norm 4.4258(10.3964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 18.9114, Epoch Time 309.1280(303.0912), Bit/dim 4.7758(best: 4.8345), Xent 1.8461, Loss 5.6988, Error 0.6414(best: 0.6488)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 41.4374(44.7440) | Bit/dim 4.7739(5.0384) | Xent 1.8717(1.9613) | Loss 12.2872(11.2715) | Error 0.6595(0.6900) Steps 0(0.00) | Grad Norm 4.1566(10.2092) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 42.7478(44.6841) | Bit/dim 4.7628(5.0301) | Xent 1.8454(1.9579) | Loss 10.3691(11.2444) | Error 0.6530(0.6889) Steps 0(0.00) | Grad Norm 3.3238(10.0026) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 42.0816(44.6060) | Bit/dim 4.7310(5.0211) | Xent 1.8752(1.9554) | Loss 10.1585(11.2118) | Error 0.6604(0.6880) Steps 0(0.00) | Grad Norm 4.5349(9.8386) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 38.1393(44.4120) | Bit/dim 4.7410(5.0127) | Xent 1.8436(1.9520) | Loss 10.2766(11.1838) | Error 0.6507(0.6869) Steps 0(0.00) | Grad Norm 6.2624(9.7313) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 43.8926(44.3964) | Bit/dim 4.7250(5.0041) | Xent 1.8653(1.9494) | Loss 10.4290(11.1611) | Error 0.6610(0.6862) Steps 0(0.00) | Grad Norm 6.2667(9.6274) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 40.0772(44.2669) | Bit/dim 4.7128(4.9953) | Xent 1.8387(1.9461) | Loss 10.2696(11.1344) | Error 0.6496(0.6851) Steps 0(0.00) | Grad Norm 7.2743(9.5568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 18.9979, Epoch Time 283.0487(302.4899), Bit/dim 4.6992(best: 4.7758), Xent 1.8142, Loss 5.6063, Error 0.6343(best: 0.6414)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 41.5635(44.1858) | Bit/dim 4.7039(4.9866) | Xent 1.8234(1.9424) | Loss 11.4140(11.1428) | Error 0.6406(0.6837) Steps 0(0.00) | Grad Norm 5.2624(9.4279) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 43.0494(44.1517) | Bit/dim 4.6834(4.9775) | Xent 1.8533(1.9398) | Loss 10.1624(11.1134) | Error 0.6562(0.6829) Steps 0(0.00) | Grad Norm 3.2437(9.2424) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 40.0706(44.0292) | Bit/dim 4.6747(4.9684) | Xent 1.8446(1.9369) | Loss 9.8841(11.0765) | Error 0.6546(0.6821) Steps 0(0.00) | Grad Norm 5.8485(9.1406) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 41.9668(43.9674) | Bit/dim 4.6729(4.9596) | Xent 1.8213(1.9334) | Loss 10.1602(11.0490) | Error 0.6442(0.6809) Steps 0(0.00) | Grad Norm 5.1623(9.0213) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 41.7986(43.9023) | Bit/dim 4.6764(4.9511) | Xent 1.7900(1.9291) | Loss 10.3151(11.0270) | Error 0.6361(0.6796) Steps 0(0.00) | Grad Norm 3.0767(8.8429) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 44.5431(43.9215) | Bit/dim 4.6586(4.9423) | Xent 1.8197(1.9258) | Loss 10.0684(10.9982) | Error 0.6458(0.6786) Steps 0(0.00) | Grad Norm 5.1955(8.7335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 20.0647, Epoch Time 288.7398(302.0774), Bit/dim 4.7050(best: 4.6992), Xent 1.7715, Loss 5.5907, Error 0.6159(best: 0.6343)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 39.2163(43.7804) | Bit/dim 4.7060(4.9352) | Xent 1.7855(1.9216) | Loss 12.0779(11.0306) | Error 0.6254(0.6770) Steps 0(0.00) | Grad Norm 9.8593(8.7673) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 41.1065(43.7001) | Bit/dim 4.8062(4.9313) | Xent 1.9027(1.9211) | Loss 10.4076(11.0119) | Error 0.6641(0.6766) Steps 0(0.00) | Grad Norm 33.7227(9.5159) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 50.5226(43.9048) | Bit/dim 4.8197(4.9280) | Xent 1.8865(1.9200) | Loss 10.7521(11.0041) | Error 0.6684(0.6763) Steps 0(0.00) | Grad Norm 24.5741(9.9677) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 42.7997(43.8717) | Bit/dim 4.7228(4.9218) | Xent 1.8081(1.9167) | Loss 10.1892(10.9797) | Error 0.6391(0.6752) Steps 0(0.00) | Grad Norm 6.9496(9.8771) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 42.7682(43.8386) | Bit/dim 4.7729(4.9174) | Xent 1.8853(1.9157) | Loss 10.4648(10.9642) | Error 0.6666(0.6750) Steps 0(0.00) | Grad Norm 25.4114(10.3432) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 45.2908(43.8821) | Bit/dim 4.6604(4.9096) | Xent 1.8331(1.9133) | Loss 10.3513(10.9459) | Error 0.6492(0.6742) Steps 0(0.00) | Grad Norm 10.3869(10.3445) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 19.9286, Epoch Time 297.1479(301.9295), Bit/dim 4.7470(best: 4.6992), Xent 1.7662, Loss 5.6301, Error 0.6165(best: 0.6159)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 41.1844(43.8012) | Bit/dim 4.7524(4.9049) | Xent 1.8091(1.9101) | Loss 11.9077(10.9747) | Error 0.6330(0.6730) Steps 0(0.00) | Grad Norm 7.9960(10.2740) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 45.4166(43.8497) | Bit/dim 4.6458(4.8972) | Xent 1.7883(1.9065) | Loss 10.2150(10.9519) | Error 0.6298(0.6717) Steps 0(0.00) | Grad Norm 5.4528(10.1294) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 44.3012(43.8632) | Bit/dim 4.7283(4.8921) | Xent 1.8311(1.9042) | Loss 10.2824(10.9318) | Error 0.6499(0.6710) Steps 0(0.00) | Grad Norm 14.4466(10.2589) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 44.0101(43.8676) | Bit/dim 4.6607(4.8851) | Xent 1.8465(1.9025) | Loss 9.6710(10.8940) | Error 0.6474(0.6703) Steps 0(0.00) | Grad Norm 12.9880(10.3408) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 42.0773(43.8139) | Bit/dim 4.7388(4.8808) | Xent 1.7907(1.8991) | Loss 10.3295(10.8771) | Error 0.6276(0.6690) Steps 0(0.00) | Grad Norm 9.0280(10.3014) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 41.6701(43.7496) | Bit/dim 4.6673(4.8744) | Xent 1.8477(1.8976) | Loss 10.1381(10.8549) | Error 0.6560(0.6686) Steps 0(0.00) | Grad Norm 16.0491(10.4738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 20.1432, Epoch Time 294.6317(301.7106), Bit/dim 4.6411(best: 4.6992), Xent 1.8373, Loss 5.5598, Error 0.6564(best: 0.6159)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 45.1933(43.7929) | Bit/dim 4.6467(4.8675) | Xent 1.8866(1.8973) | Loss 12.2812(10.8977) | Error 0.6663(0.6686) Steps 0(0.00) | Grad Norm 16.5239(10.6553) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 50.3656(43.9901) | Bit/dim 4.6249(4.8603) | Xent 1.7969(1.8942) | Loss 10.2332(10.8778) | Error 0.6344(0.6675) Steps 0(0.00) | Grad Norm 5.5715(10.5028) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 41.4385(43.9135) | Bit/dim 4.6139(4.8529) | Xent 1.8287(1.8923) | Loss 9.8956(10.8483) | Error 0.6547(0.6671) Steps 0(0.00) | Grad Norm 10.1674(10.4928) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 41.8499(43.8516) | Bit/dim 4.6053(4.8454) | Xent 1.7929(1.8893) | Loss 10.0494(10.8243) | Error 0.6259(0.6659) Steps 0(0.00) | Grad Norm 5.0789(10.3303) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 43.0955(43.8289) | Bit/dim 4.5928(4.8379) | Xent 1.7926(1.8864) | Loss 10.1290(10.8035) | Error 0.6391(0.6651) Steps 0(0.00) | Grad Norm 9.0453(10.2918) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 42.2293(43.7810) | Bit/dim 4.5866(4.8303) | Xent 1.7676(1.8828) | Loss 10.0461(10.7807) | Error 0.6209(0.6638) Steps 0(0.00) | Grad Norm 5.3354(10.1431) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 20.7971, Epoch Time 300.7926(301.6830), Bit/dim 4.5783(best: 4.6411), Xent 1.7477, Loss 5.4522, Error 0.6124(best: 0.6159)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 43.2948(43.7664) | Bit/dim 4.5717(4.8226) | Xent 1.7786(1.8797) | Loss 12.1578(10.8221) | Error 0.6334(0.6629) Steps 0(0.00) | Grad Norm 4.8688(9.9849) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 47.7728(43.8866) | Bit/dim 4.5575(4.8146) | Xent 1.7914(1.8771) | Loss 9.8321(10.7924) | Error 0.6340(0.6620) Steps 0(0.00) | Grad Norm 6.6709(9.8854) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 45.1402(43.9242) | Bit/dim 4.5821(4.8076) | Xent 1.7726(1.8739) | Loss 9.6748(10.7588) | Error 0.6321(0.6611) Steps 0(0.00) | Grad Norm 6.2846(9.7774) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 50.0104(44.1068) | Bit/dim 4.5784(4.8008) | Xent 1.7561(1.8704) | Loss 10.1263(10.7399) | Error 0.6174(0.6598) Steps 0(0.00) | Grad Norm 9.5583(9.7708) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 45.1063(44.1367) | Bit/dim 4.5561(4.7934) | Xent 1.7544(1.8669) | Loss 10.0035(10.7178) | Error 0.6228(0.6587) Steps 0(0.00) | Grad Norm 5.3605(9.6385) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 47.8127(44.2470) | Bit/dim 4.5569(4.7863) | Xent 1.7465(1.8633) | Loss 10.0323(10.6972) | Error 0.6162(0.6574) Steps 0(0.00) | Grad Norm 4.7458(9.4918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 20.8582, Epoch Time 316.1470(302.1169), Bit/dim 4.5468(best: 4.5783), Xent 1.7088, Loss 5.4012, Error 0.5992(best: 0.6124)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 45.2726(44.2778) | Bit/dim 4.5425(4.7790) | Xent 1.7315(1.8593) | Loss 11.7015(10.7273) | Error 0.6126(0.6561) Steps 0(0.00) | Grad Norm 4.5770(9.3443) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 46.3878(44.3411) | Bit/dim 4.5224(4.7713) | Xent 1.7346(1.8556) | Loss 10.1087(10.7088) | Error 0.6156(0.6548) Steps 0(0.00) | Grad Norm 4.5760(9.2013) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 46.3694(44.4019) | Bit/dim 4.5306(4.7641) | Xent 1.7618(1.8528) | Loss 10.0477(10.6889) | Error 0.6213(0.6538) Steps 0(0.00) | Grad Norm 11.0618(9.2571) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 42.2258(44.3367) | Bit/dim 4.5321(4.7571) | Xent 1.7707(1.8503) | Loss 10.1159(10.6717) | Error 0.6265(0.6530) Steps 0(0.00) | Grad Norm 13.8894(9.3960) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 41.9020(44.2636) | Bit/dim 4.5348(4.7505) | Xent 1.7792(1.8482) | Loss 9.9407(10.6498) | Error 0.6275(0.6523) Steps 0(0.00) | Grad Norm 15.3866(9.5758) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 45.3880(44.2973) | Bit/dim 4.5959(4.7458) | Xent 1.9041(1.8499) | Loss 10.2386(10.6375) | Error 0.6604(0.6525) Steps 0(0.00) | Grad Norm 29.0311(10.1594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 20.7172, Epoch Time 304.0010(302.1735), Bit/dim 4.5834(best: 4.5468), Xent 1.7592, Loss 5.4631, Error 0.6183(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 46.2347(44.3555) | Bit/dim 4.5791(4.7408) | Xent 1.7756(1.8476) | Loss 11.3986(10.6603) | Error 0.6321(0.6519) Steps 0(0.00) | Grad Norm 14.9051(10.3018) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 48.6331(44.4838) | Bit/dim 4.5157(4.7341) | Xent 1.7262(1.8440) | Loss 9.9538(10.6391) | Error 0.6145(0.6508) Steps 0(0.00) | Grad Norm 4.6189(10.1313) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 40.4927(44.3641) | Bit/dim 4.5746(4.7293) | Xent 1.7501(1.8412) | Loss 9.6111(10.6083) | Error 0.6156(0.6497) Steps 0(0.00) | Grad Norm 11.6806(10.1778) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 41.5364(44.2792) | Bit/dim 4.6316(4.7264) | Xent 1.7707(1.8391) | Loss 10.1777(10.5954) | Error 0.6286(0.6491) Steps 0(0.00) | Grad Norm 16.9039(10.3796) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 40.2640(44.1588) | Bit/dim 4.6032(4.7227) | Xent 1.9153(1.8414) | Loss 10.1304(10.5814) | Error 0.6769(0.6499) Steps 0(0.00) | Grad Norm 28.4072(10.9204) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 43.3672(44.1350) | Bit/dim 4.5203(4.7166) | Xent 1.7561(1.8388) | Loss 9.9585(10.5627) | Error 0.6270(0.6492) Steps 0(0.00) | Grad Norm 12.2386(10.9599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 20.3513, Epoch Time 296.3919(302.0000), Bit/dim 4.6158(best: 4.5468), Xent 1.8759, Loss 5.5538, Error 0.6725(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 47.6660(44.2410) | Bit/dim 4.6138(4.7135) | Xent 1.9205(1.8412) | Loss 12.2045(10.6120) | Error 0.6887(0.6504) Steps 0(0.00) | Grad Norm 22.6781(11.3115) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 47.1878(44.3294) | Bit/dim 4.5135(4.7075) | Xent 1.8148(1.8404) | Loss 10.1171(10.5971) | Error 0.6478(0.6503) Steps 0(0.00) | Grad Norm 10.9994(11.3021) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 45.4672(44.3635) | Bit/dim 4.5501(4.7028) | Xent 1.9327(1.8432) | Loss 10.1378(10.5833) | Error 0.6956(0.6517) Steps 0(0.00) | Grad Norm 19.8690(11.5591) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 48.2271(44.4794) | Bit/dim 4.6040(4.6998) | Xent 1.8735(1.8441) | Loss 10.0786(10.5682) | Error 0.6614(0.6520) Steps 0(0.00) | Grad Norm 20.3042(11.8215) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 43.5455(44.4514) | Bit/dim 4.5297(4.6947) | Xent 1.8648(1.8447) | Loss 9.9407(10.5494) | Error 0.6746(0.6527) Steps 0(0.00) | Grad Norm 6.4425(11.6601) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 46.3992(44.5098) | Bit/dim 4.5465(4.6903) | Xent 1.8935(1.8462) | Loss 10.0538(10.5345) | Error 0.6871(0.6537) Steps 0(0.00) | Grad Norm 19.1016(11.8834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 20.6699, Epoch Time 314.9075(302.3872), Bit/dim 4.6018(best: 4.5468), Xent 1.7936, Loss 5.4986, Error 0.6344(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 47.1010(44.5876) | Bit/dim 4.6039(4.6877) | Xent 1.8242(1.8455) | Loss 11.7795(10.5719) | Error 0.6486(0.6535) Steps 0(0.00) | Grad Norm 12.9764(11.9161) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 48.4997(44.7049) | Bit/dim 4.5012(4.6821) | Xent 1.8213(1.8448) | Loss 9.8992(10.5517) | Error 0.6544(0.6536) Steps 0(0.00) | Grad Norm 8.5500(11.8152) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 48.2253(44.8105) | Bit/dim 4.5258(4.6774) | Xent 1.8308(1.8444) | Loss 10.1806(10.5406) | Error 0.6486(0.6534) Steps 0(0.00) | Grad Norm 9.1280(11.7346) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 46.2831(44.8547) | Bit/dim 4.5178(4.6726) | Xent 1.7828(1.8425) | Loss 9.9077(10.5216) | Error 0.6351(0.6529) Steps 0(0.00) | Grad Norm 8.3787(11.6339) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 49.7485(45.0015) | Bit/dim 4.5317(4.6684) | Xent 1.7478(1.8397) | Loss 9.8212(10.5006) | Error 0.6214(0.6519) Steps 0(0.00) | Grad Norm 6.2719(11.4730) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 45.2651(45.0094) | Bit/dim 4.5233(4.6640) | Xent 1.7799(1.8379) | Loss 9.9318(10.4835) | Error 0.6296(0.6513) Steps 0(0.00) | Grad Norm 14.3410(11.5591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 21.5506, Epoch Time 322.7351(302.9977), Bit/dim 4.5547(best: 4.5468), Xent 1.8343, Loss 5.4718, Error 0.6418(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 43.4410(44.9624) | Bit/dim 4.5604(4.6609) | Xent 1.8867(1.8394) | Loss 11.5938(10.5168) | Error 0.6510(0.6512) Steps 0(0.00) | Grad Norm 23.5082(11.9175) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 46.8692(45.0196) | Bit/dim 4.6868(4.6617) | Xent 1.8757(1.8405) | Loss 10.0661(10.5033) | Error 0.6603(0.6515) Steps 0(0.00) | Grad Norm 28.9239(12.4277) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 41.2596(44.9068) | Bit/dim 4.7930(4.6656) | Xent 1.8097(1.8395) | Loss 10.4477(10.5016) | Error 0.6312(0.6509) Steps 0(0.00) | Grad Norm 16.0223(12.5356) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 47.8373(44.9947) | Bit/dim 4.5022(4.6607) | Xent 1.7962(1.8382) | Loss 9.8938(10.4834) | Error 0.6376(0.6505) Steps 0(0.00) | Grad Norm 4.9643(12.3084) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 44.7545(44.9875) | Bit/dim 4.6449(4.6603) | Xent 1.8303(1.8380) | Loss 10.2330(10.4759) | Error 0.6505(0.6505) Steps 0(0.00) | Grad Norm 16.3329(12.4292) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 42.4320(44.9108) | Bit/dim 4.5843(4.6580) | Xent 1.7536(1.8355) | Loss 9.9526(10.4602) | Error 0.6193(0.6496) Steps 0(0.00) | Grad Norm 8.7110(12.3176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 20.5056, Epoch Time 302.8788(302.9941), Bit/dim 4.6306(best: 4.5468), Xent 1.7312, Loss 5.4962, Error 0.6018(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 42.5829(44.8410) | Bit/dim 4.6343(4.6573) | Xent 1.7479(1.8328) | Loss 12.2253(10.5131) | Error 0.6104(0.6484) Steps 0(0.00) | Grad Norm 8.2447(12.1954) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 42.7704(44.7789) | Bit/dim 4.5685(4.6546) | Xent 1.7602(1.8307) | Loss 9.8069(10.4919) | Error 0.6222(0.6476) Steps 0(0.00) | Grad Norm 6.1973(12.0155) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 45.0340(44.7865) | Bit/dim 4.5454(4.6513) | Xent 1.7714(1.8289) | Loss 10.0045(10.4773) | Error 0.6351(0.6472) Steps 0(0.00) | Grad Norm 9.4293(11.9379) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 42.3236(44.7126) | Bit/dim 4.5493(4.6483) | Xent 1.7422(1.8263) | Loss 9.9014(10.4600) | Error 0.6173(0.6463) Steps 0(0.00) | Grad Norm 6.6665(11.7798) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 45.5813(44.7387) | Bit/dim 4.5009(4.6438) | Xent 1.7494(1.8240) | Loss 9.9818(10.4457) | Error 0.6240(0.6457) Steps 0(0.00) | Grad Norm 5.9312(11.6043) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 42.6433(44.6758) | Bit/dim 4.5037(4.6396) | Xent 1.7450(1.8216) | Loss 9.5763(10.4196) | Error 0.6168(0.6448) Steps 0(0.00) | Grad Norm 4.1028(11.3792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 19.8781, Epoch Time 296.3481(302.7947), Bit/dim 4.5001(best: 4.5468), Xent 1.7066, Loss 5.3534, Error 0.6064(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 43.9387(44.6537) | Bit/dim 4.5034(4.6356) | Xent 1.7412(1.8192) | Loss 11.5485(10.4535) | Error 0.6145(0.6439) Steps 0(0.00) | Grad Norm 6.0425(11.2191) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 45.7993(44.6881) | Bit/dim 4.4903(4.6312) | Xent 1.7410(1.8169) | Loss 9.6866(10.4305) | Error 0.6164(0.6431) Steps 0(0.00) | Grad Norm 5.4769(11.0469) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 42.9156(44.6349) | Bit/dim 4.4539(4.6259) | Xent 1.7413(1.8146) | Loss 9.7720(10.4107) | Error 0.6078(0.6420) Steps 0(0.00) | Grad Norm 11.6986(11.0664) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 42.7536(44.5785) | Bit/dim 4.4747(4.6213) | Xent 1.7631(1.8130) | Loss 9.7838(10.3919) | Error 0.6211(0.6414) Steps 0(0.00) | Grad Norm 14.5254(11.1702) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 42.3684(44.5122) | Bit/dim 4.4509(4.6162) | Xent 1.7339(1.8107) | Loss 9.8560(10.3758) | Error 0.6222(0.6408) Steps 0(0.00) | Grad Norm 9.6035(11.1232) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 50.2179(44.6834) | Bit/dim 4.4314(4.6107) | Xent 1.7421(1.8086) | Loss 9.8748(10.3608) | Error 0.6200(0.6402) Steps 0(0.00) | Grad Norm 8.8911(11.0562) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 19.8012, Epoch Time 303.5341(302.8169), Bit/dim 4.4313(best: 4.5001), Xent 1.6771, Loss 5.2699, Error 0.5955(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 47.6437(44.7722) | Bit/dim 4.4201(4.6050) | Xent 1.7082(1.8056) | Loss 12.1757(10.4152) | Error 0.6111(0.6393) Steps 0(0.00) | Grad Norm 5.1780(10.8799) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 42.8342(44.7140) | Bit/dim 4.4166(4.5993) | Xent 1.7114(1.8028) | Loss 9.3764(10.3841) | Error 0.6106(0.6385) Steps 0(0.00) | Grad Norm 5.3016(10.7125) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 46.6010(44.7706) | Bit/dim 4.4076(4.5936) | Xent 1.7128(1.8001) | Loss 9.8054(10.3667) | Error 0.6173(0.6378) Steps 0(0.00) | Grad Norm 10.0111(10.6915) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 43.1865(44.7231) | Bit/dim 4.4068(4.5880) | Xent 1.7415(1.7983) | Loss 9.9006(10.3527) | Error 0.6139(0.6371) Steps 0(0.00) | Grad Norm 13.3785(10.7721) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 49.3507(44.8619) | Bit/dim 4.3951(4.5822) | Xent 1.7318(1.7963) | Loss 9.6379(10.3313) | Error 0.6088(0.6362) Steps 0(0.00) | Grad Norm 13.9754(10.8682) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 43.4038(44.8182) | Bit/dim 4.4018(4.5768) | Xent 1.7141(1.7939) | Loss 9.7759(10.3146) | Error 0.6108(0.6355) Steps 0(0.00) | Grad Norm 12.3242(10.9119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 21.3749, Epoch Time 310.3626(303.0433), Bit/dim 4.4333(best: 4.4313), Xent 1.7029, Loss 5.2848, Error 0.6057(best: 0.5955)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 46.8405(44.8789) | Bit/dim 4.4174(4.5720) | Xent 1.7635(1.7929) | Loss 12.2165(10.3717) | Error 0.6245(0.6352) Steps 0(0.00) | Grad Norm 18.9316(11.1525) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 50.3459(45.0429) | Bit/dim 4.4990(4.5698) | Xent 1.8619(1.7950) | Loss 9.8554(10.3562) | Error 0.6636(0.6360) Steps 0(0.00) | Grad Norm 23.6917(11.5287) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 49.9962(45.1915) | Bit/dim 4.4104(4.5650) | Xent 1.7223(1.7928) | Loss 9.6645(10.3354) | Error 0.6120(0.6353) Steps 0(0.00) | Grad Norm 12.5373(11.5589) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 48.4177(45.2883) | Bit/dim 4.4513(4.5616) | Xent 1.8284(1.7939) | Loss 9.7582(10.3181) | Error 0.6450(0.6356) Steps 0(0.00) | Grad Norm 25.7356(11.9842) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 45.1638(45.2845) | Bit/dim 4.4074(4.5570) | Xent 1.8163(1.7946) | Loss 9.9086(10.3058) | Error 0.6444(0.6358) Steps 0(0.00) | Grad Norm 19.7982(12.2186) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 45.7434(45.2983) | Bit/dim 4.4642(4.5542) | Xent 1.7765(1.7940) | Loss 9.9752(10.2959) | Error 0.6339(0.6358) Steps 0(0.00) | Grad Norm 12.4394(12.2253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 20.9682, Epoch Time 323.4163(303.6545), Bit/dim 4.3783(best: 4.4313), Xent 1.7018, Loss 5.2292, Error 0.6080(best: 0.5955)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 45.0065(45.2895) | Bit/dim 4.3774(4.5489) | Xent 1.7621(1.7931) | Loss 11.8943(10.3439) | Error 0.6322(0.6357) Steps 0(0.00) | Grad Norm 11.3444(12.1988) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 51.6998(45.4818) | Bit/dim 4.4491(4.5459) | Xent 1.8494(1.7948) | Loss 10.1043(10.3367) | Error 0.6610(0.6364) Steps 0(0.00) | Grad Norm 22.8204(12.5175) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 46.6328(45.5164) | Bit/dim 4.4185(4.5421) | Xent 1.7017(1.7920) | Loss 9.9130(10.3240) | Error 0.6188(0.6359) Steps 0(0.00) | Grad Norm 10.4091(12.4542) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 50.1753(45.6561) | Bit/dim 4.3580(4.5365) | Xent 1.7472(1.7906) | Loss 9.6544(10.3039) | Error 0.6195(0.6354) Steps 0(0.00) | Grad Norm 8.8226(12.3453) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 45.2582(45.6442) | Bit/dim 4.3971(4.5324) | Xent 1.7827(1.7904) | Loss 9.3309(10.2747) | Error 0.6366(0.6354) Steps 0(0.00) | Grad Norm 9.8038(12.2690) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 44.4760(45.6092) | Bit/dim 4.3576(4.5271) | Xent 1.7540(1.7893) | Loss 9.7395(10.2586) | Error 0.6239(0.6351) Steps 0(0.00) | Grad Norm 8.4259(12.1537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 21.5898, Epoch Time 320.9698(304.1739), Bit/dim 4.3679(best: 4.3783), Xent 1.6811, Loss 5.2085, Error 0.5985(best: 0.5955)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 50.3277(45.7507) | Bit/dim 4.3616(4.5222) | Xent 1.7357(1.7877) | Loss 12.1656(10.3159) | Error 0.6146(0.6345) Steps 0(0.00) | Grad Norm 6.8084(11.9934) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 42.8357(45.6633) | Bit/dim 4.3628(4.5174) | Xent 1.7035(1.7852) | Loss 9.7039(10.2975) | Error 0.5996(0.6334) Steps 0(0.00) | Grad Norm 7.3009(11.8526) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 48.8141(45.7578) | Bit/dim 4.3459(4.5122) | Xent 1.7148(1.7831) | Loss 9.6261(10.2774) | Error 0.6099(0.6327) Steps 0(0.00) | Grad Norm 5.9574(11.6757) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 42.5010(45.6601) | Bit/dim 4.3518(4.5074) | Xent 1.6814(1.7800) | Loss 9.4642(10.2530) | Error 0.5945(0.6316) Steps 0(0.00) | Grad Norm 6.6562(11.5252) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 48.4662(45.7443) | Bit/dim 4.3356(4.5023) | Xent 1.6822(1.7771) | Loss 9.7580(10.2381) | Error 0.5986(0.6306) Steps 0(0.00) | Grad Norm 7.8697(11.4155) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 45.8720(45.7481) | Bit/dim 4.3314(4.4971) | Xent 1.6829(1.7742) | Loss 9.7012(10.2220) | Error 0.6085(0.6299) Steps 0(0.00) | Grad Norm 9.0175(11.3436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 20.1872, Epoch Time 314.6332(304.4877), Bit/dim 4.3398(best: 4.3679), Xent 1.6280, Loss 5.1539, Error 0.5773(best: 0.5955)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 46.8583(45.7814) | Bit/dim 4.3313(4.4922) | Xent 1.6661(1.7710) | Loss 11.4806(10.2598) | Error 0.5929(0.6288) Steps 0(0.00) | Grad Norm 5.3385(11.1634) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 47.7659(45.8409) | Bit/dim 4.3217(4.4870) | Xent 1.6665(1.7679) | Loss 9.5809(10.2394) | Error 0.5909(0.6277) Steps 0(0.00) | Grad Norm 4.0880(10.9511) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 43.9625(45.7846) | Bit/dim 4.3064(4.4816) | Xent 1.7030(1.7659) | Loss 9.5338(10.2182) | Error 0.6135(0.6273) Steps 0(0.00) | Grad Norm 9.8532(10.9182) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 43.0518(45.7026) | Bit/dim 4.3108(4.4765) | Xent 1.6674(1.7630) | Loss 9.5461(10.1981) | Error 0.5909(0.6262) Steps 0(0.00) | Grad Norm 11.4051(10.9328) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 47.3871(45.7531) | Bit/dim 4.2990(4.4712) | Xent 1.6774(1.7604) | Loss 9.4172(10.1746) | Error 0.6013(0.6254) Steps 0(0.00) | Grad Norm 11.7516(10.9574) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 40.6472(45.6000) | Bit/dim 4.3430(4.4673) | Xent 1.6709(1.7577) | Loss 9.5931(10.1572) | Error 0.5976(0.6246) Steps 0(0.00) | Grad Norm 11.5934(10.9765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 20.4872, Epoch Time 305.9831(304.5326), Bit/dim 4.3908(best: 4.3398), Xent 1.6162, Loss 5.1989, Error 0.5805(best: 0.5773)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 52.8346(45.8170) | Bit/dim 4.3860(4.4649) | Xent 1.6922(1.7557) | Loss 11.9842(10.2120) | Error 0.6024(0.6239) Steps 0(0.00) | Grad Norm 16.1200(11.1308) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 48.8869(45.9091) | Bit/dim 4.4756(4.4652) | Xent 1.7219(1.7547) | Loss 10.0722(10.2078) | Error 0.6114(0.6235) Steps 0(0.00) | Grad Norm 13.8948(11.2137) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 45.8207(45.9065) | Bit/dim 4.3067(4.4605) | Xent 1.6526(1.7517) | Loss 9.3439(10.1819) | Error 0.5906(0.6226) Steps 0(0.00) | Grad Norm 3.9651(10.9962) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 45.1970(45.8852) | Bit/dim 4.4226(4.4593) | Xent 1.7749(1.7524) | Loss 9.4066(10.1586) | Error 0.6206(0.6225) Steps 0(0.00) | Grad Norm 15.3091(11.1256) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 49.0493(45.9801) | Bit/dim 4.3763(4.4568) | Xent 1.7647(1.7527) | Loss 9.9289(10.1517) | Error 0.6234(0.6225) Steps 0(0.00) | Grad Norm 24.5098(11.5271) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 44.7090(45.9420) | Bit/dim 4.4468(4.4565) | Xent 1.9334(1.7581) | Loss 9.9659(10.1462) | Error 0.6698(0.6239) Steps 0(0.00) | Grad Norm 23.4740(11.8855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 21.4160, Epoch Time 324.1670(305.1216), Bit/dim 4.3884(best: 4.3398), Xent 1.6459, Loss 5.2114, Error 0.5768(best: 0.5773)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 48.1214(46.0073) | Bit/dim 4.3882(4.4545) | Xent 1.6860(1.7560) | Loss 11.7327(10.1938) | Error 0.5981(0.6232) Steps 0(0.00) | Grad Norm 5.8457(11.7044) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 42.4930(45.9019) | Bit/dim 4.3431(4.4511) | Xent 1.7368(1.7554) | Loss 9.4155(10.1704) | Error 0.6206(0.6231) Steps 0(0.00) | Grad Norm 7.0663(11.5652) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 48.6786(45.9852) | Bit/dim 4.3822(4.4491) | Xent 1.7386(1.7549) | Loss 9.6205(10.1539) | Error 0.6206(0.6230) Steps 0(0.00) | Grad Norm 10.9107(11.5456) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 53.9802(46.2251) | Bit/dim 4.3219(4.4453) | Xent 1.7039(1.7534) | Loss 9.8138(10.1437) | Error 0.6040(0.6224) Steps 0(0.00) | Grad Norm 11.7881(11.5529) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 48.7708(46.3014) | Bit/dim 4.3565(4.4426) | Xent 1.6875(1.7514) | Loss 9.7345(10.1314) | Error 0.6004(0.6218) Steps 0(0.00) | Grad Norm 6.7702(11.4094) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 50.9440(46.4407) | Bit/dim 4.3010(4.4383) | Xent 1.6947(1.7497) | Loss 9.5106(10.1128) | Error 0.6008(0.6212) Steps 0(0.00) | Grad Norm 12.7764(11.4504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 21.3818, Epoch Time 330.5920(305.8857), Bit/dim 4.3058(best: 4.3398), Xent 1.6492, Loss 5.1303, Error 0.5792(best: 0.5768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 47.6503(46.4770) | Bit/dim 4.3161(4.4347) | Xent 1.7082(1.7485) | Loss 12.2101(10.1757) | Error 0.5971(0.6204) Steps 0(0.00) | Grad Norm 11.8073(11.4611) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 44.9711(46.4318) | Bit/dim 4.3137(4.4311) | Xent 1.6827(1.7465) | Loss 9.3499(10.1510) | Error 0.6001(0.6198) Steps 0(0.00) | Grad Norm 12.5774(11.4946) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 46.4081(46.4311) | Bit/dim 4.3320(4.4281) | Xent 1.6989(1.7451) | Loss 9.5625(10.1333) | Error 0.6018(0.6193) Steps 0(0.00) | Grad Norm 11.1293(11.4836) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 53.3286(46.6380) | Bit/dim 4.2687(4.4233) | Xent 1.6796(1.7431) | Loss 9.3049(10.1085) | Error 0.5976(0.6186) Steps 0(0.00) | Grad Norm 6.2290(11.3260) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 52.6591(46.8187) | Bit/dim 4.3313(4.4205) | Xent 1.7076(1.7420) | Loss 9.3332(10.0852) | Error 0.6076(0.6183) Steps 0(0.00) | Grad Norm 12.4034(11.3583) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 50.1701(46.9192) | Bit/dim 4.2908(4.4166) | Xent 1.6834(1.7403) | Loss 9.5371(10.0688) | Error 0.6120(0.6181) Steps 0(0.00) | Grad Norm 13.2043(11.4137) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.2231, Epoch Time 333.2743(306.7074), Bit/dim 4.2958(best: 4.3058), Xent 1.6385, Loss 5.1151, Error 0.5816(best: 0.5768)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1/epoch_28_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 60.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
