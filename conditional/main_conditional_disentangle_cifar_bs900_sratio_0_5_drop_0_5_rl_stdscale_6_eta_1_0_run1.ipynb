{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=1.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4300 | Time 18.2650(18.2375) | Bit/dim 3.6926(3.6961) | Xent 0.7249(0.7181) | Loss 20.5068(26.6395) | Error 0.2411(0.2559) Steps 0(0.00) | Grad Norm 7.8357(8.4614) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 21.4811(18.1573) | Bit/dim 3.6994(3.6956) | Xent 0.6829(0.7113) | Loss 19.9351(24.7695) | Error 0.2567(0.2546) Steps 0(0.00) | Grad Norm 7.0709(8.2444) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 18.6328(18.1613) | Bit/dim 3.6794(3.6938) | Xent 0.6901(0.7066) | Loss 20.7031(23.5032) | Error 0.2500(0.2535) Steps 0(0.00) | Grad Norm 6.7368(7.7812) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.1095(18.0568) | Bit/dim 3.6796(3.6933) | Xent 0.6788(0.7022) | Loss 20.0447(22.5711) | Error 0.2367(0.2514) Steps 0(0.00) | Grad Norm 12.3491(7.9418) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 16.8230(17.8541) | Bit/dim 3.7107(3.6928) | Xent 0.7244(0.7028) | Loss 19.4199(21.7829) | Error 0.2622(0.2510) Steps 0(0.00) | Grad Norm 12.2068(8.8006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 96.3194, Epoch Time 1113.2329(1057.1410), Bit/dim 3.6927(best: inf), Xent 0.8199, Loss 4.1027, Error 0.2874(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 16.5085(17.6589) | Bit/dim 3.6791(3.6904) | Xent 0.7126(0.7131) | Loss 19.7096(27.7625) | Error 0.2656(0.2547) Steps 0(0.00) | Grad Norm 8.7837(8.8947) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.1679(17.6482) | Bit/dim 3.7037(3.6900) | Xent 0.6734(0.7065) | Loss 20.2455(25.7416) | Error 0.2356(0.2504) Steps 0(0.00) | Grad Norm 8.1829(8.7618) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 18.0813(17.5387) | Bit/dim 3.6862(3.6924) | Xent 0.6529(0.7059) | Loss 19.9055(24.1259) | Error 0.2322(0.2504) Steps 0(0.00) | Grad Norm 6.6984(8.7949) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 17.5576(17.5428) | Bit/dim 3.7085(3.6929) | Xent 0.6323(0.6953) | Loss 20.4606(22.9658) | Error 0.2278(0.2484) Steps 0(0.00) | Grad Norm 6.5276(8.0758) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 18.2756(17.5372) | Bit/dim 3.6648(3.6918) | Xent 0.6773(0.6954) | Loss 19.8325(22.1047) | Error 0.2422(0.2478) Steps 0(0.00) | Grad Norm 10.1408(8.4652) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 18.3820(17.4878) | Bit/dim 3.7006(3.6938) | Xent 0.7581(0.7029) | Loss 20.5789(21.4806) | Error 0.2856(0.2504) Steps 0(0.00) | Grad Norm 5.4550(8.6798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 92.4769, Epoch Time 1067.8306(1057.4617), Bit/dim 3.6957(best: 3.6927), Xent 0.7686, Loss 4.0800, Error 0.2695(best: 0.2874)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 20.1158(17.4393) | Bit/dim 3.6547(3.6925) | Xent 0.7305(0.6993) | Loss 20.4765(26.9349) | Error 0.2711(0.2491) Steps 0(0.00) | Grad Norm 8.3567(8.2389) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 17.7884(17.4354) | Bit/dim 3.6888(3.6941) | Xent 0.6667(0.6934) | Loss 20.1923(25.0903) | Error 0.2322(0.2473) Steps 0(0.00) | Grad Norm 6.7118(8.0194) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 17.5748(17.4074) | Bit/dim 3.6534(3.6919) | Xent 0.6805(0.6853) | Loss 19.1640(23.6775) | Error 0.2633(0.2456) Steps 0(0.00) | Grad Norm 9.5995(7.7549) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 18.0428(17.6476) | Bit/dim 3.6653(3.6902) | Xent 0.7184(0.6863) | Loss 19.5678(22.6438) | Error 0.2444(0.2451) Steps 0(0.00) | Grad Norm 5.4959(7.8357) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 18.3379(17.7691) | Bit/dim 3.7241(3.6952) | Xent 0.8199(0.7134) | Loss 20.6413(21.9234) | Error 0.2944(0.2552) Steps 0(0.00) | Grad Norm 12.2501(8.9988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 93.3537, Epoch Time 1083.4776(1058.2421), Bit/dim 3.6931(best: 3.6927), Xent 0.7617, Loss 4.0739, Error 0.2692(best: 0.2695)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.0241(17.7148) | Bit/dim 3.7018(3.6943) | Xent 0.7310(0.7133) | Loss 19.8607(28.1608) | Error 0.2567(0.2553) Steps 0(0.00) | Grad Norm 6.1888(8.8376) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.4235(17.7616) | Bit/dim 3.6766(3.6958) | Xent 0.6736(0.7043) | Loss 19.3401(25.9904) | Error 0.2333(0.2515) Steps 0(0.00) | Grad Norm 8.4920(8.6692) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 16.6724(17.6976) | Bit/dim 3.7025(3.6935) | Xent 0.6346(0.6984) | Loss 19.2391(24.3939) | Error 0.2178(0.2492) Steps 0(0.00) | Grad Norm 5.4305(8.5631) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.5636(17.7927) | Bit/dim 3.6886(3.6942) | Xent 0.6588(0.6926) | Loss 19.9982(23.2136) | Error 0.2144(0.2467) Steps 0(0.00) | Grad Norm 7.5598(8.5508) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 17.7997(17.8493) | Bit/dim 3.6757(3.6893) | Xent 0.6827(0.6887) | Loss 19.5780(22.3160) | Error 0.2500(0.2469) Steps 0(0.00) | Grad Norm 7.2102(7.8492) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 18.5784(17.8513) | Bit/dim 3.6711(3.6855) | Xent 0.6781(0.6882) | Loss 20.5486(21.6666) | Error 0.2311(0.2469) Steps 0(0.00) | Grad Norm 10.2015(8.3331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 91.4584, Epoch Time 1090.8334(1059.2199), Bit/dim 3.6863(best: 3.6927), Xent 0.7482, Loss 4.0605, Error 0.2600(best: 0.2692)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 17.4849(17.9514) | Bit/dim 3.6956(3.6830) | Xent 0.6578(0.6820) | Loss 19.5856(26.6114) | Error 0.2278(0.2444) Steps 0(0.00) | Grad Norm 5.0554(8.2829) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.7300(17.8423) | Bit/dim 3.6868(3.6854) | Xent 0.8015(0.6836) | Loss 19.8583(24.7744) | Error 0.2867(0.2447) Steps 0(0.00) | Grad Norm 16.5105(8.4787) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.6703(17.7860) | Bit/dim 3.6801(3.6862) | Xent 0.6875(0.6978) | Loss 19.5269(23.4867) | Error 0.2422(0.2486) Steps 0(0.00) | Grad Norm 9.3023(9.4243) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 18.1749(17.7494) | Bit/dim 3.6765(3.6872) | Xent 0.6935(0.7005) | Loss 19.4183(22.5381) | Error 0.2278(0.2480) Steps 0(0.00) | Grad Norm 4.8972(8.9557) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.2082(17.8336) | Bit/dim 3.6682(3.6873) | Xent 0.7230(0.6987) | Loss 18.1018(21.8419) | Error 0.2600(0.2482) Steps 0(0.00) | Grad Norm 8.7949(8.4816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 93.3365, Epoch Time 1088.8757(1060.1095), Bit/dim 3.6854(best: 3.6863), Xent 0.8015, Loss 4.0861, Error 0.2789(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.7435(17.8500) | Bit/dim 3.6753(3.6851) | Xent 0.6790(0.6985) | Loss 20.1372(27.7302) | Error 0.2522(0.2474) Steps 0(0.00) | Grad Norm 11.9949(8.9036) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 18.1780(17.8311) | Bit/dim 3.6918(3.6862) | Xent 0.6162(0.6878) | Loss 19.9403(25.6440) | Error 0.2089(0.2437) Steps 0(0.00) | Grad Norm 5.2481(8.4693) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.1850(17.7887) | Bit/dim 3.6490(3.6834) | Xent 0.7089(0.6824) | Loss 19.2673(24.1164) | Error 0.2611(0.2424) Steps 0(0.00) | Grad Norm 5.6164(7.9126) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 16.9661(17.9124) | Bit/dim 3.6820(3.6833) | Xent 0.7154(0.6850) | Loss 17.9903(23.0209) | Error 0.2533(0.2435) Steps 0(0.00) | Grad Norm 15.0893(8.2487) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 17.3344(17.9541) | Bit/dim 3.6328(3.6818) | Xent 0.7396(0.6892) | Loss 20.2624(22.2170) | Error 0.2678(0.2460) Steps 0(0.00) | Grad Norm 13.6794(8.8635) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.2477(18.1594) | Bit/dim 3.6981(3.6844) | Xent 0.6703(0.6877) | Loss 19.6584(21.5571) | Error 0.2456(0.2453) Steps 0(0.00) | Grad Norm 10.1196(8.8087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 92.5636, Epoch Time 1108.5227(1061.5619), Bit/dim 3.6835(best: 3.6854), Xent 0.7155, Loss 4.0412, Error 0.2517(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 17.4576(17.9943) | Bit/dim 3.7015(3.6866) | Xent 0.7180(0.6772) | Loss 20.0303(26.8604) | Error 0.2522(0.2423) Steps 0(0.00) | Grad Norm 7.1491(8.2848) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 18.5263(17.9243) | Bit/dim 3.6797(3.6833) | Xent 0.6378(0.6708) | Loss 20.5485(25.0695) | Error 0.2333(0.2405) Steps 0(0.00) | Grad Norm 8.8759(8.4689) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.2999(17.7360) | Bit/dim 3.6946(3.6818) | Xent 0.6197(0.6706) | Loss 19.8810(23.6638) | Error 0.2033(0.2399) Steps 0(0.00) | Grad Norm 4.7454(8.1561) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 16.8671(17.7212) | Bit/dim 3.6514(3.6840) | Xent 0.6815(0.6712) | Loss 18.7802(22.6219) | Error 0.2389(0.2405) Steps 0(0.00) | Grad Norm 8.2217(8.3151) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 16.4389(17.8202) | Bit/dim 3.7157(3.6837) | Xent 0.6430(0.6765) | Loss 19.0398(21.8918) | Error 0.2344(0.2421) Steps 0(0.00) | Grad Norm 6.3185(8.3871) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 92.2171, Epoch Time 1079.3354(1062.0951), Bit/dim 3.6815(best: 3.6835), Xent 0.7444, Loss 4.0537, Error 0.2581(best: 0.2517)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 19.8419(17.8232) | Bit/dim 3.6731(3.6835) | Xent 0.6774(0.6757) | Loss 20.5296(27.5701) | Error 0.2444(0.2406) Steps 0(0.00) | Grad Norm 14.1150(8.5666) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 19.0420(17.7266) | Bit/dim 3.6850(3.6831) | Xent 0.6275(0.6681) | Loss 20.1254(25.4898) | Error 0.2089(0.2367) Steps 0(0.00) | Grad Norm 6.2286(8.3032) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 16.8675(17.6505) | Bit/dim 3.6765(3.6796) | Xent 0.6696(0.6656) | Loss 19.6034(23.9962) | Error 0.2200(0.2371) Steps 0(0.00) | Grad Norm 9.1383(8.3938) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.9801(17.7867) | Bit/dim 3.7371(3.6822) | Xent 0.6474(0.6682) | Loss 20.5769(22.9693) | Error 0.2367(0.2370) Steps 0(0.00) | Grad Norm 5.2086(8.2565) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 18.2510(17.6991) | Bit/dim 3.7041(3.6807) | Xent 0.6138(0.6718) | Loss 20.5272(22.1098) | Error 0.2078(0.2384) Steps 0(0.00) | Grad Norm 10.2705(8.7417) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.4125(17.8172) | Bit/dim 3.7415(3.6801) | Xent 0.6786(0.6709) | Loss 20.1642(21.5321) | Error 0.2544(0.2390) Steps 0(0.00) | Grad Norm 8.9933(8.3147) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 91.6135, Epoch Time 1086.9875(1062.8419), Bit/dim 3.6779(best: 3.6815), Xent 0.7042, Loss 4.0300, Error 0.2442(best: 0.2517)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 17.1566(17.7772) | Bit/dim 3.6931(3.6799) | Xent 0.6583(0.6678) | Loss 19.0998(26.8127) | Error 0.2367(0.2380) Steps 0(0.00) | Grad Norm 9.4279(8.4132) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 17.8641(17.8414) | Bit/dim 3.6674(3.6772) | Xent 0.5985(0.6646) | Loss 19.8641(25.0264) | Error 0.2100(0.2364) Steps 0(0.00) | Grad Norm 8.8928(8.1595) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 20.2875(17.9010) | Bit/dim 3.6638(3.6773) | Xent 0.6320(0.6614) | Loss 19.8387(23.6540) | Error 0.2411(0.2353) Steps 0(0.00) | Grad Norm 7.1582(8.3481) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 19.7434(17.8310) | Bit/dim 3.6600(3.6768) | Xent 0.6487(0.6633) | Loss 20.1101(22.6246) | Error 0.2311(0.2358) Steps 0(0.00) | Grad Norm 8.6830(8.4898) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 17.6634(17.8044) | Bit/dim 3.7046(3.6760) | Xent 0.6440(0.6576) | Loss 20.7443(21.9323) | Error 0.2244(0.2342) Steps 0(0.00) | Grad Norm 5.1687(7.9795) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 91.0900, Epoch Time 1091.0569(1063.6884), Bit/dim 3.6742(best: 3.6779), Xent 0.7056, Loss 4.0269, Error 0.2484(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 16.8685(17.8166) | Bit/dim 3.6723(3.6759) | Xent 0.6538(0.6525) | Loss 18.9929(27.5164) | Error 0.2333(0.2319) Steps 0(0.00) | Grad Norm 10.0345(7.7953) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 18.4312(17.7844) | Bit/dim 3.6980(3.6757) | Xent 0.6097(0.6562) | Loss 19.7126(25.4737) | Error 0.2044(0.2317) Steps 0(0.00) | Grad Norm 6.7759(8.2535) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.4469(17.8314) | Bit/dim 3.6579(3.6751) | Xent 0.6260(0.6575) | Loss 20.5680(24.0237) | Error 0.2144(0.2327) Steps 0(0.00) | Grad Norm 5.5674(8.1341) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 15.9699(17.8079) | Bit/dim 3.6768(3.6733) | Xent 0.7212(0.6656) | Loss 18.1335(22.8975) | Error 0.2544(0.2364) Steps 0(0.00) | Grad Norm 11.2207(8.2435) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 17.8161(17.9992) | Bit/dim 3.6885(3.6776) | Xent 0.7137(0.6728) | Loss 19.6129(22.1026) | Error 0.2511(0.2401) Steps 0(0.00) | Grad Norm 12.2915(9.2365) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 18.8742(17.9973) | Bit/dim 3.6669(3.6809) | Xent 0.6460(0.6809) | Loss 20.1499(21.5386) | Error 0.2156(0.2427) Steps 0(0.00) | Grad Norm 7.4420(9.5752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 92.1196, Epoch Time 1098.0103(1064.7180), Bit/dim 3.6929(best: 3.6742), Xent 0.7431, Loss 4.0645, Error 0.2607(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 17.1212(17.8750) | Bit/dim 3.6867(3.6806) | Xent 0.6457(0.6687) | Loss 19.6240(26.9729) | Error 0.2389(0.2389) Steps 0(0.00) | Grad Norm 4.4050(9.1329) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 15.6032(17.7712) | Bit/dim 3.6782(3.6809) | Xent 0.6324(0.6575) | Loss 18.9979(25.0147) | Error 0.2289(0.2344) Steps 0(0.00) | Grad Norm 5.8785(8.5419) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 18.8749(17.7870) | Bit/dim 3.7145(3.6798) | Xent 0.6531(0.6534) | Loss 19.8945(23.7261) | Error 0.2233(0.2327) Steps 0(0.00) | Grad Norm 4.3669(8.1809) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 17.9296(17.8531) | Bit/dim 3.6906(3.6788) | Xent 0.5852(0.6476) | Loss 18.4377(22.6804) | Error 0.2244(0.2317) Steps 0(0.00) | Grad Norm 5.9724(8.1307) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.9208(17.7509) | Bit/dim 3.6846(3.6767) | Xent 0.6599(0.6478) | Loss 19.4601(21.8860) | Error 0.2367(0.2314) Steps 0(0.00) | Grad Norm 7.4080(8.1376) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 93.7041, Epoch Time 1081.4017(1065.2185), Bit/dim 3.6768(best: 3.6742), Xent 0.7617, Loss 4.0577, Error 0.2717(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.1460(17.6499) | Bit/dim 3.7170(3.6775) | Xent 0.6967(0.6601) | Loss 19.1465(28.2526) | Error 0.2478(0.2349) Steps 0(0.00) | Grad Norm 8.7808(8.6889) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.1190(17.6950) | Bit/dim 3.6466(3.6773) | Xent 0.6779(0.6519) | Loss 20.5135(26.0192) | Error 0.2356(0.2320) Steps 0(0.00) | Grad Norm 8.7813(8.2393) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 16.8935(17.8256) | Bit/dim 3.6844(3.6787) | Xent 0.5865(0.6444) | Loss 18.9978(24.3536) | Error 0.2078(0.2288) Steps 0(0.00) | Grad Norm 4.6315(8.0840) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 19.8901(17.7780) | Bit/dim 3.6578(3.6753) | Xent 0.6402(0.6470) | Loss 20.4355(23.1998) | Error 0.2467(0.2302) Steps 0(0.00) | Grad Norm 6.7418(7.7979) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 17.6648(17.6887) | Bit/dim 3.6733(3.6750) | Xent 0.7010(0.6416) | Loss 20.2201(22.3096) | Error 0.2511(0.2286) Steps 0(0.00) | Grad Norm 8.8732(7.7102) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.4579(17.6536) | Bit/dim 3.6642(3.6746) | Xent 0.6662(0.6460) | Loss 19.6439(21.6171) | Error 0.2289(0.2299) Steps 0(0.00) | Grad Norm 6.4488(8.0653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 92.9633, Epoch Time 1084.6102(1065.8003), Bit/dim 3.6743(best: 3.6742), Xent 0.7320, Loss 4.0403, Error 0.2561(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.9049(17.6811) | Bit/dim 3.6745(3.6755) | Xent 0.6511(0.6467) | Loss 19.8318(26.6118) | Error 0.2356(0.2294) Steps 0(0.00) | Grad Norm 11.0040(8.4033) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.3149(17.8408) | Bit/dim 3.6384(3.6720) | Xent 0.6379(0.6411) | Loss 19.4989(24.7897) | Error 0.2400(0.2284) Steps 0(0.00) | Grad Norm 12.1253(8.2997) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.1357(17.7730) | Bit/dim 3.6941(3.6744) | Xent 0.6506(0.6474) | Loss 19.0750(23.4781) | Error 0.2111(0.2296) Steps 0(0.00) | Grad Norm 7.4330(8.6963) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 19.6404(17.6964) | Bit/dim 3.7093(3.6737) | Xent 0.6481(0.6505) | Loss 20.7998(22.4819) | Error 0.2311(0.2297) Steps 0(0.00) | Grad Norm 10.4705(8.9632) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.5490(17.7390) | Bit/dim 3.6684(3.6741) | Xent 0.6161(0.6519) | Loss 20.4583(21.8582) | Error 0.2233(0.2317) Steps 0(0.00) | Grad Norm 6.4623(8.7296) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 93.2935, Epoch Time 1087.0668(1066.4383), Bit/dim 3.6790(best: 3.6742), Xent 0.7036, Loss 4.0308, Error 0.2476(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 18.1980(17.6482) | Bit/dim 3.6565(3.6745) | Xent 0.6143(0.6510) | Loss 20.3752(27.9581) | Error 0.2100(0.2324) Steps 0(0.00) | Grad Norm 6.4734(8.6623) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.1777(17.5110) | Bit/dim 3.6401(3.6745) | Xent 0.6310(0.6424) | Loss 18.5393(25.7331) | Error 0.2367(0.2300) Steps 0(0.00) | Grad Norm 17.3391(8.5596) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 16.0029(17.6474) | Bit/dim 3.6447(3.6736) | Xent 0.6354(0.6403) | Loss 19.7702(24.2076) | Error 0.2311(0.2296) Steps 0(0.00) | Grad Norm 10.3882(8.5663) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 18.2156(17.7684) | Bit/dim 3.6807(3.6726) | Xent 0.6536(0.6435) | Loss 19.5121(23.0062) | Error 0.2367(0.2301) Steps 0(0.00) | Grad Norm 8.1081(9.0819) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 18.7424(17.8110) | Bit/dim 3.6774(3.6731) | Xent 0.5931(0.6387) | Loss 20.9168(22.1812) | Error 0.2044(0.2283) Steps 0(0.00) | Grad Norm 7.1863(8.6573) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.9133(17.8777) | Bit/dim 3.6594(3.6718) | Xent 0.6582(0.6418) | Loss 20.5956(21.5941) | Error 0.2322(0.2291) Steps 0(0.00) | Grad Norm 9.1896(8.8987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 93.6016, Epoch Time 1093.2772(1067.2434), Bit/dim 3.6707(best: 3.6742), Xent 0.7056, Loss 4.0235, Error 0.2468(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.0030(17.8844) | Bit/dim 3.6724(3.6718) | Xent 0.6323(0.6371) | Loss 19.6442(26.9409) | Error 0.2356(0.2289) Steps 0(0.00) | Grad Norm 7.6002(8.6365) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.6265(17.8733) | Bit/dim 3.6844(3.6734) | Xent 0.6056(0.6318) | Loss 20.5218(25.1429) | Error 0.2133(0.2257) Steps 0(0.00) | Grad Norm 14.0926(8.6896) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 21.1011(17.9197) | Bit/dim 3.6670(3.6714) | Xent 0.6538(0.6289) | Loss 19.8823(23.7549) | Error 0.2411(0.2243) Steps 0(0.00) | Grad Norm 9.0050(8.3052) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 17.9482(17.9290) | Bit/dim 3.6327(3.6674) | Xent 0.6556(0.6263) | Loss 19.6199(22.7549) | Error 0.2389(0.2224) Steps 0(0.00) | Grad Norm 6.7972(8.0577) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 18.6793(17.8466) | Bit/dim 3.6518(3.6660) | Xent 0.5676(0.6269) | Loss 19.3581(21.8936) | Error 0.1956(0.2239) Steps 0(0.00) | Grad Norm 5.3378(7.8246) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 93.2734, Epoch Time 1092.1459(1067.9905), Bit/dim 3.6676(best: 3.6707), Xent 0.7221, Loss 4.0286, Error 0.2486(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.6835(17.8557) | Bit/dim 3.6701(3.6668) | Xent 0.6424(0.6236) | Loss 19.2619(28.1856) | Error 0.2289(0.2218) Steps 0(0.00) | Grad Norm 8.4471(7.4656) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.8346(17.7890) | Bit/dim 3.6688(3.6663) | Xent 0.5748(0.6165) | Loss 19.8525(25.9647) | Error 0.1989(0.2197) Steps 0(0.00) | Grad Norm 7.9062(7.6132) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 17.9532(17.6406) | Bit/dim 3.6719(3.6681) | Xent 0.7265(0.6184) | Loss 19.9740(24.3510) | Error 0.2556(0.2208) Steps 0(0.00) | Grad Norm 17.5984(8.0694) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 16.0724(17.4964) | Bit/dim 3.6798(3.6718) | Xent 0.6192(0.6244) | Loss 19.1002(23.0900) | Error 0.2078(0.2229) Steps 0(0.00) | Grad Norm 7.4288(8.7079) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 17.2038(17.6120) | Bit/dim 3.6998(3.6727) | Xent 0.6659(0.6271) | Loss 20.0136(22.3496) | Error 0.2411(0.2245) Steps 0(0.00) | Grad Norm 10.3365(8.7798) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 20.4658(17.9238) | Bit/dim 3.6448(3.6710) | Xent 0.6248(0.6337) | Loss 20.2423(21.7620) | Error 0.2200(0.2261) Steps 0(0.00) | Grad Norm 7.0668(8.4683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 94.6903, Epoch Time 1089.4000(1068.6328), Bit/dim 3.6762(best: 3.6676), Xent 0.7220, Loss 4.0372, Error 0.2473(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.2635(17.8595) | Bit/dim 3.6839(3.6741) | Xent 0.5544(0.6346) | Loss 19.4043(27.0710) | Error 0.2033(0.2255) Steps 0(0.00) | Grad Norm 7.0634(8.4183) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 18.0672(18.0673) | Bit/dim 3.6549(3.6711) | Xent 0.5533(0.6259) | Loss 19.5093(25.1888) | Error 0.1967(0.2219) Steps 0(0.00) | Grad Norm 5.5992(8.0359) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 19.8835(18.0801) | Bit/dim 3.6808(3.6704) | Xent 0.6193(0.6196) | Loss 21.0162(23.9097) | Error 0.2356(0.2199) Steps 0(0.00) | Grad Norm 11.8697(7.8971) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 17.3543(17.9938) | Bit/dim 3.6497(3.6679) | Xent 0.6823(0.6169) | Loss 19.1309(22.7426) | Error 0.2556(0.2190) Steps 0(0.00) | Grad Norm 7.8066(7.6719) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 18.0352(17.9101) | Bit/dim 3.6538(3.6676) | Xent 0.7056(0.6188) | Loss 20.2836(22.0195) | Error 0.2556(0.2196) Steps 0(0.00) | Grad Norm 9.7231(7.7476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 92.0978, Epoch Time 1097.2464(1069.4912), Bit/dim 3.6666(best: 3.6676), Xent 0.7151, Loss 4.0241, Error 0.2482(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 19.7714(18.0066) | Bit/dim 3.6920(3.6653) | Xent 0.6189(0.6198) | Loss 20.0326(28.0183) | Error 0.2267(0.2208) Steps 0(0.00) | Grad Norm 7.8130(7.7381) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 19.4380(17.8749) | Bit/dim 3.6503(3.6648) | Xent 0.6137(0.6152) | Loss 21.0267(25.8390) | Error 0.2089(0.2182) Steps 0(0.00) | Grad Norm 8.2421(7.8065) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 16.7917(17.9222) | Bit/dim 3.6997(3.6652) | Xent 0.6851(0.6156) | Loss 18.9957(24.1528) | Error 0.2467(0.2183) Steps 0(0.00) | Grad Norm 12.2111(7.8461) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 16.6250(17.9344) | Bit/dim 3.6726(3.6670) | Xent 0.6159(0.6234) | Loss 19.7051(23.0702) | Error 0.2156(0.2199) Steps 0(0.00) | Grad Norm 10.1062(8.3962) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 16.4409(17.7694) | Bit/dim 3.6824(3.6688) | Xent 0.6359(0.6203) | Loss 19.6759(22.2240) | Error 0.2244(0.2199) Steps 0(0.00) | Grad Norm 9.0249(8.0978) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 18.6208(17.7735) | Bit/dim 3.6520(3.6693) | Xent 0.6944(0.6224) | Loss 19.8730(21.6281) | Error 0.2433(0.2207) Steps 0(0.00) | Grad Norm 10.9463(8.5962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 92.6666, Epoch Time 1090.8051(1070.1306), Bit/dim 3.6707(best: 3.6666), Xent 0.7896, Loss 4.0655, Error 0.2743(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 18.9412(17.7722) | Bit/dim 3.6757(3.6694) | Xent 0.6830(0.6308) | Loss 21.0061(26.7446) | Error 0.2467(0.2253) Steps 0(0.00) | Grad Norm 7.9511(9.0013) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 17.6440(17.8840) | Bit/dim 3.7040(3.6709) | Xent 0.5833(0.6263) | Loss 19.3924(24.8838) | Error 0.2167(0.2239) Steps 0(0.00) | Grad Norm 4.4164(8.8606) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 20.2625(17.9653) | Bit/dim 3.6593(3.6713) | Xent 0.6412(0.6204) | Loss 19.8145(23.5586) | Error 0.2222(0.2210) Steps 0(0.00) | Grad Norm 8.2633(8.3805) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 17.7990(17.8838) | Bit/dim 3.6451(3.6687) | Xent 0.5563(0.6196) | Loss 19.8742(22.6279) | Error 0.2156(0.2205) Steps 0(0.00) | Grad Norm 4.8327(8.2036) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 17.4114(17.7709) | Bit/dim 3.6808(3.6671) | Xent 0.6327(0.6145) | Loss 19.2251(21.7913) | Error 0.2244(0.2184) Steps 0(0.00) | Grad Norm 7.9130(7.8819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 92.8899, Epoch Time 1093.2269(1070.8235), Bit/dim 3.6681(best: 3.6666), Xent 0.7197, Loss 4.0279, Error 0.2493(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 20.8202(18.0200) | Bit/dim 3.6337(3.6660) | Xent 0.6203(0.6118) | Loss 20.1022(27.9310) | Error 0.2167(0.2173) Steps 0(0.00) | Grad Norm 6.9993(7.8705) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 19.2051(18.0227) | Bit/dim 3.6335(3.6657) | Xent 0.5646(0.6087) | Loss 21.2144(25.8990) | Error 0.2178(0.2172) Steps 0(0.00) | Grad Norm 5.7915(7.7594) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.9129(18.0613) | Bit/dim 3.6445(3.6625) | Xent 0.5867(0.6045) | Loss 19.1359(24.2832) | Error 0.2122(0.2147) Steps 0(0.00) | Grad Norm 6.7262(7.6070) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 21.6372(18.1795) | Bit/dim 3.6495(3.6625) | Xent 0.7111(0.6025) | Loss 21.3334(23.1539) | Error 0.2489(0.2140) Steps 0(0.00) | Grad Norm 10.0709(7.3633) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 19.3898(18.2143) | Bit/dim 3.6503(3.6645) | Xent 0.5930(0.6016) | Loss 20.2133(22.2236) | Error 0.2144(0.2135) Steps 0(0.00) | Grad Norm 6.3901(7.3788) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 17.7007(18.2065) | Bit/dim 3.6662(3.6654) | Xent 0.6125(0.6029) | Loss 19.6038(21.5643) | Error 0.2267(0.2136) Steps 0(0.00) | Grad Norm 8.5005(7.8085) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 93.0047, Epoch Time 1118.9461(1072.2672), Bit/dim 3.6689(best: 3.6666), Xent 0.7182, Loss 4.0280, Error 0.2443(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 19.6808(18.2803) | Bit/dim 3.6889(3.6659) | Xent 0.6328(0.6023) | Loss 20.6152(26.7782) | Error 0.2322(0.2149) Steps 0(0.00) | Grad Norm 6.7337(8.0596) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 20.1785(18.1611) | Bit/dim 3.6835(3.6637) | Xent 0.5821(0.6047) | Loss 20.6801(24.9566) | Error 0.1989(0.2147) Steps 0(0.00) | Grad Norm 8.8423(8.4470) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 20.8519(18.1186) | Bit/dim 3.6581(3.6655) | Xent 0.6097(0.6003) | Loss 19.8792(23.5607) | Error 0.2167(0.2127) Steps 0(0.00) | Grad Norm 4.8794(7.9638) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 18.4717(18.1063) | Bit/dim 3.6711(3.6669) | Xent 0.6362(0.6032) | Loss 19.5104(22.6114) | Error 0.2167(0.2142) Steps 0(0.00) | Grad Norm 9.5039(8.0207) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 16.9610(18.0107) | Bit/dim 3.6332(3.6626) | Xent 0.5739(0.6024) | Loss 19.7915(21.7810) | Error 0.2089(0.2129) Steps 0(0.00) | Grad Norm 6.4552(7.6094) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 93.6743, Epoch Time 1106.1658(1073.2842), Bit/dim 3.6638(best: 3.6666), Xent 0.6776, Loss 4.0026, Error 0.2367(best: 0.2442)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 19.4531(18.2707) | Bit/dim 3.6334(3.6624) | Xent 0.6136(0.6016) | Loss 19.8935(28.1427) | Error 0.2167(0.2137) Steps 0(0.00) | Grad Norm 10.7375(7.8243) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 19.0060(18.2114) | Bit/dim 3.6364(3.6634) | Xent 0.5111(0.5928) | Loss 19.6179(25.9580) | Error 0.1922(0.2107) Steps 0(0.00) | Grad Norm 7.6667(7.4647) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 19.0388(18.3519) | Bit/dim 3.6542(3.6650) | Xent 0.5510(0.5890) | Loss 18.5212(24.3026) | Error 0.1911(0.2089) Steps 0(0.00) | Grad Norm 6.0985(7.3410) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 17.7453(18.2797) | Bit/dim 3.6690(3.6606) | Xent 0.6031(0.5873) | Loss 19.5319(23.0902) | Error 0.2256(0.2092) Steps 0(0.00) | Grad Norm 9.0285(7.5743) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 17.0487(18.1676) | Bit/dim 3.6954(3.6585) | Xent 0.6297(0.6002) | Loss 20.1480(22.2851) | Error 0.2289(0.2142) Steps 0(0.00) | Grad Norm 8.2392(8.2946) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 18.3152(18.1075) | Bit/dim 3.6427(3.6578) | Xent 0.5670(0.6006) | Loss 19.8937(21.6183) | Error 0.2000(0.2136) Steps 0(0.00) | Grad Norm 8.3919(8.3176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 94.1568, Epoch Time 1115.0977(1074.5386), Bit/dim 3.6643(best: 3.6638), Xent 0.7164, Loss 4.0225, Error 0.2445(best: 0.2367)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.7772(18.1916) | Bit/dim 3.6445(3.6556) | Xent 0.6261(0.5888) | Loss 20.1751(26.9694) | Error 0.2189(0.2099) Steps 0(0.00) | Grad Norm 7.9863(8.0838) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 18.2240(18.1066) | Bit/dim 3.6560(3.6570) | Xent 0.5711(0.5973) | Loss 18.6599(24.9983) | Error 0.2000(0.2125) Steps 0(0.00) | Grad Norm 6.7958(8.2035) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 17.9779(18.0863) | Bit/dim 3.6709(3.6560) | Xent 0.5609(0.5958) | Loss 19.5126(23.5659) | Error 0.1944(0.2121) Steps 0(0.00) | Grad Norm 5.5542(8.2334) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 20.0531(18.1962) | Bit/dim 3.6619(3.6596) | Xent 0.5590(0.5963) | Loss 20.4304(22.6698) | Error 0.1956(0.2125) Steps 0(0.00) | Grad Norm 10.9513(8.3999) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 16.6315(18.1433) | Bit/dim 3.6744(3.6594) | Xent 0.5164(0.5976) | Loss 19.4655(21.9111) | Error 0.1878(0.2117) Steps 0(0.00) | Grad Norm 5.0304(8.4332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 92.9083, Epoch Time 1113.4724(1075.7066), Bit/dim 3.6666(best: 3.6638), Xent 0.6841, Loss 4.0087, Error 0.2371(best: 0.2367)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 18.6643(18.1671) | Bit/dim 3.6455(3.6608) | Xent 0.5982(0.5900) | Loss 20.0144(28.0183) | Error 0.2167(0.2094) Steps 0(0.00) | Grad Norm 7.8073(7.9111) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 16.8024(18.0324) | Bit/dim 3.6714(3.6599) | Xent 0.6151(0.5916) | Loss 18.5048(25.8097) | Error 0.2067(0.2101) Steps 0(0.00) | Grad Norm 7.6831(8.0493) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 20.7094(18.0198) | Bit/dim 3.6579(3.6605) | Xent 0.5734(0.5814) | Loss 20.0670(24.2487) | Error 0.2100(0.2065) Steps 0(0.00) | Grad Norm 8.5758(7.6864) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.2057(17.8606) | Bit/dim 3.6699(3.6589) | Xent 0.5561(0.5807) | Loss 19.0929(23.0056) | Error 0.2089(0.2071) Steps 0(0.00) | Grad Norm 7.8118(7.6601) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 18.3027(17.8127) | Bit/dim 3.6846(3.6609) | Xent 0.6355(0.5824) | Loss 20.7446(22.2011) | Error 0.2200(0.2076) Steps 0(0.00) | Grad Norm 11.6563(7.8555) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 17.2101(17.8669) | Bit/dim 3.6491(3.6583) | Xent 0.6091(0.5837) | Loss 19.5219(21.5141) | Error 0.2178(0.2079) Steps 0(0.00) | Grad Norm 6.0954(7.7108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 92.6654, Epoch Time 1094.7593(1076.2782), Bit/dim 3.6574(best: 3.6638), Xent 0.6920, Loss 4.0034, Error 0.2353(best: 0.2367)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 17.7400(17.8877) | Bit/dim 3.6694(3.6597) | Xent 0.5540(0.5800) | Loss 20.1514(26.8246) | Error 0.2044(0.2065) Steps 0(0.00) | Grad Norm 9.7299(7.9015) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 16.4212(17.8942) | Bit/dim 3.6442(3.6578) | Xent 0.5595(0.5770) | Loss 18.6165(24.9411) | Error 0.2122(0.2066) Steps 0(0.00) | Grad Norm 7.2457(7.7821) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 17.5049(17.7877) | Bit/dim 3.6826(3.6581) | Xent 0.6018(0.5765) | Loss 19.5225(23.6027) | Error 0.1989(0.2066) Steps 0(0.00) | Grad Norm 5.4134(7.3039) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 18.2936(18.0495) | Bit/dim 3.6862(3.6595) | Xent 0.6658(0.5764) | Loss 20.1160(22.6788) | Error 0.2233(0.2052) Steps 0(0.00) | Grad Norm 5.7930(7.2743) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 16.8037(18.0028) | Bit/dim 3.6420(3.6566) | Xent 0.5446(0.5757) | Loss 19.7179(21.9581) | Error 0.1989(0.2056) Steps 0(0.00) | Grad Norm 8.8472(7.3030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 93.6930, Epoch Time 1100.9797(1077.0192), Bit/dim 3.6568(best: 3.6574), Xent 0.6786, Loss 3.9961, Error 0.2322(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 18.4087(17.9046) | Bit/dim 3.6386(3.6545) | Xent 0.5503(0.5738) | Loss 20.3181(27.8162) | Error 0.1967(0.2053) Steps 0(0.00) | Grad Norm 7.7680(7.3328) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 18.2207(18.0242) | Bit/dim 3.6599(3.6515) | Xent 0.5724(0.5718) | Loss 20.6987(25.7395) | Error 0.2056(0.2043) Steps 0(0.00) | Grad Norm 10.7486(7.3437) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 16.5046(17.9939) | Bit/dim 3.6757(3.6528) | Xent 0.5612(0.5777) | Loss 19.7812(24.1523) | Error 0.2111(0.2067) Steps 0(0.00) | Grad Norm 11.7644(7.8693) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 18.9582(17.9647) | Bit/dim 3.6642(3.6564) | Xent 0.5200(0.5813) | Loss 20.4205(23.0697) | Error 0.1778(0.2079) Steps 0(0.00) | Grad Norm 4.8462(8.1213) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 17.8102(18.1206) | Bit/dim 3.6492(3.6583) | Xent 0.5699(0.5795) | Loss 20.1151(22.2836) | Error 0.1944(0.2052) Steps 0(0.00) | Grad Norm 7.4284(8.2282) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 20.5836(18.1965) | Bit/dim 3.6326(3.6612) | Xent 0.5864(0.5790) | Loss 20.6615(21.7296) | Error 0.2089(0.2049) Steps 0(0.00) | Grad Norm 9.5450(8.0931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 93.7594, Epoch Time 1110.1236(1078.0123), Bit/dim 3.6615(best: 3.6568), Xent 0.7098, Loss 4.0164, Error 0.2466(best: 0.2322)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 17.8876(18.1495) | Bit/dim 3.6631(3.6602) | Xent 0.5077(0.5839) | Loss 19.5725(26.8974) | Error 0.1944(0.2075) Steps 0(0.00) | Grad Norm 5.6652(8.5238) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 17.6388(18.1127) | Bit/dim 3.6513(3.6606) | Xent 0.5558(0.5720) | Loss 19.5135(25.0159) | Error 0.1856(0.2028) Steps 0(0.00) | Grad Norm 6.1633(7.9485) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.1986(18.2421) | Bit/dim 3.6029(3.6570) | Xent 0.6127(0.5745) | Loss 18.9726(23.6851) | Error 0.2156(0.2037) Steps 0(0.00) | Grad Norm 8.9653(7.7700) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 18.0288(18.3486) | Bit/dim 3.6510(3.6580) | Xent 0.6113(0.5753) | Loss 20.7331(22.7500) | Error 0.2044(0.2039) Steps 0(0.00) | Grad Norm 8.7990(8.0700) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 17.7504(18.2600) | Bit/dim 3.6532(3.6592) | Xent 0.5781(0.5831) | Loss 18.4169(21.9717) | Error 0.1933(0.2058) Steps 0(0.00) | Grad Norm 6.5324(8.4333) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 94.4166, Epoch Time 1116.0162(1079.1524), Bit/dim 3.6575(best: 3.6568), Xent 0.6800, Loss 3.9976, Error 0.2330(best: 0.2322)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 17.7957(18.1372) | Bit/dim 3.6322(3.6580) | Xent 0.5614(0.5815) | Loss 20.0053(28.2339) | Error 0.1878(0.2050) Steps 0(0.00) | Grad Norm 8.9114(8.4422) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 19.7323(18.1481) | Bit/dim 3.6495(3.6585) | Xent 0.5583(0.5768) | Loss 19.5383(25.9875) | Error 0.1944(0.2038) Steps 0(0.00) | Grad Norm 5.6275(8.2626) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 18.4765(18.3530) | Bit/dim 3.6393(3.6554) | Xent 0.5877(0.5745) | Loss 20.0996(24.4759) | Error 0.2000(0.2029) Steps 0(0.00) | Grad Norm 6.4461(8.2604) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.8954(18.6064) | Bit/dim 3.6701(3.6548) | Xent 0.5656(0.5693) | Loss 19.9604(23.3308) | Error 0.1978(0.2011) Steps 0(0.00) | Grad Norm 8.7534(8.0951) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 20.9018(18.5750) | Bit/dim 3.6772(3.6527) | Xent 0.5232(0.5629) | Loss 20.9889(22.4433) | Error 0.1922(0.1995) Steps 0(0.00) | Grad Norm 6.1001(7.6151) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 16.7304(18.4389) | Bit/dim 3.6684(3.6533) | Xent 0.5535(0.5612) | Loss 19.2235(21.6698) | Error 0.1911(0.1995) Steps 0(0.00) | Grad Norm 6.3059(7.2975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 93.9717, Epoch Time 1127.9050(1080.6150), Bit/dim 3.6496(best: 3.6568), Xent 0.6766, Loss 3.9879, Error 0.2299(best: 0.2322)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 21.1746(18.4336) | Bit/dim 3.6617(3.6534) | Xent 0.5750(0.5612) | Loss 21.2769(27.2110) | Error 0.1911(0.2000) Steps 0(0.00) | Grad Norm 12.0170(7.7613) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 18.1616(18.2827) | Bit/dim 3.6141(3.6531) | Xent 0.5641(0.5662) | Loss 19.3255(25.1919) | Error 0.2089(0.2022) Steps 0(0.00) | Grad Norm 6.4423(7.9380) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 21.9433(18.4022) | Bit/dim 3.6430(3.6531) | Xent 0.5157(0.5626) | Loss 19.9195(23.8761) | Error 0.1967(0.2010) Steps 0(0.00) | Grad Norm 8.8274(7.5467) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 17.1835(18.3370) | Bit/dim 3.6334(3.6509) | Xent 0.5964(0.5688) | Loss 19.9911(22.8755) | Error 0.2100(0.2032) Steps 0(0.00) | Grad Norm 9.4417(7.5689) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 20.8158(18.3608) | Bit/dim 3.6815(3.6512) | Xent 0.5630(0.5642) | Loss 20.0635(22.0799) | Error 0.2078(0.2010) Steps 0(0.00) | Grad Norm 8.8182(7.3614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 93.2695, Epoch Time 1115.8058(1081.6708), Bit/dim 3.6481(best: 3.6496), Xent 0.6701, Loss 3.9831, Error 0.2276(best: 0.2299)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 15.7950(18.1711) | Bit/dim 3.6689(3.6530) | Xent 0.5824(0.5619) | Loss 19.0406(28.3197) | Error 0.2300(0.2002) Steps 0(0.00) | Grad Norm 11.6704(7.5332) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 17.0720(18.1195) | Bit/dim 3.6408(3.6505) | Xent 0.5336(0.5581) | Loss 18.8275(26.0438) | Error 0.1744(0.1991) Steps 0(0.00) | Grad Norm 6.8279(7.7671) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 18.9718(18.3024) | Bit/dim 3.6524(3.6522) | Xent 0.5610(0.5607) | Loss 20.6247(24.4969) | Error 0.1967(0.1996) Steps 0(0.00) | Grad Norm 10.2128(8.3116) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 17.6542(18.2238) | Bit/dim 3.6322(3.6514) | Xent 0.5548(0.5673) | Loss 19.2147(23.2742) | Error 0.1900(0.2021) Steps 0(0.00) | Grad Norm 7.0341(8.2683) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 16.5511(18.1753) | Bit/dim 3.6796(3.6535) | Xent 0.5719(0.5698) | Loss 18.5652(22.3003) | Error 0.1956(0.2028) Steps 0(0.00) | Grad Norm 8.5978(8.3620) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.4571(18.1963) | Bit/dim 3.6394(3.6520) | Xent 0.5382(0.5715) | Loss 20.0285(21.7199) | Error 0.1989(0.2036) Steps 0(0.00) | Grad Norm 7.0437(8.1933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 95.2035, Epoch Time 1112.1047(1082.5838), Bit/dim 3.6555(best: 3.6481), Xent 0.6982, Loss 4.0046, Error 0.2430(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 18.8384(18.1989) | Bit/dim 3.6287(3.6510) | Xent 0.5204(0.5645) | Loss 19.9012(27.0503) | Error 0.1900(0.2007) Steps 0(0.00) | Grad Norm 4.5965(7.5509) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 17.7264(18.0700) | Bit/dim 3.6274(3.6512) | Xent 0.5485(0.5570) | Loss 19.8384(25.0681) | Error 0.1989(0.1986) Steps 0(0.00) | Grad Norm 7.5608(7.1739) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 18.0637(18.1875) | Bit/dim 3.6515(3.6514) | Xent 0.5305(0.5540) | Loss 19.2610(23.7442) | Error 0.1900(0.1965) Steps 0(0.00) | Grad Norm 6.6131(7.3088) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 16.7144(18.1875) | Bit/dim 3.7014(3.6555) | Xent 0.4751(0.5527) | Loss 19.5433(22.7476) | Error 0.1622(0.1960) Steps 0(0.00) | Grad Norm 9.0125(7.7403) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 18.6605(18.1097) | Bit/dim 3.6354(3.6527) | Xent 0.5440(0.5525) | Loss 19.5962(21.9118) | Error 0.2000(0.1960) Steps 0(0.00) | Grad Norm 6.9418(7.4767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 95.4405, Epoch Time 1111.0142(1083.4367), Bit/dim 3.6525(best: 3.6481), Xent 0.6946, Loss 3.9998, Error 0.2353(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 19.1437(18.0914) | Bit/dim 3.6439(3.6488) | Xent 0.5151(0.5529) | Loss 20.3104(28.1374) | Error 0.1811(0.1957) Steps 0(0.00) | Grad Norm 4.5655(7.3723) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 17.2376(18.1136) | Bit/dim 3.6673(3.6495) | Xent 0.4593(0.5382) | Loss 20.3253(25.9734) | Error 0.1578(0.1907) Steps 0(0.00) | Grad Norm 3.6582(7.0227) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 19.0289(18.1458) | Bit/dim 3.6618(3.6497) | Xent 0.5428(0.5379) | Loss 20.8787(24.3767) | Error 0.1922(0.1909) Steps 0(0.00) | Grad Norm 8.4217(6.9270) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 18.8761(18.2377) | Bit/dim 3.6825(3.6487) | Xent 0.5146(0.5432) | Loss 19.5084(23.2243) | Error 0.1833(0.1921) Steps 0(0.00) | Grad Norm 6.0136(7.0708) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 18.5957(18.4743) | Bit/dim 3.6872(3.6484) | Xent 0.5200(0.5486) | Loss 19.6719(22.4195) | Error 0.1900(0.1939) Steps 0(0.00) | Grad Norm 4.1398(7.5288) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 17.2766(18.5584) | Bit/dim 3.6281(3.6470) | Xent 0.5689(0.5546) | Loss 18.7912(21.7829) | Error 0.2000(0.1961) Steps 0(0.00) | Grad Norm 8.5252(7.8069) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 94.7392, Epoch Time 1129.8437(1084.8289), Bit/dim 3.6473(best: 3.6481), Xent 0.6926, Loss 3.9936, Error 0.2339(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 17.8087(18.5406) | Bit/dim 3.6422(3.6486) | Xent 0.4567(0.5439) | Loss 19.3005(27.1204) | Error 0.1544(0.1933) Steps 0(0.00) | Grad Norm 6.2260(7.6738) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 18.3723(18.4480) | Bit/dim 3.6662(3.6498) | Xent 0.5642(0.5388) | Loss 19.4991(25.2025) | Error 0.2111(0.1911) Steps 0(0.00) | Grad Norm 6.1406(7.2730) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.9044(18.6092) | Bit/dim 3.6489(3.6468) | Xent 0.5381(0.5396) | Loss 19.2066(23.8123) | Error 0.1956(0.1917) Steps 0(0.00) | Grad Norm 4.7854(7.4221) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 18.7980(18.5932) | Bit/dim 3.6526(3.6482) | Xent 0.4839(0.5431) | Loss 19.5234(22.8100) | Error 0.1678(0.1923) Steps 0(0.00) | Grad Norm 5.7477(7.7778) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 17.9276(18.4128) | Bit/dim 3.6572(3.6469) | Xent 0.5122(0.5412) | Loss 20.0096(21.9488) | Error 0.1678(0.1908) Steps 0(0.00) | Grad Norm 9.0069(7.6552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 93.9387, Epoch Time 1129.9198(1086.1816), Bit/dim 3.6460(best: 3.6473), Xent 0.7113, Loss 4.0016, Error 0.2435(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 18.3908(18.5250) | Bit/dim 3.6659(3.6478) | Xent 0.5854(0.5476) | Loss 20.9033(28.0485) | Error 0.2000(0.1943) Steps 0(0.00) | Grad Norm 13.4660(8.0224) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 17.8964(18.5191) | Bit/dim 3.6151(3.6476) | Xent 0.5570(0.5510) | Loss 19.9624(25.9278) | Error 0.1811(0.1958) Steps 0(0.00) | Grad Norm 9.1855(8.1571) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 17.0067(18.6047) | Bit/dim 3.6232(3.6470) | Xent 0.6160(0.5548) | Loss 18.0969(24.3271) | Error 0.2256(0.1966) Steps 0(0.00) | Grad Norm 16.0080(8.7635) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 19.8741(18.4775) | Bit/dim 3.6290(3.6509) | Xent 0.6132(0.5635) | Loss 20.7553(23.2079) | Error 0.2144(0.1996) Steps 0(0.00) | Grad Norm 8.7983(9.0334) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 16.5767(18.3422) | Bit/dim 3.6243(3.6489) | Xent 0.5382(0.5659) | Loss 18.7748(22.2788) | Error 0.1822(0.2004) Steps 0(0.00) | Grad Norm 5.9264(9.0405) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 17.6075(18.3073) | Bit/dim 3.6481(3.6493) | Xent 0.5823(0.5601) | Loss 19.5215(21.6687) | Error 0.2100(0.1976) Steps 0(0.00) | Grad Norm 6.2804(8.1680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 94.1579, Epoch Time 1122.0275(1087.2570), Bit/dim 3.6472(best: 3.6460), Xent 0.6913, Loss 3.9929, Error 0.2365(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 18.2309(18.3370) | Bit/dim 3.6572(3.6489) | Xent 0.5547(0.5478) | Loss 20.1345(27.1728) | Error 0.1933(0.1925) Steps 0(0.00) | Grad Norm 9.5865(7.7642) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 17.1511(18.4958) | Bit/dim 3.6490(3.6490) | Xent 0.4567(0.5381) | Loss 19.1879(25.2295) | Error 0.1589(0.1891) Steps 0(0.00) | Grad Norm 6.2656(7.6188) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 17.9640(18.4292) | Bit/dim 3.6242(3.6485) | Xent 0.5063(0.5302) | Loss 18.9650(23.8769) | Error 0.1878(0.1876) Steps 0(0.00) | Grad Norm 6.9886(7.3792) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 17.5095(18.3200) | Bit/dim 3.6396(3.6468) | Xent 0.5889(0.5342) | Loss 19.7852(22.8560) | Error 0.2222(0.1887) Steps 0(0.00) | Grad Norm 10.6268(7.3492) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 19.1700(18.4226) | Bit/dim 3.6596(3.6426) | Xent 0.6405(0.5420) | Loss 20.3336(22.0907) | Error 0.2333(0.1916) Steps 0(0.00) | Grad Norm 6.5806(7.7324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 95.1682, Epoch Time 1129.9883(1088.5389), Bit/dim 3.6430(best: 3.6460), Xent 0.6869, Loss 3.9864, Error 0.2353(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 19.3073(18.5872) | Bit/dim 3.6416(3.6453) | Xent 0.5792(0.5422) | Loss 20.1142(28.5350) | Error 0.2089(0.1929) Steps 0(0.00) | Grad Norm 7.2010(7.7578) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 18.6044(18.5772) | Bit/dim 3.6394(3.6418) | Xent 0.5032(0.5369) | Loss 18.9361(26.2144) | Error 0.1767(0.1918) Steps 0(0.00) | Grad Norm 6.2412(7.6636) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 20.3948(18.5657) | Bit/dim 3.6765(3.6447) | Xent 0.5554(0.5336) | Loss 21.2374(24.6244) | Error 0.1967(0.1901) Steps 0(0.00) | Grad Norm 10.1820(7.6285) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 19.2773(18.6946) | Bit/dim 3.6429(3.6443) | Xent 0.5766(0.5423) | Loss 20.3813(23.4347) | Error 0.2033(0.1927) Steps 0(0.00) | Grad Norm 7.5438(7.7613) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.6691(18.6542) | Bit/dim 3.6448(3.6437) | Xent 0.5258(0.5416) | Loss 19.8992(22.5072) | Error 0.1922(0.1934) Steps 0(0.00) | Grad Norm 7.6375(8.0323) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 17.9166(18.5681) | Bit/dim 3.6445(3.6475) | Xent 0.5241(0.5399) | Loss 20.2530(21.7294) | Error 0.1878(0.1928) Steps 0(0.00) | Grad Norm 6.9780(7.8768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 96.0219, Epoch Time 1139.6127(1090.0711), Bit/dim 3.6459(best: 3.6430), Xent 0.7218, Loss 4.0068, Error 0.2453(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.5746(18.4786) | Bit/dim 3.6616(3.6474) | Xent 0.5341(0.5419) | Loss 20.3967(27.1858) | Error 0.1956(0.1924) Steps 0(0.00) | Grad Norm 7.9299(7.7800) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 19.9808(18.6400) | Bit/dim 3.6488(3.6507) | Xent 0.5214(0.5414) | Loss 20.5220(25.3542) | Error 0.1889(0.1927) Steps 0(0.00) | Grad Norm 7.8129(8.3000) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 18.3302(18.5575) | Bit/dim 3.6601(3.6519) | Xent 0.5637(0.5411) | Loss 19.6893(23.8583) | Error 0.2167(0.1928) Steps 0(0.00) | Grad Norm 7.3116(8.3715) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 18.4850(18.6608) | Bit/dim 3.6488(3.6492) | Xent 0.5712(0.5367) | Loss 20.1859(22.8306) | Error 0.2022(0.1904) Steps 0(0.00) | Grad Norm 8.0568(8.3069) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 18.2109(18.9445) | Bit/dim 3.7078(3.6476) | Xent 0.5654(0.5376) | Loss 19.5434(22.0815) | Error 0.2078(0.1914) Steps 0(0.00) | Grad Norm 6.3004(7.8713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 95.0242, Epoch Time 1151.4175(1091.9115), Bit/dim 3.6525(best: 3.6430), Xent 0.6818, Loss 3.9934, Error 0.2314(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 17.9794(18.9099) | Bit/dim 3.6612(3.6475) | Xent 0.5897(0.5370) | Loss 19.5615(28.4744) | Error 0.2122(0.1909) Steps 0(0.00) | Grad Norm 9.3110(7.8911) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 18.0123(18.9615) | Bit/dim 3.6581(3.6473) | Xent 0.5029(0.5417) | Loss 20.1720(26.2012) | Error 0.1878(0.1923) Steps 0(0.00) | Grad Norm 6.9632(8.2530) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 17.6914(18.8246) | Bit/dim 3.6567(3.6471) | Xent 0.5665(0.5421) | Loss 20.1028(24.6074) | Error 0.2167(0.1925) Steps 0(0.00) | Grad Norm 8.9166(8.3609) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 18.0304(18.8011) | Bit/dim 3.6278(3.6459) | Xent 0.5140(0.5341) | Loss 19.3700(23.3729) | Error 0.1800(0.1899) Steps 0(0.00) | Grad Norm 4.3902(7.7657) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 18.9158(18.8760) | Bit/dim 3.6606(3.6485) | Xent 0.4674(0.5304) | Loss 19.2705(22.5366) | Error 0.1722(0.1894) Steps 0(0.00) | Grad Norm 6.4463(7.7588) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 18.9122(18.7479) | Bit/dim 3.5926(3.6434) | Xent 0.5198(0.5342) | Loss 20.3133(21.8842) | Error 0.1844(0.1910) Steps 0(0.00) | Grad Norm 6.5618(8.1369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 96.5853, Epoch Time 1145.6451(1093.5235), Bit/dim 3.6484(best: 3.6430), Xent 0.6845, Loss 3.9906, Error 0.2352(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 20.1011(18.6537) | Bit/dim 3.6483(3.6420) | Xent 0.5239(0.5268) | Loss 20.7761(27.3483) | Error 0.1767(0.1874) Steps 0(0.00) | Grad Norm 6.9822(7.9199) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 20.1707(18.7284) | Bit/dim 3.6394(3.6439) | Xent 0.5526(0.5242) | Loss 19.6236(25.3608) | Error 0.2122(0.1865) Steps 0(0.00) | Grad Norm 5.9696(7.8259) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 21.8871(18.7850) | Bit/dim 3.6229(3.6421) | Xent 0.4894(0.5187) | Loss 20.2787(23.9610) | Error 0.1856(0.1849) Steps 0(0.00) | Grad Norm 6.5469(7.8754) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 19.0591(18.6905) | Bit/dim 3.6329(3.6429) | Xent 0.5290(0.5266) | Loss 19.5769(22.9349) | Error 0.1933(0.1875) Steps 0(0.00) | Grad Norm 9.5149(8.1511) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 18.4967(18.5599) | Bit/dim 3.6799(3.6441) | Xent 0.4987(0.5299) | Loss 19.5393(22.1460) | Error 0.1611(0.1873) Steps 0(0.00) | Grad Norm 7.6332(8.2644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 94.7640, Epoch Time 1131.9692(1094.6769), Bit/dim 3.6451(best: 3.6430), Xent 0.6700, Loss 3.9801, Error 0.2287(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 18.6330(18.5309) | Bit/dim 3.6404(3.6438) | Xent 0.5204(0.5283) | Loss 19.9963(28.6646) | Error 0.1711(0.1870) Steps 0(0.00) | Grad Norm 5.6004(7.9245) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 19.5897(18.5584) | Bit/dim 3.6579(3.6451) | Xent 0.5093(0.5225) | Loss 20.4429(26.4267) | Error 0.1756(0.1851) Steps 0(0.00) | Grad Norm 7.6190(7.6581) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 17.8324(18.6577) | Bit/dim 3.6457(3.6478) | Xent 0.5299(0.5193) | Loss 19.6305(24.7443) | Error 0.1844(0.1837) Steps 0(0.00) | Grad Norm 5.2952(7.3018) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 19.3421(18.6380) | Bit/dim 3.6255(3.6479) | Xent 0.6199(0.5225) | Loss 20.3982(23.4980) | Error 0.2067(0.1853) Steps 0(0.00) | Grad Norm 12.6601(7.7960) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 19.5545(18.7442) | Bit/dim 3.6173(3.6444) | Xent 0.5155(0.5250) | Loss 20.5623(22.6403) | Error 0.1756(0.1857) Steps 0(0.00) | Grad Norm 9.5626(7.9941) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 20.2605(18.9691) | Bit/dim 3.6335(3.6440) | Xent 0.5383(0.5266) | Loss 20.4186(21.9936) | Error 0.1889(0.1849) Steps 0(0.00) | Grad Norm 9.9980(8.0170) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 96.8192, Epoch Time 1155.5468(1096.5030), Bit/dim 3.6461(best: 3.6430), Xent 0.6863, Loss 3.9892, Error 0.2302(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.0877(18.9821) | Bit/dim 3.6508(3.6408) | Xent 0.4725(0.5128) | Loss 19.1917(27.1116) | Error 0.1644(0.1795) Steps 0(0.00) | Grad Norm 8.1837(7.4033) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 17.7540(19.2082) | Bit/dim 3.6363(3.6388) | Xent 0.5176(0.5104) | Loss 19.2334(25.3318) | Error 0.1811(0.1793) Steps 0(0.00) | Grad Norm 5.3331(7.5185) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 19.2091(19.0969) | Bit/dim 3.6447(3.6398) | Xent 0.5450(0.5101) | Loss 20.5347(24.0070) | Error 0.2044(0.1798) Steps 0(0.00) | Grad Norm 6.9987(7.2953) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 17.3777(18.9041) | Bit/dim 3.6356(3.6403) | Xent 0.4514(0.5166) | Loss 18.4166(22.8430) | Error 0.1578(0.1837) Steps 0(0.00) | Grad Norm 4.9904(7.5675) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.7374(18.9966) | Bit/dim 3.6446(3.6432) | Xent 0.5106(0.5187) | Loss 20.1243(22.1909) | Error 0.1744(0.1849) Steps 0(0.00) | Grad Norm 6.1076(7.5561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 94.0330, Epoch Time 1158.0683(1098.3500), Bit/dim 3.6468(best: 3.6430), Xent 0.7322, Loss 4.0129, Error 0.2392(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.8943(18.9068) | Bit/dim 3.6407(3.6439) | Xent 0.4879(0.5149) | Loss 19.7580(28.4217) | Error 0.1733(0.1838) Steps 0(0.00) | Grad Norm 8.8697(7.9083) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 19.5507(19.0757) | Bit/dim 3.6426(3.6435) | Xent 0.5297(0.5140) | Loss 19.5125(26.2417) | Error 0.1844(0.1835) Steps 0(0.00) | Grad Norm 9.0806(8.1229) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 19.0894(19.1693) | Bit/dim 3.6385(3.6433) | Xent 0.4540(0.5103) | Loss 19.9286(24.6237) | Error 0.1611(0.1821) Steps 0(0.00) | Grad Norm 5.7977(7.7687) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 17.5421(19.1537) | Bit/dim 3.6127(3.6402) | Xent 0.5094(0.5131) | Loss 19.6742(23.4897) | Error 0.1833(0.1829) Steps 0(0.00) | Grad Norm 7.2459(7.7808) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 19.0277(19.1144) | Bit/dim 3.6515(3.6390) | Xent 0.5145(0.5152) | Loss 19.8282(22.6098) | Error 0.1911(0.1844) Steps 0(0.00) | Grad Norm 4.8537(7.5578) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 20.1074(19.3146) | Bit/dim 3.6309(3.6391) | Xent 0.4984(0.5086) | Loss 20.0664(22.0030) | Error 0.1778(0.1822) Steps 0(0.00) | Grad Norm 8.1316(7.3496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 95.9113, Epoch Time 1177.9019(1100.7365), Bit/dim 3.6434(best: 3.6430), Xent 0.6738, Loss 3.9803, Error 0.2273(best: 0.2276)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 19.5854(19.3089) | Bit/dim 3.6542(3.6392) | Xent 0.4427(0.5052) | Loss 20.1453(27.5068) | Error 0.1611(0.1808) Steps 0(0.00) | Grad Norm 8.4766(7.0858) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 20.3679(19.2302) | Bit/dim 3.6341(3.6406) | Xent 0.5195(0.5053) | Loss 20.9385(25.5715) | Error 0.1900(0.1807) Steps 0(0.00) | Grad Norm 10.4266(7.5943) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 19.9478(19.4007) | Bit/dim 3.6370(3.6406) | Xent 0.4689(0.5067) | Loss 21.3596(24.2882) | Error 0.1711(0.1801) Steps 0(0.00) | Grad Norm 8.3216(7.7097) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 19.9340(19.5042) | Bit/dim 3.6291(3.6397) | Xent 0.4865(0.5058) | Loss 20.8561(23.2530) | Error 0.1800(0.1796) Steps 0(0.00) | Grad Norm 4.2252(7.4204) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 18.3046(19.4724) | Bit/dim 3.6022(3.6378) | Xent 0.5384(0.5068) | Loss 19.9363(22.4663) | Error 0.2033(0.1806) Steps 0(0.00) | Grad Norm 9.3184(7.7277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 96.9502, Epoch Time 1185.8838(1103.2909), Bit/dim 3.6519(best: 3.6430), Xent 0.6696, Loss 3.9867, Error 0.2266(best: 0.2273)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 19.8694(19.4587) | Bit/dim 3.6434(3.6384) | Xent 0.4965(0.5079) | Loss 20.8411(28.8782) | Error 0.1833(0.1809) Steps 0(0.00) | Grad Norm 10.1676(8.0520) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 19.0941(19.2643) | Bit/dim 3.6728(3.6411) | Xent 0.4775(0.5006) | Loss 20.6954(26.5469) | Error 0.1756(0.1780) Steps 0(0.00) | Grad Norm 7.8727(7.6672) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 18.8807(19.2458) | Bit/dim 3.6129(3.6397) | Xent 0.5162(0.4998) | Loss 20.2842(24.9288) | Error 0.1833(0.1780) Steps 0(0.00) | Grad Norm 7.3482(7.8331) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 18.8960(19.1902) | Bit/dim 3.6384(3.6392) | Xent 0.5385(0.5003) | Loss 20.7091(23.6424) | Error 0.1800(0.1781) Steps 0(0.00) | Grad Norm 9.0226(7.9345) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 20.0747(19.1151) | Bit/dim 3.6301(3.6373) | Xent 0.4716(0.5000) | Loss 20.7960(22.7103) | Error 0.1589(0.1772) Steps 0(0.00) | Grad Norm 6.0250(7.8989) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.8113(19.1394) | Bit/dim 3.6184(3.6352) | Xent 0.4888(0.5009) | Loss 20.4005(22.1000) | Error 0.1789(0.1787) Steps 0(0.00) | Grad Norm 6.5068(7.5784) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 96.3606, Epoch Time 1161.2747(1105.0305), Bit/dim 3.6359(best: 3.6430), Xent 0.6919, Loss 3.9818, Error 0.2336(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 20.3340(19.2572) | Bit/dim 3.6317(3.6355) | Xent 0.4861(0.5017) | Loss 21.0849(27.8049) | Error 0.1667(0.1786) Steps 0(0.00) | Grad Norm 7.7351(7.7497) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 20.7821(19.2444) | Bit/dim 3.6494(3.6350) | Xent 0.5204(0.5047) | Loss 21.0410(25.8498) | Error 0.1656(0.1796) Steps 0(0.00) | Grad Norm 8.2309(8.1259) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 18.7223(19.3186) | Bit/dim 3.6227(3.6347) | Xent 0.5293(0.5059) | Loss 20.4560(24.4717) | Error 0.1911(0.1800) Steps 0(0.00) | Grad Norm 7.9255(8.2412) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 18.4963(19.2237) | Bit/dim 3.6458(3.6384) | Xent 0.4645(0.5069) | Loss 20.0704(23.3663) | Error 0.1722(0.1803) Steps 0(0.00) | Grad Norm 4.3903(8.3806) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 21.7205(19.2654) | Bit/dim 3.6349(3.6373) | Xent 0.4814(0.5065) | Loss 21.4737(22.5340) | Error 0.1733(0.1811) Steps 0(0.00) | Grad Norm 6.3952(8.0122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 97.1071, Epoch Time 1173.1513(1107.0741), Bit/dim 3.6356(best: 3.6359), Xent 0.6513, Loss 3.9613, Error 0.2234(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 17.0523(19.0779) | Bit/dim 3.5989(3.6355) | Xent 0.5710(0.5052) | Loss 18.8700(28.9056) | Error 0.2000(0.1801) Steps 0(0.00) | Grad Norm 13.6965(8.0661) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.9200(19.1074) | Bit/dim 3.6450(3.6357) | Xent 0.5399(0.5042) | Loss 20.4710(26.6460) | Error 0.2044(0.1804) Steps 0(0.00) | Grad Norm 9.2383(8.0351) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 18.8444(19.1884) | Bit/dim 3.6289(3.6368) | Xent 0.5191(0.4985) | Loss 20.7605(24.9840) | Error 0.1644(0.1772) Steps 0(0.00) | Grad Norm 8.0278(7.6218) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 20.2968(19.2716) | Bit/dim 3.6271(3.6387) | Xent 0.5295(0.4978) | Loss 21.2669(23.8276) | Error 0.1856(0.1767) Steps 0(0.00) | Grad Norm 9.1682(7.5585) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 20.1164(19.2479) | Bit/dim 3.6354(3.6368) | Xent 0.5737(0.5022) | Loss 19.8323(22.9106) | Error 0.2167(0.1789) Steps 0(0.00) | Grad Norm 9.0784(7.7011) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 20.3716(19.2390) | Bit/dim 3.6106(3.6354) | Xent 0.4533(0.5009) | Loss 21.3397(22.2647) | Error 0.1622(0.1791) Steps 0(0.00) | Grad Norm 5.0704(7.3622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 97.6435, Epoch Time 1173.9749(1109.0811), Bit/dim 3.6343(best: 3.6356), Xent 0.6833, Loss 3.9759, Error 0.2263(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.8593(19.2503) | Bit/dim 3.6371(3.6360) | Xent 0.4671(0.4979) | Loss 19.8104(27.8751) | Error 0.1733(0.1780) Steps 0(0.00) | Grad Norm 6.1042(7.5166) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 21.6804(19.4199) | Bit/dim 3.6347(3.6394) | Xent 0.4962(0.4932) | Loss 21.3700(25.8830) | Error 0.1644(0.1757) Steps 0(0.00) | Grad Norm 6.9042(7.5434) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 19.8461(19.4984) | Bit/dim 3.6635(3.6364) | Xent 0.4568(0.4918) | Loss 20.1620(24.3604) | Error 0.1689(0.1758) Steps 0(0.00) | Grad Norm 6.4795(7.4118) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 22.8764(19.4785) | Bit/dim 3.6557(3.6369) | Xent 0.4996(0.4916) | Loss 20.4673(23.3059) | Error 0.1700(0.1757) Steps 0(0.00) | Grad Norm 4.3644(7.2794) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.9318(19.4272) | Bit/dim 3.6277(3.6351) | Xent 0.4911(0.4887) | Loss 20.5373(22.5096) | Error 0.1756(0.1745) Steps 0(0.00) | Grad Norm 9.9158(7.4494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 95.9338, Epoch Time 1190.7643(1111.5316), Bit/dim 3.6400(best: 3.6343), Xent 0.6847, Loss 3.9823, Error 0.2357(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 19.9934(19.4158) | Bit/dim 3.6001(3.6298) | Xent 0.5145(0.4935) | Loss 20.4965(29.1938) | Error 0.1900(0.1769) Steps 0(0.00) | Grad Norm 14.8127(7.9051) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 19.4752(19.3694) | Bit/dim 3.6196(3.6308) | Xent 0.4497(0.4968) | Loss 19.8592(26.8021) | Error 0.1578(0.1764) Steps 0(0.00) | Grad Norm 4.3318(7.8352) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 18.9342(19.4830) | Bit/dim 3.6497(3.6325) | Xent 0.4515(0.4927) | Loss 20.5030(25.0697) | Error 0.1578(0.1741) Steps 0(0.00) | Grad Norm 6.9618(7.4301) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 18.6626(19.6068) | Bit/dim 3.6222(3.6314) | Xent 0.5337(0.4992) | Loss 19.5963(23.9174) | Error 0.1911(0.1763) Steps 0(0.00) | Grad Norm 11.4530(7.4720) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 18.7317(19.5611) | Bit/dim 3.6231(3.6341) | Xent 0.5032(0.4968) | Loss 20.2079(22.8940) | Error 0.1722(0.1762) Steps 0(0.00) | Grad Norm 5.0340(7.5873) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 19.6877(19.7266) | Bit/dim 3.6594(3.6348) | Xent 0.5236(0.4925) | Loss 20.2268(22.1876) | Error 0.1789(0.1743) Steps 0(0.00) | Grad Norm 8.5020(7.5533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 96.7150, Epoch Time 1195.5879(1114.0533), Bit/dim 3.6366(best: 3.6343), Xent 0.6781, Loss 3.9756, Error 0.2293(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 19.2220(19.4962) | Bit/dim 3.5932(3.6318) | Xent 0.4571(0.4935) | Loss 21.1634(27.6784) | Error 0.1667(0.1753) Steps 0(0.00) | Grad Norm 4.1190(7.5020) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 19.8178(19.4641) | Bit/dim 3.6250(3.6330) | Xent 0.4587(0.4898) | Loss 20.7253(25.7697) | Error 0.1556(0.1736) Steps 0(0.00) | Grad Norm 9.3226(7.8091) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 18.3301(19.2218) | Bit/dim 3.6528(3.6344) | Xent 0.5258(0.4905) | Loss 20.2933(24.3131) | Error 0.1856(0.1744) Steps 0(0.00) | Grad Norm 10.0670(8.0303) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 20.1266(19.3547) | Bit/dim 3.6374(3.6358) | Xent 0.4948(0.4894) | Loss 20.4409(23.2206) | Error 0.1667(0.1747) Steps 0(0.00) | Grad Norm 4.8292(7.9069) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 18.4899(19.2603) | Bit/dim 3.5901(3.6331) | Xent 0.5495(0.4990) | Loss 20.6973(22.4995) | Error 0.1989(0.1778) Steps 0(0.00) | Grad Norm 8.0960(7.9934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 97.6830, Epoch Time 1172.6647(1115.8116), Bit/dim 3.6326(best: 3.6343), Xent 0.6623, Loss 3.9637, Error 0.2247(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 19.3703(19.5200) | Bit/dim 3.6367(3.6336) | Xent 0.4969(0.4942) | Loss 20.4954(29.2685) | Error 0.1667(0.1752) Steps 0(0.00) | Grad Norm 8.8668(7.8980) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.6575(19.6175) | Bit/dim 3.6205(3.6349) | Xent 0.5041(0.4918) | Loss 20.4662(26.8892) | Error 0.1733(0.1751) Steps 0(0.00) | Grad Norm 8.5546(7.9508) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 18.3894(19.6064) | Bit/dim 3.6190(3.6348) | Xent 0.4521(0.4909) | Loss 20.0301(25.1365) | Error 0.1544(0.1738) Steps 0(0.00) | Grad Norm 7.6925(7.8389) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 19.9441(19.5674) | Bit/dim 3.6088(3.6308) | Xent 0.4887(0.4866) | Loss 20.3313(23.8606) | Error 0.1622(0.1730) Steps 0(0.00) | Grad Norm 8.4353(7.5296) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 19.2869(19.6731) | Bit/dim 3.6488(3.6295) | Xent 0.4798(0.4857) | Loss 20.0009(22.9915) | Error 0.1633(0.1720) Steps 0(0.00) | Grad Norm 5.0669(7.3444) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 18.4829(19.6067) | Bit/dim 3.6728(3.6314) | Xent 0.5108(0.4858) | Loss 19.5251(22.2355) | Error 0.1978(0.1723) Steps 0(0.00) | Grad Norm 6.6044(7.1571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 98.6159, Epoch Time 1201.1505(1118.3718), Bit/dim 3.6382(best: 3.6326), Xent 0.6792, Loss 3.9778, Error 0.2303(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 19.0624(19.7213) | Bit/dim 3.6286(3.6313) | Xent 0.4840(0.4799) | Loss 20.5789(27.8405) | Error 0.1711(0.1705) Steps 0(0.00) | Grad Norm 9.0502(7.2524) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 19.2632(19.6023) | Bit/dim 3.6395(3.6326) | Xent 0.5179(0.4873) | Loss 20.9301(25.7965) | Error 0.1922(0.1733) Steps 0(0.00) | Grad Norm 7.5212(7.8555) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 18.5648(19.6325) | Bit/dim 3.6335(3.6336) | Xent 0.5390(0.4896) | Loss 20.1546(24.3875) | Error 0.1989(0.1752) Steps 0(0.00) | Grad Norm 16.1170(8.5642) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 20.3634(19.5682) | Bit/dim 3.6350(3.6359) | Xent 0.4608(0.4984) | Loss 20.9127(23.4048) | Error 0.1589(0.1780) Steps 0(0.00) | Grad Norm 8.3365(8.7972) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 18.7212(19.4733) | Bit/dim 3.6268(3.6370) | Xent 0.4552(0.4954) | Loss 19.8434(22.6886) | Error 0.1633(0.1772) Steps 0(0.00) | Grad Norm 9.7856(8.7053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 95.8520, Epoch Time 1185.8807(1120.3971), Bit/dim 3.6376(best: 3.6326), Xent 0.6994, Loss 3.9873, Error 0.2301(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 18.8603(19.3697) | Bit/dim 3.6437(3.6387) | Xent 0.5054(0.4960) | Loss 20.7882(29.1563) | Error 0.1767(0.1770) Steps 0(0.00) | Grad Norm 5.1840(8.2145) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 18.4532(19.3281) | Bit/dim 3.6291(3.6393) | Xent 0.4726(0.4889) | Loss 19.9113(26.7608) | Error 0.1811(0.1754) Steps 0(0.00) | Grad Norm 9.9688(8.1249) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 19.7153(19.2314) | Bit/dim 3.6156(3.6378) | Xent 0.4498(0.4829) | Loss 20.0454(25.0130) | Error 0.1522(0.1722) Steps 0(0.00) | Grad Norm 8.6380(7.9228) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 19.2749(19.3385) | Bit/dim 3.6139(3.6350) | Xent 0.4615(0.4770) | Loss 21.3299(23.7709) | Error 0.1556(0.1689) Steps 0(0.00) | Grad Norm 5.6819(7.4822) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 17.8634(19.2242) | Bit/dim 3.6198(3.6345) | Xent 0.5024(0.4751) | Loss 19.8881(22.8355) | Error 0.1844(0.1682) Steps 0(0.00) | Grad Norm 8.3449(7.4427) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 20.9854(19.4093) | Bit/dim 3.6559(3.6320) | Xent 0.5183(0.4785) | Loss 20.3976(22.1765) | Error 0.1844(0.1708) Steps 0(0.00) | Grad Norm 8.1943(7.2564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 96.9855, Epoch Time 1179.2508(1122.1627), Bit/dim 3.6374(best: 3.6326), Xent 0.6955, Loss 3.9852, Error 0.2313(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.9174(19.4053) | Bit/dim 3.5953(3.6316) | Xent 0.4916(0.4773) | Loss 20.5897(27.8125) | Error 0.1900(0.1705) Steps 0(0.00) | Grad Norm 9.1665(7.8129) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 21.0261(19.6175) | Bit/dim 3.6028(3.6322) | Xent 0.4551(0.4712) | Loss 20.8881(25.8631) | Error 0.1478(0.1673) Steps 0(0.00) | Grad Norm 9.0748(7.5543) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 19.2418(19.4591) | Bit/dim 3.6079(3.6328) | Xent 0.4375(0.4707) | Loss 21.0185(24.4673) | Error 0.1522(0.1669) Steps 0(0.00) | Grad Norm 6.4847(7.4578) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 18.2661(19.4052) | Bit/dim 3.6307(3.6317) | Xent 0.5552(0.4794) | Loss 19.8837(23.3468) | Error 0.1889(0.1691) Steps 0(0.00) | Grad Norm 10.6439(7.8126) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 18.9027(19.4963) | Bit/dim 3.6187(3.6317) | Xent 0.4671(0.4772) | Loss 20.2403(22.4978) | Error 0.1633(0.1680) Steps 0(0.00) | Grad Norm 5.4616(7.6879) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 96.5723, Epoch Time 1186.6633(1124.0977), Bit/dim 3.6350(best: 3.6326), Xent 0.6865, Loss 3.9783, Error 0.2274(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 21.4495(19.5165) | Bit/dim 3.6458(3.6341) | Xent 0.5208(0.4777) | Loss 20.3471(28.9498) | Error 0.1789(0.1675) Steps 0(0.00) | Grad Norm 13.3485(8.0444) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 19.2622(19.4455) | Bit/dim 3.6147(3.6330) | Xent 0.4291(0.4724) | Loss 19.9753(26.7148) | Error 0.1622(0.1650) Steps 0(0.00) | Grad Norm 9.1378(7.6452) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 20.1495(19.4348) | Bit/dim 3.6353(3.6313) | Xent 0.4501(0.4638) | Loss 20.3659(25.0022) | Error 0.1633(0.1621) Steps 0(0.00) | Grad Norm 7.5569(7.4038) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 18.4756(19.3889) | Bit/dim 3.6169(3.6303) | Xent 0.5182(0.4658) | Loss 19.2450(23.7438) | Error 0.1711(0.1635) Steps 0(0.00) | Grad Norm 8.8624(7.5881) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 20.0288(19.4651) | Bit/dim 3.6118(3.6274) | Xent 0.4826(0.4687) | Loss 20.6796(22.8434) | Error 0.1756(0.1650) Steps 0(0.00) | Grad Norm 6.9635(7.5878) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 19.8236(19.5792) | Bit/dim 3.6135(3.6279) | Xent 0.5061(0.4716) | Loss 21.1766(22.1904) | Error 0.1911(0.1666) Steps 0(0.00) | Grad Norm 10.8958(7.6293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 98.6116, Epoch Time 1191.1117(1126.1081), Bit/dim 3.6307(best: 3.6326), Xent 0.6642, Loss 3.9628, Error 0.2186(best: 0.2234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 18.4255(19.6537) | Bit/dim 3.6095(3.6290) | Xent 0.5729(0.4728) | Loss 20.4442(28.0149) | Error 0.2056(0.1674) Steps 0(0.00) | Grad Norm 12.8308(7.8024) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 19.1949(19.5202) | Bit/dim 3.6269(3.6293) | Xent 0.4043(0.4631) | Loss 20.1163(25.8898) | Error 0.1411(0.1641) Steps 0(0.00) | Grad Norm 4.1888(7.7056) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 18.2013(19.5418) | Bit/dim 3.6702(3.6323) | Xent 0.4297(0.4603) | Loss 19.6620(24.3854) | Error 0.1633(0.1637) Steps 0(0.00) | Grad Norm 4.5958(7.3285) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 19.0870(19.4742) | Bit/dim 3.6275(3.6280) | Xent 0.4247(0.4597) | Loss 20.7337(23.3581) | Error 0.1478(0.1635) Steps 0(0.00) | Grad Norm 5.2505(7.3412) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 21.8078(19.5161) | Bit/dim 3.6226(3.6267) | Xent 0.4624(0.4566) | Loss 21.0018(22.5825) | Error 0.1600(0.1619) Steps 0(0.00) | Grad Norm 6.9349(6.9642) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 98.0792, Epoch Time 1189.6871(1128.0155), Bit/dim 3.6319(best: 3.6307), Xent 0.6708, Loss 3.9673, Error 0.2233(best: 0.2186)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 19.3739(19.5800) | Bit/dim 3.6393(3.6257) | Xent 0.4720(0.4570) | Loss 20.2302(28.9390) | Error 0.1667(0.1625) Steps 0(0.00) | Grad Norm 5.0649(6.8477) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 20.4177(19.4406) | Bit/dim 3.6410(3.6270) | Xent 0.4325(0.4579) | Loss 20.4545(26.6508) | Error 0.1478(0.1629) Steps 0(0.00) | Grad Norm 6.7456(7.3221) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 19.9706(19.4424) | Bit/dim 3.6492(3.6287) | Xent 0.4977(0.4754) | Loss 19.7513(24.9916) | Error 0.1800(0.1691) Steps 0(0.00) | Grad Norm 10.7550(8.2005) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 21.2294(19.6536) | Bit/dim 3.6484(3.6307) | Xent 0.4924(0.4788) | Loss 21.6428(23.8209) | Error 0.1633(0.1700) Steps 0(0.00) | Grad Norm 6.7078(7.8713) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 19.2358(19.7126) | Bit/dim 3.6492(3.6286) | Xent 0.4889(0.4729) | Loss 20.2121(22.8911) | Error 0.1744(0.1682) Steps 0(0.00) | Grad Norm 7.9562(7.5332) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 22.1612(19.8613) | Bit/dim 3.6439(3.6279) | Xent 0.4281(0.4654) | Loss 20.7275(22.1676) | Error 0.1433(0.1651) Steps 0(0.00) | Grad Norm 10.9119(7.4325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 98.3825, Epoch Time 1203.3023(1130.2741), Bit/dim 3.6249(best: 3.6307), Xent 0.6726, Loss 3.9612, Error 0.2227(best: 0.2186)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 20.7120(19.7347) | Bit/dim 3.6296(3.6298) | Xent 0.5328(0.4683) | Loss 21.3701(27.8087) | Error 0.1956(0.1656) Steps 0(0.00) | Grad Norm 12.9954(7.9548) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 20.8223(19.7605) | Bit/dim 3.6107(3.6286) | Xent 0.3910(0.4649) | Loss 19.8941(25.8349) | Error 0.1256(0.1642) Steps 0(0.00) | Grad Norm 6.6997(7.7983) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 19.0673(19.7371) | Bit/dim 3.6645(3.6271) | Xent 0.4975(0.4601) | Loss 20.2058(24.3619) | Error 0.1756(0.1625) Steps 0(0.00) | Grad Norm 6.7499(7.8462) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 19.3091(19.7978) | Bit/dim 3.5961(3.6250) | Xent 0.4484(0.4580) | Loss 20.0158(23.2731) | Error 0.1578(0.1616) Steps 0(0.00) | Grad Norm 5.6070(7.2964) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 18.8935(19.6537) | Bit/dim 3.6332(3.6254) | Xent 0.4414(0.4547) | Loss 20.1507(22.4006) | Error 0.1567(0.1614) Steps 0(0.00) | Grad Norm 9.4432(7.2551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 95.3588, Epoch Time 1196.5469(1132.2623), Bit/dim 3.6352(best: 3.6249), Xent 0.6779, Loss 3.9741, Error 0.2165(best: 0.2186)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.8382(19.7627) | Bit/dim 3.6134(3.6234) | Xent 0.4538(0.4498) | Loss 21.2661(28.4748) | Error 0.1500(0.1588) Steps 0(0.00) | Grad Norm 6.2929(7.3079) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 19.3302(19.8608) | Bit/dim 3.6289(3.6215) | Xent 0.3793(0.4413) | Loss 19.6781(26.3705) | Error 0.1344(0.1568) Steps 0(0.00) | Grad Norm 5.7309(7.0788) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 18.3282(19.7713) | Bit/dim 3.6521(3.6242) | Xent 0.4513(0.4399) | Loss 20.4739(24.7345) | Error 0.1522(0.1568) Steps 0(0.00) | Grad Norm 5.1840(7.1368) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 21.7860(19.7341) | Bit/dim 3.6103(3.6224) | Xent 0.4801(0.4418) | Loss 20.5984(23.5933) | Error 0.1656(0.1571) Steps 0(0.00) | Grad Norm 6.6384(6.8961) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 19.2183(19.7355) | Bit/dim 3.6255(3.6227) | Xent 0.5383(0.4487) | Loss 20.8381(22.7630) | Error 0.1944(0.1581) Steps 0(0.00) | Grad Norm 8.8320(7.0925) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 20.9575(19.6591) | Bit/dim 3.6451(3.6250) | Xent 0.4540(0.4544) | Loss 20.6717(22.1023) | Error 0.1622(0.1589) Steps 0(0.00) | Grad Norm 5.0810(7.0024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 95.7716, Epoch Time 1196.8769(1134.2007), Bit/dim 3.6299(best: 3.6249), Xent 0.6759, Loss 3.9679, Error 0.2191(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 18.4510(19.4862) | Bit/dim 3.6202(3.6270) | Xent 0.3795(0.4444) | Loss 20.2997(27.8952) | Error 0.1389(0.1570) Steps 0(0.00) | Grad Norm 3.9627(6.5892) | Total Time 0.00(0.00)\n",
      "Iter 7500 | Time 21.7480(19.8478) | Bit/dim 3.6093(3.6230) | Xent 0.4317(0.4426) | Loss 21.3847(26.0389) | Error 0.1667(0.1568) Steps 0(0.00) | Grad Norm 5.6071(6.4778) | Total Time 0.00(0.00)\n",
      "Iter 7510 | Time 19.5514(20.1335) | Bit/dim 3.6286(3.6227) | Xent 0.4204(0.4368) | Loss 20.1205(24.6091) | Error 0.1467(0.1551) Steps 0(0.00) | Grad Norm 5.5412(6.4330) | Total Time 0.00(0.00)\n",
      "Iter 7520 | Time 20.7024(20.1750) | Bit/dim 3.6299(3.6249) | Xent 0.5017(0.4449) | Loss 20.4608(23.4535) | Error 0.1800(0.1578) Steps 0(0.00) | Grad Norm 12.7781(7.1669) | Total Time 0.00(0.00)\n",
      "Iter 7530 | Time 19.8740(20.1017) | Bit/dim 3.6373(3.6275) | Xent 0.5217(0.4584) | Loss 21.1057(22.6761) | Error 0.1744(0.1614) Steps 0(0.00) | Grad Norm 10.7645(7.5891) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 99.0367, Epoch Time 1224.4008(1136.9067), Bit/dim 3.6397(best: 3.6249), Xent 0.6775, Loss 3.9785, Error 0.2224(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 18.1522(19.9206) | Bit/dim 3.6526(3.6287) | Xent 0.4401(0.4536) | Loss 20.1247(29.1556) | Error 0.1422(0.1600) Steps 0(0.00) | Grad Norm 7.7831(7.5450) | Total Time 0.00(0.00)\n",
      "Iter 7550 | Time 18.9367(19.9265) | Bit/dim 3.6153(3.6284) | Xent 0.4969(0.4619) | Loss 19.3804(26.8628) | Error 0.1711(0.1632) Steps 0(0.00) | Grad Norm 11.6248(8.5197) | Total Time 0.00(0.00)\n",
      "Iter 7560 | Time 18.9294(19.9593) | Bit/dim 3.6147(3.6270) | Xent 0.4410(0.4583) | Loss 20.2583(25.2232) | Error 0.1522(0.1622) Steps 0(0.00) | Grad Norm 7.0394(8.0924) | Total Time 0.00(0.00)\n",
      "Iter 7570 | Time 20.0603(19.8674) | Bit/dim 3.6467(3.6296) | Xent 0.4031(0.4551) | Loss 20.4842(23.9791) | Error 0.1367(0.1605) Steps 0(0.00) | Grad Norm 6.0622(8.0284) | Total Time 0.00(0.00)\n",
      "Iter 7580 | Time 17.6922(19.8298) | Bit/dim 3.6072(3.6237) | Xent 0.4628(0.4592) | Loss 19.5999(23.0219) | Error 0.1700(0.1626) Steps 0(0.00) | Grad Norm 12.3683(8.2033) | Total Time 0.00(0.00)\n",
      "Iter 7590 | Time 19.5132(19.7142) | Bit/dim 3.6607(3.6288) | Xent 0.4748(0.4633) | Loss 20.1801(22.3042) | Error 0.1800(0.1644) Steps 0(0.00) | Grad Norm 11.5535(8.4458) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 97.5349, Epoch Time 1200.6347(1138.8186), Bit/dim 3.6308(best: 3.6249), Xent 0.6927, Loss 3.9772, Error 0.2258(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 18.1075(19.7479) | Bit/dim 3.6031(3.6278) | Xent 0.5089(0.4627) | Loss 19.2959(27.9109) | Error 0.1789(0.1640) Steps 0(0.00) | Grad Norm 9.0119(8.2233) | Total Time 0.00(0.00)\n",
      "Iter 7610 | Time 19.5284(19.7832) | Bit/dim 3.6307(3.6262) | Xent 0.4312(0.4538) | Loss 19.9696(25.8757) | Error 0.1511(0.1607) Steps 0(0.00) | Grad Norm 7.3213(7.7580) | Total Time 0.00(0.00)\n",
      "Iter 7620 | Time 18.3656(19.6069) | Bit/dim 3.6086(3.6223) | Xent 0.4059(0.4460) | Loss 20.8926(24.4772) | Error 0.1378(0.1585) Steps 0(0.00) | Grad Norm 6.9916(7.4302) | Total Time 0.00(0.00)\n",
      "Iter 7630 | Time 19.5879(19.7745) | Bit/dim 3.6415(3.6234) | Xent 0.3941(0.4419) | Loss 20.3647(23.3799) | Error 0.1289(0.1567) Steps 0(0.00) | Grad Norm 5.2814(7.1371) | Total Time 0.00(0.00)\n",
      "Iter 7640 | Time 20.0717(19.8451) | Bit/dim 3.5912(3.6258) | Xent 0.4575(0.4439) | Loss 20.2982(22.6005) | Error 0.1578(0.1569) Steps 0(0.00) | Grad Norm 6.3847(7.1335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 97.9965, Epoch Time 1205.6910(1140.8247), Bit/dim 3.6182(best: 3.6249), Xent 0.6824, Loss 3.9593, Error 0.2222(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 18.9369(19.9423) | Bit/dim 3.6465(3.6242) | Xent 0.4063(0.4404) | Loss 20.3011(29.2099) | Error 0.1289(0.1538) Steps 0(0.00) | Grad Norm 6.0486(6.8334) | Total Time 0.00(0.00)\n",
      "Iter 7660 | Time 18.2925(19.9514) | Bit/dim 3.6321(3.6236) | Xent 0.4120(0.4299) | Loss 20.2450(26.8833) | Error 0.1600(0.1521) Steps 0(0.00) | Grad Norm 8.1106(6.8978) | Total Time 0.00(0.00)\n",
      "Iter 7670 | Time 19.2114(19.9402) | Bit/dim 3.5979(3.6195) | Xent 0.5162(0.4377) | Loss 19.7138(25.1837) | Error 0.1889(0.1559) Steps 0(0.00) | Grad Norm 10.2734(7.5478) | Total Time 0.00(0.00)\n",
      "Iter 7680 | Time 18.5715(19.7454) | Bit/dim 3.6419(3.6214) | Xent 0.4243(0.4425) | Loss 21.0551(23.8992) | Error 0.1578(0.1568) Steps 0(0.00) | Grad Norm 7.7799(7.6246) | Total Time 0.00(0.00)\n",
      "Iter 7690 | Time 18.7502(19.7788) | Bit/dim 3.6381(3.6258) | Xent 0.4104(0.4416) | Loss 19.7010(22.9477) | Error 0.1311(0.1563) Steps 0(0.00) | Grad Norm 5.8475(7.4932) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
