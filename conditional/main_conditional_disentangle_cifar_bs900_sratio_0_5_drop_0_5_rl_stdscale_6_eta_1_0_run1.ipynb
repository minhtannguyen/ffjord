{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=1.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 11.9273(30.8082) | Bit/dim 8.6904(8.9513) | Xent 2.2806(2.3001) | Loss 31.0477(31.7670) | Error 0.7944(0.8601) Steps 0(0.00) | Grad Norm 22.8151(29.5505) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.7231(25.9439) | Bit/dim 8.4872(8.8626) | Xent 2.2265(2.2874) | Loss 30.9426(31.5389) | Error 0.7233(0.8325) Steps 0(0.00) | Grad Norm 9.6078(25.5835) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 12.0238(22.3030) | Bit/dim 8.3924(8.7511) | Xent 2.1747(2.2634) | Loss 29.3426(31.2562) | Error 0.7556(0.8089) Steps 0(0.00) | Grad Norm 9.3346(21.0086) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.6505(19.6347) | Bit/dim 8.1924(8.6231) | Xent 2.1132(2.2346) | Loss 30.7204(30.9829) | Error 0.7222(0.7908) Steps 0(0.00) | Grad Norm 5.9610(17.2448) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.4477(17.7569) | Bit/dim 7.9744(8.4736) | Xent 2.1010(2.2020) | Loss 29.5238(30.6269) | Error 0.7111(0.7754) Steps 0(0.00) | Grad Norm 5.7911(14.3212) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 73.1995, Epoch Time 791.5646(791.5646), Bit/dim 7.7700(best: inf), Xent 2.0787, Loss 8.8094, Error 0.7002(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.4256(16.4011) | Bit/dim 7.6554(8.2921) | Xent 2.0903(2.1718) | Loss 29.0063(36.0715) | Error 0.7089(0.7590) Steps 0(0.00) | Grad Norm 5.3012(12.0680) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 12.6357(15.5037) | Bit/dim 7.3662(8.0807) | Xent 2.0770(2.1454) | Loss 27.8847(34.1008) | Error 0.7011(0.7420) Steps 0(0.00) | Grad Norm 4.5535(10.2014) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.5822(14.8681) | Bit/dim 7.1815(7.8641) | Xent 2.0846(2.1249) | Loss 28.1883(32.5923) | Error 0.6733(0.7256) Steps 0(0.00) | Grad Norm 3.3168(8.5228) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 12.6925(14.3218) | Bit/dim 7.0766(7.6683) | Xent 2.0837(2.1141) | Loss 28.1661(31.3758) | Error 0.6911(0.7186) Steps 0(0.00) | Grad Norm 2.5766(7.0004) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.5615(14.0525) | Bit/dim 7.0199(7.5057) | Xent 2.0803(2.1033) | Loss 27.7961(30.4677) | Error 0.7078(0.7144) Steps 0(0.00) | Grad Norm 2.7233(5.7689) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.5523(13.9525) | Bit/dim 6.9998(7.3757) | Xent 2.0654(2.0947) | Loss 28.0565(29.8408) | Error 0.7289(0.7137) Steps 0(0.00) | Grad Norm 6.8063(5.2330) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 73.9717, Epoch Time 814.2497(792.2452), Bit/dim 6.9919(best: 7.7700), Xent 2.0577, Loss 8.0207, Error 0.6948(best: 0.7002)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 13.8479(13.9664) | Bit/dim 6.9380(7.2705) | Xent 2.0473(2.0854) | Loss 28.0723(34.6152) | Error 0.6867(0.7112) Steps 0(0.00) | Grad Norm 4.0745(5.1541) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.6952(14.0378) | Bit/dim 6.9082(7.1832) | Xent 2.0648(2.0760) | Loss 27.8971(32.8445) | Error 0.7133(0.7085) Steps 0(0.00) | Grad Norm 5.2216(4.7577) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 14.5457(14.0927) | Bit/dim 6.8748(7.1069) | Xent 2.0530(2.0703) | Loss 27.7224(31.4604) | Error 0.6944(0.7050) Steps 0(0.00) | Grad Norm 15.3276(5.4582) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 14.0899(14.1425) | Bit/dim 6.8473(7.0376) | Xent 1.9910(2.0588) | Loss 28.4699(30.4364) | Error 0.6800(0.7000) Steps 0(0.00) | Grad Norm 2.2228(5.6315) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.4637(14.1676) | Bit/dim 6.7213(6.9635) | Xent 1.9796(2.0480) | Loss 27.6904(29.6286) | Error 0.6822(0.6957) Steps 0(0.00) | Grad Norm 16.7872(6.5828) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 75.7522, Epoch Time 876.4923(794.7726), Bit/dim 6.6415(best: 6.9919), Xent 2.0314, Loss 7.6572, Error 0.6979(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 14.1639(14.2210) | Bit/dim 6.5810(6.8815) | Xent 2.0141(2.0451) | Loss 26.9825(35.0561) | Error 0.6800(0.6974) Steps 0(0.00) | Grad Norm 4.7593(12.8172) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.2541(14.1035) | Bit/dim 6.4620(6.7799) | Xent 2.2136(2.0472) | Loss 27.1998(32.9414) | Error 0.7956(0.7018) Steps 0(0.00) | Grad Norm 120.0538(23.8532) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 14.4065(14.0915) | Bit/dim 6.2630(6.6620) | Xent 2.0364(2.0688) | Loss 26.7135(31.3805) | Error 0.7178(0.7181) Steps 0(0.00) | Grad Norm 26.8665(35.8688) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.3943(14.0014) | Bit/dim 6.1635(6.5335) | Xent 2.2375(2.0690) | Loss 25.8781(30.0079) | Error 0.7867(0.7188) Steps 0(0.00) | Grad Norm 150.5963(42.1168) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.8383(13.9701) | Bit/dim 5.9480(6.4119) | Xent 2.0496(2.0755) | Loss 25.2402(28.9605) | Error 0.7000(0.7205) Steps 0(0.00) | Grad Norm 24.0208(51.0957) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.6021(13.8732) | Bit/dim 5.8531(6.2756) | Xent 2.0345(2.0664) | Loss 25.4173(28.0803) | Error 0.6856(0.7155) Steps 0(0.00) | Grad Norm 41.5550(46.0286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 74.4318, Epoch Time 854.7734(796.5726), Bit/dim 5.8267(best: 6.6415), Xent 2.0118, Loss 6.8326, Error 0.6741(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.5952(13.8295) | Bit/dim 5.8101(6.1539) | Xent 1.9660(2.0660) | Loss 24.9369(32.7380) | Error 0.6478(0.7160) Steps 0(0.00) | Grad Norm 22.4231(51.7858) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.0783(13.7010) | Bit/dim 5.7158(6.0538) | Xent 2.0028(2.0618) | Loss 25.5162(30.8745) | Error 0.6444(0.7141) Steps 0(0.00) | Grad Norm 6.1126(49.4136) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 12.8513(13.5648) | Bit/dim 5.6593(5.9624) | Xent 1.9587(2.0484) | Loss 24.9182(29.3013) | Error 0.6522(0.7053) Steps 0(0.00) | Grad Norm 11.6803(41.3750) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.4078(13.5889) | Bit/dim 5.7012(5.8852) | Xent 2.0861(2.0406) | Loss 24.4816(28.1967) | Error 0.7289(0.7018) Steps 0(0.00) | Grad Norm 62.5971(40.3296) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.8837(13.6049) | Bit/dim 5.6296(5.8233) | Xent 2.0029(2.0327) | Loss 25.1763(27.3736) | Error 0.6822(0.6985) Steps 0(0.00) | Grad Norm 12.7317(37.6896) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 76.0638, Epoch Time 837.3516(797.7960), Bit/dim 5.6210(best: 5.8267), Xent 1.9573, Loss 6.5996, Error 0.6522(best: 0.6741)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.9849(13.5574) | Bit/dim 5.6304(5.7720) | Xent 1.9909(2.0195) | Loss 25.1684(32.8923) | Error 0.6989(0.6940) Steps 0(0.00) | Grad Norm 39.1967(35.5187) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.4040(13.4883) | Bit/dim 5.5844(5.7248) | Xent 1.9172(2.0066) | Loss 24.7614(30.7302) | Error 0.6700(0.6895) Steps 0(0.00) | Grad Norm 43.0513(34.2955) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.4265(13.5059) | Bit/dim 5.5840(5.6820) | Xent 1.9121(1.9874) | Loss 23.8398(29.1520) | Error 0.6500(0.6828) Steps 0(0.00) | Grad Norm 8.5344(30.4941) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.6107(13.5366) | Bit/dim 5.5507(5.6472) | Xent 1.9220(1.9721) | Loss 24.8131(27.9945) | Error 0.6544(0.6782) Steps 0(0.00) | Grad Norm 14.8449(27.8518) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.2680(13.5000) | Bit/dim 5.5759(5.6213) | Xent 2.0395(1.9806) | Loss 24.5096(27.1387) | Error 0.7289(0.6836) Steps 0(0.00) | Grad Norm 56.2719(35.4860) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.7015(13.5546) | Bit/dim 5.5312(5.5965) | Xent 1.9647(2.0007) | Loss 25.2577(26.5743) | Error 0.6811(0.6939) Steps 0(0.00) | Grad Norm 14.6399(38.8435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 75.3691, Epoch Time 836.4725(798.9563), Bit/dim 5.5045(best: 5.6210), Xent 2.0179, Loss 6.5135, Error 0.6936(best: 0.6522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.2297(13.5037) | Bit/dim 5.4602(5.5639) | Xent 1.9624(2.0035) | Loss 24.6524(31.5160) | Error 0.6644(0.6939) Steps 0(0.00) | Grad Norm 9.7724(34.0101) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.3637(13.5164) | Bit/dim 5.3836(5.5316) | Xent 1.9580(1.9909) | Loss 24.7001(29.6852) | Error 0.6600(0.6883) Steps 0(0.00) | Grad Norm 14.1645(28.3662) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.3960(13.5545) | Bit/dim 5.3602(5.4920) | Xent 1.9301(1.9728) | Loss 24.7776(28.3158) | Error 0.6578(0.6792) Steps 0(0.00) | Grad Norm 24.4161(27.3017) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.0321(13.6619) | Bit/dim 5.3598(5.4601) | Xent 2.0582(1.9612) | Loss 24.9259(27.3930) | Error 0.7189(0.6761) Steps 0(0.00) | Grad Norm 96.4968(30.0133) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 14.5373(13.7626) | Bit/dim 5.3533(5.4241) | Xent 1.8763(1.9543) | Loss 24.7073(26.7190) | Error 0.6467(0.6749) Steps 0(0.00) | Grad Norm 19.2255(31.8779) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 77.8612, Epoch Time 853.1469(800.5820), Bit/dim 5.2874(best: 5.5045), Xent 1.8777, Loss 6.2263, Error 0.6315(best: 0.6522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.6997(13.8909) | Bit/dim 5.3275(5.3876) | Xent 1.9556(1.9367) | Loss 24.7731(32.7966) | Error 0.6878(0.6708) Steps 0(0.00) | Grad Norm 37.9228(29.2673) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.3920(14.0496) | Bit/dim 5.2777(5.3539) | Xent 1.9621(1.9270) | Loss 24.3055(30.6281) | Error 0.6922(0.6684) Steps 0(0.00) | Grad Norm 43.0752(28.8454) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.5962(14.0759) | Bit/dim 5.2233(5.3241) | Xent 1.9639(1.9364) | Loss 24.1987(29.0584) | Error 0.6844(0.6737) Steps 0(0.00) | Grad Norm 34.2711(32.5363) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 14.1077(14.1377) | Bit/dim 5.1799(5.2953) | Xent 1.9301(1.9232) | Loss 24.4156(27.8353) | Error 0.6767(0.6698) Steps 0(0.00) | Grad Norm 27.2586(29.5930) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.9218(14.1730) | Bit/dim 5.1357(5.2596) | Xent 1.8982(1.9198) | Loss 24.5824(26.9653) | Error 0.6811(0.6692) Steps 0(0.00) | Grad Norm 8.9099(25.6986) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.7297(14.2446) | Bit/dim 5.1438(5.2283) | Xent 1.9174(1.9156) | Loss 24.5100(26.2354) | Error 0.6900(0.6698) Steps 0(0.00) | Grad Norm 53.1386(27.7402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 77.5509, Epoch Time 883.2115(803.0609), Bit/dim 5.1111(best: 5.2874), Xent 1.8384, Loss 6.0303, Error 0.6197(best: 0.6315)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.6110(14.3207) | Bit/dim 5.0929(5.2010) | Xent 1.8595(1.9082) | Loss 24.4355(31.1477) | Error 0.6311(0.6665) Steps 0(0.00) | Grad Norm 24.7115(29.4657) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.8313(14.4313) | Bit/dim 5.0726(5.1725) | Xent 1.8527(1.9082) | Loss 23.9653(29.3805) | Error 0.6578(0.6678) Steps 0(0.00) | Grad Norm 10.3756(29.3217) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.8243(14.4678) | Bit/dim 5.0768(5.1430) | Xent 1.9954(1.9106) | Loss 24.3265(27.9501) | Error 0.7122(0.6705) Steps 0(0.00) | Grad Norm 44.5446(32.3636) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 15.0589(14.4690) | Bit/dim 5.0489(5.1173) | Xent 1.8888(1.9034) | Loss 24.0886(26.8999) | Error 0.6589(0.6680) Steps 0(0.00) | Grad Norm 21.8716(32.0699) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.2328(14.6104) | Bit/dim 4.9979(5.0876) | Xent 1.8588(1.8937) | Loss 23.8590(26.0832) | Error 0.6411(0.6621) Steps 0(0.00) | Grad Norm 20.9363(30.3063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 79.4496, Epoch Time 905.7155(806.1405), Bit/dim 5.0008(best: 5.1111), Xent 1.9435, Loss 5.9725, Error 0.6886(best: 0.6197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.9270(14.6579) | Bit/dim 4.9511(5.0606) | Xent 1.8732(1.8978) | Loss 23.6142(31.7707) | Error 0.6522(0.6641) Steps 0(0.00) | Grad Norm 43.2386(34.3056) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.7636(14.7205) | Bit/dim 4.9305(5.0294) | Xent 1.8694(1.8954) | Loss 23.4191(29.6595) | Error 0.6433(0.6631) Steps 0(0.00) | Grad Norm 25.8289(31.5990) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.3956(14.7543) | Bit/dim 4.9318(5.0020) | Xent 1.9080(1.8922) | Loss 23.8498(28.0871) | Error 0.6589(0.6615) Steps 0(0.00) | Grad Norm 51.5953(30.7699) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 14.7695(14.8179) | Bit/dim 4.9140(4.9758) | Xent 1.8239(1.8785) | Loss 23.0220(26.9266) | Error 0.6656(0.6580) Steps 0(0.00) | Grad Norm 44.1030(30.0122) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.3384(14.8887) | Bit/dim 4.9013(4.9553) | Xent 2.0363(1.8891) | Loss 23.9036(26.0876) | Error 0.7333(0.6634) Steps 0(0.00) | Grad Norm 63.3507(36.3808) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 15.8307(14.9683) | Bit/dim 4.8060(4.9314) | Xent 1.8134(1.8858) | Loss 22.9393(25.4168) | Error 0.6500(0.6630) Steps 0(0.00) | Grad Norm 12.6319(34.3735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 80.2972, Epoch Time 921.4482(809.5998), Bit/dim 4.8394(best: 5.0008), Xent 1.8145, Loss 5.7466, Error 0.6281(best: 0.6197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 15.5810(15.0435) | Bit/dim 4.8466(4.9058) | Xent 1.7760(1.8686) | Loss 23.6125(30.4241) | Error 0.6378(0.6564) Steps 0(0.00) | Grad Norm 14.6227(30.4683) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.5313(15.0901) | Bit/dim 5.2625(4.9195) | Xent 1.9847(1.8727) | Loss 25.0098(28.7678) | Error 0.6856(0.6582) Steps 0(0.00) | Grad Norm 60.1673(33.1250) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.4059(15.0277) | Bit/dim 4.9711(4.9437) | Xent 1.9861(1.9227) | Loss 23.6273(27.5042) | Error 0.7078(0.6762) Steps 0(0.00) | Grad Norm 15.8118(34.0850) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 14.8474(15.0015) | Bit/dim 4.7914(4.9234) | Xent 1.9289(1.9286) | Loss 23.3633(26.4687) | Error 0.6856(0.6787) Steps 0(0.00) | Grad Norm 9.7663(27.9298) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 14.8740(14.9247) | Bit/dim 4.7688(4.8910) | Xent 1.8852(1.9198) | Loss 23.4536(25.5848) | Error 0.6844(0.6776) Steps 0(0.00) | Grad Norm 15.7843(23.4649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 78.5906, Epoch Time 920.2551(812.9194), Bit/dim 4.7627(best: 4.8394), Xent 1.7681, Loss 5.6467, Error 0.6053(best: 0.6197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.3519(14.9217) | Bit/dim 4.7995(4.8602) | Xent 1.7692(1.8932) | Loss 23.3762(30.9916) | Error 0.6211(0.6665) Steps 0(0.00) | Grad Norm 10.7012(21.4270) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.5172(15.0227) | Bit/dim 4.7551(4.8357) | Xent 1.7704(1.8636) | Loss 23.4846(28.9274) | Error 0.6222(0.6582) Steps 0(0.00) | Grad Norm 36.7931(22.7742) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 15.4975(15.1254) | Bit/dim 4.7241(4.8091) | Xent 1.7005(1.8426) | Loss 23.3009(27.4139) | Error 0.6089(0.6511) Steps 0(0.00) | Grad Norm 14.6499(23.5459) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.0300(15.2014) | Bit/dim 4.8741(4.7973) | Xent 1.8602(1.8317) | Loss 24.4405(26.3292) | Error 0.6300(0.6465) Steps 0(0.00) | Grad Norm 69.1211(28.4532) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.0748(15.2101) | Bit/dim 4.7695(4.7933) | Xent 1.8312(1.8321) | Loss 23.2236(25.5278) | Error 0.6500(0.6464) Steps 0(0.00) | Grad Norm 27.8305(31.1651) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 16.1022(15.2808) | Bit/dim 4.7153(4.7722) | Xent 1.7833(1.8183) | Loss 23.1902(24.9114) | Error 0.6244(0.6428) Steps 0(0.00) | Grad Norm 21.4899(28.1652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 81.3021, Epoch Time 942.5134(816.8073), Bit/dim 4.6897(best: 4.7627), Xent 1.7169, Loss 5.5482, Error 0.6073(best: 0.6053)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 16.0318(15.4015) | Bit/dim 4.7049(4.7483) | Xent 1.6719(1.7900) | Loss 23.1859(29.7688) | Error 0.5900(0.6344) Steps 0(0.00) | Grad Norm 10.6882(25.0661) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.4352(15.4707) | Bit/dim 4.6673(4.7306) | Xent 1.7506(1.7801) | Loss 22.4828(28.0449) | Error 0.6322(0.6325) Steps 0(0.00) | Grad Norm 27.3518(27.6055) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 16.0910(15.6837) | Bit/dim 4.6288(4.7091) | Xent 1.7579(1.7740) | Loss 23.0956(26.8054) | Error 0.6067(0.6293) Steps 0(0.00) | Grad Norm 25.2610(27.0623) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.9474(15.7768) | Bit/dim 4.6450(4.6944) | Xent 1.7823(1.7705) | Loss 23.9166(25.9271) | Error 0.6489(0.6287) Steps 0(0.00) | Grad Norm 34.6302(28.8377) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.6284(15.8696) | Bit/dim 4.6063(4.6788) | Xent 1.7242(1.7647) | Loss 23.5824(25.2171) | Error 0.6278(0.6283) Steps 0(0.00) | Grad Norm 24.3273(29.6431) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 85.2845, Epoch Time 980.9621(821.7319), Bit/dim 4.6104(best: 4.6897), Xent 1.6292, Loss 5.4250, Error 0.5794(best: 0.6053)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.1813(15.8723) | Bit/dim 4.5927(4.6646) | Xent 1.6965(1.7522) | Loss 24.1197(31.4896) | Error 0.6067(0.6241) Steps 0(0.00) | Grad Norm 29.3379(28.9588) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 15.7713(15.8861) | Bit/dim 4.6465(4.6509) | Xent 1.6983(1.7349) | Loss 23.7410(29.3375) | Error 0.6167(0.6178) Steps 0(0.00) | Grad Norm 26.9786(28.6239) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.5080(15.9178) | Bit/dim 4.6009(4.6375) | Xent 1.7522(1.7265) | Loss 23.4482(27.7481) | Error 0.6200(0.6156) Steps 0(0.00) | Grad Norm 42.1429(27.7690) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.4120(16.0245) | Bit/dim 4.6057(4.6323) | Xent 1.7723(1.7212) | Loss 23.7397(26.6712) | Error 0.6500(0.6143) Steps 0(0.00) | Grad Norm 26.9734(27.7618) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 16.1513(16.0292) | Bit/dim 4.5861(4.6211) | Xent 1.7438(1.7176) | Loss 23.7304(25.8558) | Error 0.6311(0.6137) Steps 0(0.00) | Grad Norm 24.5449(27.0999) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.7010(16.0075) | Bit/dim 4.6113(4.6084) | Xent 1.7345(1.7169) | Loss 23.6931(25.2272) | Error 0.6089(0.6149) Steps 0(0.00) | Grad Norm 41.5339(28.5822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 86.0685, Epoch Time 986.4585(826.6737), Bit/dim 4.5524(best: 4.6104), Xent 1.6519, Loss 5.3783, Error 0.5860(best: 0.5794)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 16.0580(15.9720) | Bit/dim 4.5214(4.5911) | Xent 1.6222(1.6994) | Loss 22.5069(30.4158) | Error 0.5756(0.6079) Steps 0(0.00) | Grad Norm 16.4003(27.2596) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.6833(16.0002) | Bit/dim 4.5383(4.5824) | Xent 1.6575(1.7127) | Loss 23.1923(28.5953) | Error 0.6067(0.6114) Steps 0(0.00) | Grad Norm 13.8973(30.2038) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 15.4316(15.9782) | Bit/dim 4.5195(4.5672) | Xent 1.6404(1.7018) | Loss 23.0346(27.1780) | Error 0.6033(0.6079) Steps 0(0.00) | Grad Norm 12.4511(25.6576) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 16.0547(16.0302) | Bit/dim 4.5437(4.5555) | Xent 1.6802(1.6960) | Loss 23.3881(26.1517) | Error 0.5933(0.6074) Steps 0(0.00) | Grad Norm 50.4420(26.5805) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 15.6937(16.0313) | Bit/dim 4.6144(4.5657) | Xent 1.7139(1.7006) | Loss 23.7320(25.4781) | Error 0.6078(0.6105) Steps 0(0.00) | Grad Norm 28.6286(27.4769) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 87.1672, Epoch Time 986.7827(831.4770), Bit/dim 4.5089(best: 4.5524), Xent 1.6262, Loss 5.3219, Error 0.5829(best: 0.5794)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 14.6655(16.0064) | Bit/dim 4.5283(4.5547) | Xent 1.6565(1.6893) | Loss 22.4551(31.5811) | Error 0.6044(0.6081) Steps 0(0.00) | Grad Norm 23.5530(25.4736) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.5966(15.9834) | Bit/dim 4.4886(4.5381) | Xent 1.6382(1.6743) | Loss 23.9731(29.3471) | Error 0.5811(0.6024) Steps 0(0.00) | Grad Norm 27.8587(24.3994) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 17.7378(16.0593) | Bit/dim 4.4564(4.5201) | Xent 1.6596(1.6673) | Loss 23.6543(27.7273) | Error 0.5767(0.6000) Steps 0(0.00) | Grad Norm 35.3787(24.5238) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.6780(16.0614) | Bit/dim 4.4212(4.5018) | Xent 1.5848(1.6565) | Loss 23.2365(26.4960) | Error 0.5411(0.5938) Steps 0(0.00) | Grad Norm 16.8493(23.9231) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 15.9060(16.0984) | Bit/dim 4.4667(4.4938) | Xent 1.6163(1.6531) | Loss 23.3123(25.6774) | Error 0.5922(0.5933) Steps 0(0.00) | Grad Norm 22.6361(24.1721) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.8053(16.1325) | Bit/dim 4.4137(4.4791) | Xent 1.6338(1.6522) | Loss 22.8816(24.9956) | Error 0.5944(0.5942) Steps 0(0.00) | Grad Norm 20.8061(23.5086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 87.0820, Epoch Time 990.7565(836.2554), Bit/dim 4.4353(best: 4.5089), Xent 1.5365, Loss 5.2035, Error 0.5545(best: 0.5794)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.8397(16.2061) | Bit/dim 4.4586(4.4691) | Xent 1.6540(1.6466) | Loss 23.6446(30.2319) | Error 0.6011(0.5918) Steps 0(0.00) | Grad Norm 20.3094(24.8775) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 16.1996(16.2427) | Bit/dim 4.4088(4.4600) | Xent 1.6379(1.6442) | Loss 22.4497(28.3540) | Error 0.5656(0.5905) Steps 0(0.00) | Grad Norm 20.6674(24.5479) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 16.7661(16.3515) | Bit/dim 4.4232(4.4598) | Xent 1.5768(1.6361) | Loss 22.5533(26.9632) | Error 0.5811(0.5891) Steps 0(0.00) | Grad Norm 13.8584(24.0058) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.6670(16.3860) | Bit/dim 4.4123(4.4488) | Xent 1.6193(1.6378) | Loss 23.4775(25.8969) | Error 0.5889(0.5902) Steps 0(0.00) | Grad Norm 12.8171(23.6478) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.3070(16.4366) | Bit/dim 4.4152(4.4381) | Xent 1.6266(1.6358) | Loss 23.3664(25.1546) | Error 0.5900(0.5882) Steps 0(0.00) | Grad Norm 16.0382(22.6062) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 86.4689, Epoch Time 1011.5441(841.5140), Bit/dim 4.4045(best: 4.4353), Xent 1.5345, Loss 5.1717, Error 0.5518(best: 0.5545)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.5394(16.3519) | Bit/dim 4.3895(4.4220) | Xent 1.5689(1.6226) | Loss 22.5133(31.2036) | Error 0.5567(0.5828) Steps 0(0.00) | Grad Norm 30.4999(22.0937) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.6960(16.3316) | Bit/dim 4.3608(4.4061) | Xent 1.5458(1.6082) | Loss 22.8970(29.0001) | Error 0.5644(0.5798) Steps 0(0.00) | Grad Norm 8.7497(20.2325) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.5115(16.3974) | Bit/dim 4.3527(4.3906) | Xent 1.5738(1.5991) | Loss 22.5265(27.3613) | Error 0.5433(0.5761) Steps 0(0.00) | Grad Norm 10.1689(19.6661) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.5591(16.3714) | Bit/dim 4.3412(4.3734) | Xent 1.6034(1.5956) | Loss 21.9668(26.1565) | Error 0.5967(0.5774) Steps 0(0.00) | Grad Norm 18.6146(19.0971) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.5193(16.4138) | Bit/dim 4.3256(4.3628) | Xent 1.5467(1.5816) | Loss 22.7424(25.2500) | Error 0.5444(0.5720) Steps 0(0.00) | Grad Norm 19.4398(18.5056) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 16.6323(16.4792) | Bit/dim 4.3140(4.3530) | Xent 1.5280(1.5822) | Loss 22.1842(24.5199) | Error 0.5422(0.5712) Steps 0(0.00) | Grad Norm 11.5368(19.3026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 87.0823, Epoch Time 1008.6798(846.5290), Bit/dim 4.3146(best: 4.4045), Xent 1.4824, Loss 5.0558, Error 0.5351(best: 0.5518)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.5397(16.5394) | Bit/dim 4.3591(4.3521) | Xent 1.6568(1.5852) | Loss 22.7441(29.7056) | Error 0.6278(0.5718) Steps 0(0.00) | Grad Norm 29.1854(21.8129) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.1808(16.6443) | Bit/dim 4.3071(4.3426) | Xent 1.5826(1.5856) | Loss 22.1099(27.7555) | Error 0.5822(0.5731) Steps 0(0.00) | Grad Norm 25.6519(21.7951) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 17.6113(16.6174) | Bit/dim 4.2641(4.3307) | Xent 1.5235(1.5722) | Loss 21.5977(26.3071) | Error 0.5478(0.5694) Steps 0(0.00) | Grad Norm 14.1256(20.0127) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 15.7084(16.5163) | Bit/dim 4.2698(4.3186) | Xent 1.5809(1.5595) | Loss 21.9041(25.2187) | Error 0.5656(0.5651) Steps 0(0.00) | Grad Norm 15.8759(18.4731) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.1307(16.4589) | Bit/dim 4.3344(4.3178) | Xent 1.6035(1.5662) | Loss 22.0155(24.4834) | Error 0.5889(0.5683) Steps 0(0.00) | Grad Norm 17.9015(20.7875) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 84.2374, Epoch Time 1010.2059(851.4393), Bit/dim 4.3038(best: 4.3146), Xent 1.4705, Loss 5.0390, Error 0.5299(best: 0.5351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 16.9059(16.3902) | Bit/dim 4.2399(4.3061) | Xent 1.4925(1.5583) | Loss 22.0772(30.4574) | Error 0.5511(0.5654) Steps 0(0.00) | Grad Norm 8.2662(18.6332) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 16.7365(16.3369) | Bit/dim 4.2437(4.2935) | Xent 1.4851(1.5386) | Loss 22.0690(28.2638) | Error 0.5700(0.5600) Steps 0(0.00) | Grad Norm 5.6524(16.2100) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.3411(16.3383) | Bit/dim 4.1985(4.2783) | Xent 1.5074(1.5322) | Loss 21.7853(26.6152) | Error 0.5489(0.5568) Steps 0(0.00) | Grad Norm 6.0793(17.0004) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 16.5338(16.4021) | Bit/dim 4.2538(4.2708) | Xent 1.5739(1.5237) | Loss 22.0418(25.3813) | Error 0.5344(0.5526) Steps 0(0.00) | Grad Norm 22.2715(17.0843) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 16.9474(16.3522) | Bit/dim 4.1837(4.2614) | Xent 1.5556(1.5346) | Loss 22.6192(24.6099) | Error 0.5600(0.5548) Steps 0(0.00) | Grad Norm 17.9179(19.5484) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 15.7793(16.2828) | Bit/dim 4.2061(4.2489) | Xent 1.4543(1.5225) | Loss 21.9573(23.9149) | Error 0.5356(0.5525) Steps 0(0.00) | Grad Norm 19.7463(18.0239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 84.1866, Epoch Time 997.8374(855.8312), Bit/dim 4.2213(best: 4.3038), Xent 1.4445, Loss 4.9435, Error 0.5183(best: 0.5299)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.8186(16.1903) | Bit/dim 4.1926(4.2355) | Xent 1.4825(1.5109) | Loss 22.0965(28.9902) | Error 0.5489(0.5472) Steps 0(0.00) | Grad Norm 9.2996(16.7933) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.5354(16.1550) | Bit/dim 4.2152(4.2289) | Xent 1.4867(1.5017) | Loss 21.7692(27.1376) | Error 0.5433(0.5437) Steps 0(0.00) | Grad Norm 21.2195(17.2528) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 16.3373(16.1680) | Bit/dim 4.2261(4.2277) | Xent 1.4031(1.4871) | Loss 22.2501(25.8228) | Error 0.4967(0.5380) Steps 0(0.00) | Grad Norm 16.3587(18.0822) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 16.0593(16.1091) | Bit/dim 4.2218(4.2199) | Xent 1.4267(1.4782) | Loss 21.5661(24.7960) | Error 0.5022(0.5356) Steps 0(0.00) | Grad Norm 10.5778(16.6166) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.2077(16.0587) | Bit/dim 4.1488(4.2089) | Xent 1.4125(1.4706) | Loss 21.6451(23.9384) | Error 0.5144(0.5338) Steps 0(0.00) | Grad Norm 9.9416(15.0217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 83.7840, Epoch Time 983.0006(859.6463), Bit/dim 4.1860(best: 4.2213), Xent 1.3568, Loss 4.8644, Error 0.4921(best: 0.5183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 15.8257(16.0373) | Bit/dim 4.1737(4.1997) | Xent 1.4297(1.4611) | Loss 21.8134(29.8079) | Error 0.5178(0.5308) Steps 0(0.00) | Grad Norm 21.8424(15.4055) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.6973(16.0452) | Bit/dim 4.1439(4.1914) | Xent 1.4315(1.4448) | Loss 20.8948(27.6988) | Error 0.5122(0.5238) Steps 0(0.00) | Grad Norm 13.6592(14.2890) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.0212(16.1155) | Bit/dim 4.2150(4.1932) | Xent 1.5426(1.4614) | Loss 22.2133(26.2450) | Error 0.5578(0.5268) Steps 0(0.00) | Grad Norm 29.5841(17.7938) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.4848(16.1327) | Bit/dim 4.1691(4.1868) | Xent 1.4733(1.4757) | Loss 22.0605(25.1930) | Error 0.5622(0.5338) Steps 0(0.00) | Grad Norm 8.5787(16.6739) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.9968(16.0605) | Bit/dim 4.1173(4.1715) | Xent 1.4436(1.4661) | Loss 21.2952(24.2495) | Error 0.5256(0.5317) Steps 0(0.00) | Grad Norm 8.2361(15.2296) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.9309(15.9376) | Bit/dim 4.1679(4.1613) | Xent 1.3912(1.4574) | Loss 21.9036(23.5501) | Error 0.5033(0.5286) Steps 0(0.00) | Grad Norm 5.6765(13.9327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 83.2831, Epoch Time 981.0960(863.2898), Bit/dim 4.1256(best: 4.1860), Xent 1.3198, Loss 4.7855, Error 0.4757(best: 0.4921)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.5197(15.9133) | Bit/dim 4.1470(4.1517) | Xent 1.3903(1.4332) | Loss 21.9547(28.5560) | Error 0.5044(0.5195) Steps 0(0.00) | Grad Norm 7.7610(12.4586) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 16.0288(15.8965) | Bit/dim 4.1141(4.1440) | Xent 1.3721(1.4246) | Loss 21.5505(26.7160) | Error 0.4978(0.5159) Steps 0(0.00) | Grad Norm 10.9345(13.5940) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.6271(15.8996) | Bit/dim 4.1204(4.1346) | Xent 1.4444(1.4218) | Loss 21.4415(25.2757) | Error 0.5178(0.5127) Steps 0(0.00) | Grad Norm 26.2621(14.6139) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.6968(15.9417) | Bit/dim 4.1361(4.1272) | Xent 1.3956(1.4210) | Loss 20.9377(24.3678) | Error 0.4967(0.5143) Steps 0(0.00) | Grad Norm 15.9413(14.9753) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 16.0759(15.9817) | Bit/dim 4.1112(4.1193) | Xent 1.3991(1.4072) | Loss 21.6857(23.6511) | Error 0.5167(0.5085) Steps 0(0.00) | Grad Norm 9.5039(14.0859) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 81.2832, Epoch Time 977.2352(866.7082), Bit/dim 4.1188(best: 4.1256), Xent 1.3758, Loss 4.8067, Error 0.4961(best: 0.4757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 16.4911(15.9858) | Bit/dim 4.1022(4.1169) | Xent 1.4859(1.4108) | Loss 21.8963(29.5756) | Error 0.5300(0.5102) Steps 0(0.00) | Grad Norm 12.3894(15.6134) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 16.1283(15.9694) | Bit/dim 4.0893(4.1100) | Xent 1.3768(1.4054) | Loss 21.1219(27.4681) | Error 0.4944(0.5077) Steps 0(0.00) | Grad Norm 16.2899(14.8116) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.5480(15.9845) | Bit/dim 4.1162(4.1069) | Xent 1.3286(1.3920) | Loss 21.7968(25.9105) | Error 0.4811(0.5026) Steps 0(0.00) | Grad Norm 9.1197(14.2166) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.7071(15.8759) | Bit/dim 4.0779(4.0984) | Xent 1.3272(1.3808) | Loss 21.6480(24.6926) | Error 0.5089(0.4993) Steps 0(0.00) | Grad Norm 10.0455(12.8374) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.7994(15.8708) | Bit/dim 4.1024(4.0891) | Xent 1.4569(1.3719) | Loss 21.5711(23.8123) | Error 0.5078(0.4953) Steps 0(0.00) | Grad Norm 24.2782(12.8076) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 15.3534(15.8582) | Bit/dim 4.0787(4.0855) | Xent 1.3077(1.3665) | Loss 21.3390(23.2029) | Error 0.4789(0.4949) Steps 0(0.00) | Grad Norm 8.0217(13.2960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 82.7508, Epoch Time 972.9724(869.8961), Bit/dim 4.0640(best: 4.1188), Xent 1.2975, Loss 4.7128, Error 0.4649(best: 0.4757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.5268(15.8293) | Bit/dim 4.0530(4.0787) | Xent 1.3645(1.3565) | Loss 20.8873(28.1822) | Error 0.4767(0.4900) Steps 0(0.00) | Grad Norm 23.5306(14.4821) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 15.4599(15.7470) | Bit/dim 4.0464(4.0697) | Xent 1.3656(1.3513) | Loss 22.1714(26.3344) | Error 0.4867(0.4885) Steps 0(0.00) | Grad Norm 10.8072(13.5407) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 15.8238(15.7003) | Bit/dim 4.0672(4.0706) | Xent 1.4291(1.3541) | Loss 21.3507(25.0240) | Error 0.5111(0.4893) Steps 0(0.00) | Grad Norm 21.8401(14.9293) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.8430(15.6370) | Bit/dim 4.0403(4.0627) | Xent 1.3407(1.3513) | Loss 21.1398(23.9829) | Error 0.4778(0.4880) Steps 0(0.00) | Grad Norm 8.6045(13.7114) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 15.7954(15.6293) | Bit/dim 4.0253(4.0586) | Xent 1.3293(1.3408) | Loss 21.3103(23.1780) | Error 0.4956(0.4865) Steps 0(0.00) | Grad Norm 7.4067(12.6152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 82.3022, Epoch Time 959.2590(872.5770), Bit/dim 4.0292(best: 4.0640), Xent 1.2402, Loss 4.6493, Error 0.4479(best: 0.4649)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.1729(15.6365) | Bit/dim 4.0116(4.0496) | Xent 1.2227(1.3321) | Loss 21.1332(29.0351) | Error 0.4644(0.4847) Steps 0(0.00) | Grad Norm 11.2365(12.7921) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 14.9410(15.6194) | Bit/dim 4.0179(4.0445) | Xent 1.2639(1.3229) | Loss 20.4045(26.8644) | Error 0.4611(0.4803) Steps 0(0.00) | Grad Norm 7.1155(12.4145) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 16.3096(15.7152) | Bit/dim 4.0295(4.0414) | Xent 1.3196(1.3178) | Loss 21.3063(25.3561) | Error 0.4633(0.4778) Steps 0(0.00) | Grad Norm 13.6616(12.4977) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 16.0359(15.7020) | Bit/dim 4.0639(4.0387) | Xent 1.3316(1.3196) | Loss 20.9976(24.2333) | Error 0.4889(0.4782) Steps 0(0.00) | Grad Norm 18.8677(13.4322) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.8677(15.7130) | Bit/dim 3.9795(4.0334) | Xent 1.2790(1.3184) | Loss 20.6878(23.3610) | Error 0.4456(0.4762) Steps 0(0.00) | Grad Norm 5.9606(13.2326) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 15.1976(15.6563) | Bit/dim 4.0314(4.0262) | Xent 1.2982(1.3166) | Loss 20.5200(22.6945) | Error 0.4533(0.4747) Steps 0(0.00) | Grad Norm 18.0121(12.9839) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 81.8904, Epoch Time 961.4824(875.2441), Bit/dim 4.0104(best: 4.0292), Xent 1.2343, Loss 4.6275, Error 0.4464(best: 0.4479)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.2359(15.6940) | Bit/dim 4.0191(4.0224) | Xent 1.2703(1.3072) | Loss 21.6086(27.6943) | Error 0.4633(0.4722) Steps 0(0.00) | Grad Norm 17.0019(12.9049) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 15.9568(15.7186) | Bit/dim 4.0245(4.0161) | Xent 1.3144(1.2980) | Loss 21.1324(25.9224) | Error 0.4833(0.4698) Steps 0(0.00) | Grad Norm 12.6141(11.9455) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 15.3133(15.6946) | Bit/dim 3.9712(4.0098) | Xent 1.2705(1.2918) | Loss 20.5891(24.5477) | Error 0.4789(0.4668) Steps 0(0.00) | Grad Norm 10.9275(11.5202) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 16.1888(15.7202) | Bit/dim 3.9965(4.0079) | Xent 1.2888(1.2908) | Loss 21.0793(23.5736) | Error 0.4844(0.4675) Steps 0(0.00) | Grad Norm 11.1542(12.1083) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 15.6320(15.7323) | Bit/dim 4.0089(4.0037) | Xent 1.3587(1.2882) | Loss 21.0087(22.8474) | Error 0.4878(0.4674) Steps 0(0.00) | Grad Norm 8.0942(11.3617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 83.6164, Epoch Time 968.2101(878.0331), Bit/dim 3.9834(best: 4.0104), Xent 1.2099, Loss 4.5883, Error 0.4378(best: 0.4464)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 15.4421(15.7473) | Bit/dim 3.9621(4.0023) | Xent 1.2666(1.2851) | Loss 20.8532(28.7921) | Error 0.4422(0.4657) Steps 0(0.00) | Grad Norm 23.1516(12.2652) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.1428(15.7313) | Bit/dim 3.9933(3.9997) | Xent 1.2942(1.2885) | Loss 20.9019(26.6797) | Error 0.4600(0.4667) Steps 0(0.00) | Grad Norm 17.5963(13.6885) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.6377(15.7215) | Bit/dim 4.0178(3.9979) | Xent 1.2351(1.2852) | Loss 20.7544(25.1097) | Error 0.4544(0.4655) Steps 0(0.00) | Grad Norm 7.7044(13.8141) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 15.3202(15.6564) | Bit/dim 3.9807(3.9960) | Xent 1.2955(1.2792) | Loss 20.5525(23.9381) | Error 0.4733(0.4648) Steps 0(0.00) | Grad Norm 11.7265(13.2886) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.0247(15.6893) | Bit/dim 3.9718(3.9907) | Xent 1.2965(1.2781) | Loss 21.2292(23.0461) | Error 0.4689(0.4629) Steps 0(0.00) | Grad Norm 10.2275(13.0493) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.1823(15.6793) | Bit/dim 3.9457(3.9837) | Xent 1.2335(1.2724) | Loss 19.7892(22.3486) | Error 0.4656(0.4606) Steps 0(0.00) | Grad Norm 5.8468(11.7551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 81.4928, Epoch Time 961.8883(880.5488), Bit/dim 3.9616(best: 3.9834), Xent 1.1984, Loss 4.5608, Error 0.4374(best: 0.4378)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.0178(15.7095) | Bit/dim 3.9785(3.9826) | Xent 1.2639(1.2691) | Loss 20.9089(27.2421) | Error 0.4700(0.4584) Steps 0(0.00) | Grad Norm 12.1266(12.2583) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.4368(15.7614) | Bit/dim 3.9525(3.9797) | Xent 1.3304(1.2664) | Loss 20.2997(25.5062) | Error 0.4733(0.4574) Steps 0(0.00) | Grad Norm 24.4270(13.0935) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.4162(15.7155) | Bit/dim 3.9667(3.9733) | Xent 1.2298(1.2616) | Loss 20.0063(24.1785) | Error 0.4311(0.4550) Steps 0(0.00) | Grad Norm 8.3926(12.1838) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 16.1482(15.7278) | Bit/dim 3.9658(3.9704) | Xent 1.2722(1.2594) | Loss 20.9402(23.2619) | Error 0.4656(0.4566) Steps 0(0.00) | Grad Norm 19.3613(13.2480) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.7388(15.7167) | Bit/dim 3.9693(3.9681) | Xent 1.2529(1.2604) | Loss 19.9844(22.5327) | Error 0.4322(0.4558) Steps 0(0.00) | Grad Norm 11.0321(13.3681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 80.3434, Epoch Time 962.5167(883.0078), Bit/dim 3.9516(best: 3.9616), Xent 1.1880, Loss 4.5456, Error 0.4320(best: 0.4374)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.9321(15.6184) | Bit/dim 3.9662(3.9648) | Xent 1.2423(1.2520) | Loss 20.4460(28.1008) | Error 0.4533(0.4535) Steps 0(0.00) | Grad Norm 8.8771(12.5317) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.0687(15.5851) | Bit/dim 3.9454(3.9585) | Xent 1.2336(1.2446) | Loss 20.7663(26.0589) | Error 0.4389(0.4484) Steps 0(0.00) | Grad Norm 10.5090(12.2672) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.3127(15.5615) | Bit/dim 3.9562(3.9546) | Xent 1.1623(1.2386) | Loss 20.1305(24.5508) | Error 0.4344(0.4476) Steps 0(0.00) | Grad Norm 10.0108(12.4536) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.2348(15.5300) | Bit/dim 3.9167(3.9490) | Xent 1.2708(1.2288) | Loss 20.4389(23.4328) | Error 0.4644(0.4442) Steps 0(0.00) | Grad Norm 9.2277(12.3123) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.4414(15.4878) | Bit/dim 3.9533(3.9505) | Xent 1.2948(1.2308) | Loss 20.4529(22.5645) | Error 0.4656(0.4442) Steps 0(0.00) | Grad Norm 15.3731(13.2445) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.8569(15.4417) | Bit/dim 3.9533(3.9454) | Xent 1.3013(1.2403) | Loss 20.6731(21.9455) | Error 0.4489(0.4481) Steps 0(0.00) | Grad Norm 22.3443(13.4848) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 80.2444, Epoch Time 946.9718(884.9267), Bit/dim 3.9383(best: 3.9516), Xent 1.1875, Loss 4.5321, Error 0.4256(best: 0.4320)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 14.9374(15.4300) | Bit/dim 3.9518(3.9447) | Xent 1.1974(1.2357) | Loss 20.2186(26.6166) | Error 0.4467(0.4475) Steps 0(0.00) | Grad Norm 20.7007(13.4296) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.9674(15.3650) | Bit/dim 3.9516(3.9421) | Xent 1.2466(1.2298) | Loss 20.4297(24.9338) | Error 0.4500(0.4443) Steps 0(0.00) | Grad Norm 14.1824(12.7827) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.3545(15.4389) | Bit/dim 3.9277(3.9396) | Xent 1.2016(1.2234) | Loss 20.0785(23.6370) | Error 0.4033(0.4393) Steps 0(0.00) | Grad Norm 16.5129(12.3966) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.2325(15.4354) | Bit/dim 3.9292(3.9385) | Xent 1.2153(1.2292) | Loss 19.6790(22.7705) | Error 0.4578(0.4422) Steps 0(0.00) | Grad Norm 9.2089(13.1616) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 15.1648(15.4400) | Bit/dim 3.9747(3.9351) | Xent 1.2471(1.2261) | Loss 20.5434(22.1006) | Error 0.4544(0.4412) Steps 0(0.00) | Grad Norm 17.8701(13.0624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 80.4416, Epoch Time 946.3652(886.7699), Bit/dim 3.9297(best: 3.9383), Xent 1.1507, Loss 4.5051, Error 0.4187(best: 0.4256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.6034(15.4286) | Bit/dim 3.9427(3.9320) | Xent 1.1939(1.2232) | Loss 21.2818(27.9125) | Error 0.4211(0.4401) Steps 0(0.00) | Grad Norm 11.5560(12.4431) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 15.3629(15.3939) | Bit/dim 3.9399(3.9295) | Xent 1.1283(1.2101) | Loss 20.1068(25.8633) | Error 0.3956(0.4357) Steps 0(0.00) | Grad Norm 10.2296(11.6324) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 15.1748(15.3860) | Bit/dim 3.9186(3.9252) | Xent 1.1397(1.2023) | Loss 20.0265(24.3885) | Error 0.4089(0.4356) Steps 0(0.00) | Grad Norm 12.6235(11.2119) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 16.1343(15.4147) | Bit/dim 3.8812(3.9211) | Xent 1.2124(1.2031) | Loss 20.4434(23.2997) | Error 0.4467(0.4348) Steps 0(0.00) | Grad Norm 13.6078(11.8705) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.3074(15.3580) | Bit/dim 3.9088(3.9181) | Xent 1.2059(1.1974) | Loss 19.6469(22.4505) | Error 0.4267(0.4322) Steps 0(0.00) | Grad Norm 18.9269(11.6208) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.2497(15.3117) | Bit/dim 3.9131(3.9158) | Xent 1.1599(1.1925) | Loss 19.5614(21.8210) | Error 0.4200(0.4304) Steps 0(0.00) | Grad Norm 8.6630(10.9253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 79.0189, Epoch Time 940.1164(888.3703), Bit/dim 3.9023(best: 3.9297), Xent 1.1461, Loss 4.4754, Error 0.4108(best: 0.4187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 15.2128(15.3460) | Bit/dim 3.9268(3.9174) | Xent 1.1706(1.1814) | Loss 19.9288(26.4798) | Error 0.4022(0.4259) Steps 0(0.00) | Grad Norm 7.1184(10.4404) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.7726(15.4043) | Bit/dim 3.8852(3.9110) | Xent 1.1620(1.1754) | Loss 19.9189(24.7831) | Error 0.4144(0.4231) Steps 0(0.00) | Grad Norm 5.7703(10.5072) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.5534(15.3957) | Bit/dim 3.8824(3.9085) | Xent 1.1846(1.1885) | Loss 20.2174(23.5517) | Error 0.4344(0.4279) Steps 0(0.00) | Grad Norm 13.9840(12.8010) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.9172(15.3490) | Bit/dim 3.8710(3.9086) | Xent 1.1421(1.1908) | Loss 19.8806(22.6315) | Error 0.4167(0.4302) Steps 0(0.00) | Grad Norm 22.6860(13.9998) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.6932(15.3722) | Bit/dim 3.9106(3.9077) | Xent 1.1576(1.1917) | Loss 19.6500(21.9574) | Error 0.3944(0.4285) Steps 0(0.00) | Grad Norm 11.0825(14.2527) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 79.4396, Epoch Time 943.2799(890.0176), Bit/dim 3.8962(best: 3.9023), Xent 1.1158, Loss 4.4541, Error 0.3986(best: 0.4108)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.0948(15.3355) | Bit/dim 3.9161(3.9082) | Xent 1.0798(1.1850) | Loss 19.9106(27.3611) | Error 0.3656(0.4250) Steps 0(0.00) | Grad Norm 7.8150(13.4010) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 16.0601(15.4190) | Bit/dim 3.8741(3.9076) | Xent 1.1747(1.1790) | Loss 20.4825(25.4759) | Error 0.4178(0.4229) Steps 0(0.00) | Grad Norm 15.8339(13.4952) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 16.5695(15.4831) | Bit/dim 3.8913(3.9046) | Xent 1.2032(1.1801) | Loss 20.1540(24.0573) | Error 0.4144(0.4227) Steps 0(0.00) | Grad Norm 21.8309(13.4458) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 15.7265(15.4805) | Bit/dim 3.9118(3.9018) | Xent 1.1940(1.1756) | Loss 19.9364(23.0179) | Error 0.4267(0.4215) Steps 0(0.00) | Grad Norm 12.2313(13.3351) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.3698(15.5309) | Bit/dim 3.8848(3.8979) | Xent 1.2086(1.1727) | Loss 20.3300(22.2520) | Error 0.4400(0.4198) Steps 0(0.00) | Grad Norm 22.4272(12.7303) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.3343(15.4792) | Bit/dim 3.8683(3.8945) | Xent 1.1974(1.1732) | Loss 20.0257(21.6548) | Error 0.4356(0.4203) Steps 0(0.00) | Grad Norm 12.5322(13.1468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 79.4649, Epoch Time 952.1504(891.8816), Bit/dim 3.8869(best: 3.8962), Xent 1.1126, Loss 4.4432, Error 0.3979(best: 0.3986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.0279(15.4691) | Bit/dim 3.9024(3.8937) | Xent 1.2089(1.1668) | Loss 20.1745(26.3109) | Error 0.4378(0.4190) Steps 0(0.00) | Grad Norm 21.5339(13.2318) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.6895(15.5041) | Bit/dim 3.8529(3.8884) | Xent 1.0715(1.1586) | Loss 19.5681(24.5940) | Error 0.3889(0.4149) Steps 0(0.00) | Grad Norm 7.8869(12.6931) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.0184(15.4690) | Bit/dim 3.8938(3.8883) | Xent 1.1010(1.1493) | Loss 19.3770(23.3325) | Error 0.4067(0.4127) Steps 0(0.00) | Grad Norm 13.7196(11.7383) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.4493(15.4570) | Bit/dim 3.8848(3.8850) | Xent 1.1278(1.1412) | Loss 19.7714(22.4310) | Error 0.4056(0.4101) Steps 0(0.00) | Grad Norm 10.3335(11.7169) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 14.8974(15.4492) | Bit/dim 3.8860(3.8859) | Xent 1.2990(1.1517) | Loss 19.8363(21.7557) | Error 0.4578(0.4134) Steps 0(0.00) | Grad Norm 10.6805(11.8582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 80.7511, Epoch Time 948.5549(893.5818), Bit/dim 3.8763(best: 3.8869), Xent 1.1105, Loss 4.4315, Error 0.3985(best: 0.3979)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.9601(15.4430) | Bit/dim 3.8592(3.8860) | Xent 1.1384(1.1531) | Loss 19.4831(27.3328) | Error 0.4022(0.4132) Steps 0(0.00) | Grad Norm 6.7947(12.1536) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 16.1894(15.4147) | Bit/dim 3.8807(3.8870) | Xent 1.1618(1.1463) | Loss 19.9003(25.3886) | Error 0.4200(0.4109) Steps 0(0.00) | Grad Norm 13.7351(12.3499) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.5452(15.4628) | Bit/dim 3.8901(3.8850) | Xent 1.1004(1.1390) | Loss 20.6878(23.9592) | Error 0.3911(0.4074) Steps 0(0.00) | Grad Norm 10.9989(12.2554) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.7380(15.4476) | Bit/dim 3.8612(3.8829) | Xent 1.1340(1.1386) | Loss 20.1539(22.9474) | Error 0.4133(0.4086) Steps 0(0.00) | Grad Norm 6.1902(11.6942) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.4738(15.4970) | Bit/dim 3.8333(3.8778) | Xent 1.1300(1.1231) | Loss 19.8825(22.1778) | Error 0.4033(0.4043) Steps 0(0.00) | Grad Norm 7.7622(11.0113) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.7614(15.5497) | Bit/dim 3.8349(3.8744) | Xent 1.0945(1.1221) | Loss 20.0985(21.5935) | Error 0.3867(0.4028) Steps 0(0.00) | Grad Norm 7.6701(11.2006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 80.7919, Epoch Time 953.0113(895.3646), Bit/dim 3.8677(best: 3.8763), Xent 1.0599, Loss 4.3977, Error 0.3832(best: 0.3979)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.0469(15.5387) | Bit/dim 3.8695(3.8707) | Xent 1.0797(1.1080) | Loss 20.1480(26.4663) | Error 0.3678(0.3975) Steps 0(0.00) | Grad Norm 9.7676(10.6031) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 15.2007(15.5208) | Bit/dim 3.8796(3.8669) | Xent 1.0494(1.1059) | Loss 20.3448(24.7312) | Error 0.3778(0.3962) Steps 0(0.00) | Grad Norm 9.7172(10.8795) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.5545(15.5556) | Bit/dim 3.9203(3.8683) | Xent 1.0749(1.0995) | Loss 20.0029(23.4645) | Error 0.3744(0.3942) Steps 0(0.00) | Grad Norm 10.2571(10.9497) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.8499(15.5934) | Bit/dim 3.8457(3.8666) | Xent 1.1252(1.0936) | Loss 19.7245(22.5417) | Error 0.4133(0.3913) Steps 0(0.00) | Grad Norm 9.7630(10.7707) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.8441(15.6289) | Bit/dim 3.8854(3.8677) | Xent 1.0761(1.0921) | Loss 20.2507(21.8907) | Error 0.3978(0.3915) Steps 0(0.00) | Grad Norm 12.2573(10.4467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 80.3620, Epoch Time 955.8272(897.1785), Bit/dim 3.8920(best: 3.8677), Xent 1.2366, Loss 4.5103, Error 0.4454(best: 0.3832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.6586(15.5888) | Bit/dim 3.8892(3.8725) | Xent 1.1439(1.1158) | Loss 20.3990(27.6360) | Error 0.3878(0.3973) Steps 0(0.00) | Grad Norm 14.0899(12.9579) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.6928(15.5391) | Bit/dim 3.8674(3.8719) | Xent 1.0976(1.1172) | Loss 20.9740(25.6519) | Error 0.3844(0.3980) Steps 0(0.00) | Grad Norm 9.1021(12.6533) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.2296(15.5167) | Bit/dim 3.8739(3.8709) | Xent 1.0785(1.1039) | Loss 20.7566(24.1891) | Error 0.3778(0.3942) Steps 0(0.00) | Grad Norm 6.7501(11.9035) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 16.0039(15.4956) | Bit/dim 3.8460(3.8656) | Xent 1.0197(1.0942) | Loss 20.0325(23.1182) | Error 0.3667(0.3908) Steps 0(0.00) | Grad Norm 8.1706(10.9948) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 15.2459(15.4982) | Bit/dim 3.8730(3.8631) | Xent 1.0760(1.0924) | Loss 19.4536(22.2240) | Error 0.3889(0.3897) Steps 0(0.00) | Grad Norm 15.6876(11.4149) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 14.7332(15.4525) | Bit/dim 3.8566(3.8583) | Xent 1.1298(1.0954) | Loss 20.0759(21.6833) | Error 0.4111(0.3898) Steps 0(0.00) | Grad Norm 7.8282(11.1928) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 80.6942, Epoch Time 948.1959(898.7090), Bit/dim 3.8484(best: 3.8677), Xent 1.0428, Loss 4.3698, Error 0.3721(best: 0.3832)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.7350(15.5730) | Bit/dim 3.8623(3.8589) | Xent 1.0367(1.0819) | Loss 20.1256(26.4767) | Error 0.3844(0.3865) Steps 0(0.00) | Grad Norm 6.7533(10.7561) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 15.8458(15.5674) | Bit/dim 3.8707(3.8546) | Xent 1.0636(1.0702) | Loss 20.3161(24.7744) | Error 0.3811(0.3825) Steps 0(0.00) | Grad Norm 5.4238(10.2905) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 15.0541(15.5416) | Bit/dim 3.8436(3.8521) | Xent 1.1028(1.0662) | Loss 19.5453(23.5146) | Error 0.3822(0.3800) Steps 0(0.00) | Grad Norm 16.9949(10.5634) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.6004(15.5284) | Bit/dim 3.8335(3.8481) | Xent 1.0020(1.0634) | Loss 19.8833(22.6200) | Error 0.3789(0.3799) Steps 0(0.00) | Grad Norm 8.6730(9.9160) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.7103(15.5167) | Bit/dim 3.8390(3.8464) | Xent 1.0777(1.0618) | Loss 20.0182(21.9032) | Error 0.3767(0.3788) Steps 0(0.00) | Grad Norm 15.2975(11.4104) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 79.1642, Epoch Time 952.9173(900.3353), Bit/dim 3.8550(best: 3.8484), Xent 1.1350, Loss 4.4225, Error 0.4037(best: 0.3721)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 16.0298(15.4754) | Bit/dim 3.8344(3.8464) | Xent 1.1519(1.0750) | Loss 20.4118(27.5305) | Error 0.4011(0.3843) Steps 0(0.00) | Grad Norm 11.9460(12.2143) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.5734(15.4788) | Bit/dim 3.8745(3.8500) | Xent 1.0540(1.0762) | Loss 19.6877(25.5818) | Error 0.3589(0.3832) Steps 0(0.00) | Grad Norm 12.0404(11.7492) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.1782(15.5338) | Bit/dim 3.8598(3.8483) | Xent 1.0383(1.0690) | Loss 20.6111(24.1555) | Error 0.3700(0.3811) Steps 0(0.00) | Grad Norm 18.2824(11.7032) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.7950(15.5810) | Bit/dim 3.8450(3.8461) | Xent 1.0054(1.0598) | Loss 19.9560(23.0962) | Error 0.3444(0.3775) Steps 0(0.00) | Grad Norm 7.7839(10.7568) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.9618(15.6461) | Bit/dim 3.8321(3.8434) | Xent 1.0339(1.0524) | Loss 20.7002(22.2927) | Error 0.3767(0.3759) Steps 0(0.00) | Grad Norm 8.5676(10.7802) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.6545(15.6368) | Bit/dim 3.8016(3.8388) | Xent 1.0812(1.0543) | Loss 20.7662(21.7565) | Error 0.3700(0.3756) Steps 0(0.00) | Grad Norm 13.1544(10.8497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 80.8833, Epoch Time 959.0294(902.0961), Bit/dim 3.8339(best: 3.8484), Xent 1.0463, Loss 4.3571, Error 0.3787(best: 0.3721)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.6905(15.6213) | Bit/dim 3.8215(3.8385) | Xent 1.0184(1.0581) | Loss 20.0012(26.5657) | Error 0.3678(0.3769) Steps 0(0.00) | Grad Norm 8.0740(11.4755) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.4689(15.6446) | Bit/dim 3.8402(3.8376) | Xent 1.0621(1.0515) | Loss 20.3449(24.8325) | Error 0.4000(0.3757) Steps 0(0.00) | Grad Norm 19.1679(11.8411) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.6481(15.6723) | Bit/dim 3.8504(3.8380) | Xent 0.9886(1.0519) | Loss 19.9105(23.5941) | Error 0.3378(0.3763) Steps 0(0.00) | Grad Norm 10.3096(12.2503) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.7730(15.5890) | Bit/dim 3.7917(3.8374) | Xent 1.0601(1.0505) | Loss 19.4060(22.6041) | Error 0.3756(0.3757) Steps 0(0.00) | Grad Norm 8.9765(12.2817) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.9557(15.6355) | Bit/dim 3.8255(3.8332) | Xent 1.0798(1.0506) | Loss 20.0783(21.9673) | Error 0.3867(0.3752) Steps 0(0.00) | Grad Norm 15.3486(12.4100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 80.1097, Epoch Time 957.7887(903.7669), Bit/dim 3.8311(best: 3.8339), Xent 0.9776, Loss 4.3199, Error 0.3513(best: 0.3721)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.3002(15.5889) | Bit/dim 3.8577(3.8348) | Xent 0.9707(1.0387) | Loss 19.9620(27.4621) | Error 0.3400(0.3712) Steps 0(0.00) | Grad Norm 13.3550(12.3892) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.6072(15.5149) | Bit/dim 3.8438(3.8325) | Xent 1.0431(1.0292) | Loss 20.0281(25.4670) | Error 0.3600(0.3674) Steps 0(0.00) | Grad Norm 12.0717(11.2328) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.9027(15.4741) | Bit/dim 3.8521(3.8279) | Xent 1.0198(1.0250) | Loss 20.2223(24.0522) | Error 0.3556(0.3644) Steps 0(0.00) | Grad Norm 14.6300(11.2160) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.7458(15.4797) | Bit/dim 3.8290(3.8262) | Xent 1.0041(1.0213) | Loss 20.7524(22.9752) | Error 0.3778(0.3641) Steps 0(0.00) | Grad Norm 7.1924(11.1248) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 14.3753(15.4513) | Bit/dim 3.8214(3.8273) | Xent 1.0251(1.0352) | Loss 19.5051(22.2370) | Error 0.3744(0.3695) Steps 0(0.00) | Grad Norm 10.0645(11.7983) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.4499(15.4598) | Bit/dim 3.8457(3.8289) | Xent 1.0476(1.0323) | Loss 19.8000(21.6713) | Error 0.3800(0.3687) Steps 0(0.00) | Grad Norm 13.0255(11.2826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 80.2381, Epoch Time 945.0850(905.0064), Bit/dim 3.8197(best: 3.8311), Xent 0.9658, Loss 4.3026, Error 0.3451(best: 0.3513)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.2211(15.4929) | Bit/dim 3.8232(3.8239) | Xent 1.0153(1.0242) | Loss 20.0982(26.3961) | Error 0.3511(0.3665) Steps 0(0.00) | Grad Norm 14.6259(11.0508) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 14.8213(15.5361) | Bit/dim 3.8133(3.8238) | Xent 1.0164(1.0188) | Loss 19.6613(24.7043) | Error 0.3689(0.3642) Steps 0(0.00) | Grad Norm 6.1823(10.6528) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.4168(15.4718) | Bit/dim 3.7899(3.8219) | Xent 1.0070(1.0092) | Loss 20.2389(23.4855) | Error 0.3522(0.3596) Steps 0(0.00) | Grad Norm 11.0345(10.0897) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 15.1442(15.4276) | Bit/dim 3.8322(3.8223) | Xent 1.1345(1.0210) | Loss 19.7825(22.5878) | Error 0.3900(0.3631) Steps 0(0.00) | Grad Norm 22.8911(11.3229) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.3696(15.4378) | Bit/dim 3.8309(3.8203) | Xent 1.0747(1.0359) | Loss 19.7020(21.9065) | Error 0.3878(0.3685) Steps 0(0.00) | Grad Norm 16.7582(12.2919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 81.2620, Epoch Time 949.8329(906.3512), Bit/dim 3.8218(best: 3.8197), Xent 0.9966, Loss 4.3201, Error 0.3569(best: 0.3451)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 16.3762(15.4703) | Bit/dim 3.8078(3.8229) | Xent 0.9850(1.0378) | Loss 20.1977(27.5380) | Error 0.3400(0.3683) Steps 0(0.00) | Grad Norm 7.9297(12.2982) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.5561(15.5269) | Bit/dim 3.8158(3.8196) | Xent 1.0302(1.0288) | Loss 19.8981(25.5702) | Error 0.3733(0.3654) Steps 0(0.00) | Grad Norm 11.3360(11.6494) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 16.1042(15.5325) | Bit/dim 3.8286(3.8179) | Xent 0.9546(1.0177) | Loss 20.5192(24.1044) | Error 0.3267(0.3617) Steps 0(0.00) | Grad Norm 7.1139(10.7388) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.6958(15.5139) | Bit/dim 3.8288(3.8197) | Xent 1.0265(1.0104) | Loss 19.8364(23.0119) | Error 0.3700(0.3602) Steps 0(0.00) | Grad Norm 6.2524(10.5042) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.6028(15.5373) | Bit/dim 3.8001(3.8133) | Xent 1.0048(1.0014) | Loss 19.8280(22.2066) | Error 0.3511(0.3564) Steps 0(0.00) | Grad Norm 11.2334(10.2756) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.2089(15.4935) | Bit/dim 3.8191(3.8138) | Xent 1.0400(1.0125) | Loss 20.1338(21.7322) | Error 0.3756(0.3595) Steps 0(0.00) | Grad Norm 8.1591(12.1272) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 81.7235, Epoch Time 954.5651(907.7976), Bit/dim 3.8342(best: 3.8197), Xent 1.0419, Loss 4.3551, Error 0.3694(best: 0.3451)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.9555(15.6004) | Bit/dim 3.8139(3.8170) | Xent 0.9867(1.0169) | Loss 19.6886(26.5227) | Error 0.3444(0.3618) Steps 0(0.00) | Grad Norm 10.2606(12.6256) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.5287(15.5663) | Bit/dim 3.8390(3.8167) | Xent 0.9758(1.0078) | Loss 20.1953(24.8103) | Error 0.3400(0.3582) Steps 0(0.00) | Grad Norm 8.0102(11.2790) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.0831(15.5868) | Bit/dim 3.8179(3.8130) | Xent 0.9565(0.9995) | Loss 19.7217(23.6000) | Error 0.3400(0.3555) Steps 0(0.00) | Grad Norm 7.5131(10.6644) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.5291(15.5685) | Bit/dim 3.8352(3.8088) | Xent 1.1157(1.0033) | Loss 20.6655(22.6570) | Error 0.4022(0.3572) Steps 0(0.00) | Grad Norm 13.7929(11.0548) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 15.5564(15.5402) | Bit/dim 3.7952(3.8104) | Xent 1.0884(1.0103) | Loss 20.0794(21.9430) | Error 0.3833(0.3598) Steps 0(0.00) | Grad Norm 13.3072(11.8002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 80.9942, Epoch Time 956.9853(909.2733), Bit/dim 3.8084(best: 3.8197), Xent 0.9644, Loss 4.2906, Error 0.3403(best: 0.3451)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 16.0253(15.5365) | Bit/dim 3.7913(3.8100) | Xent 0.9563(1.0072) | Loss 20.0767(27.9219) | Error 0.3522(0.3590) Steps 0(0.00) | Grad Norm 12.0065(11.5525) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 16.0683(15.4751) | Bit/dim 3.8260(3.8089) | Xent 0.9565(0.9959) | Loss 20.0012(25.8395) | Error 0.3611(0.3571) Steps 0(0.00) | Grad Norm 7.8379(10.8565) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 16.0782(15.4717) | Bit/dim 3.8019(3.8089) | Xent 0.9567(0.9849) | Loss 19.8172(24.3220) | Error 0.3411(0.3540) Steps 0(0.00) | Grad Norm 7.9122(10.1910) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.1777(15.4552) | Bit/dim 3.7839(3.8054) | Xent 0.9948(0.9782) | Loss 20.1797(23.1792) | Error 0.3656(0.3505) Steps 0(0.00) | Grad Norm 18.1821(10.0775) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.1323(15.3590) | Bit/dim 3.7332(3.8039) | Xent 1.0309(0.9823) | Loss 19.4331(22.3115) | Error 0.3722(0.3499) Steps 0(0.00) | Grad Norm 11.6701(10.6954) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 14.9665(15.3480) | Bit/dim 3.8028(3.8043) | Xent 0.9889(0.9787) | Loss 19.6789(21.7064) | Error 0.3422(0.3479) Steps 0(0.00) | Grad Norm 11.7404(10.3217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 79.3679, Epoch Time 941.1032(910.2282), Bit/dim 3.7983(best: 3.8084), Xent 0.9284, Loss 4.2625, Error 0.3341(best: 0.3403)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.3499(15.3510) | Bit/dim 3.8234(3.8027) | Xent 0.9306(0.9708) | Loss 19.3774(26.4816) | Error 0.3356(0.3459) Steps 0(0.00) | Grad Norm 15.6984(10.2003) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.6563(15.3451) | Bit/dim 3.7791(3.8014) | Xent 0.9279(0.9670) | Loss 19.7037(24.7521) | Error 0.3267(0.3465) Steps 0(0.00) | Grad Norm 15.7024(10.3028) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.3571(15.4167) | Bit/dim 3.8015(3.7994) | Xent 0.9263(0.9633) | Loss 19.9288(23.5537) | Error 0.3356(0.3446) Steps 0(0.00) | Grad Norm 8.8550(10.2323) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 15.4915(15.4400) | Bit/dim 3.7894(3.7976) | Xent 0.9016(0.9627) | Loss 20.4718(22.6419) | Error 0.3289(0.3440) Steps 0(0.00) | Grad Norm 8.4608(10.2822) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.3754(15.4098) | Bit/dim 3.7716(3.7966) | Xent 0.8179(0.9579) | Loss 20.2089(21.9219) | Error 0.2856(0.3414) Steps 0(0.00) | Grad Norm 4.7713(10.6254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 81.7813, Epoch Time 947.1235(911.3350), Bit/dim 3.7884(best: 3.7983), Xent 0.9296, Loss 4.2532, Error 0.3350(best: 0.3341)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.5269(15.3654) | Bit/dim 3.7696(3.7952) | Xent 0.9516(0.9577) | Loss 19.8771(27.6798) | Error 0.3478(0.3410) Steps 0(0.00) | Grad Norm 6.9416(10.5865) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.3800(15.4030) | Bit/dim 3.8176(3.7940) | Xent 0.9525(0.9522) | Loss 19.8007(25.6721) | Error 0.3167(0.3382) Steps 0(0.00) | Grad Norm 8.9013(10.1817) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.7038(15.3849) | Bit/dim 3.7744(3.7932) | Xent 0.9178(0.9587) | Loss 19.9043(24.2017) | Error 0.3344(0.3396) Steps 0(0.00) | Grad Norm 7.0258(11.3338) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 16.2232(15.3967) | Bit/dim 3.8224(3.7924) | Xent 0.9507(0.9573) | Loss 20.0681(23.0920) | Error 0.3378(0.3412) Steps 0(0.00) | Grad Norm 8.7915(11.3113) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.7337(15.4418) | Bit/dim 3.7926(3.7940) | Xent 1.0897(0.9645) | Loss 20.2992(22.2426) | Error 0.3844(0.3427) Steps 0(0.00) | Grad Norm 15.9295(11.4089) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.6711(15.4308) | Bit/dim 3.7785(3.7941) | Xent 0.9703(0.9707) | Loss 20.1130(21.6742) | Error 0.3333(0.3447) Steps 0(0.00) | Grad Norm 8.1164(11.6734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 81.6378, Epoch Time 948.6814(912.4554), Bit/dim 3.8120(best: 3.7884), Xent 0.9347, Loss 4.2793, Error 0.3341(best: 0.3341)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.3080(15.4770) | Bit/dim 3.7571(3.7937) | Xent 0.9383(0.9646) | Loss 20.4156(26.7450) | Error 0.3333(0.3438) Steps 0(0.00) | Grad Norm 11.4020(11.8194) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.3488(15.4417) | Bit/dim 3.7942(3.7940) | Xent 0.9234(0.9545) | Loss 19.6566(24.9898) | Error 0.3289(0.3413) Steps 0(0.00) | Grad Norm 12.1299(10.8792) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.9678(15.4115) | Bit/dim 3.7927(3.7904) | Xent 0.8945(0.9478) | Loss 20.5998(23.7422) | Error 0.3156(0.3400) Steps 0(0.00) | Grad Norm 6.1704(9.9042) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 15.3915(15.4514) | Bit/dim 3.7643(3.7905) | Xent 0.8930(0.9407) | Loss 20.2854(22.8343) | Error 0.3167(0.3371) Steps 0(0.00) | Grad Norm 9.3622(9.6917) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.6542(15.4043) | Bit/dim 3.8334(3.7908) | Xent 1.0088(0.9445) | Loss 20.7290(22.1140) | Error 0.3511(0.3369) Steps 0(0.00) | Grad Norm 16.8950(10.3578) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 82.5386, Epoch Time 948.6553(913.5414), Bit/dim 3.7871(best: 3.7884), Xent 0.9278, Loss 4.2510, Error 0.3296(best: 0.3341)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 14.5498(15.3548) | Bit/dim 3.8202(3.7877) | Xent 0.9330(0.9443) | Loss 19.9047(28.0183) | Error 0.3322(0.3360) Steps 0(0.00) | Grad Norm 16.1359(10.7413) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.4848(15.4179) | Bit/dim 3.7917(3.7877) | Xent 0.9923(0.9520) | Loss 20.2355(25.9680) | Error 0.3522(0.3378) Steps 0(0.00) | Grad Norm 18.7002(11.6899) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.2636(15.4071) | Bit/dim 3.7794(3.7857) | Xent 0.9567(0.9484) | Loss 20.5046(24.4497) | Error 0.3378(0.3371) Steps 0(0.00) | Grad Norm 11.5032(11.4984) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.2977(15.4375) | Bit/dim 3.8005(3.7893) | Xent 1.0885(0.9734) | Loss 20.0556(23.3299) | Error 0.3911(0.3465) Steps 0(0.00) | Grad Norm 12.5584(12.5280) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.7402(15.4943) | Bit/dim 3.7507(3.7877) | Xent 0.9571(0.9738) | Loss 20.2540(22.5189) | Error 0.3644(0.3474) Steps 0(0.00) | Grad Norm 9.3957(12.1760) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.9001(15.4164) | Bit/dim 3.7546(3.7851) | Xent 0.9005(0.9639) | Loss 20.1244(21.9040) | Error 0.3300(0.3429) Steps 0(0.00) | Grad Norm 6.9468(11.0554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 80.9187, Epoch Time 949.0516(914.6067), Bit/dim 3.7865(best: 3.7871), Xent 0.8958, Loss 4.2344, Error 0.3183(best: 0.3296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.8691(15.3588) | Bit/dim 3.7722(3.7833) | Xent 0.9126(0.9429) | Loss 19.5609(26.8844) | Error 0.3322(0.3352) Steps 0(0.00) | Grad Norm 7.3352(9.9217) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.2063(15.3177) | Bit/dim 3.7816(3.7790) | Xent 0.8938(0.9385) | Loss 19.7190(25.1090) | Error 0.3322(0.3351) Steps 0(0.00) | Grad Norm 5.6103(10.1199) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 15.7855(15.3748) | Bit/dim 3.8011(3.7800) | Xent 0.9479(0.9438) | Loss 20.2493(23.8088) | Error 0.3344(0.3377) Steps 0(0.00) | Grad Norm 21.3190(10.9124) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.1336(15.3563) | Bit/dim 3.7752(3.7822) | Xent 0.9881(0.9529) | Loss 19.9993(22.8458) | Error 0.3567(0.3413) Steps 0(0.00) | Grad Norm 11.1803(11.9173) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.3198(15.2958) | Bit/dim 3.7582(3.7824) | Xent 0.8921(0.9461) | Loss 20.5330(22.1302) | Error 0.3189(0.3384) Steps 0(0.00) | Grad Norm 6.1162(11.5263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 82.0049, Epoch Time 940.3050(915.3777), Bit/dim 3.7769(best: 3.7865), Xent 0.8928, Loss 4.2233, Error 0.3188(best: 0.3183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.1418(15.3101) | Bit/dim 3.7580(3.7811) | Xent 0.9106(0.9349) | Loss 19.9902(27.9841) | Error 0.3144(0.3343) Steps 0(0.00) | Grad Norm 14.9865(11.5072) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.6121(15.3457) | Bit/dim 3.7646(3.7784) | Xent 0.8708(0.9324) | Loss 19.2719(25.9138) | Error 0.3044(0.3328) Steps 0(0.00) | Grad Norm 10.8894(11.1200) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.3862(15.2879) | Bit/dim 3.7552(3.7768) | Xent 0.9356(0.9245) | Loss 19.5128(24.3468) | Error 0.3411(0.3306) Steps 0(0.00) | Grad Norm 4.6142(10.5832) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.6362(15.3104) | Bit/dim 3.7956(3.7791) | Xent 0.9301(0.9209) | Loss 20.4333(23.2867) | Error 0.3322(0.3298) Steps 0(0.00) | Grad Norm 15.5208(10.9448) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.4362(15.3479) | Bit/dim 3.7613(3.7782) | Xent 0.9593(0.9272) | Loss 20.0978(22.4653) | Error 0.3367(0.3306) Steps 0(0.00) | Grad Norm 10.5598(11.3279) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.5566(15.3749) | Bit/dim 3.7636(3.7748) | Xent 0.8900(0.9333) | Loss 19.4009(21.8547) | Error 0.3167(0.3326) Steps 0(0.00) | Grad Norm 10.3974(11.5635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 83.1410, Epoch Time 946.7803(916.3198), Bit/dim 3.7702(best: 3.7769), Xent 0.8973, Loss 4.2188, Error 0.3185(best: 0.3183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.6655(15.4286) | Bit/dim 3.7608(3.7733) | Xent 0.8975(0.9317) | Loss 20.2568(26.7866) | Error 0.3178(0.3311) Steps 0(0.00) | Grad Norm 6.8339(11.4222) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 15.3085(15.4587) | Bit/dim 3.7897(3.7747) | Xent 0.9705(0.9332) | Loss 19.3834(25.0471) | Error 0.3444(0.3306) Steps 0(0.00) | Grad Norm 11.4800(11.3088) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 14.7085(15.4274) | Bit/dim 3.7473(3.7726) | Xent 0.9470(0.9282) | Loss 20.3251(23.7203) | Error 0.3256(0.3293) Steps 0(0.00) | Grad Norm 7.5664(10.7568) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 14.5475(15.3915) | Bit/dim 3.7628(3.7709) | Xent 0.8265(0.9133) | Loss 19.9674(22.8016) | Error 0.2978(0.3250) Steps 0(0.00) | Grad Norm 15.3635(10.7721) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.2605(15.3656) | Bit/dim 3.7757(3.7706) | Xent 0.9034(0.9125) | Loss 20.1765(22.1364) | Error 0.3422(0.3257) Steps 0(0.00) | Grad Norm 6.1569(10.4736) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 83.3348, Epoch Time 949.8976(917.3271), Bit/dim 3.7745(best: 3.7702), Xent 0.9271, Loss 4.2380, Error 0.3259(best: 0.3183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 14.9001(15.3925) | Bit/dim 3.7355(3.7691) | Xent 0.9018(0.9060) | Loss 19.5403(27.9235) | Error 0.3156(0.3222) Steps 0(0.00) | Grad Norm 7.9870(10.8093) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 16.1363(15.4072) | Bit/dim 3.7191(3.7676) | Xent 0.8294(0.9036) | Loss 20.5046(25.9119) | Error 0.2933(0.3204) Steps 0(0.00) | Grad Norm 8.1109(10.3340) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.7989(15.3536) | Bit/dim 3.7249(3.7672) | Xent 0.8133(0.8986) | Loss 20.6973(24.3781) | Error 0.2978(0.3198) Steps 0(0.00) | Grad Norm 7.4532(10.5850) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.2494(15.3609) | Bit/dim 3.7585(3.7673) | Xent 0.9015(0.9060) | Loss 20.3184(23.3120) | Error 0.3233(0.3232) Steps 0(0.00) | Grad Norm 5.0275(10.9486) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.5498(15.3684) | Bit/dim 3.7763(3.7675) | Xent 0.9104(0.9061) | Loss 20.1682(22.5408) | Error 0.3067(0.3231) Steps 0(0.00) | Grad Norm 9.5502(11.1859) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.4705(15.3639) | Bit/dim 3.7455(3.7678) | Xent 0.9581(0.9129) | Loss 20.0840(21.9596) | Error 0.3489(0.3265) Steps 0(0.00) | Grad Norm 16.5688(11.8630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 84.0562, Epoch Time 947.4260(918.2301), Bit/dim 3.7665(best: 3.7702), Xent 0.9621, Loss 4.2475, Error 0.3400(best: 0.3183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.6099(15.3662) | Bit/dim 3.8015(3.7684) | Xent 0.9160(0.9188) | Loss 20.9980(26.9828) | Error 0.3244(0.3289) Steps 0(0.00) | Grad Norm 12.4670(12.4845) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.8026(15.3846) | Bit/dim 3.7687(3.7691) | Xent 0.8744(0.9092) | Loss 20.3537(25.1979) | Error 0.2978(0.3247) Steps 0(0.00) | Grad Norm 5.8788(11.2965) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.1780(15.3555) | Bit/dim 3.7995(3.7671) | Xent 0.9191(0.9080) | Loss 20.2802(23.8721) | Error 0.3411(0.3241) Steps 0(0.00) | Grad Norm 12.9889(10.7770) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.1277(15.3421) | Bit/dim 3.7785(3.7670) | Xent 0.8783(0.9074) | Loss 20.1055(22.9267) | Error 0.3122(0.3242) Steps 0(0.00) | Grad Norm 9.4805(10.6734) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.3405(15.3139) | Bit/dim 3.7604(3.7619) | Xent 0.9648(0.9006) | Loss 20.6992(22.2272) | Error 0.3422(0.3215) Steps 0(0.00) | Grad Norm 7.7032(9.9589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 84.2184, Epoch Time 945.1200(919.0368), Bit/dim 3.7642(best: 3.7665), Xent 0.8599, Loss 4.1941, Error 0.3050(best: 0.3183)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.1760(15.3239) | Bit/dim 3.7539(3.7642) | Xent 0.8284(0.8873) | Loss 20.0318(28.1426) | Error 0.3022(0.3160) Steps 0(0.00) | Grad Norm 7.7862(9.1279) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.5153(15.2985) | Bit/dim 3.7671(3.7644) | Xent 0.8914(0.8799) | Loss 20.6955(26.0910) | Error 0.3211(0.3132) Steps 0(0.00) | Grad Norm 8.1730(8.6100) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.4239(15.3245) | Bit/dim 3.7454(3.7602) | Xent 0.9284(0.8827) | Loss 20.1051(24.5536) | Error 0.3344(0.3140) Steps 0(0.00) | Grad Norm 14.3292(9.4489) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 14.7702(15.3034) | Bit/dim 3.7588(3.7608) | Xent 0.9416(0.8843) | Loss 20.5014(23.4143) | Error 0.3344(0.3151) Steps 0(0.00) | Grad Norm 7.5619(9.6966) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 15.9079(15.2392) | Bit/dim 3.7655(3.7589) | Xent 0.8790(0.8832) | Loss 20.0261(22.5675) | Error 0.3144(0.3150) Steps 0(0.00) | Grad Norm 9.5858(9.3508) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 16.0993(15.2801) | Bit/dim 3.7563(3.7567) | Xent 0.8649(0.8792) | Loss 20.0414(21.9138) | Error 0.3067(0.3141) Steps 0(0.00) | Grad Norm 9.2782(9.4297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 84.5626, Epoch Time 942.4847(919.7402), Bit/dim 3.7578(best: 3.7642), Xent 0.8638, Loss 4.1897, Error 0.3058(best: 0.3050)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 14.9577(15.3094) | Bit/dim 3.7502(3.7598) | Xent 0.8198(0.8693) | Loss 19.5999(27.0264) | Error 0.2922(0.3105) Steps 0(0.00) | Grad Norm 8.2193(9.4001) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.1193(15.2517) | Bit/dim 3.7345(3.7592) | Xent 0.8706(0.8740) | Loss 20.5374(25.2509) | Error 0.3111(0.3117) Steps 0(0.00) | Grad Norm 13.7734(9.9937) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 14.8915(15.2276) | Bit/dim 3.7753(3.7574) | Xent 0.8410(0.8781) | Loss 19.3592(23.8390) | Error 0.2922(0.3131) Steps 0(0.00) | Grad Norm 5.9483(9.8637) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.1043(15.2699) | Bit/dim 3.7147(3.7528) | Xent 0.8735(0.8692) | Loss 20.1929(22.8760) | Error 0.2933(0.3105) Steps 0(0.00) | Grad Norm 6.4706(9.5933) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 14.9223(15.2907) | Bit/dim 3.7773(3.7533) | Xent 0.9910(0.8850) | Loss 20.8371(22.2128) | Error 0.3378(0.3147) Steps 0(0.00) | Grad Norm 19.9940(11.0008) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 83.2733, Epoch Time 943.2767(920.4463), Bit/dim 3.7707(best: 3.7578), Xent 0.8949, Loss 4.2181, Error 0.3188(best: 0.3050)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 14.6789(15.3658) | Bit/dim 3.7765(3.7560) | Xent 0.9144(0.8906) | Loss 19.5889(28.0664) | Error 0.3200(0.3178) Steps 0(0.00) | Grad Norm 12.5334(11.4838) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 14.7148(15.3470) | Bit/dim 3.7677(3.7583) | Xent 0.7630(0.8787) | Loss 20.1205(26.0034) | Error 0.2900(0.3144) Steps 0(0.00) | Grad Norm 9.7469(10.8153) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.1177(15.3887) | Bit/dim 3.7592(3.7578) | Xent 0.8936(0.8781) | Loss 19.8671(24.4941) | Error 0.3122(0.3137) Steps 0(0.00) | Grad Norm 8.2997(10.3297) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 14.8586(15.4204) | Bit/dim 3.7376(3.7541) | Xent 0.8564(0.8661) | Loss 20.1085(23.3474) | Error 0.2933(0.3079) Steps 0(0.00) | Grad Norm 8.3083(9.7055) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.7585(15.3309) | Bit/dim 3.7710(3.7540) | Xent 0.8924(0.8645) | Loss 20.1110(22.4666) | Error 0.3300(0.3069) Steps 0(0.00) | Grad Norm 6.8131(9.5394) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.1925(15.2901) | Bit/dim 3.7358(3.7492) | Xent 0.8431(0.8702) | Loss 20.3355(21.8458) | Error 0.3067(0.3093) Steps 0(0.00) | Grad Norm 9.7576(9.9788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 83.8283, Epoch Time 946.2715(921.2210), Bit/dim 3.7487(best: 3.7578), Xent 0.8742, Loss 4.1858, Error 0.3081(best: 0.3050)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.7195(15.3400) | Bit/dim 3.7453(3.7495) | Xent 0.7991(0.8598) | Loss 19.5542(27.0446) | Error 0.2944(0.3064) Steps 0(0.00) | Grad Norm 8.3759(10.0760) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.9203(15.3730) | Bit/dim 3.7536(3.7468) | Xent 0.8178(0.8589) | Loss 20.7860(25.2356) | Error 0.2900(0.3057) Steps 0(0.00) | Grad Norm 11.5531(9.6668) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 16.1100(15.4141) | Bit/dim 3.7343(3.7438) | Xent 0.8235(0.8549) | Loss 20.6376(23.8535) | Error 0.2844(0.3042) Steps 0(0.00) | Grad Norm 9.4238(9.7229) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.4204(15.3928) | Bit/dim 3.7448(3.7425) | Xent 0.8108(0.8508) | Loss 20.5486(22.9010) | Error 0.2689(0.3030) Steps 0(0.00) | Grad Norm 9.7719(9.5072) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.3965(15.2973) | Bit/dim 3.7597(3.7448) | Xent 0.8362(0.8543) | Loss 20.0556(22.1982) | Error 0.2911(0.3033) Steps 0(0.00) | Grad Norm 5.1414(9.1867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 84.1538, Epoch Time 946.9347(921.9925), Bit/dim 3.7416(best: 3.7487), Xent 0.8907, Loss 4.1869, Error 0.3123(best: 0.3050)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 14.5035(15.2992) | Bit/dim 3.7293(3.7438) | Xent 0.9460(0.8669) | Loss 20.2836(28.0367) | Error 0.3389(0.3062) Steps 0(0.00) | Grad Norm 21.6408(10.7421) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.0523(15.2486) | Bit/dim 3.7379(3.7496) | Xent 0.8448(0.8623) | Loss 20.0567(25.9755) | Error 0.3211(0.3048) Steps 0(0.00) | Grad Norm 11.0234(10.8857) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.4802(15.1797) | Bit/dim 3.7354(3.7493) | Xent 0.8901(0.8563) | Loss 19.9823(24.4443) | Error 0.3233(0.3028) Steps 0(0.00) | Grad Norm 11.6631(10.6297) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.1742(15.2202) | Bit/dim 3.7714(3.7489) | Xent 0.9087(0.8556) | Loss 20.1213(23.3042) | Error 0.3156(0.3030) Steps 0(0.00) | Grad Norm 9.3353(10.5985) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.5291(15.2379) | Bit/dim 3.6889(3.7433) | Xent 0.8690(0.8509) | Loss 20.6579(22.5102) | Error 0.2967(0.3015) Steps 0(0.00) | Grad Norm 9.2653(10.0653) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.4545(15.1935) | Bit/dim 3.7399(3.7403) | Xent 0.8296(0.8481) | Loss 20.0755(21.9136) | Error 0.3056(0.3008) Steps 0(0.00) | Grad Norm 9.5487(9.7352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 83.6044, Epoch Time 936.0235(922.4134), Bit/dim 3.7490(best: 3.7416), Xent 0.8408, Loss 4.1694, Error 0.2986(best: 0.3050)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 14.5258(15.2129) | Bit/dim 3.7327(3.7406) | Xent 0.8786(0.8482) | Loss 20.7733(26.9547) | Error 0.3133(0.3008) Steps 0(0.00) | Grad Norm 8.4851(10.6556) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 14.9635(15.2465) | Bit/dim 3.7345(3.7384) | Xent 0.8347(0.8501) | Loss 20.7392(25.2340) | Error 0.2967(0.3013) Steps 0(0.00) | Grad Norm 10.3318(10.4282) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.4419(15.2322) | Bit/dim 3.7784(3.7445) | Xent 0.8418(0.8434) | Loss 20.0719(23.9426) | Error 0.3011(0.3012) Steps 0(0.00) | Grad Norm 11.9339(10.1845) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.0566(15.2689) | Bit/dim 3.7473(3.7429) | Xent 0.8377(0.8381) | Loss 20.0714(22.9773) | Error 0.2978(0.2986) Steps 0(0.00) | Grad Norm 9.0308(9.7243) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.3535(15.3244) | Bit/dim 3.7188(3.7413) | Xent 0.8504(0.8349) | Loss 20.5307(22.2411) | Error 0.3033(0.2976) Steps 0(0.00) | Grad Norm 5.8564(9.5668) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 83.8450, Epoch Time 944.4884(923.0756), Bit/dim 3.7387(best: 3.7416), Xent 0.8422, Loss 4.1598, Error 0.2993(best: 0.2986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.4705(15.3565) | Bit/dim 3.7242(3.7383) | Xent 0.7803(0.8268) | Loss 19.7266(28.1427) | Error 0.2744(0.2940) Steps 0(0.00) | Grad Norm 5.0786(9.3939) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 14.6767(15.3547) | Bit/dim 3.7205(3.7382) | Xent 0.8363(0.8230) | Loss 20.1488(26.1136) | Error 0.3044(0.2928) Steps 0(0.00) | Grad Norm 10.3929(9.3778) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.4885(15.3991) | Bit/dim 3.7190(3.7374) | Xent 0.9915(0.8360) | Loss 21.0382(24.6634) | Error 0.3544(0.2972) Steps 0(0.00) | Grad Norm 12.3130(10.3478) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 14.9301(15.3710) | Bit/dim 3.7356(3.7364) | Xent 0.8554(0.8408) | Loss 20.6661(23.5338) | Error 0.2944(0.2984) Steps 0(0.00) | Grad Norm 7.0579(10.2155) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 14.8987(15.3720) | Bit/dim 3.7079(3.7361) | Xent 0.8901(0.8401) | Loss 20.1133(22.7175) | Error 0.3244(0.2985) Steps 0(0.00) | Grad Norm 7.4870(9.8377) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.1776(15.3811) | Bit/dim 3.7777(3.7367) | Xent 0.7563(0.8326) | Loss 20.1721(22.1356) | Error 0.2689(0.2962) Steps 0(0.00) | Grad Norm 5.3247(9.1520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 84.3481, Epoch Time 950.4592(923.8971), Bit/dim 3.7375(best: 3.7387), Xent 0.8371, Loss 4.1560, Error 0.2967(best: 0.2986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 14.8354(15.3479) | Bit/dim 3.7093(3.7357) | Xent 0.8691(0.8326) | Loss 20.4495(27.3422) | Error 0.3078(0.2974) Steps 0(0.00) | Grad Norm 11.0074(9.4020) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.6254(15.3940) | Bit/dim 3.7449(3.7351) | Xent 0.7870(0.8261) | Loss 20.2592(25.4663) | Error 0.2922(0.2951) Steps 0(0.00) | Grad Norm 5.4042(9.4679) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.0859(15.3333) | Bit/dim 3.7468(3.7330) | Xent 0.8604(0.8285) | Loss 19.6325(24.0890) | Error 0.3089(0.2952) Steps 0(0.00) | Grad Norm 13.7382(9.7692) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 14.9111(15.3424) | Bit/dim 3.7005(3.7338) | Xent 0.9040(0.8395) | Loss 20.5408(23.1329) | Error 0.3289(0.3001) Steps 0(0.00) | Grad Norm 14.2792(10.7103) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 14.3701(15.3241) | Bit/dim 3.7563(3.7340) | Xent 0.9257(0.8380) | Loss 19.9478(22.4124) | Error 0.3322(0.2996) Steps 0(0.00) | Grad Norm 12.5914(10.5345) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 84.5067, Epoch Time 944.4022(924.5123), Bit/dim 3.7350(best: 3.7375), Xent 0.8324, Loss 4.1511, Error 0.2950(best: 0.2967)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.8383(15.3192) | Bit/dim 3.7637(3.7384) | Xent 0.7770(0.8329) | Loss 20.1610(28.3134) | Error 0.2767(0.2980) Steps 0(0.00) | Grad Norm 7.8655(10.5128) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 14.6582(15.3055) | Bit/dim 3.7156(3.7378) | Xent 0.7822(0.8323) | Loss 19.9753(26.1858) | Error 0.2767(0.2973) Steps 0(0.00) | Grad Norm 5.8629(10.5916) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.1148(15.3399) | Bit/dim 3.7074(3.7347) | Xent 0.8134(0.8261) | Loss 19.2572(24.6037) | Error 0.2822(0.2949) Steps 0(0.00) | Grad Norm 7.6973(10.0735) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.0062(15.3867) | Bit/dim 3.7546(3.7312) | Xent 0.8474(0.8282) | Loss 20.5981(23.4219) | Error 0.3089(0.2963) Steps 0(0.00) | Grad Norm 9.7510(9.9432) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.7727(15.3572) | Bit/dim 3.7461(3.7338) | Xent 0.8112(0.8242) | Loss 20.6169(22.6144) | Error 0.2822(0.2937) Steps 0(0.00) | Grad Norm 5.6532(9.1524) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.0762(15.3982) | Bit/dim 3.7353(3.7325) | Xent 0.8351(0.8207) | Loss 20.3877(21.9893) | Error 0.2856(0.2931) Steps 0(0.00) | Grad Norm 10.1721(9.4525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 85.0529, Epoch Time 950.7521(925.2995), Bit/dim 3.7224(best: 3.7350), Xent 0.8049, Loss 4.1248, Error 0.2853(best: 0.2950)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.2378(15.4051) | Bit/dim 3.7499(3.7306) | Xent 0.7951(0.8130) | Loss 20.1401(27.1848) | Error 0.2933(0.2911) Steps 0(0.00) | Grad Norm 7.5472(9.1634) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.4833(15.4086) | Bit/dim 3.7563(3.7289) | Xent 0.7724(0.8067) | Loss 20.1494(25.3973) | Error 0.2689(0.2883) Steps 0(0.00) | Grad Norm 9.3060(9.3148) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.4934(15.4500) | Bit/dim 3.7641(3.7296) | Xent 0.8694(0.8174) | Loss 20.1437(24.0838) | Error 0.3156(0.2923) Steps 0(0.00) | Grad Norm 11.7083(10.1030) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.5093(15.4555) | Bit/dim 3.7544(3.7295) | Xent 0.8812(0.8194) | Loss 20.7144(23.1214) | Error 0.2956(0.2921) Steps 0(0.00) | Grad Norm 8.0244(9.6660) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.3828(15.3970) | Bit/dim 3.7195(3.7265) | Xent 0.7777(0.8179) | Loss 20.3461(22.3844) | Error 0.2567(0.2910) Steps 0(0.00) | Grad Norm 12.2432(9.7316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 87.0845, Epoch Time 954.8177(926.1850), Bit/dim 3.7203(best: 3.7224), Xent 0.8515, Loss 4.1461, Error 0.3037(best: 0.2853)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.4311(15.4321) | Bit/dim 3.6757(3.7282) | Xent 0.8354(0.8232) | Loss 20.2918(28.3548) | Error 0.3011(0.2931) Steps 0(0.00) | Grad Norm 8.2787(10.9040) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.8797(15.5093) | Bit/dim 3.7160(3.7292) | Xent 0.8190(0.8189) | Loss 20.1936(26.2984) | Error 0.3011(0.2933) Steps 0(0.00) | Grad Norm 6.3359(10.3410) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.1080(15.4844) | Bit/dim 3.7366(3.7273) | Xent 0.7918(0.8108) | Loss 20.2791(24.7076) | Error 0.2789(0.2897) Steps 0(0.00) | Grad Norm 8.3829(10.2085) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.4266(15.4769) | Bit/dim 3.7145(3.7264) | Xent 0.7975(0.8126) | Loss 20.1198(23.5057) | Error 0.2822(0.2894) Steps 0(0.00) | Grad Norm 10.1361(10.1779) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.8977(15.4515) | Bit/dim 3.7429(3.7279) | Xent 0.7404(0.8041) | Loss 19.7248(22.6281) | Error 0.2633(0.2847) Steps 0(0.00) | Grad Norm 8.5720(9.7392) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.5053(15.4362) | Bit/dim 3.7339(3.7254) | Xent 0.7673(0.8021) | Loss 20.4589(22.0593) | Error 0.2622(0.2831) Steps 0(0.00) | Grad Norm 8.1824(9.9130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 85.6755, Epoch Time 954.8172(927.0440), Bit/dim 3.7230(best: 3.7203), Xent 0.7885, Loss 4.1172, Error 0.2749(best: 0.2853)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.7880(15.4241) | Bit/dim 3.7262(3.7216) | Xent 0.7167(0.7941) | Loss 20.6337(27.0203) | Error 0.2644(0.2827) Steps 0(0.00) | Grad Norm 10.4180(9.9222) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.6265(15.5159) | Bit/dim 3.7196(3.7226) | Xent 0.7161(0.7865) | Loss 20.5968(25.2869) | Error 0.2567(0.2796) Steps 0(0.00) | Grad Norm 5.0679(9.3691) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.5378(15.4852) | Bit/dim 3.7157(3.7208) | Xent 0.9121(0.7912) | Loss 20.5739(23.9547) | Error 0.3278(0.2822) Steps 0(0.00) | Grad Norm 18.2697(9.4677) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 15.0240(15.4692) | Bit/dim 3.7142(3.7215) | Xent 0.8198(0.7951) | Loss 20.6242(23.0271) | Error 0.2856(0.2840) Steps 0(0.00) | Grad Norm 13.3206(10.0650) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.7853(15.5618) | Bit/dim 3.7371(3.7236) | Xent 0.8393(0.7994) | Loss 21.0274(22.4122) | Error 0.2822(0.2838) Steps 0(0.00) | Grad Norm 12.5595(10.1474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 86.8428, Epoch Time 962.5330(928.1087), Bit/dim 3.7231(best: 3.7203), Xent 0.8023, Loss 4.1243, Error 0.2822(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.7564(15.5903) | Bit/dim 3.6898(3.7254) | Xent 0.7606(0.7970) | Loss 20.6093(28.6119) | Error 0.2800(0.2832) Steps 0(0.00) | Grad Norm 8.9527(9.9200) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.6184(15.5210) | Bit/dim 3.7262(3.7243) | Xent 0.7601(0.7823) | Loss 20.9530(26.3983) | Error 0.2811(0.2779) Steps 0(0.00) | Grad Norm 5.9628(9.2496) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.4252(15.5839) | Bit/dim 3.7387(3.7215) | Xent 0.7555(0.7803) | Loss 20.5118(24.7952) | Error 0.2633(0.2771) Steps 0(0.00) | Grad Norm 8.1993(8.9947) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.2257(15.5604) | Bit/dim 3.7294(3.7206) | Xent 0.7788(0.7800) | Loss 20.6617(23.6453) | Error 0.2767(0.2782) Steps 0(0.00) | Grad Norm 12.7389(9.3871) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 15.8697(15.5617) | Bit/dim 3.7191(3.7210) | Xent 0.8614(0.7887) | Loss 20.8702(22.8235) | Error 0.3144(0.2814) Steps 0(0.00) | Grad Norm 7.0719(9.3607) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.6448(15.5789) | Bit/dim 3.7299(3.7216) | Xent 0.8623(0.7937) | Loss 20.2658(22.1525) | Error 0.3200(0.2837) Steps 0(0.00) | Grad Norm 6.9278(8.9893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 86.8361, Epoch Time 960.4803(929.0798), Bit/dim 3.7205(best: 3.7203), Xent 0.7997, Loss 4.1203, Error 0.2828(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 14.5597(15.5206) | Bit/dim 3.7335(3.7212) | Xent 0.7272(0.7826) | Loss 20.1420(27.1739) | Error 0.2522(0.2785) Steps 0(0.00) | Grad Norm 6.8708(8.7447) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 15.1087(15.5314) | Bit/dim 3.7282(3.7188) | Xent 0.8750(0.7792) | Loss 20.1271(25.3270) | Error 0.3144(0.2801) Steps 0(0.00) | Grad Norm 14.5277(8.8252) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.5429(15.5578) | Bit/dim 3.7275(3.7192) | Xent 0.8214(0.7815) | Loss 20.0031(24.0300) | Error 0.2822(0.2795) Steps 0(0.00) | Grad Norm 10.2985(8.9244) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 15.4280(15.5075) | Bit/dim 3.6869(3.7172) | Xent 0.8267(0.7818) | Loss 20.5189(23.0946) | Error 0.3056(0.2791) Steps 0(0.00) | Grad Norm 9.7967(8.9487) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.0295(15.6274) | Bit/dim 3.7047(3.7150) | Xent 0.7473(0.7837) | Loss 20.7608(22.3831) | Error 0.2822(0.2797) Steps 0(0.00) | Grad Norm 7.4648(9.2684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 87.1969, Epoch Time 962.8445(930.0928), Bit/dim 3.7211(best: 3.7203), Xent 0.7901, Loss 4.1162, Error 0.2773(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.5483(15.6657) | Bit/dim 3.7457(3.7174) | Xent 0.8393(0.7812) | Loss 20.5385(28.6571) | Error 0.3044(0.2782) Steps 0(0.00) | Grad Norm 14.5593(9.4180) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 16.2152(15.7722) | Bit/dim 3.7107(3.7159) | Xent 0.7697(0.7824) | Loss 20.0415(26.5343) | Error 0.2589(0.2772) Steps 0(0.00) | Grad Norm 11.8452(9.3246) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.5781(15.7772) | Bit/dim 3.7236(3.7149) | Xent 0.7497(0.7788) | Loss 20.8169(25.0099) | Error 0.2756(0.2772) Steps 0(0.00) | Grad Norm 6.4580(9.0565) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.8559(15.7967) | Bit/dim 3.7206(3.7136) | Xent 0.7654(0.7725) | Loss 21.2660(23.8511) | Error 0.2622(0.2743) Steps 0(0.00) | Grad Norm 5.8749(8.4311) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.0001(15.7541) | Bit/dim 3.7120(3.7130) | Xent 0.8085(0.7771) | Loss 20.6288(23.0323) | Error 0.2822(0.2773) Steps 0(0.00) | Grad Norm 10.4850(9.0391) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.7165(15.6869) | Bit/dim 3.7388(3.7153) | Xent 0.7718(0.7797) | Loss 21.0787(22.4432) | Error 0.2578(0.2794) Steps 0(0.00) | Grad Norm 13.3531(10.0600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 86.1665, Epoch Time 970.9328(931.3180), Bit/dim 3.7242(best: 3.7203), Xent 0.8152, Loss 4.1318, Error 0.2924(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.0581(15.6919) | Bit/dim 3.6676(3.7170) | Xent 0.7108(0.7735) | Loss 20.4021(27.4472) | Error 0.2644(0.2785) Steps 0(0.00) | Grad Norm 6.5526(9.6872) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.4308(15.6458) | Bit/dim 3.6712(3.7162) | Xent 0.7207(0.7600) | Loss 20.7256(25.5847) | Error 0.2589(0.2733) Steps 0(0.00) | Grad Norm 8.6489(9.4189) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.5225(15.6836) | Bit/dim 3.7350(3.7156) | Xent 0.8198(0.7573) | Loss 20.5933(24.2699) | Error 0.2967(0.2729) Steps 0(0.00) | Grad Norm 10.4808(9.1547) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.9608(15.6449) | Bit/dim 3.6863(3.7140) | Xent 0.7864(0.7639) | Loss 20.7805(23.2832) | Error 0.2833(0.2740) Steps 0(0.00) | Grad Norm 5.1073(9.1867) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.4619(15.6831) | Bit/dim 3.7176(3.7104) | Xent 0.7622(0.7636) | Loss 20.9010(22.5721) | Error 0.2767(0.2727) Steps 0(0.00) | Grad Norm 8.6888(9.2795) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 87.4327, Epoch Time 968.4041(932.4305), Bit/dim 3.7142(best: 3.7203), Xent 0.7865, Loss 4.1075, Error 0.2782(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 14.4302(15.7066) | Bit/dim 3.7378(3.7104) | Xent 0.7650(0.7680) | Loss 20.3051(28.7440) | Error 0.2756(0.2753) Steps 0(0.00) | Grad Norm 10.2113(9.4412) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.9700(15.7401) | Bit/dim 3.7046(3.7078) | Xent 0.7417(0.7635) | Loss 20.9207(26.5820) | Error 0.2522(0.2732) Steps 0(0.00) | Grad Norm 6.5572(9.1747) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.6553(15.7158) | Bit/dim 3.7159(3.7096) | Xent 0.7174(0.7633) | Loss 20.8193(25.0318) | Error 0.2656(0.2735) Steps 0(0.00) | Grad Norm 11.0567(9.8174) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.5636(15.7149) | Bit/dim 3.7215(3.7127) | Xent 0.7365(0.7604) | Loss 20.5114(23.8338) | Error 0.2522(0.2714) Steps 0(0.00) | Grad Norm 7.3325(9.1931) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 14.8591(15.7262) | Bit/dim 3.7094(3.7115) | Xent 0.7664(0.7640) | Loss 20.4165(22.9662) | Error 0.2844(0.2721) Steps 0(0.00) | Grad Norm 4.5785(9.9717) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.4498(15.6690) | Bit/dim 3.7082(3.7079) | Xent 0.7790(0.7627) | Loss 20.3479(22.2512) | Error 0.2833(0.2711) Steps 0(0.00) | Grad Norm 14.9450(10.4314) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 88.4112, Epoch Time 969.6527(933.5472), Bit/dim 3.7074(best: 3.7142), Xent 0.7635, Loss 4.0891, Error 0.2679(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.8078(15.6938) | Bit/dim 3.6968(3.7104) | Xent 0.7353(0.7574) | Loss 20.9177(27.5181) | Error 0.2667(0.2705) Steps 0(0.00) | Grad Norm 10.8197(10.2475) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.6023(15.7609) | Bit/dim 3.6973(3.7078) | Xent 0.6693(0.7496) | Loss 20.1674(25.7100) | Error 0.2433(0.2680) Steps 0(0.00) | Grad Norm 5.6604(9.8734) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.1659(15.7656) | Bit/dim 3.7254(3.7069) | Xent 0.6925(0.7458) | Loss 21.3381(24.3632) | Error 0.2511(0.2664) Steps 0(0.00) | Grad Norm 6.5677(8.9684) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.9625(15.7937) | Bit/dim 3.6753(3.7027) | Xent 0.7865(0.7478) | Loss 21.6064(23.3838) | Error 0.2833(0.2653) Steps 0(0.00) | Grad Norm 8.9814(8.7983) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.9967(15.8134) | Bit/dim 3.7287(3.7025) | Xent 0.7469(0.7495) | Loss 19.9556(22.5889) | Error 0.2633(0.2660) Steps 0(0.00) | Grad Norm 10.8592(9.4322) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 87.0400, Epoch Time 976.4623(934.8347), Bit/dim 3.7071(best: 3.7074), Xent 0.7783, Loss 4.0963, Error 0.2747(best: 0.2679)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 17.4570(15.8748) | Bit/dim 3.7031(3.7057) | Xent 0.6600(0.7464) | Loss 21.2121(28.5560) | Error 0.2311(0.2657) Steps 0(0.00) | Grad Norm 6.9777(9.3070) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 15.9642(15.9178) | Bit/dim 3.7302(3.7049) | Xent 0.7087(0.7478) | Loss 20.3480(26.5081) | Error 0.2489(0.2659) Steps 0(0.00) | Grad Norm 6.6360(8.9434) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 15.7917(15.8697) | Bit/dim 3.6817(3.7028) | Xent 0.8793(0.7543) | Loss 21.3513(25.0004) | Error 0.3178(0.2690) Steps 0(0.00) | Grad Norm 15.8531(9.1968) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.4434(15.8599) | Bit/dim 3.7180(3.7046) | Xent 0.7663(0.7549) | Loss 21.0463(23.9005) | Error 0.2778(0.2703) Steps 0(0.00) | Grad Norm 8.8170(9.7249) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 14.9000(15.7939) | Bit/dim 3.6828(3.7045) | Xent 0.7661(0.7544) | Loss 20.8730(23.0278) | Error 0.2856(0.2704) Steps 0(0.00) | Grad Norm 10.2439(9.0684) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.1306(15.7828) | Bit/dim 3.7039(3.7051) | Xent 0.7464(0.7515) | Loss 20.3409(22.3396) | Error 0.2744(0.2690) Steps 0(0.00) | Grad Norm 7.5574(9.0278) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 87.2941, Epoch Time 976.3054(936.0788), Bit/dim 3.7068(best: 3.7071), Xent 0.7550, Loss 4.0843, Error 0.2656(best: 0.2679)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.6425(15.8712) | Bit/dim 3.7222(3.7037) | Xent 0.7586(0.7400) | Loss 21.3643(27.5667) | Error 0.2633(0.2644) Steps 0(0.00) | Grad Norm 15.5622(9.1527) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 14.9877(15.9238) | Bit/dim 3.7118(3.7040) | Xent 0.7543(0.7420) | Loss 20.5144(25.7573) | Error 0.2644(0.2646) Steps 0(0.00) | Grad Norm 12.8058(9.2467) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 15.9121(15.9243) | Bit/dim 3.7124(3.7042) | Xent 0.7047(0.7355) | Loss 20.9583(24.4188) | Error 0.2489(0.2623) Steps 0(0.00) | Grad Norm 7.9861(9.1528) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.4723(15.8787) | Bit/dim 3.6571(3.7023) | Xent 0.7379(0.7398) | Loss 21.0852(23.3923) | Error 0.2611(0.2633) Steps 0(0.00) | Grad Norm 6.3514(9.0157) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 15.7438(15.8531) | Bit/dim 3.6989(3.7012) | Xent 0.6616(0.7347) | Loss 20.5957(22.6498) | Error 0.2356(0.2612) Steps 0(0.00) | Grad Norm 6.8922(8.9117) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 89.0207, Epoch Time 982.7974(937.4803), Bit/dim 3.7184(best: 3.7068), Xent 0.8431, Loss 4.1400, Error 0.2965(best: 0.2656)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 15.8393(15.8499) | Bit/dim 3.6911(3.7034) | Xent 0.7011(0.7391) | Loss 20.4873(28.7993) | Error 0.2544(0.2634) Steps 0(0.00) | Grad Norm 7.1729(9.3868) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 15.3286(15.8174) | Bit/dim 3.7165(3.7046) | Xent 0.7426(0.7375) | Loss 20.5077(26.6349) | Error 0.2689(0.2625) Steps 0(0.00) | Grad Norm 9.5766(9.3842) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.3376(15.7703) | Bit/dim 3.6966(3.7020) | Xent 0.8034(0.7297) | Loss 20.4767(25.0244) | Error 0.2789(0.2605) Steps 0(0.00) | Grad Norm 9.7042(9.1683) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.4560(15.7184) | Bit/dim 3.7270(3.7002) | Xent 0.7123(0.7353) | Loss 20.4324(23.8520) | Error 0.2567(0.2627) Steps 0(0.00) | Grad Norm 6.9963(9.0692) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 14.9881(15.7053) | Bit/dim 3.7362(3.7014) | Xent 0.7510(0.7357) | Loss 20.1652(22.9974) | Error 0.2656(0.2633) Steps 0(0.00) | Grad Norm 6.3728(8.5315) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 14.8427(15.6900) | Bit/dim 3.7542(3.7035) | Xent 0.8361(0.7437) | Loss 21.1443(22.3398) | Error 0.2856(0.2653) Steps 0(0.00) | Grad Norm 13.0777(9.4432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 87.7918, Epoch Time 968.8217(938.4206), Bit/dim 3.7084(best: 3.7068), Xent 0.7522, Loss 4.0845, Error 0.2627(best: 0.2656)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.9880(15.7649) | Bit/dim 3.6671(3.7008) | Xent 0.7246(0.7359) | Loss 20.4217(27.6919) | Error 0.2711(0.2624) Steps 0(0.00) | Grad Norm 9.1123(9.2452) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.9464(15.7807) | Bit/dim 3.6811(3.7000) | Xent 0.7272(0.7335) | Loss 19.9776(25.7900) | Error 0.2678(0.2610) Steps 0(0.00) | Grad Norm 9.6148(9.5287) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.5730(15.8361) | Bit/dim 3.7332(3.7007) | Xent 0.7129(0.7282) | Loss 20.6190(24.4276) | Error 0.2433(0.2588) Steps 0(0.00) | Grad Norm 9.4496(9.5110) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.1696(15.8470) | Bit/dim 3.7153(3.6997) | Xent 0.7020(0.7323) | Loss 21.0424(23.4771) | Error 0.2444(0.2607) Steps 0(0.00) | Grad Norm 7.1928(9.3043) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.9731(15.8141) | Bit/dim 3.7102(3.7011) | Xent 0.6426(0.7276) | Loss 20.2393(22.6288) | Error 0.2322(0.2592) Steps 0(0.00) | Grad Norm 5.5445(8.8472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 88.7119, Epoch Time 981.4219(939.7106), Bit/dim 3.6992(best: 3.7068), Xent 0.7358, Loss 4.0671, Error 0.2556(best: 0.2627)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 15.8568(15.8929) | Bit/dim 3.7207(3.6998) | Xent 0.7030(0.7216) | Loss 20.8344(28.9250) | Error 0.2600(0.2582) Steps 0(0.00) | Grad Norm 9.9051(8.7053) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 15.7939(15.8424) | Bit/dim 3.7015(3.6987) | Xent 0.7304(0.7165) | Loss 20.2564(26.7356) | Error 0.2567(0.2563) Steps 0(0.00) | Grad Norm 8.1169(8.9167) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.6195(15.8017) | Bit/dim 3.6917(3.6977) | Xent 0.7815(0.7212) | Loss 20.4395(25.1327) | Error 0.2667(0.2567) Steps 0(0.00) | Grad Norm 7.3103(9.0663) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 14.9805(15.7821) | Bit/dim 3.6766(3.6981) | Xent 0.7256(0.7169) | Loss 20.9561(23.9778) | Error 0.2556(0.2549) Steps 0(0.00) | Grad Norm 9.2494(8.7919) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 16.0129(15.8271) | Bit/dim 3.6733(3.6973) | Xent 0.7022(0.7224) | Loss 20.3936(23.1032) | Error 0.2422(0.2557) Steps 0(0.00) | Grad Norm 7.0927(8.6519) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.0041(15.8464) | Bit/dim 3.7223(3.6975) | Xent 0.7101(0.7205) | Loss 21.4404(22.4861) | Error 0.2500(0.2554) Steps 0(0.00) | Grad Norm 7.4659(8.4601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 89.0726, Epoch Time 977.5779(940.8466), Bit/dim 3.6921(best: 3.6992), Xent 0.7553, Loss 4.0697, Error 0.2663(best: 0.2556)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.1105(15.8674) | Bit/dim 3.7118(3.6979) | Xent 0.7337(0.7142) | Loss 21.1088(27.7835) | Error 0.2700(0.2546) Steps 0(0.00) | Grad Norm 7.4535(8.7952) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.2196(15.8499) | Bit/dim 3.6771(3.6966) | Xent 0.6886(0.7222) | Loss 19.7335(25.8612) | Error 0.2433(0.2576) Steps 0(0.00) | Grad Norm 11.6050(9.1253) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.2741(15.9197) | Bit/dim 3.7409(3.6970) | Xent 0.7625(0.7272) | Loss 20.2056(24.4733) | Error 0.2589(0.2592) Steps 0(0.00) | Grad Norm 10.5648(9.0451) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.6938(15.8633) | Bit/dim 3.6967(3.6945) | Xent 0.6779(0.7245) | Loss 20.3352(23.4235) | Error 0.2422(0.2585) Steps 0(0.00) | Grad Norm 4.9701(9.3754) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 15.6515(15.9212) | Bit/dim 3.7004(3.6958) | Xent 0.6832(0.7141) | Loss 20.7057(22.6525) | Error 0.2411(0.2544) Steps 0(0.00) | Grad Norm 8.8717(8.8630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 88.8403, Epoch Time 982.8558(942.1069), Bit/dim 3.6948(best: 3.6921), Xent 0.7439, Loss 4.0668, Error 0.2620(best: 0.2556)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 15.9350(15.9762) | Bit/dim 3.6403(3.6924) | Xent 0.6734(0.7075) | Loss 20.8065(29.0523) | Error 0.2533(0.2523) Steps 0(0.00) | Grad Norm 5.9507(8.4901) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.1183(16.0322) | Bit/dim 3.6961(3.6934) | Xent 0.7495(0.7013) | Loss 21.5898(26.8962) | Error 0.2700(0.2501) Steps 0(0.00) | Grad Norm 10.8187(8.4584) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.2427(15.9965) | Bit/dim 3.7308(3.6949) | Xent 0.7720(0.7019) | Loss 20.4229(25.2756) | Error 0.2800(0.2496) Steps 0(0.00) | Grad Norm 9.2407(8.5139) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
