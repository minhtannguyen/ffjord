{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_25_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1536, bias=True)\n",
      "  (project_class): LinearZeros(in_features=768, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1386550\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.2912(29.7358) | Bit/dim 8.9848(9.1918) | Xent 2.3026(2.3026) | Loss 8.9848(9.1918) | Error 0.9011(0.8978) Steps 574(574.00) | Grad Norm 12.7955(17.0606) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.2453(25.3908) | Bit/dim 8.5526(9.0745) | Xent 2.3026(2.3026) | Loss 8.5526(9.0745) | Error 0.9022(0.8990) Steps 574(574.00) | Grad Norm 4.6971(14.6543) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 13.7367(22.2441) | Bit/dim 8.3911(8.9198) | Xent 2.3026(2.3026) | Loss 8.3911(8.9198) | Error 0.9044(0.8992) Steps 574(574.00) | Grad Norm 3.4688(11.8032) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.6255(19.9659) | Bit/dim 8.2559(8.7508) | Xent 2.3026(2.3026) | Loss 8.2559(8.7508) | Error 0.9200(0.9007) Steps 574(574.00) | Grad Norm 2.9509(9.5160) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.2163(18.2469) | Bit/dim 7.9393(8.5738) | Xent 2.3026(2.3026) | Loss 7.9393(8.5738) | Error 0.9144(0.8999) Steps 574(574.00) | Grad Norm 2.5741(7.7586) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 84.0392, Epoch Time 857.2743(857.2743), Bit/dim 7.7873(best: inf), Xent 2.3026, Loss 7.7873, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.8139(17.0167) | Bit/dim 7.7034(8.3730) | Xent 2.3026(2.3026) | Loss 7.7034(8.3730) | Error 0.9000(0.9000) Steps 574(574.00) | Grad Norm 2.7013(6.4135) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.7347(16.0478) | Bit/dim 7.3746(8.1412) | Xent 2.3026(2.3026) | Loss 7.3746(8.1412) | Error 0.9200(0.9013) Steps 574(574.00) | Grad Norm 2.2047(5.3375) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 13.6811(15.3857) | Bit/dim 7.1545(7.9057) | Xent 2.3026(2.3026) | Loss 7.1545(7.9057) | Error 0.8833(0.9000) Steps 574(574.00) | Grad Norm 1.2279(4.3742) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.8080(15.0355) | Bit/dim 7.0670(7.6938) | Xent 2.3026(2.3026) | Loss 7.0670(7.6938) | Error 0.8967(0.8993) Steps 592(577.72) | Grad Norm 0.9125(3.5042) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.4332(15.1610) | Bit/dim 7.0117(7.5196) | Xent 2.3026(2.3026) | Loss 7.0117(7.5196) | Error 0.9022(0.9007) Steps 610(585.64) | Grad Norm 0.4909(2.7523) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.9818(15.3793) | Bit/dim 6.9900(7.3824) | Xent 2.3026(2.3026) | Loss 6.9900(7.3824) | Error 0.8889(0.9002) Steps 628(593.72) | Grad Norm 0.5148(2.1596) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 82.0807, Epoch Time 892.2138(858.3225), Bit/dim 6.9750(best: 7.7873), Xent 2.3026, Loss 6.9750, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 15.9341(15.4872) | Bit/dim 6.9448(7.2720) | Xent 2.3026(2.3026) | Loss 6.9448(7.2720) | Error 0.8978(0.8999) Steps 628(602.72) | Grad Norm 0.5402(1.7071) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.1778(15.6049) | Bit/dim 6.8703(7.1775) | Xent 2.3026(2.3026) | Loss 6.8703(7.1775) | Error 0.8944(0.9011) Steps 628(609.36) | Grad Norm 0.4382(1.3532) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 16.3686(15.7944) | Bit/dim 6.8298(7.0926) | Xent 2.3026(2.3026) | Loss 6.8298(7.0926) | Error 0.9056(0.9015) Steps 634(615.40) | Grad Norm 0.4292(1.1234) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.2571(15.9579) | Bit/dim 6.7497(7.0109) | Xent 2.3026(2.3026) | Loss 6.7497(7.0109) | Error 0.8956(0.8998) Steps 640(620.81) | Grad Norm 0.6575(0.9860) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 17.3552(16.1253) | Bit/dim 6.5794(6.9174) | Xent 2.3026(2.3026) | Loss 6.5794(6.9174) | Error 0.9022(0.8991) Steps 640(625.85) | Grad Norm 7.0066(1.3671) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 83.8935, Epoch Time 995.2491(862.4303), Bit/dim 6.4686(best: 6.9750), Xent 2.3026, Loss 6.4686, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 16.2958(16.2245) | Bit/dim 6.3597(6.8032) | Xent 2.3026(2.3026) | Loss 6.3597(6.8032) | Error 0.9089(0.8999) Steps 640(629.56) | Grad Norm 8.8809(4.4874) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 16.4702(16.3239) | Bit/dim 6.2105(6.6649) | Xent 2.3026(2.3026) | Loss 6.2105(6.6649) | Error 0.8933(0.8992) Steps 640(632.30) | Grad Norm 28.6278(8.8337) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 18.1505(16.6015) | Bit/dim 5.9439(6.5067) | Xent 2.3026(2.3026) | Loss 5.9439(6.5067) | Error 0.8911(0.9000) Steps 670(638.84) | Grad Norm 5.6129(11.5284) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.2692(16.8835) | Bit/dim 5.8333(6.3465) | Xent 2.3026(2.3026) | Loss 5.8333(6.3465) | Error 0.8967(0.8996) Steps 670(646.36) | Grad Norm 14.2425(12.9253) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 16.8284(16.9345) | Bit/dim 5.7038(6.1936) | Xent 2.3026(2.3026) | Loss 5.7038(6.1936) | Error 0.9011(0.8994) Steps 658(649.68) | Grad Norm 1.9568(11.6841) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 18.3516(17.0641) | Bit/dim 5.6769(6.0599) | Xent 2.3026(2.3026) | Loss 5.6769(6.0599) | Error 0.9167(0.9002) Steps 676(654.47) | Grad Norm 9.6261(10.4800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 88.3839, Epoch Time 1049.9851(868.0569), Bit/dim 5.6548(best: 6.4686), Xent 2.3026, Loss 5.6548, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 18.1023(17.2131) | Bit/dim 5.6221(5.9490) | Xent 2.3026(2.3026) | Loss 5.6221(5.9490) | Error 0.9011(0.8994) Steps 664(658.66) | Grad Norm 9.3898(10.6119) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 18.2269(17.3108) | Bit/dim 5.6354(5.8571) | Xent 2.3026(2.3026) | Loss 5.6354(5.8571) | Error 0.8878(0.8984) Steps 664(662.54) | Grad Norm 2.2737(9.0055) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.8574(17.4454) | Bit/dim 5.5859(5.7843) | Xent 2.3026(2.3026) | Loss 5.5859(5.7843) | Error 0.8933(0.8979) Steps 670(666.52) | Grad Norm 20.9540(8.7794) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.7491(17.3802) | Bit/dim 5.5751(5.7284) | Xent 2.3026(2.3026) | Loss 5.5751(5.7284) | Error 0.8933(0.8994) Steps 664(666.79) | Grad Norm 10.0108(10.2107) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.9477(17.1942) | Bit/dim 5.5047(5.6825) | Xent 2.3026(2.3026) | Loss 5.5047(5.6825) | Error 0.8900(0.8999) Steps 664(665.46) | Grad Norm 5.5881(8.8702) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 87.2035, Epoch Time 1059.7442(873.8076), Bit/dim 5.5181(best: 5.6548), Xent 2.3026, Loss 5.5181, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 17.5189(17.2082) | Bit/dim 5.5107(5.6382) | Xent 2.3026(2.3026) | Loss 5.5107(5.6382) | Error 0.8911(0.9017) Steps 670(666.12) | Grad Norm 2.3168(7.8232) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.7875(17.2556) | Bit/dim 5.4751(5.6014) | Xent 2.3026(2.3026) | Loss 5.4751(5.6014) | Error 0.9078(0.9019) Steps 676(668.12) | Grad Norm 4.7146(7.3421) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 17.5675(17.3277) | Bit/dim 5.4599(5.5669) | Xent 2.3026(2.3026) | Loss 5.4599(5.5669) | Error 0.8911(0.9012) Steps 676(669.88) | Grad Norm 7.3108(6.9087) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.1343(17.3098) | Bit/dim 5.4588(5.5346) | Xent 2.3026(2.3026) | Loss 5.4588(5.5346) | Error 0.8922(0.9013) Steps 676(671.17) | Grad Norm 5.1121(6.4962) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 17.8248(17.3579) | Bit/dim 5.4217(5.4997) | Xent 2.3026(2.3026) | Loss 5.4217(5.4997) | Error 0.8911(0.9003) Steps 688(673.14) | Grad Norm 15.3681(7.3006) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.3391(17.2997) | Bit/dim 5.3071(5.4606) | Xent 2.3026(2.3026) | Loss 5.3071(5.4606) | Error 0.9144(0.8997) Steps 676(674.40) | Grad Norm 8.2362(8.3708) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 88.0790, Epoch Time 1060.6404(879.4125), Bit/dim 5.4253(best: 5.5181), Xent 2.3026, Loss 5.4253, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 16.8292(17.2317) | Bit/dim 5.3651(5.4287) | Xent 2.3026(2.3026) | Loss 5.3651(5.4287) | Error 0.9144(0.9005) Steps 676(675.49) | Grad Norm 13.0299(10.1824) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 16.9304(17.1353) | Bit/dim 5.2781(5.3914) | Xent 2.3026(2.3026) | Loss 5.2781(5.3914) | Error 0.8900(0.9003) Steps 682(677.26) | Grad Norm 11.5695(9.7429) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 17.5658(17.1972) | Bit/dim 5.1940(5.3451) | Xent 2.3026(2.3026) | Loss 5.1940(5.3451) | Error 0.9167(0.8998) Steps 688(678.26) | Grad Norm 2.1243(8.8155) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 17.7781(17.3039) | Bit/dim 5.1180(5.2956) | Xent 2.3026(2.3026) | Loss 5.1180(5.2956) | Error 0.8811(0.8998) Steps 700(681.66) | Grad Norm 6.5297(8.5355) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.0377(17.2526) | Bit/dim 5.0399(5.2493) | Xent 2.3026(2.3026) | Loss 5.0399(5.2493) | Error 0.9033(0.8996) Steps 688(683.56) | Grad Norm 4.8220(7.7077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 91.3583, Epoch Time 1055.4683(884.6942), Bit/dim 5.0963(best: 5.4253), Xent 2.3026, Loss 5.0963, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 17.9242(17.2746) | Bit/dim 5.1089(5.2095) | Xent 2.3026(2.3026) | Loss 5.1089(5.2095) | Error 0.8978(0.8997) Steps 694(685.39) | Grad Norm 4.1803(8.0137) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.8217(17.2102) | Bit/dim 5.0113(5.1662) | Xent 2.3026(2.3026) | Loss 5.0113(5.1662) | Error 0.9100(0.9003) Steps 676(683.01) | Grad Norm 10.0054(7.4186) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 16.8031(17.1312) | Bit/dim 5.0195(5.1277) | Xent 2.3026(2.3026) | Loss 5.0195(5.1277) | Error 0.8867(0.8995) Steps 676(680.24) | Grad Norm 8.6953(8.1744) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 17.3791(17.0923) | Bit/dim 4.9044(5.0861) | Xent 2.3026(2.3026) | Loss 4.9044(5.0861) | Error 0.8956(0.8994) Steps 670(677.84) | Grad Norm 2.2781(7.6379) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 17.2081(17.0035) | Bit/dim 4.8811(5.0429) | Xent 2.3026(2.3026) | Loss 4.8811(5.0429) | Error 0.8967(0.8992) Steps 670(675.80) | Grad Norm 7.8942(7.6417) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 16.9855(17.0370) | Bit/dim 4.8704(5.0028) | Xent 2.3026(2.3026) | Loss 4.8704(5.0028) | Error 0.9111(0.9004) Steps 670(674.57) | Grad Norm 6.2660(7.1969) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 91.3561, Epoch Time 1044.5134(889.4888), Bit/dim 4.8675(best: 5.0963), Xent 2.3026, Loss 4.8675, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 17.3338(17.0211) | Bit/dim 4.8876(4.9651) | Xent 2.3026(2.3026) | Loss 4.8876(4.9651) | Error 0.8944(0.9004) Steps 676(673.15) | Grad Norm 13.6625(7.0807) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 17.6878(17.0511) | Bit/dim 5.2627(4.9639) | Xent 2.3026(2.3026) | Loss 5.2627(4.9639) | Error 0.9033(0.8997) Steps 688(672.78) | Grad Norm 46.8045(10.7491) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 17.5154(17.1110) | Bit/dim 5.0235(4.9886) | Xent 2.3026(2.3026) | Loss 5.0235(4.9886) | Error 0.8867(0.9000) Steps 688(675.92) | Grad Norm 10.7377(12.8995) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 17.5485(17.2312) | Bit/dim 4.8795(4.9675) | Xent 2.3026(2.3026) | Loss 4.8795(4.9675) | Error 0.8967(0.8992) Steps 694(681.48) | Grad Norm 5.9800(11.3286) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.1314(17.3702) | Bit/dim 4.8150(4.9307) | Xent 2.3026(2.3026) | Loss 4.8150(4.9307) | Error 0.8989(0.9005) Steps 676(682.00) | Grad Norm 4.0655(9.3004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 90.6557, Epoch Time 1063.9449(894.7225), Bit/dim 4.7708(best: 4.8675), Xent 2.3026, Loss 4.7708, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.1022(17.4495) | Bit/dim 4.7263(4.8911) | Xent 2.3026(2.3026) | Loss 4.7263(4.8911) | Error 0.8933(0.9005) Steps 682(682.13) | Grad Norm 2.5488(7.4391) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 18.2688(17.6614) | Bit/dim 4.7126(4.8492) | Xent 2.3026(2.3026) | Loss 4.7126(4.8492) | Error 0.8911(0.9012) Steps 706(685.48) | Grad Norm 1.8393(6.0485) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 18.6787(17.8268) | Bit/dim 4.6924(4.8142) | Xent 2.3026(2.3026) | Loss 4.6924(4.8142) | Error 0.8867(0.9007) Steps 706(689.30) | Grad Norm 1.8091(5.1121) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.5851(17.9967) | Bit/dim 4.6400(4.7802) | Xent 2.3026(2.3026) | Loss 4.6400(4.7802) | Error 0.8978(0.8997) Steps 718(696.13) | Grad Norm 1.9056(4.3333) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.8834(18.1428) | Bit/dim 4.6630(4.7490) | Xent 2.3026(2.3026) | Loss 4.6630(4.7490) | Error 0.9200(0.8999) Steps 718(701.65) | Grad Norm 9.1686(4.3920) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.6290(18.2355) | Bit/dim 4.6166(4.7212) | Xent 2.3026(2.3026) | Loss 4.6166(4.7212) | Error 0.8956(0.8997) Steps 730(705.55) | Grad Norm 8.5861(5.0048) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 93.5082, Epoch Time 1120.2825(901.4893), Bit/dim 4.6524(best: 4.7708), Xent 2.3026, Loss 4.6524, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.0314(18.3527) | Bit/dim 4.8810(4.7075) | Xent 2.3026(2.3026) | Loss 4.8810(4.7075) | Error 0.8922(0.8998) Steps 712(710.14) | Grad Norm 26.1399(6.6385) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 17.8175(18.2090) | Bit/dim 4.9611(4.8103) | Xent 2.3026(2.3026) | Loss 4.9611(4.8103) | Error 0.9111(0.9001) Steps 694(708.14) | Grad Norm 16.3655(10.0344) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 17.6443(18.0473) | Bit/dim 4.7953(4.8251) | Xent 2.3026(2.3026) | Loss 4.7953(4.8251) | Error 0.8844(0.9000) Steps 676(700.52) | Grad Norm 6.6113(9.1861) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 17.8184(17.9309) | Bit/dim 4.6487(4.7946) | Xent 2.3026(2.3026) | Loss 4.6487(4.7946) | Error 0.8922(0.8995) Steps 682(694.89) | Grad Norm 2.7933(7.7155) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 18.0004(17.9305) | Bit/dim 4.6200(4.7521) | Xent 2.3026(2.3026) | Loss 4.6200(4.7521) | Error 0.9078(0.8992) Steps 694(693.74) | Grad Norm 2.5409(6.4011) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 94.5509, Epoch Time 1099.3889(907.4263), Bit/dim 4.6003(best: 4.6524), Xent 2.3026, Loss 4.6003, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 19.1199(18.0931) | Bit/dim 4.6016(4.7138) | Xent 2.3026(2.3026) | Loss 4.6016(4.7138) | Error 0.9156(0.9010) Steps 718(697.46) | Grad Norm 1.9645(5.3779) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 18.9052(18.2586) | Bit/dim 4.5544(4.6774) | Xent 2.3026(2.3026) | Loss 4.5544(4.6774) | Error 0.8989(0.9012) Steps 700(701.60) | Grad Norm 0.8891(4.4740) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 18.8748(18.2821) | Bit/dim 4.5316(4.6450) | Xent 2.3026(2.3026) | Loss 4.5316(4.6450) | Error 0.8978(0.9001) Steps 712(701.61) | Grad Norm 1.3275(3.7438) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 18.9555(18.3464) | Bit/dim 4.5062(4.6124) | Xent 2.3026(2.3026) | Loss 4.5062(4.6124) | Error 0.9044(0.8995) Steps 712(703.13) | Grad Norm 4.5638(3.4203) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 18.4089(18.3738) | Bit/dim 4.5311(4.5871) | Xent 2.3026(2.3026) | Loss 4.5311(4.5871) | Error 0.9089(0.8991) Steps 706(704.97) | Grad Norm 2.1348(3.3001) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.2161(18.3835) | Bit/dim 4.5181(4.5685) | Xent 2.3026(2.3026) | Loss 4.5181(4.5685) | Error 0.9122(0.9002) Steps 706(706.95) | Grad Norm 8.0181(3.8229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 94.7700, Epoch Time 1131.1586(914.1382), Bit/dim 4.5029(best: 4.6003), Xent 2.3026, Loss 4.5029, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 17.5251(18.2658) | Bit/dim 4.4596(4.5484) | Xent 2.3026(2.3026) | Loss 4.4596(4.5484) | Error 0.8933(0.8998) Steps 712(709.83) | Grad Norm 2.2326(3.9389) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 18.6113(18.2161) | Bit/dim 4.4807(4.5270) | Xent 2.3026(2.3026) | Loss 4.4807(4.5270) | Error 0.9067(0.9003) Steps 712(709.09) | Grad Norm 4.0602(3.5464) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 18.6467(18.2671) | Bit/dim 4.4588(4.5084) | Xent 2.3026(2.3026) | Loss 4.4588(4.5084) | Error 0.8789(0.8993) Steps 712(709.27) | Grad Norm 3.6940(4.3619) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 18.4860(18.3372) | Bit/dim 4.4278(4.4875) | Xent 2.3026(2.3026) | Loss 4.4278(4.4875) | Error 0.9156(0.9008) Steps 700(708.71) | Grad Norm 6.2768(3.9157) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 18.0919(18.4052) | Bit/dim 4.4986(4.4936) | Xent 2.3026(2.3026) | Loss 4.4986(4.4936) | Error 0.9056(0.9010) Steps 712(710.93) | Grad Norm 9.0835(6.1812) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 93.7859, Epoch Time 1119.2038(920.2902), Bit/dim 4.4706(best: 4.5029), Xent 2.3026, Loss 4.4706, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 17.8795(18.3249) | Bit/dim 4.4441(4.4917) | Xent 2.3026(2.3026) | Loss 4.4441(4.4917) | Error 0.9044(0.9002) Steps 694(708.00) | Grad Norm 4.7337(6.4100) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 18.0249(18.3581) | Bit/dim 4.3839(4.4725) | Xent 2.3026(2.3026) | Loss 4.3839(4.4725) | Error 0.8911(0.8989) Steps 688(705.56) | Grad Norm 2.2685(5.5342) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 18.7432(18.4473) | Bit/dim 4.3937(4.4535) | Xent 2.3026(2.3026) | Loss 4.3937(4.4535) | Error 0.9033(0.8997) Steps 724(707.89) | Grad Norm 1.4803(4.7054) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 18.8883(18.5840) | Bit/dim 4.3733(4.4308) | Xent 2.3026(2.3026) | Loss 4.3733(4.4308) | Error 0.8911(0.8996) Steps 730(712.23) | Grad Norm 2.2336(4.0522) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.0008(18.6905) | Bit/dim 4.3508(4.4099) | Xent 2.3026(2.3026) | Loss 4.3508(4.4099) | Error 0.9044(0.8990) Steps 742(717.04) | Grad Norm 1.3935(3.5238) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 18.4475(18.7229) | Bit/dim 4.5821(4.4508) | Xent 2.3026(2.3026) | Loss 4.5821(4.4508) | Error 0.8989(0.8999) Steps 706(720.13) | Grad Norm 13.2061(5.8922) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 95.5562, Epoch Time 1142.2633(926.9494), Bit/dim 4.5039(best: 4.4706), Xent 2.3026, Loss 4.5039, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.9680(18.8723) | Bit/dim 4.4447(4.4650) | Xent 2.3026(2.3026) | Loss 4.4447(4.4650) | Error 0.8822(0.8994) Steps 742(729.53) | Grad Norm 3.7993(5.8046) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 18.9525(18.8192) | Bit/dim 4.3397(4.4481) | Xent 2.3026(2.3026) | Loss 4.3397(4.4481) | Error 0.9033(0.9003) Steps 700(725.12) | Grad Norm 2.3807(5.0117) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 18.3399(18.7346) | Bit/dim 4.3185(4.4223) | Xent 2.3026(2.3026) | Loss 4.3185(4.4223) | Error 0.9211(0.9002) Steps 700(717.48) | Grad Norm 1.8650(4.1042) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 18.8934(18.7209) | Bit/dim 4.3237(4.3970) | Xent 2.3026(2.3026) | Loss 4.3237(4.3970) | Error 0.9056(0.9012) Steps 706(712.44) | Grad Norm 0.9330(3.3423) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 18.7995(18.7219) | Bit/dim 4.2619(4.3730) | Xent 2.3026(2.3026) | Loss 4.2619(4.3730) | Error 0.8878(0.9007) Steps 700(708.88) | Grad Norm 1.0554(2.7474) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 89.9663, Epoch Time 1137.7749(933.2742), Bit/dim 4.2839(best: 4.4706), Xent 2.3026, Loss 4.2839, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 18.2736(18.5645) | Bit/dim 4.2650(4.3474) | Xent 2.3026(2.3026) | Loss 4.2650(4.3474) | Error 0.8967(0.8998) Steps 688(705.55) | Grad Norm 1.3594(2.5341) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 17.6840(18.3468) | Bit/dim 4.2778(4.3271) | Xent 2.3026(2.3026) | Loss 4.2778(4.3271) | Error 0.9056(0.9000) Steps 706(702.91) | Grad Norm 3.8662(2.4669) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 18.2766(18.2767) | Bit/dim 4.2418(4.3062) | Xent 2.3026(2.3026) | Loss 4.2418(4.3062) | Error 0.9089(0.9006) Steps 694(701.87) | Grad Norm 5.2165(2.6680) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 18.3223(18.2715) | Bit/dim 4.2759(4.2957) | Xent 2.3026(2.3026) | Loss 4.2759(4.2957) | Error 0.8944(0.9006) Steps 706(703.13) | Grad Norm 8.0642(3.7952) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 18.8357(18.3936) | Bit/dim 4.2795(4.3029) | Xent 2.3026(2.3026) | Loss 4.2795(4.3029) | Error 0.8967(0.9003) Steps 718(707.22) | Grad Norm 9.5751(5.3536) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 17.8494(18.3655) | Bit/dim 4.2586(4.2925) | Xent 2.3026(2.3026) | Loss 4.2586(4.2925) | Error 0.9144(0.8996) Steps 712(707.57) | Grad Norm 2.9272(5.0052) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 89.5356, Epoch Time 1108.1789(938.5213), Bit/dim 4.2354(best: 4.2839), Xent 2.3026, Loss 4.2354, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 18.0030(18.3285) | Bit/dim 4.1846(4.2722) | Xent 2.3026(2.3026) | Loss 4.1846(4.2722) | Error 0.9122(0.8992) Steps 706(706.67) | Grad Norm 2.0339(4.3482) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 18.8997(18.4492) | Bit/dim 4.1669(4.2538) | Xent 2.3026(2.3026) | Loss 4.1669(4.2538) | Error 0.8978(0.9006) Steps 730(712.25) | Grad Norm 1.5454(3.7686) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 19.1840(18.6597) | Bit/dim 4.2865(4.2369) | Xent 2.3026(2.3026) | Loss 4.2865(4.2369) | Error 0.8922(0.9001) Steps 736(718.05) | Grad Norm 9.0296(4.0289) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 19.9964(18.8180) | Bit/dim 4.2084(4.2354) | Xent 2.3026(2.3026) | Loss 4.2084(4.2354) | Error 0.9111(0.9003) Steps 736(724.74) | Grad Norm 4.6183(4.9376) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 19.6029(18.9121) | Bit/dim 4.1484(4.2200) | Xent 2.3026(2.3026) | Loss 4.1484(4.2200) | Error 0.8800(0.8995) Steps 760(728.40) | Grad Norm 4.1871(4.7650) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 93.2178, Epoch Time 1152.2367(944.9328), Bit/dim 4.1505(best: 4.2354), Xent 2.3026, Loss 4.1505, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 18.9742(18.9719) | Bit/dim 4.1502(4.2026) | Xent 2.3026(2.3026) | Loss 4.1502(4.2026) | Error 0.8956(0.8996) Steps 754(731.47) | Grad Norm 2.1387(4.4754) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 19.1437(19.1431) | Bit/dim 4.1902(4.1921) | Xent 2.3026(2.3026) | Loss 4.1902(4.1921) | Error 0.9056(0.9008) Steps 754(736.75) | Grad Norm 7.5479(4.6970) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 19.1666(19.2361) | Bit/dim 4.1141(4.1756) | Xent 2.3026(2.3026) | Loss 4.1141(4.1756) | Error 0.8967(0.9008) Steps 760(742.42) | Grad Norm 3.8781(4.4299) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 19.7822(19.3423) | Bit/dim 4.0978(4.1586) | Xent 2.3026(2.3026) | Loss 4.0978(4.1586) | Error 0.8878(0.8998) Steps 766(746.74) | Grad Norm 0.6908(3.9828) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 20.2719(19.5218) | Bit/dim 4.0767(4.1449) | Xent 2.3026(2.3026) | Loss 4.0767(4.1449) | Error 0.9133(0.8996) Steps 778(751.20) | Grad Norm 2.7011(4.2070) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 20.0593(19.5929) | Bit/dim 4.0814(4.1300) | Xent 2.3026(2.3026) | Loss 4.0814(4.1300) | Error 0.8922(0.8993) Steps 772(755.01) | Grad Norm 5.3749(4.1888) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 94.8275, Epoch Time 1195.1528(952.4394), Bit/dim 4.0956(best: 4.1505), Xent 2.3026, Loss 4.0956, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 20.4741(19.6655) | Bit/dim 4.0941(4.1191) | Xent 2.3026(2.3026) | Loss 4.0941(4.1191) | Error 0.8867(0.8998) Steps 772(757.22) | Grad Norm 8.4957(4.3917) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 20.4697(19.7287) | Bit/dim 4.0674(4.1099) | Xent 2.3026(2.3026) | Loss 4.0674(4.1099) | Error 0.9111(0.8990) Steps 778(761.60) | Grad Norm 3.2214(4.8325) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 20.3659(19.8874) | Bit/dim 4.0270(4.0970) | Xent 2.3026(2.3026) | Loss 4.0270(4.0970) | Error 0.9033(0.9006) Steps 796(766.11) | Grad Norm 3.8291(4.7933) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 20.9162(19.9632) | Bit/dim 4.0447(4.0842) | Xent 2.3026(2.3026) | Loss 4.0447(4.0842) | Error 0.9033(0.8990) Steps 790(771.79) | Grad Norm 3.2164(4.7056) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 20.1778(20.0434) | Bit/dim 4.0548(4.0745) | Xent 2.3026(2.3026) | Loss 4.0548(4.0745) | Error 0.8889(0.8990) Steps 796(776.02) | Grad Norm 9.1366(4.7550) | Total Time 14.00(14.00)\n",
      "Epoch 0019 | Time 99.1067, Epoch Time 1222.9999(960.5562), Bit/dim 4.0305(best: 4.0956), Xent 2.3026, Loss 4.0305, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 20.5499(20.1245) | Bit/dim 4.0082(4.0605) | Xent 2.3026(2.3026) | Loss 4.0082(4.0605) | Error 0.9089(0.9010) Steps 790(779.23) | Grad Norm 3.1820(4.4894) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 20.5614(20.1738) | Bit/dim 4.0122(4.0500) | Xent 2.3026(2.3026) | Loss 4.0122(4.0500) | Error 0.8756(0.9002) Steps 772(782.82) | Grad Norm 4.7420(4.1794) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 21.0985(20.2479) | Bit/dim 4.0171(4.0391) | Xent 2.3026(2.3026) | Loss 4.0171(4.0391) | Error 0.9022(0.8999) Steps 808(786.68) | Grad Norm 2.1429(3.8721) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 21.1661(20.3252) | Bit/dim 3.9918(4.0337) | Xent 2.3026(2.3026) | Loss 3.9918(4.0337) | Error 0.9011(0.9006) Steps 790(787.54) | Grad Norm 5.7795(4.4861) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 20.4704(20.3840) | Bit/dim 3.9904(4.0254) | Xent 2.3026(2.3026) | Loss 3.9904(4.0254) | Error 0.9144(0.8997) Steps 796(790.17) | Grad Norm 5.9508(4.3778) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 20.5775(20.3929) | Bit/dim 3.9765(4.0160) | Xent 2.3026(2.3026) | Loss 3.9765(4.0160) | Error 0.9011(0.8998) Steps 790(790.96) | Grad Norm 2.2551(4.0792) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 99.5249, Epoch Time 1242.6036(969.0176), Bit/dim 3.9792(best: 4.0305), Xent 2.3026, Loss 3.9792, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 20.8584(20.4013) | Bit/dim 3.9627(4.0049) | Xent 2.3026(2.3026) | Loss 3.9627(4.0049) | Error 0.8911(0.9004) Steps 802(794.00) | Grad Norm 5.1224(3.8724) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 20.1768(20.3710) | Bit/dim 3.9505(3.9955) | Xent 2.3026(2.3026) | Loss 3.9505(3.9955) | Error 0.9044(0.8999) Steps 790(795.18) | Grad Norm 2.6936(3.7024) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 20.1233(20.4190) | Bit/dim 3.9640(3.9893) | Xent 2.3026(2.3026) | Loss 3.9640(3.9893) | Error 0.8978(0.8992) Steps 796(796.90) | Grad Norm 2.8314(4.1281) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 20.3586(20.4331) | Bit/dim 3.9314(3.9795) | Xent 2.3026(2.3026) | Loss 3.9314(3.9795) | Error 0.9156(0.8989) Steps 814(798.23) | Grad Norm 2.1005(3.8315) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 20.9630(20.5378) | Bit/dim 3.9372(3.9725) | Xent 2.3026(2.3026) | Loss 3.9372(3.9725) | Error 0.9056(0.9004) Steps 808(801.88) | Grad Norm 4.0561(4.1430) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 101.0633, Epoch Time 1249.1067(977.4203), Bit/dim 3.9406(best: 3.9792), Xent 2.3026, Loss 3.9406, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 20.7923(20.6382) | Bit/dim 3.9683(3.9658) | Xent 2.3026(2.3026) | Loss 3.9683(3.9658) | Error 0.8944(0.9001) Steps 796(804.03) | Grad Norm 3.3422(4.1072) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 20.8228(20.7508) | Bit/dim 3.9334(3.9583) | Xent 2.3026(2.3026) | Loss 3.9334(3.9583) | Error 0.9011(0.9001) Steps 814(805.59) | Grad Norm 6.7257(4.0934) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 21.3918(20.7657) | Bit/dim 3.9087(3.9514) | Xent 2.3026(2.3026) | Loss 3.9087(3.9514) | Error 0.8900(0.8996) Steps 820(808.13) | Grad Norm 1.9215(3.9904) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.7760(20.7802) | Bit/dim 3.8956(3.9398) | Xent 2.3026(2.3026) | Loss 3.8956(3.9398) | Error 0.9011(0.8994) Steps 814(809.31) | Grad Norm 5.0358(3.9901) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 20.9544(20.8032) | Bit/dim 3.9215(3.9345) | Xent 2.3026(2.3026) | Loss 3.9215(3.9345) | Error 0.8944(0.8995) Steps 796(808.56) | Grad Norm 8.4829(4.3740) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 20.7616(20.8072) | Bit/dim 3.8692(3.9310) | Xent 2.3026(2.3026) | Loss 3.8692(3.9310) | Error 0.9022(0.9005) Steps 820(809.06) | Grad Norm 3.7306(4.3093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 100.4797, Epoch Time 1266.8261(986.1024), Bit/dim 3.9134(best: 3.9406), Xent 2.3026, Loss 3.9134, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 20.6396(20.8361) | Bit/dim 3.9143(3.9229) | Xent 2.3026(2.3026) | Loss 3.9143(3.9229) | Error 0.9011(0.9023) Steps 820(810.42) | Grad Norm 4.7270(4.1810) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 21.1723(20.8573) | Bit/dim 3.8908(3.9173) | Xent 2.3026(2.3026) | Loss 3.8908(3.9173) | Error 0.8922(0.9001) Steps 808(810.61) | Grad Norm 1.3749(4.0966) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 20.9511(20.8790) | Bit/dim 3.8801(3.9127) | Xent 2.3026(2.3026) | Loss 3.8801(3.9127) | Error 0.8978(0.8988) Steps 808(808.95) | Grad Norm 2.9420(4.2221) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 20.9760(20.8410) | Bit/dim 3.8828(3.9057) | Xent 2.3026(2.3026) | Loss 3.8828(3.9057) | Error 0.8967(0.8999) Steps 796(808.16) | Grad Norm 1.3398(4.1181) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 20.7115(20.8445) | Bit/dim 3.8677(3.9002) | Xent 2.3026(2.3026) | Loss 3.8677(3.9002) | Error 0.9078(0.9000) Steps 790(808.07) | Grad Norm 1.3647(4.0691) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 98.2085, Epoch Time 1261.6524(994.3689), Bit/dim 3.8722(best: 3.9134), Xent 2.3026, Loss 3.8722, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 20.1522(20.7574) | Bit/dim 3.8867(3.8915) | Xent 2.3026(2.3026) | Loss 3.8867(3.8915) | Error 0.8889(0.9000) Steps 802(807.29) | Grad Norm 5.4132(4.0872) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 20.8598(20.6809) | Bit/dim 3.8779(3.8882) | Xent 2.3026(2.3026) | Loss 3.8779(3.8882) | Error 0.9089(0.9003) Steps 814(806.58) | Grad Norm 2.0847(4.2812) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 20.0487(20.6110) | Bit/dim 3.8347(3.8804) | Xent 2.3026(2.3026) | Loss 3.8347(3.8804) | Error 0.8856(0.8984) Steps 808(803.66) | Grad Norm 1.6314(4.0892) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 20.6565(20.5448) | Bit/dim 3.8586(3.8757) | Xent 2.3026(2.3026) | Loss 3.8586(3.8757) | Error 0.9078(0.8998) Steps 790(801.45) | Grad Norm 2.5567(4.3053) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 20.3697(20.4712) | Bit/dim 3.8213(3.8705) | Xent 2.3026(2.3026) | Loss 3.8213(3.8705) | Error 0.9044(0.9006) Steps 790(800.07) | Grad Norm 3.0402(4.2735) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 20.3406(20.4250) | Bit/dim 3.8863(3.8688) | Xent 2.3026(2.3026) | Loss 3.8863(3.8688) | Error 0.9044(0.9009) Steps 796(798.52) | Grad Norm 7.2242(4.4329) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 96.7210, Epoch Time 1235.6426(1001.6072), Bit/dim 3.8640(best: 3.8722), Xent 2.3026, Loss 3.8640, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 19.7880(20.3705) | Bit/dim 3.8845(3.8641) | Xent 2.3026(2.3026) | Loss 3.8845(3.8641) | Error 0.9000(0.9016) Steps 802(799.01) | Grad Norm 5.3629(4.3714) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 20.8236(20.3494) | Bit/dim 3.7989(3.8615) | Xent 2.3026(2.3026) | Loss 3.7989(3.8615) | Error 0.8878(0.9011) Steps 790(798.49) | Grad Norm 5.4785(4.2513) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 20.1458(20.3289) | Bit/dim 3.8543(3.8572) | Xent 2.3026(2.3026) | Loss 3.8543(3.8572) | Error 0.9033(0.8999) Steps 796(797.57) | Grad Norm 2.1069(4.1416) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 20.0600(20.2928) | Bit/dim 3.8389(3.8524) | Xent 2.3026(2.3026) | Loss 3.8389(3.8524) | Error 0.8900(0.8997) Steps 796(798.16) | Grad Norm 5.4072(4.2649) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 20.9279(20.3169) | Bit/dim 3.8222(3.8443) | Xent 2.3026(2.3026) | Loss 3.8222(3.8443) | Error 0.9056(0.9001) Steps 802(797.85) | Grad Norm 2.9861(4.2246) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 96.9037, Epoch Time 1230.6072(1008.4772), Bit/dim 3.8258(best: 3.8640), Xent 2.3026, Loss 3.8258, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 20.2635(20.3601) | Bit/dim 3.8315(3.8383) | Xent 2.3026(2.3026) | Loss 3.8315(3.8383) | Error 0.8978(0.9006) Steps 796(798.78) | Grad Norm 5.2988(4.0888) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 20.2576(20.3959) | Bit/dim 3.8270(3.8344) | Xent 2.3026(2.3026) | Loss 3.8270(3.8344) | Error 0.8778(0.8988) Steps 796(797.63) | Grad Norm 1.9912(3.9960) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 19.5773(20.4115) | Bit/dim 3.8355(3.8334) | Xent 2.3026(2.3026) | Loss 3.8355(3.8334) | Error 0.9000(0.8998) Steps 790(796.91) | Grad Norm 5.9250(4.2910) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 20.4747(20.4042) | Bit/dim 3.8038(3.8324) | Xent 2.3026(2.3026) | Loss 3.8038(3.8324) | Error 0.9211(0.9000) Steps 808(797.12) | Grad Norm 1.5239(4.3692) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 20.3619(20.3746) | Bit/dim 3.8041(3.8261) | Xent 2.3026(2.3026) | Loss 3.8041(3.8261) | Error 0.9111(0.9001) Steps 808(796.01) | Grad Norm 7.4237(4.3183) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 19.9794(20.3793) | Bit/dim 3.8013(3.8229) | Xent 2.3026(2.3026) | Loss 3.8013(3.8229) | Error 0.8978(0.9001) Steps 790(795.17) | Grad Norm 2.4309(4.2981) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 96.4816, Epoch Time 1237.4869(1015.3474), Bit/dim 3.8063(best: 3.8258), Xent 2.3026, Loss 3.8063, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 20.2341(20.3601) | Bit/dim 3.8022(3.8197) | Xent 2.3026(2.3026) | Loss 3.8022(3.8197) | Error 0.9222(0.9014) Steps 790(793.99) | Grad Norm 4.6768(4.1411) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 20.6342(20.2946) | Bit/dim 3.8054(3.8120) | Xent 2.3026(2.3026) | Loss 3.8054(3.8120) | Error 0.9067(0.9009) Steps 802(794.58) | Grad Norm 5.0530(4.0148) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 20.3148(20.2698) | Bit/dim 3.8206(3.8096) | Xent 2.3026(2.3026) | Loss 3.8206(3.8096) | Error 0.8922(0.9002) Steps 790(795.17) | Grad Norm 4.0540(4.0552) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 20.0944(20.1845) | Bit/dim 3.7576(3.8017) | Xent 2.3026(2.3026) | Loss 3.7576(3.8017) | Error 0.9100(0.8997) Steps 790(795.06) | Grad Norm 2.9606(4.0573) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 96.6654, Epoch Time 1223.8520(1021.6026), Bit/dim 3.7993(best: 3.8063), Xent 2.3026, Loss 3.7993, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 19.9604(20.1855) | Bit/dim 3.7978(3.7982) | Xent 2.3026(2.3026) | Loss 3.7978(3.7982) | Error 0.9011(0.8993) Steps 784(795.09) | Grad Norm 3.9793(4.5582) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 20.1330(20.1263) | Bit/dim 3.8337(3.7997) | Xent 2.3026(2.3026) | Loss 3.8337(3.7997) | Error 0.8856(0.8999) Steps 790(794.41) | Grad Norm 1.7636(4.3996) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 19.8597(20.1860) | Bit/dim 3.7784(3.7952) | Xent 2.3026(2.3026) | Loss 3.7784(3.7952) | Error 0.8944(0.9003) Steps 790(794.19) | Grad Norm 2.4443(3.8611) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 20.3533(20.1640) | Bit/dim 3.7802(3.7913) | Xent 2.3026(2.3026) | Loss 3.7802(3.7913) | Error 0.8911(0.9010) Steps 790(792.44) | Grad Norm 3.8327(3.9918) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 19.5431(20.1690) | Bit/dim 3.8178(3.7874) | Xent 2.3026(2.3026) | Loss 3.8178(3.7874) | Error 0.9000(0.9000) Steps 790(793.23) | Grad Norm 7.6314(3.9582) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 20.1598(20.1682) | Bit/dim 3.7846(3.7849) | Xent 2.3026(2.3026) | Loss 3.7846(3.7849) | Error 0.8800(0.8992) Steps 772(791.58) | Grad Norm 4.2795(4.1777) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 95.6804, Epoch Time 1221.9310(1027.6124), Bit/dim 3.7726(best: 3.7993), Xent 2.3026, Loss 3.7726, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 20.6907(20.1543) | Bit/dim 3.7166(3.7791) | Xent 2.3026(2.3026) | Loss 3.7166(3.7791) | Error 0.8867(0.8983) Steps 808(792.64) | Grad Norm 5.9678(4.1365) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 20.0488(20.1548) | Bit/dim 3.7477(3.7749) | Xent 2.3026(2.3026) | Loss 3.7477(3.7749) | Error 0.8989(0.8975) Steps 772(791.53) | Grad Norm 6.8696(4.2795) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 20.2855(20.1463) | Bit/dim 3.7565(3.7727) | Xent 2.3026(2.3026) | Loss 3.7565(3.7727) | Error 0.8856(0.8984) Steps 790(790.95) | Grad Norm 1.9531(4.1878) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 19.6744(20.1996) | Bit/dim 3.7598(3.7702) | Xent 2.3026(2.3026) | Loss 3.7598(3.7702) | Error 0.9100(0.8984) Steps 772(790.76) | Grad Norm 4.1362(3.6871) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 20.0064(20.1927) | Bit/dim 3.7853(3.7716) | Xent 2.3026(2.3026) | Loss 3.7853(3.7716) | Error 0.9067(0.8996) Steps 802(789.73) | Grad Norm 7.3103(4.3133) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 96.6581, Epoch Time 1223.6495(1033.4935), Bit/dim 3.7566(best: 3.7726), Xent 2.3026, Loss 3.7566, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 20.2147(20.1993) | Bit/dim 3.7790(3.7698) | Xent 2.3026(2.3026) | Loss 3.7790(3.7698) | Error 0.8900(0.9007) Steps 772(787.33) | Grad Norm 2.0424(4.2243) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 20.0687(20.1884) | Bit/dim 3.7417(3.7649) | Xent 2.3026(2.3026) | Loss 3.7417(3.7649) | Error 0.8900(0.9013) Steps 766(785.26) | Grad Norm 4.2345(4.0361) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 20.2636(20.2152) | Bit/dim 3.7693(3.7624) | Xent 2.3026(2.3026) | Loss 3.7693(3.7624) | Error 0.8967(0.9006) Steps 772(784.80) | Grad Norm 3.9384(4.0888) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 20.9133(20.2589) | Bit/dim 3.7514(3.7605) | Xent 2.3026(2.3026) | Loss 3.7514(3.7605) | Error 0.9078(0.9008) Steps 790(786.08) | Grad Norm 3.3182(4.2682) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 20.7589(20.2696) | Bit/dim 3.7343(3.7592) | Xent 2.3026(2.3026) | Loss 3.7343(3.7592) | Error 0.9211(0.8997) Steps 802(786.94) | Grad Norm 4.6786(4.2326) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 20.4154(20.2522) | Bit/dim 3.7483(3.7547) | Xent 2.3026(2.3026) | Loss 3.7483(3.7547) | Error 0.9178(0.9004) Steps 808(786.43) | Grad Norm 4.1765(3.9279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 96.0710, Epoch Time 1229.3850(1039.3703), Bit/dim 3.7465(best: 3.7566), Xent 2.3026, Loss 3.7465, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 20.9443(20.2486) | Bit/dim 3.7339(3.7524) | Xent 2.3026(2.3026) | Loss 3.7339(3.7524) | Error 0.8933(0.9000) Steps 778(784.66) | Grad Norm 4.8631(3.8460) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 20.1081(20.2001) | Bit/dim 3.7622(3.7496) | Xent 2.3026(2.3026) | Loss 3.7622(3.7496) | Error 0.9100(0.8990) Steps 772(783.68) | Grad Norm 3.2908(3.8060) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 20.6802(20.2310) | Bit/dim 3.7634(3.7459) | Xent 2.3026(2.3026) | Loss 3.7634(3.7459) | Error 0.8933(0.9002) Steps 808(785.18) | Grad Norm 5.3019(3.8683) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 19.7727(20.1913) | Bit/dim 3.7211(3.7443) | Xent 2.3026(2.3026) | Loss 3.7211(3.7443) | Error 0.8911(0.9002) Steps 790(786.13) | Grad Norm 4.1688(4.2079) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 20.1015(20.1956) | Bit/dim 3.7641(3.7445) | Xent 2.3026(2.3026) | Loss 3.7641(3.7445) | Error 0.8944(0.9002) Steps 790(785.88) | Grad Norm 5.0893(4.2085) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 97.0810, Epoch Time 1223.6084(1044.8974), Bit/dim 3.7369(best: 3.7465), Xent 2.3026, Loss 3.7369, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 19.5338(20.1465) | Bit/dim 3.7272(3.7400) | Xent 2.3026(2.3026) | Loss 3.7272(3.7400) | Error 0.9078(0.9003) Steps 772(785.01) | Grad Norm 4.2163(4.2246) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 20.0211(20.0283) | Bit/dim 3.7339(3.7389) | Xent 2.3026(2.3026) | Loss 3.7339(3.7389) | Error 0.9178(0.9006) Steps 760(782.79) | Grad Norm 4.5620(4.1634) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_25_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
