{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 103.0647(103.0647) | Bit/dim 8.9613(8.9613) | Xent 2.3026(2.3026) | Loss 22.3688(22.3688) | Error 0.8982(0.8982) Steps 0(0.00) | Grad Norm 29.9451(29.9451) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 45.6283(101.3416) | Bit/dim 8.8759(8.9587) | Xent 2.2924(2.3023) | Loss 22.0403(22.3589) | Error 0.7726(0.8945) Steps 0(0.00) | Grad Norm 26.9394(29.8549) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 47.0841(99.7138) | Bit/dim 8.7784(8.9533) | Xent 2.2788(2.3016) | Loss 22.1175(22.3517) | Error 0.7674(0.8907) Steps 0(0.00) | Grad Norm 22.8161(29.6438) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 43.7365(98.0345) | Bit/dim 8.6993(8.9457) | Xent 2.2611(2.3004) | Loss 21.6918(22.3319) | Error 0.7582(0.8867) Steps 0(0.00) | Grad Norm 17.2432(29.2718) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 42.3893(96.3652) | Bit/dim 8.6212(8.9360) | Xent 2.2400(2.2985) | Loss 21.6775(22.3122) | Error 0.7546(0.8827) Steps 0(0.00) | Grad Norm 12.1446(28.7579) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 43.0859(94.7668) | Bit/dim 8.5327(8.9239) | Xent 2.2217(2.2962) | Loss 21.2717(22.2810) | Error 0.7521(0.8788) Steps 0(0.00) | Grad Norm 9.0691(28.1673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.5023, Epoch Time 377.5217(377.5217), Bit/dim 8.4927(best: inf), Xent 2.1957, Loss 9.5906, Error 0.7393(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 57.0158(93.6343) | Bit/dim 8.5218(8.9118) | Xent 2.2010(2.2934) | Loss 24.5090(22.3479) | Error 0.7532(0.8750) Steps 0(0.00) | Grad Norm 9.0890(27.5949) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 44.3747(92.1565) | Bit/dim 8.4652(8.8984) | Xent 2.1830(2.2901) | Loss 21.2747(22.3157) | Error 0.7599(0.8716) Steps 0(0.00) | Grad Norm 11.8349(27.1221) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 43.8176(90.7063) | Bit/dim 8.4492(8.8849) | Xent 2.1676(2.2864) | Loss 21.0345(22.2772) | Error 0.7594(0.8682) Steps 0(0.00) | Grad Norm 14.5066(26.7437) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 47.0651(89.3971) | Bit/dim 8.4114(8.8707) | Xent 2.1515(2.2823) | Loss 20.9981(22.2389) | Error 0.7566(0.8649) Steps 0(0.00) | Grad Norm 15.6033(26.4095) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 47.0990(88.1281) | Bit/dim 8.3267(8.8544) | Xent 2.1535(2.2785) | Loss 20.9582(22.2004) | Error 0.7601(0.8617) Steps 0(0.00) | Grad Norm 15.6125(26.0855) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 44.1921(86.8100) | Bit/dim 8.2414(8.8360) | Xent 2.1280(2.2740) | Loss 20.7319(22.1564) | Error 0.7385(0.8580) Steps 0(0.00) | Grad Norm 12.8947(25.6898) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 20.2990, Epoch Time 319.8726(375.7922), Bit/dim 8.1902(best: 8.4927), Xent 2.1062, Loss 9.2433, Error 0.7264(best: 0.7393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 44.4016(85.5378) | Bit/dim 8.1762(8.8162) | Xent 2.1105(2.2691) | Loss 23.7351(22.2037) | Error 0.7344(0.8543) Steps 0(0.00) | Grad Norm 10.1043(25.2223) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 44.5119(84.3070) | Bit/dim 8.1260(8.7955) | Xent 2.0899(2.2637) | Loss 20.2019(22.1437) | Error 0.7225(0.8504) Steps 0(0.00) | Grad Norm 7.3293(24.6855) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 46.9677(83.1868) | Bit/dim 8.0817(8.7741) | Xent 2.0809(2.2582) | Loss 20.3776(22.0907) | Error 0.7245(0.8466) Steps 0(0.00) | Grad Norm 7.8983(24.1819) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 42.7936(81.9750) | Bit/dim 8.0083(8.7511) | Xent 2.0878(2.2531) | Loss 19.9758(22.0273) | Error 0.7304(0.8431) Steps 0(0.00) | Grad Norm 10.0696(23.7585) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 43.2561(80.8135) | Bit/dim 7.9643(8.7275) | Xent 2.0702(2.2476) | Loss 19.7728(21.9596) | Error 0.7335(0.8398) Steps 0(0.00) | Grad Norm 11.1478(23.3802) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 43.8034(79.7032) | Bit/dim 7.8604(8.7015) | Xent 2.0656(2.2421) | Loss 19.7447(21.8932) | Error 0.7299(0.8365) Steps 0(0.00) | Grad Norm 11.1771(23.0141) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 20.5292, Epoch Time 302.1599(373.5832), Bit/dim 7.7589(best: 8.1902), Xent 2.0369, Loss 8.7773, Error 0.7015(best: 0.7264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 44.6500(78.6516) | Bit/dim 7.7655(8.6734) | Xent 2.0434(2.2362) | Loss 22.5725(21.9136) | Error 0.7110(0.8328) Steps 0(0.00) | Grad Norm 9.2393(22.6008) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 44.2593(77.6198) | Bit/dim 7.6605(8.6430) | Xent 2.0382(2.2302) | Loss 19.4439(21.8395) | Error 0.6956(0.8286) Steps 0(0.00) | Grad Norm 6.3512(22.1133) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 46.0564(76.6729) | Bit/dim 7.5836(8.6113) | Xent 2.0454(2.2247) | Loss 19.1549(21.7589) | Error 0.6932(0.8246) Steps 0(0.00) | Grad Norm 5.7766(21.6232) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 47.2264(75.7895) | Bit/dim 7.4989(8.5779) | Xent 2.0325(2.2189) | Loss 18.9328(21.6741) | Error 0.6991(0.8208) Steps 0(0.00) | Grad Norm 7.8945(21.2114) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 48.2448(74.9632) | Bit/dim 7.4356(8.5436) | Xent 2.0283(2.2132) | Loss 18.7604(21.5867) | Error 0.7057(0.8174) Steps 0(0.00) | Grad Norm 10.0264(20.8758) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 45.5510(74.0808) | Bit/dim 7.3697(8.5084) | Xent 2.0407(2.2080) | Loss 18.6770(21.4994) | Error 0.7050(0.8140) Steps 0(0.00) | Grad Norm 9.2918(20.5283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 20.6631, Epoch Time 312.6306(371.7547), Bit/dim 7.3009(best: 7.7589), Xent 2.0150, Loss 8.3085, Error 0.6733(best: 0.7015)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 42.2981(73.1273) | Bit/dim 7.3007(8.4722) | Xent 2.0248(2.2025) | Loss 21.9312(21.5124) | Error 0.6876(0.8102) Steps 0(0.00) | Grad Norm 5.7180(20.0840) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 47.5353(72.3596) | Bit/dim 7.2432(8.4353) | Xent 2.0238(2.1972) | Loss 18.4602(21.4208) | Error 0.6789(0.8063) Steps 0(0.00) | Grad Norm 4.9137(19.6289) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 47.3961(71.6107) | Bit/dim 7.2019(8.3983) | Xent 2.0260(2.1920) | Loss 18.2144(21.3246) | Error 0.6887(0.8027) Steps 0(0.00) | Grad Norm 7.8971(19.2769) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 46.3454(70.8527) | Bit/dim 7.1586(8.3611) | Xent 2.0375(2.1874) | Loss 18.2890(21.2336) | Error 0.6859(0.7992) Steps 0(0.00) | Grad Norm 7.0749(18.9109) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 45.4176(70.0896) | Bit/dim 7.1250(8.3240) | Xent 2.0312(2.1827) | Loss 18.1218(21.1402) | Error 0.6844(0.7958) Steps 0(0.00) | Grad Norm 4.3804(18.4750) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 45.4992(69.3519) | Bit/dim 7.1034(8.2874) | Xent 2.0350(2.1783) | Loss 18.1302(21.0499) | Error 0.6906(0.7926) Steps 0(0.00) | Grad Norm 5.1789(18.0761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 20.9974, Epoch Time 311.6265(369.9508), Bit/dim 7.0801(best: 7.3009), Xent 2.0346, Loss 8.0974, Error 0.6969(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 45.3159(68.6309) | Bit/dim 7.0662(8.2508) | Xent 2.0391(2.1741) | Loss 21.0469(21.0498) | Error 0.7099(0.7902) Steps 0(0.00) | Grad Norm 6.0028(17.7139) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 45.6175(67.9405) | Bit/dim 7.0580(8.2150) | Xent 2.0480(2.1703) | Loss 17.9719(20.9575) | Error 0.7232(0.7881) Steps 0(0.00) | Grad Norm 3.3735(17.2837) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 49.1438(67.3766) | Bit/dim 7.0666(8.1805) | Xent 2.0479(2.1667) | Loss 18.0755(20.8710) | Error 0.7101(0.7858) Steps 0(0.00) | Grad Norm 5.1937(16.9210) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 49.4617(66.8391) | Bit/dim 7.0529(8.1467) | Xent 2.0542(2.1633) | Loss 18.0135(20.7853) | Error 0.7116(0.7836) Steps 0(0.00) | Grad Norm 3.8583(16.5291) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 49.2401(66.3111) | Bit/dim 7.0255(8.1131) | Xent 2.0401(2.1596) | Loss 18.0209(20.7024) | Error 0.7077(0.7813) Steps 0(0.00) | Grad Norm 5.3180(16.1928) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 50.2518(65.8294) | Bit/dim 7.0185(8.0802) | Xent 2.0450(2.1561) | Loss 17.9412(20.6195) | Error 0.7116(0.7792) Steps 0(0.00) | Grad Norm 3.7398(15.8192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 21.5111, Epoch Time 326.4432(368.6456), Bit/dim 7.0206(best: 7.0801), Xent 2.0383, Loss 8.0398, Error 0.6924(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 51.4519(65.3980) | Bit/dim 7.0253(8.0486) | Xent 2.0456(2.1528) | Loss 21.5577(20.6477) | Error 0.7069(0.7770) Steps 0(0.00) | Grad Norm 6.9868(15.5542) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 53.1047(65.0292) | Bit/dim 7.0142(8.0176) | Xent 2.0494(2.1497) | Loss 18.0404(20.5695) | Error 0.7120(0.7751) Steps 0(0.00) | Grad Norm 3.8778(15.2039) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 49.8005(64.5724) | Bit/dim 6.9962(7.9869) | Xent 2.0363(2.1463) | Loss 17.9590(20.4911) | Error 0.7116(0.7732) Steps 0(0.00) | Grad Norm 3.7835(14.8613) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 49.3272(64.1150) | Bit/dim 6.9971(7.9572) | Xent 2.0468(2.1433) | Loss 17.9760(20.4157) | Error 0.7056(0.7712) Steps 0(0.00) | Grad Norm 5.6828(14.5859) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 56.1887(63.8772) | Bit/dim 6.9973(7.9284) | Xent 2.0314(2.1400) | Loss 17.9382(20.3414) | Error 0.7111(0.7694) Steps 0(0.00) | Grad Norm 11.1251(14.4821) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 59.3565(63.7416) | Bit/dim 6.9911(7.9003) | Xent 2.0427(2.1371) | Loss 18.0833(20.2736) | Error 0.7239(0.7680) Steps 0(0.00) | Grad Norm 20.1215(14.6513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 22.3184, Epoch Time 357.6463(368.3156), Bit/dim 6.9966(best: 7.0206), Xent 2.0735, Loss 8.0334, Error 0.7358(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 53.4237(63.4321) | Bit/dim 6.9836(7.8728) | Xent 2.0948(2.1358) | Loss 21.0577(20.2971) | Error 0.7495(0.7674) Steps 0(0.00) | Grad Norm 31.1487(15.1462) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 52.1534(63.0937) | Bit/dim 6.9689(7.8457) | Xent 2.0075(2.1319) | Loss 17.7740(20.2215) | Error 0.6879(0.7651) Steps 0(0.00) | Grad Norm 2.8083(14.7761) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 58.1818(62.9464) | Bit/dim 6.9728(7.8195) | Xent 2.0750(2.1302) | Loss 18.0226(20.1555) | Error 0.7485(0.7646) Steps 0(0.00) | Grad Norm 32.3713(15.3039) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 56.5914(62.7557) | Bit/dim 6.9786(7.7943) | Xent 2.0286(2.1272) | Loss 17.8781(20.0872) | Error 0.7112(0.7630) Steps 0(0.00) | Grad Norm 23.0778(15.5372) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 57.4520(62.5966) | Bit/dim 6.9648(7.7694) | Xent 2.0160(2.1239) | Loss 17.7758(20.0178) | Error 0.6927(0.7609) Steps 0(0.00) | Grad Norm 8.1173(15.3146) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 58.4165(62.4712) | Bit/dim 6.9501(7.7448) | Xent 1.9961(2.1200) | Loss 17.8668(19.9533) | Error 0.6753(0.7583) Steps 0(0.00) | Grad Norm 5.8085(15.0294) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 22.0962, Epoch Time 374.0735(368.4883), Bit/dim 6.9498(best: 6.9966), Xent 2.0137, Loss 7.9566, Error 0.6994(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 57.9645(62.3360) | Bit/dim 6.9513(7.7210) | Xent 2.0171(2.1169) | Loss 21.0571(19.9864) | Error 0.7144(0.7570) Steps 0(0.00) | Grad Norm 22.5189(15.2541) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 57.4027(62.1880) | Bit/dim 6.9496(7.6979) | Xent 2.0685(2.1155) | Loss 17.9388(19.9250) | Error 0.7408(0.7565) Steps 0(0.00) | Grad Norm 37.2713(15.9146) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 56.6102(62.0207) | Bit/dim 6.9237(7.6746) | Xent 2.0010(2.1120) | Loss 17.8230(19.8619) | Error 0.6771(0.7541) Steps 0(0.00) | Grad Norm 7.3614(15.6580) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 57.2162(61.8765) | Bit/dim 6.9386(7.6526) | Xent 2.1356(2.1128) | Loss 17.7638(19.7990) | Error 0.7451(0.7538) Steps 0(0.00) | Grad Norm 49.3806(16.6697) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 55.3462(61.6806) | Bit/dim 6.9131(7.6304) | Xent 2.0005(2.1094) | Loss 17.6589(19.7348) | Error 0.6916(0.7520) Steps 0(0.00) | Grad Norm 19.2658(16.7475) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 53.3260(61.4300) | Bit/dim 6.9869(7.6111) | Xent 2.4304(2.1190) | Loss 18.3536(19.6933) | Error 0.8103(0.7537) Steps 0(0.00) | Grad Norm 82.8843(18.7316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 22.3288, Epoch Time 376.1473(368.7181), Bit/dim 7.0241(best: 6.9498), Xent 2.4883, Loss 8.2683, Error 0.8189(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 55.2925(61.2459) | Bit/dim 7.0147(7.5932) | Xent 2.5147(2.1309) | Loss 21.7142(19.7540) | Error 0.8266(0.7559) Steps 0(0.00) | Grad Norm 88.6587(20.8295) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 56.4996(61.1035) | Bit/dim 6.8947(7.5722) | Xent 2.0192(2.1275) | Loss 17.6658(19.6913) | Error 0.7074(0.7544) Steps 0(0.00) | Grad Norm 27.6312(21.0335) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 53.6447(60.8797) | Bit/dim 7.0199(7.5557) | Xent 2.4280(2.1366) | Loss 18.3216(19.6502) | Error 0.8121(0.7562) Steps 0(0.00) | Grad Norm 73.8644(22.6184) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 52.0276(60.6141) | Bit/dim 7.1353(7.5430) | Xent 2.6257(2.1512) | Loss 18.8136(19.6251) | Error 0.8014(0.7575) Steps 0(0.00) | Grad Norm 85.1516(24.4944) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 52.4679(60.3698) | Bit/dim 6.9687(7.5258) | Xent 2.2826(2.1552) | Loss 18.0259(19.5772) | Error 0.7841(0.7583) Steps 0(0.00) | Grad Norm 54.7957(25.4035) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 54.7794(60.2020) | Bit/dim 6.8776(7.5064) | Xent 1.9878(2.1501) | Loss 17.7113(19.5212) | Error 0.6904(0.7563) Steps 0(0.00) | Grad Norm 8.6548(24.9010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.9557, Epoch Time 362.9228(368.5443), Bit/dim 6.9266(best: 6.9498), Xent 2.1082, Loss 7.9807, Error 0.7769(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 51.6317(59.9449) | Bit/dim 6.9271(7.4890) | Xent 2.1239(2.1494) | Loss 21.0576(19.5673) | Error 0.7981(0.7575) Steps 0(0.00) | Grad Norm 33.5717(25.1611) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 50.5796(59.6640) | Bit/dim 6.9445(7.4727) | Xent 2.0583(2.1466) | Loss 17.8588(19.5160) | Error 0.7509(0.7573) Steps 0(0.00) | Grad Norm 25.6418(25.1755) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 53.9098(59.4913) | Bit/dim 6.8764(7.4548) | Xent 2.0910(2.1450) | Loss 17.7040(19.4617) | Error 0.7502(0.7571) Steps 0(0.00) | Grad Norm 19.8617(25.0161) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 52.6151(59.2851) | Bit/dim 6.9100(7.4384) | Xent 2.1162(2.1441) | Loss 17.6697(19.4079) | Error 0.7499(0.7569) Steps 0(0.00) | Grad Norm 26.5022(25.0607) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 47.3772(58.9278) | Bit/dim 6.8315(7.4202) | Xent 2.0461(2.1412) | Loss 17.4938(19.3505) | Error 0.7259(0.7560) Steps 0(0.00) | Grad Norm 12.3346(24.6789) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 54.0605(58.7818) | Bit/dim 6.8992(7.4046) | Xent 2.2416(2.1442) | Loss 17.9542(19.3086) | Error 0.8014(0.7573) Steps 0(0.00) | Grad Norm 35.2488(24.9960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 22.0786, Epoch Time 347.7948(367.9218), Bit/dim 6.9107(best: 6.9266), Xent 2.0400, Loss 7.9307, Error 0.7212(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 49.8021(58.5124) | Bit/dim 6.9129(7.3898) | Xent 2.0382(2.1410) | Loss 21.0508(19.3608) | Error 0.7226(0.7563) Steps 0(0.00) | Grad Norm 14.0928(24.6689) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 48.2535(58.2046) | Bit/dim 6.8158(7.3726) | Xent 2.2111(2.1431) | Loss 17.8101(19.3143) | Error 0.7746(0.7569) Steps 0(0.00) | Grad Norm 23.6608(24.6387) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 49.0564(57.9302) | Bit/dim 6.8165(7.3559) | Xent 2.0877(2.1414) | Loss 17.5503(19.2614) | Error 0.7355(0.7562) Steps 0(0.00) | Grad Norm 14.8808(24.3459) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 51.1050(57.7254) | Bit/dim 6.8175(7.3398) | Xent 2.1652(2.1421) | Loss 17.6271(19.2124) | Error 0.7923(0.7573) Steps 0(0.00) | Grad Norm 24.3392(24.3457) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 47.1096(57.4070) | Bit/dim 6.7866(7.3232) | Xent 2.1097(2.1412) | Loss 17.3535(19.1566) | Error 0.7498(0.7571) Steps 0(0.00) | Grad Norm 15.2057(24.0715) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 49.4253(57.1675) | Bit/dim 6.7637(7.3064) | Xent 2.0644(2.1389) | Loss 17.6011(19.1099) | Error 0.7192(0.7559) Steps 0(0.00) | Grad Norm 12.0408(23.7106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 21.3565, Epoch Time 332.0255(366.8449), Bit/dim 6.7189(best: 6.9107), Xent 2.0727, Loss 7.7552, Error 0.7264(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 51.6937(57.0033) | Bit/dim 6.7206(7.2888) | Xent 2.0731(2.1369) | Loss 20.5643(19.1536) | Error 0.7223(0.7549) Steps 0(0.00) | Grad Norm 11.5852(23.3469) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 47.2934(56.7120) | Bit/dim 6.7279(7.2720) | Xent 2.0174(2.1333) | Loss 17.2991(19.0979) | Error 0.6866(0.7529) Steps 0(0.00) | Grad Norm 8.1088(22.8897) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 50.0002(56.5107) | Bit/dim 6.6966(7.2547) | Xent 2.0249(2.1301) | Loss 17.2978(19.0439) | Error 0.7129(0.7517) Steps 0(0.00) | Grad Norm 12.4948(22.5779) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 48.5473(56.2718) | Bit/dim 6.6447(7.2364) | Xent 2.0508(2.1277) | Loss 17.1322(18.9866) | Error 0.7255(0.7509) Steps 0(0.00) | Grad Norm 15.6430(22.3698) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 48.7385(56.0458) | Bit/dim 6.6053(7.2175) | Xent 1.9996(2.1238) | Loss 17.0733(18.9292) | Error 0.6669(0.7484) Steps 0(0.00) | Grad Norm 5.4479(21.8622) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 46.8132(55.7688) | Bit/dim 6.5944(7.1988) | Xent 2.0696(2.1222) | Loss 17.0927(18.8741) | Error 0.7395(0.7481) Steps 0(0.00) | Grad Norm 22.3171(21.8758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 21.0428, Epoch Time 329.8376(365.7347), Bit/dim 6.5298(best: 6.7189), Xent 1.9944, Loss 7.5270, Error 0.6644(best: 0.6733)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 48.7512(55.5583) | Bit/dim 6.5301(7.1787) | Xent 1.9980(2.1185) | Loss 20.0751(18.9101) | Error 0.6740(0.7459) Steps 0(0.00) | Grad Norm 9.1734(21.4947) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 44.3471(55.2219) | Bit/dim 6.5070(7.1586) | Xent 2.0194(2.1155) | Loss 16.8088(18.8471) | Error 0.7114(0.7448) Steps 0(0.00) | Grad Norm 27.5858(21.6775) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 44.7239(54.9070) | Bit/dim 6.4418(7.1371) | Xent 2.0119(2.1124) | Loss 16.5951(18.7795) | Error 0.6935(0.7433) Steps 0(0.00) | Grad Norm 18.7855(21.5907) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 50.9721(54.7889) | Bit/dim 6.4161(7.1155) | Xent 2.0873(2.1116) | Loss 16.6994(18.7171) | Error 0.7336(0.7430) Steps 0(0.00) | Grad Norm 46.0889(22.3257) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 48.5805(54.6027) | Bit/dim 6.3396(7.0922) | Xent 2.0941(2.1111) | Loss 16.5464(18.6520) | Error 0.7478(0.7432) Steps 0(0.00) | Grad Norm 44.4264(22.9887) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 48.0499(54.4061) | Bit/dim 6.3254(7.0692) | Xent 2.0241(2.1085) | Loss 16.4703(18.5865) | Error 0.7380(0.7430) Steps 0(0.00) | Grad Norm 37.4893(23.4237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 20.9391, Epoch Time 322.4321(364.4356), Bit/dim 6.2123(best: 6.5298), Xent 2.0032, Loss 7.2139, Error 0.7030(best: 0.6644)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 49.2117(54.2503) | Bit/dim 6.2053(7.0433) | Xent 2.0096(2.1055) | Loss 19.4034(18.6110) | Error 0.7117(0.7421) Steps 0(0.00) | Grad Norm 30.9162(23.6485) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 44.4958(53.9576) | Bit/dim 6.3043(7.0211) | Xent 2.0935(2.1052) | Loss 16.3394(18.5429) | Error 0.7305(0.7417) Steps 0(0.00) | Grad Norm 78.6592(25.2988) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 47.2889(53.7576) | Bit/dim 6.2182(6.9970) | Xent 2.0783(2.1044) | Loss 16.2730(18.4748) | Error 0.7219(0.7411) Steps 0(0.00) | Grad Norm 74.9007(26.7869) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 45.5680(53.5119) | Bit/dim 6.1060(6.9703) | Xent 2.0116(2.1016) | Loss 15.8843(18.3971) | Error 0.7166(0.7404) Steps 0(0.00) | Grad Norm 43.9879(27.3029) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 44.2945(53.2354) | Bit/dim 6.0408(6.9424) | Xent 2.0121(2.0989) | Loss 15.7923(18.3189) | Error 0.7117(0.7395) Steps 0(0.00) | Grad Norm 38.0255(27.6246) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 46.7620(53.0412) | Bit/dim 6.0631(6.9160) | Xent 2.0351(2.0970) | Loss 15.9055(18.2465) | Error 0.7095(0.7386) Steps 0(0.00) | Grad Norm 55.9055(28.4730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 20.2828, Epoch Time 313.7828(362.9160), Bit/dim 6.0211(best: 6.2123), Xent 1.9987, Loss 7.0205, Error 0.6828(best: 0.6644)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 47.3322(52.8699) | Bit/dim 6.0065(6.8887) | Xent 2.0216(2.0947) | Loss 18.9433(18.2674) | Error 0.6987(0.7374) Steps 0(0.00) | Grad Norm 40.7613(28.8416) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 48.8554(52.7495) | Bit/dim 5.9625(6.8609) | Xent 2.0080(2.0921) | Loss 15.6150(18.1879) | Error 0.6947(0.7361) Steps 0(0.00) | Grad Norm 39.1161(29.1499) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 47.1078(52.5802) | Bit/dim 5.9543(6.8337) | Xent 2.0056(2.0895) | Loss 15.5901(18.1099) | Error 0.6932(0.7349) Steps 0(0.00) | Grad Norm 29.4847(29.1599) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 44.3123(52.3322) | Bit/dim 5.8925(6.8055) | Xent 2.0406(2.0881) | Loss 15.5758(18.0339) | Error 0.7286(0.7347) Steps 0(0.00) | Grad Norm 39.3169(29.4646) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 45.6430(52.1315) | Bit/dim 5.8751(6.7776) | Xent 2.0271(2.0862) | Loss 15.5865(17.9605) | Error 0.7239(0.7344) Steps 0(0.00) | Grad Norm 27.6174(29.4092) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 43.6709(51.8777) | Bit/dim 5.8595(6.7501) | Xent 2.0007(2.0837) | Loss 15.3371(17.8818) | Error 0.6877(0.7330) Steps 0(0.00) | Grad Norm 33.1256(29.5207) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 20.3130, Epoch Time 313.0826(361.4210), Bit/dim 5.7952(best: 6.0211), Xent 2.0002, Loss 6.7953, Error 0.7109(best: 0.6644)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 45.1360(51.6754) | Bit/dim 5.7761(6.7208) | Xent 2.0200(2.0818) | Loss 18.5474(17.9018) | Error 0.7157(0.7324) Steps 0(0.00) | Grad Norm 20.0680(29.2371) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 44.0326(51.4461) | Bit/dim 5.8470(6.6946) | Xent 2.0356(2.0804) | Loss 15.4595(17.8285) | Error 0.7205(0.7321) Steps 0(0.00) | Grad Norm 29.5791(29.2474) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 43.4861(51.2073) | Bit/dim 5.7994(6.6678) | Xent 2.0342(2.0790) | Loss 15.2442(17.7510) | Error 0.7140(0.7315) Steps 0(0.00) | Grad Norm 14.3958(28.8018) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 43.3271(50.9709) | Bit/dim 5.7330(6.6397) | Xent 2.0066(2.0768) | Loss 15.0195(17.6690) | Error 0.6987(0.7306) Steps 0(0.00) | Grad Norm 12.6156(28.3162) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 43.2975(50.7407) | Bit/dim 5.8195(6.6151) | Xent 2.0124(2.0749) | Loss 15.3254(17.5987) | Error 0.7019(0.7297) Steps 0(0.00) | Grad Norm 25.5571(28.2335) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 46.2713(50.6066) | Bit/dim 5.7008(6.5877) | Xent 2.0173(2.0732) | Loss 15.1406(17.5250) | Error 0.7164(0.7293) Steps 0(0.00) | Grad Norm 27.6746(28.2167) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 19.7836, Epoch Time 300.8608(359.6042), Bit/dim 5.7320(best: 5.7952), Xent 2.0007, Loss 6.7324, Error 0.7014(best: 0.6644)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 45.1503(50.4430) | Bit/dim 5.7336(6.5621) | Xent 2.0212(2.0716) | Loss 18.2718(17.5474) | Error 0.7139(0.7288) Steps 0(0.00) | Grad Norm 21.5638(28.0171) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 44.6114(50.2680) | Bit/dim 5.6622(6.5351) | Xent 1.9905(2.0692) | Loss 14.9953(17.4708) | Error 0.6763(0.7273) Steps 0(0.00) | Grad Norm 9.7599(27.4694) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 44.9893(50.1096) | Bit/dim 5.6437(6.5083) | Xent 1.9827(2.0666) | Loss 15.0859(17.3993) | Error 0.6694(0.7255) Steps 0(0.00) | Grad Norm 4.8452(26.7907) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 43.6138(49.9148) | Bit/dim 5.6483(6.4825) | Xent 1.9756(2.0638) | Loss 14.9701(17.3264) | Error 0.6776(0.7241) Steps 0(0.00) | Grad Norm 10.7260(26.3087) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 42.3227(49.6870) | Bit/dim 5.6166(6.4565) | Xent 2.0310(2.0629) | Loss 14.9969(17.2565) | Error 0.7185(0.7239) Steps 0(0.00) | Grad Norm 27.5269(26.3453) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 43.9827(49.5159) | Bit/dim 5.6235(6.4316) | Xent 2.0539(2.0626) | Loss 15.0479(17.1902) | Error 0.7314(0.7241) Steps 0(0.00) | Grad Norm 40.6668(26.7749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 19.8952, Epoch Time 300.5659(357.8330), Bit/dim 5.5945(best: 5.7320), Xent 1.9476, Loss 6.5683, Error 0.6587(best: 0.6644)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 47.5798(49.4578) | Bit/dim 5.5846(6.4061) | Xent 1.9715(2.0599) | Loss 17.8446(17.2099) | Error 0.6780(0.7228) Steps 0(0.00) | Grad Norm 8.0950(26.2145) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 42.9217(49.2617) | Bit/dim 5.5964(6.3819) | Xent 1.9495(2.0565) | Loss 14.7376(17.1357) | Error 0.6626(0.7209) Steps 0(0.00) | Grad Norm 7.9635(25.6670) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 47.7753(49.2171) | Bit/dim 5.5624(6.3573) | Xent 1.9869(2.0545) | Loss 14.7805(17.0650) | Error 0.6875(0.7199) Steps 0(0.00) | Grad Norm 23.3250(25.5967) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 43.0213(49.0312) | Bit/dim 5.5649(6.3335) | Xent 2.0264(2.0536) | Loss 14.8014(16.9971) | Error 0.7044(0.7195) Steps 0(0.00) | Grad Norm 34.8256(25.8736) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 44.2357(48.8874) | Bit/dim 5.5217(6.3091) | Xent 1.9473(2.0504) | Loss 14.4500(16.9207) | Error 0.6749(0.7181) Steps 0(0.00) | Grad Norm 7.1254(25.3112) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 44.6318(48.7597) | Bit/dim 5.5133(6.2853) | Xent 2.0222(2.0496) | Loss 14.5908(16.8508) | Error 0.7064(0.7178) Steps 0(0.00) | Grad Norm 33.6589(25.5616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 19.8393, Epoch Time 306.3847(356.2896), Bit/dim 5.4821(best: 5.5945), Xent 1.9315, Loss 6.4478, Error 0.6533(best: 0.6587)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 43.1065(48.5901) | Bit/dim 5.4874(6.2613) | Xent 1.9516(2.0466) | Loss 17.8309(16.8802) | Error 0.6731(0.7164) Steps 0(0.00) | Grad Norm 14.0519(25.2163) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 42.5373(48.4085) | Bit/dim 5.5070(6.2387) | Xent 2.0150(2.0457) | Loss 14.6732(16.8140) | Error 0.7190(0.7165) Steps 0(0.00) | Grad Norm 38.7080(25.6211) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 43.8235(48.2710) | Bit/dim 5.5310(6.2175) | Xent 2.0035(2.0444) | Loss 14.7503(16.7521) | Error 0.7125(0.7164) Steps 0(0.00) | Grad Norm 32.3352(25.8225) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 42.0563(48.0845) | Bit/dim 5.4525(6.1945) | Xent 1.9341(2.0411) | Loss 14.4760(16.6838) | Error 0.6636(0.7148) Steps 0(0.00) | Grad Norm 6.3126(25.2372) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 42.9623(47.9309) | Bit/dim 5.5364(6.1748) | Xent 2.1496(2.0444) | Loss 14.7743(16.6265) | Error 0.7639(0.7163) Steps 0(0.00) | Grad Norm 60.0797(26.2825) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 40.5574(47.7097) | Bit/dim 5.3972(6.1515) | Xent 1.9495(2.0415) | Loss 14.3989(16.5597) | Error 0.6653(0.7148) Steps 0(0.00) | Grad Norm 6.0608(25.6758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 19.3662, Epoch Time 290.1736(354.3061), Bit/dim 5.4385(best: 5.4821), Xent 1.9806, Loss 6.4288, Error 0.7054(best: 0.6533)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 42.8016(47.5624) | Bit/dim 5.4359(6.1300) | Xent 1.9891(2.0399) | Loss 17.5796(16.5903) | Error 0.7149(0.7148) Steps 0(0.00) | Grad Norm 25.7722(25.6787) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 43.6543(47.4452) | Bit/dim 5.3698(6.1072) | Xent 1.9708(2.0379) | Loss 14.3255(16.5224) | Error 0.6684(0.7134) Steps 0(0.00) | Grad Norm 10.3546(25.2190) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 43.7909(47.3356) | Bit/dim 5.3754(6.0852) | Xent 1.9939(2.0366) | Loss 14.4119(16.4590) | Error 0.7039(0.7131) Steps 0(0.00) | Grad Norm 17.7728(24.9956) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 44.5983(47.2534) | Bit/dim 5.3985(6.0646) | Xent 1.9489(2.0339) | Loss 14.4834(16.3998) | Error 0.6640(0.7116) Steps 0(0.00) | Grad Norm 13.9392(24.6639) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 43.5686(47.1429) | Bit/dim 5.3523(6.0433) | Xent 1.9717(2.0321) | Loss 14.3540(16.3384) | Error 0.6810(0.7107) Steps 0(0.00) | Grad Norm 12.4704(24.2981) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 46.6921(47.1294) | Bit/dim 5.3531(6.0226) | Xent 1.9813(2.0305) | Loss 14.3504(16.2788) | Error 0.6925(0.7101) Steps 0(0.00) | Grad Norm 24.9939(24.3190) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 20.4645, Epoch Time 301.6012(352.7250), Bit/dim 5.2936(best: 5.4385), Xent 1.9706, Loss 6.2790, Error 0.6925(best: 0.6533)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 47.1767(47.1308) | Bit/dim 5.2938(6.0007) | Xent 2.0074(2.0298) | Loss 17.2894(16.3091) | Error 0.7104(0.7102) Steps 0(0.00) | Grad Norm 25.2994(24.3484) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 42.9259(47.0046) | Bit/dim 5.3114(5.9800) | Xent 1.9516(2.0275) | Loss 14.0725(16.2420) | Error 0.6774(0.7092) Steps 0(0.00) | Grad Norm 19.8739(24.2141) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 44.8814(46.9409) | Bit/dim 5.2543(5.9582) | Xent 1.9304(2.0246) | Loss 13.9907(16.1744) | Error 0.6660(0.7079) Steps 0(0.00) | Grad Norm 2.8856(23.5743) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 47.2388(46.9499) | Bit/dim 5.3089(5.9388) | Xent 1.9650(2.0228) | Loss 14.2754(16.1175) | Error 0.6889(0.7073) Steps 0(0.00) | Grad Norm 25.2658(23.6250) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 44.8548(46.8870) | Bit/dim 5.2533(5.9182) | Xent 1.9343(2.0201) | Loss 14.0506(16.0555) | Error 0.6761(0.7064) Steps 0(0.00) | Grad Norm 19.3546(23.4969) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 46.4823(46.8749) | Bit/dim 5.2539(5.8983) | Xent 1.9332(2.0175) | Loss 14.0357(15.9949) | Error 0.6649(0.7051) Steps 0(0.00) | Grad Norm 13.5570(23.1987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 20.5581, Epoch Time 309.5771(351.4305), Bit/dim 5.2046(best: 5.2936), Xent 1.8921, Loss 6.1507, Error 0.6416(best: 0.6533)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 51.0105(46.9990) | Bit/dim 5.2261(5.8781) | Xent 1.9057(2.0142) | Loss 17.0774(16.0273) | Error 0.6615(0.7038) Steps 0(0.00) | Grad Norm 8.6569(22.7625) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 47.1993(47.0050) | Bit/dim 5.1871(5.8574) | Xent 1.9272(2.0116) | Loss 13.8833(15.9630) | Error 0.6733(0.7029) Steps 0(0.00) | Grad Norm 15.5078(22.5448) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 47.2962(47.0137) | Bit/dim 5.1948(5.8375) | Xent 1.9748(2.0105) | Loss 14.0239(15.9049) | Error 0.7003(0.7028) Steps 0(0.00) | Grad Norm 18.5506(22.4250) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 47.9927(47.0431) | Bit/dim 5.1606(5.8172) | Xent 1.9278(2.0080) | Loss 13.9044(15.8448) | Error 0.6701(0.7018) Steps 0(0.00) | Grad Norm 5.3914(21.9140) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 48.2348(47.0788) | Bit/dim 5.1355(5.7967) | Xent 1.9447(2.0061) | Loss 13.8092(15.7838) | Error 0.6943(0.7016) Steps 0(0.00) | Grad Norm 15.8585(21.7323) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 47.8526(47.1020) | Bit/dim 5.1516(5.7774) | Xent 1.9639(2.0048) | Loss 13.8534(15.7259) | Error 0.6886(0.7012) Steps 0(0.00) | Grad Norm 16.0297(21.5612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 20.8052, Epoch Time 326.0715(350.6698), Bit/dim 5.1060(best: 5.2046), Xent 1.9033, Loss 6.0577, Error 0.6401(best: 0.6416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 47.5726(47.1162) | Bit/dim 5.1133(5.7575) | Xent 1.9158(2.0021) | Loss 17.3048(15.7732) | Error 0.6643(0.7001) Steps 0(0.00) | Grad Norm 7.9301(21.1523) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 47.3718(47.1238) | Bit/dim 5.1195(5.7383) | Xent 1.9568(2.0008) | Loss 13.6519(15.7096) | Error 0.6983(0.7001) Steps 0(0.00) | Grad Norm 19.4519(21.1013) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 51.3106(47.2494) | Bit/dim 5.1467(5.7206) | Xent 1.9273(1.9986) | Loss 13.8914(15.6550) | Error 0.6634(0.6990) Steps 0(0.00) | Grad Norm 13.4287(20.8711) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 50.0758(47.3342) | Bit/dim 5.0902(5.7017) | Xent 1.9610(1.9975) | Loss 13.8023(15.5995) | Error 0.6947(0.6988) Steps 0(0.00) | Grad Norm 20.5414(20.8612) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 47.1327(47.3282) | Bit/dim 5.0883(5.6833) | Xent 1.9503(1.9960) | Loss 13.7769(15.5448) | Error 0.6786(0.6982) Steps 0(0.00) | Grad Norm 17.6974(20.7663) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 45.7990(47.2823) | Bit/dim 5.0725(5.6649) | Xent 1.9592(1.9949) | Loss 13.7759(15.4917) | Error 0.7040(0.6984) Steps 0(0.00) | Grad Norm 16.6524(20.6429) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 20.3616, Epoch Time 325.6198(349.9183), Bit/dim 5.1177(best: 5.1060), Xent 1.8927, Loss 6.0640, Error 0.6428(best: 0.6401)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 47.2758(47.2821) | Bit/dim 5.1165(5.6485) | Xent 1.9078(1.9923) | Loss 16.7393(15.5291) | Error 0.6525(0.6970) Steps 0(0.00) | Grad Norm 18.5354(20.5797) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 49.0813(47.3361) | Bit/dim 5.1145(5.6325) | Xent 1.9172(1.9901) | Loss 13.7637(15.4762) | Error 0.6738(0.6963) Steps 0(0.00) | Grad Norm 27.9697(20.8014) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 45.4142(47.2784) | Bit/dim 5.0527(5.6151) | Xent 1.8994(1.9873) | Loss 13.5705(15.4190) | Error 0.6665(0.6954) Steps 0(0.00) | Grad Norm 12.9137(20.5647) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 49.4651(47.3440) | Bit/dim 5.0314(5.5976) | Xent 1.8715(1.9839) | Loss 13.5084(15.3617) | Error 0.6516(0.6941) Steps 0(0.00) | Grad Norm 4.8233(20.0925) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 50.0148(47.4241) | Bit/dim 5.0362(5.5807) | Xent 1.9298(1.9823) | Loss 13.6360(15.3099) | Error 0.6798(0.6937) Steps 0(0.00) | Grad Norm 16.2720(19.9779) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 47.1990(47.4174) | Bit/dim 5.0300(5.5642) | Xent 1.9048(1.9799) | Loss 13.5894(15.2583) | Error 0.6658(0.6928) Steps 0(0.00) | Grad Norm 13.3266(19.7783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 21.5378, Epoch Time 325.9924(349.2005), Bit/dim 5.0048(best: 5.1060), Xent 1.8677, Loss 5.9386, Error 0.6386(best: 0.6401)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 46.4390(47.3880) | Bit/dim 5.0127(5.5477) | Xent 1.9086(1.9778) | Loss 16.6481(15.3000) | Error 0.6655(0.6920) Steps 0(0.00) | Grad Norm 7.3513(19.4055) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 46.8352(47.3715) | Bit/dim 4.9784(5.5306) | Xent 1.8690(1.9745) | Loss 13.4488(15.2445) | Error 0.6491(0.6907) Steps 0(0.00) | Grad Norm 4.0873(18.9460) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 46.9087(47.3576) | Bit/dim 4.9971(5.5146) | Xent 1.8734(1.9715) | Loss 13.4394(15.1903) | Error 0.6530(0.6896) Steps 0(0.00) | Grad Norm 5.6697(18.5477) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 50.6089(47.4551) | Bit/dim 4.9636(5.4981) | Xent 1.8738(1.9686) | Loss 13.3989(15.1366) | Error 0.6558(0.6886) Steps 0(0.00) | Grad Norm 5.7725(18.1644) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 53.6831(47.6420) | Bit/dim 4.9569(5.4818) | Xent 1.8771(1.9658) | Loss 13.4122(15.0848) | Error 0.6515(0.6875) Steps 0(0.00) | Grad Norm 7.1789(17.8349) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 48.3512(47.6632) | Bit/dim 4.9619(5.4662) | Xent 1.8795(1.9632) | Loss 13.4158(15.0348) | Error 0.6555(0.6865) Steps 0(0.00) | Grad Norm 14.3214(17.7295) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 21.5415, Epoch Time 330.0292(348.6253), Bit/dim 4.9400(best: 5.0048), Xent 1.8844, Loss 5.8822, Error 0.6635(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 51.2665(47.7713) | Bit/dim 4.9493(5.4507) | Xent 1.9204(1.9619) | Loss 16.5745(15.0810) | Error 0.6789(0.6863) Steps 0(0.00) | Grad Norm 22.5346(17.8736) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 49.1620(47.8131) | Bit/dim 4.9760(5.4365) | Xent 1.9457(1.9615) | Loss 13.4412(15.0318) | Error 0.6813(0.6861) Steps 0(0.00) | Grad Norm 29.3739(18.2186) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 51.8789(47.9350) | Bit/dim 4.9189(5.4209) | Xent 1.8621(1.9585) | Loss 13.1657(14.9758) | Error 0.6479(0.6850) Steps 0(0.00) | Grad Norm 5.9554(17.8507) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 51.6674(48.0470) | Bit/dim 4.9319(5.4063) | Xent 1.9352(1.9578) | Loss 13.4771(14.9308) | Error 0.6864(0.6850) Steps 0(0.00) | Grad Norm 26.8657(18.1212) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 45.8136(47.9800) | Bit/dim 4.9666(5.3931) | Xent 1.9437(1.9574) | Loss 13.4944(14.8877) | Error 0.6894(0.6852) Steps 0(0.00) | Grad Norm 31.3596(18.5183) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 47.0303(47.9515) | Bit/dim 4.9220(5.3790) | Xent 1.8865(1.9552) | Loss 13.2733(14.8393) | Error 0.6681(0.6847) Steps 0(0.00) | Grad Norm 18.2355(18.5099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 21.8203, Epoch Time 334.7314(348.2085), Bit/dim 4.9614(best: 4.9400), Xent 2.0195, Loss 5.9712, Error 0.7085(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 53.2225(48.1096) | Bit/dim 4.9688(5.3666) | Xent 2.0514(1.9581) | Loss 16.9643(14.9030) | Error 0.7248(0.6859) Steps 0(0.00) | Grad Norm 33.3941(18.9564) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 51.1356(48.2004) | Bit/dim 4.9383(5.3538) | Xent 1.9389(1.9575) | Loss 13.4806(14.8604) | Error 0.6886(0.6859) Steps 0(0.00) | Grad Norm 20.8029(19.0118) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 49.7335(48.2464) | Bit/dim 4.9196(5.3408) | Xent 1.9409(1.9570) | Loss 13.4800(14.8190) | Error 0.6890(0.6860) Steps 0(0.00) | Grad Norm 22.9319(19.1294) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 49.4326(48.2820) | Bit/dim 5.1311(5.3345) | Xent 2.1010(1.9614) | Loss 14.0042(14.7945) | Error 0.7431(0.6877) Steps 0(0.00) | Grad Norm 40.8984(19.7825) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 50.4825(48.3480) | Bit/dim 5.0174(5.3250) | Xent 1.9510(1.9610) | Loss 13.5241(14.7564) | Error 0.6905(0.6878) Steps 0(0.00) | Grad Norm 24.7451(19.9313) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 50.9097(48.4249) | Bit/dim 5.1297(5.3191) | Xent 2.3220(1.9719) | Loss 14.3120(14.7431) | Error 0.7748(0.6904) Steps 0(0.00) | Grad Norm 59.3865(21.1150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 21.5347, Epoch Time 342.2303(348.0292), Bit/dim 5.0141(best: 4.9400), Xent 2.0585, Loss 6.0433, Error 0.7249(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 51.5275(48.5179) | Bit/dim 5.0179(5.3101) | Xent 2.0898(1.9754) | Loss 16.9542(14.8094) | Error 0.7324(0.6917) Steps 0(0.00) | Grad Norm 30.8661(21.4075) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 53.2227(48.6591) | Bit/dim 5.0181(5.3013) | Xent 2.0134(1.9766) | Loss 13.7798(14.7785) | Error 0.7256(0.6927) Steps 0(0.00) | Grad Norm 14.3716(21.1964) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 52.9721(48.7885) | Bit/dim 4.9758(5.2915) | Xent 2.0725(1.9794) | Loss 13.6944(14.7460) | Error 0.7511(0.6945) Steps 0(0.00) | Grad Norm 12.3710(20.9317) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 51.7942(48.8786) | Bit/dim 5.0210(5.2834) | Xent 2.1081(1.9833) | Loss 13.7893(14.7173) | Error 0.7518(0.6962) Steps 0(0.00) | Grad Norm 19.2745(20.8820) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 48.5210(48.8679) | Bit/dim 4.9717(5.2741) | Xent 2.0236(1.9845) | Loss 13.5735(14.6830) | Error 0.7124(0.6967) Steps 0(0.00) | Grad Norm 8.0298(20.4964) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 53.1387(48.9960) | Bit/dim 5.0251(5.2666) | Xent 2.0249(1.9857) | Loss 13.6949(14.6533) | Error 0.7095(0.6971) Steps 0(0.00) | Grad Norm 6.4675(20.0755) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 20.9815, Epoch Time 347.8117(348.0227), Bit/dim 5.0154(best: 4.9400), Xent 2.0168, Loss 6.0238, Error 0.7142(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 48.4379(48.9793) | Bit/dim 5.0146(5.2590) | Xent 2.0520(1.9877) | Loss 16.9061(14.7209) | Error 0.7314(0.6981) Steps 0(0.00) | Grad Norm 9.6222(19.7619) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 49.5642(48.9968) | Bit/dim 4.9764(5.2506) | Xent 2.0610(1.9899) | Loss 13.5800(14.6867) | Error 0.7303(0.6990) Steps 0(0.00) | Grad Norm 12.8752(19.5553) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 48.5684(48.9840) | Bit/dim 4.8878(5.2397) | Xent 2.0221(1.9909) | Loss 13.3966(14.6480) | Error 0.7177(0.6996) Steps 0(0.00) | Grad Norm 8.4919(19.2234) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 47.7855(48.9480) | Bit/dim 4.9027(5.2296) | Xent 2.0332(1.9921) | Loss 13.3787(14.6099) | Error 0.7243(0.7003) Steps 0(0.00) | Grad Norm 7.4716(18.8709) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 49.9413(48.9778) | Bit/dim 4.9085(5.2199) | Xent 2.0216(1.9930) | Loss 13.4527(14.5752) | Error 0.7165(0.7008) Steps 0(0.00) | Grad Norm 7.6264(18.5335) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 48.2849(48.9570) | Bit/dim 4.9008(5.2104) | Xent 1.9692(1.9923) | Loss 13.3952(14.5398) | Error 0.6963(0.7007) Steps 0(0.00) | Grad Norm 7.6804(18.2079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 20.8311, Epoch Time 329.7832(347.4755), Bit/dim 4.8701(best: 4.9400), Xent 1.9356, Loss 5.8379, Error 0.6846(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 50.8535(49.0139) | Bit/dim 4.8743(5.2003) | Xent 1.9601(1.9913) | Loss 16.3584(14.5944) | Error 0.6974(0.7006) Steps 0(0.00) | Grad Norm 6.7044(17.8628) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 50.6088(49.0618) | Bit/dim 4.8490(5.1898) | Xent 1.9492(1.9901) | Loss 13.3553(14.5572) | Error 0.6956(0.7004) Steps 0(0.00) | Grad Norm 7.7057(17.5581) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 50.6350(49.1090) | Bit/dim 4.8405(5.1793) | Xent 1.9321(1.9883) | Loss 13.2620(14.5183) | Error 0.6861(0.7000) Steps 0(0.00) | Grad Norm 7.8091(17.2657) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 50.3880(49.1473) | Bit/dim 4.8278(5.1687) | Xent 1.9410(1.9869) | Loss 13.2360(14.4799) | Error 0.6813(0.6995) Steps 0(0.00) | Grad Norm 4.1518(16.8722) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 50.6038(49.1910) | Bit/dim 4.8147(5.1581) | Xent 1.9349(1.9854) | Loss 13.2224(14.4421) | Error 0.6801(0.6989) Steps 0(0.00) | Grad Norm 7.2274(16.5829) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 49.9283(49.2132) | Bit/dim 4.8017(5.1474) | Xent 1.9056(1.9830) | Loss 13.1394(14.4031) | Error 0.6658(0.6979) Steps 0(0.00) | Grad Norm 6.7112(16.2867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 21.6874, Epoch Time 340.7325(347.2732), Bit/dim 4.7951(best: 4.8701), Xent 1.8501, Loss 5.7201, Error 0.6454(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 49.3733(49.2180) | Bit/dim 4.7910(5.1367) | Xent 1.9050(1.9806) | Loss 16.2171(14.4575) | Error 0.6693(0.6970) Steps 0(0.00) | Grad Norm 4.5986(15.9361) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 52.3465(49.3118) | Bit/dim 4.7880(5.1263) | Xent 1.8677(1.9772) | Loss 13.0196(14.4143) | Error 0.6519(0.6957) Steps 0(0.00) | Grad Norm 3.4150(15.5605) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 50.5592(49.3492) | Bit/dim 4.7730(5.1157) | Xent 1.8854(1.9745) | Loss 12.9478(14.3703) | Error 0.6669(0.6948) Steps 0(0.00) | Grad Norm 3.3981(15.1956) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 52.4247(49.4415) | Bit/dim 4.7617(5.1050) | Xent 1.8673(1.9713) | Loss 12.9785(14.3286) | Error 0.6587(0.6937) Steps 0(0.00) | Grad Norm 4.5783(14.8771) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 50.4893(49.4729) | Bit/dim 4.7490(5.0944) | Xent 1.8487(1.9676) | Loss 12.9472(14.2871) | Error 0.6554(0.6926) Steps 0(0.00) | Grad Norm 6.0406(14.6120) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 52.0422(49.5500) | Bit/dim 4.7536(5.0841) | Xent 1.8420(1.9638) | Loss 12.9817(14.2480) | Error 0.6451(0.6911) Steps 0(0.00) | Grad Norm 9.9136(14.4710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 21.0131, Epoch Time 344.2352(347.1820), Bit/dim 4.7429(best: 4.7951), Xent 1.8411, Loss 5.6634, Error 0.6367(best: 0.6386)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 51.7787(49.6169) | Bit/dim 4.7447(5.0740) | Xent 1.8809(1.9613) | Loss 16.2751(14.3088) | Error 0.6554(0.6901) Steps 0(0.00) | Grad Norm 18.7312(14.5988) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 47.9718(49.5675) | Bit/dim 4.7484(5.0642) | Xent 1.9909(1.9622) | Loss 13.0720(14.2717) | Error 0.7006(0.6904) Steps 0(0.00) | Grad Norm 34.2496(15.1884) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 49.3109(49.5598) | Bit/dim 4.7308(5.0542) | Xent 1.8303(1.9583) | Loss 12.9396(14.2317) | Error 0.6364(0.6888) Steps 0(0.00) | Grad Norm 4.5972(14.8706) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 49.2525(49.5506) | Bit/dim 4.7518(5.0451) | Xent 1.9734(1.9587) | Loss 13.0323(14.1957) | Error 0.6820(0.6886) Steps 0(0.00) | Grad Norm 28.4756(15.2788) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 49.0006(49.5341) | Bit/dim 4.7855(5.0373) | Xent 1.8723(1.9561) | Loss 12.9106(14.1572) | Error 0.6607(0.6877) Steps 0(0.00) | Grad Norm 15.4948(15.2853) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 49.9227(49.5458) | Bit/dim 4.7201(5.0278) | Xent 1.8896(1.9541) | Loss 12.7947(14.1163) | Error 0.6736(0.6873) Steps 0(0.00) | Grad Norm 18.7586(15.3895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 20.9511, Epoch Time 333.8499(346.7821), Bit/dim 4.7556(best: 4.7429), Xent 1.9525, Loss 5.7319, Error 0.6923(best: 0.6367)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.7968(49.4933) | Bit/dim 4.7531(5.0196) | Xent 1.9841(1.9550) | Loss 16.3140(14.1822) | Error 0.6927(0.6875) Steps 0(0.00) | Grad Norm 32.5550(15.9044) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 48.5575(49.4652) | Bit/dim 4.7121(5.0103) | Xent 1.9029(1.9535) | Loss 12.8921(14.1435) | Error 0.6707(0.6870) Steps 0(0.00) | Grad Norm 17.9243(15.9650) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 51.7139(49.5327) | Bit/dim 4.7118(5.0014) | Xent 1.9442(1.9532) | Loss 12.9623(14.1081) | Error 0.6843(0.6869) Steps 0(0.00) | Grad Norm 24.9818(16.2355) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 49.4498(49.5302) | Bit/dim 4.7038(4.9925) | Xent 1.8727(1.9508) | Loss 12.9396(14.0731) | Error 0.6618(0.6861) Steps 0(0.00) | Grad Norm 16.2573(16.2362) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 50.8139(49.5687) | Bit/dim 4.6993(4.9837) | Xent 1.8617(1.9481) | Loss 12.8954(14.0377) | Error 0.6570(0.6853) Steps 0(0.00) | Grad Norm 11.9132(16.1065) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 50.0130(49.5820) | Bit/dim 4.7476(4.9766) | Xent 1.9508(1.9482) | Loss 12.9965(14.0065) | Error 0.6992(0.6857) Steps 0(0.00) | Grad Norm 24.9855(16.3729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 21.5385, Epoch Time 335.6800(346.4490), Bit/dim 4.6901(best: 4.7429), Xent 1.9493, Loss 5.6648, Error 0.6947(best: 0.6367)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 53.1564(49.6893) | Bit/dim 4.6834(4.9678) | Xent 1.9747(1.9490) | Loss 16.1081(14.0695) | Error 0.6926(0.6859) Steps 0(0.00) | Grad Norm 27.0690(16.6937) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 50.9163(49.7261) | Bit/dim 4.6517(4.9583) | Xent 1.9103(1.9478) | Loss 12.7863(14.0310) | Error 0.6786(0.6857) Steps 0(0.00) | Grad Norm 13.6271(16.6017) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 50.8843(49.7608) | Bit/dim 4.6481(4.9490) | Xent 1.8733(1.9456) | Loss 12.8086(13.9944) | Error 0.6626(0.6850) Steps 0(0.00) | Grad Norm 7.5928(16.3315) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 48.5513(49.7245) | Bit/dim 4.6548(4.9402) | Xent 1.8735(1.9434) | Loss 12.7794(13.9579) | Error 0.6575(0.6842) Steps 0(0.00) | Grad Norm 5.9010(16.0186) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 48.6741(49.6930) | Bit/dim 4.6413(4.9312) | Xent 1.8743(1.9414) | Loss 12.7266(13.9210) | Error 0.6633(0.6835) Steps 0(0.00) | Grad Norm 6.9460(15.7464) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 50.0821(49.7047) | Bit/dim 4.6438(4.9226) | Xent 1.8456(1.9385) | Loss 12.6937(13.8842) | Error 0.6498(0.6825) Steps 0(0.00) | Grad Norm 6.1469(15.4584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 21.9261, Epoch Time 340.2107(346.2619), Bit/dim 4.6816(best: 4.6901), Xent 1.8406, Loss 5.6019, Error 0.6509(best: 0.6367)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 55.4045(49.8757) | Bit/dim 4.6826(4.9154) | Xent 1.8956(1.9372) | Loss 16.1975(13.9536) | Error 0.6680(0.6821) Steps 0(0.00) | Grad Norm 19.1060(15.5678) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 46.5722(49.7766) | Bit/dim 4.9602(4.9167) | Xent 1.9161(1.9366) | Loss 13.4613(13.9388) | Error 0.6720(0.6818) Steps 0(0.00) | Grad Norm 24.5552(15.8374) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 48.9689(49.7524) | Bit/dim 4.7058(4.9104) | Xent 1.8232(1.9332) | Loss 12.8676(13.9067) | Error 0.6390(0.6805) Steps 0(0.00) | Grad Norm 8.6816(15.6228) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 52.8925(49.8466) | Bit/dim 5.0139(4.9135) | Xent 2.0091(1.9354) | Loss 13.7631(13.9023) | Error 0.6816(0.6805) Steps 0(0.00) | Grad Norm 37.3490(16.2746) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 49.3170(49.8307) | Bit/dim 4.7188(4.9077) | Xent 1.8461(1.9328) | Loss 12.9121(13.8726) | Error 0.6511(0.6796) Steps 0(0.00) | Grad Norm 8.8414(16.0516) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 47.7456(49.7681) | Bit/dim 4.7685(4.9035) | Xent 1.8623(1.9306) | Loss 12.9583(13.8452) | Error 0.6626(0.6791) Steps 0(0.00) | Grad Norm 9.3112(15.8494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 21.1224, Epoch Time 338.0822(346.0165), Bit/dim 4.7776(best: 4.6816), Xent 1.8734, Loss 5.7143, Error 0.6520(best: 0.6367)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 50.9077(49.8023) | Bit/dim 4.7786(4.8997) | Xent 1.9058(1.9299) | Loss 16.3354(13.9199) | Error 0.6749(0.6790) Steps 0(0.00) | Grad Norm 11.2207(15.7105) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 51.3861(49.8498) | Bit/dim 4.6887(4.8934) | Xent 1.8630(1.9279) | Loss 12.9384(13.8905) | Error 0.6613(0.6785) Steps 0(0.00) | Grad Norm 7.6362(15.4683) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 49.4224(49.8370) | Bit/dim 4.6786(4.8870) | Xent 1.8780(1.9264) | Loss 12.8804(13.8602) | Error 0.6646(0.6781) Steps 0(0.00) | Grad Norm 11.5470(15.3506) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 52.6485(49.9213) | Bit/dim 4.7042(4.8815) | Xent 1.8161(1.9231) | Loss 12.8464(13.8298) | Error 0.6264(0.6765) Steps 0(0.00) | Grad Norm 10.0886(15.1928) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 49.7567(49.9164) | Bit/dim 4.6698(4.8751) | Xent 1.8698(1.9215) | Loss 12.8624(13.8007) | Error 0.6600(0.6760) Steps 0(0.00) | Grad Norm 12.1025(15.1001) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 51.0226(49.9496) | Bit/dim 4.6758(4.8691) | Xent 1.8865(1.9204) | Loss 12.9189(13.7743) | Error 0.6653(0.6757) Steps 0(0.00) | Grad Norm 14.9644(15.0960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 21.3215, Epoch Time 342.6601(345.9158), Bit/dim 4.6723(best: 4.6816), Xent 1.7858, Loss 5.5652, Error 0.6218(best: 0.6367)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 48.0854(49.8937) | Bit/dim 4.6743(4.8633) | Xent 1.8097(1.9171) | Loss 16.0402(13.8423) | Error 0.6369(0.6745) Steps 0(0.00) | Grad Norm 8.7338(14.9051) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 49.2723(49.8750) | Bit/dim 4.6308(4.8563) | Xent 1.9101(1.9169) | Loss 12.8149(13.8114) | Error 0.6866(0.6749) Steps 0(0.00) | Grad Norm 21.0422(15.0892) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 50.3839(49.8903) | Bit/dim 4.6264(4.8494) | Xent 1.8355(1.9145) | Loss 12.7511(13.7796) | Error 0.6499(0.6741) Steps 0(0.00) | Grad Norm 17.3911(15.1583) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 48.5750(49.8508) | Bit/dim 4.6288(4.8428) | Xent 1.7767(1.9103) | Loss 12.5757(13.7435) | Error 0.6260(0.6727) Steps 0(0.00) | Grad Norm 7.1130(14.9169) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 48.9396(49.8235) | Bit/dim 4.6075(4.8357) | Xent 1.8522(1.9086) | Loss 12.7023(13.7123) | Error 0.6584(0.6723) Steps 0(0.00) | Grad Norm 14.7474(14.9118) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 51.2548(49.8664) | Bit/dim 4.5833(4.8282) | Xent 1.8279(1.9062) | Loss 12.6348(13.6799) | Error 0.6451(0.6715) Steps 0(0.00) | Grad Norm 12.3025(14.8336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 21.4607, Epoch Time 334.3102(345.5676), Bit/dim 4.5891(best: 4.6723), Xent 1.7570, Loss 5.4676, Error 0.6131(best: 0.6218)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 47.3617(49.7913) | Bit/dim 4.5855(4.8209) | Xent 1.7850(1.9025) | Loss 15.8384(13.7447) | Error 0.6325(0.6703) Steps 0(0.00) | Grad Norm 9.9821(14.6880) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 49.5018(49.7826) | Bit/dim 4.5860(4.8138) | Xent 1.7973(1.8994) | Loss 12.6489(13.7118) | Error 0.6381(0.6693) Steps 0(0.00) | Grad Norm 13.3004(14.6464) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 50.1162(49.7926) | Bit/dim 4.5666(4.8064) | Xent 1.7752(1.8956) | Loss 12.6191(13.6790) | Error 0.6219(0.6679) Steps 0(0.00) | Grad Norm 9.0961(14.4799) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 50.4133(49.8112) | Bit/dim 4.5554(4.7989) | Xent 1.7736(1.8920) | Loss 12.4819(13.6431) | Error 0.6266(0.6667) Steps 0(0.00) | Grad Norm 8.4455(14.2989) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 51.5531(49.8635) | Bit/dim 4.5483(4.7914) | Xent 1.7764(1.8885) | Loss 12.4227(13.6065) | Error 0.6389(0.6658) Steps 0(0.00) | Grad Norm 5.1805(14.0253) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 49.9992(49.8676) | Bit/dim 4.5405(4.7839) | Xent 1.7741(1.8851) | Loss 12.4172(13.5708) | Error 0.6311(0.6648) Steps 0(0.00) | Grad Norm 7.2045(13.8207) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 21.3995, Epoch Time 336.3580(345.2913), Bit/dim 4.5246(best: 4.5891), Xent 1.7158, Loss 5.3825, Error 0.6007(best: 0.6131)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 48.8207(49.8362) | Bit/dim 4.5220(4.7760) | Xent 1.7431(1.8808) | Loss 15.6378(13.6328) | Error 0.6191(0.6634) Steps 0(0.00) | Grad Norm 5.5302(13.5720) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 53.8298(49.9560) | Bit/dim 4.5135(4.7681) | Xent 1.7605(1.8772) | Loss 12.2733(13.5921) | Error 0.6302(0.6624) Steps 0(0.00) | Grad Norm 6.4553(13.3585) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 50.1554(49.9620) | Bit/dim 4.5015(4.7601) | Xent 1.7667(1.8739) | Loss 12.3895(13.5560) | Error 0.6229(0.6612) Steps 0(0.00) | Grad Norm 7.0565(13.1694) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 49.5042(49.9482) | Bit/dim 4.5029(4.7524) | Xent 1.7163(1.8692) | Loss 12.3609(13.5201) | Error 0.6070(0.6596) Steps 0(0.00) | Grad Norm 5.3652(12.9353) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 54.0310(50.0707) | Bit/dim 4.4903(4.7445) | Xent 1.7367(1.8652) | Loss 12.2897(13.4832) | Error 0.6229(0.6585) Steps 0(0.00) | Grad Norm 7.4393(12.7704) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 49.1703(50.0437) | Bit/dim 4.5112(4.7375) | Xent 1.6949(1.8601) | Loss 12.3199(13.4483) | Error 0.5984(0.6567) Steps 0(0.00) | Grad Norm 7.8351(12.6223) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 21.7598, Epoch Time 343.1434(345.2269), Bit/dim 4.5183(best: 4.5246), Xent 1.7071, Loss 5.3719, Error 0.6090(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 50.6569(50.0621) | Bit/dim 4.5273(4.7312) | Xent 1.7645(1.8572) | Loss 15.8768(13.5212) | Error 0.6307(0.6559) Steps 0(0.00) | Grad Norm 16.5628(12.7406) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 52.3030(50.1293) | Bit/dim 4.7038(4.7304) | Xent 1.7701(1.8546) | Loss 12.9052(13.5027) | Error 0.6407(0.6555) Steps 0(0.00) | Grad Norm 19.8216(12.9530) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 50.4588(50.1392) | Bit/dim 4.5051(4.7237) | Xent 1.7485(1.8514) | Loss 12.2690(13.4657) | Error 0.6174(0.6543) Steps 0(0.00) | Grad Norm 16.5608(13.0612) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 53.1993(50.2310) | Bit/dim 4.5738(4.7192) | Xent 2.0156(1.8563) | Loss 12.6903(13.4424) | Error 0.6991(0.6557) Steps 0(0.00) | Grad Norm 49.0646(14.1413) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 55.6313(50.3930) | Bit/dim 4.5891(4.7153) | Xent 1.7754(1.8539) | Loss 12.5776(13.4165) | Error 0.6285(0.6549) Steps 0(0.00) | Grad Norm 9.8968(14.0140) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 51.2298(50.4181) | Bit/dim 4.5249(4.7095) | Xent 1.8003(1.8523) | Loss 12.4372(13.3871) | Error 0.6394(0.6544) Steps 0(0.00) | Grad Norm 21.3993(14.2355) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 22.4907, Epoch Time 352.2201(345.4367), Bit/dim 4.5402(best: 4.5183), Xent 2.1401, Loss 5.6102, Error 0.7160(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 56.1871(50.5912) | Bit/dim 4.5418(4.7045) | Xent 2.1809(1.8622) | Loss 16.2943(13.4743) | Error 0.7241(0.6565) Steps 0(0.00) | Grad Norm 57.8682(15.5445) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 53.8200(50.6880) | Bit/dim 4.5595(4.7002) | Xent 1.7998(1.8603) | Loss 12.5306(13.4460) | Error 0.6347(0.6558) Steps 0(0.00) | Grad Norm 25.0936(15.8310) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 50.6590(50.6872) | Bit/dim 4.5959(4.6970) | Xent 2.4053(1.8766) | Loss 13.3003(13.4416) | Error 0.7685(0.6592) Steps 0(0.00) | Grad Norm 49.4764(16.8404) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 52.5099(50.7419) | Bit/dim 4.6270(4.6949) | Xent 1.9750(1.8796) | Loss 12.9365(13.4265) | Error 0.7065(0.6606) Steps 0(0.00) | Grad Norm 21.0267(16.9659) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 50.8986(50.7466) | Bit/dim 4.5140(4.6895) | Xent 2.0443(1.8845) | Loss 12.7561(13.4064) | Error 0.7359(0.6629) Steps 0(0.00) | Grad Norm 8.9424(16.7252) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 57.8169(50.9587) | Bit/dim 4.5565(4.6855) | Xent 1.9786(1.8874) | Loss 12.7312(13.3861) | Error 0.6925(0.6638) Steps 0(0.00) | Grad Norm 11.8492(16.5790) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 22.7405, Epoch Time 360.4010(345.8856), Bit/dim 4.5729(best: 4.5183), Xent 1.8319, Loss 5.4889, Error 0.6500(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 57.4039(51.1520) | Bit/dim 4.5735(4.6822) | Xent 1.8510(1.8863) | Loss 16.0709(13.4667) | Error 0.6626(0.6637) Steps 0(0.00) | Grad Norm 13.0657(16.4736) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 54.4167(51.2500) | Bit/dim 4.5510(4.6782) | Xent 1.8805(1.8861) | Loss 12.7290(13.4445) | Error 0.6726(0.6640) Steps 0(0.00) | Grad Norm 7.5983(16.2073) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 54.7412(51.3547) | Bit/dim 4.5061(4.6731) | Xent 1.8691(1.8856) | Loss 12.4360(13.4143) | Error 0.6641(0.6640) Steps 0(0.00) | Grad Norm 6.6708(15.9212) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 53.9587(51.4328) | Bit/dim 4.5008(4.6679) | Xent 1.8587(1.8848) | Loss 12.4713(13.3860) | Error 0.6525(0.6637) Steps 0(0.00) | Grad Norm 4.6128(15.5820) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 53.8749(51.5061) | Bit/dim 4.5169(4.6634) | Xent 1.8588(1.8840) | Loss 12.4984(13.3594) | Error 0.6549(0.6634) Steps 0(0.00) | Grad Norm 8.1675(15.3595) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 54.6452(51.6003) | Bit/dim 4.4709(4.6576) | Xent 1.8282(1.8823) | Loss 12.3438(13.3289) | Error 0.6416(0.6627) Steps 0(0.00) | Grad Norm 6.0910(15.0815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 22.5570, Epoch Time 367.4186(346.5316), Bit/dim 4.4500(best: 4.5183), Xent 1.7777, Loss 5.3388, Error 0.6188(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 50.8424(51.5775) | Bit/dim 4.4439(4.6512) | Xent 1.8230(1.8805) | Loss 15.6580(13.3988) | Error 0.6395(0.6620) Steps 0(0.00) | Grad Norm 3.7171(14.7405) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 50.3556(51.5409) | Bit/dim 4.4550(4.6453) | Xent 1.8046(1.8783) | Loss 12.3451(13.3671) | Error 0.6364(0.6613) Steps 0(0.00) | Grad Norm 8.1085(14.5416) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 49.5714(51.4818) | Bit/dim 4.4764(4.6402) | Xent 1.7854(1.8755) | Loss 12.3763(13.3374) | Error 0.6233(0.6601) Steps 0(0.00) | Grad Norm 8.9373(14.3734) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 51.1779(51.4727) | Bit/dim 4.4808(4.6354) | Xent 1.8332(1.8742) | Loss 12.5295(13.3132) | Error 0.6445(0.6597) Steps 0(0.00) | Grad Norm 13.1749(14.3375) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 51.8391(51.4837) | Bit/dim 4.5675(4.6334) | Xent 1.8108(1.8723) | Loss 12.5200(13.2894) | Error 0.6394(0.6591) Steps 0(0.00) | Grad Norm 12.5205(14.2830) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 51.9362(51.4972) | Bit/dim 4.4208(4.6270) | Xent 1.8014(1.8702) | Loss 12.2497(13.2582) | Error 0.6378(0.6584) Steps 0(0.00) | Grad Norm 7.6004(14.0825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 21.9349, Epoch Time 343.3127(346.4350), Bit/dim 4.4738(best: 4.4500), Xent 1.7315, Loss 5.3396, Error 0.6107(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 53.9764(51.5716) | Bit/dim 4.4655(4.6222) | Xent 1.7934(1.8679) | Loss 15.6794(13.3308) | Error 0.6319(0.6576) Steps 0(0.00) | Grad Norm 9.9454(13.9584) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 50.6648(51.5444) | Bit/dim 4.4860(4.6181) | Xent 1.7670(1.8648) | Loss 12.2687(13.2990) | Error 0.6241(0.6566) Steps 0(0.00) | Grad Norm 8.6104(13.7980) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 51.0471(51.5295) | Bit/dim 4.4807(4.6140) | Xent 1.7328(1.8609) | Loss 12.2769(13.2683) | Error 0.6102(0.6552) Steps 0(0.00) | Grad Norm 7.9579(13.6228) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 53.8554(51.5993) | Bit/dim 4.4400(4.6088) | Xent 1.7672(1.8581) | Loss 12.1502(13.2348) | Error 0.6287(0.6544) Steps 0(0.00) | Grad Norm 10.7052(13.5352) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 55.2272(51.7081) | Bit/dim 4.4057(4.6027) | Xent 1.7962(1.8562) | Loss 12.2495(13.2052) | Error 0.6301(0.6537) Steps 0(0.00) | Grad Norm 13.7595(13.5420) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 52.4327(51.7298) | Bit/dim 4.4513(4.5981) | Xent 1.7903(1.8542) | Loss 12.2202(13.1757) | Error 0.6394(0.6533) Steps 0(0.00) | Grad Norm 17.6682(13.6657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 21.9325, Epoch Time 354.9220(346.6896), Bit/dim 4.4352(best: 4.4500), Xent 1.7568, Loss 5.3136, Error 0.6256(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 50.7083(51.6992) | Bit/dim 4.4367(4.5933) | Xent 1.7742(1.8518) | Loss 15.5453(13.2467) | Error 0.6362(0.6528) Steps 0(0.00) | Grad Norm 17.2764(13.7741) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 52.0751(51.7105) | Bit/dim 4.3973(4.5874) | Xent 1.8489(1.8518) | Loss 12.2030(13.2154) | Error 0.6496(0.6527) Steps 0(0.00) | Grad Norm 17.3006(13.8799) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 52.2535(51.7268) | Bit/dim 4.3651(4.5807) | Xent 1.7654(1.8492) | Loss 12.1953(13.1848) | Error 0.6222(0.6518) Steps 0(0.00) | Grad Norm 6.1241(13.6472) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 53.3329(51.7749) | Bit/dim 4.4038(4.5754) | Xent 1.7478(1.8461) | Loss 12.1185(13.1528) | Error 0.6248(0.6509) Steps 0(0.00) | Grad Norm 7.6324(13.4667) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 52.4191(51.7943) | Bit/dim 4.3644(4.5691) | Xent 1.7315(1.8427) | Loss 12.0828(13.1207) | Error 0.6184(0.6500) Steps 0(0.00) | Grad Norm 4.9386(13.2109) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 54.2461(51.8678) | Bit/dim 4.4091(4.5643) | Xent 1.7240(1.8391) | Loss 12.1344(13.0912) | Error 0.6122(0.6488) Steps 0(0.00) | Grad Norm 8.9711(13.0837) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.3283, Epoch Time 353.6173(346.8975), Bit/dim 4.4103(best: 4.4352), Xent 1.6926, Loss 5.2566, Error 0.5943(best: 0.6007)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 53.8094(51.9261) | Bit/dim 4.4187(4.5599) | Xent 1.7053(1.8351) | Loss 15.2659(13.1564) | Error 0.6091(0.6476) Steps 0(0.00) | Grad Norm 10.6146(13.0096) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 56.4779(52.0626) | Bit/dim 4.3668(4.5541) | Xent 1.7035(1.8312) | Loss 11.9645(13.1206) | Error 0.6045(0.6464) Steps 0(0.00) | Grad Norm 8.3965(12.8712) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 56.0281(52.1816) | Bit/dim 4.3618(4.5484) | Xent 1.7149(1.8277) | Loss 11.9851(13.0866) | Error 0.6064(0.6452) Steps 0(0.00) | Grad Norm 9.9174(12.7826) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 50.2271(52.1230) | Bit/dim 4.3965(4.5438) | Xent 1.7742(1.8261) | Loss 12.1530(13.0586) | Error 0.6287(0.6447) Steps 0(0.00) | Grad Norm 14.6441(12.8385) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 53.4685(52.1633) | Bit/dim 4.3207(4.5371) | Xent 1.6952(1.8221) | Loss 11.9817(13.0263) | Error 0.5976(0.6432) Steps 0(0.00) | Grad Norm 4.9794(12.6027) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 54.8325(52.2434) | Bit/dim 4.3619(4.5319) | Xent 1.7847(1.8210) | Loss 12.0981(12.9984) | Error 0.6305(0.6429) Steps 0(0.00) | Grad Norm 22.9480(12.9131) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.3075, Epoch Time 363.2663(347.3885), Bit/dim 4.3813(best: 4.4103), Xent 1.7713, Loss 5.2669, Error 0.6164(best: 0.5943)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 51.7652(52.2291) | Bit/dim 4.3853(4.5275) | Xent 1.8091(1.8207) | Loss 15.7944(13.0823) | Error 0.6342(0.6426) Steps 0(0.00) | Grad Norm 29.4337(13.4087) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 53.9208(52.2798) | Bit/dim 4.4228(4.5243) | Xent 1.7727(1.8192) | Loss 12.2938(13.0586) | Error 0.6226(0.6420) Steps 0(0.00) | Grad Norm 19.4619(13.5903) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 54.7588(52.3542) | Bit/dim 4.3695(4.5197) | Xent 1.7348(1.8167) | Loss 12.1430(13.0312) | Error 0.6175(0.6413) Steps 0(0.00) | Grad Norm 14.1853(13.6081) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 51.1203(52.3172) | Bit/dim 4.3262(4.5139) | Xent 1.7396(1.8144) | Loss 12.1066(13.0034) | Error 0.6169(0.6405) Steps 0(0.00) | Grad Norm 16.5494(13.6964) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 53.7311(52.3596) | Bit/dim 4.3865(4.5100) | Xent 1.7283(1.8118) | Loss 12.1722(12.9785) | Error 0.6113(0.6397) Steps 0(0.00) | Grad Norm 16.3057(13.7746) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 56.3328(52.4788) | Bit/dim 4.3319(4.5047) | Xent 1.7361(1.8095) | Loss 12.0071(12.9493) | Error 0.6221(0.6391) Steps 0(0.00) | Grad Norm 14.3121(13.7908) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.9261, Epoch Time 360.4199(347.7795), Bit/dim 4.3254(best: 4.3813), Xent 1.6494, Loss 5.1501, Error 0.5828(best: 0.5943)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 59.2319(52.6814) | Bit/dim 4.3365(4.4997) | Xent 1.7021(1.8063) | Loss 15.5476(13.0273) | Error 0.6013(0.6380) Steps 0(0.00) | Grad Norm 8.6881(13.6377) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 53.8764(52.7172) | Bit/dim 4.3088(4.4939) | Xent 1.6852(1.8027) | Loss 11.8435(12.9918) | Error 0.5999(0.6369) Steps 0(0.00) | Grad Norm 6.2912(13.4173) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 51.1107(52.6690) | Bit/dim 4.3321(4.4891) | Xent 1.6619(1.7984) | Loss 11.9128(12.9594) | Error 0.5910(0.6355) Steps 0(0.00) | Grad Norm 5.7200(13.1864) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 52.1114(52.6523) | Bit/dim 4.2805(4.4828) | Xent 1.6765(1.7948) | Loss 11.8898(12.9273) | Error 0.5980(0.6344) Steps 0(0.00) | Grad Norm 5.7154(12.9622) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 55.7476(52.7452) | Bit/dim 4.2945(4.4772) | Xent 1.6594(1.7907) | Loss 11.8008(12.8935) | Error 0.5931(0.6331) Steps 0(0.00) | Grad Norm 9.6451(12.8627) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 53.6252(52.7716) | Bit/dim 4.2807(4.4713) | Xent 1.6530(1.7866) | Loss 11.8119(12.8611) | Error 0.5893(0.6318) Steps 0(0.00) | Grad Norm 9.9226(12.7745) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 22.8797, Epoch Time 364.5480(348.2825), Bit/dim 4.2852(best: 4.3254), Xent 1.6477, Loss 5.1091, Error 0.5835(best: 0.5828)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 51.6393(52.7376) | Bit/dim 4.2917(4.4659) | Xent 1.7072(1.7842) | Loss 15.5301(12.9412) | Error 0.6058(0.6310) Steps 0(0.00) | Grad Norm 19.0495(12.9628) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 54.7005(52.7965) | Bit/dim 4.3626(4.4628) | Xent 1.7288(1.7826) | Loss 11.8708(12.9090) | Error 0.6133(0.6305) Steps 0(0.00) | Grad Norm 19.7773(13.1672) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 56.2555(52.9002) | Bit/dim 4.3146(4.4583) | Xent 1.6306(1.7780) | Loss 11.8731(12.8780) | Error 0.5779(0.6289) Steps 0(0.00) | Grad Norm 10.0935(13.0750) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 50.7608(52.8361) | Bit/dim 4.3072(4.4538) | Xent 1.7618(1.7775) | Loss 12.0095(12.8519) | Error 0.6204(0.6287) Steps 0(0.00) | Grad Norm 28.5304(13.5387) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 52.2602(52.8188) | Bit/dim 4.2975(4.4491) | Xent 1.7547(1.7768) | Loss 11.9174(12.8239) | Error 0.6231(0.6285) Steps 0(0.00) | Grad Norm 21.6344(13.7815) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 53.2474(52.8316) | Bit/dim 4.2668(4.4437) | Xent 1.6813(1.7740) | Loss 11.8293(12.7940) | Error 0.5926(0.6274) Steps 0(0.00) | Grad Norm 9.2075(13.6443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 23.2551, Epoch Time 357.6971(348.5650), Bit/dim 4.2920(best: 4.2852), Xent 1.6055, Loss 5.0948, Error 0.5719(best: 0.5828)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 52.7713(52.8298) | Bit/dim 4.2892(4.4390) | Xent 1.6685(1.7708) | Loss 15.4677(12.8743) | Error 0.5985(0.6265) Steps 0(0.00) | Grad Norm 7.9048(13.4721) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 56.5245(52.9407) | Bit/dim 4.2596(4.4336) | Xent 1.6590(1.7674) | Loss 11.7651(12.8410) | Error 0.5968(0.6257) Steps 0(0.00) | Grad Norm 6.4573(13.2617) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 54.3066(52.9817) | Bit/dim 4.2629(4.4285) | Xent 1.6690(1.7645) | Loss 11.7242(12.8075) | Error 0.5896(0.6246) Steps 0(0.00) | Grad Norm 8.1820(13.1093) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 53.5624(52.9991) | Bit/dim 4.2411(4.4229) | Xent 1.6516(1.7611) | Loss 11.7796(12.7766) | Error 0.5893(0.6235) Steps 0(0.00) | Grad Norm 4.8622(12.8619) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 51.3449(52.9494) | Bit/dim 4.2384(4.4174) | Xent 1.6659(1.7582) | Loss 11.7920(12.7471) | Error 0.5934(0.6226) Steps 0(0.00) | Grad Norm 9.6056(12.7642) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 56.2743(53.0492) | Bit/dim 4.2475(4.4123) | Xent 1.6766(1.7558) | Loss 11.8974(12.7216) | Error 0.5895(0.6216) Steps 0(0.00) | Grad Norm 12.5802(12.7587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 22.8358, Epoch Time 363.8016(349.0221), Bit/dim 4.2406(best: 4.2852), Xent 1.6307, Loss 5.0559, Error 0.5804(best: 0.5719)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 53.4554(53.0614) | Bit/dim 4.2339(4.4069) | Xent 1.7081(1.7544) | Loss 15.3437(12.8003) | Error 0.6064(0.6212) Steps 0(0.00) | Grad Norm 16.9259(12.8837) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 58.2563(53.2172) | Bit/dim 4.2600(4.4025) | Xent 1.6935(1.7525) | Loss 11.7616(12.7691) | Error 0.6001(0.6205) Steps 0(0.00) | Grad Norm 16.5203(12.9928) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 56.8183(53.3253) | Bit/dim 4.2607(4.3983) | Xent 1.6239(1.7487) | Loss 11.7578(12.7388) | Error 0.5795(0.6193) Steps 0(0.00) | Grad Norm 7.9305(12.8409) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 57.4082(53.4477) | Bit/dim 4.2753(4.3946) | Xent 1.7092(1.7475) | Loss 11.8807(12.7130) | Error 0.6064(0.6189) Steps 0(0.00) | Grad Norm 22.8334(13.1407) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 57.3426(53.5646) | Bit/dim 4.3435(4.3930) | Xent 1.8770(1.7514) | Loss 12.1615(12.6965) | Error 0.6654(0.6203) Steps 0(0.00) | Grad Norm 21.9949(13.4063) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 56.2232(53.6444) | Bit/dim 4.2344(4.3883) | Xent 1.7344(1.7509) | Loss 11.8542(12.6712) | Error 0.6126(0.6201) Steps 0(0.00) | Grad Norm 10.8394(13.3293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 22.8044, Epoch Time 378.3899(349.9031), Bit/dim 4.3228(best: 4.2406), Xent 1.7786, Loss 5.2121, Error 0.6494(best: 0.5719)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 56.7980(53.7390) | Bit/dim 4.3265(4.3864) | Xent 1.8451(1.7537) | Loss 15.6511(12.7606) | Error 0.6518(0.6210) Steps 0(0.00) | Grad Norm 23.1076(13.6227) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 57.4899(53.8515) | Bit/dim 4.2485(4.3823) | Xent 1.6857(1.7517) | Loss 11.7734(12.7310) | Error 0.6020(0.6205) Steps 0(0.00) | Grad Norm 6.0048(13.3941) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 55.6051(53.9041) | Bit/dim 4.2769(4.3791) | Xent 1.7148(1.7506) | Loss 11.8364(12.7042) | Error 0.6102(0.6201) Steps 0(0.00) | Grad Norm 9.4849(13.2768) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 55.2443(53.9443) | Bit/dim 4.2473(4.3752) | Xent 1.7452(1.7504) | Loss 11.8094(12.6773) | Error 0.6202(0.6201) Steps 0(0.00) | Grad Norm 13.3718(13.2797) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 58.2131(54.0724) | Bit/dim 4.2586(4.3717) | Xent 1.7666(1.7509) | Loss 11.8838(12.6535) | Error 0.6315(0.6205) Steps 0(0.00) | Grad Norm 20.9945(13.5111) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 52.3003(54.0192) | Bit/dim 4.2577(4.3682) | Xent 1.9470(1.7568) | Loss 12.0068(12.6341) | Error 0.6704(0.6220) Steps 0(0.00) | Grad Norm 28.5375(13.9619) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.1549, Epoch Time 373.6441(350.6153), Bit/dim 4.2586(best: 4.2406), Xent 1.6699, Loss 5.0936, Error 0.5977(best: 0.5719)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 51.2497(53.9361) | Bit/dim 4.2615(4.3650) | Xent 1.7156(1.7555) | Loss 15.1506(12.7096) | Error 0.6054(0.6215) Steps 0(0.00) | Grad Norm 10.8445(13.8684) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 54.6756(53.9583) | Bit/dim 4.2787(4.3625) | Xent 1.8163(1.7574) | Loss 12.0045(12.6884) | Error 0.6444(0.6222) Steps 0(0.00) | Grad Norm 25.4456(14.2157) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 55.7163(54.0110) | Bit/dim 4.2505(4.3591) | Xent 1.6887(1.7553) | Loss 11.7386(12.6600) | Error 0.6056(0.6217) Steps 0(0.00) | Grad Norm 9.4944(14.0741) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 56.6869(54.0913) | Bit/dim 4.2229(4.3550) | Xent 1.7276(1.7545) | Loss 11.7081(12.6314) | Error 0.6210(0.6217) Steps 0(0.00) | Grad Norm 11.6010(13.9999) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 54.7807(54.1120) | Bit/dim 4.2498(4.3519) | Xent 1.6919(1.7526) | Loss 11.6596(12.6022) | Error 0.6011(0.6210) Steps 0(0.00) | Grad Norm 8.7956(13.8438) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 59.2107(54.2650) | Bit/dim 4.2181(4.3478) | Xent 1.6918(1.7508) | Loss 11.7743(12.5774) | Error 0.6053(0.6206) Steps 0(0.00) | Grad Norm 7.0837(13.6410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.4038, Epoch Time 370.4825(351.2114), Bit/dim 4.2322(best: 4.2406), Xent 1.6242, Loss 5.0443, Error 0.5671(best: 0.5719)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 49.6952(54.1279) | Bit/dim 4.2238(4.3441) | Xent 1.6598(1.7480) | Loss 15.1056(12.6533) | Error 0.5907(0.6197) Steps 0(0.00) | Grad Norm 8.5384(13.4879) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 53.5474(54.1105) | Bit/dim 4.1938(4.3396) | Xent 1.6436(1.7449) | Loss 11.6079(12.6219) | Error 0.5865(0.6187) Steps 0(0.00) | Grad Norm 7.1834(13.2987) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 53.0738(54.0794) | Bit/dim 4.1840(4.3349) | Xent 1.6438(1.7419) | Loss 11.7428(12.5955) | Error 0.5879(0.6178) Steps 0(0.00) | Grad Norm 6.1609(13.0846) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 51.5833(54.0045) | Bit/dim 4.1829(4.3304) | Xent 1.6107(1.7379) | Loss 11.6056(12.5658) | Error 0.5685(0.6163) Steps 0(0.00) | Grad Norm 6.7462(12.8945) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 54.4237(54.0171) | Bit/dim 4.1864(4.3261) | Xent 1.6604(1.7356) | Loss 11.7145(12.5403) | Error 0.5951(0.6156) Steps 0(0.00) | Grad Norm 7.4838(12.7321) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 52.9531(53.9851) | Bit/dim 4.1691(4.3214) | Xent 1.6220(1.7322) | Loss 11.6022(12.5121) | Error 0.5769(0.6145) Steps 0(0.00) | Grad Norm 4.8971(12.4971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 22.5391, Epoch Time 353.5553(351.2817), Bit/dim 4.1825(best: 4.2322), Xent 1.5543, Loss 4.9597, Error 0.5523(best: 0.5671)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 51.3000(53.9046) | Bit/dim 4.1737(4.3169) | Xent 1.5953(1.7281) | Loss 14.8661(12.5828) | Error 0.5699(0.6131) Steps 0(0.00) | Grad Norm 5.6188(12.2907) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 58.8794(54.0538) | Bit/dim 4.1816(4.3129) | Xent 1.5981(1.7242) | Loss 11.4443(12.5486) | Error 0.5690(0.6118) Steps 0(0.00) | Grad Norm 7.9309(12.1599) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 57.6672(54.1622) | Bit/dim 4.2037(4.3096) | Xent 1.6303(1.7214) | Loss 11.6225(12.5208) | Error 0.5727(0.6106) Steps 0(0.00) | Grad Norm 17.4425(12.3184) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 57.8787(54.2737) | Bit/dim 4.2440(4.3076) | Xent 1.7753(1.7230) | Loss 11.8568(12.5009) | Error 0.6300(0.6112) Steps 0(0.00) | Grad Norm 23.0789(12.6412) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 52.0325(54.2065) | Bit/dim 4.1889(4.3041) | Xent 1.6265(1.7201) | Loss 11.5555(12.4725) | Error 0.5796(0.6103) Steps 0(0.00) | Grad Norm 8.1548(12.5066) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 55.9291(54.2582) | Bit/dim 4.1703(4.3000) | Xent 1.6847(1.7190) | Loss 11.7057(12.4495) | Error 0.6010(0.6100) Steps 0(0.00) | Grad Norm 9.6561(12.4211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 22.6633, Epoch Time 371.9872(351.9028), Bit/dim 4.1753(best: 4.1825), Xent 1.5976, Loss 4.9741, Error 0.5671(best: 0.5523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 55.6655(54.3004) | Bit/dim 4.1659(4.2960) | Xent 1.6777(1.7178) | Loss 14.8781(12.5224) | Error 0.5978(0.6096) Steps 0(0.00) | Grad Norm 9.3680(12.3295) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 55.0715(54.3235) | Bit/dim 4.1655(4.2921) | Xent 1.6490(1.7157) | Loss 11.6351(12.4958) | Error 0.5994(0.6093) Steps 0(0.00) | Grad Norm 15.4775(12.4240) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 53.9290(54.3117) | Bit/dim 4.2021(4.2894) | Xent 1.6468(1.7137) | Loss 11.6604(12.4707) | Error 0.5988(0.6090) Steps 0(0.00) | Grad Norm 17.4237(12.5740) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 54.9238(54.3300) | Bit/dim 4.1607(4.2855) | Xent 1.7426(1.7145) | Loss 11.7030(12.4477) | Error 0.6151(0.6092) Steps 0(0.00) | Grad Norm 23.7850(12.9103) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 54.7017(54.3412) | Bit/dim 4.1594(4.2818) | Xent 1.6646(1.7130) | Loss 11.5375(12.4204) | Error 0.5913(0.6087) Steps 0(0.00) | Grad Norm 16.3630(13.0139) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 52.3026(54.2800) | Bit/dim 4.1397(4.2775) | Xent 1.6085(1.7099) | Loss 11.3965(12.3897) | Error 0.5774(0.6077) Steps 0(0.00) | Grad Norm 7.0858(12.8360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 22.8851, Epoch Time 365.0303(352.2967), Bit/dim 4.1565(best: 4.1753), Xent 1.5405, Loss 4.9268, Error 0.5469(best: 0.5523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 54.5654(54.2886) | Bit/dim 4.1487(4.2736) | Xent 1.5841(1.7061) | Loss 14.8533(12.4636) | Error 0.5635(0.6064) Steps 0(0.00) | Grad Norm 7.4461(12.6743) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 59.6289(54.4488) | Bit/dim 4.1295(4.2693) | Xent 1.6047(1.7031) | Loss 11.4159(12.4321) | Error 0.5759(0.6055) Steps 0(0.00) | Grad Norm 7.6150(12.5226) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 55.9175(54.4929) | Bit/dim 4.1312(4.2652) | Xent 1.5728(1.6992) | Loss 11.4961(12.4041) | Error 0.5719(0.6045) Steps 0(0.00) | Grad Norm 7.1901(12.3626) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 53.4525(54.4617) | Bit/dim 4.1432(4.2615) | Xent 1.5697(1.6953) | Loss 11.3523(12.3725) | Error 0.5651(0.6033) Steps 0(0.00) | Grad Norm 7.3605(12.2125) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 55.8595(54.5036) | Bit/dim 4.1601(4.2585) | Xent 1.5995(1.6924) | Loss 11.3693(12.3424) | Error 0.5691(0.6023) Steps 0(0.00) | Grad Norm 13.2868(12.2447) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 53.5125(54.4739) | Bit/dim 4.1443(4.2550) | Xent 1.5654(1.6886) | Loss 11.3901(12.3138) | Error 0.5595(0.6010) Steps 0(0.00) | Grad Norm 8.7855(12.1410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 22.8737, Epoch Time 371.5883(352.8754), Bit/dim 4.1265(best: 4.1565), Xent 1.5198, Loss 4.8864, Error 0.5487(best: 0.5469)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 56.8312(54.5446) | Bit/dim 4.1296(4.2513) | Xent 1.5571(1.6847) | Loss 14.9647(12.3934) | Error 0.5639(0.5999) Steps 0(0.00) | Grad Norm 10.2590(12.0845) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 55.9838(54.5878) | Bit/dim 4.1007(4.2468) | Xent 1.6022(1.6822) | Loss 11.3027(12.3606) | Error 0.5850(0.5994) Steps 0(0.00) | Grad Norm 13.8902(12.1387) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 55.7377(54.6223) | Bit/dim 4.1145(4.2428) | Xent 1.6093(1.6800) | Loss 11.3935(12.3316) | Error 0.5741(0.5987) Steps 0(0.00) | Grad Norm 14.0039(12.1946) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 55.0823(54.6361) | Bit/dim 4.1038(4.2386) | Xent 1.5996(1.6776) | Loss 11.3637(12.3026) | Error 0.5780(0.5980) Steps 0(0.00) | Grad Norm 13.1046(12.2219) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 53.3913(54.5987) | Bit/dim 4.0869(4.2341) | Xent 1.5886(1.6749) | Loss 11.3062(12.2727) | Error 0.5762(0.5974) Steps 0(0.00) | Grad Norm 10.5920(12.1730) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 53.5010(54.5658) | Bit/dim 4.0766(4.2293) | Xent 1.5366(1.6708) | Loss 11.2009(12.2405) | Error 0.5495(0.5959) Steps 0(0.00) | Grad Norm 3.0153(11.8983) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 23.0454, Epoch Time 369.2566(353.3669), Bit/dim 4.0805(best: 4.1265), Xent 1.5173, Loss 4.8391, Error 0.5393(best: 0.5469)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 53.7296(54.5407) | Bit/dim 4.0886(4.2251) | Xent 1.5654(1.6676) | Loss 14.8001(12.3173) | Error 0.5609(0.5949) Steps 0(0.00) | Grad Norm 8.4965(11.7963) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 57.5042(54.6296) | Bit/dim 4.0699(4.2205) | Xent 1.5817(1.6650) | Loss 11.2487(12.2853) | Error 0.5681(0.5941) Steps 0(0.00) | Grad Norm 13.9789(11.8617) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 58.2820(54.7392) | Bit/dim 4.0857(4.2164) | Xent 1.6144(1.6635) | Loss 11.3341(12.2567) | Error 0.5820(0.5937) Steps 0(0.00) | Grad Norm 18.6594(12.0657) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 57.1791(54.8124) | Bit/dim 4.0798(4.2123) | Xent 1.6337(1.6626) | Loss 11.4247(12.2318) | Error 0.5846(0.5935) Steps 0(0.00) | Grad Norm 19.1329(12.2777) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 54.0718(54.7902) | Bit/dim 4.3336(4.2160) | Xent 1.7016(1.6638) | Loss 12.0752(12.2271) | Error 0.6066(0.5939) Steps 0(0.00) | Grad Norm 18.2713(12.4575) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 58.1498(54.8909) | Bit/dim 4.1377(4.2136) | Xent 1.7470(1.6663) | Loss 11.6098(12.2086) | Error 0.6201(0.5946) Steps 0(0.00) | Grad Norm 24.0842(12.8063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 23.1281, Epoch Time 377.4670(354.0899), Bit/dim 4.2072(best: 4.0805), Xent 1.7880, Loss 5.1012, Error 0.6287(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 55.0440(54.8955) | Bit/dim 4.2067(4.2134) | Xent 1.8573(1.6720) | Loss 15.6072(12.3105) | Error 0.6350(0.5959) Steps 0(0.00) | Grad Norm 25.6447(13.1914) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 49.0520(54.7202) | Bit/dim 4.3007(4.2160) | Xent 1.7895(1.6755) | Loss 11.8271(12.2960) | Error 0.6386(0.5971) Steps 0(0.00) | Grad Norm 14.6030(13.2338) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 55.5335(54.7446) | Bit/dim 4.1862(4.2151) | Xent 1.7650(1.6782) | Loss 11.7236(12.2788) | Error 0.6201(0.5978) Steps 0(0.00) | Grad Norm 15.9497(13.3153) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 56.7972(54.8062) | Bit/dim 4.2193(4.2153) | Xent 1.9395(1.6861) | Loss 12.0561(12.2722) | Error 0.6761(0.6002) Steps 0(0.00) | Grad Norm 24.4109(13.6481) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 54.8874(54.8086) | Bit/dim 4.2272(4.2156) | Xent 1.7859(1.6890) | Loss 11.7941(12.2578) | Error 0.6361(0.6013) Steps 0(0.00) | Grad Norm 12.2526(13.6063) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 54.2145(54.7908) | Bit/dim 4.1465(4.2135) | Xent 1.7122(1.6897) | Loss 11.6130(12.2385) | Error 0.6058(0.6014) Steps 0(0.00) | Grad Norm 5.9823(13.3775) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 21.8284, Epoch Time 363.2855(354.3657), Bit/dim 4.2115(best: 4.0805), Xent 1.6430, Loss 5.0330, Error 0.5902(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 50.8473(54.6725) | Bit/dim 4.2088(4.2134) | Xent 1.6699(1.6892) | Loss 15.0827(12.3238) | Error 0.5961(0.6012) Steps 0(0.00) | Grad Norm 8.7083(13.2375) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 53.8759(54.6486) | Bit/dim 4.1686(4.2121) | Xent 1.6352(1.6875) | Loss 11.5953(12.3019) | Error 0.5890(0.6009) Steps 0(0.00) | Grad Norm 5.0008(12.9904) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 55.9296(54.6870) | Bit/dim 4.1691(4.2108) | Xent 1.6564(1.6866) | Loss 11.4830(12.2774) | Error 0.5956(0.6007) Steps 0(0.00) | Grad Norm 8.5501(12.8572) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 56.0177(54.7270) | Bit/dim 4.1532(4.2090) | Xent 1.7011(1.6870) | Loss 11.4911(12.2538) | Error 0.6111(0.6010) Steps 0(0.00) | Grad Norm 12.1811(12.8369) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 56.7803(54.7886) | Bit/dim 4.1401(4.2070) | Xent 1.7604(1.6892) | Loss 11.5989(12.2341) | Error 0.6200(0.6016) Steps 0(0.00) | Grad Norm 14.0992(12.8747) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 60.4488(54.9584) | Bit/dim 4.1235(4.2045) | Xent 1.7877(1.6922) | Loss 11.5746(12.2144) | Error 0.6406(0.6028) Steps 0(0.00) | Grad Norm 15.0777(12.9408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.3015, Epoch Time 372.6435(354.9141), Bit/dim 4.1117(best: 4.0805), Xent 1.6364, Loss 4.9299, Error 0.5871(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 56.6938(55.0104) | Bit/dim 4.0960(4.2012) | Xent 1.7052(1.6926) | Loss 15.1358(12.3020) | Error 0.6065(0.6029) Steps 0(0.00) | Grad Norm 12.7444(12.9349) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 58.1843(55.1057) | Bit/dim 4.1224(4.1988) | Xent 1.6584(1.6916) | Loss 11.6268(12.2817) | Error 0.5873(0.6024) Steps 0(0.00) | Grad Norm 8.9452(12.8152) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 62.2297(55.3194) | Bit/dim 4.0913(4.1956) | Xent 1.6340(1.6898) | Loss 11.4378(12.2564) | Error 0.5839(0.6018) Steps 0(0.00) | Grad Norm 8.8419(12.6961) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 55.9645(55.3387) | Bit/dim 4.1066(4.1930) | Xent 1.6336(1.6881) | Loss 11.4691(12.2328) | Error 0.5815(0.6012) Steps 0(0.00) | Grad Norm 13.1052(12.7083) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 52.5890(55.2562) | Bit/dim 4.1318(4.1911) | Xent 1.6804(1.6879) | Loss 11.4257(12.2086) | Error 0.5950(0.6010) Steps 0(0.00) | Grad Norm 12.8315(12.7120) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 53.1132(55.1919) | Bit/dim 4.1003(4.1884) | Xent 1.6219(1.6859) | Loss 11.4682(12.1864) | Error 0.5809(0.6004) Steps 0(0.00) | Grad Norm 8.3714(12.5818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 23.0771, Epoch Time 377.3986(355.5886), Bit/dim 4.1064(best: 4.0805), Xent 1.5989, Loss 4.9058, Error 0.5733(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 57.5872(55.2638) | Bit/dim 4.0966(4.1856) | Xent 1.6644(1.6853) | Loss 15.0538(12.2724) | Error 0.5927(0.6002) Steps 0(0.00) | Grad Norm 16.2411(12.6916) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 54.7813(55.2493) | Bit/dim 4.0761(4.1824) | Xent 1.6261(1.6835) | Loss 11.3474(12.2447) | Error 0.5806(0.5996) Steps 0(0.00) | Grad Norm 9.2625(12.5887) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 53.0887(55.1845) | Bit/dim 4.0831(4.1794) | Xent 1.6109(1.6813) | Loss 11.3885(12.2190) | Error 0.5677(0.5987) Steps 0(0.00) | Grad Norm 8.1532(12.4556) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 55.6701(55.1991) | Bit/dim 4.0580(4.1757) | Xent 1.6081(1.6791) | Loss 11.2313(12.1893) | Error 0.5796(0.5981) Steps 0(0.00) | Grad Norm 3.6346(12.1910) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 56.6605(55.2429) | Bit/dim 4.0864(4.1731) | Xent 1.6105(1.6771) | Loss 11.4164(12.1662) | Error 0.5797(0.5975) Steps 0(0.00) | Grad Norm 8.6044(12.0834) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 56.0395(55.2668) | Bit/dim 4.0349(4.1689) | Xent 1.5745(1.6740) | Loss 11.2350(12.1382) | Error 0.5653(0.5966) Steps 0(0.00) | Grad Norm 4.4820(11.8554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 22.9155, Epoch Time 372.5685(356.0980), Bit/dim 4.0659(best: 4.0805), Xent 1.5303, Loss 4.8310, Error 0.5465(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 54.0648(55.2308) | Bit/dim 4.0640(4.1658) | Xent 1.5690(1.6708) | Loss 14.7294(12.2160) | Error 0.5613(0.5955) Steps 0(0.00) | Grad Norm 7.4538(11.7233) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 58.8019(55.3379) | Bit/dim 4.0447(4.1621) | Xent 1.5672(1.6677) | Loss 11.2581(12.1872) | Error 0.5590(0.5944) Steps 0(0.00) | Grad Norm 9.2536(11.6492) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 56.8782(55.3841) | Bit/dim 4.0702(4.1594) | Xent 1.6361(1.6668) | Loss 11.3707(12.1627) | Error 0.5730(0.5938) Steps 0(0.00) | Grad Norm 20.7503(11.9223) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 54.8373(55.3677) | Bit/dim 4.0928(4.1574) | Xent 1.8694(1.6729) | Loss 11.7057(12.1490) | Error 0.6392(0.5951) Steps 0(0.00) | Grad Norm 36.0646(12.6465) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 55.5023(55.3717) | Bit/dim 4.0754(4.1549) | Xent 1.5998(1.6707) | Loss 11.2813(12.1230) | Error 0.5755(0.5946) Steps 0(0.00) | Grad Norm 8.2990(12.5161) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 55.0450(55.3619) | Bit/dim 4.0611(4.1521) | Xent 1.6723(1.6707) | Loss 11.3590(12.1001) | Error 0.5985(0.5947) Steps 0(0.00) | Grad Norm 16.4936(12.6354) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 23.6126, Epoch Time 374.7664(356.6580), Bit/dim 4.1121(best: 4.0659), Xent 1.6301, Loss 4.9272, Error 0.5882(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 60.7134(55.5225) | Bit/dim 4.1081(4.1508) | Xent 1.7021(1.6717) | Loss 15.2214(12.1937) | Error 0.5965(0.5947) Steps 0(0.00) | Grad Norm 19.0571(12.8281) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 56.6762(55.5571) | Bit/dim 4.1429(4.1505) | Xent 1.7244(1.6732) | Loss 11.5089(12.1732) | Error 0.6145(0.5953) Steps 0(0.00) | Grad Norm 15.4684(12.9073) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 57.8413(55.6256) | Bit/dim 4.0981(4.1490) | Xent 1.6768(1.6734) | Loss 11.3888(12.1496) | Error 0.6045(0.5956) Steps 0(0.00) | Grad Norm 8.3786(12.7714) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 57.1913(55.6726) | Bit/dim 4.0720(4.1467) | Xent 1.6086(1.6714) | Loss 11.3466(12.1255) | Error 0.5734(0.5949) Steps 0(0.00) | Grad Norm 7.4180(12.6108) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 53.7418(55.6147) | Bit/dim 4.1017(4.1453) | Xent 1.5867(1.6689) | Loss 11.2812(12.1002) | Error 0.5620(0.5939) Steps 0(0.00) | Grad Norm 9.0235(12.5032) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 55.0318(55.5972) | Bit/dim 4.0713(4.1431) | Xent 1.5995(1.6668) | Loss 11.3000(12.0762) | Error 0.5711(0.5933) Steps 0(0.00) | Grad Norm 8.1723(12.3733) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 23.3384, Epoch Time 379.9451(357.3567), Bit/dim 4.0910(best: 4.0659), Xent 1.5685, Loss 4.8753, Error 0.5610(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 52.2205(55.4959) | Bit/dim 4.0975(4.1417) | Xent 1.6105(1.6651) | Loss 14.9220(12.1616) | Error 0.5739(0.5927) Steps 0(0.00) | Grad Norm 14.4076(12.4343) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 52.8295(55.4159) | Bit/dim 4.0790(4.1398) | Xent 1.6083(1.6634) | Loss 11.4016(12.1388) | Error 0.5765(0.5922) Steps 0(0.00) | Grad Norm 11.2698(12.3994) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 56.1545(55.4380) | Bit/dim 4.0549(4.1373) | Xent 1.5832(1.6610) | Loss 11.2306(12.1115) | Error 0.5641(0.5913) Steps 0(0.00) | Grad Norm 7.1049(12.2405) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 55.7735(55.4481) | Bit/dim 4.0446(4.1345) | Xent 1.5646(1.6581) | Loss 11.2562(12.0859) | Error 0.5621(0.5905) Steps 0(0.00) | Grad Norm 5.5607(12.0401) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 57.1487(55.4991) | Bit/dim 4.0444(4.1318) | Xent 1.5482(1.6548) | Loss 11.1557(12.0580) | Error 0.5559(0.5894) Steps 0(0.00) | Grad Norm 7.0370(11.8901) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 57.5621(55.5610) | Bit/dim 4.0330(4.1288) | Xent 1.5633(1.6521) | Loss 11.2830(12.0347) | Error 0.5627(0.5886) Steps 0(0.00) | Grad Norm 8.0952(11.7762) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 23.6062, Epoch Time 371.0172(357.7665), Bit/dim 4.0211(best: 4.0659), Xent 1.4940, Loss 4.7681, Error 0.5321(best: 0.5393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 55.0355(55.5453) | Bit/dim 4.0172(4.1255) | Xent 1.5140(1.6479) | Loss 14.5725(12.1108) | Error 0.5441(0.5873) Steps 0(0.00) | Grad Norm 5.6375(11.5920) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 58.2577(55.6266) | Bit/dim 4.0260(4.1225) | Xent 1.5490(1.6449) | Loss 11.2652(12.0855) | Error 0.5610(0.5865) Steps 0(0.00) | Grad Norm 5.1266(11.3981) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 57.1126(55.6712) | Bit/dim 4.0111(4.1192) | Xent 1.5295(1.6415) | Loss 11.0925(12.0557) | Error 0.5451(0.5853) Steps 0(0.00) | Grad Norm 4.5314(11.1921) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 53.0753(55.5933) | Bit/dim 4.0012(4.1156) | Xent 1.5255(1.6380) | Loss 11.1230(12.0277) | Error 0.5511(0.5842) Steps 0(0.00) | Grad Norm 6.0391(11.0375) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 58.7441(55.6879) | Bit/dim 4.0034(4.1123) | Xent 1.5397(1.6351) | Loss 11.1648(12.0018) | Error 0.5523(0.5833) Steps 0(0.00) | Grad Norm 9.1218(10.9800) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 56.6528(55.7168) | Bit/dim 3.9971(4.1088) | Xent 1.5909(1.6337) | Loss 11.1415(11.9760) | Error 0.5662(0.5828) Steps 0(0.00) | Grad Norm 12.4514(11.0242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 22.7894, Epoch Time 377.2903(358.3522), Bit/dim 4.0024(best: 4.0211), Xent 1.5378, Loss 4.7713, Error 0.5524(best: 0.5321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 57.7469(55.7777) | Bit/dim 3.9986(4.1055) | Xent 1.5973(1.6326) | Loss 14.4348(12.0498) | Error 0.5747(0.5825) Steps 0(0.00) | Grad Norm 17.9347(11.2315) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 54.4410(55.7376) | Bit/dim 4.0214(4.1030) | Xent 1.6347(1.6327) | Loss 11.2460(12.0257) | Error 0.5836(0.5826) Steps 0(0.00) | Grad Norm 19.9713(11.4937) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 57.1231(55.7792) | Bit/dim 3.9984(4.0998) | Xent 1.5636(1.6306) | Loss 11.1015(11.9979) | Error 0.5511(0.5816) Steps 0(0.00) | Grad Norm 11.0029(11.4789) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 56.6371(55.8049) | Bit/dim 4.0030(4.0969) | Xent 1.5434(1.6280) | Loss 11.0913(11.9707) | Error 0.5546(0.5808) Steps 0(0.00) | Grad Norm 10.8333(11.4596) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 56.3879(55.8224) | Bit/dim 3.9906(4.0937) | Xent 1.6342(1.6282) | Loss 11.1399(11.9458) | Error 0.5851(0.5809) Steps 0(0.00) | Grad Norm 15.9609(11.5946) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 56.9562(55.8564) | Bit/dim 3.9826(4.0904) | Xent 1.5421(1.6256) | Loss 10.9321(11.9154) | Error 0.5560(0.5802) Steps 0(0.00) | Grad Norm 7.1303(11.4607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 23.3372, Epoch Time 378.5218(358.9573), Bit/dim 3.9795(best: 4.0024), Xent 1.5134, Loss 4.7362, Error 0.5402(best: 0.5321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 57.1709(55.8958) | Bit/dim 3.9647(4.0866) | Xent 1.5680(1.6239) | Loss 14.5775(11.9953) | Error 0.5609(0.5796) Steps 0(0.00) | Grad Norm 10.5919(11.4346) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 56.0709(55.9011) | Bit/dim 3.9837(4.0836) | Xent 1.5346(1.6212) | Loss 11.1053(11.9686) | Error 0.5469(0.5786) Steps 0(0.00) | Grad Norm 5.8800(11.2680) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 56.6494(55.9235) | Bit/dim 3.9813(4.0805) | Xent 1.5078(1.6178) | Loss 11.0654(11.9415) | Error 0.5446(0.5776) Steps 0(0.00) | Grad Norm 9.6577(11.2197) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 58.3012(55.9949) | Bit/dim 3.9759(4.0773) | Xent 1.5331(1.6153) | Loss 11.1619(11.9181) | Error 0.5471(0.5767) Steps 0(0.00) | Grad Norm 8.1202(11.1267) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 57.1804(56.0304) | Bit/dim 3.9754(4.0743) | Xent 1.4832(1.6113) | Loss 11.0919(11.8933) | Error 0.5324(0.5754) Steps 0(0.00) | Grad Norm 4.9736(10.9421) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 63.3067(56.2487) | Bit/dim 3.9690(4.0711) | Xent 1.4756(1.6072) | Loss 10.9840(11.8660) | Error 0.5272(0.5739) Steps 0(0.00) | Grad Norm 6.3550(10.8045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 22.6963, Epoch Time 386.9384(359.7967), Bit/dim 3.9739(best: 3.9795), Xent 1.4320, Loss 4.6898, Error 0.5144(best: 0.5321)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 57.2328(56.2782) | Bit/dim 3.9652(4.0680) | Xent 1.4776(1.6033) | Loss 14.4759(11.9443) | Error 0.5323(0.5727) Steps 0(0.00) | Grad Norm 5.0477(10.6318) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 55.4138(56.2523) | Bit/dim 3.9718(4.0651) | Xent 1.4721(1.5994) | Loss 10.9714(11.9151) | Error 0.5315(0.5714) Steps 0(0.00) | Grad Norm 4.5850(10.4504) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 58.1198(56.3083) | Bit/dim 3.9602(4.0619) | Xent 1.4937(1.5962) | Loss 10.9668(11.8867) | Error 0.5357(0.5704) Steps 0(0.00) | Grad Norm 5.7653(10.3098) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 58.8566(56.3848) | Bit/dim 3.9483(4.0585) | Xent 1.4722(1.5925) | Loss 10.8347(11.8551) | Error 0.5357(0.5693) Steps 0(0.00) | Grad Norm 4.6636(10.1404) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 57.2919(56.4120) | Bit/dim 3.9437(4.0551) | Xent 1.4478(1.5882) | Loss 10.8997(11.8265) | Error 0.5229(0.5679) Steps 0(0.00) | Grad Norm 2.3423(9.9065) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 58.3167(56.4691) | Bit/dim 3.9371(4.0515) | Xent 1.4353(1.5836) | Loss 10.9083(11.7989) | Error 0.5169(0.5664) Steps 0(0.00) | Grad Norm 3.5723(9.7165) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 23.2818, Epoch Time 384.3070(360.5320), Bit/dim 3.9480(best: 3.9739), Xent 1.4083, Loss 4.6521, Error 0.5074(best: 0.5144)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 56.5186(56.4706) | Bit/dim 3.9357(4.0481) | Xent 1.4392(1.5793) | Loss 14.2833(11.8734) | Error 0.5242(0.5651) Steps 0(0.00) | Grad Norm 3.9785(9.5443) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 52.8646(56.3624) | Bit/dim 3.9481(4.0451) | Xent 1.4491(1.5753) | Loss 10.8752(11.8435) | Error 0.5240(0.5639) Steps 0(0.00) | Grad Norm 3.6169(9.3665) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 56.6591(56.3713) | Bit/dim 3.9424(4.0420) | Xent 1.4593(1.5719) | Loss 10.8809(11.8146) | Error 0.5250(0.5627) Steps 0(0.00) | Grad Norm 5.3354(9.2456) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 53.8337(56.2952) | Bit/dim 3.9344(4.0388) | Xent 1.4640(1.5686) | Loss 10.8881(11.7868) | Error 0.5255(0.5616) Steps 0(0.00) | Grad Norm 11.1453(9.3026) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 56.0461(56.2877) | Bit/dim 3.9748(4.0368) | Xent 1.7603(1.5744) | Loss 11.2783(11.7716) | Error 0.6070(0.5630) Steps 0(0.00) | Grad Norm 26.5122(9.8189) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 56.9510(56.3076) | Bit/dim 3.9648(4.0347) | Xent 1.7221(1.5788) | Loss 11.2228(11.7551) | Error 0.6035(0.5642) Steps 0(0.00) | Grad Norm 18.2730(10.0725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 23.0977, Epoch Time 371.6180(360.8646), Bit/dim 3.9564(best: 3.9480), Xent 1.5415, Loss 4.7271, Error 0.5542(best: 0.5074)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 56.4876(56.3130) | Bit/dim 3.9593(4.0324) | Xent 1.5912(1.5792) | Loss 14.7547(11.8451) | Error 0.5709(0.5644) Steps 0(0.00) | Grad Norm 11.5305(10.1162) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 57.0322(56.3346) | Bit/dim 3.9759(4.0307) | Xent 1.6089(1.5801) | Loss 11.2040(11.8259) | Error 0.5801(0.5649) Steps 0(0.00) | Grad Norm 14.7239(10.2544) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 56.5168(56.3401) | Bit/dim 3.9900(4.0295) | Xent 1.6119(1.5810) | Loss 11.0388(11.8023) | Error 0.5757(0.5652) Steps 0(0.00) | Grad Norm 16.0995(10.4298) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 56.0979(56.3328) | Bit/dim 3.9652(4.0276) | Xent 1.5382(1.5797) | Loss 11.1172(11.7817) | Error 0.5583(0.5650) Steps 0(0.00) | Grad Norm 9.8758(10.4132) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 58.5873(56.4004) | Bit/dim 3.9391(4.0249) | Xent 1.5399(1.5786) | Loss 10.9379(11.7564) | Error 0.5600(0.5648) Steps 0(0.00) | Grad Norm 8.9690(10.3699) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 59.2216(56.4851) | Bit/dim 3.9716(4.0233) | Xent 1.5223(1.5769) | Loss 11.1010(11.7367) | Error 0.5544(0.5645) Steps 0(0.00) | Grad Norm 8.9841(10.3283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 22.4823, Epoch Time 382.1306(361.5026), Bit/dim 3.9483(best: 3.9480), Xent 1.4580, Loss 4.6773, Error 0.5266(best: 0.5074)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 56.5020(56.4856) | Bit/dim 3.9477(4.0210) | Xent 1.4896(1.5742) | Loss 14.4580(11.8184) | Error 0.5488(0.5641) Steps 0(0.00) | Grad Norm 4.7739(10.1617) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 59.6247(56.5798) | Bit/dim 3.9701(4.0195) | Xent 1.4938(1.5718) | Loss 10.8527(11.7894) | Error 0.5366(0.5632) Steps 0(0.00) | Grad Norm 6.4157(10.0493) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 61.6186(56.7309) | Bit/dim 3.9440(4.0173) | Xent 1.4840(1.5692) | Loss 10.9624(11.7646) | Error 0.5403(0.5625) Steps 0(0.00) | Grad Norm 3.7869(9.8614) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 58.7498(56.7915) | Bit/dim 3.9480(4.0152) | Xent 1.4665(1.5661) | Loss 10.9376(11.7398) | Error 0.5385(0.5618) Steps 0(0.00) | Grad Norm 6.1626(9.7504) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 58.7617(56.8506) | Bit/dim 3.9505(4.0132) | Xent 1.4540(1.5628) | Loss 10.9047(11.7147) | Error 0.5261(0.5607) Steps 0(0.00) | Grad Norm 6.9914(9.6677) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 52.4568(56.7188) | Bit/dim 3.9417(4.0111) | Xent 1.4821(1.5603) | Loss 10.9266(11.6911) | Error 0.5311(0.5599) Steps 0(0.00) | Grad Norm 8.2422(9.6249) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 22.6101, Epoch Time 386.1332(362.2415), Bit/dim 3.9359(best: 3.9480), Xent 1.4243, Loss 4.6481, Error 0.5171(best: 0.5074)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 56.6827(56.7177) | Bit/dim 3.9295(4.0086) | Xent 1.4636(1.5574) | Loss 14.4469(11.7738) | Error 0.5295(0.5589) Steps 0(0.00) | Grad Norm 8.7333(9.5982) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 54.5884(56.6538) | Bit/dim 3.9195(4.0060) | Xent 1.5232(1.5564) | Loss 10.9174(11.7481) | Error 0.5454(0.5585) Steps 0(0.00) | Grad Norm 13.5583(9.7170) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 53.3247(56.5540) | Bit/dim 3.9449(4.0041) | Xent 1.5429(1.5560) | Loss 10.8765(11.7219) | Error 0.5510(0.5583) Steps 0(0.00) | Grad Norm 18.9229(9.9931) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 54.7675(56.5004) | Bit/dim 3.9587(4.0028) | Xent 1.5434(1.5556) | Loss 10.9177(11.6978) | Error 0.5520(0.5581) Steps 0(0.00) | Grad Norm 14.6018(10.1314) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 56.7346(56.5074) | Bit/dim 3.9290(4.0006) | Xent 1.4567(1.5527) | Loss 10.8102(11.6712) | Error 0.5291(0.5573) Steps 0(0.00) | Grad Norm 4.0720(9.9496) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 56.3731(56.5034) | Bit/dim 3.9373(3.9987) | Xent 1.5031(1.5512) | Loss 10.9868(11.6506) | Error 0.5449(0.5569) Steps 0(0.00) | Grad Norm 13.0468(10.0425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 22.2685, Epoch Time 370.2187(362.4808), Bit/dim 3.9232(best: 3.9359), Xent 1.4401, Loss 4.6433, Error 0.5157(best: 0.5074)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 59.0525(56.5798) | Bit/dim 3.9154(3.9962) | Xent 1.4923(1.5494) | Loss 14.2456(11.7285) | Error 0.5368(0.5563) Steps 0(0.00) | Grad Norm 6.3567(9.9320) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 53.1086(56.4757) | Bit/dim 3.9397(3.9945) | Xent 1.4408(1.5461) | Loss 10.7918(11.7004) | Error 0.5157(0.5551) Steps 0(0.00) | Grad Norm 6.8418(9.8392) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 56.5128(56.4768) | Bit/dim 3.9145(3.9921) | Xent 1.4513(1.5433) | Loss 10.8403(11.6746) | Error 0.5185(0.5540) Steps 0(0.00) | Grad Norm 6.6389(9.7432) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 53.1170(56.3760) | Bit/dim 3.9202(3.9899) | Xent 1.4562(1.5407) | Loss 10.7694(11.6474) | Error 0.5256(0.5531) Steps 0(0.00) | Grad Norm 6.6247(9.6497) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 55.8832(56.3612) | Bit/dim 3.9164(3.9877) | Xent 1.4383(1.5376) | Loss 10.8793(11.6244) | Error 0.5217(0.5522) Steps 0(0.00) | Grad Norm 5.1529(9.5148) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 56.7349(56.3724) | Bit/dim 3.9044(3.9852) | Xent 1.4077(1.5337) | Loss 10.7478(11.5981) | Error 0.5105(0.5509) Steps 0(0.00) | Grad Norm 4.1830(9.3548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 22.5964, Epoch Time 372.6173(362.7849), Bit/dim 3.9155(best: 3.9232), Xent 1.3959, Loss 4.6135, Error 0.5061(best: 0.5074)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 53.9255(56.2990) | Bit/dim 3.9133(3.9831) | Xent 1.4314(1.5306) | Loss 14.4156(11.6826) | Error 0.5197(0.5500) Steps 0(0.00) | Grad Norm 7.7582(9.3069) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 59.2797(56.3884) | Bit/dim 3.9219(3.9812) | Xent 1.4642(1.5287) | Loss 10.9168(11.6596) | Error 0.5262(0.5493) Steps 0(0.00) | Grad Norm 11.4832(9.3722) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 54.2463(56.3242) | Bit/dim 3.9221(3.9794) | Xent 1.5428(1.5291) | Loss 10.9821(11.6393) | Error 0.5400(0.5490) Steps 0(0.00) | Grad Norm 16.6508(9.5906) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 55.9241(56.3122) | Bit/dim 3.9431(3.9784) | Xent 1.6706(1.5333) | Loss 11.1855(11.6257) | Error 0.5844(0.5501) Steps 0(0.00) | Grad Norm 17.9199(9.8405) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 55.3994(56.2848) | Bit/dim 4.0053(3.9792) | Xent 1.6429(1.5366) | Loss 11.2325(11.6139) | Error 0.5946(0.5514) Steps 0(0.00) | Grad Norm 18.6796(10.1056) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 57.7787(56.3296) | Bit/dim 3.9924(3.9796) | Xent 1.8763(1.5468) | Loss 11.3925(11.6073) | Error 0.6451(0.5542) Steps 0(0.00) | Grad Norm 30.4052(10.7146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 22.8614, Epoch Time 374.9475(363.1498), Bit/dim 4.0079(best: 3.9155), Xent 1.5103, Loss 4.7631, Error 0.5518(best: 0.5061)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 53.6049(56.2479) | Bit/dim 4.0129(3.9806) | Xent 1.5637(1.5473) | Loss 14.7832(11.7025) | Error 0.5674(0.5546) Steps 0(0.00) | Grad Norm 14.0838(10.8157) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 54.4963(56.1953) | Bit/dim 4.0130(3.9815) | Xent 1.5098(1.5462) | Loss 11.1437(11.6858) | Error 0.5421(0.5542) Steps 0(0.00) | Grad Norm 9.0976(10.7641) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 52.8353(56.0945) | Bit/dim 4.0139(3.9825) | Xent 1.5075(1.5450) | Loss 11.2623(11.6731) | Error 0.5524(0.5542) Steps 0(0.00) | Grad Norm 7.5448(10.6676) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 56.4903(56.1064) | Bit/dim 3.9553(3.9817) | Xent 1.4754(1.5429) | Loss 10.9711(11.6520) | Error 0.5285(0.5534) Steps 0(0.00) | Grad Norm 4.6315(10.4865) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 55.2091(56.0795) | Bit/dim 3.9846(3.9818) | Xent 1.4925(1.5414) | Loss 11.0018(11.6325) | Error 0.5488(0.5533) Steps 0(0.00) | Grad Norm 7.5803(10.3993) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 53.3223(55.9968) | Bit/dim 3.9399(3.9805) | Xent 1.5059(1.5404) | Loss 11.0264(11.6143) | Error 0.5400(0.5529) Steps 0(0.00) | Grad Norm 8.6489(10.3468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 22.7899, Epoch Time 364.5008(363.1903), Bit/dim 3.9583(best: 3.9155), Xent 1.4556, Loss 4.6861, Error 0.5291(best: 0.5061)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 53.9737(55.9361) | Bit/dim 3.9657(3.9801) | Xent 1.4839(1.5387) | Loss 14.6048(11.7040) | Error 0.5463(0.5527) Steps 0(0.00) | Grad Norm 11.1551(10.3710) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 56.6994(55.9590) | Bit/dim 3.9764(3.9800) | Xent 1.5700(1.5396) | Loss 11.0469(11.6843) | Error 0.5555(0.5528) Steps 0(0.00) | Grad Norm 21.5002(10.7049) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 54.9414(55.9284) | Bit/dim 4.0094(3.9808) | Xent 1.8314(1.5484) | Loss 11.4902(11.6785) | Error 0.6432(0.5555) Steps 0(0.00) | Grad Norm 23.6617(11.0936) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 53.9852(55.8701) | Bit/dim 3.9775(3.9807) | Xent 1.5087(1.5472) | Loss 10.9919(11.6579) | Error 0.5509(0.5553) Steps 0(0.00) | Grad Norm 6.9691(10.9699) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 55.3318(55.8540) | Bit/dim 3.9826(3.9808) | Xent 1.5858(1.5483) | Loss 11.0459(11.6395) | Error 0.5697(0.5558) Steps 0(0.00) | Grad Norm 13.0569(11.0325) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 53.7697(55.7915) | Bit/dim 3.9731(3.9806) | Xent 1.5138(1.5473) | Loss 10.9665(11.6193) | Error 0.5408(0.5553) Steps 0(0.00) | Grad Norm 6.2461(10.8889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 23.2319, Epoch Time 367.8021(363.3287), Bit/dim 3.9580(best: 3.9155), Xent 1.4690, Loss 4.6925, Error 0.5212(best: 0.5061)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 58.3469(55.8681) | Bit/dim 3.9587(3.9799) | Xent 1.5299(1.5468) | Loss 14.4091(11.7030) | Error 0.5517(0.5552) Steps 0(0.00) | Grad Norm 7.0066(10.7724) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 58.0663(55.9341) | Bit/dim 3.9586(3.9793) | Xent 1.4949(1.5452) | Loss 11.0516(11.6835) | Error 0.5368(0.5547) Steps 0(0.00) | Grad Norm 4.9299(10.5972) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 58.1986(56.0020) | Bit/dim 3.9477(3.9783) | Xent 1.4990(1.5438) | Loss 10.9575(11.6617) | Error 0.5333(0.5540) Steps 0(0.00) | Grad Norm 7.3355(10.4993) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 56.8256(56.0267) | Bit/dim 3.9414(3.9772) | Xent 1.4904(1.5422) | Loss 10.8729(11.6380) | Error 0.5341(0.5534) Steps 0(0.00) | Grad Norm 5.5829(10.3518) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 56.5017(56.0410) | Bit/dim 3.9438(3.9762) | Xent 1.4648(1.5399) | Loss 10.9052(11.6161) | Error 0.5245(0.5525) Steps 0(0.00) | Grad Norm 5.8907(10.2180) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 55.8594(56.0355) | Bit/dim 3.9254(3.9747) | Xent 1.4347(1.5367) | Loss 10.8378(11.5927) | Error 0.5197(0.5516) Steps 0(0.00) | Grad Norm 3.3315(10.0114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 23.0274, Epoch Time 382.2081(363.8950), Bit/dim 3.9364(best: 3.9155), Xent 1.3939, Loss 4.6333, Error 0.5039(best: 0.5061)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 60.4877(56.1691) | Bit/dim 3.9385(3.9736) | Xent 1.4320(1.5336) | Loss 14.4336(11.6779) | Error 0.5155(0.5505) Steps 0(0.00) | Grad Norm 5.7470(9.8835) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 56.0800(56.1664) | Bit/dim 3.9327(3.9724) | Xent 1.4319(1.5305) | Loss 10.8716(11.6538) | Error 0.5162(0.5495) Steps 0(0.00) | Grad Norm 5.7290(9.7588) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 54.9298(56.1293) | Bit/dim 3.9256(3.9710) | Xent 1.4373(1.5278) | Loss 10.8727(11.6303) | Error 0.5222(0.5486) Steps 0(0.00) | Grad Norm 6.0537(9.6477) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 60.0145(56.2459) | Bit/dim 3.9026(3.9689) | Xent 1.4221(1.5246) | Loss 10.8205(11.6060) | Error 0.5148(0.5476) Steps 0(0.00) | Grad Norm 5.1829(9.5137) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 55.9327(56.2365) | Bit/dim 3.9025(3.9669) | Xent 1.4201(1.5214) | Loss 10.8203(11.5825) | Error 0.5142(0.5466) Steps 0(0.00) | Grad Norm 3.6575(9.3380) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 57.9997(56.2894) | Bit/dim 3.8961(3.9648) | Xent 1.4149(1.5183) | Loss 10.7376(11.5571) | Error 0.5102(0.5455) Steps 0(0.00) | Grad Norm 3.3650(9.1588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 22.9976, Epoch Time 384.5783(364.5155), Bit/dim 3.9017(best: 3.9155), Xent 1.3582, Loss 4.5808, Error 0.4892(best: 0.5039)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 57.8748(56.3369) | Bit/dim 3.8941(3.9627) | Xent 1.4142(1.5151) | Loss 14.4805(11.6448) | Error 0.5112(0.5445) Steps 0(0.00) | Grad Norm 3.3466(8.9845) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 54.1665(56.2718) | Bit/dim 3.8772(3.9601) | Xent 1.3915(1.5114) | Loss 10.6882(11.6161) | Error 0.5061(0.5433) Steps 0(0.00) | Grad Norm 2.9824(8.8044) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 60.0268(56.3845) | Bit/dim 3.8781(3.9577) | Xent 1.3656(1.5070) | Loss 10.7154(11.5891) | Error 0.4900(0.5417) Steps 0(0.00) | Grad Norm 3.3804(8.6417) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 56.8877(56.3996) | Bit/dim 3.8994(3.9559) | Xent 1.3912(1.5036) | Loss 10.7489(11.5639) | Error 0.4998(0.5405) Steps 0(0.00) | Grad Norm 4.8093(8.5267) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 57.9414(56.4458) | Bit/dim 3.8856(3.9538) | Xent 1.4162(1.5009) | Loss 10.6857(11.5375) | Error 0.5126(0.5397) Steps 0(0.00) | Grad Norm 9.8265(8.5657) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 58.2843(56.5010) | Bit/dim 3.8953(3.9521) | Xent 1.6048(1.5041) | Loss 10.9900(11.5211) | Error 0.5654(0.5404) Steps 0(0.00) | Grad Norm 21.9909(8.9685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 23.2427, Epoch Time 384.5132(365.1155), Bit/dim 3.9422(best: 3.9017), Xent 1.8233, Loss 4.8538, Error 0.6296(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 55.6606(56.4758) | Bit/dim 3.9451(3.9518) | Xent 1.8603(1.5148) | Loss 14.8445(11.6208) | Error 0.6330(0.5432) Steps 0(0.00) | Grad Norm 23.7659(9.4124) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 61.0145(56.6119) | Bit/dim 3.9481(3.9517) | Xent 1.6224(1.5180) | Loss 11.0980(11.6051) | Error 0.5769(0.5442) Steps 0(0.00) | Grad Norm 11.1042(9.4632) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 58.1235(56.6573) | Bit/dim 3.9576(3.9519) | Xent 1.5570(1.5192) | Loss 11.1163(11.5905) | Error 0.5625(0.5448) Steps 0(0.00) | Grad Norm 11.7400(9.5315) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 56.2069(56.6438) | Bit/dim 3.9127(3.9507) | Xent 1.6522(1.5231) | Loss 11.1324(11.5767) | Error 0.5953(0.5463) Steps 0(0.00) | Grad Norm 15.5511(9.7120) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 54.1153(56.5679) | Bit/dim 3.9579(3.9509) | Xent 1.8405(1.5327) | Loss 11.3532(11.5700) | Error 0.6331(0.5489) Steps 0(0.00) | Grad Norm 24.8306(10.1656) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 54.3377(56.5010) | Bit/dim 4.0023(3.9525) | Xent 1.6011(1.5347) | Loss 11.1902(11.5586) | Error 0.5727(0.5496) Steps 0(0.00) | Grad Norm 12.3976(10.2326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 23.4296, Epoch Time 379.2054(365.5382), Bit/dim 3.9732(best: 3.9017), Xent 1.4670, Loss 4.7067, Error 0.5218(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 56.8576(56.5117) | Bit/dim 3.9790(3.9533) | Xent 1.5055(1.5338) | Loss 14.7416(11.6541) | Error 0.5365(0.5492) Steps 0(0.00) | Grad Norm 8.6290(10.1845) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 57.7401(56.5486) | Bit/dim 3.9596(3.9535) | Xent 1.5387(1.5340) | Loss 11.0394(11.6357) | Error 0.5533(0.5493) Steps 0(0.00) | Grad Norm 6.7507(10.0814) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 58.6917(56.6128) | Bit/dim 3.9749(3.9541) | Xent 1.5923(1.5357) | Loss 11.0891(11.6193) | Error 0.5705(0.5500) Steps 0(0.00) | Grad Norm 8.5675(10.0360) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 60.7143(56.7359) | Bit/dim 3.9378(3.9536) | Xent 1.5093(1.5349) | Loss 11.0262(11.6015) | Error 0.5468(0.5499) Steps 0(0.00) | Grad Norm 5.2292(9.8918) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 59.1323(56.8078) | Bit/dim 3.9416(3.9533) | Xent 1.4873(1.5335) | Loss 11.0747(11.5857) | Error 0.5434(0.5497) Steps 0(0.00) | Grad Norm 5.7068(9.7663) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 57.0554(56.8152) | Bit/dim 3.9331(3.9527) | Xent 1.4659(1.5315) | Loss 10.9161(11.5656) | Error 0.5325(0.5492) Steps 0(0.00) | Grad Norm 3.9301(9.5912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 24.4735, Epoch Time 390.2236(366.2787), Bit/dim 3.9373(best: 3.9017), Xent 1.4336, Loss 4.6541, Error 0.5192(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 60.4593(56.9245) | Bit/dim 3.9355(3.9521) | Xent 1.4768(1.5298) | Loss 14.7704(11.6617) | Error 0.5331(0.5487) Steps 0(0.00) | Grad Norm 4.1323(9.4274) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 60.5239(57.0325) | Bit/dim 3.9202(3.9512) | Xent 1.4698(1.5280) | Loss 10.9144(11.6393) | Error 0.5334(0.5482) Steps 0(0.00) | Grad Norm 4.7424(9.2869) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 60.1840(57.1271) | Bit/dim 3.9096(3.9499) | Xent 1.4714(1.5263) | Loss 10.9770(11.6194) | Error 0.5337(0.5478) Steps 0(0.00) | Grad Norm 4.3785(9.1396) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 56.2155(57.0997) | Bit/dim 3.9107(3.9488) | Xent 1.4612(1.5244) | Loss 10.8826(11.5973) | Error 0.5305(0.5473) Steps 0(0.00) | Grad Norm 4.7478(9.0079) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 59.1487(57.1612) | Bit/dim 3.8999(3.9473) | Xent 1.4246(1.5214) | Loss 10.8522(11.5750) | Error 0.5216(0.5465) Steps 0(0.00) | Grad Norm 3.5742(8.8449) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 58.3289(57.1962) | Bit/dim 3.8973(3.9458) | Xent 1.4050(1.5179) | Loss 10.7893(11.5514) | Error 0.5116(0.5454) Steps 0(0.00) | Grad Norm 3.1597(8.6743) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 23.2623, Epoch Time 394.3497(367.1209), Bit/dim 3.8964(best: 3.9017), Xent 1.3722, Loss 4.5825, Error 0.4996(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 56.2529(57.1679) | Bit/dim 3.8974(3.9443) | Xent 1.4295(1.5153) | Loss 14.2019(11.6309) | Error 0.5114(0.5444) Steps 0(0.00) | Grad Norm 3.9813(8.5335) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 56.2028(57.1390) | Bit/dim 3.8870(3.9426) | Xent 1.4212(1.5124) | Loss 10.7415(11.6042) | Error 0.5105(0.5434) Steps 0(0.00) | Grad Norm 5.7312(8.4494) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 56.2535(57.1124) | Bit/dim 3.8796(3.9407) | Xent 1.4403(1.5103) | Loss 10.8580(11.5819) | Error 0.5204(0.5427) Steps 0(0.00) | Grad Norm 9.3847(8.4775) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 61.6986(57.2500) | Bit/dim 3.8873(3.9391) | Xent 1.4846(1.5095) | Loss 10.7976(11.5583) | Error 0.5297(0.5423) Steps 0(0.00) | Grad Norm 17.6947(8.7540) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 57.3299(57.2524) | Bit/dim 3.9414(3.9392) | Xent 1.7036(1.5153) | Loss 11.2209(11.5482) | Error 0.6025(0.5441) Steps 0(0.00) | Grad Norm 18.9379(9.0595) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 63.4089(57.4371) | Bit/dim 3.9107(3.9383) | Xent 1.4918(1.5146) | Loss 10.9191(11.5293) | Error 0.5377(0.5439) Steps 0(0.00) | Grad Norm 10.3221(9.0974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 24.2367, Epoch Time 391.2177(367.8438), Bit/dim 3.9904(best: 3.8964), Xent 1.5027, Loss 4.7417, Error 0.5345(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 62.1154(57.5774) | Bit/dim 4.0005(3.9402) | Xent 1.5383(1.5153) | Loss 14.8843(11.6300) | Error 0.5533(0.5442) Steps 0(0.00) | Grad Norm 14.9391(9.2727) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 60.5193(57.6657) | Bit/dim 3.9376(3.9401) | Xent 1.5264(1.5157) | Loss 11.0192(11.6117) | Error 0.5565(0.5446) Steps 0(0.00) | Grad Norm 7.6755(9.2247) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 59.1580(57.7104) | Bit/dim 3.9182(3.9395) | Xent 1.4590(1.5140) | Loss 10.9624(11.5922) | Error 0.5242(0.5440) Steps 0(0.00) | Grad Norm 6.7624(9.1509) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 55.2839(57.6377) | Bit/dim 3.9519(3.9398) | Xent 1.4543(1.5122) | Loss 10.9438(11.5727) | Error 0.5294(0.5435) Steps 0(0.00) | Grad Norm 6.4044(9.0685) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 54.4451(57.5419) | Bit/dim 3.9325(3.9396) | Xent 1.4357(1.5099) | Loss 10.9497(11.5540) | Error 0.5188(0.5428) Steps 0(0.00) | Grad Norm 5.1869(8.9520) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 57.0583(57.5274) | Bit/dim 3.9135(3.9388) | Xent 1.4506(1.5081) | Loss 10.7275(11.5292) | Error 0.5256(0.5423) Steps 0(0.00) | Grad Norm 7.0627(8.8954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 23.5748, Epoch Time 387.9241(368.4462), Bit/dim 3.9124(best: 3.8964), Xent 1.4248, Loss 4.6248, Error 0.5169(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 59.6880(57.5922) | Bit/dim 3.9174(3.9382) | Xent 1.4476(1.5063) | Loss 14.4331(11.6164) | Error 0.5259(0.5418) Steps 0(0.00) | Grad Norm 9.5822(8.9160) | Total Time 0.00(0.00)\n",
      "Iter 0524 | Time 56.9019(57.5715) | Bit/dim 3.9288(3.9379) | Xent 1.4750(1.5053) | Loss 10.8576(11.5936) | Error 0.5162(0.5410) Steps 0(0.00) | Grad Norm 14.9575(9.0972) | Total Time 0.00(0.00)\n",
      "Iter 0525 | Time 55.4434(57.5076) | Bit/dim 3.9192(3.9373) | Xent 1.5823(1.5077) | Loss 10.9426(11.5741) | Error 0.5588(0.5416) Steps 0(0.00) | Grad Norm 14.6150(9.2627) | Total Time 0.00(0.00)\n",
      "Iter 0526 | Time 59.6840(57.5729) | Bit/dim 3.9148(3.9367) | Xent 1.4523(1.5060) | Loss 10.7228(11.5485) | Error 0.5284(0.5412) Steps 0(0.00) | Grad Norm 6.5108(9.1802) | Total Time 0.00(0.00)\n",
      "Iter 0527 | Time 57.5389(57.5719) | Bit/dim 3.8902(3.9353) | Xent 1.4998(1.5058) | Loss 10.8813(11.5285) | Error 0.5356(0.5410) Steps 0(0.00) | Grad Norm 6.6417(9.1040) | Total Time 0.00(0.00)\n",
      "Iter 0528 | Time 58.9059(57.6119) | Bit/dim 3.9282(3.9351) | Xent 1.4550(1.5043) | Loss 10.9695(11.5117) | Error 0.5270(0.5406) Steps 0(0.00) | Grad Norm 6.2722(9.0191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 23.7491, Epoch Time 387.4097(369.0151), Bit/dim 3.8952(best: 3.8964), Xent 1.3990, Loss 4.5947, Error 0.5043(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 56.8300(57.5885) | Bit/dim 3.9007(3.9340) | Xent 1.4487(1.5026) | Loss 14.4301(11.5993) | Error 0.5191(0.5399) Steps 0(0.00) | Grad Norm 5.4589(8.9123) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 63.6536(57.7704) | Bit/dim 3.8919(3.9328) | Xent 1.4048(1.4997) | Loss 10.7836(11.5748) | Error 0.5039(0.5388) Steps 0(0.00) | Grad Norm 4.1100(8.7682) | Total Time 0.00(0.00)\n",
      "Iter 0531 | Time 56.2292(57.7242) | Bit/dim 3.8756(3.9311) | Xent 1.4127(1.4971) | Loss 10.7026(11.5486) | Error 0.5161(0.5382) Steps 0(0.00) | Grad Norm 5.1820(8.6606) | Total Time 0.00(0.00)\n",
      "Iter 0532 | Time 55.2671(57.6505) | Bit/dim 3.8661(3.9291) | Xent 1.4152(1.4946) | Loss 10.7550(11.5248) | Error 0.5048(0.5372) Steps 0(0.00) | Grad Norm 3.2069(8.4970) | Total Time 0.00(0.00)\n",
      "Iter 0533 | Time 59.4404(57.7042) | Bit/dim 3.8796(3.9276) | Xent 1.3959(1.4916) | Loss 10.7877(11.5027) | Error 0.5080(0.5363) Steps 0(0.00) | Grad Norm 4.9042(8.3892) | Total Time 0.00(0.00)\n",
      "Iter 0534 | Time 59.9845(57.7726) | Bit/dim 3.8708(3.9259) | Xent 1.3889(1.4886) | Loss 10.6910(11.4784) | Error 0.5055(0.5354) Steps 0(0.00) | Grad Norm 3.9651(8.2565) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 23.3100, Epoch Time 390.5343(369.6607), Bit/dim 3.8671(best: 3.8952), Xent 1.3585, Loss 4.5463, Error 0.4929(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 56.3934(57.7312) | Bit/dim 3.8620(3.9240) | Xent 1.3743(1.4851) | Loss 14.1367(11.5581) | Error 0.4992(0.5343) Steps 0(0.00) | Grad Norm 5.4435(8.1721) | Total Time 0.00(0.00)\n",
      "Iter 0536 | Time 59.5165(57.7848) | Bit/dim 3.8617(3.9221) | Xent 1.3978(1.4825) | Loss 10.6465(11.5308) | Error 0.5108(0.5336) Steps 0(0.00) | Grad Norm 8.6889(8.1876) | Total Time 0.00(0.00)\n",
      "Iter 0537 | Time 56.3872(57.7428) | Bit/dim 3.8772(3.9208) | Xent 1.5051(1.4832) | Loss 10.8495(11.5103) | Error 0.5364(0.5337) Steps 0(0.00) | Grad Norm 14.6886(8.3826) | Total Time 0.00(0.00)\n",
      "Iter 0538 | Time 58.1682(57.7556) | Bit/dim 3.9085(3.9204) | Xent 1.6204(1.4873) | Loss 11.0513(11.4966) | Error 0.5687(0.5347) Steps 0(0.00) | Grad Norm 21.8056(8.7853) | Total Time 0.00(0.00)\n",
      "Iter 0539 | Time 55.7961(57.6968) | Bit/dim 3.8960(3.9197) | Xent 1.5479(1.4891) | Loss 10.9428(11.4800) | Error 0.5555(0.5353) Steps 0(0.00) | Grad Norm 11.0786(8.8541) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 61.2027(57.8020) | Bit/dim 3.8737(3.9183) | Xent 1.5118(1.4898) | Loss 10.8837(11.4621) | Error 0.5457(0.5356) Steps 0(0.00) | Grad Norm 7.3142(8.8079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 24.2535, Epoch Time 387.2389(370.1880), Bit/dim 3.8843(best: 3.8671), Xent 1.4412, Loss 4.6050, Error 0.5313(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 57.5796(57.7953) | Bit/dim 3.8776(3.9171) | Xent 1.4796(1.4895) | Loss 14.5856(11.5558) | Error 0.5356(0.5356) Steps 0(0.00) | Grad Norm 7.9897(8.7834) | Total Time 0.00(0.00)\n",
      "Iter 0542 | Time 58.3559(57.8121) | Bit/dim 3.8817(3.9160) | Xent 1.4536(1.4884) | Loss 10.8537(11.5347) | Error 0.5200(0.5352) Steps 0(0.00) | Grad Norm 7.2283(8.7367) | Total Time 0.00(0.00)\n",
      "Iter 0543 | Time 58.6011(57.8358) | Bit/dim 3.8716(3.9147) | Xent 1.4168(1.4863) | Loss 10.6956(11.5095) | Error 0.5142(0.5346) Steps 0(0.00) | Grad Norm 4.0699(8.5967) | Total Time 0.00(0.00)\n",
      "Iter 0544 | Time 59.5999(57.8887) | Bit/dim 3.8679(3.9133) | Xent 1.4546(1.4853) | Loss 10.7313(11.4862) | Error 0.5212(0.5342) Steps 0(0.00) | Grad Norm 5.4614(8.5027) | Total Time 0.00(0.00)\n",
      "Iter 0545 | Time 53.4390(57.7552) | Bit/dim 3.8585(3.9116) | Xent 1.4173(1.4833) | Loss 10.7141(11.4630) | Error 0.5111(0.5335) Steps 0(0.00) | Grad Norm 4.4648(8.3815) | Total Time 0.00(0.00)\n",
      "Iter 0546 | Time 55.9359(57.7007) | Bit/dim 3.8693(3.9104) | Xent 1.4103(1.4811) | Loss 10.6770(11.4394) | Error 0.5069(0.5327) Steps 0(0.00) | Grad Norm 3.3473(8.2305) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 23.4160, Epoch Time 382.5415(370.5586), Bit/dim 3.8574(best: 3.8671), Xent 1.3672, Loss 4.5409, Error 0.4932(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 59.5852(57.7572) | Bit/dim 3.8449(3.9084) | Xent 1.4137(1.4791) | Loss 14.2000(11.5223) | Error 0.5171(0.5322) Steps 0(0.00) | Grad Norm 5.1058(8.1368) | Total Time 0.00(0.00)\n",
      "Iter 0548 | Time 61.0251(57.8552) | Bit/dim 3.8542(3.9068) | Xent 1.4128(1.4771) | Loss 10.6085(11.4948) | Error 0.5131(0.5316) Steps 0(0.00) | Grad Norm 6.0256(8.0734) | Total Time 0.00(0.00)\n",
      "Iter 0549 | Time 58.2288(57.8664) | Bit/dim 3.8642(3.9055) | Xent 1.3878(1.4744) | Loss 10.7699(11.4731) | Error 0.4936(0.5305) Steps 0(0.00) | Grad Norm 5.3437(7.9915) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 58.8294(57.8953) | Bit/dim 3.8458(3.9037) | Xent 1.3694(1.4713) | Loss 10.6296(11.4478) | Error 0.5002(0.5296) Steps 0(0.00) | Grad Norm 3.7465(7.8642) | Total Time 0.00(0.00)\n",
      "Iter 0551 | Time 57.9464(57.8969) | Bit/dim 3.8453(3.9020) | Xent 1.3475(1.4675) | Loss 10.6164(11.4229) | Error 0.4830(0.5282) Steps 0(0.00) | Grad Norm 3.3826(7.7297) | Total Time 0.00(0.00)\n",
      "Iter 0552 | Time 60.0492(57.9614) | Bit/dim 3.8395(3.9001) | Xent 1.3556(1.4642) | Loss 10.6289(11.3990) | Error 0.4874(0.5270) Steps 0(0.00) | Grad Norm 3.2726(7.5960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 23.3958, Epoch Time 394.8210(371.2865), Bit/dim 3.8424(best: 3.8574), Xent 1.3149, Loss 4.4999, Error 0.4790(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 61.7896(58.0763) | Bit/dim 3.8365(3.8982) | Xent 1.3639(1.4612) | Loss 14.2145(11.4835) | Error 0.4910(0.5259) Steps 0(0.00) | Grad Norm 3.5516(7.4747) | Total Time 0.00(0.00)\n",
      "Iter 0554 | Time 57.1896(58.0497) | Bit/dim 3.8384(3.8964) | Xent 1.3456(1.4577) | Loss 10.5089(11.4543) | Error 0.4955(0.5250) Steps 0(0.00) | Grad Norm 3.6773(7.3608) | Total Time 0.00(0.00)\n",
      "Iter 0555 | Time 56.5341(58.0042) | Bit/dim 3.8378(3.8946) | Xent 1.3408(1.4542) | Loss 10.5889(11.4283) | Error 0.4819(0.5237) Steps 0(0.00) | Grad Norm 2.9183(7.2275) | Total Time 0.00(0.00)\n",
      "Iter 0556 | Time 57.9258(58.0019) | Bit/dim 3.8208(3.8924) | Xent 1.3328(1.4506) | Loss 10.5470(11.4019) | Error 0.4819(0.5224) Steps 0(0.00) | Grad Norm 2.5466(7.0871) | Total Time 0.00(0.00)\n",
      "Iter 0557 | Time 57.6241(57.9905) | Bit/dim 3.8301(3.8905) | Xent 1.3348(1.4471) | Loss 10.5609(11.3766) | Error 0.4842(0.5213) Steps 0(0.00) | Grad Norm 2.4820(6.9489) | Total Time 0.00(0.00)\n",
      "Iter 0558 | Time 57.4096(57.9731) | Bit/dim 3.8162(3.8883) | Xent 1.3399(1.4439) | Loss 10.5148(11.3508) | Error 0.4874(0.5203) Steps 0(0.00) | Grad Norm 3.0182(6.8310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 23.3597, Epoch Time 387.4125(371.7703), Bit/dim 3.8185(best: 3.8424), Xent 1.2985, Loss 4.4677, Error 0.4736(best: 0.4790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 58.6242(57.9926) | Bit/dim 3.8111(3.8860) | Xent 1.3420(1.4408) | Loss 14.0707(11.4324) | Error 0.4898(0.5193) Steps 0(0.00) | Grad Norm 4.3675(6.7571) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 57.6098(57.9811) | Bit/dim 3.8220(3.8841) | Xent 1.3572(1.4383) | Loss 10.5283(11.4053) | Error 0.4899(0.5185) Steps 0(0.00) | Grad Norm 11.4414(6.8976) | Total Time 0.00(0.00)\n",
      "Iter 0561 | Time 52.5712(57.8188) | Bit/dim 3.8461(3.8829) | Xent 1.5904(1.4429) | Loss 10.8136(11.3875) | Error 0.5655(0.5199) Steps 0(0.00) | Grad Norm 22.2872(7.3593) | Total Time 0.00(0.00)\n",
      "Iter 0562 | Time 61.1484(57.9187) | Bit/dim 3.8873(3.8831) | Xent 1.8070(1.4538) | Loss 11.1781(11.3812) | Error 0.5857(0.5218) Steps 0(0.00) | Grad Norm 24.5635(7.8754) | Total Time 0.00(0.00)\n",
      "Iter 0563 | Time 56.9921(57.8909) | Bit/dim 4.0572(3.8883) | Xent 1.6942(1.4610) | Loss 11.2953(11.3786) | Error 0.6046(0.5243) Steps 0(0.00) | Grad Norm 19.0372(8.2103) | Total Time 0.00(0.00)\n",
      "Iter 0564 | Time 61.8685(58.0103) | Bit/dim 4.0564(3.8933) | Xent 2.1346(1.4812) | Loss 12.0102(11.3976) | Error 0.6997(0.5296) Steps 0(0.00) | Grad Norm 22.9689(8.6530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 24.2898, Epoch Time 388.5583(372.2739), Bit/dim 4.1283(best: 3.8185), Xent 2.0082, Loss 5.1324, Error 0.6772(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 59.9594(58.0687) | Bit/dim 4.1385(3.9007) | Xent 2.0890(1.4994) | Loss 16.8288(11.5605) | Error 0.6891(0.5344) Steps 0(0.00) | Grad Norm 28.6212(9.2521) | Total Time 0.00(0.00)\n",
      "Iter 0566 | Time 61.3002(58.1657) | Bit/dim 4.2550(3.9113) | Xent 2.5390(1.5306) | Loss 12.8885(11.6004) | Error 0.7650(0.5413) Steps 0(0.00) | Grad Norm 30.9180(9.9021) | Total Time 0.00(0.00)\n",
      "Iter 0567 | Time 60.2217(58.2274) | Bit/dim 4.3944(3.9258) | Xent 1.9812(1.5441) | Loss 12.5176(11.6279) | Error 0.6867(0.5457) Steps 0(0.00) | Grad Norm 17.9164(10.1425) | Total Time 0.00(0.00)\n",
      "Iter 0568 | Time 68.0531(58.5221) | Bit/dim 4.3878(3.9397) | Xent 3.2744(1.5961) | Loss 14.0590(11.7008) | Error 0.8351(0.5543) Steps 0(0.00) | Grad Norm 51.2072(11.3744) | Total Time 0.00(0.00)\n",
      "Iter 0569 | Time 61.2751(58.6047) | Bit/dim 4.4379(3.9546) | Xent 1.7569(1.6009) | Loss 12.5535(11.7264) | Error 0.6331(0.5567) Steps 0(0.00) | Grad Norm 12.6931(11.4140) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 61.9970(58.7065) | Bit/dim 4.5383(3.9721) | Xent 2.5908(1.6306) | Loss 13.5846(11.7821) | Error 0.7792(0.5634) Steps 0(0.00) | Grad Norm 52.6247(12.6503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 24.4243, Epoch Time 412.8779(373.4920), Bit/dim 4.8306(best: 3.8185), Xent 1.9859, Loss 5.8235, Error 0.7216(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 62.7512(58.8278) | Bit/dim 4.8381(3.9981) | Xent 2.0355(1.6427) | Loss 18.2092(11.9750) | Error 0.7330(0.5685) Steps 0(0.00) | Grad Norm 12.5245(12.6465) | Total Time 0.00(0.00)\n",
      "Iter 0572 | Time 62.2796(58.9314) | Bit/dim 4.4696(4.0123) | Xent 2.0289(1.6543) | Loss 12.6102(11.9940) | Error 0.7085(0.5727) Steps 0(0.00) | Grad Norm 13.6214(12.6758) | Total Time 0.00(0.00)\n",
      "Iter 0573 | Time 63.8299(59.0783) | Bit/dim 4.2658(4.0199) | Xent 2.0467(1.6661) | Loss 12.3401(12.0044) | Error 0.7140(0.5769) Steps 0(0.00) | Grad Norm 10.9786(12.6249) | Total Time 0.00(0.00)\n",
      "Iter 0574 | Time 63.3322(59.2059) | Bit/dim 4.2804(4.0277) | Xent 1.8297(1.6710) | Loss 12.1788(12.0096) | Error 0.6532(0.5792) Steps 0(0.00) | Grad Norm 6.7887(12.4498) | Total Time 0.00(0.00)\n",
      "Iter 0575 | Time 62.2551(59.2974) | Bit/dim 4.2150(4.0333) | Xent 1.8867(1.6775) | Loss 12.0008(12.0094) | Error 0.6685(0.5819) Steps 0(0.00) | Grad Norm 8.5894(12.3340) | Total Time 0.00(0.00)\n",
      "Iter 0576 | Time 58.9905(59.2882) | Bit/dim 4.2342(4.0393) | Xent 1.8147(1.6816) | Loss 12.0489(12.0105) | Error 0.6438(0.5837) Steps 0(0.00) | Grad Norm 6.1476(12.1484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 24.9269, Epoch Time 413.9542(374.7059), Bit/dim 4.1856(best: 3.8185), Xent 1.7858, Loss 5.0785, Error 0.6301(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 64.3581(59.4403) | Bit/dim 4.1883(4.0438) | Xent 1.8554(1.6868) | Loss 15.5156(12.1157) | Error 0.6539(0.5858) Steps 0(0.00) | Grad Norm 10.3974(12.0959) | Total Time 0.00(0.00)\n",
      "Iter 0578 | Time 61.5390(59.5033) | Bit/dim 4.2168(4.0490) | Xent 1.9649(1.6951) | Loss 12.1449(12.1166) | Error 0.6741(0.5885) Steps 0(0.00) | Grad Norm 24.4577(12.4667) | Total Time 0.00(0.00)\n",
      "Iter 0579 | Time 60.0959(59.5211) | Bit/dim 4.4454(4.0609) | Xent 2.1164(1.7078) | Loss 12.7449(12.1354) | Error 0.7209(0.5925) Steps 0(0.00) | Grad Norm 25.3892(12.8544) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 63.2088(59.6317) | Bit/dim 4.2531(4.0666) | Xent 1.9661(1.7155) | Loss 12.2082(12.1376) | Error 0.7041(0.5958) Steps 0(0.00) | Grad Norm 15.3238(12.9285) | Total Time 0.00(0.00)\n",
      "Iter 0581 | Time 67.2439(59.8601) | Bit/dim 4.1912(4.0704) | Xent 1.7597(1.7168) | Loss 11.8473(12.1289) | Error 0.6102(0.5962) Steps 0(0.00) | Grad Norm 10.1242(12.8443) | Total Time 0.00(0.00)\n",
      "Iter 0582 | Time 63.6124(59.9726) | Bit/dim 4.1986(4.0742) | Xent 1.7460(1.7177) | Loss 11.8885(12.1217) | Error 0.6115(0.5967) Steps 0(0.00) | Grad Norm 8.2194(12.7056) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 25.2480, Epoch Time 421.0598(376.0965), Bit/dim 4.1658(best: 3.8185), Xent 1.7032, Loss 5.0174, Error 0.5967(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 65.0533(60.1250) | Bit/dim 4.1665(4.0770) | Xent 1.7618(1.7190) | Loss 16.4027(12.2501) | Error 0.6336(0.5978) Steps 0(0.00) | Grad Norm 7.8890(12.5611) | Total Time 0.00(0.00)\n",
      "Iter 0584 | Time 65.0244(60.2720) | Bit/dim 4.1503(4.0792) | Xent 1.7298(1.7194) | Loss 11.8085(12.2369) | Error 0.6038(0.5980) Steps 0(0.00) | Grad Norm 9.4648(12.4682) | Total Time 0.00(0.00)\n",
      "Iter 0585 | Time 70.5241(60.5796) | Bit/dim 4.1297(4.0807) | Xent 1.7680(1.7208) | Loss 11.6981(12.2207) | Error 0.6240(0.5988) Steps 0(0.00) | Grad Norm 15.4645(12.5581) | Total Time 0.00(0.00)\n",
      "Iter 0586 | Time 61.6356(60.6113) | Bit/dim 4.2277(4.0851) | Xent 2.2214(1.7358) | Loss 12.4430(12.2274) | Error 0.7403(0.6030) Steps 0(0.00) | Grad Norm 26.6504(12.9809) | Total Time 0.00(0.00)\n",
      "Iter 0587 | Time 61.0336(60.6239) | Bit/dim 4.1283(4.0864) | Xent 1.7004(1.7348) | Loss 11.7087(12.2118) | Error 0.6082(0.6032) Steps 0(0.00) | Grad Norm 7.8084(12.8257) | Total Time 0.00(0.00)\n",
      "Iter 0588 | Time 65.7735(60.7784) | Bit/dim 4.1675(4.0888) | Xent 1.9727(1.7419) | Loss 11.9823(12.2049) | Error 0.6681(0.6051) Steps 0(0.00) | Grad Norm 21.7119(13.0923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 25.1634, Epoch Time 430.1034(377.7167), Bit/dim 4.1891(best: 3.8185), Xent 2.0781, Loss 5.2281, Error 0.7060(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 62.6304(60.8340) | Bit/dim 4.1968(4.0921) | Xent 2.1451(1.7540) | Loss 15.8084(12.3130) | Error 0.7224(0.6086) Steps 0(0.00) | Grad Norm 28.8891(13.5662) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 67.8100(61.0433) | Bit/dim 4.1596(4.0941) | Xent 1.7645(1.7543) | Loss 11.8709(12.2998) | Error 0.6245(0.6091) Steps 0(0.00) | Grad Norm 8.9433(13.4275) | Total Time 0.00(0.00)\n",
      "Iter 0591 | Time 66.7579(61.2147) | Bit/dim 4.1351(4.0953) | Xent 1.9161(1.7592) | Loss 11.9706(12.2899) | Error 0.6804(0.6113) Steps 0(0.00) | Grad Norm 16.3258(13.5144) | Total Time 0.00(0.00)\n",
      "Iter 0592 | Time 67.4414(61.4015) | Bit/dim 4.0945(4.0953) | Xent 1.7123(1.7578) | Loss 11.6252(12.2700) | Error 0.6109(0.6112) Steps 0(0.00) | Grad Norm 5.8901(13.2857) | Total Time 0.00(0.00)\n",
      "Iter 0593 | Time 67.8710(61.5956) | Bit/dim 4.1227(4.0961) | Xent 1.8233(1.7597) | Loss 11.8841(12.2584) | Error 0.6555(0.6126) Steps 0(0.00) | Grad Norm 10.0330(13.1881) | Total Time 0.00(0.00)\n",
      "Iter 0594 | Time 71.9756(61.9070) | Bit/dim 4.0736(4.0955) | Xent 1.7067(1.7581) | Loss 11.6724(12.2408) | Error 0.6051(0.6123) Steps 0(0.00) | Grad Norm 5.3012(12.9515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 25.3025, Epoch Time 445.5619(379.7521), Bit/dim 4.0752(best: 3.8185), Xent 1.6602, Loss 4.9053, Error 0.5821(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 65.5061(62.0150) | Bit/dim 4.0828(4.0951) | Xent 1.7182(1.7569) | Loss 16.1578(12.3583) | Error 0.6131(0.6124) Steps 0(0.00) | Grad Norm 7.6581(12.7927) | Total Time 0.00(0.00)\n",
      "Iter 0596 | Time 71.7086(62.3058) | Bit/dim 4.0528(4.0938) | Xent 1.6699(1.7543) | Loss 11.5055(12.3327) | Error 0.5949(0.6118) Steps 0(0.00) | Grad Norm 6.3762(12.6002) | Total Time 0.00(0.00)\n",
      "Iter 0597 | Time 69.6613(62.5264) | Bit/dim 4.0301(4.0919) | Xent 1.6092(1.7500) | Loss 11.2649(12.3007) | Error 0.5654(0.6104) Steps 0(0.00) | Grad Norm 4.3228(12.3519) | Total Time 0.00(0.00)\n",
      "Iter 0598 | Time 65.1462(62.6050) | Bit/dim 4.0184(4.0897) | Xent 1.6223(1.7462) | Loss 11.3371(12.2718) | Error 0.5815(0.6096) Steps 0(0.00) | Grad Norm 5.8361(12.1564) | Total Time 0.00(0.00)\n",
      "Iter 0599 | Time 64.3754(62.6581) | Bit/dim 3.9987(4.0870) | Xent 1.6235(1.7425) | Loss 11.2439(12.2409) | Error 0.5791(0.6087) Steps 0(0.00) | Grad Norm 4.6583(11.9315) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 64.6525(62.7180) | Bit/dim 3.9903(4.0841) | Xent 1.6055(1.7384) | Loss 11.2027(12.2098) | Error 0.5655(0.6074) Steps 0(0.00) | Grad Norm 5.1730(11.7287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 24.9498, Epoch Time 441.5131(381.6049), Bit/dim 3.9922(best: 3.8185), Xent 1.5243, Loss 4.7543, Error 0.5396(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 61.9092(62.6937) | Bit/dim 3.9907(4.0813) | Xent 1.5419(1.7325) | Loss 14.8898(12.2902) | Error 0.5479(0.6056) Steps 0(0.00) | Grad Norm 4.4547(11.5105) | Total Time 0.00(0.00)\n",
      "Iter 0602 | Time 68.4685(62.8670) | Bit/dim 3.9933(4.0786) | Xent 1.5630(1.7274) | Loss 11.2328(12.2585) | Error 0.5474(0.6038) Steps 0(0.00) | Grad Norm 7.1894(11.3809) | Total Time 0.00(0.00)\n",
      "Iter 0603 | Time 68.5223(63.0366) | Bit/dim 3.9616(4.0751) | Xent 1.5614(1.7224) | Loss 11.0784(12.2231) | Error 0.5561(0.6024) Steps 0(0.00) | Grad Norm 3.9189(11.1570) | Total Time 0.00(0.00)\n",
      "Iter 0604 | Time 64.8675(63.0915) | Bit/dim 3.9468(4.0713) | Xent 1.5451(1.7171) | Loss 11.0954(12.1892) | Error 0.5506(0.6009) Steps 0(0.00) | Grad Norm 5.7965(10.9962) | Total Time 0.00(0.00)\n",
      "Iter 0605 | Time 66.6050(63.1969) | Bit/dim 3.9617(4.0680) | Xent 1.5511(1.7121) | Loss 11.0788(12.1559) | Error 0.5437(0.5991) Steps 0(0.00) | Grad Norm 8.0243(10.9070) | Total Time 0.00(0.00)\n",
      "Iter 0606 | Time 65.7112(63.2724) | Bit/dim 3.9363(4.0640) | Xent 1.5232(1.7064) | Loss 11.0988(12.1242) | Error 0.5463(0.5976) Steps 0(0.00) | Grad Norm 5.4652(10.7438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 25.1416, Epoch Time 437.0894(383.2694), Bit/dim 3.9255(best: 3.8185), Xent 1.4633, Loss 4.6571, Error 0.5139(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 68.7059(63.4354) | Bit/dim 3.9235(4.0598) | Xent 1.4991(1.7002) | Loss 14.4973(12.1954) | Error 0.5337(0.5956) Steps 0(0.00) | Grad Norm 3.0508(10.5130) | Total Time 0.00(0.00)\n",
      "Iter 0608 | Time 65.5876(63.4999) | Bit/dim 3.9288(4.0559) | Xent 1.5203(1.6948) | Loss 11.1314(12.1635) | Error 0.5354(0.5938) Steps 0(0.00) | Grad Norm 6.5195(10.3932) | Total Time 0.00(0.00)\n",
      "Iter 0609 | Time 69.0038(63.6651) | Bit/dim 3.9129(4.0516) | Xent 1.5091(1.6893) | Loss 10.9755(12.1279) | Error 0.5393(0.5922) Steps 0(0.00) | Grad Norm 7.5206(10.3070) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 65.5546(63.7217) | Bit/dim 3.9167(4.0475) | Xent 1.5028(1.6837) | Loss 11.0439(12.0953) | Error 0.5306(0.5903) Steps 0(0.00) | Grad Norm 6.1643(10.1827) | Total Time 0.00(0.00)\n",
      "Iter 0611 | Time 64.9458(63.7585) | Bit/dim 3.8997(4.0431) | Xent 1.4758(1.6774) | Loss 10.9938(12.0623) | Error 0.5254(0.5884) Steps 0(0.00) | Grad Norm 3.1905(9.9730) | Total Time 0.00(0.00)\n",
      "Iter 0612 | Time 70.2337(63.9527) | Bit/dim 3.8856(4.0384) | Xent 1.4873(1.6717) | Loss 10.9031(12.0275) | Error 0.5274(0.5866) Steps 0(0.00) | Grad Norm 2.3850(9.7453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 24.4943, Epoch Time 444.5370(385.1075), Bit/dim 3.8888(best: 3.8185), Xent 1.4293, Loss 4.6035, Error 0.5073(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 66.0733(64.0163) | Bit/dim 3.8915(4.0340) | Xent 1.4474(1.6650) | Loss 14.3471(12.0971) | Error 0.5139(0.5844) Steps 0(0.00) | Grad Norm 4.9037(9.6001) | Total Time 0.00(0.00)\n",
      "Iter 0614 | Time 64.2246(64.0226) | Bit/dim 3.8814(4.0294) | Xent 1.5217(1.6607) | Loss 10.8542(12.0598) | Error 0.5395(0.5830) Steps 0(0.00) | Grad Norm 10.2554(9.6197) | Total Time 0.00(0.00)\n",
      "Iter 0615 | Time 62.2698(63.9700) | Bit/dim 3.8954(4.0254) | Xent 1.6171(1.6594) | Loss 11.1088(12.0313) | Error 0.5753(0.5828) Steps 0(0.00) | Grad Norm 17.5624(9.8580) | Total Time 0.00(0.00)\n",
      "Iter 0616 | Time 61.8291(63.9058) | Bit/dim 3.9259(4.0224) | Xent 1.8107(1.6639) | Loss 11.2878(12.0090) | Error 0.6229(0.5840) Steps 0(0.00) | Grad Norm 24.4485(10.2957) | Total Time 0.00(0.00)\n",
      "Iter 0617 | Time 65.5708(63.9557) | Bit/dim 3.8862(4.0183) | Xent 1.4924(1.6588) | Loss 10.8516(11.9742) | Error 0.5409(0.5827) Steps 0(0.00) | Grad Norm 6.0587(10.1686) | Total Time 0.00(0.00)\n",
      "Iter 0618 | Time 64.9130(63.9845) | Bit/dim 3.8837(4.0143) | Xent 1.5801(1.6564) | Loss 10.9293(11.9429) | Error 0.5685(0.5823) Steps 0(0.00) | Grad Norm 11.9441(10.2219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 24.6754, Epoch Time 425.6783(386.3246), Bit/dim 3.8878(best: 3.8185), Xent 1.5144, Loss 4.6450, Error 0.5432(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 65.7405(64.0371) | Bit/dim 3.8959(4.0107) | Xent 1.5449(1.6531) | Loss 14.5659(12.0216) | Error 0.5557(0.5815) Steps 0(0.00) | Grad Norm 11.2105(10.2515) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 62.7842(63.9995) | Bit/dim 3.8735(4.0066) | Xent 1.4951(1.6483) | Loss 10.8718(11.9871) | Error 0.5288(0.5799) Steps 0(0.00) | Grad Norm 6.4503(10.1375) | Total Time 0.00(0.00)\n",
      "Iter 0621 | Time 65.6300(64.0485) | Bit/dim 3.8790(4.0028) | Xent 1.5323(1.6449) | Loss 10.9300(11.9554) | Error 0.5473(0.5789) Steps 0(0.00) | Grad Norm 9.5658(10.1204) | Total Time 0.00(0.00)\n",
      "Iter 0622 | Time 60.5092(63.9423) | Bit/dim 3.8727(3.9989) | Xent 1.4893(1.6402) | Loss 10.8279(11.9216) | Error 0.5296(0.5775) Steps 0(0.00) | Grad Norm 4.3287(9.9466) | Total Time 0.00(0.00)\n",
      "Iter 0623 | Time 65.1054(63.9772) | Bit/dim 3.8645(3.9948) | Xent 1.5196(1.6366) | Loss 10.8541(11.8895) | Error 0.5368(0.5762) Steps 0(0.00) | Grad Norm 7.2454(9.8656) | Total Time 0.00(0.00)\n",
      "Iter 0624 | Time 62.6718(63.9380) | Bit/dim 3.8555(3.9907) | Xent 1.4550(1.6311) | Loss 10.7836(11.8564) | Error 0.5190(0.5745) Steps 0(0.00) | Grad Norm 3.0037(9.6597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 23.0574, Epoch Time 421.8631(387.3907), Bit/dim 3.8594(best: 3.8185), Xent 1.4378, Loss 4.5783, Error 0.5113(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 60.0152(63.8203) | Bit/dim 3.8602(3.9867) | Xent 1.4781(1.6265) | Loss 14.2638(11.9286) | Error 0.5337(0.5733) Steps 0(0.00) | Grad Norm 6.4210(9.5626) | Total Time 0.00(0.00)\n",
      "Iter 0626 | Time 63.3975(63.8076) | Bit/dim 3.8460(3.9825) | Xent 1.4604(1.6215) | Loss 10.7209(11.8924) | Error 0.5211(0.5717) Steps 0(0.00) | Grad Norm 2.5486(9.3521) | Total Time 0.00(0.00)\n",
      "Iter 0627 | Time 62.6006(63.7714) | Bit/dim 3.8546(3.9787) | Xent 1.4510(1.6164) | Loss 10.7783(11.8589) | Error 0.5184(0.5701) Steps 0(0.00) | Grad Norm 6.2667(9.2596) | Total Time 0.00(0.00)\n",
      "Iter 0628 | Time 61.1908(63.6940) | Bit/dim 3.8434(3.9746) | Xent 1.4417(1.6112) | Loss 10.6467(11.8226) | Error 0.5160(0.5685) Steps 0(0.00) | Grad Norm 4.2460(9.1092) | Total Time 0.00(0.00)\n",
      "Iter 0629 | Time 61.7168(63.6347) | Bit/dim 3.8547(3.9710) | Xent 1.4694(1.6069) | Loss 10.7622(11.7908) | Error 0.5189(0.5670) Steps 0(0.00) | Grad Norm 7.7793(9.0693) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 60.7534(63.5483) | Bit/dim 3.8542(3.9675) | Xent 1.4473(1.6021) | Loss 10.7030(11.7581) | Error 0.5156(0.5655) Steps 0(0.00) | Grad Norm 6.2270(8.9840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 23.2875, Epoch Time 408.6715(388.0292), Bit/dim 3.8428(best: 3.8185), Xent 1.4009, Loss 4.5433, Error 0.4965(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 60.2176(63.4483) | Bit/dim 3.8478(3.9639) | Xent 1.4249(1.5968) | Loss 14.2660(11.8334) | Error 0.5110(0.5638) Steps 0(0.00) | Grad Norm 6.1205(8.8981) | Total Time 0.00(0.00)\n",
      "Iter 0632 | Time 62.8198(63.4295) | Bit/dim 3.8434(3.9603) | Xent 1.4519(1.5925) | Loss 10.7067(11.7996) | Error 0.5151(0.5624) Steps 0(0.00) | Grad Norm 4.6560(8.7708) | Total Time 0.00(0.00)\n",
      "Iter 0633 | Time 58.2712(63.2747) | Bit/dim 3.8451(3.9569) | Xent 1.4167(1.5872) | Loss 10.7163(11.7671) | Error 0.5060(0.5607) Steps 0(0.00) | Grad Norm 4.0774(8.6300) | Total Time 0.00(0.00)\n",
      "Iter 0634 | Time 61.7485(63.2289) | Bit/dim 3.8394(3.9533) | Xent 1.4330(1.5826) | Loss 10.7350(11.7361) | Error 0.5110(0.5592) Steps 0(0.00) | Grad Norm 5.5408(8.5374) | Total Time 0.00(0.00)\n",
      "Iter 0635 | Time 59.8276(63.1269) | Bit/dim 3.8166(3.9492) | Xent 1.4098(1.5774) | Loss 10.5678(11.7011) | Error 0.5057(0.5576) Steps 0(0.00) | Grad Norm 4.1047(8.4044) | Total Time 0.00(0.00)\n",
      "Iter 0636 | Time 60.9297(63.0610) | Bit/dim 3.8210(3.9454) | Xent 1.4521(1.5736) | Loss 10.6607(11.6698) | Error 0.5206(0.5565) Steps 0(0.00) | Grad Norm 8.2137(8.3987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 23.0607, Epoch Time 403.2243(388.4850), Bit/dim 3.8292(best: 3.8185), Xent 1.4057, Loss 4.5320, Error 0.5031(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 57.2377(62.8863) | Bit/dim 3.8322(3.9420) | Xent 1.4431(1.5697) | Loss 13.9119(11.7371) | Error 0.5206(0.5554) Steps 0(0.00) | Grad Norm 9.2830(8.4252) | Total Time 0.00(0.00)\n",
      "Iter 0638 | Time 60.9084(62.8270) | Bit/dim 3.8198(3.9383) | Xent 1.5059(1.5678) | Loss 10.7324(11.7070) | Error 0.5479(0.5552) Steps 0(0.00) | Grad Norm 11.8100(8.5267) | Total Time 0.00(0.00)\n",
      "Iter 0639 | Time 58.1741(62.6874) | Bit/dim 3.8494(3.9356) | Xent 1.4890(1.5654) | Loss 10.6892(11.6764) | Error 0.5297(0.5544) Steps 0(0.00) | Grad Norm 11.5025(8.6160) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 57.7688(62.5398) | Bit/dim 3.8207(3.9322) | Xent 1.4523(1.5620) | Loss 10.6331(11.6451) | Error 0.5249(0.5535) Steps 0(0.00) | Grad Norm 7.2970(8.5764) | Total Time 0.00(0.00)\n",
      "Iter 0641 | Time 58.3413(62.4139) | Bit/dim 3.8318(3.9292) | Xent 1.4810(1.5596) | Loss 10.8023(11.6198) | Error 0.5315(0.5529) Steps 0(0.00) | Grad Norm 9.3557(8.5998) | Total Time 0.00(0.00)\n",
      "Iter 0642 | Time 58.9192(62.3090) | Bit/dim 3.8276(3.9261) | Xent 1.4213(1.5555) | Loss 10.5908(11.5890) | Error 0.5029(0.5514) Steps 0(0.00) | Grad Norm 5.2667(8.4998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 22.6784, Epoch Time 389.9241(388.5282), Bit/dim 3.8253(best: 3.8185), Xent 1.3790, Loss 4.5148, Error 0.4934(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 60.0502(62.2412) | Bit/dim 3.8122(3.9227) | Xent 1.4083(1.5511) | Loss 13.9249(11.6591) | Error 0.5035(0.5499) Steps 0(0.00) | Grad Norm 5.7720(8.4180) | Total Time 0.00(0.00)\n",
      "Iter 0644 | Time 62.6616(62.2539) | Bit/dim 3.8306(3.9200) | Xent 1.4439(1.5478) | Loss 10.7125(11.6307) | Error 0.5120(0.5488) Steps 0(0.00) | Grad Norm 6.7201(8.3670) | Total Time 0.00(0.00)\n",
      "Iter 0645 | Time 63.3764(62.2875) | Bit/dim 3.8126(3.9167) | Xent 1.4132(1.5438) | Loss 10.5905(11.5994) | Error 0.5017(0.5474) Steps 0(0.00) | Grad Norm 3.1019(8.2091) | Total Time 0.00(0.00)\n",
      "Iter 0646 | Time 59.4085(62.2012) | Bit/dim 3.8218(3.9139) | Xent 1.4242(1.5402) | Loss 10.6259(11.5702) | Error 0.5097(0.5463) Steps 0(0.00) | Grad Norm 5.2672(8.1208) | Total Time 0.00(0.00)\n",
      "Iter 0647 | Time 57.9796(62.0745) | Bit/dim 3.8121(3.9108) | Xent 1.3931(1.5358) | Loss 10.5897(11.5408) | Error 0.5008(0.5449) Steps 0(0.00) | Grad Norm 3.0378(7.9683) | Total Time 0.00(0.00)\n",
      "Iter 0648 | Time 59.0515(61.9838) | Bit/dim 3.8126(3.9079) | Xent 1.4443(1.5331) | Loss 10.5999(11.5126) | Error 0.5188(0.5441) Steps 0(0.00) | Grad Norm 6.4899(7.9240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 23.6182, Epoch Time 401.8406(388.9276), Bit/dim 3.8093(best: 3.8185), Xent 1.3866, Loss 4.5026, Error 0.4939(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 59.7534(61.9169) | Bit/dim 3.8242(3.9054) | Xent 1.4445(1.5304) | Loss 13.9337(11.5852) | Error 0.5172(0.5433) Steps 0(0.00) | Grad Norm 9.2658(7.9642) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 56.8307(61.7643) | Bit/dim 3.8379(3.9034) | Xent 1.5547(1.5311) | Loss 10.8409(11.5629) | Error 0.5463(0.5434) Steps 0(0.00) | Grad Norm 20.7044(8.3464) | Total Time 0.00(0.00)\n",
      "Iter 0651 | Time 59.8970(61.7083) | Bit/dim 3.9206(3.9039) | Xent 1.9496(1.5437) | Loss 11.3925(11.5578) | Error 0.6458(0.5465) Steps 0(0.00) | Grad Norm 28.4441(8.9494) | Total Time 0.00(0.00)\n",
      "Iter 0652 | Time 62.9567(61.7458) | Bit/dim 3.8965(3.9036) | Xent 1.4821(1.5418) | Loss 10.9068(11.5383) | Error 0.5401(0.5463) Steps 0(0.00) | Grad Norm 10.4380(8.9940) | Total Time 0.00(0.00)\n",
      "Iter 0653 | Time 61.2353(61.7304) | Bit/dim 3.8900(3.9032) | Xent 1.5450(1.5419) | Loss 10.9317(11.5201) | Error 0.5571(0.5466) Steps 0(0.00) | Grad Norm 10.2465(9.0316) | Total Time 0.00(0.00)\n",
      "Iter 0654 | Time 58.2582(61.6263) | Bit/dim 3.8926(3.9029) | Xent 1.4975(1.5406) | Loss 10.8682(11.5005) | Error 0.5329(0.5462) Steps 0(0.00) | Grad Norm 8.0530(9.0022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 24.0158, Epoch Time 398.4687(389.2138), Bit/dim 3.8802(best: 3.8093), Xent 1.4458, Loss 4.6031, Error 0.5155(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 66.6032(61.7756) | Bit/dim 3.8678(3.9019) | Xent 1.5159(1.5398) | Loss 14.5490(11.5920) | Error 0.5441(0.5461) Steps 0(0.00) | Grad Norm 6.1501(8.9167) | Total Time 0.00(0.00)\n",
      "Iter 0656 | Time 60.3753(61.7336) | Bit/dim 3.8873(3.9014) | Xent 1.4787(1.5380) | Loss 10.8378(11.5693) | Error 0.5305(0.5457) Steps 0(0.00) | Grad Norm 6.2910(8.8379) | Total Time 0.00(0.00)\n",
      "Iter 0657 | Time 58.2629(61.6295) | Bit/dim 3.8754(3.9007) | Xent 1.4823(1.5363) | Loss 10.8600(11.5481) | Error 0.5319(0.5452) Steps 0(0.00) | Grad Norm 5.1125(8.7262) | Total Time 0.00(0.00)\n",
      "Iter 0658 | Time 60.0310(61.5815) | Bit/dim 3.8721(3.8998) | Xent 1.4605(1.5341) | Loss 10.8129(11.5260) | Error 0.5220(0.5445) Steps 0(0.00) | Grad Norm 5.9029(8.6415) | Total Time 0.00(0.00)\n",
      "Iter 0659 | Time 62.2742(61.6023) | Bit/dim 3.8720(3.8990) | Xent 1.4287(1.5309) | Loss 10.7887(11.5039) | Error 0.5121(0.5436) Steps 0(0.00) | Grad Norm 4.6626(8.5221) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 59.8217(61.5489) | Bit/dim 3.8757(3.8983) | Xent 1.4332(1.5280) | Loss 10.8301(11.4837) | Error 0.5200(0.5429) Steps 0(0.00) | Grad Norm 5.0214(8.4171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 23.8417, Epoch Time 406.9280(389.7452), Bit/dim 3.8594(best: 3.8093), Xent 1.3686, Loss 4.5437, Error 0.4892(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 63.3653(61.6034) | Bit/dim 3.8692(3.8974) | Xent 1.4145(1.5246) | Loss 14.4240(11.5719) | Error 0.5111(0.5419) Steps 0(0.00) | Grad Norm 3.9922(8.2843) | Total Time 0.00(0.00)\n",
      "Iter 0662 | Time 65.9825(61.7347) | Bit/dim 3.8367(3.8956) | Xent 1.4292(1.5217) | Loss 10.6718(11.5449) | Error 0.5108(0.5410) Steps 0(0.00) | Grad Norm 3.7272(8.1476) | Total Time 0.00(0.00)\n",
      "Iter 0663 | Time 58.9326(61.6507) | Bit/dim 3.8416(3.8940) | Xent 1.4272(1.5189) | Loss 10.6675(11.5186) | Error 0.5176(0.5403) Steps 0(0.00) | Grad Norm 3.3395(8.0034) | Total Time 0.00(0.00)\n",
      "Iter 0664 | Time 61.5052(61.6463) | Bit/dim 3.8372(3.8922) | Xent 1.4268(1.5161) | Loss 10.6508(11.4925) | Error 0.5217(0.5397) Steps 0(0.00) | Grad Norm 6.2447(7.9506) | Total Time 0.00(0.00)\n",
      "Iter 0665 | Time 62.3122(61.6663) | Bit/dim 3.8424(3.8908) | Xent 1.4598(1.5144) | Loss 10.7339(11.4698) | Error 0.5174(0.5390) Steps 0(0.00) | Grad Norm 9.1824(7.9876) | Total Time 0.00(0.00)\n",
      "Iter 0666 | Time 64.5274(61.7521) | Bit/dim 3.8514(3.8896) | Xent 1.4851(1.5135) | Loss 10.8647(11.4516) | Error 0.5274(0.5387) Steps 0(0.00) | Grad Norm 11.2318(8.0849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 23.9027, Epoch Time 416.5289(390.5487), Bit/dim 3.8373(best: 3.8093), Xent 1.4331, Loss 4.5539, Error 0.5155(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 62.9142(61.7870) | Bit/dim 3.8365(3.8880) | Xent 1.4724(1.5123) | Loss 14.3770(11.5394) | Error 0.5220(0.5382) Steps 0(0.00) | Grad Norm 9.7211(8.1340) | Total Time 0.00(0.00)\n",
      "Iter 0668 | Time 62.2625(61.8012) | Bit/dim 3.8203(3.8859) | Xent 1.4241(1.5097) | Loss 10.7565(11.5159) | Error 0.5117(0.5374) Steps 0(0.00) | Grad Norm 4.3991(8.0219) | Total Time 0.00(0.00)\n",
      "Iter 0669 | Time 63.0050(61.8374) | Bit/dim 3.8156(3.8838) | Xent 1.4049(1.5065) | Loss 10.6163(11.4889) | Error 0.5062(0.5365) Steps 0(0.00) | Grad Norm 4.9513(7.9298) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 62.5018(61.8573) | Bit/dim 3.8254(3.8821) | Xent 1.4087(1.5036) | Loss 10.6286(11.4631) | Error 0.5017(0.5354) Steps 0(0.00) | Grad Norm 6.8181(7.8965) | Total Time 0.00(0.00)\n",
      "Iter 0671 | Time 64.2027(61.9277) | Bit/dim 3.8299(3.8805) | Xent 1.4278(1.5013) | Loss 10.7457(11.4416) | Error 0.5139(0.5348) Steps 0(0.00) | Grad Norm 5.6954(7.8304) | Total Time 0.00(0.00)\n",
      "Iter 0672 | Time 61.8504(61.9253) | Bit/dim 3.8114(3.8784) | Xent 1.4082(1.4985) | Loss 10.6285(11.4172) | Error 0.5070(0.5339) Steps 0(0.00) | Grad Norm 3.9078(7.7127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 23.7463, Epoch Time 416.7555(391.3349), Bit/dim 3.8104(best: 3.8093), Xent 1.3446, Loss 4.4827, Error 0.4806(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 62.3942(61.9394) | Bit/dim 3.8208(3.8767) | Xent 1.3962(1.4955) | Loss 14.2552(11.5023) | Error 0.4951(0.5328) Steps 0(0.00) | Grad Norm 3.7460(7.5937) | Total Time 0.00(0.00)\n",
      "Iter 0674 | Time 61.0944(61.9141) | Bit/dim 3.8019(3.8745) | Xent 1.3753(1.4918) | Loss 10.6283(11.4761) | Error 0.4941(0.5316) Steps 0(0.00) | Grad Norm 3.9750(7.4852) | Total Time 0.00(0.00)\n",
      "Iter 0675 | Time 61.8095(61.9109) | Bit/dim 3.7972(3.8722) | Xent 1.3928(1.4889) | Loss 10.6456(11.4512) | Error 0.5050(0.5308) Steps 0(0.00) | Grad Norm 3.9676(7.3797) | Total Time 0.00(0.00)\n",
      "Iter 0676 | Time 62.5922(61.9314) | Bit/dim 3.8132(3.8704) | Xent 1.3679(1.4852) | Loss 10.4988(11.4226) | Error 0.4962(0.5298) Steps 0(0.00) | Grad Norm 3.3529(7.2589) | Total Time 0.00(0.00)\n",
      "Iter 0677 | Time 61.2389(61.9106) | Bit/dim 3.7922(3.8680) | Xent 1.3805(1.4821) | Loss 10.4107(11.3923) | Error 0.4960(0.5288) Steps 0(0.00) | Grad Norm 4.0739(7.1633) | Total Time 0.00(0.00)\n",
      "Iter 0678 | Time 59.6735(61.8435) | Bit/dim 3.7888(3.8657) | Xent 1.3794(1.4790) | Loss 10.4951(11.3653) | Error 0.4908(0.5276) Steps 0(0.00) | Grad Norm 5.5337(7.1144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 23.3711, Epoch Time 408.3194(391.8445), Bit/dim 3.7952(best: 3.8093), Xent 1.3615, Loss 4.4760, Error 0.4860(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 59.0941(61.7610) | Bit/dim 3.8016(3.8637) | Xent 1.4174(1.4772) | Loss 13.9740(11.4436) | Error 0.5064(0.5270) Steps 0(0.00) | Grad Norm 7.9378(7.1391) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 57.6058(61.6363) | Bit/dim 3.7890(3.8615) | Xent 1.4202(1.4755) | Loss 10.5798(11.4177) | Error 0.5068(0.5264) Steps 0(0.00) | Grad Norm 10.2289(7.2318) | Total Time 0.00(0.00)\n",
      "Iter 0681 | Time 58.5381(61.5434) | Bit/dim 3.8119(3.8600) | Xent 1.4551(1.4749) | Loss 10.5647(11.3921) | Error 0.5276(0.5264) Steps 0(0.00) | Grad Norm 12.2663(7.3828) | Total Time 0.00(0.00)\n",
      "Iter 0682 | Time 61.6519(61.5466) | Bit/dim 3.7958(3.8581) | Xent 1.4291(1.4735) | Loss 10.6007(11.3684) | Error 0.5145(0.5261) Steps 0(0.00) | Grad Norm 11.5215(7.5070) | Total Time 0.00(0.00)\n",
      "Iter 0683 | Time 61.4826(61.5447) | Bit/dim 3.7806(3.8558) | Xent 1.3795(1.4707) | Loss 10.5102(11.3426) | Error 0.5019(0.5253) Steps 0(0.00) | Grad Norm 4.7461(7.4242) | Total Time 0.00(0.00)\n",
      "Iter 0684 | Time 63.3499(61.5989) | Bit/dim 3.7933(3.8539) | Xent 1.4338(1.4696) | Loss 10.5534(11.3189) | Error 0.5224(0.5253) Steps 0(0.00) | Grad Norm 6.1454(7.3858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 23.5732, Epoch Time 401.0963(392.1220), Bit/dim 3.7930(best: 3.7952), Xent 1.3687, Loss 4.4774, Error 0.4870(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 61.1874(61.5865) | Bit/dim 3.7782(3.8516) | Xent 1.4188(1.4680) | Loss 14.1295(11.4032) | Error 0.4995(0.5245) Steps 0(0.00) | Grad Norm 6.8723(7.3704) | Total Time 0.00(0.00)\n",
      "Iter 0686 | Time 61.8678(61.5950) | Bit/dim 3.7870(3.8497) | Xent 1.3852(1.4656) | Loss 10.4838(11.3757) | Error 0.4931(0.5235) Steps 0(0.00) | Grad Norm 6.1496(7.3338) | Total Time 0.00(0.00)\n",
      "Iter 0687 | Time 60.7864(61.5707) | Bit/dim 3.7980(3.8481) | Xent 1.4058(1.4638) | Loss 10.5793(11.3518) | Error 0.5052(0.5230) Steps 0(0.00) | Grad Norm 7.8016(7.3478) | Total Time 0.00(0.00)\n",
      "Iter 0688 | Time 62.4569(61.5973) | Bit/dim 3.8038(3.8468) | Xent 1.3725(1.4610) | Loss 10.4414(11.3245) | Error 0.4861(0.5219) Steps 0(0.00) | Grad Norm 4.9849(7.2769) | Total Time 0.00(0.00)\n",
      "Iter 0689 | Time 60.0756(61.5517) | Bit/dim 3.7752(3.8447) | Xent 1.3812(1.4586) | Loss 10.4419(11.2980) | Error 0.4914(0.5210) Steps 0(0.00) | Grad Norm 4.8162(7.2031) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 60.6982(61.5260) | Bit/dim 3.7874(3.8429) | Xent 1.3703(1.4560) | Loss 10.4875(11.2737) | Error 0.4981(0.5203) Steps 0(0.00) | Grad Norm 4.5342(7.1230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 23.4745, Epoch Time 406.6003(392.5564), Bit/dim 3.7869(best: 3.7930), Xent 1.3106, Loss 4.4422, Error 0.4692(best: 0.4736)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 60.5413(61.4965) | Bit/dim 3.7850(3.8412) | Xent 1.3339(1.4523) | Loss 13.9138(11.3529) | Error 0.4785(0.5190) Steps 0(0.00) | Grad Norm 4.3611(7.0402) | Total Time 0.00(0.00)\n",
      "Iter 0692 | Time 57.9933(61.3914) | Bit/dim 3.7806(3.8394) | Xent 1.3679(1.4498) | Loss 10.4242(11.3250) | Error 0.4865(0.5181) Steps 0(0.00) | Grad Norm 6.2315(7.0159) | Total Time 0.00(0.00)\n",
      "Iter 0693 | Time 62.6760(61.4300) | Bit/dim 3.7884(3.8378) | Xent 1.3889(1.4480) | Loss 10.4707(11.2994) | Error 0.4990(0.5175) Steps 0(0.00) | Grad Norm 7.4366(7.0285) | Total Time 0.00(0.00)\n",
      "Iter 0694 | Time 57.8225(61.3217) | Bit/dim 3.7853(3.8363) | Xent 1.4465(1.4479) | Loss 10.5820(11.2779) | Error 0.5126(0.5173) Steps 0(0.00) | Grad Norm 11.6156(7.1662) | Total Time 0.00(0.00)\n",
      "Iter 0695 | Time 58.8034(61.2462) | Bit/dim 3.8273(3.8360) | Xent 1.5242(1.4502) | Loss 10.7765(11.2628) | Error 0.5349(0.5179) Steps 0(0.00) | Grad Norm 16.4262(7.4440) | Total Time 0.00(0.00)\n",
      "Iter 0696 | Time 64.4135(61.3412) | Bit/dim 3.8130(3.8353) | Xent 1.6012(1.4547) | Loss 10.8323(11.2499) | Error 0.5510(0.5189) Steps 0(0.00) | Grad Norm 15.7243(7.6924) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 23.8904, Epoch Time 402.0825(392.8422), Bit/dim 3.8084(best: 3.7869), Xent 1.3878, Loss 4.5023, Error 0.5004(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 60.4574(61.3147) | Bit/dim 3.8048(3.8344) | Xent 1.4298(1.4540) | Loss 14.1441(11.3367) | Error 0.5212(0.5189) Steps 0(0.00) | Grad Norm 5.5969(7.6295) | Total Time 0.00(0.00)\n",
      "Iter 0698 | Time 59.0493(61.2467) | Bit/dim 3.8137(3.8338) | Xent 1.4659(1.4543) | Loss 10.5868(11.3142) | Error 0.5275(0.5192) Steps 0(0.00) | Grad Norm 9.3346(7.6807) | Total Time 0.00(0.00)\n",
      "Iter 0699 | Time 61.2455(61.2467) | Bit/dim 3.8068(3.8330) | Xent 1.4188(1.4533) | Loss 10.6012(11.2928) | Error 0.5159(0.5191) Steps 0(0.00) | Grad Norm 5.2455(7.6076) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 59.5128(61.1947) | Bit/dim 3.8191(3.8326) | Xent 1.4200(1.4523) | Loss 10.6865(11.2747) | Error 0.5106(0.5188) Steps 0(0.00) | Grad Norm 5.0999(7.5324) | Total Time 0.00(0.00)\n",
      "Iter 0701 | Time 59.4379(61.1420) | Bit/dim 3.7956(3.8314) | Xent 1.4199(1.4513) | Loss 10.6002(11.2544) | Error 0.5130(0.5187) Steps 0(0.00) | Grad Norm 5.7461(7.4788) | Total Time 0.00(0.00)\n",
      "Iter 0702 | Time 60.3406(61.1179) | Bit/dim 3.8096(3.8308) | Xent 1.4067(1.4500) | Loss 10.6442(11.2361) | Error 0.5069(0.5183) Steps 0(0.00) | Grad Norm 4.7441(7.3967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 23.7380, Epoch Time 399.9945(393.0567), Bit/dim 3.8047(best: 3.7869), Xent 1.3332, Loss 4.4713, Error 0.4832(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 62.6933(61.1652) | Bit/dim 3.7962(3.8297) | Xent 1.3661(1.4475) | Loss 13.9671(11.3180) | Error 0.4935(0.5176) Steps 0(0.00) | Grad Norm 4.5498(7.3113) | Total Time 0.00(0.00)\n",
      "Iter 0704 | Time 61.3988(61.1722) | Bit/dim 3.7986(3.8288) | Xent 1.3914(1.4458) | Loss 10.5944(11.2963) | Error 0.4980(0.5170) Steps 0(0.00) | Grad Norm 5.0220(7.2427) | Total Time 0.00(0.00)\n",
      "Iter 0705 | Time 62.2579(61.2048) | Bit/dim 3.7872(3.8276) | Xent 1.3664(1.4434) | Loss 10.4803(11.2719) | Error 0.4988(0.5164) Steps 0(0.00) | Grad Norm 4.2076(7.1516) | Total Time 0.00(0.00)\n",
      "Iter 0706 | Time 58.7502(61.1311) | Bit/dim 3.8053(3.8269) | Xent 1.3667(1.4411) | Loss 10.4619(11.2476) | Error 0.4871(0.5155) Steps 0(0.00) | Grad Norm 3.9007(7.0541) | Total Time 0.00(0.00)\n",
      "Iter 0707 | Time 62.9261(61.1850) | Bit/dim 3.7928(3.8259) | Xent 1.3747(1.4391) | Loss 10.4461(11.2235) | Error 0.4951(0.5149) Steps 0(0.00) | Grad Norm 4.7842(6.9860) | Total Time 0.00(0.00)\n",
      "Iter 0708 | Time 61.6238(61.1981) | Bit/dim 3.7921(3.8249) | Xent 1.3728(1.4371) | Loss 10.5334(11.2028) | Error 0.4944(0.5143) Steps 0(0.00) | Grad Norm 5.3767(6.9377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 23.0878, Epoch Time 408.5218(393.5207), Bit/dim 3.7942(best: 3.7869), Xent 1.3280, Loss 4.4582, Error 0.4777(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 59.7755(61.1555) | Bit/dim 3.8010(3.8241) | Xent 1.3674(1.4350) | Loss 13.9178(11.2843) | Error 0.4879(0.5135) Steps 0(0.00) | Grad Norm 6.6739(6.9298) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 55.6669(60.9908) | Bit/dim 3.7943(3.8232) | Xent 1.3912(1.4337) | Loss 10.4385(11.2589) | Error 0.4950(0.5130) Steps 0(0.00) | Grad Norm 6.8969(6.9288) | Total Time 0.00(0.00)\n",
      "Iter 0711 | Time 61.5212(61.0067) | Bit/dim 3.7773(3.8219) | Xent 1.3739(1.4319) | Loss 10.4943(11.2359) | Error 0.4932(0.5124) Steps 0(0.00) | Grad Norm 5.3576(6.8817) | Total Time 0.00(0.00)\n",
      "Iter 0712 | Time 60.4103(60.9888) | Bit/dim 3.7675(3.8202) | Xent 1.3394(1.4291) | Loss 10.3570(11.2096) | Error 0.4844(0.5115) Steps 0(0.00) | Grad Norm 2.8233(6.7599) | Total Time 0.00(0.00)\n",
      "Iter 0713 | Time 57.6658(60.8891) | Bit/dim 3.7748(3.8189) | Xent 1.3683(1.4273) | Loss 10.4865(11.1879) | Error 0.4915(0.5109) Steps 0(0.00) | Grad Norm 5.1014(6.7102) | Total Time 0.00(0.00)\n",
      "Iter 0714 | Time 58.5224(60.8181) | Bit/dim 3.7846(3.8178) | Xent 1.3778(1.4258) | Loss 10.5342(11.1683) | Error 0.4986(0.5106) Steps 0(0.00) | Grad Norm 8.0286(6.7497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 22.9200, Epoch Time 392.4851(393.4896), Bit/dim 3.7885(best: 3.7869), Xent 1.3593, Loss 4.4682, Error 0.4864(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 59.8849(60.7901) | Bit/dim 3.7852(3.8169) | Xent 1.4087(1.4253) | Loss 13.8863(11.2498) | Error 0.5095(0.5105) Steps 0(0.00) | Grad Norm 11.4410(6.8905) | Total Time 0.00(0.00)\n",
      "Iter 0716 | Time 60.9766(60.7957) | Bit/dim 3.7953(3.8162) | Xent 1.4458(1.4259) | Loss 10.6798(11.2327) | Error 0.5135(0.5106) Steps 0(0.00) | Grad Norm 13.2415(7.0810) | Total Time 0.00(0.00)\n",
      "Iter 0717 | Time 60.9371(60.8000) | Bit/dim 3.7825(3.8152) | Xent 1.4644(1.4271) | Loss 10.6254(11.2145) | Error 0.5269(0.5111) Steps 0(0.00) | Grad Norm 11.9321(7.2265) | Total Time 0.00(0.00)\n",
      "Iter 0718 | Time 55.5463(60.6424) | Bit/dim 3.8485(3.8162) | Xent 1.3996(1.4263) | Loss 10.7086(11.1993) | Error 0.5025(0.5109) Steps 0(0.00) | Grad Norm 11.2555(7.3474) | Total Time 0.00(0.00)\n",
      "Iter 0719 | Time 61.7548(60.6757) | Bit/dim 3.7828(3.8152) | Xent 1.4184(1.4260) | Loss 10.6045(11.1815) | Error 0.5040(0.5106) Steps 0(0.00) | Grad Norm 7.1865(7.3426) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 61.0130(60.6859) | Bit/dim 3.8096(3.8150) | Xent 1.4124(1.4256) | Loss 10.4955(11.1609) | Error 0.5079(0.5106) Steps 0(0.00) | Grad Norm 8.0414(7.3635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 23.6447, Epoch Time 399.8589(393.6807), Bit/dim 3.8155(best: 3.7869), Xent 1.3804, Loss 4.5057, Error 0.4938(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 59.5963(60.6532) | Bit/dim 3.8296(3.8155) | Xent 1.4050(1.4250) | Loss 13.9831(11.2456) | Error 0.4995(0.5102) Steps 0(0.00) | Grad Norm 9.4611(7.4265) | Total Time 0.00(0.00)\n",
      "Iter 0722 | Time 59.1812(60.6090) | Bit/dim 3.8012(3.8150) | Xent 1.4106(1.4246) | Loss 10.5956(11.2261) | Error 0.5135(0.5103) Steps 0(0.00) | Grad Norm 7.0314(7.4146) | Total Time 0.00(0.00)\n",
      "Iter 0723 | Time 62.8479(60.6762) | Bit/dim 3.7863(3.8142) | Xent 1.3651(1.4228) | Loss 10.5554(11.2059) | Error 0.4932(0.5098) Steps 0(0.00) | Grad Norm 5.1638(7.3471) | Total Time 0.00(0.00)\n",
      "Iter 0724 | Time 60.7144(60.6773) | Bit/dim 3.7803(3.8132) | Xent 1.3815(1.4215) | Loss 10.5053(11.1849) | Error 0.4972(0.5094) Steps 0(0.00) | Grad Norm 4.7233(7.2684) | Total Time 0.00(0.00)\n",
      "Iter 0725 | Time 60.9536(60.6856) | Bit/dim 3.7935(3.8126) | Xent 1.3502(1.4194) | Loss 10.4348(11.1624) | Error 0.4812(0.5086) Steps 0(0.00) | Grad Norm 5.1202(7.2039) | Total Time 0.00(0.00)\n",
      "Iter 0726 | Time 63.6213(60.7737) | Bit/dim 3.7737(3.8114) | Xent 1.3700(1.4179) | Loss 10.4661(11.1415) | Error 0.4939(0.5082) Steps 0(0.00) | Grad Norm 5.5571(7.1545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 23.9102, Epoch Time 406.4927(394.0650), Bit/dim 3.7839(best: 3.7869), Xent 1.3023, Loss 4.4350, Error 0.4686(best: 0.4692)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 59.6233(60.7392) | Bit/dim 3.7832(3.8106) | Xent 1.3622(1.4162) | Loss 13.9804(11.2267) | Error 0.4934(0.5077) Steps 0(0.00) | Grad Norm 4.9693(7.0890) | Total Time 0.00(0.00)\n",
      "Iter 0728 | Time 57.9361(60.6551) | Bit/dim 3.7760(3.8095) | Xent 1.3421(1.4140) | Loss 10.4738(11.2041) | Error 0.4848(0.5070) Steps 0(0.00) | Grad Norm 5.6577(7.0460) | Total Time 0.00(0.00)\n",
      "Iter 0729 | Time 60.9567(60.6641) | Bit/dim 3.7708(3.8084) | Xent 1.3310(1.4115) | Loss 10.4047(11.1801) | Error 0.4791(0.5062) Steps 0(0.00) | Grad Norm 5.3334(6.9946) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 61.8250(60.6989) | Bit/dim 3.7762(3.8074) | Xent 1.3338(1.4092) | Loss 10.4513(11.1583) | Error 0.4769(0.5053) Steps 0(0.00) | Grad Norm 2.8432(6.8701) | Total Time 0.00(0.00)\n",
      "Iter 0731 | Time 60.0882(60.6806) | Bit/dim 3.7594(3.8060) | Xent 1.3300(1.4068) | Loss 10.3639(11.1344) | Error 0.4774(0.5045) Steps 0(0.00) | Grad Norm 3.2466(6.7614) | Total Time 0.00(0.00)\n",
      "Iter 0732 | Time 57.1287(60.5741) | Bit/dim 3.7779(3.8051) | Xent 1.3373(1.4047) | Loss 10.3509(11.1109) | Error 0.4759(0.5036) Steps 0(0.00) | Grad Norm 4.2384(6.6857) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 23.6510, Epoch Time 397.5725(394.1703), Bit/dim 3.7610(best: 3.7839), Xent 1.3111, Loss 4.4165, Error 0.4734(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 61.2668(60.5949) | Bit/dim 3.7646(3.8039) | Xent 1.3555(1.4033) | Loss 13.8609(11.1934) | Error 0.4806(0.5029) Steps 0(0.00) | Grad Norm 6.3924(6.6769) | Total Time 0.00(0.00)\n",
      "Iter 0734 | Time 60.2808(60.5854) | Bit/dim 3.7599(3.8026) | Xent 1.4230(1.4038) | Loss 10.4741(11.1718) | Error 0.5059(0.5030) Steps 0(0.00) | Grad Norm 12.5974(6.8545) | Total Time 0.00(0.00)\n",
      "Iter 0735 | Time 62.5105(60.6432) | Bit/dim 3.8054(3.8027) | Xent 1.6032(1.4098) | Loss 10.8172(11.1612) | Error 0.5585(0.5047) Steps 0(0.00) | Grad Norm 18.4941(7.2037) | Total Time 0.00(0.00)\n",
      "Iter 0736 | Time 57.9854(60.5634) | Bit/dim 3.8008(3.8026) | Xent 1.4470(1.4109) | Loss 10.6865(11.1470) | Error 0.5141(0.5050) Steps 0(0.00) | Grad Norm 9.1237(7.2613) | Total Time 0.00(0.00)\n",
      "Iter 0737 | Time 60.1056(60.5497) | Bit/dim 3.8108(3.8029) | Xent 1.4108(1.4109) | Loss 10.5311(11.1285) | Error 0.5034(0.5049) Steps 0(0.00) | Grad Norm 8.2831(7.2920) | Total Time 0.00(0.00)\n",
      "Iter 0738 | Time 59.6630(60.5231) | Bit/dim 3.7943(3.8026) | Xent 1.4684(1.4127) | Loss 10.6729(11.1148) | Error 0.5259(0.5055) Steps 0(0.00) | Grad Norm 8.3493(7.3237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 24.6062, Epoch Time 402.4176(394.4177), Bit/dim 3.7943(best: 3.7610), Xent 1.3764, Loss 4.4825, Error 0.4976(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 61.8320(60.5624) | Bit/dim 3.7901(3.8022) | Xent 1.4469(1.4137) | Loss 14.1165(11.2049) | Error 0.5210(0.5060) Steps 0(0.00) | Grad Norm 8.0469(7.3454) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 54.4142(60.3779) | Bit/dim 3.8096(3.8024) | Xent 1.4240(1.4140) | Loss 10.5165(11.1842) | Error 0.5094(0.5061) Steps 0(0.00) | Grad Norm 9.0241(7.3957) | Total Time 0.00(0.00)\n",
      "Iter 0741 | Time 54.1938(60.1924) | Bit/dim 3.8042(3.8025) | Xent 1.4378(1.4147) | Loss 10.6243(11.1674) | Error 0.5162(0.5064) Steps 0(0.00) | Grad Norm 11.2520(7.5114) | Total Time 0.00(0.00)\n",
      "Iter 0742 | Time 61.1119(60.2200) | Bit/dim 3.8371(3.8035) | Xent 1.5255(1.4180) | Loss 10.8412(11.1576) | Error 0.5306(0.5071) Steps 0(0.00) | Grad Norm 14.9708(7.7352) | Total Time 0.00(0.00)\n",
      "Iter 0743 | Time 57.8699(60.1495) | Bit/dim 3.8500(3.8049) | Xent 1.5194(1.4211) | Loss 10.7322(11.1449) | Error 0.5306(0.5078) Steps 0(0.00) | Grad Norm 12.1915(7.8689) | Total Time 0.00(0.00)\n",
      "Iter 0744 | Time 55.2758(60.0033) | Bit/dim 3.8557(3.8064) | Xent 1.4430(1.4217) | Loss 10.6236(11.1292) | Error 0.5115(0.5079) Steps 0(0.00) | Grad Norm 8.2192(7.8794) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 22.4763, Epoch Time 383.1269(394.0790), Bit/dim 3.8514(best: 3.7610), Xent 1.3685, Loss 4.5357, Error 0.4835(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 60.3394(60.0134) | Bit/dim 3.8535(3.8079) | Xent 1.4158(1.4216) | Loss 14.1004(11.2184) | Error 0.5065(0.5079) Steps 0(0.00) | Grad Norm 6.4335(7.8360) | Total Time 0.00(0.00)\n",
      "Iter 0746 | Time 62.3058(60.0821) | Bit/dim 3.8082(3.8079) | Xent 1.4037(1.4210) | Loss 10.5812(11.1993) | Error 0.5024(0.5077) Steps 0(0.00) | Grad Norm 4.3260(7.7307) | Total Time 0.00(0.00)\n",
      "Iter 0747 | Time 61.0463(60.1111) | Bit/dim 3.8337(3.8086) | Xent 1.3967(1.4203) | Loss 10.6706(11.1834) | Error 0.5026(0.5076) Steps 0(0.00) | Grad Norm 7.4153(7.7213) | Total Time 0.00(0.00)\n",
      "Iter 0748 | Time 64.4670(60.2417) | Bit/dim 3.8137(3.8088) | Xent 1.4273(1.4205) | Loss 10.7041(11.1690) | Error 0.5191(0.5079) Steps 0(0.00) | Grad Norm 7.2066(7.7058) | Total Time 0.00(0.00)\n",
      "Iter 0749 | Time 58.5451(60.1908) | Bit/dim 3.8176(3.8091) | Xent 1.4147(1.4203) | Loss 10.6322(11.1529) | Error 0.5110(0.5080) Steps 0(0.00) | Grad Norm 6.1100(7.6579) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 63.5556(60.2918) | Bit/dim 3.8113(3.8091) | Xent 1.3739(1.4189) | Loss 10.4617(11.1322) | Error 0.4880(0.5074) Steps 0(0.00) | Grad Norm 3.9027(7.5453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 23.7950, Epoch Time 409.8004(394.5506), Bit/dim 3.8075(best: 3.7610), Xent 1.3367, Loss 4.4758, Error 0.4830(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 62.2827(60.3515) | Bit/dim 3.8054(3.8090) | Xent 1.4006(1.4184) | Loss 13.9381(11.2163) | Error 0.5045(0.5073) Steps 0(0.00) | Grad Norm 4.7563(7.4616) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 61.8530(60.3966) | Bit/dim 3.8081(3.8090) | Xent 1.3947(1.4177) | Loss 10.6619(11.1997) | Error 0.4996(0.5071) Steps 0(0.00) | Grad Norm 6.1357(7.4218) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 59.8939(60.3815) | Bit/dim 3.7936(3.8085) | Xent 1.3723(1.4163) | Loss 10.5430(11.1800) | Error 0.4918(0.5066) Steps 0(0.00) | Grad Norm 6.6209(7.3978) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 58.7803(60.3334) | Bit/dim 3.7945(3.8081) | Xent 1.3456(1.4142) | Loss 10.5043(11.1597) | Error 0.4800(0.5058) Steps 0(0.00) | Grad Norm 6.5569(7.3726) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 60.1227(60.3271) | Bit/dim 3.7966(3.8078) | Xent 1.3629(1.4127) | Loss 10.4471(11.1384) | Error 0.4915(0.5054) Steps 0(0.00) | Grad Norm 4.6636(7.2913) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 56.9840(60.2268) | Bit/dim 3.7796(3.8069) | Xent 1.3615(1.4111) | Loss 10.4184(11.1168) | Error 0.4950(0.5051) Steps 0(0.00) | Grad Norm 3.0808(7.1650) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 23.6256, Epoch Time 400.3135(394.7235), Bit/dim 3.7808(best: 3.7610), Xent 1.3108, Loss 4.4362, Error 0.4702(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 56.3293(60.1099) | Bit/dim 3.7736(3.8059) | Xent 1.3321(1.4087) | Loss 13.9456(11.2016) | Error 0.4791(0.5043) Steps 0(0.00) | Grad Norm 3.4718(7.0542) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 58.0713(60.0487) | Bit/dim 3.7749(3.8050) | Xent 1.3630(1.4074) | Loss 10.4495(11.1791) | Error 0.4856(0.5038) Steps 0(0.00) | Grad Norm 3.8948(6.9594) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 56.8648(59.9532) | Bit/dim 3.7825(3.8043) | Xent 1.3496(1.4056) | Loss 10.4544(11.1573) | Error 0.4890(0.5033) Steps 0(0.00) | Grad Norm 4.3378(6.8808) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 61.1169(59.9881) | Bit/dim 3.7701(3.8033) | Xent 1.3287(1.4033) | Loss 10.5230(11.1383) | Error 0.4811(0.5027) Steps 0(0.00) | Grad Norm 3.8624(6.7902) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 57.2095(59.9048) | Bit/dim 3.7733(3.8024) | Xent 1.3419(1.4015) | Loss 10.4130(11.1165) | Error 0.4875(0.5022) Steps 0(0.00) | Grad Norm 4.5295(6.7224) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 61.1377(59.9418) | Bit/dim 3.7639(3.8012) | Xent 1.3366(1.3995) | Loss 10.4457(11.0964) | Error 0.4850(0.5017) Steps 0(0.00) | Grad Norm 4.6518(6.6603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 23.3540, Epoch Time 389.7555(394.5745), Bit/dim 3.7670(best: 3.7610), Xent 1.2904, Loss 4.4122, Error 0.4655(best: 0.4686)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 61.0012(59.9735) | Bit/dim 3.7558(3.7999) | Xent 1.3356(1.3976) | Loss 13.9631(11.1824) | Error 0.4839(0.5011) Steps 0(0.00) | Grad Norm 6.2402(6.6477) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 61.5257(60.0201) | Bit/dim 3.7572(3.7986) | Xent 1.3707(1.3968) | Loss 10.4247(11.1597) | Error 0.4872(0.5007) Steps 0(0.00) | Grad Norm 9.9057(6.7454) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 62.3587(60.0903) | Bit/dim 3.7843(3.7982) | Xent 1.4898(1.3996) | Loss 10.6556(11.1446) | Error 0.5220(0.5014) Steps 0(0.00) | Grad Norm 14.8590(6.9888) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 58.9817(60.0570) | Bit/dim 3.7735(3.7974) | Xent 1.5207(1.4032) | Loss 10.6091(11.1285) | Error 0.5357(0.5024) Steps 0(0.00) | Grad Norm 14.2154(7.2056) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 58.7165(60.0168) | Bit/dim 3.7683(3.7965) | Xent 1.4048(1.4033) | Loss 10.4908(11.1094) | Error 0.5077(0.5026) Steps 0(0.00) | Grad Norm 6.2524(7.1770) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 59.0196(59.9869) | Bit/dim 3.7609(3.7955) | Xent 1.3666(1.4022) | Loss 10.4782(11.0904) | Error 0.4916(0.5022) Steps 0(0.00) | Grad Norm 5.2119(7.1181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 22.6859, Epoch Time 399.8861(394.7338), Bit/dim 3.7638(best: 3.7610), Xent 1.3500, Loss 4.4388, Error 0.4867(best: 0.4655)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 60.0860(59.9899) | Bit/dim 3.7572(3.7943) | Xent 1.3887(1.4018) | Loss 13.8085(11.1720) | Error 0.4994(0.5021) Steps 0(0.00) | Grad Norm 5.9334(7.0825) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 60.0360(59.9912) | Bit/dim 3.7738(3.7937) | Xent 1.3479(1.4002) | Loss 10.4515(11.1504) | Error 0.4821(0.5015) Steps 0(0.00) | Grad Norm 6.3430(7.0603) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 59.4877(59.9761) | Bit/dim 3.7702(3.7930) | Xent 1.3781(1.3995) | Loss 10.3676(11.1269) | Error 0.4924(0.5013) Steps 0(0.00) | Grad Norm 6.6634(7.0484) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 55.8064(59.8510) | Bit/dim 3.7689(3.7923) | Xent 1.3436(1.3978) | Loss 10.4280(11.1059) | Error 0.4842(0.5008) Steps 0(0.00) | Grad Norm 3.6030(6.9451) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 60.3067(59.8647) | Bit/dim 3.7694(3.7916) | Xent 1.3152(1.3953) | Loss 10.4648(11.0867) | Error 0.4734(0.4999) Steps 0(0.00) | Grad Norm 5.1583(6.8915) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 56.8948(59.7756) | Bit/dim 3.7587(3.7906) | Xent 1.2938(1.3923) | Loss 10.4020(11.0661) | Error 0.4706(0.4991) Steps 0(0.00) | Grad Norm 3.2375(6.7818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 23.7757, Epoch Time 392.8151(394.6762), Bit/dim 3.7604(best: 3.7610), Xent 1.2700, Loss 4.3954, Error 0.4586(best: 0.4655)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 61.0789(59.8147) | Bit/dim 3.7642(3.7898) | Xent 1.3050(1.3897) | Loss 13.9610(11.1530) | Error 0.4710(0.4982) Steps 0(0.00) | Grad Norm 3.3750(6.6796) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 58.3326(59.7703) | Bit/dim 3.7460(3.7885) | Xent 1.3168(1.3875) | Loss 10.4647(11.1323) | Error 0.4681(0.4973) Steps 0(0.00) | Grad Norm 3.3127(6.5786) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 61.5196(59.8227) | Bit/dim 3.7540(3.7875) | Xent 1.3147(1.3853) | Loss 10.3582(11.1091) | Error 0.4716(0.4965) Steps 0(0.00) | Grad Norm 2.5066(6.4565) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 62.3635(59.8990) | Bit/dim 3.7443(3.7862) | Xent 1.2995(1.3827) | Loss 10.3498(11.0863) | Error 0.4650(0.4956) Steps 0(0.00) | Grad Norm 3.4334(6.3658) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 59.1689(59.8770) | Bit/dim 3.7576(3.7853) | Xent 1.3228(1.3809) | Loss 10.4073(11.0660) | Error 0.4745(0.4950) Steps 0(0.00) | Grad Norm 4.0997(6.2978) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 57.1097(59.7940) | Bit/dim 3.7567(3.7845) | Xent 1.3098(1.3788) | Loss 10.3838(11.0455) | Error 0.4620(0.4940) Steps 0(0.00) | Grad Norm 4.7950(6.2527) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 22.8778, Epoch Time 398.4199(394.7886), Bit/dim 3.7472(best: 3.7604), Xent 1.2919, Loss 4.3931, Error 0.4659(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 58.9047(59.7673) | Bit/dim 3.7552(3.7836) | Xent 1.3446(1.3778) | Loss 13.7175(11.1257) | Error 0.4886(0.4938) Steps 0(0.00) | Grad Norm 5.8071(6.2393) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 60.5209(59.7900) | Bit/dim 3.7545(3.7827) | Xent 1.3891(1.3781) | Loss 10.3778(11.1032) | Error 0.5016(0.4941) Steps 0(0.00) | Grad Norm 10.3214(6.3618) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 56.5780(59.6936) | Bit/dim 3.7712(3.7824) | Xent 1.5245(1.3825) | Loss 10.6181(11.0887) | Error 0.5415(0.4955) Steps 0(0.00) | Grad Norm 14.2978(6.5999) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 58.6967(59.6637) | Bit/dim 3.7918(3.7826) | Xent 1.4764(1.3853) | Loss 10.5740(11.0732) | Error 0.5399(0.4968) Steps 0(0.00) | Grad Norm 10.1948(6.7077) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 57.0447(59.5851) | Bit/dim 3.8150(3.7836) | Xent 1.5299(1.3897) | Loss 10.6403(11.0602) | Error 0.5410(0.4981) Steps 0(0.00) | Grad Norm 13.4540(6.9101) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 58.4566(59.5513) | Bit/dim 3.8312(3.7850) | Xent 1.5060(1.3932) | Loss 10.6946(11.0493) | Error 0.5249(0.4989) Steps 0(0.00) | Grad Norm 15.1530(7.1574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 24.5351, Epoch Time 390.6811(394.6653), Bit/dim 3.8467(best: 3.7472), Xent 1.7270, Loss 4.7102, Error 0.6106(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 59.4144(59.5472) | Bit/dim 3.8480(3.7869) | Xent 1.8033(1.4055) | Loss 14.8495(11.1633) | Error 0.6245(0.5027) Steps 0(0.00) | Grad Norm 22.0852(7.6052) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 58.3894(59.5124) | Bit/dim 3.9356(3.7914) | Xent 1.7016(1.4143) | Loss 11.0352(11.1594) | Error 0.5819(0.5051) Steps 0(0.00) | Grad Norm 13.9134(7.7945) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 57.6591(59.4568) | Bit/dim 3.9344(3.7957) | Xent 1.5379(1.4180) | Loss 10.9379(11.1528) | Error 0.5396(0.5061) Steps 0(0.00) | Grad Norm 7.0967(7.7736) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 56.3648(59.3641) | Bit/dim 3.9374(3.7999) | Xent 1.4986(1.4205) | Loss 10.9961(11.1481) | Error 0.5331(0.5069) Steps 0(0.00) | Grad Norm 6.6198(7.7389) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 58.2245(59.3299) | Bit/dim 3.9346(3.8040) | Xent 1.4694(1.4219) | Loss 10.9489(11.1421) | Error 0.5364(0.5078) Steps 0(0.00) | Grad Norm 6.7726(7.7100) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 59.0532(59.3216) | Bit/dim 3.8949(3.8067) | Xent 1.4454(1.4226) | Loss 10.8516(11.1334) | Error 0.5217(0.5082) Steps 0(0.00) | Grad Norm 5.2726(7.6368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 24.7360, Epoch Time 389.6604(394.5152), Bit/dim 3.8863(best: 3.7472), Xent 1.3611, Loss 4.5668, Error 0.4895(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 67.4341(59.5650) | Bit/dim 3.8811(3.8089) | Xent 1.4046(1.4221) | Loss 14.5904(11.2371) | Error 0.4980(0.5079) Steps 0(0.00) | Grad Norm 4.2167(7.5342) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 62.1906(59.6437) | Bit/dim 3.8661(3.8106) | Xent 1.4096(1.4217) | Loss 10.6981(11.2209) | Error 0.5066(0.5079) Steps 0(0.00) | Grad Norm 3.8267(7.4230) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 59.3063(59.6336) | Bit/dim 3.8712(3.8125) | Xent 1.4054(1.4212) | Loss 10.6258(11.2031) | Error 0.5037(0.5078) Steps 0(0.00) | Grad Norm 5.2208(7.3569) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 63.5963(59.7525) | Bit/dim 3.8587(3.8138) | Xent 1.4378(1.4217) | Loss 10.7481(11.1894) | Error 0.5092(0.5078) Steps 0(0.00) | Grad Norm 6.9229(7.3439) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 62.6623(59.8398) | Bit/dim 3.8570(3.8151) | Xent 1.4556(1.4227) | Loss 10.7847(11.1773) | Error 0.5117(0.5079) Steps 0(0.00) | Grad Norm 11.2084(7.4598) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 62.9882(59.9342) | Bit/dim 3.8768(3.8170) | Xent 1.5904(1.4278) | Loss 10.9403(11.1702) | Error 0.5546(0.5093) Steps 0(0.00) | Grad Norm 12.6695(7.6161) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 23.8946, Epoch Time 418.2914(395.2285), Bit/dim 3.8460(best: 3.7472), Xent 1.3665, Loss 4.5292, Error 0.4934(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 57.2605(59.8540) | Bit/dim 3.8436(3.8178) | Xent 1.4060(1.4271) | Loss 14.3601(11.2659) | Error 0.5055(0.5092) Steps 0(0.00) | Grad Norm 4.8123(7.5320) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 62.6783(59.9387) | Bit/dim 3.8415(3.8185) | Xent 1.5101(1.4296) | Loss 10.7580(11.2506) | Error 0.5279(0.5098) Steps 0(0.00) | Grad Norm 8.4119(7.5584) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 66.0797(60.1230) | Bit/dim 3.8467(3.8193) | Xent 1.4592(1.4305) | Loss 10.6311(11.2321) | Error 0.5266(0.5103) Steps 0(0.00) | Grad Norm 7.5435(7.5580) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 61.8573(60.1750) | Bit/dim 3.8342(3.8198) | Xent 1.4151(1.4300) | Loss 10.6832(11.2156) | Error 0.5070(0.5102) Steps 0(0.00) | Grad Norm 5.2950(7.4901) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 62.3660(60.2407) | Bit/dim 3.8268(3.8200) | Xent 1.4203(1.4297) | Loss 10.6721(11.1993) | Error 0.5097(0.5102) Steps 0(0.00) | Grad Norm 5.5871(7.4330) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 65.3176(60.3930) | Bit/dim 3.8030(3.8195) | Xent 1.3994(1.4288) | Loss 10.5451(11.1797) | Error 0.4995(0.5098) Steps 0(0.00) | Grad Norm 4.6912(7.3507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 23.5814, Epoch Time 414.7877(395.8152), Bit/dim 3.8170(best: 3.7472), Xent 1.3384, Loss 4.4863, Error 0.4803(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 61.6512(60.4308) | Bit/dim 3.8141(3.8193) | Xent 1.3773(1.4273) | Loss 13.9991(11.2642) | Error 0.4916(0.5093) Steps 0(0.00) | Grad Norm 3.1250(7.2240) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 65.6171(60.5864) | Bit/dim 3.8159(3.8192) | Xent 1.3754(1.4257) | Loss 10.6252(11.2451) | Error 0.4950(0.5089) Steps 0(0.00) | Grad Norm 4.0567(7.1289) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 64.7183(60.7103) | Bit/dim 3.7889(3.8183) | Xent 1.3700(1.4241) | Loss 10.4914(11.2225) | Error 0.4931(0.5084) Steps 0(0.00) | Grad Norm 2.7395(6.9973) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 62.4081(60.7613) | Bit/dim 3.7985(3.8177) | Xent 1.3527(1.4219) | Loss 10.4784(11.2001) | Error 0.4809(0.5076) Steps 0(0.00) | Grad Norm 3.3989(6.8893) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 63.4867(60.8430) | Bit/dim 3.7796(3.8166) | Xent 1.3279(1.4191) | Loss 10.5250(11.1799) | Error 0.4686(0.5064) Steps 0(0.00) | Grad Norm 2.6615(6.7625) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 60.1112(60.8211) | Bit/dim 3.7719(3.8152) | Xent 1.3410(1.4168) | Loss 10.4634(11.1584) | Error 0.4760(0.5055) Steps 0(0.00) | Grad Norm 2.7481(6.6420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 23.1053, Epoch Time 417.0010(396.4508), Bit/dim 3.7792(best: 3.7472), Xent 1.2929, Loss 4.4256, Error 0.4659(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 56.4473(60.6899) | Bit/dim 3.7778(3.8141) | Xent 1.3343(1.4143) | Loss 13.6728(11.2338) | Error 0.4806(0.5047) Steps 0(0.00) | Grad Norm 2.4635(6.5167) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 60.4328(60.6821) | Bit/dim 3.7807(3.8131) | Xent 1.3292(1.4117) | Loss 10.4391(11.2100) | Error 0.4758(0.5039) Steps 0(0.00) | Grad Norm 2.4560(6.3949) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 57.7098(60.5930) | Bit/dim 3.7730(3.8119) | Xent 1.3285(1.4092) | Loss 10.4099(11.1860) | Error 0.4771(0.5031) Steps 0(0.00) | Grad Norm 1.9919(6.2628) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 57.9806(60.5146) | Bit/dim 3.7574(3.8103) | Xent 1.3128(1.4063) | Loss 10.2732(11.1586) | Error 0.4794(0.5024) Steps 0(0.00) | Grad Norm 2.2432(6.1422) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 61.2377(60.5363) | Bit/dim 3.7636(3.8089) | Xent 1.3055(1.4033) | Loss 10.3705(11.1350) | Error 0.4677(0.5013) Steps 0(0.00) | Grad Norm 1.9641(6.0168) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 57.9665(60.4592) | Bit/dim 3.7689(3.8077) | Xent 1.2957(1.4001) | Loss 10.4047(11.1130) | Error 0.4635(0.5002) Steps 0(0.00) | Grad Norm 2.1315(5.9003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 23.0492, Epoch Time 390.6710(396.2774), Bit/dim 3.7583(best: 3.7472), Xent 1.2597, Loss 4.3882, Error 0.4528(best: 0.4586)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 59.9472(60.4438) | Bit/dim 3.7544(3.8061) | Xent 1.3004(1.3971) | Loss 13.7905(11.1934) | Error 0.4653(0.4991) Steps 0(0.00) | Grad Norm 2.0808(5.7857) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 60.5682(60.4476) | Bit/dim 3.7568(3.8046) | Xent 1.3046(1.3943) | Loss 10.3842(11.1691) | Error 0.4655(0.4981) Steps 0(0.00) | Grad Norm 2.8100(5.6964) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 58.1556(60.3788) | Bit/dim 3.7465(3.8029) | Xent 1.2978(1.3914) | Loss 10.2423(11.1413) | Error 0.4683(0.4972) Steps 0(0.00) | Grad Norm 2.3163(5.5950) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 60.7258(60.3892) | Bit/dim 3.7527(3.8014) | Xent 1.3091(1.3890) | Loss 10.3998(11.1190) | Error 0.4657(0.4963) Steps 0(0.00) | Grad Norm 2.7983(5.5111) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 58.0744(60.3198) | Bit/dim 3.7413(3.7996) | Xent 1.2805(1.3857) | Loss 10.2706(11.0936) | Error 0.4613(0.4952) Steps 0(0.00) | Grad Norm 2.7115(5.4271) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 60.0533(60.3118) | Bit/dim 3.7347(3.7976) | Xent 1.2935(1.3829) | Loss 10.2323(11.0678) | Error 0.4555(0.4940) Steps 0(0.00) | Grad Norm 3.4382(5.3675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 23.3006, Epoch Time 396.7828(396.2926), Bit/dim 3.7481(best: 3.7472), Xent 1.2641, Loss 4.3802, Error 0.4510(best: 0.4528)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 58.4402(60.2556) | Bit/dim 3.7402(3.7959) | Xent 1.3095(1.3807) | Loss 13.6856(11.1463) | Error 0.4619(0.4931) Steps 0(0.00) | Grad Norm 6.7779(5.4098) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 58.0621(60.1898) | Bit/dim 3.7545(3.7946) | Xent 1.3514(1.3799) | Loss 10.3173(11.1214) | Error 0.4752(0.4925) Steps 0(0.00) | Grad Norm 12.1422(5.6118) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 55.4362(60.0472) | Bit/dim 3.7815(3.7943) | Xent 1.5398(1.3847) | Loss 10.5942(11.1056) | Error 0.5297(0.4937) Steps 0(0.00) | Grad Norm 17.8990(5.9804) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 62.4133(60.1182) | Bit/dim 3.8113(3.7948) | Xent 1.5060(1.3883) | Loss 10.7219(11.0941) | Error 0.5289(0.4947) Steps 0(0.00) | Grad Norm 12.3507(6.1715) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 56.6665(60.0147) | Bit/dim 3.8342(3.7959) | Xent 1.4002(1.3887) | Loss 10.6087(11.0795) | Error 0.4929(0.4947) Steps 0(0.00) | Grad Norm 11.7388(6.3385) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 57.6599(59.9440) | Bit/dim 3.7842(3.7956) | Xent 1.5239(1.3927) | Loss 10.6491(11.0666) | Error 0.5503(0.4963) Steps 0(0.00) | Grad Norm 9.3774(6.4297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 22.2377, Epoch Time 386.9903(396.0135), Bit/dim 3.7869(best: 3.7472), Xent 1.3612, Loss 4.4675, Error 0.4947(best: 0.4510)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 57.3028(59.8648) | Bit/dim 3.7779(3.7951) | Xent 1.4103(1.3932) | Loss 13.9647(11.1536) | Error 0.5041(0.4966) Steps 0(0.00) | Grad Norm 5.4102(6.3991) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 58.3789(59.8202) | Bit/dim 3.7851(3.7948) | Xent 1.3863(1.3930) | Loss 10.5279(11.1348) | Error 0.4960(0.4965) Steps 0(0.00) | Grad Norm 5.0388(6.3583) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 57.7164(59.7571) | Bit/dim 3.8058(3.7951) | Xent 1.3546(1.3919) | Loss 10.4651(11.1147) | Error 0.4768(0.4960) Steps 0(0.00) | Grad Norm 5.7237(6.3392) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 57.9866(59.7040) | Bit/dim 3.7824(3.7947) | Xent 1.3923(1.3919) | Loss 10.5488(11.0977) | Error 0.4976(0.4960) Steps 0(0.00) | Grad Norm 6.9632(6.3580) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 57.0313(59.6238) | Bit/dim 3.7822(3.7943) | Xent 1.3666(1.3911) | Loss 10.4178(11.0773) | Error 0.4848(0.4957) Steps 0(0.00) | Grad Norm 6.5012(6.3623) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 60.6952(59.6559) | Bit/dim 3.7768(3.7938) | Xent 1.3403(1.3896) | Loss 10.3697(11.0561) | Error 0.4865(0.4954) Steps 0(0.00) | Grad Norm 3.2036(6.2675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 23.6133, Epoch Time 388.9461(395.8015), Bit/dim 3.7685(best: 3.7472), Xent 1.3107, Loss 4.4238, Error 0.4669(best: 0.4510)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 60.3413(59.6765) | Bit/dim 3.7737(3.7932) | Xent 1.3467(1.3883) | Loss 13.9431(11.1427) | Error 0.4788(0.4949) Steps 0(0.00) | Grad Norm 4.5066(6.2147) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 58.2745(59.6344) | Bit/dim 3.7759(3.7927) | Xent 1.3188(1.3862) | Loss 10.4062(11.1206) | Error 0.4731(0.4942) Steps 0(0.00) | Grad Norm 4.1056(6.1514) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 61.6446(59.6947) | Bit/dim 3.7691(3.7920) | Xent 1.3068(1.3839) | Loss 10.3829(11.0985) | Error 0.4625(0.4933) Steps 0(0.00) | Grad Norm 2.7110(6.0482) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 58.0084(59.6441) | Bit/dim 3.7621(3.7911) | Xent 1.3222(1.3820) | Loss 10.4188(11.0781) | Error 0.4736(0.4927) Steps 0(0.00) | Grad Norm 3.6342(5.9758) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 57.4139(59.5772) | Bit/dim 3.7643(3.7903) | Xent 1.2926(1.3793) | Loss 10.4195(11.0583) | Error 0.4607(0.4917) Steps 0(0.00) | Grad Norm 2.2160(5.8630) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 54.9165(59.4374) | Bit/dim 3.7476(3.7890) | Xent 1.3144(1.3774) | Loss 10.3893(11.0383) | Error 0.4706(0.4911) Steps 0(0.00) | Grad Norm 3.0582(5.7788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 22.3896, Epoch Time 389.2758(395.6057), Bit/dim 3.7505(best: 3.7472), Xent 1.2471, Loss 4.3740, Error 0.4447(best: 0.4510)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 62.0208(59.5149) | Bit/dim 3.7506(3.7878) | Xent 1.2808(1.3745) | Loss 13.7159(11.1186) | Error 0.4605(0.4902) Steps 0(0.00) | Grad Norm 3.3142(5.7049) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 55.6505(59.3990) | Bit/dim 3.7436(3.7865) | Xent 1.2815(1.3717) | Loss 10.3043(11.0942) | Error 0.4510(0.4890) Steps 0(0.00) | Grad Norm 2.9205(5.6214) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 55.1241(59.2707) | Bit/dim 3.7538(3.7855) | Xent 1.3043(1.3697) | Loss 10.3827(11.0728) | Error 0.4683(0.4884) Steps 0(0.00) | Grad Norm 2.8823(5.5392) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 57.0952(59.2055) | Bit/dim 3.7469(3.7844) | Xent 1.2839(1.3671) | Loss 10.2154(11.0471) | Error 0.4577(0.4875) Steps 0(0.00) | Grad Norm 2.3478(5.4434) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 56.7869(59.1329) | Bit/dim 3.7248(3.7826) | Xent 1.2655(1.3640) | Loss 10.2472(11.0231) | Error 0.4525(0.4864) Steps 0(0.00) | Grad Norm 1.9965(5.3400) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 58.0817(59.1014) | Bit/dim 3.7388(3.7813) | Xent 1.2645(1.3611) | Loss 10.2586(11.0002) | Error 0.4560(0.4855) Steps 0(0.00) | Grad Norm 2.9284(5.2677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 22.3141, Epoch Time 383.5508(395.2441), Bit/dim 3.7345(best: 3.7472), Xent 1.2299, Loss 4.3494, Error 0.4396(best: 0.4447)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 57.7185(59.0599) | Bit/dim 3.7365(3.7799) | Xent 1.2782(1.3586) | Loss 13.7438(11.0825) | Error 0.4524(0.4845) Steps 0(0.00) | Grad Norm 3.2778(5.2080) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 55.0206(58.9387) | Bit/dim 3.7420(3.7788) | Xent 1.2900(1.3565) | Loss 10.3891(11.0617) | Error 0.4619(0.4838) Steps 0(0.00) | Grad Norm 5.5694(5.2188) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 58.8991(58.9375) | Bit/dim 3.7366(3.7775) | Xent 1.3145(1.3553) | Loss 10.2887(11.0385) | Error 0.4636(0.4832) Steps 0(0.00) | Grad Norm 12.1790(5.4276) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 57.9421(58.9077) | Bit/dim 3.7917(3.7780) | Xent 1.6420(1.3639) | Loss 10.8750(11.0336) | Error 0.5556(0.4854) Steps 0(0.00) | Grad Norm 19.3342(5.8448) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 59.0444(58.9118) | Bit/dim 3.7765(3.7779) | Xent 1.5213(1.3686) | Loss 10.6031(11.0207) | Error 0.5237(0.4865) Steps 0(0.00) | Grad Norm 12.0260(6.0303) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 58.6602(58.9042) | Bit/dim 3.8156(3.7790) | Xent 1.4584(1.3713) | Loss 10.6181(11.0086) | Error 0.5131(0.4873) Steps 0(0.00) | Grad Norm 7.2334(6.0664) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 21.9887, Epoch Time 384.7141(394.9282), Bit/dim 3.7743(best: 3.7345), Xent 1.3496, Loss 4.4491, Error 0.4865(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 54.3992(58.7691) | Bit/dim 3.7787(3.7790) | Xent 1.4086(1.3724) | Loss 13.9486(11.0968) | Error 0.5031(0.4878) Steps 0(0.00) | Grad Norm 4.6389(6.0235) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 57.3191(58.7256) | Bit/dim 3.8185(3.7802) | Xent 1.4492(1.3747) | Loss 10.6045(11.0820) | Error 0.5169(0.4887) Steps 0(0.00) | Grad Norm 10.4329(6.1558) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 56.9515(58.6723) | Bit/dim 3.8262(3.7816) | Xent 1.5920(1.3812) | Loss 10.7423(11.0718) | Error 0.5486(0.4905) Steps 0(0.00) | Grad Norm 16.4337(6.4642) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 57.4207(58.6348) | Bit/dim 3.8959(3.7850) | Xent 1.4211(1.3824) | Loss 10.7828(11.0632) | Error 0.5171(0.4913) Steps 0(0.00) | Grad Norm 9.8038(6.5643) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 56.7820(58.5792) | Bit/dim 3.8562(3.7872) | Xent 1.3681(1.3820) | Loss 10.7231(11.0530) | Error 0.4952(0.4914) Steps 0(0.00) | Grad Norm 5.3242(6.5271) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 62.6387(58.7010) | Bit/dim 3.8388(3.7887) | Xent 1.3915(1.3823) | Loss 10.6970(11.0423) | Error 0.4875(0.4913) Steps 0(0.00) | Grad Norm 5.2066(6.4875) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 23.5383, Epoch Time 385.3633(394.6412), Bit/dim 3.8456(best: 3.7345), Xent 1.3291, Loss 4.5102, Error 0.4687(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 57.6476(58.6694) | Bit/dim 3.8489(3.7905) | Xent 1.3996(1.3828) | Loss 14.4775(11.1453) | Error 0.4960(0.4914) Steps 0(0.00) | Grad Norm 4.1204(6.4165) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 57.7772(58.6426) | Bit/dim 3.8281(3.7916) | Xent 1.3560(1.3820) | Loss 10.6395(11.1302) | Error 0.4865(0.4913) Steps 0(0.00) | Grad Norm 4.0202(6.3446) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 58.6365(58.6424) | Bit/dim 3.8069(3.7921) | Xent 1.3577(1.3813) | Loss 10.4810(11.1107) | Error 0.4836(0.4911) Steps 0(0.00) | Grad Norm 3.5760(6.2616) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 62.0365(58.7443) | Bit/dim 3.8099(3.7926) | Xent 1.3476(1.3802) | Loss 10.3702(11.0885) | Error 0.4859(0.4909) Steps 0(0.00) | Grad Norm 4.0942(6.1965) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 58.2218(58.7286) | Bit/dim 3.8026(3.7929) | Xent 1.3404(1.3790) | Loss 10.4738(11.0700) | Error 0.4758(0.4904) Steps 0(0.00) | Grad Norm 3.9765(6.1299) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 58.7288(58.7286) | Bit/dim 3.8043(3.7933) | Xent 1.3130(1.3771) | Loss 10.4629(11.0518) | Error 0.4739(0.4899) Steps 0(0.00) | Grad Norm 3.5610(6.0529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 23.2020, Epoch Time 392.0734(394.5642), Bit/dim 3.8034(best: 3.7345), Xent 1.2680, Loss 4.4374, Error 0.4515(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 56.7766(58.6700) | Bit/dim 3.7919(3.7932) | Xent 1.3140(1.3752) | Loss 13.8940(11.1371) | Error 0.4719(0.4894) Steps 0(0.00) | Grad Norm 4.3029(6.0004) | Total Time 0.00(0.00)\n",
      "Iter 0866 | Time 58.9280(58.6778) | Bit/dim 3.7837(3.7929) | Xent 1.3178(1.3735) | Loss 10.4413(11.1162) | Error 0.4674(0.4887) Steps 0(0.00) | Grad Norm 5.1684(5.9754) | Total Time 0.00(0.00)\n",
      "Iter 0867 | Time 58.3454(58.6678) | Bit/dim 3.7909(3.7929) | Xent 1.3238(1.3720) | Loss 10.4383(11.0959) | Error 0.4744(0.4883) Steps 0(0.00) | Grad Norm 7.5922(6.0239) | Total Time 0.00(0.00)\n",
      "Iter 0868 | Time 59.2669(58.6858) | Bit/dim 3.8032(3.7932) | Xent 1.3534(1.3714) | Loss 10.5115(11.0783) | Error 0.4759(0.4879) Steps 0(0.00) | Grad Norm 9.9171(6.1407) | Total Time 0.00(0.00)\n",
      "Iter 0869 | Time 58.9840(58.6947) | Bit/dim 3.7854(3.7930) | Xent 1.4132(1.3727) | Loss 10.5654(11.0630) | Error 0.4974(0.4882) Steps 0(0.00) | Grad Norm 11.7961(6.3104) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 59.0905(58.7066) | Bit/dim 3.7776(3.7925) | Xent 1.4198(1.3741) | Loss 10.4986(11.0460) | Error 0.5088(0.4888) Steps 0(0.00) | Grad Norm 9.3699(6.4022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 23.5712, Epoch Time 391.4746(394.4715), Bit/dim 3.7643(best: 3.7345), Xent 1.2916, Loss 4.4101, Error 0.4580(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 60.6950(58.7663) | Bit/dim 3.7626(3.7916) | Xent 1.3315(1.3728) | Loss 13.9936(11.1344) | Error 0.4728(0.4884) Steps 0(0.00) | Grad Norm 2.6437(6.2894) | Total Time 0.00(0.00)\n",
      "Iter 0872 | Time 59.9399(58.8015) | Bit/dim 3.7591(3.7906) | Xent 1.3537(1.3722) | Loss 10.4305(11.1133) | Error 0.4816(0.4882) Steps 0(0.00) | Grad Norm 5.5442(6.2671) | Total Time 0.00(0.00)\n",
      "Iter 0873 | Time 61.9225(58.8951) | Bit/dim 3.7656(3.7899) | Xent 1.3361(1.3711) | Loss 10.3669(11.0909) | Error 0.4786(0.4879) Steps 0(0.00) | Grad Norm 4.6651(6.2190) | Total Time 0.00(0.00)\n",
      "Iter 0874 | Time 58.9832(58.8977) | Bit/dim 3.7703(3.7893) | Xent 1.3027(1.3691) | Loss 10.3952(11.0701) | Error 0.4689(0.4873) Steps 0(0.00) | Grad Norm 3.9727(6.1516) | Total Time 0.00(0.00)\n",
      "Iter 0875 | Time 60.8595(58.9566) | Bit/dim 3.7570(3.7883) | Xent 1.2893(1.3667) | Loss 10.4584(11.0517) | Error 0.4579(0.4864) Steps 0(0.00) | Grad Norm 2.5056(6.0422) | Total Time 0.00(0.00)\n",
      "Iter 0876 | Time 58.2643(58.9358) | Bit/dim 3.7618(3.7875) | Xent 1.3177(1.3652) | Loss 10.3737(11.0314) | Error 0.4708(0.4859) Steps 0(0.00) | Grad Norm 2.9218(5.9486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 23.7175, Epoch Time 400.6085(394.6556), Bit/dim 3.7548(best: 3.7345), Xent 1.2508, Loss 4.3802, Error 0.4471(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 60.7198(58.9893) | Bit/dim 3.7426(3.7862) | Xent 1.2933(1.3631) | Loss 13.8013(11.1145) | Error 0.4613(0.4852) Steps 0(0.00) | Grad Norm 3.1373(5.8643) | Total Time 0.00(0.00)\n",
      "Iter 0878 | Time 64.6040(59.1578) | Bit/dim 3.7635(3.7855) | Xent 1.2970(1.3611) | Loss 10.4757(11.0953) | Error 0.4620(0.4845) Steps 0(0.00) | Grad Norm 3.4085(5.7906) | Total Time 0.00(0.00)\n",
      "Iter 0879 | Time 59.2975(59.1620) | Bit/dim 3.7606(3.7848) | Xent 1.2672(1.3583) | Loss 10.2287(11.0693) | Error 0.4505(0.4835) Steps 0(0.00) | Grad Norm 2.0598(5.6787) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 56.9833(59.0966) | Bit/dim 3.7521(3.7838) | Xent 1.2680(1.3556) | Loss 10.3154(11.0467) | Error 0.4521(0.4825) Steps 0(0.00) | Grad Norm 2.6126(5.5867) | Total Time 0.00(0.00)\n",
      "Iter 0881 | Time 61.2603(59.1615) | Bit/dim 3.7411(3.7825) | Xent 1.2528(1.3525) | Loss 10.2328(11.0223) | Error 0.4496(0.4816) Steps 0(0.00) | Grad Norm 2.3066(5.4883) | Total Time 0.00(0.00)\n",
      "Iter 0882 | Time 63.0612(59.2785) | Bit/dim 3.7522(3.7816) | Xent 1.2498(1.3494) | Loss 10.3858(11.0032) | Error 0.4500(0.4806) Steps 0(0.00) | Grad Norm 2.7912(5.4074) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 22.6575, Epoch Time 405.1182(394.9695), Bit/dim 3.7467(best: 3.7345), Xent 1.2166, Loss 4.3550, Error 0.4340(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 58.1547(59.2448) | Bit/dim 3.7460(3.7805) | Xent 1.2645(1.3469) | Loss 13.7966(11.0870) | Error 0.4469(0.4796) Steps 0(0.00) | Grad Norm 3.6800(5.3556) | Total Time 0.00(0.00)\n",
      "Iter 0884 | Time 59.8642(59.2634) | Bit/dim 3.7453(3.7795) | Xent 1.2722(1.3446) | Loss 10.3070(11.0636) | Error 0.4577(0.4789) Steps 0(0.00) | Grad Norm 5.3661(5.3559) | Total Time 0.00(0.00)\n",
      "Iter 0885 | Time 55.4620(59.1493) | Bit/dim 3.7459(3.7785) | Xent 1.3107(1.3436) | Loss 10.3574(11.0424) | Error 0.4745(0.4788) Steps 0(0.00) | Grad Norm 8.2857(5.4438) | Total Time 0.00(0.00)\n",
      "Iter 0886 | Time 60.0096(59.1751) | Bit/dim 3.7474(3.7775) | Xent 1.3485(1.3437) | Loss 10.4242(11.0239) | Error 0.4732(0.4786) Steps 0(0.00) | Grad Norm 10.3902(5.5922) | Total Time 0.00(0.00)\n",
      "Iter 0887 | Time 59.6016(59.1879) | Bit/dim 3.7397(3.7764) | Xent 1.2877(1.3421) | Loss 10.3712(11.0043) | Error 0.4563(0.4780) Steps 0(0.00) | Grad Norm 7.8183(5.6589) | Total Time 0.00(0.00)\n",
      "Iter 0888 | Time 58.7729(59.1755) | Bit/dim 3.7176(3.7746) | Xent 1.2434(1.3391) | Loss 10.2576(10.9819) | Error 0.4440(0.4770) Steps 0(0.00) | Grad Norm 2.1596(5.5540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 23.8384, Epoch Time 391.8169(394.8749), Bit/dim 3.7359(best: 3.7345), Xent 1.2261, Loss 4.3489, Error 0.4343(best: 0.4340)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 59.0841(59.1727) | Bit/dim 3.7263(3.7732) | Xent 1.2681(1.3370) | Loss 13.7962(11.0663) | Error 0.4580(0.4764) Steps 0(0.00) | Grad Norm 4.5148(5.5228) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 64.2568(59.3253) | Bit/dim 3.7341(3.7720) | Xent 1.2350(1.3339) | Loss 10.2177(11.0408) | Error 0.4436(0.4754) Steps 0(0.00) | Grad Norm 4.6059(5.4953) | Total Time 0.00(0.00)\n",
      "Iter 0891 | Time 58.1449(59.2899) | Bit/dim 3.7291(3.7707) | Xent 1.2545(1.3315) | Loss 10.2373(11.0167) | Error 0.4487(0.4746) Steps 0(0.00) | Grad Norm 3.1890(5.4261) | Total Time 0.00(0.00)\n",
      "Iter 0892 | Time 59.1401(59.2854) | Bit/dim 3.7342(3.7696) | Xent 1.2193(1.3282) | Loss 10.2390(10.9934) | Error 0.4363(0.4735) Steps 0(0.00) | Grad Norm 2.4002(5.3353) | Total Time 0.00(0.00)\n",
      "Iter 0893 | Time 60.3661(59.3178) | Bit/dim 3.7221(3.7682) | Xent 1.2174(1.3248) | Loss 10.2144(10.9700) | Error 0.4331(0.4722) Steps 0(0.00) | Grad Norm 3.4058(5.2774) | Total Time 0.00(0.00)\n",
      "Iter 0894 | Time 61.3771(59.3796) | Bit/dim 3.7338(3.7672) | Xent 1.2498(1.3226) | Loss 10.1948(10.9468) | Error 0.4420(0.4713) Steps 0(0.00) | Grad Norm 4.6185(5.2577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 23.2641, Epoch Time 401.3742(395.0699), Bit/dim 3.7320(best: 3.7345), Xent 1.1827, Loss 4.3234, Error 0.4221(best: 0.4340)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 58.6909(59.3589) | Bit/dim 3.7296(3.7660) | Xent 1.2357(1.3200) | Loss 13.5832(11.0259) | Error 0.4453(0.4706) Steps 0(0.00) | Grad Norm 4.1279(5.2238) | Total Time 0.00(0.00)\n",
      "Iter 0896 | Time 58.8939(59.3450) | Bit/dim 3.7231(3.7647) | Xent 1.2414(1.3176) | Loss 10.1690(11.0002) | Error 0.4344(0.4695) Steps 0(0.00) | Grad Norm 4.1973(5.1930) | Total Time 0.00(0.00)\n",
      "Iter 0897 | Time 61.2674(59.4026) | Bit/dim 3.7196(3.7634) | Xent 1.2444(1.3154) | Loss 10.2442(10.9775) | Error 0.4409(0.4686) Steps 0(0.00) | Grad Norm 5.5637(5.2041) | Total Time 0.00(0.00)\n",
      "Iter 0898 | Time 57.6096(59.3488) | Bit/dim 3.7250(3.7622) | Xent 1.2964(1.3149) | Loss 10.1947(10.9540) | Error 0.4590(0.4683) Steps 0(0.00) | Grad Norm 9.6301(5.3369) | Total Time 0.00(0.00)\n",
      "Iter 0899 | Time 59.7237(59.3601) | Bit/dim 3.7438(3.7617) | Xent 1.4196(1.3180) | Loss 10.4749(10.9396) | Error 0.4955(0.4691) Steps 0(0.00) | Grad Norm 13.2527(5.5744) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 59.9277(59.3771) | Bit/dim 3.7523(3.7614) | Xent 1.3583(1.3192) | Loss 10.4294(10.9243) | Error 0.4751(0.4693) Steps 0(0.00) | Grad Norm 8.4192(5.6597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 23.4387, Epoch Time 395.7897(395.0915), Bit/dim 3.7339(best: 3.7320), Xent 1.3914, Loss 4.4296, Error 0.5049(best: 0.4221)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 59.1660(59.3708) | Bit/dim 3.7265(3.7604) | Xent 1.4446(1.3230) | Loss 13.9444(11.0149) | Error 0.5180(0.4708) Steps 0(0.00) | Grad Norm 10.8957(5.8168) | Total Time 0.00(0.00)\n",
      "Iter 0902 | Time 55.2618(59.2475) | Bit/dim 3.7936(3.7614) | Xent 1.4272(1.3261) | Loss 10.5721(11.0016) | Error 0.5121(0.4720) Steps 0(0.00) | Grad Norm 13.5616(6.0491) | Total Time 0.00(0.00)\n",
      "Iter 0903 | Time 59.7665(59.2631) | Bit/dim 3.7666(3.7615) | Xent 1.4537(1.3299) | Loss 10.5996(10.9896) | Error 0.5240(0.4736) Steps 0(0.00) | Grad Norm 9.2837(6.1462) | Total Time 0.00(0.00)\n",
      "Iter 0904 | Time 62.5617(59.3620) | Bit/dim 3.8165(3.7632) | Xent 1.3622(1.3309) | Loss 10.5912(10.9776) | Error 0.4890(0.4740) Steps 0(0.00) | Grad Norm 6.2156(6.1482) | Total Time 0.00(0.00)\n",
      "Iter 0905 | Time 62.1169(59.4447) | Bit/dim 3.8048(3.7644) | Xent 1.3320(1.3309) | Loss 10.5519(10.9649) | Error 0.4751(0.4741) Steps 0(0.00) | Grad Norm 8.1023(6.2069) | Total Time 0.00(0.00)\n",
      "Iter 0906 | Time 63.1318(59.5553) | Bit/dim 3.7833(3.7650) | Xent 1.3186(1.3306) | Loss 10.4377(10.9490) | Error 0.4692(0.4739) Steps 0(0.00) | Grad Norm 4.7424(6.1629) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 23.3251, Epoch Time 401.6299(395.2876), Bit/dim 3.8112(best: 3.7320), Xent 1.2586, Loss 4.4405, Error 0.4469(best: 0.4221)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 66.3419(59.7589) | Bit/dim 3.7994(3.7660) | Xent 1.3041(1.3298) | Loss 14.2537(11.0482) | Error 0.4709(0.4738) Steps 0(0.00) | Grad Norm 4.5162(6.1135) | Total Time 0.00(0.00)\n",
      "Iter 0908 | Time 60.2085(59.7724) | Bit/dim 3.8018(3.7671) | Xent 1.2911(1.3286) | Loss 10.4025(11.0288) | Error 0.4544(0.4733) Steps 0(0.00) | Grad Norm 3.7262(6.0419) | Total Time 0.00(0.00)\n",
      "Iter 0909 | Time 62.9010(59.8662) | Bit/dim 3.7800(3.7675) | Xent 1.2764(1.3270) | Loss 10.4016(11.0100) | Error 0.4529(0.4726) Steps 0(0.00) | Grad Norm 3.0349(5.9517) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 61.7995(59.9242) | Bit/dim 3.7744(3.7677) | Xent 1.2637(1.3251) | Loss 10.4387(10.9929) | Error 0.4570(0.4722) Steps 0(0.00) | Grad Norm 3.5091(5.8784) | Total Time 0.00(0.00)\n",
      "Iter 0911 | Time 60.6043(59.9446) | Bit/dim 3.7788(3.7680) | Xent 1.2443(1.3227) | Loss 10.2899(10.9718) | Error 0.4427(0.4713) Steps 0(0.00) | Grad Norm 3.1720(5.7972) | Total Time 0.00(0.00)\n",
      "Iter 0912 | Time 59.6632(59.9362) | Bit/dim 3.7722(3.7681) | Xent 1.2436(1.3203) | Loss 10.2787(10.9510) | Error 0.4464(0.4705) Steps 0(0.00) | Grad Norm 3.6876(5.7339) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 23.4754, Epoch Time 410.7408(395.7512), Bit/dim 3.7684(best: 3.7320), Xent 1.2176, Loss 4.3772, Error 0.4332(best: 0.4221)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 60.5631(59.9550) | Bit/dim 3.7697(3.7682) | Xent 1.2721(1.3189) | Loss 13.7635(11.0354) | Error 0.4496(0.4699) Steps 0(0.00) | Grad Norm 5.4623(5.7258) | Total Time 0.00(0.00)\n",
      "Iter 0914 | Time 58.7850(59.9199) | Bit/dim 3.7678(3.7682) | Xent 1.2945(1.3182) | Loss 10.3986(11.0162) | Error 0.4605(0.4696) Steps 0(0.00) | Grad Norm 10.1397(5.8582) | Total Time 0.00(0.00)\n",
      "Iter 0915 | Time 61.1135(59.9557) | Bit/dim 3.7644(3.7681) | Xent 1.3811(1.3200) | Loss 10.4296(10.9987) | Error 0.4810(0.4700) Steps 0(0.00) | Grad Norm 10.9540(6.0111) | Total Time 0.00(0.00)\n",
      "Iter 0916 | Time 59.1561(59.9317) | Bit/dim 3.7541(3.7676) | Xent 1.2875(1.3191) | Loss 10.2333(10.9757) | Error 0.4566(0.4696) Steps 0(0.00) | Grad Norm 5.4010(5.9928) | Total Time 0.00(0.00)\n",
      "Iter 0917 | Time 59.9216(59.9314) | Bit/dim 3.7617(3.7675) | Xent 1.2696(1.3176) | Loss 10.3146(10.9559) | Error 0.4541(0.4691) Steps 0(0.00) | Grad Norm 4.5121(5.9484) | Total Time 0.00(0.00)\n",
      "Iter 0918 | Time 58.2395(59.8807) | Bit/dim 3.7401(3.7666) | Xent 1.3031(1.3172) | Loss 10.2768(10.9355) | Error 0.4627(0.4689) Steps 0(0.00) | Grad Norm 4.5812(5.9073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 23.0450, Epoch Time 396.7692(395.7818), Bit/dim 3.7443(best: 3.7320), Xent 1.2051, Loss 4.3469, Error 0.4271(best: 0.4221)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 60.6110(59.9026) | Bit/dim 3.7361(3.7657) | Xent 1.2428(1.3149) | Loss 13.7337(11.0194) | Error 0.4460(0.4682) Steps 0(0.00) | Grad Norm 3.1964(5.8260) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 64.7345(60.0475) | Bit/dim 3.7456(3.7651) | Xent 1.2616(1.3133) | Loss 10.4358(11.0019) | Error 0.4481(0.4676) Steps 0(0.00) | Grad Norm 3.8997(5.7682) | Total Time 0.00(0.00)\n",
      "Iter 0921 | Time 63.0981(60.1390) | Bit/dim 3.7417(3.7644) | Xent 1.2294(1.3108) | Loss 10.2513(10.9794) | Error 0.4389(0.4668) Steps 0(0.00) | Grad Norm 2.7850(5.6787) | Total Time 0.00(0.00)\n",
      "Iter 0922 | Time 60.0741(60.1371) | Bit/dim 3.7452(3.7638) | Xent 1.2465(1.3089) | Loss 10.2994(10.9590) | Error 0.4406(0.4660) Steps 0(0.00) | Grad Norm 4.5936(5.6462) | Total Time 0.00(0.00)\n",
      "Iter 0923 | Time 60.4375(60.1461) | Bit/dim 3.7421(3.7632) | Xent 1.2132(1.3060) | Loss 10.1900(10.9359) | Error 0.4335(0.4650) Steps 0(0.00) | Grad Norm 3.1506(5.5713) | Total Time 0.00(0.00)\n",
      "Iter 0924 | Time 62.0456(60.2031) | Bit/dim 3.7443(3.7626) | Xent 1.2118(1.3032) | Loss 10.3128(10.9172) | Error 0.4304(0.4640) Steps 0(0.00) | Grad Norm 3.2491(5.5016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 23.3892, Epoch Time 410.0920(396.2111), Bit/dim 3.7350(best: 3.7320), Xent 1.1735, Loss 4.3217, Error 0.4188(best: 0.4221)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 59.0954(60.1699) | Bit/dim 3.7331(3.7617) | Xent 1.2118(1.3004) | Loss 13.8086(11.0040) | Error 0.4285(0.4629) Steps 0(0.00) | Grad Norm 3.8606(5.4524) | Total Time 0.00(0.00)\n",
      "Iter 0926 | Time 64.4084(60.2970) | Bit/dim 3.7334(3.7609) | Xent 1.1979(1.2974) | Loss 10.2314(10.9808) | Error 0.4235(0.4617) Steps 0(0.00) | Grad Norm 2.4777(5.3632) | Total Time 0.00(0.00)\n",
      "Iter 0927 | Time 62.7485(60.3706) | Bit/dim 3.7403(3.7603) | Xent 1.2054(1.2946) | Loss 10.1539(10.9560) | Error 0.4321(0.4608) Steps 0(0.00) | Grad Norm 3.1192(5.2959) | Total Time 0.00(0.00)\n",
      "Iter 0928 | Time 62.1505(60.4240) | Bit/dim 3.7213(3.7591) | Xent 1.2245(1.2925) | Loss 10.2079(10.9336) | Error 0.4387(0.4602) Steps 0(0.00) | Grad Norm 4.0891(5.2596) | Total Time 0.00(0.00)\n",
      "Iter 0929 | Time 62.5659(60.4882) | Bit/dim 3.7181(3.7579) | Xent 1.2435(1.2910) | Loss 10.1861(10.9111) | Error 0.4441(0.4597) Steps 0(0.00) | Grad Norm 5.1539(5.2565) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
