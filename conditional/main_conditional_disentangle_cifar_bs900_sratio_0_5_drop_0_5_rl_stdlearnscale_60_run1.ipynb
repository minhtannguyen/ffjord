{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_60_run1/epoch_13_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_60_run1', scale=1.0, scale_fac=1.0, scale_std=60.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0720 | Time 12.5431(14.2491) | Bit/dim 4.5725(4.5944) | Xent 1.6876(1.7138) | Loss 10.4216(11.0892) | Error 0.5989(0.6086) Steps 0(0.00) | Grad Norm 59.5873(21.2947) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 13.8116(14.0348) | Bit/dim 4.5970(4.5860) | Xent 1.7607(1.7318) | Loss 10.6889(10.9512) | Error 0.6356(0.6142) Steps 0(0.00) | Grad Norm 25.8234(24.8928) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 12.7707(13.8597) | Bit/dim 4.5093(4.5717) | Xent 1.7602(1.7268) | Loss 10.3515(10.8254) | Error 0.6167(0.6126) Steps 0(0.00) | Grad Norm 6.7491(22.6342) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 12.7651(13.7789) | Bit/dim 4.5577(4.5615) | Xent 1.7441(1.7191) | Loss 10.4287(10.7316) | Error 0.6244(0.6098) Steps 0(0.00) | Grad Norm 32.2958(22.1536) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 13.3920(13.6978) | Bit/dim 4.5886(4.5551) | Xent 1.8667(1.7368) | Loss 10.8638(10.6823) | Error 0.6700(0.6181) Steps 0(0.00) | Grad Norm 47.5925(25.2693) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 13.5416(13.6567) | Bit/dim 4.5660(4.5554) | Xent 1.7691(1.7511) | Loss 10.6329(10.6569) | Error 0.6211(0.6239) Steps 0(0.00) | Grad Norm 17.1248(24.5872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 76.1337, Epoch Time 854.3351(710.5839), Bit/dim 4.5273(best: inf), Xent 1.6714, Loss 5.3629, Error 0.5939(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 12.7799(13.4908) | Bit/dim 4.4859(4.5454) | Xent 1.6331(1.7344) | Loss 10.1377(10.9587) | Error 0.5822(0.6169) Steps 0(0.00) | Grad Norm 7.4741(20.7359) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 13.2694(13.4646) | Bit/dim 4.4771(4.5290) | Xent 1.7648(1.7221) | Loss 10.3751(10.7935) | Error 0.6067(0.6117) Steps 0(0.00) | Grad Norm 8.6486(17.4652) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 13.2967(13.4520) | Bit/dim 4.4615(4.5142) | Xent 1.6265(1.7004) | Loss 10.2215(10.6571) | Error 0.5644(0.6053) Steps 0(0.00) | Grad Norm 15.8278(16.9077) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 13.9230(13.4378) | Bit/dim 4.4333(4.4979) | Xent 1.7181(1.6980) | Loss 10.4109(10.5526) | Error 0.6267(0.6047) Steps 0(0.00) | Grad Norm 30.7747(18.4250) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.8646(13.5578) | Bit/dim 4.4671(4.4894) | Xent 1.6688(1.6995) | Loss 10.4814(10.5097) | Error 0.6089(0.6056) Steps 0(0.00) | Grad Norm 24.4786(19.2302) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 73.6443, Epoch Time 829.2616(714.1443), Bit/dim 4.4508(best: 4.5273), Xent 1.6169, Loss 5.2593, Error 0.5754(best: 0.5939)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 13.8966(13.5751) | Bit/dim 4.4026(4.4733) | Xent 1.6053(1.6921) | Loss 10.1631(10.9051) | Error 0.5878(0.6036) Steps 0(0.00) | Grad Norm 12.4689(19.0359) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 12.6032(13.7082) | Bit/dim 4.4240(4.4625) | Xent 1.5652(1.6747) | Loss 9.8968(10.7288) | Error 0.5600(0.5985) Steps 0(0.00) | Grad Norm 10.5497(16.6597) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 13.5572(13.7376) | Bit/dim 4.4056(4.4557) | Xent 1.6085(1.6650) | Loss 10.2268(10.5956) | Error 0.5589(0.5939) Steps 0(0.00) | Grad Norm 11.5759(16.6766) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 14.4996(13.7525) | Bit/dim 4.4568(4.4422) | Xent 1.6527(1.6577) | Loss 10.2541(10.4845) | Error 0.5800(0.5928) Steps 0(0.00) | Grad Norm 24.2741(15.8907) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.1995(13.7547) | Bit/dim 4.4067(4.4302) | Xent 1.7172(1.6585) | Loss 10.2713(10.4029) | Error 0.6178(0.5943) Steps 0(0.00) | Grad Norm 11.0153(16.1825) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 14.7203(13.8130) | Bit/dim 4.4305(4.4223) | Xent 1.6270(1.6557) | Loss 10.2772(10.3601) | Error 0.5933(0.5932) Steps 0(0.00) | Grad Norm 9.0812(15.9086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 73.0409, Epoch Time 852.4940(718.2948), Bit/dim 4.3910(best: 4.4508), Xent 1.5466, Loss 5.1642, Error 0.5572(best: 0.5754)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 13.6460(13.9332) | Bit/dim 4.4159(4.4136) | Xent 1.6449(1.6467) | Loss 10.0524(10.7075) | Error 0.5856(0.5891) Steps 0(0.00) | Grad Norm 8.9325(15.9063) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.3831(13.9497) | Bit/dim 4.3240(4.3980) | Xent 1.6880(1.6414) | Loss 9.9788(10.5525) | Error 0.6189(0.5886) Steps 0(0.00) | Grad Norm 13.0507(15.3996) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 13.5339(13.9229) | Bit/dim 4.3662(4.3890) | Xent 1.6124(1.6298) | Loss 10.1121(10.4382) | Error 0.5789(0.5841) Steps 0(0.00) | Grad Norm 10.6826(16.6101) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 14.1470(13.8371) | Bit/dim 4.3674(4.3844) | Xent 1.6074(1.6231) | Loss 9.9171(10.3353) | Error 0.5622(0.5835) Steps 0(0.00) | Grad Norm 24.5042(16.6983) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 13.1059(13.8282) | Bit/dim 4.3334(4.3725) | Xent 1.5818(1.6269) | Loss 10.0599(10.2774) | Error 0.5744(0.5854) Steps 0(0.00) | Grad Norm 5.6942(16.6607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 73.7486, Epoch Time 855.0690(722.3980), Bit/dim 4.3488(best: 4.3910), Xent 1.5367, Loss 5.1172, Error 0.5503(best: 0.5572)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 14.5800(13.9356) | Bit/dim 4.2914(4.3635) | Xent 1.5851(1.6191) | Loss 10.1505(10.6502) | Error 0.5656(0.5819) Steps 0(0.00) | Grad Norm 13.2812(16.8810) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 14.7852(13.9821) | Bit/dim 4.3244(4.3557) | Xent 1.5577(1.6074) | Loss 9.7790(10.4620) | Error 0.5467(0.5778) Steps 0(0.00) | Grad Norm 13.3521(16.5990) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 13.4877(13.9676) | Bit/dim 4.2499(4.3423) | Xent 1.5230(1.5953) | Loss 9.7706(10.3249) | Error 0.5489(0.5745) Steps 0(0.00) | Grad Norm 5.5021(15.1797) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 13.5629(13.8926) | Bit/dim 4.3272(4.3320) | Xent 1.6717(1.5924) | Loss 10.0143(10.2200) | Error 0.5900(0.5729) Steps 0(0.00) | Grad Norm 16.4834(14.2672) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 13.3457(13.8368) | Bit/dim 4.3571(4.3299) | Xent 1.6167(1.6146) | Loss 10.3041(10.2071) | Error 0.5744(0.5794) Steps 0(0.00) | Grad Norm 15.3557(16.7638) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 14.0636(13.8549) | Bit/dim 4.2910(4.3235) | Xent 1.6170(1.6220) | Loss 10.0408(10.1637) | Error 0.5722(0.5821) Steps 0(0.00) | Grad Norm 8.8291(15.4772) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 72.3512, Epoch Time 853.8917(726.3428), Bit/dim 4.2866(best: 4.3488), Xent 1.5234, Loss 5.0483, Error 0.5453(best: 0.5503)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 14.3436(14.0813) | Bit/dim 4.2898(4.3158) | Xent 1.5779(1.6039) | Loss 9.9541(10.4594) | Error 0.5678(0.5748) Steps 0(0.00) | Grad Norm 9.3688(14.1761) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.8483(14.2775) | Bit/dim 4.2757(4.3042) | Xent 1.5407(1.5851) | Loss 9.8678(10.3182) | Error 0.5478(0.5697) Steps 0(0.00) | Grad Norm 22.6515(13.7243) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 15.5717(14.3218) | Bit/dim 4.2559(4.3011) | Xent 1.5777(1.5787) | Loss 10.1717(10.2288) | Error 0.5622(0.5681) Steps 0(0.00) | Grad Norm 10.3121(14.1766) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 14.5630(14.3814) | Bit/dim 4.2635(4.2890) | Xent 1.5112(1.5707) | Loss 9.8961(10.1331) | Error 0.5456(0.5660) Steps 0(0.00) | Grad Norm 15.0865(14.0013) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 13.4668(14.2837) | Bit/dim 4.2379(4.2762) | Xent 1.4714(1.5591) | Loss 9.7912(10.0423) | Error 0.5233(0.5620) Steps 0(0.00) | Grad Norm 13.8560(13.2565) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 74.9114, Epoch Time 888.2289(731.1994), Bit/dim 4.2300(best: 4.2866), Xent 1.5719, Loss 5.0160, Error 0.5701(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 14.8415(14.3675) | Bit/dim 4.2484(4.2647) | Xent 1.5262(1.5533) | Loss 9.6058(10.4569) | Error 0.5478(0.5606) Steps 0(0.00) | Grad Norm 13.3030(14.1958) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 15.3779(14.5808) | Bit/dim 4.2077(4.2517) | Xent 1.5018(1.5394) | Loss 9.7675(10.2994) | Error 0.5411(0.5559) Steps 0(0.00) | Grad Norm 10.8511(13.2940) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 15.6086(14.7797) | Bit/dim 4.2108(4.2407) | Xent 1.5106(1.5313) | Loss 9.8190(10.1691) | Error 0.5533(0.5532) Steps 0(0.00) | Grad Norm 17.8879(12.8344) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 15.3347(15.0705) | Bit/dim 4.1712(4.2311) | Xent 1.4361(1.5134) | Loss 9.4620(10.0505) | Error 0.5378(0.5479) Steps 0(0.00) | Grad Norm 11.6835(12.0275) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 15.3228(15.1786) | Bit/dim 4.2296(4.2329) | Xent 1.6126(1.5300) | Loss 9.8881(10.0235) | Error 0.5867(0.5512) Steps 0(0.00) | Grad Norm 22.4812(15.0151) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 15.5373(15.3678) | Bit/dim 4.2205(4.2301) | Xent 1.5039(1.5356) | Loss 9.7899(9.9696) | Error 0.5533(0.5545) Steps 0(0.00) | Grad Norm 12.7162(14.7239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 76.3346, Epoch Time 944.2058(737.5896), Bit/dim 4.2093(best: 4.2300), Xent 1.4080, Loss 4.9133, Error 0.5108(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 12.9020(15.1590) | Bit/dim 4.1840(4.2217) | Xent 1.4733(1.5257) | Loss 9.5913(10.3074) | Error 0.5267(0.5510) Steps 0(0.00) | Grad Norm 12.6029(14.6790) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 13.5588(14.9610) | Bit/dim 4.1660(4.2122) | Xent 1.4492(1.5061) | Loss 9.6649(10.1468) | Error 0.5289(0.5438) Steps 0(0.00) | Grad Norm 4.2007(13.2400) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 14.1208(14.7980) | Bit/dim 4.1696(4.2007) | Xent 1.4655(1.4917) | Loss 9.6587(10.0044) | Error 0.5433(0.5412) Steps 0(0.00) | Grad Norm 6.4096(11.9126) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 15.5308(14.6770) | Bit/dim 4.2017(4.1894) | Xent 1.5112(1.4834) | Loss 9.7503(9.8861) | Error 0.5300(0.5377) Steps 0(0.00) | Grad Norm 11.1189(11.6087) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 13.4815(14.5390) | Bit/dim 4.1492(4.1849) | Xent 1.4227(1.4711) | Loss 9.5780(9.8036) | Error 0.5133(0.5336) Steps 0(0.00) | Grad Norm 6.2092(11.8762) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 73.2091, Epoch Time 877.9887(741.8015), Bit/dim 4.1485(best: 4.2093), Xent 1.3499, Loss 4.8234, Error 0.4898(best: 0.5108)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 13.0590(14.4108) | Bit/dim 4.0979(4.1763) | Xent 1.3648(1.4589) | Loss 9.3988(10.1860) | Error 0.5022(0.5293) Steps 0(0.00) | Grad Norm 10.0860(11.4958) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.1664(14.2761) | Bit/dim 4.1746(4.1747) | Xent 1.6894(1.4709) | Loss 9.8890(10.0370) | Error 0.5944(0.5325) Steps 0(0.00) | Grad Norm 37.2153(14.7175) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 13.9707(14.3213) | Bit/dim 4.1544(4.1729) | Xent 1.4832(1.4697) | Loss 9.7537(9.9476) | Error 0.5211(0.5313) Steps 0(0.00) | Grad Norm 10.8817(14.4609) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 13.1102(14.3796) | Bit/dim 4.1324(4.1662) | Xent 1.4574(1.4569) | Loss 9.4296(9.8275) | Error 0.5211(0.5265) Steps 0(0.00) | Grad Norm 6.7377(12.5380) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 13.6269(14.3962) | Bit/dim 4.1026(4.1554) | Xent 1.4313(1.4458) | Loss 9.4774(9.7329) | Error 0.5189(0.5226) Steps 0(0.00) | Grad Norm 13.2970(12.0629) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 14.1603(14.3024) | Bit/dim 4.1155(4.1505) | Xent 1.5212(1.4470) | Loss 9.5992(9.6731) | Error 0.5544(0.5221) Steps 0(0.00) | Grad Norm 17.9244(13.0941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 71.1925, Epoch Time 871.9378(745.7056), Bit/dim 4.1170(best: 4.1485), Xent 1.3481, Loss 4.7911, Error 0.4894(best: 0.4898)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 13.3425(14.2118) | Bit/dim 4.1332(4.1445) | Xent 1.3955(1.4349) | Loss 9.2562(9.9325) | Error 0.5178(0.5190) Steps 0(0.00) | Grad Norm 12.9387(12.9173) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.0153(14.1327) | Bit/dim 4.1673(4.1367) | Xent 1.3446(1.4196) | Loss 9.4929(9.8019) | Error 0.4922(0.5127) Steps 0(0.00) | Grad Norm 13.9813(11.8596) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 13.6576(14.1075) | Bit/dim 4.0803(4.1267) | Xent 1.3673(1.4057) | Loss 9.4767(9.7112) | Error 0.4933(0.5086) Steps 0(0.00) | Grad Norm 7.4086(11.9913) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 13.4974(14.1433) | Bit/dim 4.0792(4.1212) | Xent 1.3658(1.3992) | Loss 9.4198(9.6325) | Error 0.5044(0.5059) Steps 0(0.00) | Grad Norm 15.4530(11.6063) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 13.7208(14.1146) | Bit/dim 4.0771(4.1165) | Xent 1.3872(1.4004) | Loss 9.4674(9.5707) | Error 0.4944(0.5076) Steps 0(0.00) | Grad Norm 8.5452(11.8178) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 71.9045, Epoch Time 862.9868(749.2241), Bit/dim 4.0881(best: 4.1170), Xent 1.2976, Loss 4.7369, Error 0.4690(best: 0.4894)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 13.5278(14.2150) | Bit/dim 4.0510(4.1083) | Xent 1.2938(1.3857) | Loss 9.2217(9.9651) | Error 0.4844(0.5047) Steps 0(0.00) | Grad Norm 9.0908(11.0939) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 13.4059(14.1887) | Bit/dim 4.1100(4.1026) | Xent 1.2973(1.3757) | Loss 9.3826(9.8041) | Error 0.4700(0.5012) Steps 0(0.00) | Grad Norm 8.6942(10.5676) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.9112(14.0798) | Bit/dim 4.0808(4.1018) | Xent 1.4249(1.3818) | Loss 9.5816(9.6981) | Error 0.5144(0.5022) Steps 0(0.00) | Grad Norm 20.4339(12.6150) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.3810(14.0096) | Bit/dim 4.0830(4.0952) | Xent 1.3510(1.3809) | Loss 9.5372(9.6166) | Error 0.4867(0.5013) Steps 0(0.00) | Grad Norm 10.9279(12.1291) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 13.1801(13.8825) | Bit/dim 4.0693(4.0902) | Xent 1.3687(1.3732) | Loss 9.3244(9.5299) | Error 0.4889(0.4981) Steps 0(0.00) | Grad Norm 7.7294(11.5561) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 13.8535(13.9065) | Bit/dim 4.0932(4.0878) | Xent 1.4271(1.3726) | Loss 9.3816(9.4763) | Error 0.5167(0.4971) Steps 0(0.00) | Grad Norm 24.4921(12.4029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 71.8277, Epoch Time 852.4479(752.3208), Bit/dim 4.0673(best: 4.0881), Xent 1.3710, Loss 4.7528, Error 0.4978(best: 0.4690)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 13.9328(13.9705) | Bit/dim 4.0840(4.0814) | Xent 1.3932(1.3687) | Loss 9.5182(9.8082) | Error 0.4933(0.4945) Steps 0(0.00) | Grad Norm 10.3819(11.8786) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 13.5441(13.9724) | Bit/dim 4.0432(4.0740) | Xent 1.3301(1.3533) | Loss 8.9222(9.6680) | Error 0.4767(0.4896) Steps 0(0.00) | Grad Norm 6.5399(10.6728) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 13.3722(13.9308) | Bit/dim 4.0639(4.0716) | Xent 1.3154(1.3473) | Loss 9.0902(9.5667) | Error 0.4811(0.4873) Steps 0(0.00) | Grad Norm 6.9652(11.0418) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 13.3172(14.0589) | Bit/dim 4.0518(4.0704) | Xent 1.3087(1.3444) | Loss 9.2039(9.5016) | Error 0.4711(0.4878) Steps 0(0.00) | Grad Norm 12.3024(12.0997) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 14.2621(14.0902) | Bit/dim 4.0462(4.0682) | Xent 1.3598(1.3386) | Loss 9.3798(9.4506) | Error 0.4911(0.4865) Steps 0(0.00) | Grad Norm 7.7149(11.8416) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 72.7393, Epoch Time 863.8461(755.6665), Bit/dim 4.0387(best: 4.0673), Xent 1.2409, Loss 4.6591, Error 0.4436(best: 0.4690)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 13.3672(14.0256) | Bit/dim 4.0618(4.0623) | Xent 1.3046(1.3361) | Loss 9.3742(9.7985) | Error 0.4744(0.4843) Steps 0(0.00) | Grad Norm 20.3562(11.9550) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 13.5447(14.0844) | Bit/dim 4.0726(4.0578) | Xent 1.3506(1.3314) | Loss 9.3320(9.6579) | Error 0.5111(0.4821) Steps 0(0.00) | Grad Norm 15.9432(12.4288) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 13.6253(13.9726) | Bit/dim 4.0599(4.0519) | Xent 1.3091(1.3231) | Loss 9.0643(9.5386) | Error 0.4911(0.4801) Steps 0(0.00) | Grad Norm 11.9299(12.2903) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 13.5350(14.1355) | Bit/dim 4.0220(4.0450) | Xent 1.2772(1.3173) | Loss 9.0551(9.4488) | Error 0.4611(0.4784) Steps 0(0.00) | Grad Norm 5.2446(11.5383) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 13.1933(14.1458) | Bit/dim 4.0506(4.0378) | Xent 1.2884(1.3125) | Loss 9.1566(9.3764) | Error 0.4667(0.4766) Steps 0(0.00) | Grad Norm 9.1383(10.8945) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 14.2625(14.2280) | Bit/dim 4.0375(4.0325) | Xent 1.2760(1.3076) | Loss 9.2146(9.3502) | Error 0.4567(0.4724) Steps 0(0.00) | Grad Norm 9.4276(10.7278) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 73.4017, Epoch Time 870.2012(759.1026), Bit/dim 4.0244(best: 4.0387), Xent 1.2471, Loss 4.6480, Error 0.4480(best: 0.4436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 13.3257(14.1654) | Bit/dim 3.9953(4.0281) | Xent 1.3599(1.3036) | Loss 9.2513(9.6839) | Error 0.4878(0.4709) Steps 0(0.00) | Grad Norm 12.3215(11.4104) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.1007(14.2068) | Bit/dim 4.0218(4.0262) | Xent 1.3751(1.3107) | Loss 9.3255(9.5509) | Error 0.4789(0.4718) Steps 0(0.00) | Grad Norm 11.3715(12.4750) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 12.7136(14.2337) | Bit/dim 4.0323(4.0232) | Xent 1.3567(1.3144) | Loss 9.0535(9.4712) | Error 0.4856(0.4729) Steps 0(0.00) | Grad Norm 15.2202(12.2729) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 13.9396(14.1104) | Bit/dim 4.0525(4.0229) | Xent 1.4521(1.3137) | Loss 9.5066(9.4017) | Error 0.5100(0.4751) Steps 0(0.00) | Grad Norm 24.3862(12.2209) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 13.7713(14.0748) | Bit/dim 3.9943(4.0195) | Xent 1.2343(1.3075) | Loss 8.9917(9.3491) | Error 0.4578(0.4753) Steps 0(0.00) | Grad Norm 10.0659(12.4856) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 70.1197, Epoch Time 859.9564(762.1282), Bit/dim 4.0066(best: 4.0244), Xent 1.2859, Loss 4.6495, Error 0.4632(best: 0.4436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.2978(14.0538) | Bit/dim 4.0195(4.0153) | Xent 1.3184(1.3057) | Loss 9.2966(9.7240) | Error 0.4767(0.4755) Steps 0(0.00) | Grad Norm 12.5359(12.7048) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 13.9840(14.0966) | Bit/dim 3.9993(4.0106) | Xent 1.2975(1.2972) | Loss 9.1939(9.5762) | Error 0.4589(0.4704) Steps 0(0.00) | Grad Norm 8.4989(11.8539) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 14.9215(14.0200) | Bit/dim 4.0003(4.0064) | Xent 1.2891(1.2857) | Loss 9.1665(9.4513) | Error 0.4578(0.4654) Steps 0(0.00) | Grad Norm 12.1159(11.3989) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 15.2868(14.1982) | Bit/dim 4.0028(4.0031) | Xent 1.3551(1.2872) | Loss 9.2690(9.3668) | Error 0.4911(0.4680) Steps 0(0.00) | Grad Norm 21.7787(12.2964) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 14.7689(14.1956) | Bit/dim 4.0360(4.0016) | Xent 1.2561(1.2852) | Loss 9.1502(9.3061) | Error 0.4644(0.4649) Steps 0(0.00) | Grad Norm 19.5045(12.6619) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 13.9009(14.2803) | Bit/dim 4.0081(3.9997) | Xent 1.3017(1.2810) | Loss 9.2201(9.2641) | Error 0.4667(0.4634) Steps 0(0.00) | Grad Norm 14.0874(12.1862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 72.1142, Epoch Time 872.5405(765.4406), Bit/dim 3.9809(best: 4.0066), Xent 1.2030, Loss 4.5823, Error 0.4407(best: 0.4436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 14.4661(14.2780) | Bit/dim 3.9751(3.9914) | Xent 1.2487(1.2741) | Loss 9.2962(9.6036) | Error 0.4600(0.4614) Steps 0(0.00) | Grad Norm 9.4504(11.4173) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 13.8257(14.2929) | Bit/dim 3.9830(3.9913) | Xent 1.2531(1.2694) | Loss 9.0394(9.4694) | Error 0.4478(0.4601) Steps 0(0.00) | Grad Norm 17.5832(10.8595) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 13.1479(14.1905) | Bit/dim 3.9697(3.9871) | Xent 1.1720(1.2666) | Loss 8.9351(9.3731) | Error 0.4233(0.4586) Steps 0(0.00) | Grad Norm 10.4981(11.0860) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 13.0331(14.0781) | Bit/dim 3.9708(3.9861) | Xent 1.2609(1.2651) | Loss 9.2079(9.2989) | Error 0.4522(0.4582) Steps 0(0.00) | Grad Norm 17.2525(11.9516) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 14.5820(14.1221) | Bit/dim 3.9823(3.9822) | Xent 1.2325(1.2578) | Loss 9.1903(9.2463) | Error 0.4456(0.4573) Steps 0(0.00) | Grad Norm 13.2385(12.1122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 71.8084, Epoch Time 864.3606(768.4082), Bit/dim 3.9786(best: 3.9809), Xent 1.1804, Loss 4.5688, Error 0.4238(best: 0.4407)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.1355(14.1113) | Bit/dim 3.9584(3.9809) | Xent 1.2415(1.2557) | Loss 8.8592(9.6556) | Error 0.4567(0.4543) Steps 0(0.00) | Grad Norm 15.5317(12.8033) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 13.9352(14.0297) | Bit/dim 3.9596(3.9799) | Xent 1.2051(1.2497) | Loss 9.0885(9.5051) | Error 0.4322(0.4511) Steps 0(0.00) | Grad Norm 10.3196(12.0896) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.1263(14.0669) | Bit/dim 3.9646(3.9745) | Xent 1.2280(1.2407) | Loss 8.9027(9.3757) | Error 0.4267(0.4477) Steps 0(0.00) | Grad Norm 11.3380(11.5317) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 13.2203(14.1368) | Bit/dim 3.9628(3.9704) | Xent 1.2181(1.2451) | Loss 8.9832(9.3076) | Error 0.4256(0.4482) Steps 0(0.00) | Grad Norm 4.9664(11.9828) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.1273(14.0575) | Bit/dim 3.9503(3.9691) | Xent 1.2445(1.2463) | Loss 8.7883(9.2109) | Error 0.4633(0.4495) Steps 0(0.00) | Grad Norm 4.7989(11.2208) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.9244(14.1003) | Bit/dim 3.9570(3.9657) | Xent 1.2665(1.2511) | Loss 8.9925(9.1761) | Error 0.4700(0.4514) Steps 0(0.00) | Grad Norm 14.2693(11.6495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 71.8696, Epoch Time 860.3536(771.1665), Bit/dim 3.9535(best: 3.9786), Xent 1.1640, Loss 4.5356, Error 0.4176(best: 0.4238)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 13.4879(13.9967) | Bit/dim 3.9234(3.9631) | Xent 1.2216(1.2443) | Loss 8.7713(9.4931) | Error 0.4578(0.4499) Steps 0(0.00) | Grad Norm 13.9953(11.9288) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 13.7041(13.9887) | Bit/dim 3.9257(3.9587) | Xent 1.2308(1.2396) | Loss 8.9000(9.3648) | Error 0.4478(0.4467) Steps 0(0.00) | Grad Norm 18.7059(11.6182) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.1562(13.9794) | Bit/dim 3.9371(3.9574) | Xent 1.2377(1.2413) | Loss 8.9367(9.2495) | Error 0.4367(0.4480) Steps 0(0.00) | Grad Norm 12.7433(11.8645) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 13.6963(14.0079) | Bit/dim 3.9623(3.9549) | Xent 1.1739(1.2329) | Loss 8.9313(9.1917) | Error 0.4233(0.4443) Steps 0(0.00) | Grad Norm 13.7877(11.6739) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 15.2722(14.0471) | Bit/dim 3.9026(3.9502) | Xent 1.2155(1.2236) | Loss 8.9801(9.1214) | Error 0.4322(0.4417) Steps 0(0.00) | Grad Norm 6.4636(11.2192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 72.3020, Epoch Time 856.6430(773.7308), Bit/dim 3.9338(best: 3.9535), Xent 1.1653, Loss 4.5164, Error 0.4208(best: 0.4176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 13.5801(13.9541) | Bit/dim 3.9505(3.9454) | Xent 1.2299(1.2239) | Loss 9.0132(9.5236) | Error 0.4478(0.4423) Steps 0(0.00) | Grad Norm 17.3723(11.5862) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 13.2106(13.9185) | Bit/dim 3.9489(3.9440) | Xent 1.2357(1.2160) | Loss 8.9195(9.3939) | Error 0.4300(0.4387) Steps 0(0.00) | Grad Norm 15.2416(11.2482) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 15.3435(13.9236) | Bit/dim 3.9180(3.9398) | Xent 1.2227(1.2114) | Loss 8.8504(9.2680) | Error 0.4344(0.4364) Steps 0(0.00) | Grad Norm 10.9289(11.1591) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 13.5528(13.8355) | Bit/dim 3.9492(3.9407) | Xent 1.1723(1.2098) | Loss 8.9262(9.1938) | Error 0.4311(0.4361) Steps 0(0.00) | Grad Norm 19.4714(11.5398) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 14.1861(13.9188) | Bit/dim 3.9086(3.9376) | Xent 1.2684(1.2178) | Loss 9.0961(9.1449) | Error 0.4578(0.4390) Steps 0(0.00) | Grad Norm 7.1447(11.0293) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 14.2308(13.9398) | Bit/dim 3.9427(3.9362) | Xent 1.2326(1.2202) | Loss 8.7873(9.0813) | Error 0.4378(0.4391) Steps 0(0.00) | Grad Norm 8.0609(11.3644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 71.4237, Epoch Time 850.0381(776.0200), Bit/dim 3.9330(best: 3.9338), Xent 1.1708, Loss 4.5184, Error 0.4207(best: 0.4176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 13.5934(13.9977) | Bit/dim 3.9557(3.9339) | Xent 1.2175(1.2104) | Loss 8.8078(9.4243) | Error 0.4433(0.4353) Steps 0(0.00) | Grad Norm 21.0846(11.5307) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.3499(14.1754) | Bit/dim 3.9425(3.9324) | Xent 1.1329(1.2042) | Loss 8.9629(9.3169) | Error 0.4122(0.4321) Steps 0(0.00) | Grad Norm 7.5566(11.6199) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.3524(14.2056) | Bit/dim 3.8886(3.9291) | Xent 1.2094(1.2065) | Loss 9.0394(9.2348) | Error 0.4267(0.4338) Steps 0(0.00) | Grad Norm 15.5790(11.8900) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 13.5820(14.2260) | Bit/dim 3.9266(3.9270) | Xent 1.2315(1.2110) | Loss 9.0070(9.1449) | Error 0.4467(0.4358) Steps 0(0.00) | Grad Norm 9.2354(12.0653) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 12.8331(14.1008) | Bit/dim 3.9358(3.9270) | Xent 1.2148(1.2062) | Loss 8.7942(9.0830) | Error 0.4333(0.4342) Steps 0(0.00) | Grad Norm 11.7837(12.5083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 72.2630, Epoch Time 867.9366(778.7775), Bit/dim 3.9189(best: 3.9330), Xent 1.1280, Loss 4.4829, Error 0.4058(best: 0.4176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 13.3161(14.1251) | Bit/dim 3.9197(3.9244) | Xent 1.1649(1.2005) | Loss 8.8697(9.4816) | Error 0.4156(0.4316) Steps 0(0.00) | Grad Norm 11.2841(11.7035) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.0618(14.0881) | Bit/dim 3.8754(3.9238) | Xent 1.1350(1.1988) | Loss 8.8501(9.3409) | Error 0.4089(0.4318) Steps 0(0.00) | Grad Norm 10.9916(11.5417) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 14.6617(14.0410) | Bit/dim 3.8908(3.9172) | Xent 1.1842(1.1877) | Loss 9.0182(9.2230) | Error 0.4456(0.4267) Steps 0(0.00) | Grad Norm 6.9275(10.8322) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 14.8899(14.1117) | Bit/dim 3.9105(3.9158) | Xent 1.2384(1.1893) | Loss 8.7789(9.1406) | Error 0.4433(0.4275) Steps 0(0.00) | Grad Norm 7.3166(10.9907) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 14.3366(14.1000) | Bit/dim 3.8998(3.9105) | Xent 1.1902(1.1831) | Loss 8.8480(9.0617) | Error 0.4211(0.4252) Steps 0(0.00) | Grad Norm 10.8229(10.7036) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 13.7769(14.1517) | Bit/dim 3.9397(3.9093) | Xent 1.1649(1.1811) | Loss 8.7911(8.9979) | Error 0.4178(0.4248) Steps 0(0.00) | Grad Norm 6.7496(9.7572) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 72.6987, Epoch Time 867.8932(781.4510), Bit/dim 3.9057(best: 3.9189), Xent 1.0934, Loss 4.4525, Error 0.3901(best: 0.4058)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 13.3514(14.1337) | Bit/dim 3.9320(3.9079) | Xent 1.1171(1.1671) | Loss 8.9077(9.3573) | Error 0.3900(0.4180) Steps 0(0.00) | Grad Norm 13.4605(10.1276) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 13.9139(14.2003) | Bit/dim 3.8864(3.9080) | Xent 1.1965(1.1639) | Loss 8.8319(9.2378) | Error 0.4478(0.4171) Steps 0(0.00) | Grad Norm 15.1009(10.7741) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 13.8834(14.1810) | Bit/dim 3.9209(3.9048) | Xent 1.1214(1.1590) | Loss 8.9067(9.1362) | Error 0.3911(0.4134) Steps 0(0.00) | Grad Norm 10.4311(10.5696) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 14.3575(14.1713) | Bit/dim 3.8839(3.9013) | Xent 1.1394(1.1589) | Loss 8.8882(9.0648) | Error 0.4367(0.4164) Steps 0(0.00) | Grad Norm 10.6194(10.5423) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 13.1367(14.1132) | Bit/dim 3.8960(3.8994) | Xent 1.1576(1.1567) | Loss 8.8271(9.0036) | Error 0.4278(0.4163) Steps 0(0.00) | Grad Norm 5.3717(10.5496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 73.1186, Epoch Time 865.5413(783.9737), Bit/dim 3.8991(best: 3.9057), Xent 1.1082, Loss 4.4532, Error 0.3943(best: 0.3901)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.4244(14.0197) | Bit/dim 3.9070(3.8969) | Xent 1.1507(1.1553) | Loss 8.8052(9.4184) | Error 0.4156(0.4150) Steps 0(0.00) | Grad Norm 12.9917(10.4783) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 14.4961(14.1457) | Bit/dim 3.8952(3.8957) | Xent 1.1192(1.1502) | Loss 8.9369(9.2660) | Error 0.4078(0.4139) Steps 0(0.00) | Grad Norm 11.8129(10.9508) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 12.8463(14.1173) | Bit/dim 3.8932(3.8942) | Xent 1.2136(1.1541) | Loss 8.3711(9.1277) | Error 0.4400(0.4159) Steps 0(0.00) | Grad Norm 13.5615(11.6277) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 13.5211(14.0670) | Bit/dim 3.8753(3.8931) | Xent 1.1693(1.1503) | Loss 8.7018(9.0437) | Error 0.4233(0.4141) Steps 0(0.00) | Grad Norm 13.9252(11.5397) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 14.6081(14.1317) | Bit/dim 3.8950(3.8933) | Xent 1.0867(1.1583) | Loss 8.8841(9.0090) | Error 0.3833(0.4158) Steps 0(0.00) | Grad Norm 13.4212(12.2186) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 14.0602(14.0864) | Bit/dim 3.8827(3.8924) | Xent 1.0235(1.1548) | Loss 8.6226(8.9494) | Error 0.3833(0.4148) Steps 0(0.00) | Grad Norm 6.2622(11.4278) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 71.1941, Epoch Time 863.9954(786.3744), Bit/dim 3.8885(best: 3.8991), Xent 1.0808, Loss 4.4288, Error 0.3872(best: 0.3901)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.0984(14.0696) | Bit/dim 3.8688(3.8933) | Xent 1.1706(1.1530) | Loss 8.9246(9.3176) | Error 0.4144(0.4130) Steps 0(0.00) | Grad Norm 17.6378(11.6725) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 13.3829(14.0146) | Bit/dim 3.8755(3.8898) | Xent 1.0941(1.1465) | Loss 8.7193(9.1948) | Error 0.4011(0.4108) Steps 0(0.00) | Grad Norm 14.2879(12.0831) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 14.0222(14.0348) | Bit/dim 3.8812(3.8867) | Xent 1.0960(1.1407) | Loss 8.9987(9.1033) | Error 0.3900(0.4100) Steps 0(0.00) | Grad Norm 14.5915(11.9895) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.1847(14.0592) | Bit/dim 3.8861(3.8834) | Xent 1.1408(1.1345) | Loss 8.9190(9.0040) | Error 0.4078(0.4071) Steps 0(0.00) | Grad Norm 15.2833(12.0547) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 13.8104(14.1681) | Bit/dim 3.9069(3.8878) | Xent 1.1651(1.1322) | Loss 8.8397(8.9554) | Error 0.4222(0.4073) Steps 0(0.00) | Grad Norm 11.3274(11.9032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 71.4149, Epoch Time 862.1915(788.6489), Bit/dim 3.8822(best: 3.8885), Xent 1.0577, Loss 4.4110, Error 0.3735(best: 0.3872)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 14.2201(14.0799) | Bit/dim 3.8795(3.8862) | Xent 1.1370(1.1327) | Loss 8.8914(9.3738) | Error 0.4211(0.4078) Steps 0(0.00) | Grad Norm 11.7839(11.4453) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 13.8265(14.0787) | Bit/dim 3.8933(3.8837) | Xent 1.0936(1.1252) | Loss 8.8490(9.2343) | Error 0.4100(0.4065) Steps 0(0.00) | Grad Norm 9.2255(11.1199) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 13.8936(13.9806) | Bit/dim 3.9278(3.8811) | Xent 1.1103(1.1199) | Loss 8.9884(9.1065) | Error 0.4022(0.4038) Steps 0(0.00) | Grad Norm 18.3100(11.3575) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 12.5991(13.9909) | Bit/dim 3.8922(3.8808) | Xent 1.0051(1.1093) | Loss 8.8191(9.0246) | Error 0.3722(0.4000) Steps 0(0.00) | Grad Norm 9.0819(10.7996) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.7709(14.0874) | Bit/dim 3.8889(3.8800) | Xent 1.1861(1.1119) | Loss 8.9128(8.9778) | Error 0.4056(0.3999) Steps 0(0.00) | Grad Norm 17.6437(11.3762) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 13.1314(14.0136) | Bit/dim 3.8288(3.8749) | Xent 1.0907(1.1126) | Loss 8.6598(8.9329) | Error 0.4033(0.3996) Steps 0(0.00) | Grad Norm 10.9376(11.4001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 72.9772, Epoch Time 858.8721(790.7556), Bit/dim 3.8744(best: 3.8822), Xent 1.0507, Loss 4.3997, Error 0.3761(best: 0.3735)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 14.6074(14.0027) | Bit/dim 3.8389(3.8745) | Xent 1.0407(1.1060) | Loss 8.7947(9.2684) | Error 0.3733(0.3975) Steps 0(0.00) | Grad Norm 7.9480(10.9486) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 14.5316(14.0366) | Bit/dim 3.8683(3.8732) | Xent 1.1070(1.0947) | Loss 8.8247(9.1257) | Error 0.4011(0.3942) Steps 0(0.00) | Grad Norm 7.3287(10.5923) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 13.6324(14.0003) | Bit/dim 3.8873(3.8732) | Xent 1.0974(1.0988) | Loss 8.7372(9.0288) | Error 0.3889(0.3951) Steps 0(0.00) | Grad Norm 6.8657(12.0293) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 13.9914(14.0578) | Bit/dim 3.9067(3.8712) | Xent 1.1240(1.1079) | Loss 8.9680(8.9785) | Error 0.4044(0.3975) Steps 0(0.00) | Grad Norm 10.7811(12.3189) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 14.7660(14.1733) | Bit/dim 3.8463(3.8723) | Xent 1.1290(1.1132) | Loss 8.7688(8.9429) | Error 0.4100(0.3985) Steps 0(0.00) | Grad Norm 10.3939(12.6070) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 72.1005, Epoch Time 869.4410(793.1161), Bit/dim 3.8734(best: 3.8744), Xent 1.0502, Loss 4.3985, Error 0.3722(best: 0.3735)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 13.7548(14.2014) | Bit/dim 3.8319(3.8684) | Xent 1.0671(1.1125) | Loss 8.5409(9.3144) | Error 0.3800(0.3983) Steps 0(0.00) | Grad Norm 9.5685(12.0812) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 13.9990(14.4511) | Bit/dim 3.8406(3.8664) | Xent 0.9984(1.0998) | Loss 8.4886(9.1728) | Error 0.3500(0.3927) Steps 0(0.00) | Grad Norm 9.5122(11.4825) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 13.5027(14.3873) | Bit/dim 3.8728(3.8638) | Xent 1.0415(1.0824) | Loss 8.7151(9.0460) | Error 0.3644(0.3860) Steps 0(0.00) | Grad Norm 6.3987(10.3878) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 12.8261(14.3435) | Bit/dim 3.8476(3.8615) | Xent 1.0603(1.0805) | Loss 8.7687(8.9659) | Error 0.3678(0.3848) Steps 0(0.00) | Grad Norm 10.2546(10.4603) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 13.3090(14.3635) | Bit/dim 3.8912(3.8603) | Xent 1.0882(1.0810) | Loss 8.7616(8.9222) | Error 0.3989(0.3848) Steps 0(0.00) | Grad Norm 12.7966(10.9557) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 14.4886(14.4519) | Bit/dim 3.8948(3.8623) | Xent 1.0725(1.0869) | Loss 8.7155(8.8738) | Error 0.4056(0.3888) Steps 0(0.00) | Grad Norm 7.1083(12.1098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 72.3604, Epoch Time 885.0483(795.8741), Bit/dim 3.8634(best: 3.8734), Xent 1.0874, Loss 4.4071, Error 0.3851(best: 0.3722)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 13.6288(14.3018) | Bit/dim 3.8825(3.8614) | Xent 1.0546(1.0859) | Loss 8.7504(9.2277) | Error 0.3889(0.3895) Steps 0(0.00) | Grad Norm 12.6258(12.1220) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 14.4117(14.3867) | Bit/dim 3.8545(3.8593) | Xent 1.0672(1.0822) | Loss 8.8496(9.1077) | Error 0.3844(0.3890) Steps 0(0.00) | Grad Norm 9.1583(11.2414) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.1305(14.3033) | Bit/dim 3.8337(3.8541) | Xent 0.9648(1.0777) | Loss 8.6548(8.9959) | Error 0.3289(0.3880) Steps 0(0.00) | Grad Norm 4.8141(10.4196) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 13.2268(14.2729) | Bit/dim 3.8403(3.8541) | Xent 1.0094(1.0670) | Loss 8.4987(8.9208) | Error 0.3633(0.3839) Steps 0(0.00) | Grad Norm 12.8795(10.4595) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 13.8505(14.3114) | Bit/dim 3.8688(3.8543) | Xent 1.0364(1.0742) | Loss 8.5806(8.8679) | Error 0.3833(0.3862) Steps 0(0.00) | Grad Norm 7.1915(12.0640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 72.9667, Epoch Time 871.3801(798.1393), Bit/dim 3.8463(best: 3.8634), Xent 1.0282, Loss 4.3604, Error 0.3653(best: 0.3722)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.3339(14.3505) | Bit/dim 3.8418(3.8546) | Xent 0.9577(1.0649) | Loss 8.5975(9.2880) | Error 0.3522(0.3832) Steps 0(0.00) | Grad Norm 5.0821(10.9833) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 12.9626(14.3505) | Bit/dim 3.8331(3.8515) | Xent 1.0646(1.0524) | Loss 8.6993(9.1299) | Error 0.3856(0.3780) Steps 0(0.00) | Grad Norm 10.7915(10.0179) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.7236(14.3316) | Bit/dim 3.8512(3.8512) | Xent 1.1166(1.0574) | Loss 8.8122(9.0179) | Error 0.3900(0.3790) Steps 0(0.00) | Grad Norm 7.7153(10.6204) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 14.3328(14.3648) | Bit/dim 3.8348(3.8449) | Xent 1.0231(1.0501) | Loss 8.5627(8.9260) | Error 0.3611(0.3758) Steps 0(0.00) | Grad Norm 9.1566(10.0810) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.5884(14.3333) | Bit/dim 3.8204(3.8436) | Xent 1.0571(1.0495) | Loss 8.5863(8.8525) | Error 0.3878(0.3755) Steps 0(0.00) | Grad Norm 9.5389(10.7229) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.0918(14.4227) | Bit/dim 3.8446(3.8433) | Xent 1.0340(1.0435) | Loss 8.7452(8.7938) | Error 0.4044(0.3755) Steps 0(0.00) | Grad Norm 6.6229(9.6070) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 73.8087, Epoch Time 883.7356(800.7072), Bit/dim 3.8291(best: 3.8463), Xent 0.9684, Loss 4.3133, Error 0.3396(best: 0.3653)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 13.8118(14.4898) | Bit/dim 3.8376(3.8420) | Xent 1.0869(1.0525) | Loss 8.7647(9.1704) | Error 0.3767(0.3773) Steps 0(0.00) | Grad Norm 11.7676(9.9863) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 13.7284(14.3473) | Bit/dim 3.8502(3.8406) | Xent 1.0396(1.0486) | Loss 8.5937(9.0116) | Error 0.3700(0.3761) Steps 0(0.00) | Grad Norm 7.8847(9.8734) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.0381(14.5493) | Bit/dim 3.8303(3.8381) | Xent 1.0104(1.0372) | Loss 8.6259(8.9284) | Error 0.3644(0.3729) Steps 0(0.00) | Grad Norm 13.1362(9.7766) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 13.8794(14.4456) | Bit/dim 3.8809(3.8379) | Xent 0.9833(1.0370) | Loss 8.6620(8.8437) | Error 0.3578(0.3717) Steps 0(0.00) | Grad Norm 9.5727(10.1306) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 13.9195(14.3867) | Bit/dim 3.8273(3.8354) | Xent 1.0015(1.0358) | Loss 8.5389(8.7819) | Error 0.3511(0.3703) Steps 0(0.00) | Grad Norm 7.1756(10.5312) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 72.2196, Epoch Time 877.5052(803.0111), Bit/dim 3.8439(best: 3.8291), Xent 0.9787, Loss 4.3332, Error 0.3449(best: 0.3396)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 13.3132(14.2207) | Bit/dim 3.8347(3.8347) | Xent 1.0771(1.0383) | Loss 8.5414(9.1854) | Error 0.3833(0.3712) Steps 0(0.00) | Grad Norm 17.1341(11.0685) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 13.8248(14.2419) | Bit/dim 3.8282(3.8335) | Xent 1.0356(1.0227) | Loss 8.6247(9.0230) | Error 0.3678(0.3651) Steps 0(0.00) | Grad Norm 10.9607(10.8627) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 13.4777(14.1535) | Bit/dim 3.7831(3.8319) | Xent 1.0909(1.0221) | Loss 8.5802(8.8973) | Error 0.3911(0.3657) Steps 0(0.00) | Grad Norm 9.6328(10.3239) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 14.8798(14.1073) | Bit/dim 3.7881(3.8324) | Xent 1.0311(1.0268) | Loss 8.6042(8.8355) | Error 0.3778(0.3688) Steps 0(0.00) | Grad Norm 6.7569(10.1965) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 16.0093(14.2115) | Bit/dim 3.8461(3.8334) | Xent 1.0906(1.0267) | Loss 8.6234(8.7845) | Error 0.3822(0.3676) Steps 0(0.00) | Grad Norm 13.6791(10.6228) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.6811(14.2181) | Bit/dim 3.8307(3.8315) | Xent 1.0436(1.0246) | Loss 8.7619(8.7442) | Error 0.3978(0.3678) Steps 0(0.00) | Grad Norm 10.1228(10.4880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 72.0445, Epoch Time 866.7537(804.9234), Bit/dim 3.8305(best: 3.8291), Xent 0.9962, Loss 4.3286, Error 0.3585(best: 0.3396)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 12.9686(14.0954) | Bit/dim 3.8211(3.8319) | Xent 1.0313(1.0237) | Loss 8.5094(9.0785) | Error 0.3678(0.3680) Steps 0(0.00) | Grad Norm 8.6756(10.7146) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 13.9785(14.0584) | Bit/dim 3.8170(3.8283) | Xent 0.9979(1.0165) | Loss 8.4057(8.9485) | Error 0.3511(0.3645) Steps 0(0.00) | Grad Norm 6.1863(10.4473) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 12.5225(14.1260) | Bit/dim 3.8033(3.8265) | Xent 1.0472(1.0184) | Loss 8.3581(8.8535) | Error 0.3733(0.3651) Steps 0(0.00) | Grad Norm 7.7130(10.3435) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 13.6137(14.1302) | Bit/dim 3.8222(3.8272) | Xent 1.0595(1.0189) | Loss 8.6911(8.8017) | Error 0.3767(0.3639) Steps 0(0.00) | Grad Norm 9.9498(10.1803) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 13.7501(14.2021) | Bit/dim 3.8457(3.8276) | Xent 1.0026(1.0211) | Loss 8.6029(8.7671) | Error 0.3500(0.3655) Steps 0(0.00) | Grad Norm 9.8918(11.1609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 74.1984, Epoch Time 865.4811(806.7401), Bit/dim 3.8209(best: 3.8291), Xent 0.9575, Loss 4.2996, Error 0.3363(best: 0.3396)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 14.6424(14.1738) | Bit/dim 3.8152(3.8225) | Xent 0.9777(1.0101) | Loss 8.7854(9.1767) | Error 0.3522(0.3616) Steps 0(0.00) | Grad Norm 7.1322(10.0767) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.8725(14.2179) | Bit/dim 3.8361(3.8234) | Xent 0.9099(0.9963) | Loss 8.7099(9.0253) | Error 0.3311(0.3569) Steps 0(0.00) | Grad Norm 5.4811(9.4339) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.7759(14.2112) | Bit/dim 3.8328(3.8244) | Xent 1.0103(0.9925) | Loss 8.6979(8.8930) | Error 0.3944(0.3570) Steps 0(0.00) | Grad Norm 11.5989(9.3376) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.0383(14.2416) | Bit/dim 3.8586(3.8256) | Xent 0.9878(0.9907) | Loss 8.7586(8.8185) | Error 0.3600(0.3560) Steps 0(0.00) | Grad Norm 7.9027(9.1849) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 16.2516(14.3825) | Bit/dim 3.7846(3.8247) | Xent 0.9811(0.9881) | Loss 8.4775(8.7510) | Error 0.3589(0.3546) Steps 0(0.00) | Grad Norm 12.8491(10.0437) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 13.5884(14.3254) | Bit/dim 3.7867(3.8215) | Xent 0.9709(0.9937) | Loss 8.5081(8.6893) | Error 0.3478(0.3553) Steps 0(0.00) | Grad Norm 7.8691(10.5630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 73.6520, Epoch Time 878.9687(808.9070), Bit/dim 3.8254(best: 3.8209), Xent 0.9675, Loss 4.3091, Error 0.3388(best: 0.3363)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 14.3114(14.3100) | Bit/dim 3.8185(3.8197) | Xent 0.9661(0.9902) | Loss 8.6445(9.0654) | Error 0.3544(0.3535) Steps 0(0.00) | Grad Norm 12.0903(10.3407) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 12.8080(14.2203) | Bit/dim 3.8324(3.8208) | Xent 0.9266(0.9885) | Loss 8.3750(8.9331) | Error 0.3311(0.3529) Steps 0(0.00) | Grad Norm 10.7950(11.0016) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 14.0531(14.3843) | Bit/dim 3.7638(3.8195) | Xent 1.0009(0.9926) | Loss 8.7195(8.8399) | Error 0.3422(0.3531) Steps 0(0.00) | Grad Norm 10.5586(10.8083) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.5854(14.3527) | Bit/dim 3.8114(3.8158) | Xent 0.9430(0.9971) | Loss 8.7170(8.7733) | Error 0.3267(0.3534) Steps 0(0.00) | Grad Norm 7.1683(10.7124) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 14.6233(14.4027) | Bit/dim 3.8021(3.8135) | Xent 0.8889(0.9929) | Loss 8.3991(8.7160) | Error 0.3111(0.3529) Steps 0(0.00) | Grad Norm 5.5428(9.9773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 74.6745, Epoch Time 885.1552(811.1944), Bit/dim 3.8038(best: 3.8209), Xent 0.9194, Loss 4.2635, Error 0.3287(best: 0.3363)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 13.5745(14.5460) | Bit/dim 3.8010(3.8126) | Xent 0.9579(0.9849) | Loss 8.4322(9.1382) | Error 0.3467(0.3515) Steps 0(0.00) | Grad Norm 8.1725(9.9457) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 13.1607(14.4708) | Bit/dim 3.7969(3.8088) | Xent 0.9362(0.9778) | Loss 8.5836(8.9611) | Error 0.3344(0.3483) Steps 0(0.00) | Grad Norm 6.0912(9.6947) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.9860(14.3878) | Bit/dim 3.8194(3.8077) | Xent 1.0655(0.9771) | Loss 8.6644(8.8570) | Error 0.3789(0.3479) Steps 0(0.00) | Grad Norm 12.8667(9.6369) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 16.7533(14.5593) | Bit/dim 3.8004(3.8052) | Xent 0.9639(0.9810) | Loss 8.5168(8.7811) | Error 0.3578(0.3485) Steps 0(0.00) | Grad Norm 8.7164(9.8295) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 14.7553(14.4788) | Bit/dim 3.7853(3.8080) | Xent 0.9775(0.9823) | Loss 8.4871(8.7359) | Error 0.3467(0.3503) Steps 0(0.00) | Grad Norm 10.3625(10.0101) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 14.0984(14.4567) | Bit/dim 3.8193(3.8085) | Xent 0.9835(0.9707) | Loss 8.5272(8.6670) | Error 0.3467(0.3455) Steps 0(0.00) | Grad Norm 8.7633(9.5543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 73.4374, Epoch Time 884.5748(813.3958), Bit/dim 3.8047(best: 3.8038), Xent 0.9520, Loss 4.2807, Error 0.3335(best: 0.3287)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 14.5769(14.3888) | Bit/dim 3.8250(3.8096) | Xent 1.0621(0.9718) | Loss 8.6317(9.0331) | Error 0.3822(0.3456) Steps 0(0.00) | Grad Norm 14.4851(10.3277) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.4932(14.3378) | Bit/dim 3.8096(3.8073) | Xent 0.8979(0.9601) | Loss 8.5915(8.8853) | Error 0.3144(0.3413) Steps 0(0.00) | Grad Norm 6.8053(9.9390) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 13.0938(14.3084) | Bit/dim 3.7917(3.8053) | Xent 0.9527(0.9534) | Loss 8.2019(8.7871) | Error 0.3478(0.3383) Steps 0(0.00) | Grad Norm 6.4816(9.5329) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 13.9871(14.2791) | Bit/dim 3.7932(3.8024) | Xent 0.9800(0.9541) | Loss 8.3746(8.6982) | Error 0.3556(0.3395) Steps 0(0.00) | Grad Norm 8.7097(9.7884) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 13.6563(14.3027) | Bit/dim 3.8076(3.7997) | Xent 0.9984(0.9556) | Loss 8.2636(8.6402) | Error 0.3489(0.3396) Steps 0(0.00) | Grad Norm 12.1609(10.4130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 73.6190, Epoch Time 875.2493(815.2514), Bit/dim 3.7982(best: 3.8038), Xent 0.9222, Loss 4.2593, Error 0.3226(best: 0.3287)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 16.0073(14.3703) | Bit/dim 3.7856(3.7995) | Xent 0.8718(0.9495) | Loss 8.4750(9.0601) | Error 0.3167(0.3377) Steps 0(0.00) | Grad Norm 4.6684(10.3170) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 14.6320(14.3847) | Bit/dim 3.7816(3.7992) | Xent 0.9307(0.9494) | Loss 8.4872(8.9292) | Error 0.3122(0.3381) Steps 0(0.00) | Grad Norm 8.6898(10.5105) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 14.6928(14.5911) | Bit/dim 3.8022(3.8010) | Xent 1.0083(0.9530) | Loss 8.7159(8.8404) | Error 0.3544(0.3389) Steps 0(0.00) | Grad Norm 15.8178(10.4300) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 13.6017(14.4198) | Bit/dim 3.8028(3.8044) | Xent 0.9782(0.9580) | Loss 8.2948(8.7525) | Error 0.3467(0.3400) Steps 0(0.00) | Grad Norm 10.2321(10.9792) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 13.6744(14.3704) | Bit/dim 3.8190(3.8061) | Xent 1.0283(0.9678) | Loss 8.5926(8.7081) | Error 0.3822(0.3436) Steps 0(0.00) | Grad Norm 7.3831(10.7767) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.7624(14.3243) | Bit/dim 3.8012(3.8034) | Xent 0.9583(0.9697) | Loss 8.6377(8.6576) | Error 0.3400(0.3433) Steps 0(0.00) | Grad Norm 11.9154(10.3032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 74.1713, Epoch Time 882.9147(817.2813), Bit/dim 3.8025(best: 3.7982), Xent 0.9160, Loss 4.2605, Error 0.3231(best: 0.3226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 13.9919(14.3198) | Bit/dim 3.7595(3.8015) | Xent 0.9643(0.9661) | Loss 8.5413(9.0199) | Error 0.3433(0.3420) Steps 0(0.00) | Grad Norm 10.3757(10.0476) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 16.3030(14.3742) | Bit/dim 3.8201(3.8013) | Xent 0.8976(0.9507) | Loss 8.6500(8.8777) | Error 0.3178(0.3377) Steps 0(0.00) | Grad Norm 9.3761(9.1137) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 13.1690(14.3296) | Bit/dim 3.7720(3.7975) | Xent 0.8901(0.9387) | Loss 8.1612(8.7774) | Error 0.3222(0.3320) Steps 0(0.00) | Grad Norm 10.9845(8.8359) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 13.9816(14.3226) | Bit/dim 3.7862(3.7938) | Xent 0.9362(0.9321) | Loss 8.5488(8.6925) | Error 0.3200(0.3287) Steps 0(0.00) | Grad Norm 5.1129(8.4131) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 14.4720(14.2982) | Bit/dim 3.7761(3.7941) | Xent 0.9913(0.9408) | Loss 8.5386(8.6575) | Error 0.3522(0.3334) Steps 0(0.00) | Grad Norm 14.2320(9.2800) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 74.9113, Epoch Time 874.8329(819.0079), Bit/dim 3.7905(best: 3.7982), Xent 0.9121, Loss 4.2465, Error 0.3224(best: 0.3226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 13.8646(14.1802) | Bit/dim 3.7584(3.7926) | Xent 0.9727(0.9457) | Loss 8.5393(9.0983) | Error 0.3611(0.3369) Steps 0(0.00) | Grad Norm 8.7923(9.7210) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 13.8723(14.1439) | Bit/dim 3.7741(3.7952) | Xent 0.9465(0.9457) | Loss 8.3067(8.9365) | Error 0.3311(0.3365) Steps 0(0.00) | Grad Norm 8.9385(9.7665) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 13.8962(14.0884) | Bit/dim 3.7949(3.7955) | Xent 0.9116(0.9468) | Loss 8.4250(8.8091) | Error 0.3189(0.3366) Steps 0(0.00) | Grad Norm 9.4701(9.9145) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 13.5812(14.1079) | Bit/dim 3.8113(3.7922) | Xent 0.8685(0.9350) | Loss 8.4984(8.7280) | Error 0.3100(0.3337) Steps 0(0.00) | Grad Norm 5.8710(8.9526) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.4665(14.2432) | Bit/dim 3.8049(3.7903) | Xent 0.9285(0.9321) | Loss 8.6595(8.6648) | Error 0.3278(0.3323) Steps 0(0.00) | Grad Norm 8.9454(8.6863) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 14.0377(14.2003) | Bit/dim 3.7929(3.7875) | Xent 1.0292(0.9375) | Loss 8.4742(8.5993) | Error 0.3478(0.3340) Steps 0(0.00) | Grad Norm 10.9383(9.1630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 73.1162, Epoch Time 867.6886(820.4683), Bit/dim 3.7930(best: 3.7905), Xent 0.9284, Loss 4.2572, Error 0.3300(best: 0.3224)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 19.1251(14.5055) | Bit/dim 3.7827(3.7857) | Xent 0.9034(0.9401) | Loss 8.7067(8.9801) | Error 0.3178(0.3332) Steps 0(0.00) | Grad Norm 9.6047(9.7931) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.0749(14.4417) | Bit/dim 3.8319(3.7907) | Xent 0.9466(0.9323) | Loss 8.5033(8.8496) | Error 0.3289(0.3324) Steps 0(0.00) | Grad Norm 12.0638(10.1096) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 14.2839(14.3852) | Bit/dim 3.7890(3.7889) | Xent 0.8828(0.9269) | Loss 8.4484(8.7468) | Error 0.3200(0.3291) Steps 0(0.00) | Grad Norm 8.0537(9.2585) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 13.5305(14.4024) | Bit/dim 3.7668(3.7878) | Xent 0.8808(0.9203) | Loss 8.5346(8.6781) | Error 0.3144(0.3261) Steps 0(0.00) | Grad Norm 5.4899(8.5677) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 14.2991(14.5196) | Bit/dim 3.7796(3.7876) | Xent 0.9212(0.9205) | Loss 8.3821(8.6450) | Error 0.3356(0.3265) Steps 0(0.00) | Grad Norm 9.1887(8.9540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 74.7121, Epoch Time 895.5432(822.7206), Bit/dim 3.7848(best: 3.7905), Xent 0.8894, Loss 4.2295, Error 0.3123(best: 0.3224)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.0787(14.5563) | Bit/dim 3.7855(3.7868) | Xent 0.8205(0.9125) | Loss 8.3107(9.0106) | Error 0.3111(0.3241) Steps 0(0.00) | Grad Norm 6.0422(9.0462) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 13.1066(14.4932) | Bit/dim 3.7603(3.7816) | Xent 0.8850(0.9109) | Loss 8.4897(8.8448) | Error 0.2989(0.3245) Steps 0(0.00) | Grad Norm 11.4400(9.1889) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 14.8531(14.5981) | Bit/dim 3.7891(3.7815) | Xent 0.9567(0.9111) | Loss 8.5865(8.7519) | Error 0.3456(0.3241) Steps 0(0.00) | Grad Norm 9.5754(9.3456) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 14.2932(14.4180) | Bit/dim 3.7911(3.7829) | Xent 0.8852(0.9146) | Loss 8.4326(8.6660) | Error 0.3167(0.3254) Steps 0(0.00) | Grad Norm 8.9209(9.4747) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 13.4256(14.3205) | Bit/dim 3.7939(3.7809) | Xent 0.8950(0.9118) | Loss 8.6415(8.5897) | Error 0.3244(0.3242) Steps 0(0.00) | Grad Norm 6.5579(9.5479) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 14.2171(14.4139) | Bit/dim 3.7698(3.7802) | Xent 0.9376(0.9204) | Loss 8.5149(8.5784) | Error 0.3533(0.3271) Steps 0(0.00) | Grad Norm 9.6718(9.9461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 74.2538, Epoch Time 882.3160(824.5084), Bit/dim 3.7779(best: 3.7848), Xent 0.9230, Loss 4.2394, Error 0.3272(best: 0.3123)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.2413(14.3051) | Bit/dim 3.7962(3.7812) | Xent 0.9269(0.9131) | Loss 8.5653(8.9251) | Error 0.3244(0.3236) Steps 0(0.00) | Grad Norm 6.1038(9.5193) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.5330(14.2838) | Bit/dim 3.7785(3.7800) | Xent 0.9864(0.9101) | Loss 8.5018(8.7862) | Error 0.3656(0.3238) Steps 0(0.00) | Grad Norm 19.0625(9.5336) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.0419(14.3206) | Bit/dim 3.8173(3.7788) | Xent 0.9297(0.9156) | Loss 8.7339(8.6934) | Error 0.3200(0.3248) Steps 0(0.00) | Grad Norm 10.8220(9.7029) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 14.2009(14.2263) | Bit/dim 3.7938(3.7772) | Xent 0.9384(0.9087) | Loss 8.6718(8.6254) | Error 0.3256(0.3225) Steps 0(0.00) | Grad Norm 10.9468(9.1110) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 13.5765(14.1746) | Bit/dim 3.7545(3.7728) | Xent 0.8946(0.9014) | Loss 8.4161(8.5676) | Error 0.3322(0.3213) Steps 0(0.00) | Grad Norm 10.2427(8.7955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 73.9851, Epoch Time 866.8934(825.7800), Bit/dim 3.7704(best: 3.7779), Xent 0.9023, Loss 4.2215, Error 0.3184(best: 0.3123)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 14.1763(14.2404) | Bit/dim 3.7793(3.7715) | Xent 0.9279(0.9016) | Loss 8.3010(8.9924) | Error 0.3433(0.3212) Steps 0(0.00) | Grad Norm 15.6953(9.0090) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 13.9975(14.3653) | Bit/dim 3.7936(3.7714) | Xent 0.9790(0.9080) | Loss 8.4028(8.8436) | Error 0.3289(0.3229) Steps 0(0.00) | Grad Norm 8.9198(9.2307) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.4841(14.5633) | Bit/dim 3.7632(3.7689) | Xent 0.8313(0.9030) | Loss 8.3661(8.7416) | Error 0.2889(0.3214) Steps 0(0.00) | Grad Norm 6.2885(8.7621) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.2638(14.5739) | Bit/dim 3.7865(3.7705) | Xent 0.8761(0.9016) | Loss 8.6205(8.6633) | Error 0.3000(0.3214) Steps 0(0.00) | Grad Norm 10.9441(8.6441) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.8646(14.7280) | Bit/dim 3.7818(3.7736) | Xent 0.8865(0.8987) | Loss 8.4502(8.6115) | Error 0.3189(0.3201) Steps 0(0.00) | Grad Norm 10.2456(9.0708) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 13.4058(14.6874) | Bit/dim 3.7916(3.7727) | Xent 0.9003(0.9016) | Loss 8.4195(8.5781) | Error 0.3222(0.3204) Steps 0(0.00) | Grad Norm 5.8955(9.0412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 75.1348, Epoch Time 905.9913(828.1863), Bit/dim 3.7749(best: 3.7704), Xent 0.8686, Loss 4.2092, Error 0.3047(best: 0.3123)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 13.9388(14.6303) | Bit/dim 3.7666(3.7752) | Xent 0.9073(0.8975) | Loss 8.4242(8.9459) | Error 0.3067(0.3181) Steps 0(0.00) | Grad Norm 18.0096(9.2480) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 14.2731(14.5047) | Bit/dim 3.7956(3.7737) | Xent 0.8557(0.8960) | Loss 8.5300(8.8085) | Error 0.3011(0.3176) Steps 0(0.00) | Grad Norm 7.6341(9.8844) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 13.4600(14.4132) | Bit/dim 3.7829(3.7745) | Xent 0.8981(0.8933) | Loss 8.3577(8.6973) | Error 0.3244(0.3174) Steps 0(0.00) | Grad Norm 15.0553(10.1556) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 13.7946(14.3828) | Bit/dim 3.7748(3.7716) | Xent 0.9380(0.9098) | Loss 8.3571(8.6322) | Error 0.3467(0.3233) Steps 0(0.00) | Grad Norm 16.8487(10.8241) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 14.4976(14.4651) | Bit/dim 3.7500(3.7717) | Xent 0.8433(0.9098) | Loss 8.3020(8.5814) | Error 0.2989(0.3214) Steps 0(0.00) | Grad Norm 6.0456(10.8053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 73.1146, Epoch Time 878.3720(829.6919), Bit/dim 3.7748(best: 3.7704), Xent 0.8757, Loss 4.2127, Error 0.3067(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 14.0869(14.3900) | Bit/dim 3.7699(3.7721) | Xent 0.8773(0.8966) | Loss 8.2904(8.9876) | Error 0.3100(0.3163) Steps 0(0.00) | Grad Norm 8.5030(9.9370) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 14.9932(14.3741) | Bit/dim 3.7901(3.7719) | Xent 0.8059(0.8932) | Loss 8.2291(8.8294) | Error 0.3033(0.3180) Steps 0(0.00) | Grad Norm 7.9563(10.0530) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 13.6179(14.3602) | Bit/dim 3.7782(3.7710) | Xent 0.9035(0.8965) | Loss 8.4412(8.7322) | Error 0.3144(0.3189) Steps 0(0.00) | Grad Norm 8.3074(10.1575) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 14.8630(14.4009) | Bit/dim 3.7438(3.7682) | Xent 0.8581(0.8911) | Loss 8.4544(8.6487) | Error 0.3033(0.3183) Steps 0(0.00) | Grad Norm 4.9715(9.5092) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.8691(14.4959) | Bit/dim 3.7966(3.7696) | Xent 0.8676(0.8846) | Loss 8.5438(8.5831) | Error 0.3122(0.3167) Steps 0(0.00) | Grad Norm 7.5156(8.4751) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.1124(14.6103) | Bit/dim 3.7894(3.7691) | Xent 0.9019(0.8817) | Loss 8.4292(8.5314) | Error 0.3122(0.3141) Steps 0(0.00) | Grad Norm 6.5296(8.1192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 75.2620, Epoch Time 890.7835(831.5246), Bit/dim 3.7702(best: 3.7704), Xent 0.8676, Loss 4.2041, Error 0.3058(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 16.5811(14.7877) | Bit/dim 3.7907(3.7691) | Xent 0.7965(0.8722) | Loss 8.4213(8.8912) | Error 0.2722(0.3097) Steps 0(0.00) | Grad Norm 8.8929(8.2273) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.2220(14.9531) | Bit/dim 3.7562(3.7692) | Xent 0.8334(0.8756) | Loss 8.4320(8.7610) | Error 0.3044(0.3109) Steps 0(0.00) | Grad Norm 9.2324(8.3705) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 14.3373(14.9673) | Bit/dim 3.7762(3.7682) | Xent 0.8791(0.8769) | Loss 8.5382(8.6699) | Error 0.2911(0.3114) Steps 0(0.00) | Grad Norm 9.5200(9.2022) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.4761(14.9158) | Bit/dim 3.7645(3.7711) | Xent 0.8743(0.8791) | Loss 8.3974(8.6095) | Error 0.3211(0.3114) Steps 0(0.00) | Grad Norm 9.3929(9.7078) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.8463(14.8629) | Bit/dim 3.7738(3.7691) | Xent 0.8330(0.8805) | Loss 8.4011(8.5491) | Error 0.3078(0.3122) Steps 0(0.00) | Grad Norm 6.6300(9.5004) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 74.1486, Epoch Time 914.8506(834.0244), Bit/dim 3.7639(best: 3.7702), Xent 0.9042, Loss 4.2160, Error 0.3222(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 14.9811(14.8351) | Bit/dim 3.7603(3.7661) | Xent 0.8652(0.8953) | Loss 8.2530(8.9765) | Error 0.3144(0.3149) Steps 0(0.00) | Grad Norm 13.9936(10.1737) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 14.0824(14.8077) | Bit/dim 3.7574(3.7686) | Xent 0.9119(0.8881) | Loss 8.4197(8.8217) | Error 0.3278(0.3144) Steps 0(0.00) | Grad Norm 7.4416(9.9751) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.0764(14.9317) | Bit/dim 3.7295(3.7665) | Xent 0.8702(0.8793) | Loss 8.3615(8.6990) | Error 0.3078(0.3119) Steps 0(0.00) | Grad Norm 6.2304(9.4621) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.0926(15.0048) | Bit/dim 3.7489(3.7644) | Xent 0.9006(0.8770) | Loss 8.4656(8.6184) | Error 0.3244(0.3118) Steps 0(0.00) | Grad Norm 8.2191(9.0152) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.1809(15.0713) | Bit/dim 3.7449(3.7619) | Xent 0.9598(0.8730) | Loss 8.5212(8.5615) | Error 0.3467(0.3106) Steps 0(0.00) | Grad Norm 11.1559(8.8351) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 14.4282(15.0439) | Bit/dim 3.7291(3.7587) | Xent 0.8732(0.8716) | Loss 8.3695(8.5159) | Error 0.3356(0.3100) Steps 0(0.00) | Grad Norm 6.9604(8.5674) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 75.1905, Epoch Time 920.0768(836.6060), Bit/dim 3.7590(best: 3.7639), Xent 0.8808, Loss 4.1994, Error 0.3121(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 16.6319(15.0200) | Bit/dim 3.7610(3.7548) | Xent 0.8255(0.8659) | Loss 8.1342(8.8674) | Error 0.2844(0.3083) Steps 0(0.00) | Grad Norm 6.3007(8.2938) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 13.7556(14.8396) | Bit/dim 3.7312(3.7551) | Xent 0.8077(0.8628) | Loss 8.3179(8.7212) | Error 0.3011(0.3083) Steps 0(0.00) | Grad Norm 10.1382(8.4589) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.2305(14.8398) | Bit/dim 3.7538(3.7554) | Xent 0.8645(0.8588) | Loss 8.4398(8.6288) | Error 0.3211(0.3076) Steps 0(0.00) | Grad Norm 7.1405(8.3523) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.3838(14.9387) | Bit/dim 3.7875(3.7587) | Xent 0.9498(0.8676) | Loss 8.4140(8.5721) | Error 0.3444(0.3109) Steps 0(0.00) | Grad Norm 13.1974(9.2882) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.1596(14.8105) | Bit/dim 3.7471(3.7595) | Xent 0.8155(0.8746) | Loss 8.4031(8.5109) | Error 0.2856(0.3126) Steps 0(0.00) | Grad Norm 5.5853(9.7538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 77.1811, Epoch Time 909.0325(838.7788), Bit/dim 3.7647(best: 3.7590), Xent 0.8721, Loss 4.2007, Error 0.3084(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 13.9987(14.8265) | Bit/dim 3.7621(3.7595) | Xent 0.8072(0.8694) | Loss 8.2841(8.9397) | Error 0.2856(0.3113) Steps 0(0.00) | Grad Norm 6.8049(9.5265) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.7243(14.9225) | Bit/dim 3.7661(3.7592) | Xent 0.8691(0.8666) | Loss 8.5086(8.7884) | Error 0.2933(0.3098) Steps 0(0.00) | Grad Norm 11.9359(9.0637) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 14.5006(14.9254) | Bit/dim 3.7577(3.7593) | Xent 0.7838(0.8610) | Loss 8.2356(8.6714) | Error 0.2856(0.3087) Steps 0(0.00) | Grad Norm 4.7604(9.1796) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 14.3414(14.9880) | Bit/dim 3.7337(3.7552) | Xent 0.8294(0.8533) | Loss 8.2377(8.5887) | Error 0.2911(0.3055) Steps 0(0.00) | Grad Norm 7.8973(9.2715) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 18.5236(15.1742) | Bit/dim 3.7313(3.7523) | Xent 0.9142(0.8532) | Loss 8.5408(8.5358) | Error 0.3122(0.3036) Steps 0(0.00) | Grad Norm 11.0962(8.9227) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 16.0267(15.2404) | Bit/dim 3.7475(3.7499) | Xent 0.8726(0.8624) | Loss 8.4145(8.5006) | Error 0.3089(0.3052) Steps 0(0.00) | Grad Norm 11.9369(9.8078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 77.3205, Epoch Time 928.8334(841.4804), Bit/dim 3.7636(best: 3.7590), Xent 0.8926, Loss 4.2099, Error 0.3088(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.1046(15.2145) | Bit/dim 3.7238(3.7525) | Xent 0.7892(0.8593) | Loss 8.3168(8.8777) | Error 0.2800(0.3042) Steps 0(0.00) | Grad Norm 6.5063(9.5608) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.0923(15.1848) | Bit/dim 3.7688(3.7519) | Xent 0.8926(0.8541) | Loss 8.4430(8.7351) | Error 0.3156(0.3031) Steps 0(0.00) | Grad Norm 6.4256(8.9608) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 14.6557(15.0604) | Bit/dim 3.7609(3.7548) | Xent 0.8205(0.8511) | Loss 8.3456(8.6326) | Error 0.2911(0.3011) Steps 0(0.00) | Grad Norm 11.4741(9.3169) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 14.8150(15.1054) | Bit/dim 3.7814(3.7539) | Xent 0.8585(0.8551) | Loss 8.4231(8.5615) | Error 0.3400(0.3035) Steps 0(0.00) | Grad Norm 7.5661(9.0875) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.5053(15.1488) | Bit/dim 3.7215(3.7522) | Xent 0.7945(0.8474) | Loss 8.2994(8.5140) | Error 0.2700(0.3011) Steps 0(0.00) | Grad Norm 6.6254(9.4747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 76.0437, Epoch Time 925.3386(843.9962), Bit/dim 3.7487(best: 3.7590), Xent 0.8587, Loss 4.1780, Error 0.2982(best: 0.3047)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.2586(15.2139) | Bit/dim 3.7649(3.7482) | Xent 0.8501(0.8380) | Loss 8.4675(8.9256) | Error 0.3144(0.2985) Steps 0(0.00) | Grad Norm 9.2427(8.8431) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 14.8409(15.1813) | Bit/dim 3.7155(3.7492) | Xent 0.8591(0.8351) | Loss 8.1735(8.7517) | Error 0.2967(0.2979) Steps 0(0.00) | Grad Norm 6.1051(9.1885) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.0470(15.2078) | Bit/dim 3.7181(3.7466) | Xent 0.8001(0.8385) | Loss 8.2783(8.6476) | Error 0.2811(0.2989) Steps 0(0.00) | Grad Norm 4.1442(9.1297) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.0484(15.3854) | Bit/dim 3.7316(3.7433) | Xent 0.8625(0.8323) | Loss 8.5016(8.5667) | Error 0.3133(0.2958) Steps 0(0.00) | Grad Norm 6.0317(8.6623) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 14.6445(15.3525) | Bit/dim 3.7145(3.7428) | Xent 0.8377(0.8355) | Loss 8.4009(8.5221) | Error 0.2878(0.2961) Steps 0(0.00) | Grad Norm 11.8002(8.7401) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 14.9163(15.4180) | Bit/dim 3.7904(3.7443) | Xent 0.7413(0.8359) | Loss 7.9920(8.4644) | Error 0.2711(0.2970) Steps 0(0.00) | Grad Norm 7.3571(8.5997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 76.3224, Epoch Time 939.8951(846.8731), Bit/dim 3.7472(best: 3.7487), Xent 0.8426, Loss 4.1685, Error 0.2924(best: 0.2982)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.0788(15.3973) | Bit/dim 3.7509(3.7422) | Xent 0.8171(0.8342) | Loss 8.2697(8.8239) | Error 0.2889(0.2954) Steps 0(0.00) | Grad Norm 8.1908(8.8600) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 14.3750(15.2752) | Bit/dim 3.7472(3.7416) | Xent 0.7956(0.8283) | Loss 8.2442(8.6756) | Error 0.2889(0.2942) Steps 0(0.00) | Grad Norm 5.6047(8.4425) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.0859(15.2584) | Bit/dim 3.7517(3.7416) | Xent 0.8082(0.8277) | Loss 8.2724(8.5668) | Error 0.2944(0.2957) Steps 0(0.00) | Grad Norm 6.8042(8.4451) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.9241(15.3099) | Bit/dim 3.7722(3.7425) | Xent 0.9272(0.8338) | Loss 8.4899(8.5170) | Error 0.3133(0.2969) Steps 0(0.00) | Grad Norm 10.2329(8.9236) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 14.8118(15.2668) | Bit/dim 3.6981(3.7438) | Xent 0.8167(0.8311) | Loss 8.3666(8.4517) | Error 0.2756(0.2951) Steps 0(0.00) | Grad Norm 9.3139(8.9242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 77.3703, Epoch Time 931.0154(849.3974), Bit/dim 3.7390(best: 3.7472), Xent 0.8155, Loss 4.1468, Error 0.2872(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 14.5505(15.3336) | Bit/dim 3.7460(3.7415) | Xent 0.8283(0.8294) | Loss 8.3702(8.9140) | Error 0.2944(0.2960) Steps 0(0.00) | Grad Norm 8.4768(8.8220) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 14.6077(15.3024) | Bit/dim 3.7300(3.7416) | Xent 0.8482(0.8204) | Loss 8.3095(8.7529) | Error 0.3022(0.2934) Steps 0(0.00) | Grad Norm 7.5641(8.4554) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.6958(15.4123) | Bit/dim 3.7454(3.7430) | Xent 0.8714(0.8309) | Loss 8.5794(8.6592) | Error 0.3044(0.2974) Steps 0(0.00) | Grad Norm 14.0528(9.0849) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 16.4166(15.3970) | Bit/dim 3.7416(3.7411) | Xent 0.8132(0.8300) | Loss 8.5153(8.5778) | Error 0.2756(0.2955) Steps 0(0.00) | Grad Norm 11.4103(8.9879) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.9250(15.2845) | Bit/dim 3.7655(3.7421) | Xent 0.8273(0.8257) | Loss 8.0837(8.4916) | Error 0.2944(0.2937) Steps 0(0.00) | Grad Norm 7.7913(8.7996) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 14.7059(15.2923) | Bit/dim 3.7393(3.7376) | Xent 0.8867(0.8249) | Loss 8.3576(8.4275) | Error 0.3111(0.2923) Steps 0(0.00) | Grad Norm 10.1827(8.6897) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 77.4276, Epoch Time 937.2294(852.0324), Bit/dim 3.7395(best: 3.7390), Xent 0.8459, Loss 4.1624, Error 0.2990(best: 0.2872)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 16.2394(15.3391) | Bit/dim 3.7854(3.7423) | Xent 0.8189(0.8256) | Loss 8.4020(8.7992) | Error 0.2978(0.2927) Steps 0(0.00) | Grad Norm 10.1574(9.1961) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 14.6866(15.3205) | Bit/dim 3.7133(3.7400) | Xent 0.7360(0.8269) | Loss 8.1368(8.6739) | Error 0.2678(0.2924) Steps 0(0.00) | Grad Norm 6.5513(9.5612) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.3497(15.3657) | Bit/dim 3.7275(3.7374) | Xent 0.7492(0.8176) | Loss 8.2293(8.5747) | Error 0.2589(0.2903) Steps 0(0.00) | Grad Norm 5.4376(8.9258) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 15.5783(15.4437) | Bit/dim 3.6911(3.7371) | Xent 0.8231(0.8176) | Loss 8.2292(8.4955) | Error 0.2967(0.2907) Steps 0(0.00) | Grad Norm 5.8533(8.3657) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.9668(15.5195) | Bit/dim 3.7451(3.7356) | Xent 0.8114(0.8153) | Loss 8.2487(8.4315) | Error 0.2989(0.2913) Steps 0(0.00) | Grad Norm 4.7741(7.7956) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 76.9422, Epoch Time 945.8317(854.8463), Bit/dim 3.7317(best: 3.7390), Xent 0.8326, Loss 4.1480, Error 0.2960(best: 0.2872)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.0387(15.5265) | Bit/dim 3.7318(3.7335) | Xent 0.8589(0.8153) | Loss 8.2931(8.8730) | Error 0.3244(0.2921) Steps 0(0.00) | Grad Norm 9.5443(8.1933) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.7330(15.4703) | Bit/dim 3.7641(3.7328) | Xent 0.8613(0.8166) | Loss 8.3073(8.7114) | Error 0.3122(0.2912) Steps 0(0.00) | Grad Norm 12.4340(8.2432) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.6187(15.4772) | Bit/dim 3.7258(3.7326) | Xent 0.7915(0.8202) | Loss 8.2979(8.5969) | Error 0.2700(0.2921) Steps 0(0.00) | Grad Norm 10.6786(8.3369) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 16.5561(15.5602) | Bit/dim 3.7477(3.7336) | Xent 0.8048(0.8150) | Loss 8.4810(8.5195) | Error 0.2956(0.2899) Steps 0(0.00) | Grad Norm 6.0050(8.5282) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 14.9638(15.5809) | Bit/dim 3.7277(3.7325) | Xent 0.7861(0.8134) | Loss 8.2837(8.4769) | Error 0.2822(0.2885) Steps 0(0.00) | Grad Norm 10.6810(8.6688) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 14.8671(15.4301) | Bit/dim 3.7254(3.7316) | Xent 0.8046(0.8068) | Loss 8.2268(8.4204) | Error 0.2833(0.2867) Steps 0(0.00) | Grad Norm 7.0047(8.4734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 78.7450, Epoch Time 945.7622(857.5738), Bit/dim 3.7359(best: 3.7317), Xent 0.8065, Loss 4.1391, Error 0.2833(best: 0.2872)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.4423(15.4506) | Bit/dim 3.7210(3.7290) | Xent 0.7549(0.8011) | Loss 8.1977(8.7898) | Error 0.2667(0.2858) Steps 0(0.00) | Grad Norm 5.5461(8.1924) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 16.1214(15.5192) | Bit/dim 3.6866(3.7279) | Xent 0.8124(0.7988) | Loss 8.2356(8.6464) | Error 0.2967(0.2850) Steps 0(0.00) | Grad Norm 11.3571(8.2770) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 16.0959(15.5067) | Bit/dim 3.7324(3.7267) | Xent 0.8010(0.7979) | Loss 8.1692(8.5388) | Error 0.2867(0.2841) Steps 0(0.00) | Grad Norm 8.0556(8.0285) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 14.7020(15.4553) | Bit/dim 3.7573(3.7290) | Xent 0.8166(0.7980) | Loss 8.1627(8.4785) | Error 0.2922(0.2830) Steps 0(0.00) | Grad Norm 11.1770(8.4589) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 15.0816(15.6277) | Bit/dim 3.7168(3.7304) | Xent 0.8078(0.7994) | Loss 8.1587(8.4374) | Error 0.2778(0.2837) Steps 0(0.00) | Grad Norm 5.9107(8.2164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 80.4701, Epoch Time 957.4153(860.5690), Bit/dim 3.7247(best: 3.7317), Xent 0.8213, Loss 4.1354, Error 0.2936(best: 0.2833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 15.5338(15.6049) | Bit/dim 3.7262(3.7273) | Xent 0.8507(0.8027) | Loss 8.1623(8.8597) | Error 0.2967(0.2850) Steps 0(0.00) | Grad Norm 12.6367(8.2162) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 14.6416(15.5334) | Bit/dim 3.7360(3.7286) | Xent 0.7557(0.7965) | Loss 8.2347(8.7159) | Error 0.2767(0.2839) Steps 0(0.00) | Grad Norm 6.6047(8.1061) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.5856(15.5746) | Bit/dim 3.7040(3.7280) | Xent 0.7983(0.7938) | Loss 8.3591(8.5990) | Error 0.2856(0.2831) Steps 0(0.00) | Grad Norm 13.1664(8.5601) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.3799(15.5143) | Bit/dim 3.7146(3.7268) | Xent 0.8382(0.7985) | Loss 8.3418(8.5247) | Error 0.3011(0.2851) Steps 0(0.00) | Grad Norm 9.9034(8.5006) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 18.7461(15.6250) | Bit/dim 3.7061(3.7260) | Xent 0.8133(0.7988) | Loss 8.2642(8.4388) | Error 0.2889(0.2868) Steps 0(0.00) | Grad Norm 11.1377(8.8945) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 16.5540(15.7559) | Bit/dim 3.7095(3.7228) | Xent 0.7739(0.8015) | Loss 8.2516(8.3940) | Error 0.2833(0.2870) Steps 0(0.00) | Grad Norm 5.7883(8.6164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 79.9760, Epoch Time 955.3032(863.4111), Bit/dim 3.7268(best: 3.7247), Xent 0.8005, Loss 4.1271, Error 0.2811(best: 0.2833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.0162(15.7091) | Bit/dim 3.6921(3.7232) | Xent 0.8397(0.7965) | Loss 8.3390(8.7449) | Error 0.2978(0.2845) Steps 0(0.00) | Grad Norm 13.1913(8.7096) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.0124(15.7648) | Bit/dim 3.7175(3.7224) | Xent 0.7694(0.7923) | Loss 8.3668(8.6113) | Error 0.2711(0.2827) Steps 0(0.00) | Grad Norm 7.0885(8.3455) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.5386(15.8061) | Bit/dim 3.6936(3.7208) | Xent 0.7593(0.7852) | Loss 8.2886(8.5324) | Error 0.2700(0.2794) Steps 0(0.00) | Grad Norm 6.6387(8.4159) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.0429(15.7387) | Bit/dim 3.7201(3.7208) | Xent 0.7678(0.7840) | Loss 8.3400(8.4528) | Error 0.2722(0.2785) Steps 0(0.00) | Grad Norm 5.4797(8.4991) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.0295(15.6699) | Bit/dim 3.7566(3.7232) | Xent 0.8278(0.7873) | Loss 8.3581(8.4185) | Error 0.2922(0.2793) Steps 0(0.00) | Grad Norm 11.5503(8.4586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 80.2103, Epoch Time 960.2252(866.3155), Bit/dim 3.7254(best: 3.7247), Xent 0.8166, Loss 4.1337, Error 0.2875(best: 0.2811)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.2056(15.8154) | Bit/dim 3.7338(3.7222) | Xent 0.7515(0.7838) | Loss 8.0641(8.8446) | Error 0.2700(0.2791) Steps 0(0.00) | Grad Norm 9.7457(8.5293) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.5993(15.7424) | Bit/dim 3.7078(3.7209) | Xent 0.7415(0.7744) | Loss 8.3392(8.6854) | Error 0.2478(0.2748) Steps 0(0.00) | Grad Norm 7.0860(8.3450) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.5173(15.7071) | Bit/dim 3.6994(3.7171) | Xent 0.8478(0.7867) | Loss 8.4677(8.5804) | Error 0.2989(0.2794) Steps 0(0.00) | Grad Norm 12.1525(8.7628) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.1624(15.7125) | Bit/dim 3.7461(3.7178) | Xent 0.7940(0.7827) | Loss 8.2803(8.4782) | Error 0.2944(0.2793) Steps 0(0.00) | Grad Norm 6.2481(8.3378) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 18.5237(15.8620) | Bit/dim 3.7206(3.7207) | Xent 0.7824(0.7850) | Loss 8.3195(8.4306) | Error 0.2744(0.2797) Steps 0(0.00) | Grad Norm 5.4927(8.0005) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.7859(15.9370) | Bit/dim 3.7086(3.7202) | Xent 0.8011(0.7741) | Loss 8.2597(8.3670) | Error 0.2689(0.2762) Steps 0(0.00) | Grad Norm 4.1381(7.5659) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 80.7999, Epoch Time 973.1790(869.5214), Bit/dim 3.7187(best: 3.7247), Xent 0.7916, Loss 4.1145, Error 0.2768(best: 0.2811)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 14.7192(15.8825) | Bit/dim 3.7345(3.7246) | Xent 0.8642(0.8005) | Loss 8.2984(8.7748) | Error 0.2867(0.2858) Steps 0(0.00) | Grad Norm 11.9006(9.3384) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 16.0971(15.9068) | Bit/dim 3.7119(3.7268) | Xent 0.8065(0.7994) | Loss 8.5203(8.6462) | Error 0.2822(0.2852) Steps 0(0.00) | Grad Norm 13.3335(9.8244) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 15.4420(15.8563) | Bit/dim 3.7080(3.7290) | Xent 0.7560(0.7999) | Loss 8.0886(8.5639) | Error 0.2700(0.2842) Steps 0(0.00) | Grad Norm 5.6793(9.5518) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 16.8537(16.0561) | Bit/dim 3.6888(3.7256) | Xent 0.7872(0.8004) | Loss 7.8991(8.4868) | Error 0.2744(0.2837) Steps 0(0.00) | Grad Norm 6.1344(9.2561) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.7194(16.0409) | Bit/dim 3.6880(3.7236) | Xent 0.8134(0.7908) | Loss 8.0265(8.4125) | Error 0.2667(0.2811) Steps 0(0.00) | Grad Norm 5.2284(8.3869) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 81.7593, Epoch Time 981.4252(872.8785), Bit/dim 3.7156(best: 3.7187), Xent 0.7953, Loss 4.1132, Error 0.2789(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.6303(16.1170) | Bit/dim 3.6999(3.7190) | Xent 0.6804(0.7807) | Loss 8.1614(8.8474) | Error 0.2400(0.2776) Steps 0(0.00) | Grad Norm 10.7466(8.3553) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.9604(16.0419) | Bit/dim 3.6995(3.7196) | Xent 0.8205(0.7803) | Loss 8.5307(8.7041) | Error 0.2889(0.2767) Steps 0(0.00) | Grad Norm 10.2766(8.6587) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 15.7510(15.9582) | Bit/dim 3.7048(3.7198) | Xent 0.8482(0.7828) | Loss 8.3635(8.5969) | Error 0.3133(0.2789) Steps 0(0.00) | Grad Norm 7.4021(8.5661) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.5484(15.8988) | Bit/dim 3.7021(3.7194) | Xent 0.7031(0.7766) | Loss 8.1788(8.4909) | Error 0.2467(0.2765) Steps 0(0.00) | Grad Norm 4.1595(7.9345) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.5024(15.8685) | Bit/dim 3.6979(3.7198) | Xent 0.7123(0.7700) | Loss 7.9457(8.4141) | Error 0.2500(0.2733) Steps 0(0.00) | Grad Norm 11.2582(8.0450) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 15.4600(15.8177) | Bit/dim 3.7139(3.7170) | Xent 0.8868(0.7756) | Loss 8.1636(8.3561) | Error 0.3211(0.2769) Steps 0(0.00) | Grad Norm 11.6501(8.1315) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 79.5281, Epoch Time 964.5152(875.6276), Bit/dim 3.7120(best: 3.7156), Xent 0.7858, Loss 4.1049, Error 0.2770(best: 0.2768)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.3513(15.8274) | Bit/dim 3.7175(3.7178) | Xent 0.7357(0.7705) | Loss 8.2309(8.7209) | Error 0.2633(0.2751) Steps 0(0.00) | Grad Norm 6.2249(8.6717) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.4782(15.8735) | Bit/dim 3.7421(3.7194) | Xent 0.7583(0.7657) | Loss 8.3664(8.6171) | Error 0.2822(0.2737) Steps 0(0.00) | Grad Norm 11.8774(8.5044) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.2321(15.8855) | Bit/dim 3.7393(3.7170) | Xent 0.7689(0.7634) | Loss 8.3162(8.5113) | Error 0.2811(0.2727) Steps 0(0.00) | Grad Norm 6.3576(8.4371) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_60_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_60_run1/epoch_13_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 60.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
