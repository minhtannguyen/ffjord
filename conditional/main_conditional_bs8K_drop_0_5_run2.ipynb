{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_8K_drop_0_5_run2/epoch_115_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_8K_drop_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0806 | Time 69.4022(31.2786) | Bit/dim 1.1836(1.2162) | Xent 0.0542(0.0672) | Loss 1.2107(1.2498) | Error 0.0180(0.0210) Steps 422(416.43) | Grad Norm 0.5449(2.7381) | Total Time 10.00(10.00)\n",
      "Iter 0807 | Time 31.8139(31.2946) | Bit/dim 1.1826(1.2152) | Xent 0.0586(0.0669) | Loss 1.2119(1.2487) | Error 0.0179(0.0209) Steps 434(416.96) | Grad Norm 0.4475(2.6693) | Total Time 10.00(10.00)\n",
      "Iter 0808 | Time 30.4539(31.2694) | Bit/dim 1.1822(1.2142) | Xent 0.0541(0.0665) | Loss 1.2093(1.2475) | Error 0.0158(0.0207) Steps 434(417.47) | Grad Norm 0.4269(2.6021) | Total Time 10.00(10.00)\n",
      "Iter 0809 | Time 30.8817(31.2578) | Bit/dim 1.1816(1.2133) | Xent 0.0470(0.0659) | Loss 1.2051(1.2462) | Error 0.0139(0.0205) Steps 434(417.96) | Grad Norm 0.3679(2.5350) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 30.6020(31.2381) | Bit/dim 1.1800(1.2123) | Xent 0.0528(0.0656) | Loss 1.2064(1.2450) | Error 0.0164(0.0204) Steps 434(418.44) | Grad Norm 0.3077(2.4682) | Total Time 10.00(10.00)\n",
      "Iter 0811 | Time 31.6759(31.2512) | Bit/dim 1.1734(1.2111) | Xent 0.0461(0.0650) | Loss 1.1964(1.2436) | Error 0.0148(0.0202) Steps 434(418.91) | Grad Norm 0.3631(2.4051) | Total Time 10.00(10.00)\n",
      "Iter 0812 | Time 31.3006(31.2527) | Bit/dim 1.1741(1.2100) | Xent 0.0514(0.0646) | Loss 1.1998(1.2423) | Error 0.0164(0.0201) Steps 434(419.36) | Grad Norm 0.3433(2.3432) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 25.8634, Epoch Time 294.5589(232.4503), Bit/dim 1.1695(best: inf), Xent 0.0313, Loss 1.1852, Error 0.0102(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0813 | Time 32.3384(31.2853) | Bit/dim 1.1761(1.2090) | Xent 0.0477(0.0641) | Loss 1.1999(1.2410) | Error 0.0158(0.0200) Steps 434(419.80) | Grad Norm 0.3247(2.2827) | Total Time 10.00(10.00)\n",
      "Iter 0814 | Time 30.2256(31.2535) | Bit/dim 1.1786(1.2081) | Xent 0.0425(0.0634) | Loss 1.1999(1.2398) | Error 0.0121(0.0197) Steps 428(420.05) | Grad Norm 0.3037(2.2233) | Total Time 10.00(10.00)\n",
      "Iter 0815 | Time 30.4793(31.2303) | Bit/dim 1.1749(1.2071) | Xent 0.0529(0.0631) | Loss 1.2013(1.2386) | Error 0.0168(0.0196) Steps 434(420.47) | Grad Norm 0.3376(2.1667) | Total Time 10.00(10.00)\n",
      "Iter 0816 | Time 31.2189(31.2299) | Bit/dim 1.1705(1.2060) | Xent 0.0481(0.0626) | Loss 1.1945(1.2373) | Error 0.0158(0.0195) Steps 428(420.69) | Grad Norm 0.3260(2.1115) | Total Time 10.00(10.00)\n",
      "Iter 0817 | Time 31.9480(31.2515) | Bit/dim 1.1761(1.2051) | Xent 0.0517(0.0623) | Loss 1.2020(1.2362) | Error 0.0164(0.0194) Steps 434(421.09) | Grad Norm 0.3721(2.0593) | Total Time 10.00(10.00)\n",
      "Iter 0818 | Time 32.6497(31.2934) | Bit/dim 1.1702(1.2040) | Xent 0.0447(0.0618) | Loss 1.1926(1.2349) | Error 0.0154(0.0193) Steps 434(421.48) | Grad Norm 0.2636(2.0054) | Total Time 10.00(10.00)\n",
      "Iter 0819 | Time 30.6277(31.2734) | Bit/dim 1.1759(1.2032) | Xent 0.0529(0.0615) | Loss 1.2024(1.2339) | Error 0.0159(0.0192) Steps 434(421.86) | Grad Norm 0.2968(1.9542) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 16.1379, Epoch Time 248.2324(232.9237), Bit/dim 1.1680(best: 1.1695), Xent 0.0326, Loss 1.1843, Error 0.0112(best: 0.0102)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0820 | Time 30.9936(31.2650) | Bit/dim 1.1728(1.2023) | Xent 0.0526(0.0613) | Loss 1.1991(1.2329) | Error 0.0166(0.0191) Steps 434(422.22) | Grad Norm 0.3084(1.9048) | Total Time 10.00(10.00)\n",
      "Iter 0821 | Time 31.4177(31.2696) | Bit/dim 1.1793(1.2016) | Xent 0.0549(0.0611) | Loss 1.2068(1.2321) | Error 0.0152(0.0190) Steps 434(422.57) | Grad Norm 0.3043(1.8568) | Total Time 10.00(10.00)\n",
      "Iter 0822 | Time 30.1047(31.2347) | Bit/dim 1.1737(1.2007) | Xent 0.0523(0.0608) | Loss 1.1999(1.2311) | Error 0.0151(0.0189) Steps 428(422.74) | Grad Norm 0.2259(1.8079) | Total Time 10.00(10.00)\n",
      "Iter 0823 | Time 31.0764(31.2299) | Bit/dim 1.1701(1.1998) | Xent 0.0408(0.0602) | Loss 1.1905(1.2299) | Error 0.0136(0.0187) Steps 434(423.07) | Grad Norm 0.2324(1.7606) | Total Time 10.00(10.00)\n",
      "Iter 0824 | Time 32.0642(31.2550) | Bit/dim 1.1747(1.1991) | Xent 0.0456(0.0598) | Loss 1.1976(1.2290) | Error 0.0128(0.0186) Steps 428(423.22) | Grad Norm 0.2278(1.7146) | Total Time 10.00(10.00)\n",
      "Iter 0825 | Time 30.4083(31.2296) | Bit/dim 1.1694(1.1982) | Xent 0.0471(0.0594) | Loss 1.1929(1.2279) | Error 0.0136(0.0184) Steps 434(423.54) | Grad Norm 0.3031(1.6723) | Total Time 10.00(10.00)\n",
      "Iter 0826 | Time 30.6325(31.2116) | Bit/dim 1.1723(1.1974) | Xent 0.0492(0.0591) | Loss 1.1969(1.2269) | Error 0.0164(0.0183) Steps 434(423.86) | Grad Norm 0.3103(1.6314) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 16.0192, Epoch Time 245.2890(233.2947), Bit/dim 1.1664(best: 1.1680), Xent 0.0319, Loss 1.1824, Error 0.0102(best: 0.0102)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0827 | Time 30.6268(31.1941) | Bit/dim 1.1723(1.1967) | Xent 0.0470(0.0587) | Loss 1.1958(1.2260) | Error 0.0144(0.0182) Steps 434(424.16) | Grad Norm 0.2116(1.5888) | Total Time 10.00(10.00)\n",
      "Iter 0828 | Time 32.2899(31.2270) | Bit/dim 1.1695(1.1958) | Xent 0.0455(0.0583) | Loss 1.1923(1.2250) | Error 0.0154(0.0181) Steps 434(424.46) | Grad Norm 0.2347(1.5482) | Total Time 10.00(10.00)\n",
      "Iter 0829 | Time 30.6063(31.2083) | Bit/dim 1.1692(1.1950) | Xent 0.0467(0.0580) | Loss 1.1925(1.2240) | Error 0.0132(0.0180) Steps 434(424.74) | Grad Norm 0.2403(1.5090) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 31.6158(31.2206) | Bit/dim 1.1752(1.1944) | Xent 0.0452(0.0576) | Loss 1.1978(1.2232) | Error 0.0138(0.0179) Steps 434(425.02) | Grad Norm 0.2521(1.4713) | Total Time 10.00(10.00)\n",
      "Iter 0831 | Time 31.5257(31.2297) | Bit/dim 1.1701(1.1937) | Xent 0.0511(0.0574) | Loss 1.1956(1.2224) | Error 0.0171(0.0178) Steps 434(425.29) | Grad Norm 0.2614(1.4350) | Total Time 10.00(10.00)\n",
      "Iter 0832 | Time 31.0985(31.2258) | Bit/dim 1.1722(1.1931) | Xent 0.0523(0.0572) | Loss 1.1983(1.2217) | Error 0.0160(0.0178) Steps 434(425.55) | Grad Norm 0.2167(1.3984) | Total Time 10.00(10.00)\n",
      "Iter 0833 | Time 31.1269(31.2228) | Bit/dim 1.1766(1.1926) | Xent 0.0468(0.0569) | Loss 1.2000(1.2210) | Error 0.0149(0.0177) Steps 428(425.63) | Grad Norm 0.2236(1.3632) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 16.5223, Epoch Time 247.7925(233.7296), Bit/dim 1.1662(best: 1.1664), Xent 0.0299, Loss 1.1812, Error 0.0101(best: 0.0102)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0834 | Time 31.9357(31.2442) | Bit/dim 1.1734(1.1920) | Xent 0.0496(0.0567) | Loss 1.1982(1.2204) | Error 0.0155(0.0176) Steps 428(425.70) | Grad Norm 0.2839(1.3308) | Total Time 10.00(10.00)\n",
      "Iter 0835 | Time 32.0941(31.2697) | Bit/dim 1.1751(1.1915) | Xent 0.0479(0.0564) | Loss 1.1990(1.2197) | Error 0.0140(0.0175) Steps 434(425.95) | Grad Norm 0.2595(1.2987) | Total Time 10.00(10.00)\n",
      "Iter 0836 | Time 32.0662(31.2936) | Bit/dim 1.1659(1.1907) | Xent 0.0466(0.0561) | Loss 1.1892(1.2188) | Error 0.0150(0.0175) Steps 428(426.01) | Grad Norm 0.1989(1.2657) | Total Time 10.00(10.00)\n",
      "Iter 0837 | Time 31.3873(31.2964) | Bit/dim 1.1732(1.1902) | Xent 0.0536(0.0561) | Loss 1.2000(1.2182) | Error 0.0170(0.0174) Steps 422(425.89) | Grad Norm 0.2250(1.2344) | Total Time 10.00(10.00)\n",
      "Iter 0838 | Time 30.5237(31.2732) | Bit/dim 1.1712(1.1896) | Xent 0.0470(0.0558) | Loss 1.1947(1.2175) | Error 0.0151(0.0174) Steps 416(425.59) | Grad Norm 0.2897(1.2061) | Total Time 10.00(10.00)\n",
      "Iter 0839 | Time 32.1240(31.2988) | Bit/dim 1.1755(1.1892) | Xent 0.0515(0.0557) | Loss 1.2013(1.2170) | Error 0.0165(0.0173) Steps 428(425.66) | Grad Norm 0.2387(1.1771) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 30.8530(31.2854) | Bit/dim 1.1751(1.1888) | Xent 0.0522(0.0556) | Loss 1.2013(1.2166) | Error 0.0158(0.0173) Steps 434(425.91) | Grad Norm 0.2592(1.1495) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 16.6389, Epoch Time 250.0042(234.2179), Bit/dim 1.1704(best: 1.1662), Xent 0.0320, Loss 1.1864, Error 0.0104(best: 0.0101)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0841 | Time 31.5451(31.2932) | Bit/dim 1.1741(1.1883) | Xent 0.0514(0.0554) | Loss 1.1998(1.2161) | Error 0.0169(0.0173) Steps 428(425.98) | Grad Norm 0.2596(1.1228) | Total Time 10.00(10.00)\n",
      "Iter 0842 | Time 31.9126(31.3118) | Bit/dim 1.1742(1.1879) | Xent 0.0537(0.0554) | Loss 1.2011(1.2156) | Error 0.0164(0.0173) Steps 428(426.04) | Grad Norm 0.2680(1.0972) | Total Time 10.00(10.00)\n",
      "Iter 0843 | Time 31.5562(31.3191) | Bit/dim 1.1789(1.1876) | Xent 0.0461(0.0551) | Loss 1.2019(1.2152) | Error 0.0155(0.0172) Steps 434(426.28) | Grad Norm 0.2873(1.0729) | Total Time 10.00(10.00)\n",
      "Iter 0844 | Time 33.3932(31.3813) | Bit/dim 1.1796(1.1874) | Xent 0.0584(0.0552) | Loss 1.2088(1.2150) | Error 0.0191(0.0173) Steps 440(426.69) | Grad Norm 0.2587(1.0485) | Total Time 10.00(10.00)\n",
      "Iter 0845 | Time 33.6879(31.4505) | Bit/dim 1.1800(1.1872) | Xent 0.0570(0.0553) | Loss 1.2085(1.2148) | Error 0.0192(0.0173) Steps 440(427.09) | Grad Norm 0.2186(1.0236) | Total Time 10.00(10.00)\n",
      "Iter 0846 | Time 32.9840(31.4965) | Bit/dim 1.1852(1.1871) | Xent 0.0560(0.0553) | Loss 1.2132(1.2148) | Error 0.0175(0.0173) Steps 428(427.11) | Grad Norm 0.2082(0.9991) | Total Time 10.00(10.00)\n",
      "Iter 0847 | Time 32.5426(31.5279) | Bit/dim 1.1809(1.1869) | Xent 0.0564(0.0553) | Loss 1.2091(1.2146) | Error 0.0169(0.0173) Steps 434(427.32) | Grad Norm 0.2549(0.9768) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 16.5385, Epoch Time 257.0211(234.9020), Bit/dim 1.1736(best: 1.1662), Xent 0.0303, Loss 1.1887, Error 0.0101(best: 0.0101)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0848 | Time 32.8676(31.5681) | Bit/dim 1.1825(1.1868) | Xent 0.0570(0.0554) | Loss 1.2110(1.2145) | Error 0.0155(0.0173) Steps 428(427.34) | Grad Norm 0.2033(0.9536) | Total Time 10.00(10.00)\n",
      "Iter 0849 | Time 31.6092(31.5693) | Bit/dim 1.1775(1.1865) | Xent 0.0598(0.0555) | Loss 1.2074(1.2143) | Error 0.0175(0.0173) Steps 428(427.36) | Grad Norm 0.3577(0.9357) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 32.9023(31.6093) | Bit/dim 1.1747(1.1862) | Xent 0.0523(0.0554) | Loss 1.2009(1.2139) | Error 0.0174(0.0173) Steps 434(427.56) | Grad Norm 0.2864(0.9162) | Total Time 10.00(10.00)\n",
      "Iter 0851 | Time 32.9184(31.6486) | Bit/dim 1.1803(1.1860) | Xent 0.0514(0.0553) | Loss 1.2060(1.2136) | Error 0.0171(0.0173) Steps 434(427.75) | Grad Norm 0.2920(0.8975) | Total Time 10.00(10.00)\n",
      "Iter 0852 | Time 33.4837(31.7036) | Bit/dim 1.1743(1.1856) | Xent 0.0496(0.0551) | Loss 1.1992(1.2132) | Error 0.0144(0.0172) Steps 428(427.76) | Grad Norm 0.2489(0.8780) | Total Time 10.00(10.00)\n",
      "Iter 0853 | Time 32.3853(31.7241) | Bit/dim 1.1846(1.1856) | Xent 0.0585(0.0552) | Loss 1.2138(1.2132) | Error 0.0186(0.0172) Steps 434(427.95) | Grad Norm 0.3142(0.8611) | Total Time 10.00(10.00)\n",
      "Iter 0854 | Time 33.5363(31.7785) | Bit/dim 1.1780(1.1854) | Xent 0.0527(0.0551) | Loss 1.2043(1.2130) | Error 0.0180(0.0172) Steps 422(427.77) | Grad Norm 0.4229(0.8480) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 16.2194, Epoch Time 258.5630(235.6118), Bit/dim 1.1733(best: 1.1662), Xent 0.0312, Loss 1.1889, Error 0.0112(best: 0.0101)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0855 | Time 32.1264(31.7889) | Bit/dim 1.1765(1.1851) | Xent 0.0485(0.0549) | Loss 1.2007(1.2126) | Error 0.0150(0.0172) Steps 434(427.96) | Grad Norm 0.3115(0.8319) | Total Time 10.00(10.00)\n",
      "Iter 0856 | Time 32.2117(31.8016) | Bit/dim 1.1774(1.1849) | Xent 0.0516(0.0548) | Loss 1.2032(1.2123) | Error 0.0156(0.0171) Steps 428(427.96) | Grad Norm 0.2447(0.8143) | Total Time 10.00(10.00)\n",
      "Iter 0857 | Time 33.2435(31.8448) | Bit/dim 1.1791(1.1847) | Xent 0.0524(0.0548) | Loss 1.2053(1.2121) | Error 0.0159(0.0171) Steps 428(427.96) | Grad Norm 0.3232(0.7995) | Total Time 10.00(10.00)\n",
      "Iter 0858 | Time 32.0716(31.8516) | Bit/dim 1.1764(1.1845) | Xent 0.0536(0.0547) | Loss 1.2032(1.2118) | Error 0.0166(0.0171) Steps 422(427.78) | Grad Norm 0.4044(0.7877) | Total Time 10.00(10.00)\n",
      "Iter 0859 | Time 31.0859(31.8287) | Bit/dim 1.1791(1.1843) | Xent 0.0531(0.0547) | Loss 1.2057(1.2116) | Error 0.0178(0.0171) Steps 434(427.97) | Grad Norm 0.2628(0.7719) | Total Time 10.00(10.00)\n",
      "Iter 0860 | Time 31.4797(31.8182) | Bit/dim 1.1778(1.1841) | Xent 0.0565(0.0547) | Loss 1.2061(1.2115) | Error 0.0181(0.0171) Steps 422(427.79) | Grad Norm 0.2883(0.7574) | Total Time 10.00(10.00)\n",
      "Iter 0861 | Time 30.9131(31.7910) | Bit/dim 1.1786(1.1839) | Xent 0.0572(0.0548) | Loss 1.2071(1.2113) | Error 0.0192(0.0172) Steps 422(427.61) | Grad Norm 0.2239(0.7414) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 16.5585, Epoch Time 252.2922(236.1122), Bit/dim 1.1720(best: 1.1662), Xent 0.0307, Loss 1.1874, Error 0.0099(best: 0.0101)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0862 | Time 33.0427(31.8286) | Bit/dim 1.1807(1.1838) | Xent 0.0602(0.0550) | Loss 1.2108(1.2113) | Error 0.0175(0.0172) Steps 434(427.81) | Grad Norm 0.2431(0.7265) | Total Time 10.00(10.00)\n",
      "Iter 0863 | Time 33.5342(31.8798) | Bit/dim 1.1769(1.1836) | Xent 0.0613(0.0552) | Loss 1.2075(1.2112) | Error 0.0202(0.0173) Steps 434(427.99) | Grad Norm 0.2906(0.7134) | Total Time 10.00(10.00)\n",
      "Iter 0864 | Time 31.7080(31.8746) | Bit/dim 1.1797(1.1835) | Xent 0.0497(0.0550) | Loss 1.2046(1.2110) | Error 0.0152(0.0172) Steps 434(428.17) | Grad Norm 0.3026(0.7011) | Total Time 10.00(10.00)\n",
      "Iter 0865 | Time 32.1472(31.8828) | Bit/dim 1.1768(1.1833) | Xent 0.0558(0.0550) | Loss 1.2047(1.2108) | Error 0.0182(0.0173) Steps 446(428.71) | Grad Norm 0.2481(0.6875) | Total Time 10.00(10.00)\n",
      "Iter 0866 | Time 33.2155(31.9228) | Bit/dim 1.1750(1.1831) | Xent 0.0502(0.0549) | Loss 1.2001(1.2105) | Error 0.0145(0.0172) Steps 440(429.05) | Grad Norm 0.2196(0.6735) | Total Time 10.00(10.00)\n",
      "Iter 0867 | Time 34.7644(32.0080) | Bit/dim 1.1787(1.1829) | Xent 0.0608(0.0551) | Loss 1.2091(1.2105) | Error 0.0174(0.0172) Steps 446(429.55) | Grad Norm 0.2809(0.6617) | Total Time 10.00(10.00)\n",
      "Iter 0868 | Time 33.0368(32.0389) | Bit/dim 1.1793(1.1828) | Xent 0.0503(0.0549) | Loss 1.2045(1.2103) | Error 0.0152(0.0171) Steps 446(430.05) | Grad Norm 0.2318(0.6488) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 16.8001, Epoch Time 260.6352(236.8479), Bit/dim 1.1725(best: 1.1662), Xent 0.0305, Loss 1.1877, Error 0.0117(best: 0.0099)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0869 | Time 32.6284(32.0566) | Bit/dim 1.1749(1.1826) | Xent 0.0568(0.0550) | Loss 1.2033(1.2101) | Error 0.0181(0.0172) Steps 446(430.53) | Grad Norm 0.2341(0.6363) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 33.4197(32.0975) | Bit/dim 1.1770(1.1824) | Xent 0.0479(0.0548) | Loss 1.2009(1.2098) | Error 0.0159(0.0171) Steps 440(430.81) | Grad Norm 0.1965(0.6231) | Total Time 10.00(10.00)\n",
      "Iter 0871 | Time 32.1019(32.0976) | Bit/dim 1.1790(1.1823) | Xent 0.0627(0.0550) | Loss 1.2104(1.2098) | Error 0.0176(0.0171) Steps 440(431.09) | Grad Norm 0.2116(0.6108) | Total Time 10.00(10.00)\n",
      "Iter 0872 | Time 32.6993(32.1156) | Bit/dim 1.1840(1.1824) | Xent 0.0536(0.0550) | Loss 1.2108(1.2098) | Error 0.0172(0.0171) Steps 440(431.35) | Grad Norm 0.2117(0.5988) | Total Time 10.00(10.00)\n",
      "Iter 0873 | Time 33.2567(32.1499) | Bit/dim 1.1777(1.1822) | Xent 0.0510(0.0548) | Loss 1.2032(1.2096) | Error 0.0169(0.0171) Steps 440(431.61) | Grad Norm 0.2009(0.5869) | Total Time 10.00(10.00)\n",
      "Iter 0874 | Time 32.0371(32.1465) | Bit/dim 1.1758(1.1820) | Xent 0.0583(0.0549) | Loss 1.2049(1.2095) | Error 0.0186(0.0172) Steps 434(431.68) | Grad Norm 0.2651(0.5772) | Total Time 10.00(10.00)\n",
      "Iter 0875 | Time 33.1675(32.1771) | Bit/dim 1.1762(1.1819) | Xent 0.0523(0.0549) | Loss 1.2023(1.2093) | Error 0.0150(0.0171) Steps 440(431.93) | Grad Norm 0.2666(0.5679) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 16.7984, Epoch Time 258.5107(237.4978), Bit/dim 1.1733(best: 1.1662), Xent 0.0326, Loss 1.1896, Error 0.0114(best: 0.0099)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0876 | Time 33.6830(32.2223) | Bit/dim 1.1812(1.1818) | Xent 0.0566(0.0549) | Loss 1.2095(1.2093) | Error 0.0176(0.0171) Steps 434(432.00) | Grad Norm 0.2573(0.5586) | Total Time 10.00(10.00)\n",
      "Iter 0877 | Time 33.5762(32.2629) | Bit/dim 1.1824(1.1819) | Xent 0.0590(0.0550) | Loss 1.2119(1.2094) | Error 0.0194(0.0172) Steps 434(432.06) | Grad Norm 0.2210(0.5485) | Total Time 10.00(10.00)\n",
      "Iter 0878 | Time 34.1419(32.3193) | Bit/dim 1.1788(1.1818) | Xent 0.0511(0.0549) | Loss 1.2043(1.2092) | Error 0.0160(0.0172) Steps 446(432.47) | Grad Norm 0.2337(0.5390) | Total Time 10.00(10.00)\n",
      "Iter 0879 | Time 33.4259(32.3525) | Bit/dim 1.1825(1.1818) | Xent 0.0547(0.0549) | Loss 1.2098(1.2092) | Error 0.0162(0.0171) Steps 446(432.88) | Grad Norm 0.1879(0.5285) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 34.3531(32.4125) | Bit/dim 1.1807(1.1818) | Xent 0.0579(0.0550) | Loss 1.2097(1.2093) | Error 0.0175(0.0171) Steps 446(433.27) | Grad Norm 0.2673(0.5207) | Total Time 10.00(10.00)\n",
      "Iter 0881 | Time 33.6934(32.4509) | Bit/dim 1.1809(1.1817) | Xent 0.0552(0.0550) | Loss 1.2085(1.2092) | Error 0.0185(0.0172) Steps 446(433.66) | Grad Norm 0.2134(0.5114) | Total Time 10.00(10.00)\n",
      "Iter 0882 | Time 33.5282(32.4832) | Bit/dim 1.1797(1.1817) | Xent 0.0479(0.0548) | Loss 1.2037(1.2091) | Error 0.0152(0.0171) Steps 446(434.03) | Grad Norm 0.2335(0.5031) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 17.2989, Epoch Time 266.0376(238.3540), Bit/dim 1.1744(best: 1.1662), Xent 0.0298, Loss 1.1893, Error 0.0100(best: 0.0099)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0883 | Time 33.4590(32.5125) | Bit/dim 1.1827(1.1817) | Xent 0.0578(0.0549) | Loss 1.2116(1.2091) | Error 0.0175(0.0171) Steps 446(434.38) | Grad Norm 0.2796(0.4964) | Total Time 10.00(10.00)\n",
      "Iter 0884 | Time 33.3618(32.5380) | Bit/dim 1.1842(1.1818) | Xent 0.0483(0.0547) | Loss 1.2084(1.2091) | Error 0.0166(0.0171) Steps 446(434.73) | Grad Norm 0.2636(0.4894) | Total Time 10.00(10.00)\n",
      "Iter 0885 | Time 32.2599(32.5297) | Bit/dim 1.1796(1.1817) | Xent 0.0536(0.0547) | Loss 1.2065(1.2090) | Error 0.0179(0.0171) Steps 440(434.89) | Grad Norm 0.2305(0.4816) | Total Time 10.00(10.00)\n",
      "Iter 0886 | Time 33.3583(32.5545) | Bit/dim 1.1818(1.1817) | Xent 0.0555(0.0547) | Loss 1.2095(1.2091) | Error 0.0165(0.0171) Steps 446(435.22) | Grad Norm 0.2774(0.4755) | Total Time 10.00(10.00)\n",
      "Iter 0887 | Time 33.9307(32.5958) | Bit/dim 1.1777(1.1816) | Xent 0.0574(0.0548) | Loss 1.2064(1.2090) | Error 0.0194(0.0172) Steps 446(435.55) | Grad Norm 0.2899(0.4699) | Total Time 10.00(10.00)\n",
      "Iter 0888 | Time 34.7135(32.6593) | Bit/dim 1.1823(1.1816) | Xent 0.0547(0.0548) | Loss 1.2096(1.2090) | Error 0.0159(0.0172) Steps 452(436.04) | Grad Norm 0.2114(0.4622) | Total Time 10.00(10.00)\n",
      "Iter 0889 | Time 32.9786(32.6689) | Bit/dim 1.1790(1.1815) | Xent 0.0575(0.0548) | Loss 1.2077(1.2090) | Error 0.0179(0.0172) Steps 440(436.16) | Grad Norm 0.2669(0.4563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 16.9992, Epoch Time 263.4529(239.1069), Bit/dim 1.1745(best: 1.1662), Xent 0.0286, Loss 1.1888, Error 0.0087(best: 0.0099)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0890 | Time 32.4695(32.6629) | Bit/dim 1.1823(1.1816) | Xent 0.0536(0.0548) | Loss 1.2091(1.2090) | Error 0.0170(0.0172) Steps 440(436.28) | Grad Norm 0.2417(0.4499) | Total Time 10.00(10.00)\n",
      "Iter 0891 | Time 32.2332(32.6500) | Bit/dim 1.1806(1.1815) | Xent 0.0496(0.0546) | Loss 1.2054(1.2088) | Error 0.0151(0.0171) Steps 440(436.39) | Grad Norm 0.2045(0.4425) | Total Time 10.00(10.00)\n",
      "Iter 0892 | Time 33.9986(32.6905) | Bit/dim 1.1805(1.1815) | Xent 0.0647(0.0549) | Loss 1.2129(1.2090) | Error 0.0184(0.0171) Steps 440(436.50) | Grad Norm 0.2428(0.4365) | Total Time 10.00(10.00)\n",
      "Iter 0893 | Time 32.1029(32.6729) | Bit/dim 1.1803(1.1815) | Xent 0.0494(0.0548) | Loss 1.2050(1.2089) | Error 0.0156(0.0171) Steps 434(436.42) | Grad Norm 0.2539(0.4311) | Total Time 10.00(10.00)\n",
      "Iter 0894 | Time 32.4978(32.6676) | Bit/dim 1.1769(1.1813) | Xent 0.0528(0.0547) | Loss 1.2033(1.2087) | Error 0.0174(0.0171) Steps 434(436.35) | Grad Norm 0.2290(0.4250) | Total Time 10.00(10.00)\n",
      "Iter 0895 | Time 33.8388(32.7027) | Bit/dim 1.1830(1.1814) | Xent 0.0533(0.0547) | Loss 1.2097(1.2087) | Error 0.0181(0.0171) Steps 434(436.28) | Grad Norm 0.2754(0.4205) | Total Time 10.00(10.00)\n",
      "Iter 0896 | Time 33.2582(32.7194) | Bit/dim 1.1778(1.1813) | Xent 0.0550(0.0547) | Loss 1.2053(1.2086) | Error 0.0156(0.0171) Steps 434(436.21) | Grad Norm 0.3470(0.4183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 16.7981, Epoch Time 259.6327(239.7227), Bit/dim 1.1742(best: 1.1662), Xent 0.0307, Loss 1.1895, Error 0.0107(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0897 | Time 33.6888(32.7485) | Bit/dim 1.1799(1.1812) | Xent 0.0518(0.0546) | Loss 1.2058(1.2085) | Error 0.0161(0.0171) Steps 434(436.14) | Grad Norm 0.3147(0.4152) | Total Time 10.00(10.00)\n",
      "Iter 0898 | Time 32.2114(32.7324) | Bit/dim 1.1787(1.1812) | Xent 0.0578(0.0547) | Loss 1.2076(1.2085) | Error 0.0179(0.0171) Steps 434(436.08) | Grad Norm 0.2375(0.4099) | Total Time 10.00(10.00)\n",
      "Iter 0899 | Time 32.5360(32.7265) | Bit/dim 1.1776(1.1810) | Xent 0.0523(0.0546) | Loss 1.2037(1.2084) | Error 0.0151(0.0170) Steps 440(436.20) | Grad Norm 0.2263(0.4044) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 32.0233(32.7054) | Bit/dim 1.1820(1.1811) | Xent 0.0552(0.0546) | Loss 1.2096(1.2084) | Error 0.0181(0.0171) Steps 434(436.13) | Grad Norm 0.2849(0.4008) | Total Time 10.00(10.00)\n",
      "Iter 0901 | Time 34.6488(32.7637) | Bit/dim 1.1781(1.1810) | Xent 0.0581(0.0547) | Loss 1.2071(1.2084) | Error 0.0182(0.0171) Steps 440(436.25) | Grad Norm 0.3237(0.3985) | Total Time 10.00(10.00)\n",
      "Iter 0902 | Time 31.7777(32.7341) | Bit/dim 1.1844(1.1811) | Xent 0.0602(0.0549) | Loss 1.2145(1.2085) | Error 0.0184(0.0171) Steps 428(436.00) | Grad Norm 0.2837(0.3950) | Total Time 10.00(10.00)\n",
      "Iter 0903 | Time 32.8963(32.7390) | Bit/dim 1.1794(1.1810) | Xent 0.0557(0.0549) | Loss 1.2073(1.2085) | Error 0.0181(0.0172) Steps 434(435.94) | Grad Norm 0.2713(0.3913) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 16.4661, Epoch Time 258.6870(240.2916), Bit/dim 1.1737(best: 1.1662), Xent 0.0305, Loss 1.1890, Error 0.0102(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0904 | Time 34.7793(32.8002) | Bit/dim 1.1858(1.1812) | Xent 0.0562(0.0550) | Loss 1.2139(1.2087) | Error 0.0175(0.0172) Steps 434(435.88) | Grad Norm 0.2645(0.3875) | Total Time 10.00(10.00)\n",
      "Iter 0905 | Time 35.4585(32.8799) | Bit/dim 1.1811(1.1812) | Xent 0.0578(0.0551) | Loss 1.2100(1.2087) | Error 0.0172(0.0172) Steps 446(436.18) | Grad Norm 0.3134(0.3853) | Total Time 10.00(10.00)\n",
      "Iter 0906 | Time 35.0970(32.9465) | Bit/dim 1.1812(1.1812) | Xent 0.0452(0.0548) | Loss 1.2038(1.2086) | Error 0.0146(0.0171) Steps 446(436.48) | Grad Norm 0.2433(0.3810) | Total Time 10.00(10.00)\n",
      "Iter 0907 | Time 34.7273(32.9999) | Bit/dim 1.1798(1.1811) | Xent 0.0508(0.0546) | Loss 1.2053(1.2085) | Error 0.0144(0.0170) Steps 458(437.12) | Grad Norm 0.3026(0.3787) | Total Time 10.00(10.00)\n",
      "Iter 0908 | Time 35.4951(33.0747) | Bit/dim 1.1759(1.1810) | Xent 0.0517(0.0546) | Loss 1.2017(1.2083) | Error 0.0162(0.0170) Steps 458(437.75) | Grad Norm 0.2995(0.3763) | Total Time 10.00(10.00)\n",
      "Iter 0909 | Time 34.8170(33.1270) | Bit/dim 1.1758(1.1808) | Xent 0.0530(0.0545) | Loss 1.2023(1.2081) | Error 0.0162(0.0170) Steps 458(438.36) | Grad Norm 0.2014(0.3710) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 35.6028(33.2013) | Bit/dim 1.1797(1.1808) | Xent 0.0574(0.0546) | Loss 1.2084(1.2081) | Error 0.0191(0.0170) Steps 458(438.95) | Grad Norm 0.2378(0.3671) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 17.6203, Epoch Time 276.1001(241.3659), Bit/dim 1.1740(best: 1.1662), Xent 0.0296, Loss 1.1888, Error 0.0103(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0911 | Time 35.0471(33.2567) | Bit/dim 1.1788(1.1807) | Xent 0.0555(0.0546) | Loss 1.2066(1.2080) | Error 0.0155(0.0170) Steps 458(439.52) | Grad Norm 0.2234(0.3627) | Total Time 10.00(10.00)\n",
      "Iter 0912 | Time 34.8347(33.3040) | Bit/dim 1.1767(1.1806) | Xent 0.0531(0.0546) | Loss 1.2032(1.2079) | Error 0.0159(0.0170) Steps 458(440.07) | Grad Norm 0.2300(0.3588) | Total Time 10.00(10.00)\n",
      "Iter 0913 | Time 36.4732(33.3991) | Bit/dim 1.1817(1.1806) | Xent 0.0568(0.0546) | Loss 1.2101(1.2080) | Error 0.0194(0.0170) Steps 458(440.61) | Grad Norm 0.2290(0.3549) | Total Time 10.00(10.00)\n",
      "Iter 0914 | Time 37.1900(33.5128) | Bit/dim 1.1864(1.1808) | Xent 0.0544(0.0546) | Loss 1.2136(1.2081) | Error 0.0169(0.0170) Steps 452(440.95) | Grad Norm 0.1979(0.3502) | Total Time 10.00(10.00)\n",
      "Iter 0915 | Time 34.0240(33.5281) | Bit/dim 1.1813(1.1808) | Xent 0.0448(0.0543) | Loss 1.2037(1.2080) | Error 0.0140(0.0169) Steps 446(441.10) | Grad Norm 0.1898(0.3453) | Total Time 10.00(10.00)\n",
      "Iter 0916 | Time 34.6073(33.5605) | Bit/dim 1.1789(1.1808) | Xent 0.0609(0.0545) | Loss 1.2094(1.2080) | Error 0.0192(0.0170) Steps 452(441.43) | Grad Norm 0.2360(0.3421) | Total Time 10.00(10.00)\n",
      "Iter 0917 | Time 35.2078(33.6099) | Bit/dim 1.1758(1.1806) | Xent 0.0539(0.0545) | Loss 1.2028(1.2079) | Error 0.0158(0.0170) Steps 452(441.75) | Grad Norm 0.2367(0.3389) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 17.3555, Epoch Time 277.1037(242.4380), Bit/dim 1.1740(best: 1.1662), Xent 0.0310, Loss 1.1895, Error 0.0109(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0918 | Time 34.3201(33.6312) | Bit/dim 1.1783(1.1806) | Xent 0.0496(0.0544) | Loss 1.2031(1.2077) | Error 0.0154(0.0169) Steps 452(442.06) | Grad Norm 0.2185(0.3353) | Total Time 10.00(10.00)\n",
      "Iter 0919 | Time 35.2210(33.6789) | Bit/dim 1.1751(1.1804) | Xent 0.0552(0.0544) | Loss 1.2027(1.2076) | Error 0.0170(0.0169) Steps 452(442.35) | Grad Norm 0.2590(0.3330) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 35.3086(33.7278) | Bit/dim 1.1854(1.1805) | Xent 0.0534(0.0544) | Loss 1.2121(1.2077) | Error 0.0160(0.0169) Steps 458(442.82) | Grad Norm 0.2182(0.3296) | Total Time 10.00(10.00)\n",
      "Iter 0921 | Time 35.4654(33.7799) | Bit/dim 1.1802(1.1805) | Xent 0.0523(0.0543) | Loss 1.2064(1.2077) | Error 0.0181(0.0169) Steps 452(443.10) | Grad Norm 0.2392(0.3269) | Total Time 10.00(10.00)\n",
      "Iter 0922 | Time 36.9223(33.8742) | Bit/dim 1.1824(1.1806) | Xent 0.0540(0.0543) | Loss 1.2095(1.2077) | Error 0.0189(0.0170) Steps 458(443.55) | Grad Norm 0.2412(0.3243) | Total Time 10.00(10.00)\n",
      "Iter 0923 | Time 35.3488(33.9185) | Bit/dim 1.1751(1.1804) | Xent 0.0466(0.0541) | Loss 1.1984(1.2075) | Error 0.0150(0.0169) Steps 446(443.62) | Grad Norm 0.2607(0.3224) | Total Time 10.00(10.00)\n",
      "Iter 0924 | Time 36.3254(33.9907) | Bit/dim 1.1801(1.1804) | Xent 0.0525(0.0540) | Loss 1.2063(1.2074) | Error 0.0165(0.0169) Steps 464(444.23) | Grad Norm 0.2095(0.3190) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 17.3722, Epoch Time 278.5598(243.5217), Bit/dim 1.1737(best: 1.1662), Xent 0.0306, Loss 1.1890, Error 0.0104(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0925 | Time 36.4054(34.0631) | Bit/dim 1.1799(1.1804) | Xent 0.0574(0.0541) | Loss 1.2086(1.2075) | Error 0.0174(0.0169) Steps 464(444.82) | Grad Norm 0.3349(0.3195) | Total Time 10.00(10.00)\n",
      "Iter 0926 | Time 36.2536(34.1288) | Bit/dim 1.1766(1.1803) | Xent 0.0527(0.0541) | Loss 1.2030(1.2073) | Error 0.0166(0.0169) Steps 458(445.22) | Grad Norm 0.1992(0.3159) | Total Time 10.00(10.00)\n",
      "Iter 0927 | Time 35.3600(34.1658) | Bit/dim 1.1792(1.1803) | Xent 0.0527(0.0540) | Loss 1.2056(1.2073) | Error 0.0168(0.0169) Steps 464(445.78) | Grad Norm 0.2307(0.3133) | Total Time 10.00(10.00)\n",
      "Iter 0928 | Time 36.1569(34.2255) | Bit/dim 1.1800(1.1802) | Xent 0.0520(0.0540) | Loss 1.2060(1.2072) | Error 0.0165(0.0169) Steps 464(446.33) | Grad Norm 0.2610(0.3117) | Total Time 10.00(10.00)\n",
      "Iter 0929 | Time 35.7408(34.2709) | Bit/dim 1.1796(1.1802) | Xent 0.0503(0.0539) | Loss 1.2047(1.2072) | Error 0.0158(0.0169) Steps 464(446.86) | Grad Norm 0.2429(0.3097) | Total Time 10.00(10.00)\n",
      "Iter 0930 | Time 36.5649(34.3398) | Bit/dim 1.1818(1.1803) | Xent 0.0577(0.0540) | Loss 1.2106(1.2073) | Error 0.0171(0.0169) Steps 464(447.37) | Grad Norm 0.2403(0.3076) | Total Time 10.00(10.00)\n",
      "Iter 0931 | Time 34.6562(34.3493) | Bit/dim 1.1806(1.1803) | Xent 0.0584(0.0541) | Loss 1.2098(1.2073) | Error 0.0181(0.0169) Steps 458(447.69) | Grad Norm 0.2398(0.3056) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 17.5070, Epoch Time 280.9188(244.6436), Bit/dim 1.1741(best: 1.1662), Xent 0.0300, Loss 1.1891, Error 0.0097(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0932 | Time 35.8056(34.3929) | Bit/dim 1.1803(1.1803) | Xent 0.0532(0.0541) | Loss 1.2068(1.2073) | Error 0.0146(0.0168) Steps 458(448.00) | Grad Norm 0.2627(0.3043) | Total Time 10.00(10.00)\n",
      "Iter 0933 | Time 36.0003(34.4412) | Bit/dim 1.1798(1.1803) | Xent 0.0581(0.0542) | Loss 1.2089(1.2074) | Error 0.0180(0.0169) Steps 464(448.48) | Grad Norm 0.3203(0.3047) | Total Time 10.00(10.00)\n",
      "Iter 0934 | Time 37.0408(34.5192) | Bit/dim 1.1833(1.1804) | Xent 0.0510(0.0541) | Loss 1.2088(1.2074) | Error 0.0168(0.0169) Steps 458(448.77) | Grad Norm 0.2433(0.3029) | Total Time 10.00(10.00)\n",
      "Iter 0935 | Time 37.1341(34.5976) | Bit/dim 1.1783(1.1803) | Xent 0.0514(0.0540) | Loss 1.2039(1.2073) | Error 0.0164(0.0169) Steps 458(449.04) | Grad Norm 0.2439(0.3011) | Total Time 10.00(10.00)\n",
      "Iter 0936 | Time 35.2647(34.6176) | Bit/dim 1.1753(1.1801) | Xent 0.0517(0.0540) | Loss 1.2011(1.2071) | Error 0.0168(0.0169) Steps 464(449.49) | Grad Norm 0.2410(0.2993) | Total Time 10.00(10.00)\n",
      "Iter 0937 | Time 35.3579(34.6398) | Bit/dim 1.1747(1.1800) | Xent 0.0509(0.0539) | Loss 1.2002(1.2069) | Error 0.0161(0.0168) Steps 464(449.93) | Grad Norm 0.3091(0.2996) | Total Time 10.00(10.00)\n",
      "Iter 0938 | Time 34.9712(34.6498) | Bit/dim 1.1856(1.1802) | Xent 0.0548(0.0539) | Loss 1.2129(1.2071) | Error 0.0172(0.0168) Steps 452(449.99) | Grad Norm 0.2504(0.2981) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 17.2906, Epoch Time 281.2219(245.7409), Bit/dim 1.1728(best: 1.1662), Xent 0.0301, Loss 1.1879, Error 0.0104(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0939 | Time 35.2155(34.6667) | Bit/dim 1.1794(1.1801) | Xent 0.0535(0.0539) | Loss 1.2061(1.2071) | Error 0.0161(0.0168) Steps 464(450.41) | Grad Norm 0.3396(0.2994) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 35.5786(34.6941) | Bit/dim 1.1797(1.1801) | Xent 0.0451(0.0536) | Loss 1.2022(1.2069) | Error 0.0149(0.0168) Steps 458(450.64) | Grad Norm 0.3173(0.2999) | Total Time 10.00(10.00)\n",
      "Iter 0941 | Time 34.6500(34.6928) | Bit/dim 1.1783(1.1801) | Xent 0.0529(0.0536) | Loss 1.2047(1.2069) | Error 0.0174(0.0168) Steps 452(450.68) | Grad Norm 0.3061(0.3001) | Total Time 10.00(10.00)\n",
      "Iter 0942 | Time 35.4445(34.7153) | Bit/dim 1.1767(1.1800) | Xent 0.0548(0.0536) | Loss 1.2041(1.2068) | Error 0.0179(0.0168) Steps 458(450.90) | Grad Norm 0.2646(0.2990) | Total Time 10.00(10.00)\n",
      "Iter 0943 | Time 35.1467(34.7283) | Bit/dim 1.1801(1.1800) | Xent 0.0545(0.0537) | Loss 1.2073(1.2068) | Error 0.0168(0.0168) Steps 452(450.93) | Grad Norm 0.2949(0.2989) | Total Time 10.00(10.00)\n",
      "Iter 0944 | Time 34.7700(34.7295) | Bit/dim 1.1771(1.1799) | Xent 0.0449(0.0534) | Loss 1.1995(1.2066) | Error 0.0125(0.0167) Steps 458(451.14) | Grad Norm 0.3395(0.3001) | Total Time 10.00(10.00)\n",
      "Iter 0945 | Time 36.2668(34.7756) | Bit/dim 1.1815(1.1799) | Xent 0.0580(0.0535) | Loss 1.2105(1.2067) | Error 0.0192(0.0168) Steps 452(451.17) | Grad Norm 0.2530(0.2987) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 16.9040, Epoch Time 276.2858(246.6573), Bit/dim 1.1732(best: 1.1662), Xent 0.0304, Loss 1.1884, Error 0.0099(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0946 | Time 35.7386(34.8045) | Bit/dim 1.1775(1.1798) | Xent 0.0555(0.0536) | Loss 1.2053(1.2066) | Error 0.0166(0.0168) Steps 452(451.19) | Grad Norm 0.2684(0.2978) | Total Time 10.00(10.00)\n",
      "Iter 0947 | Time 35.0747(34.8126) | Bit/dim 1.1748(1.1797) | Xent 0.0508(0.0535) | Loss 1.2002(1.2065) | Error 0.0158(0.0167) Steps 452(451.22) | Grad Norm 0.2567(0.2966) | Total Time 10.00(10.00)\n",
      "Iter 0948 | Time 34.6797(34.8086) | Bit/dim 1.1836(1.1798) | Xent 0.0536(0.0535) | Loss 1.2104(1.2066) | Error 0.0174(0.0167) Steps 452(451.24) | Grad Norm 0.2519(0.2952) | Total Time 10.00(10.00)\n",
      "Iter 0949 | Time 34.9934(34.8142) | Bit/dim 1.1744(1.1797) | Xent 0.0515(0.0534) | Loss 1.2002(1.2064) | Error 0.0161(0.0167) Steps 458(451.44) | Grad Norm 0.3322(0.2964) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 35.7796(34.8431) | Bit/dim 1.1817(1.1797) | Xent 0.0550(0.0535) | Loss 1.2092(1.2065) | Error 0.0174(0.0167) Steps 452(451.46) | Grad Norm 0.3774(0.2988) | Total Time 10.00(10.00)\n",
      "Iter 0951 | Time 36.2078(34.8841) | Bit/dim 1.1879(1.1800) | Xent 0.0537(0.0535) | Loss 1.2147(1.2067) | Error 0.0151(0.0167) Steps 464(451.84) | Grad Norm 0.3883(0.3015) | Total Time 10.00(10.00)\n",
      "Iter 0952 | Time 39.0652(35.0095) | Bit/dim 1.1817(1.1800) | Xent 0.0437(0.0532) | Loss 1.2035(1.2066) | Error 0.0146(0.0166) Steps 476(452.56) | Grad Norm 0.4003(0.3044) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 17.5815, Epoch Time 281.7617(247.7104), Bit/dim 1.1778(best: 1.1662), Xent 0.0299, Loss 1.1927, Error 0.0102(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0953 | Time 39.4888(35.1439) | Bit/dim 1.1846(1.1801) | Xent 0.0543(0.0532) | Loss 1.2118(1.2068) | Error 0.0181(0.0167) Steps 476(453.27) | Grad Norm 0.2857(0.3039) | Total Time 10.00(10.00)\n",
      "Iter 0954 | Time 38.3620(35.2404) | Bit/dim 1.1821(1.1802) | Xent 0.0548(0.0533) | Loss 1.2095(1.2069) | Error 0.0159(0.0167) Steps 476(453.95) | Grad Norm 0.4092(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 0955 | Time 38.6146(35.3417) | Bit/dim 1.1804(1.1802) | Xent 0.0484(0.0531) | Loss 1.2046(1.2068) | Error 0.0162(0.0166) Steps 476(454.61) | Grad Norm 0.4901(0.3125) | Total Time 10.00(10.00)\n",
      "Iter 0956 | Time 37.9727(35.4206) | Bit/dim 1.1793(1.1802) | Xent 0.0564(0.0532) | Loss 1.2075(1.2068) | Error 0.0172(0.0167) Steps 476(455.25) | Grad Norm 0.4129(0.3155) | Total Time 10.00(10.00)\n",
      "Iter 0957 | Time 37.0669(35.4700) | Bit/dim 1.1897(1.1805) | Xent 0.0508(0.0532) | Loss 1.2151(1.2071) | Error 0.0169(0.0167) Steps 470(455.69) | Grad Norm 0.5501(0.3226) | Total Time 10.00(10.00)\n",
      "Iter 0958 | Time 37.8720(35.5420) | Bit/dim 1.1892(1.1807) | Xent 0.0539(0.0532) | Loss 1.2162(1.2073) | Error 0.0168(0.0167) Steps 470(456.12) | Grad Norm 0.4534(0.3265) | Total Time 10.00(10.00)\n",
      "Iter 0959 | Time 39.2579(35.6535) | Bit/dim 1.1912(1.1810) | Xent 0.0585(0.0533) | Loss 1.2204(1.2077) | Error 0.0181(0.0167) Steps 470(456.54) | Grad Norm 0.2968(0.3256) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 17.6295, Epoch Time 298.6780(249.2395), Bit/dim 1.1802(best: 1.1662), Xent 0.0317, Loss 1.1961, Error 0.0112(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0960 | Time 36.8816(35.6904) | Bit/dim 1.1887(1.1813) | Xent 0.0507(0.0533) | Loss 1.2141(1.2079) | Error 0.0166(0.0167) Steps 476(457.12) | Grad Norm 0.6445(0.3352) | Total Time 10.00(10.00)\n",
      "Iter 0961 | Time 37.9766(35.7590) | Bit/dim 1.1835(1.1813) | Xent 0.0467(0.0531) | Loss 1.2068(1.2079) | Error 0.0158(0.0167) Steps 470(457.51) | Grad Norm 0.4408(0.3383) | Total Time 10.00(10.00)\n",
      "Iter 0962 | Time 38.0226(35.8269) | Bit/dim 1.1886(1.1816) | Xent 0.0485(0.0529) | Loss 1.2129(1.2080) | Error 0.0145(0.0166) Steps 476(458.06) | Grad Norm 0.3747(0.3394) | Total Time 10.00(10.00)\n",
      "Iter 0963 | Time 38.6408(35.9113) | Bit/dim 1.1850(1.1817) | Xent 0.0508(0.0529) | Loss 1.2105(1.2081) | Error 0.0149(0.0166) Steps 476(458.60) | Grad Norm 0.5369(0.3454) | Total Time 10.00(10.00)\n",
      "Iter 0964 | Time 39.2295(36.0108) | Bit/dim 1.1876(1.1818) | Xent 0.0501(0.0528) | Loss 1.2127(1.2082) | Error 0.0151(0.0165) Steps 476(459.12) | Grad Norm 0.3167(0.3445) | Total Time 10.00(10.00)\n",
      "Iter 0965 | Time 37.0466(36.0419) | Bit/dim 1.1912(1.1821) | Xent 0.0580(0.0529) | Loss 1.2202(1.2086) | Error 0.0192(0.0166) Steps 470(459.45) | Grad Norm 0.5263(0.3499) | Total Time 10.00(10.00)\n",
      "Iter 0966 | Time 39.0213(36.1313) | Bit/dim 1.1878(1.1823) | Xent 0.0574(0.0531) | Loss 1.2165(1.2088) | Error 0.0188(0.0167) Steps 482(460.13) | Grad Norm 0.3836(0.3510) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 17.8698, Epoch Time 297.0763(250.6746), Bit/dim 1.1844(best: 1.1662), Xent 0.0313, Loss 1.2001, Error 0.0109(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0967 | Time 37.3985(36.1693) | Bit/dim 1.1909(1.1826) | Xent 0.0492(0.0530) | Loss 1.2155(1.2090) | Error 0.0160(0.0167) Steps 482(460.78) | Grad Norm 0.4349(0.3535) | Total Time 10.00(10.00)\n",
      "Iter 0968 | Time 37.5640(36.2111) | Bit/dim 1.1891(1.1827) | Xent 0.0474(0.0528) | Loss 1.2128(1.2091) | Error 0.0154(0.0166) Steps 482(461.42) | Grad Norm 0.4611(0.3567) | Total Time 10.00(10.00)\n",
      "Iter 0969 | Time 37.3766(36.2461) | Bit/dim 1.1907(1.1830) | Xent 0.0482(0.0527) | Loss 1.2148(1.2093) | Error 0.0151(0.0166) Steps 476(461.86) | Grad Norm 0.3266(0.3558) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 38.1762(36.3040) | Bit/dim 1.1920(1.1833) | Xent 0.0427(0.0524) | Loss 1.2133(1.2094) | Error 0.0139(0.0165) Steps 470(462.10) | Grad Norm 0.4735(0.3593) | Total Time 10.00(10.00)\n",
      "Iter 0971 | Time 38.5136(36.3703) | Bit/dim 1.1859(1.1833) | Xent 0.0519(0.0523) | Loss 1.2118(1.2095) | Error 0.0154(0.0165) Steps 476(462.52) | Grad Norm 0.4587(0.3623) | Total Time 10.00(10.00)\n",
      "Iter 0972 | Time 39.0076(36.4494) | Bit/dim 1.1929(1.1836) | Xent 0.0612(0.0526) | Loss 1.2235(1.2099) | Error 0.0186(0.0165) Steps 470(462.74) | Grad Norm 0.4215(0.3641) | Total Time 10.00(10.00)\n",
      "Iter 0973 | Time 38.1323(36.4999) | Bit/dim 1.1903(1.1838) | Xent 0.0507(0.0526) | Loss 1.2157(1.2101) | Error 0.0154(0.0165) Steps 476(463.14) | Grad Norm 0.3745(0.3644) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 17.8174, Epoch Time 296.3592(252.0451), Bit/dim 1.1830(best: 1.1662), Xent 0.0288, Loss 1.1974, Error 0.0096(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0974 | Time 37.3498(36.5254) | Bit/dim 1.1907(1.1840) | Xent 0.0529(0.0526) | Loss 1.2172(1.2103) | Error 0.0171(0.0165) Steps 482(463.71) | Grad Norm 0.6063(0.3717) | Total Time 10.00(10.00)\n",
      "Iter 0975 | Time 38.5247(36.5854) | Bit/dim 1.1862(1.1841) | Xent 0.0464(0.0524) | Loss 1.2094(1.2103) | Error 0.0155(0.0165) Steps 476(464.07) | Grad Norm 0.2723(0.3687) | Total Time 10.00(10.00)\n",
      "Iter 0976 | Time 36.8413(36.5931) | Bit/dim 1.1948(1.1844) | Xent 0.0477(0.0522) | Loss 1.2186(1.2105) | Error 0.0159(0.0165) Steps 476(464.43) | Grad Norm 0.4942(0.3724) | Total Time 10.00(10.00)\n",
      "Iter 0977 | Time 36.9110(36.6026) | Bit/dim 1.1890(1.1846) | Xent 0.0636(0.0526) | Loss 1.2208(1.2108) | Error 0.0199(0.0166) Steps 476(464.78) | Grad Norm 0.4208(0.3739) | Total Time 10.00(10.00)\n",
      "Iter 0978 | Time 38.3657(36.6555) | Bit/dim 1.1888(1.1847) | Xent 0.0616(0.0529) | Loss 1.2196(1.2111) | Error 0.0188(0.0166) Steps 476(465.12) | Grad Norm 0.3829(0.3742) | Total Time 10.00(10.00)\n",
      "Iter 0979 | Time 39.1361(36.7299) | Bit/dim 1.1887(1.1848) | Xent 0.0502(0.0528) | Loss 1.2138(1.2112) | Error 0.0175(0.0166) Steps 470(465.26) | Grad Norm 0.3406(0.3732) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 36.7274(36.7298) | Bit/dim 1.1862(1.1848) | Xent 0.0492(0.0527) | Loss 1.2108(1.2112) | Error 0.0149(0.0166) Steps 470(465.40) | Grad Norm 0.4050(0.3741) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 17.4731, Epoch Time 294.0262(253.3045), Bit/dim 1.1823(best: 1.1662), Xent 0.0316, Loss 1.1981, Error 0.0108(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0981 | Time 37.6197(36.7565) | Bit/dim 1.1866(1.1849) | Xent 0.0530(0.0527) | Loss 1.2131(1.2112) | Error 0.0169(0.0166) Steps 470(465.54) | Grad Norm 0.3585(0.3736) | Total Time 10.00(10.00)\n",
      "Iter 0982 | Time 37.8267(36.7886) | Bit/dim 1.1897(1.1850) | Xent 0.0550(0.0527) | Loss 1.2172(1.2114) | Error 0.0159(0.0166) Steps 470(465.68) | Grad Norm 0.3775(0.3738) | Total Time 10.00(10.00)\n",
      "Iter 0983 | Time 38.2028(36.8310) | Bit/dim 1.1912(1.1852) | Xent 0.0483(0.0526) | Loss 1.2153(1.2115) | Error 0.0158(0.0166) Steps 470(465.81) | Grad Norm 0.3559(0.3732) | Total Time 10.00(10.00)\n",
      "Iter 0984 | Time 37.7296(36.8580) | Bit/dim 1.1876(1.1853) | Xent 0.0534(0.0526) | Loss 1.2143(1.2116) | Error 0.0169(0.0166) Steps 464(465.75) | Grad Norm 0.4871(0.3766) | Total Time 10.00(10.00)\n",
      "Iter 0985 | Time 37.3321(36.8722) | Bit/dim 1.1877(1.1854) | Xent 0.0595(0.0528) | Loss 1.2174(1.2118) | Error 0.0189(0.0166) Steps 482(466.24) | Grad Norm 0.4201(0.3779) | Total Time 10.00(10.00)\n",
      "Iter 0986 | Time 38.4000(36.9181) | Bit/dim 1.1952(1.1857) | Xent 0.0529(0.0528) | Loss 1.2216(1.2121) | Error 0.0170(0.0166) Steps 488(466.89) | Grad Norm 0.4051(0.3788) | Total Time 10.00(10.00)\n",
      "Iter 0987 | Time 38.0205(36.9511) | Bit/dim 1.1984(1.1860) | Xent 0.0487(0.0527) | Loss 1.2227(1.2124) | Error 0.0170(0.0167) Steps 482(467.35) | Grad Norm 0.3419(0.3777) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 17.9635, Epoch Time 295.6130(254.5738), Bit/dim 1.1912(best: 1.1662), Xent 0.0298, Loss 1.2061, Error 0.0104(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0988 | Time 37.8743(36.9788) | Bit/dim 1.1995(1.1864) | Xent 0.0550(0.0528) | Loss 1.2269(1.2128) | Error 0.0156(0.0166) Steps 476(467.61) | Grad Norm 0.3528(0.3769) | Total Time 10.00(10.00)\n",
      "Iter 0989 | Time 37.4278(36.9923) | Bit/dim 1.1940(1.1867) | Xent 0.0500(0.0527) | Loss 1.2190(1.2130) | Error 0.0159(0.0166) Steps 476(467.86) | Grad Norm 0.2987(0.3746) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 38.4131(37.0349) | Bit/dim 1.1920(1.1868) | Xent 0.0560(0.0528) | Loss 1.2200(1.2132) | Error 0.0169(0.0166) Steps 476(468.10) | Grad Norm 0.4244(0.3761) | Total Time 10.00(10.00)\n",
      "Iter 0991 | Time 38.0284(37.0647) | Bit/dim 1.1968(1.1871) | Xent 0.0558(0.0529) | Loss 1.2247(1.2136) | Error 0.0158(0.0166) Steps 476(468.34) | Grad Norm 0.4305(0.3777) | Total Time 10.00(10.00)\n",
      "Iter 0992 | Time 37.8292(37.0877) | Bit/dim 1.1981(1.1875) | Xent 0.0529(0.0529) | Loss 1.2246(1.2139) | Error 0.0179(0.0166) Steps 476(468.57) | Grad Norm 0.5418(0.3826) | Total Time 10.00(10.00)\n",
      "Iter 0993 | Time 37.6764(37.1053) | Bit/dim 1.1969(1.1877) | Xent 0.0507(0.0528) | Loss 1.2222(1.2142) | Error 0.0174(0.0166) Steps 476(468.79) | Grad Norm 0.3753(0.3824) | Total Time 10.00(10.00)\n",
      "Iter 0994 | Time 38.1209(37.1358) | Bit/dim 1.1974(1.1880) | Xent 0.0495(0.0527) | Loss 1.2222(1.2144) | Error 0.0154(0.0166) Steps 482(469.19) | Grad Norm 0.5216(0.3866) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 18.1861, Epoch Time 296.4205(255.8292), Bit/dim 1.1911(best: 1.1662), Xent 0.0327, Loss 1.2074, Error 0.0110(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0995 | Time 37.3036(37.1408) | Bit/dim 1.1988(1.1884) | Xent 0.0530(0.0527) | Loss 1.2253(1.2147) | Error 0.0180(0.0167) Steps 482(469.57) | Grad Norm 0.3401(0.3852) | Total Time 10.00(10.00)\n",
      "Iter 0996 | Time 38.2360(37.1737) | Bit/dim 1.1991(1.1887) | Xent 0.0524(0.0527) | Loss 1.2253(1.2150) | Error 0.0174(0.0167) Steps 482(469.94) | Grad Norm 0.4277(0.3865) | Total Time 10.00(10.00)\n",
      "Iter 0997 | Time 37.9316(37.1964) | Bit/dim 1.1978(1.1890) | Xent 0.0535(0.0527) | Loss 1.2246(1.2153) | Error 0.0186(0.0167) Steps 482(470.31) | Grad Norm 0.4106(0.3872) | Total Time 10.00(10.00)\n",
      "Iter 0998 | Time 40.0535(37.2821) | Bit/dim 1.1998(1.1893) | Xent 0.0574(0.0529) | Loss 1.2285(1.2157) | Error 0.0182(0.0168) Steps 482(470.66) | Grad Norm 0.4763(0.3898) | Total Time 10.00(10.00)\n",
      "Iter 0999 | Time 39.6143(37.3521) | Bit/dim 1.2013(1.1896) | Xent 0.0531(0.0529) | Loss 1.2278(1.2161) | Error 0.0161(0.0168) Steps 488(471.18) | Grad Norm 0.4863(0.3927) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 39.5633(37.4184) | Bit/dim 1.2022(1.1900) | Xent 0.0492(0.0528) | Loss 1.2268(1.2164) | Error 0.0150(0.0167) Steps 476(471.32) | Grad Norm 0.5088(0.3962) | Total Time 10.00(10.00)\n",
      "Iter 1001 | Time 37.9941(37.4357) | Bit/dim 1.2053(1.1905) | Xent 0.0505(0.0527) | Loss 1.2306(1.2168) | Error 0.0155(0.0167) Steps 476(471.46) | Grad Norm 0.5782(0.4017) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 18.6346, Epoch Time 301.9673(257.2133), Bit/dim 1.2009(best: 1.1662), Xent 0.0294, Loss 1.2156, Error 0.0093(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1002 | Time 39.5026(37.4977) | Bit/dim 1.2113(1.1911) | Xent 0.0422(0.0524) | Loss 1.2324(1.2173) | Error 0.0138(0.0166) Steps 494(472.14) | Grad Norm 0.7375(0.4118) | Total Time 10.00(10.00)\n",
      "Iter 1003 | Time 41.1772(37.6081) | Bit/dim 1.2033(1.1915) | Xent 0.0539(0.0524) | Loss 1.2303(1.2177) | Error 0.0179(0.0166) Steps 494(472.79) | Grad Norm 0.3711(0.4105) | Total Time 10.00(10.00)\n",
      "Iter 1004 | Time 41.6548(37.7295) | Bit/dim 1.2074(1.1919) | Xent 0.0494(0.0524) | Loss 1.2321(1.2181) | Error 0.0148(0.0166) Steps 494(473.43) | Grad Norm 0.4126(0.4106) | Total Time 10.00(10.00)\n",
      "Iter 1005 | Time 39.9204(37.7952) | Bit/dim 1.2104(1.1925) | Xent 0.0470(0.0522) | Loss 1.2339(1.2186) | Error 0.0146(0.0165) Steps 494(474.05) | Grad Norm 0.4434(0.4116) | Total Time 10.00(10.00)\n",
      "Iter 1006 | Time 39.9194(37.8589) | Bit/dim 1.2126(1.1931) | Xent 0.0576(0.0524) | Loss 1.2414(1.2193) | Error 0.0174(0.0165) Steps 494(474.65) | Grad Norm 0.4046(0.4114) | Total Time 10.00(10.00)\n",
      "Iter 1007 | Time 40.1084(37.9264) | Bit/dim 1.2116(1.1937) | Xent 0.0588(0.0525) | Loss 1.2409(1.2199) | Error 0.0189(0.0166) Steps 488(475.05) | Grad Norm 0.6940(0.4199) | Total Time 10.00(10.00)\n",
      "Iter 1008 | Time 39.5608(37.9755) | Bit/dim 1.2163(1.1943) | Xent 0.0507(0.0525) | Loss 1.2417(1.2206) | Error 0.0151(0.0166) Steps 494(475.62) | Grad Norm 0.7130(0.4286) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 18.3133, Epoch Time 313.0282(258.8878), Bit/dim 1.2152(best: 1.1662), Xent 0.0316, Loss 1.2310, Error 0.0099(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1009 | Time 39.2406(38.0134) | Bit/dim 1.2236(1.1952) | Xent 0.0495(0.0524) | Loss 1.2483(1.2214) | Error 0.0149(0.0165) Steps 494(476.17) | Grad Norm 0.4009(0.4278) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 39.4714(38.0571) | Bit/dim 1.2259(1.1961) | Xent 0.0503(0.0523) | Loss 1.2511(1.2223) | Error 0.0146(0.0165) Steps 488(476.52) | Grad Norm 0.7383(0.4371) | Total Time 10.00(10.00)\n",
      "Iter 1011 | Time 39.6484(38.1049) | Bit/dim 1.2267(1.1971) | Xent 0.0560(0.0524) | Loss 1.2547(1.2233) | Error 0.0192(0.0165) Steps 500(477.23) | Grad Norm 0.7071(0.4452) | Total Time 10.00(10.00)\n",
      "Iter 1012 | Time 39.6806(38.1522) | Bit/dim 1.2276(1.1980) | Xent 0.0563(0.0526) | Loss 1.2558(1.2243) | Error 0.0168(0.0165) Steps 482(477.37) | Grad Norm 0.5648(0.4488) | Total Time 10.00(10.00)\n",
      "Iter 1013 | Time 39.5197(38.1932) | Bit/dim 1.2256(1.1988) | Xent 0.0556(0.0527) | Loss 1.2534(1.2251) | Error 0.0164(0.0165) Steps 494(477.87) | Grad Norm 0.8904(0.4621) | Total Time 10.00(10.00)\n",
      "Iter 1014 | Time 38.9514(38.2159) | Bit/dim 1.2249(1.1996) | Xent 0.0462(0.0525) | Loss 1.2481(1.2258) | Error 0.0148(0.0165) Steps 482(477.99) | Grad Norm 1.4030(0.4903) | Total Time 10.00(10.00)\n",
      "Iter 1015 | Time 39.1111(38.2428) | Bit/dim 1.2266(1.2004) | Xent 0.0488(0.0524) | Loss 1.2510(1.2266) | Error 0.0151(0.0164) Steps 488(478.29) | Grad Norm 0.7042(0.4967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 18.0767, Epoch Time 306.6278(260.3200), Bit/dim 1.2188(best: 1.1662), Xent 0.0309, Loss 1.2342, Error 0.0103(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1016 | Time 39.1069(38.2687) | Bit/dim 1.2254(1.2011) | Xent 0.0525(0.0524) | Loss 1.2517(1.2273) | Error 0.0156(0.0164) Steps 488(478.58) | Grad Norm 0.8072(0.5060) | Total Time 10.00(10.00)\n",
      "Iter 1017 | Time 39.1676(38.2957) | Bit/dim 1.2274(1.2019) | Xent 0.0517(0.0523) | Loss 1.2532(1.2281) | Error 0.0142(0.0164) Steps 488(478.87) | Grad Norm 0.8843(0.5174) | Total Time 10.00(10.00)\n",
      "Iter 1018 | Time 40.2252(38.3536) | Bit/dim 1.2228(1.2026) | Xent 0.0569(0.0525) | Loss 1.2513(1.2288) | Error 0.0190(0.0164) Steps 500(479.50) | Grad Norm 0.7686(0.5249) | Total Time 10.00(10.00)\n",
      "Iter 1019 | Time 40.6415(38.4222) | Bit/dim 1.2241(1.2032) | Xent 0.0508(0.0524) | Loss 1.2494(1.2294) | Error 0.0169(0.0164) Steps 488(479.76) | Grad Norm 0.5186(0.5247) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 40.3514(38.4801) | Bit/dim 1.2258(1.2039) | Xent 0.0548(0.0525) | Loss 1.2532(1.2301) | Error 0.0158(0.0164) Steps 488(480.00) | Grad Norm 0.9378(0.5371) | Total Time 10.00(10.00)\n",
      "Iter 1021 | Time 39.9019(38.5227) | Bit/dim 1.2209(1.2044) | Xent 0.0551(0.0526) | Loss 1.2484(1.2307) | Error 0.0170(0.0164) Steps 482(480.06) | Grad Norm 1.1769(0.5563) | Total Time 10.00(10.00)\n",
      "Iter 1022 | Time 41.6716(38.6172) | Bit/dim 1.2419(1.2055) | Xent 0.0532(0.0526) | Loss 1.2685(1.2318) | Error 0.0178(0.0165) Steps 500(480.66) | Grad Norm 1.0800(0.5720) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 19.0012, Epoch Time 312.5494(261.8869), Bit/dim 1.2292(best: 1.1662), Xent 0.0332, Loss 1.2458, Error 0.0109(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1023 | Time 41.8941(38.7155) | Bit/dim 1.2345(1.2064) | Xent 0.0528(0.0526) | Loss 1.2609(1.2327) | Error 0.0161(0.0165) Steps 506(481.42) | Grad Norm 0.9590(0.5836) | Total Time 10.00(10.00)\n",
      "Iter 1024 | Time 42.4531(38.8276) | Bit/dim 1.2499(1.2077) | Xent 0.0561(0.0527) | Loss 1.2779(1.2340) | Error 0.0178(0.0165) Steps 506(482.16) | Grad Norm 0.7675(0.5891) | Total Time 10.00(10.00)\n",
      "Iter 1025 | Time 42.4550(38.9365) | Bit/dim 1.2458(1.2088) | Xent 0.0517(0.0527) | Loss 1.2716(1.2352) | Error 0.0152(0.0165) Steps 506(482.87) | Grad Norm 0.7963(0.5954) | Total Time 10.00(10.00)\n",
      "Iter 1026 | Time 43.4242(39.0711) | Bit/dim 1.2481(1.2100) | Xent 0.0548(0.0527) | Loss 1.2755(1.2364) | Error 0.0171(0.0165) Steps 500(483.39) | Grad Norm 1.1731(0.6127) | Total Time 10.00(10.00)\n",
      "Iter 1027 | Time 42.3498(39.1695) | Bit/dim 1.2495(1.2112) | Xent 0.0470(0.0526) | Loss 1.2730(1.2375) | Error 0.0151(0.0164) Steps 512(484.25) | Grad Norm 1.8404(0.6495) | Total Time 10.00(10.00)\n",
      "Iter 1028 | Time 44.1298(39.3183) | Bit/dim 1.2799(1.2133) | Xent 0.0682(0.0530) | Loss 1.3140(1.2398) | Error 0.0205(0.0166) Steps 524(485.44) | Grad Norm 2.7829(0.7135) | Total Time 10.00(10.00)\n",
      "Iter 1029 | Time 44.9409(39.4869) | Bit/dim 1.2869(1.2155) | Xent 0.0633(0.0533) | Loss 1.3185(1.2421) | Error 0.0185(0.0166) Steps 524(486.60) | Grad Norm 2.4553(0.7658) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 19.6537, Epoch Time 333.8438(264.0456), Bit/dim 1.3051(best: 1.1662), Xent 0.0355, Loss 1.3228, Error 0.0113(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1030 | Time 45.8754(39.6786) | Bit/dim 1.3126(1.2184) | Xent 0.0632(0.0536) | Loss 1.3442(1.2452) | Error 0.0178(0.0167) Steps 536(488.08) | Grad Norm 1.7900(0.7965) | Total Time 10.00(10.00)\n",
      "Iter 1031 | Time 48.2079(39.9345) | Bit/dim 1.3509(1.2224) | Xent 0.0802(0.0544) | Loss 1.3911(1.2496) | Error 0.0239(0.0169) Steps 554(490.05) | Grad Norm 2.8333(0.8576) | Total Time 10.00(10.00)\n",
      "Iter 1032 | Time 49.0107(40.2068) | Bit/dim 1.4021(1.2278) | Xent 0.0771(0.0551) | Loss 1.4407(1.2553) | Error 0.0235(0.0171) Steps 566(492.33) | Grad Norm 3.4587(0.9356) | Total Time 10.00(10.00)\n",
      "Iter 1033 | Time 53.1576(40.5953) | Bit/dim 1.5079(1.2362) | Xent 0.1239(0.0572) | Loss 1.5698(1.2647) | Error 0.0379(0.0177) Steps 584(495.08) | Grad Norm 3.7857(1.0211) | Total Time 10.00(10.00)\n",
      "Iter 1034 | Time 56.4728(41.0716) | Bit/dim 1.5869(1.2467) | Xent 0.1464(0.0599) | Loss 1.6601(1.2766) | Error 0.0487(0.0186) Steps 614(498.65) | Grad Norm 4.1674(1.1155) | Total Time 10.00(10.00)\n",
      "Iter 1035 | Time 58.8845(41.6060) | Bit/dim 1.6874(1.2599) | Xent 0.1407(0.0623) | Loss 1.7578(1.2910) | Error 0.0416(0.0193) Steps 644(503.01) | Grad Norm 3.8287(1.1969) | Total Time 10.00(10.00)\n",
      "Iter 1036 | Time 58.7483(42.1203) | Bit/dim 1.7086(1.2734) | Xent 0.1702(0.0655) | Loss 1.7937(1.3061) | Error 0.0561(0.0204) Steps 638(507.06) | Grad Norm 14.2973(1.5899) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 23.3106, Epoch Time 406.3561(268.3149), Bit/dim 2.3951(best: 1.1662), Xent 0.5925, Loss 2.6914, Error 0.1219(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1037 | Time 61.7310(42.7086) | Bit/dim 2.3953(1.3070) | Xent 0.7276(0.0854) | Loss 2.7591(1.3497) | Error 0.1558(0.0245) Steps 650(511.35) | Grad Norm 36.8437(2.6475) | Total Time 10.00(10.00)\n",
      "Iter 1038 | Time 62.4111(43.2997) | Bit/dim 1.6901(1.3185) | Xent 0.3696(0.0939) | Loss 1.8748(1.3655) | Error 0.1082(0.0270) Steps 662(515.87) | Grad Norm 24.1445(3.2925) | Total Time 10.00(10.00)\n",
      "Iter 1039 | Time 60.5445(43.8170) | Bit/dim 1.7005(1.3300) | Xent 0.2051(0.0972) | Loss 1.8031(1.3786) | Error 0.0645(0.0281) Steps 650(519.89) | Grad Norm 11.0197(3.5243) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 58.9288(44.2704) | Bit/dim 1.6404(1.3393) | Xent 0.1409(0.0985) | Loss 1.7108(1.3886) | Error 0.0425(0.0286) Steps 638(523.44) | Grad Norm 5.3900(3.5802) | Total Time 10.00(10.00)\n",
      "Iter 1041 | Time 58.5435(44.6986) | Bit/dim 1.6457(1.3485) | Xent 0.1382(0.0997) | Loss 1.7148(1.3983) | Error 0.0423(0.0290) Steps 632(526.69) | Grad Norm 4.7344(3.6149) | Total Time 10.00(10.00)\n",
      "Iter 1042 | Time 59.0960(45.1305) | Bit/dim 1.6864(1.3586) | Xent 0.1979(0.1027) | Loss 1.7853(1.4100) | Error 0.0626(0.0300) Steps 632(529.85) | Grad Norm 6.5722(3.7036) | Total Time 10.00(10.00)\n",
      "Iter 1043 | Time 60.8148(45.6010) | Bit/dim 1.6978(1.3688) | Xent 0.1978(0.1055) | Loss 1.7967(1.4216) | Error 0.0616(0.0309) Steps 644(533.28) | Grad Norm 5.9245(3.7702) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 22.5461, Epoch Time 457.4299(273.9883), Bit/dim 1.6688(best: 1.1662), Xent 0.1041, Loss 1.7208, Error 0.0339(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1044 | Time 61.5921(46.0807) | Bit/dim 1.6724(1.3779) | Xent 0.1938(0.1082) | Loss 1.7693(1.4320) | Error 0.0606(0.0318) Steps 638(536.42) | Grad Norm 3.0214(3.7478) | Total Time 10.00(10.00)\n",
      "Iter 1045 | Time 60.3989(46.5103) | Bit/dim 1.6655(1.3865) | Xent 0.1629(0.1098) | Loss 1.7469(1.4414) | Error 0.0509(0.0324) Steps 644(539.65) | Grad Norm 4.2353(3.7624) | Total Time 10.00(10.00)\n",
      "Iter 1046 | Time 62.2937(46.9838) | Bit/dim 1.6544(1.3946) | Xent 0.1528(0.1111) | Loss 1.7308(1.4501) | Error 0.0495(0.0329) Steps 668(543.50) | Grad Norm 4.3372(3.7796) | Total Time 10.00(10.00)\n",
      "Iter 1047 | Time 60.5802(47.3917) | Bit/dim 1.6443(1.4020) | Xent 0.1481(0.1122) | Loss 1.7183(1.4582) | Error 0.0466(0.0333) Steps 656(546.87) | Grad Norm 2.6600(3.7460) | Total Time 10.00(10.00)\n",
      "Iter 1048 | Time 61.2056(47.8061) | Bit/dim 1.6353(1.4090) | Xent 0.1506(0.1134) | Loss 1.7106(1.4657) | Error 0.0467(0.0337) Steps 662(550.33) | Grad Norm 2.6594(3.7134) | Total Time 10.00(10.00)\n",
      "Iter 1049 | Time 61.4497(48.2154) | Bit/dim 1.6233(1.4155) | Xent 0.1242(0.1137) | Loss 1.6854(1.4723) | Error 0.0377(0.0338) Steps 668(553.86) | Grad Norm 2.3175(3.6716) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 63.3867(48.6705) | Bit/dim 1.6192(1.4216) | Xent 0.1276(0.1141) | Loss 1.6830(1.4786) | Error 0.0435(0.0341) Steps 680(557.64) | Grad Norm 1.8426(3.6167) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 23.5003, Epoch Time 466.8939(279.7755), Bit/dim 1.6150(best: 1.1662), Xent 0.0649, Loss 1.6474, Error 0.0209(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1051 | Time 63.6468(49.1198) | Bit/dim 1.6231(1.4276) | Xent 0.1111(0.1140) | Loss 1.6786(1.4846) | Error 0.0349(0.0341) Steps 674(561.13) | Grad Norm 2.9215(3.5958) | Total Time 10.00(10.00)\n",
      "Iter 1052 | Time 62.4740(49.5205) | Bit/dim 1.6107(1.4331) | Xent 0.1113(0.1139) | Loss 1.6664(1.4901) | Error 0.0353(0.0342) Steps 674(564.52) | Grad Norm 2.3358(3.5580) | Total Time 10.00(10.00)\n",
      "Iter 1053 | Time 64.8184(49.9794) | Bit/dim 1.6219(1.4388) | Xent 0.1167(0.1140) | Loss 1.6803(1.4958) | Error 0.0377(0.0343) Steps 668(567.62) | Grad Norm 1.7847(3.5048) | Total Time 10.00(10.00)\n",
      "Iter 1054 | Time 66.5366(50.4761) | Bit/dim 1.6274(1.4444) | Xent 0.1192(0.1142) | Loss 1.6870(1.5015) | Error 0.0370(0.0344) Steps 692(571.35) | Grad Norm 2.4350(3.4727) | Total Time 10.00(10.00)\n",
      "Iter 1055 | Time 65.4202(50.9244) | Bit/dim 1.6255(1.4499) | Xent 0.1042(0.1139) | Loss 1.6776(1.5068) | Error 0.0327(0.0343) Steps 680(574.61) | Grad Norm 2.3329(3.4385) | Total Time 10.00(10.00)\n",
      "Iter 1056 | Time 63.8723(51.3129) | Bit/dim 1.6114(1.4547) | Xent 0.1171(0.1140) | Loss 1.6700(1.5117) | Error 0.0343(0.0343) Steps 698(578.31) | Grad Norm 1.6033(3.3835) | Total Time 10.00(10.00)\n",
      "Iter 1057 | Time 64.8435(51.7188) | Bit/dim 1.6032(1.4592) | Xent 0.1183(0.1141) | Loss 1.6624(1.5162) | Error 0.0380(0.0344) Steps 698(581.90) | Grad Norm 1.9493(3.3405) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 24.3929, Epoch Time 488.7362(286.0443), Bit/dim 1.5985(best: 1.1662), Xent 0.0614, Loss 1.6292, Error 0.0185(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1058 | Time 64.6145(52.1057) | Bit/dim 1.6023(1.4635) | Xent 0.1156(0.1142) | Loss 1.6601(1.5206) | Error 0.0374(0.0345) Steps 704(585.57) | Grad Norm 1.9387(3.2984) | Total Time 10.00(10.00)\n",
      "Iter 1059 | Time 66.4423(52.5358) | Bit/dim 1.6032(1.4677) | Xent 0.1131(0.1141) | Loss 1.6598(1.5247) | Error 0.0373(0.0346) Steps 704(589.12) | Grad Norm 1.5621(3.2463) | Total Time 10.00(10.00)\n",
      "Iter 1060 | Time 65.8395(52.9349) | Bit/dim 1.6017(1.4717) | Xent 0.1138(0.1141) | Loss 1.6586(1.5287) | Error 0.0384(0.0347) Steps 704(592.57) | Grad Norm 1.5618(3.1958) | Total Time 10.00(10.00)\n",
      "Iter 1061 | Time 65.4233(53.3095) | Bit/dim 1.6084(1.4758) | Xent 0.1170(0.1142) | Loss 1.6669(1.5329) | Error 0.0369(0.0348) Steps 710(596.09) | Grad Norm 1.8466(3.1553) | Total Time 10.00(10.00)\n",
      "Iter 1062 | Time 64.6734(53.6504) | Bit/dim 1.6042(1.4796) | Xent 0.1104(0.1141) | Loss 1.6594(1.5367) | Error 0.0339(0.0348) Steps 716(599.69) | Grad Norm 1.6081(3.1089) | Total Time 10.00(10.00)\n",
      "Iter 1063 | Time 67.8248(54.0757) | Bit/dim 1.6112(1.4836) | Xent 0.1168(0.1142) | Loss 1.6697(1.5407) | Error 0.0373(0.0348) Steps 710(603.00) | Grad Norm 1.4111(3.0580) | Total Time 10.00(10.00)\n",
      "Iter 1064 | Time 68.7148(54.5148) | Bit/dim 1.5963(1.4870) | Xent 0.1167(0.1142) | Loss 1.6546(1.5441) | Error 0.0357(0.0349) Steps 722(606.57) | Grad Norm 1.5408(3.0124) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 25.0558, Epoch Time 501.1294(292.4969), Bit/dim 1.5943(best: 1.1662), Xent 0.0634, Loss 1.6260, Error 0.0199(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1065 | Time 68.1785(54.9248) | Bit/dim 1.5996(1.4903) | Xent 0.1222(0.1145) | Loss 1.6607(1.5476) | Error 0.0413(0.0350) Steps 734(610.39) | Grad Norm 1.5940(2.9699) | Total Time 10.00(10.00)\n",
      "Iter 1066 | Time 67.7942(55.3108) | Bit/dim 1.6052(1.4938) | Xent 0.1170(0.1146) | Loss 1.6637(1.5511) | Error 0.0385(0.0352) Steps 716(613.56) | Grad Norm 1.3397(2.9210) | Total Time 10.00(10.00)\n",
      "Iter 1067 | Time 70.7328(55.7735) | Bit/dim 1.6006(1.4970) | Xent 0.1111(0.1145) | Loss 1.6562(1.5542) | Error 0.0343(0.0351) Steps 728(616.99) | Grad Norm 1.7681(2.8864) | Total Time 10.00(10.00)\n",
      "Iter 1068 | Time 69.6337(56.1893) | Bit/dim 1.6081(1.5003) | Xent 0.1044(0.1142) | Loss 1.6603(1.5574) | Error 0.0349(0.0351) Steps 728(620.32) | Grad Norm 1.8893(2.8565) | Total Time 10.00(10.00)\n",
      "Iter 1069 | Time 69.9851(56.6032) | Bit/dim 1.5962(1.5032) | Xent 0.0951(0.1136) | Loss 1.6438(1.5600) | Error 0.0298(0.0350) Steps 734(623.73) | Grad Norm 0.9114(2.7981) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 68.7431(56.9674) | Bit/dim 1.5974(1.5060) | Xent 0.0984(0.1131) | Loss 1.6466(1.5626) | Error 0.0314(0.0348) Steps 740(627.22) | Grad Norm 1.6608(2.7640) | Total Time 10.00(10.00)\n",
      "Iter 1071 | Time 70.9034(57.3855) | Bit/dim 1.5915(1.5086) | Xent 0.1073(0.1129) | Loss 1.6451(1.5651) | Error 0.0351(0.0349) Steps 740(630.60) | Grad Norm 1.4856(2.7257) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 25.2492, Epoch Time 523.7111(299.4333), Bit/dim 1.5852(best: 1.1662), Xent 0.0555, Loss 1.6130, Error 0.0163(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1072 | Time 70.0048(57.7640) | Bit/dim 1.5911(1.5111) | Xent 0.0918(0.1123) | Loss 1.6370(1.5672) | Error 0.0276(0.0346) Steps 746(634.06) | Grad Norm 0.8424(2.6692) | Total Time 10.00(10.00)\n",
      "Iter 1073 | Time 71.5736(58.1783) | Bit/dim 1.5844(1.5133) | Xent 0.1011(0.1120) | Loss 1.6350(1.5693) | Error 0.0312(0.0345) Steps 746(637.42) | Grad Norm 1.2802(2.6275) | Total Time 10.00(10.00)\n",
      "Iter 1074 | Time 69.5987(58.5209) | Bit/dim 1.5960(1.5158) | Xent 0.0907(0.1113) | Loss 1.6414(1.5714) | Error 0.0280(0.0343) Steps 740(640.50) | Grad Norm 1.3614(2.5895) | Total Time 10.00(10.00)\n",
      "Iter 1075 | Time 69.7951(58.8592) | Bit/dim 1.5849(1.5178) | Xent 0.0949(0.1108) | Loss 1.6324(1.5733) | Error 0.0300(0.0342) Steps 740(643.49) | Grad Norm 1.0827(2.5443) | Total Time 10.00(10.00)\n",
      "Iter 1076 | Time 69.6926(59.1842) | Bit/dim 1.5830(1.5198) | Xent 0.1053(0.1107) | Loss 1.6357(1.5751) | Error 0.0335(0.0342) Steps 740(646.38) | Grad Norm 1.2709(2.5061) | Total Time 10.00(10.00)\n",
      "Iter 1077 | Time 68.2184(59.4552) | Bit/dim 1.5779(1.5215) | Xent 0.0871(0.1100) | Loss 1.6215(1.5765) | Error 0.0295(0.0340) Steps 728(648.83) | Grad Norm 1.5802(2.4783) | Total Time 10.00(10.00)\n",
      "Iter 1078 | Time 72.2518(59.8391) | Bit/dim 1.5802(1.5233) | Xent 0.0964(0.1096) | Loss 1.6284(1.5781) | Error 0.0285(0.0339) Steps 752(651.92) | Grad Norm 0.9635(2.4329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 25.4535, Epoch Time 528.9847(306.3198), Bit/dim 1.5756(best: 1.1662), Xent 0.0521, Loss 1.6017, Error 0.0155(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1079 | Time 68.8306(60.1088) | Bit/dim 1.5772(1.5249) | Xent 0.0935(0.1091) | Loss 1.6240(1.5795) | Error 0.0279(0.0337) Steps 734(654.39) | Grad Norm 1.6654(2.4099) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 70.3397(60.4158) | Bit/dim 1.5816(1.5266) | Xent 0.0974(0.1087) | Loss 1.6303(1.5810) | Error 0.0315(0.0336) Steps 758(657.50) | Grad Norm 1.1939(2.3734) | Total Time 10.00(10.00)\n",
      "Iter 1081 | Time 71.5373(60.7494) | Bit/dim 1.5715(1.5280) | Xent 0.0863(0.1081) | Loss 1.6146(1.5820) | Error 0.0268(0.0334) Steps 758(660.51) | Grad Norm 1.1866(2.3378) | Total Time 10.00(10.00)\n",
      "Iter 1082 | Time 72.4241(61.0996) | Bit/dim 1.5725(1.5293) | Xent 0.0844(0.1073) | Loss 1.6147(1.5830) | Error 0.0256(0.0332) Steps 746(663.07) | Grad Norm 1.3818(2.3091) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_8K_drop_0_5_run2 --resume ../experiments_published/cnf_conditional_8K_drop_0_5_run2/epoch_115_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
