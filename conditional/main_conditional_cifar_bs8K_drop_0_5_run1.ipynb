{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2401 | Time 113.9257(62.2032) | Bit/dim 3.7014(3.6966) | Xent 0.2028(0.2187) | Loss 3.8028(3.8060) | Error 0.0726(0.0784) Steps 670(669.50) | Grad Norm 7.7385(4.6959) | Total Time 14.00(14.00)\n",
      "Iter 2402 | Time 63.7927(62.2509) | Bit/dim 3.6937(3.6965) | Xent 0.1886(0.2178) | Loss 3.7880(3.8055) | Error 0.0665(0.0780) Steps 676(669.70) | Grad Norm 6.1657(4.7400) | Total Time 14.00(14.00)\n",
      "Iter 2403 | Time 61.0305(62.2143) | Bit/dim 3.6834(3.6961) | Xent 0.1732(0.2165) | Loss 3.7700(3.8044) | Error 0.0620(0.0776) Steps 676(669.89) | Grad Norm 3.9711(4.7169) | Total Time 14.00(14.00)\n",
      "Iter 2404 | Time 59.0443(62.1192) | Bit/dim 3.6903(3.6960) | Xent 0.1613(0.2148) | Loss 3.7710(3.8034) | Error 0.0555(0.0769) Steps 676(670.07) | Grad Norm 1.5730(4.6226) | Total Time 14.00(14.00)\n",
      "Iter 2405 | Time 61.4749(62.0998) | Bit/dim 3.6879(3.6957) | Xent 0.1806(0.2138) | Loss 3.7782(3.8026) | Error 0.0651(0.0765) Steps 670(670.07) | Grad Norm 2.6534(4.5635) | Total Time 14.00(14.00)\n",
      "Iter 2406 | Time 60.0261(62.0376) | Bit/dim 3.6981(3.6958) | Xent 0.1769(0.2127) | Loss 3.7865(3.8021) | Error 0.0621(0.0761) Steps 682(670.43) | Grad Norm 3.5103(4.5319) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 40.0854, Epoch Time 476.1571(404.2424), Bit/dim 3.7094(best: inf), Xent 2.2286, Loss 4.8237, Error 0.4255(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 72.0757(62.3388) | Bit/dim 3.6901(3.6956) | Xent 0.1791(0.2117) | Loss 3.7796(3.8015) | Error 0.0641(0.0758) Steps 682(670.77) | Grad Norm 4.4392(4.5291) | Total Time 14.00(14.00)\n",
      "Iter 2408 | Time 63.0221(62.3593) | Bit/dim 3.6906(3.6955) | Xent 0.1773(0.2107) | Loss 3.7793(3.8008) | Error 0.0631(0.0754) Steps 658(670.39) | Grad Norm 4.5451(4.5296) | Total Time 14.00(14.00)\n",
      "Iter 2409 | Time 61.7809(62.3419) | Bit/dim 3.6971(3.6955) | Xent 0.1652(0.2093) | Loss 3.7797(3.8002) | Error 0.0583(0.0749) Steps 670(670.38) | Grad Norm 3.0948(4.4866) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 61.6930(62.3224) | Bit/dim 3.6929(3.6954) | Xent 0.1676(0.2081) | Loss 3.7767(3.7995) | Error 0.0579(0.0744) Steps 676(670.55) | Grad Norm 2.2004(4.4180) | Total Time 14.00(14.00)\n",
      "Iter 2411 | Time 62.9432(62.3411) | Bit/dim 3.6845(3.6951) | Xent 0.1662(0.2068) | Loss 3.7676(3.7985) | Error 0.0573(0.0738) Steps 676(670.71) | Grad Norm 1.2616(4.3233) | Total Time 14.00(14.00)\n",
      "Iter 2412 | Time 62.5674(62.3479) | Bit/dim 3.6858(3.6948) | Xent 0.1610(0.2054) | Loss 3.7663(3.7975) | Error 0.0585(0.0734) Steps 670(670.69) | Grad Norm 2.6120(4.2719) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 24.6561, Epoch Time 425.1608(404.8699), Bit/dim 3.7064(best: 3.7094), Xent 2.2234, Loss 4.8180, Error 0.4217(best: 0.4255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 61.2974(62.3163) | Bit/dim 3.6830(3.6945) | Xent 0.1618(0.2041) | Loss 3.7639(3.7965) | Error 0.0574(0.0729) Steps 676(670.85) | Grad Norm 3.3449(4.2441) | Total Time 14.00(14.00)\n",
      "Iter 2414 | Time 61.7628(62.2997) | Bit/dim 3.6790(3.6940) | Xent 0.1710(0.2031) | Loss 3.7645(3.7956) | Error 0.0591(0.0725) Steps 676(671.00) | Grad Norm 3.2309(4.2137) | Total Time 14.00(14.00)\n",
      "Iter 2415 | Time 58.4694(62.1848) | Bit/dim 3.6924(3.6940) | Xent 0.1568(0.2017) | Loss 3.7708(3.7948) | Error 0.0549(0.0720) Steps 664(670.79) | Grad Norm 2.8899(4.1740) | Total Time 14.00(14.00)\n",
      "Iter 2416 | Time 59.9087(62.1165) | Bit/dim 3.6828(3.6936) | Xent 0.1613(0.2005) | Loss 3.7635(3.7939) | Error 0.0511(0.0713) Steps 676(670.95) | Grad Norm 1.4838(4.0933) | Total Time 14.00(14.00)\n",
      "Iter 2417 | Time 59.2158(62.0295) | Bit/dim 3.6927(3.6936) | Xent 0.1439(0.1988) | Loss 3.7646(3.7930) | Error 0.0511(0.0707) Steps 670(670.92) | Grad Norm 1.1819(4.0060) | Total Time 14.00(14.00)\n",
      "Iter 2418 | Time 59.5527(61.9552) | Bit/dim 3.7047(3.6939) | Xent 0.1533(0.1975) | Loss 3.7814(3.7927) | Error 0.0563(0.0703) Steps 676(671.07) | Grad Norm 1.4953(3.9306) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 24.1604, Epoch Time 400.4033(404.7359), Bit/dim 3.7064(best: 3.7064), Xent 2.2337, Loss 4.8233, Error 0.4184(best: 0.4217)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 59.0280(61.8674) | Bit/dim 3.6954(3.6940) | Xent 0.1608(0.1964) | Loss 3.7758(3.7922) | Error 0.0554(0.0698) Steps 676(671.22) | Grad Norm 2.3909(3.8845) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 58.3619(61.7622) | Bit/dim 3.6687(3.6932) | Xent 0.1618(0.1953) | Loss 3.7496(3.7909) | Error 0.0549(0.0694) Steps 676(671.37) | Grad Norm 2.5950(3.8458) | Total Time 14.00(14.00)\n",
      "Iter 2421 | Time 60.0727(61.7115) | Bit/dim 3.6787(3.6928) | Xent 0.1524(0.1940) | Loss 3.7549(3.7898) | Error 0.0549(0.0690) Steps 670(671.32) | Grad Norm 1.8427(3.7857) | Total Time 14.00(14.00)\n",
      "Iter 2422 | Time 62.0855(61.7228) | Bit/dim 3.6953(3.6929) | Xent 0.1457(0.1926) | Loss 3.7682(3.7892) | Error 0.0519(0.0684) Steps 664(671.10) | Grad Norm 1.5406(3.7183) | Total Time 14.00(14.00)\n",
      "Iter 2423 | Time 61.9729(61.7303) | Bit/dim 3.6908(3.6928) | Xent 0.1470(0.1912) | Loss 3.7642(3.7884) | Error 0.0490(0.0679) Steps 658(670.71) | Grad Norm 1.0498(3.6383) | Total Time 14.00(14.00)\n",
      "Iter 2424 | Time 60.6189(61.6969) | Bit/dim 3.7025(3.6931) | Xent 0.1443(0.1898) | Loss 3.7746(3.7880) | Error 0.0504(0.0673) Steps 658(670.33) | Grad Norm 1.3276(3.5689) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 24.3532, Epoch Time 402.5692(404.6709), Bit/dim 3.7070(best: 3.7064), Xent 2.3053, Loss 4.8597, Error 0.4237(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 58.1462(61.5904) | Bit/dim 3.6864(3.6929) | Xent 0.1463(0.1885) | Loss 3.7596(3.7871) | Error 0.0520(0.0669) Steps 670(670.32) | Grad Norm 1.9039(3.5190) | Total Time 14.00(14.00)\n",
      "Iter 2426 | Time 62.6539(61.6223) | Bit/dim 3.6974(3.6930) | Xent 0.1478(0.1873) | Loss 3.7713(3.7867) | Error 0.0536(0.0665) Steps 676(670.49) | Grad Norm 2.1004(3.4764) | Total Time 14.00(14.00)\n",
      "Iter 2427 | Time 58.7043(61.5348) | Bit/dim 3.6958(3.6931) | Xent 0.1488(0.1861) | Loss 3.7702(3.7862) | Error 0.0506(0.0660) Steps 682(670.84) | Grad Norm 1.7019(3.4232) | Total Time 14.00(14.00)\n",
      "Iter 2428 | Time 60.5565(61.5054) | Bit/dim 3.6928(3.6931) | Xent 0.1494(0.1850) | Loss 3.7675(3.7856) | Error 0.0486(0.0655) Steps 670(670.81) | Grad Norm 1.2755(3.3588) | Total Time 14.00(14.00)\n",
      "Iter 2429 | Time 60.7505(61.4828) | Bit/dim 3.6796(3.6927) | Xent 0.1542(0.1841) | Loss 3.7567(3.7847) | Error 0.0516(0.0651) Steps 670(670.79) | Grad Norm 1.1252(3.2918) | Total Time 14.00(14.00)\n",
      "Iter 2430 | Time 60.4891(61.4530) | Bit/dim 3.6822(3.6924) | Xent 0.1514(0.1831) | Loss 3.7579(3.7839) | Error 0.0504(0.0646) Steps 676(670.94) | Grad Norm 1.2777(3.2313) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 24.2768, Epoch Time 401.3314(404.5708), Bit/dim 3.7072(best: 3.7064), Xent 2.3109, Loss 4.8627, Error 0.4237(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 61.2671(61.4474) | Bit/dim 3.6975(3.6925) | Xent 0.1543(0.1823) | Loss 3.7746(3.7837) | Error 0.0547(0.0643) Steps 664(670.73) | Grad Norm 1.2928(3.1732) | Total Time 14.00(14.00)\n",
      "Iter 2432 | Time 59.3280(61.3838) | Bit/dim 3.6989(3.6927) | Xent 0.1455(0.1812) | Loss 3.7717(3.7833) | Error 0.0483(0.0638) Steps 670(670.71) | Grad Norm 1.6275(3.1268) | Total Time 14.00(14.00)\n",
      "Iter 2433 | Time 61.0138(61.3727) | Bit/dim 3.6887(3.6926) | Xent 0.1408(0.1799) | Loss 3.7591(3.7826) | Error 0.0487(0.0634) Steps 664(670.51) | Grad Norm 1.3180(3.0726) | Total Time 14.00(14.00)\n",
      "Iter 2434 | Time 59.9677(61.3306) | Bit/dim 3.6829(3.6923) | Xent 0.1446(0.1789) | Loss 3.7552(3.7818) | Error 0.0527(0.0631) Steps 664(670.32) | Grad Norm 0.9023(3.0074) | Total Time 14.00(14.00)\n",
      "Iter 2435 | Time 60.5724(61.3078) | Bit/dim 3.6826(3.6920) | Xent 0.1368(0.1776) | Loss 3.7510(3.7808) | Error 0.0489(0.0627) Steps 676(670.49) | Grad Norm 1.0030(2.9473) | Total Time 14.00(14.00)\n",
      "Iter 2436 | Time 59.7438(61.2609) | Bit/dim 3.6800(3.6917) | Xent 0.1531(0.1769) | Loss 3.7566(3.7801) | Error 0.0540(0.0624) Steps 670(670.47) | Grad Norm 1.0820(2.8914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 24.5334, Epoch Time 402.8032(404.5177), Bit/dim 3.7060(best: 3.7064), Xent 2.3348, Loss 4.8733, Error 0.4220(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 58.3131(61.1725) | Bit/dim 3.6971(3.6918) | Xent 0.1418(0.1758) | Loss 3.7680(3.7797) | Error 0.0503(0.0620) Steps 664(670.28) | Grad Norm 1.0088(2.8349) | Total Time 14.00(14.00)\n",
      "Iter 2438 | Time 60.8948(61.1641) | Bit/dim 3.6898(3.6918) | Xent 0.1416(0.1748) | Loss 3.7606(3.7792) | Error 0.0469(0.0616) Steps 664(670.09) | Grad Norm 1.3595(2.7906) | Total Time 14.00(14.00)\n",
      "Iter 2439 | Time 58.5634(61.0861) | Bit/dim 3.6908(3.6917) | Xent 0.1419(0.1738) | Loss 3.7618(3.7786) | Error 0.0517(0.0613) Steps 676(670.27) | Grad Norm 1.1456(2.7413) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 62.1619(61.1184) | Bit/dim 3.6888(3.6916) | Xent 0.1552(0.1733) | Loss 3.7664(3.7783) | Error 0.0547(0.0611) Steps 676(670.44) | Grad Norm 1.0042(2.6892) | Total Time 14.00(14.00)\n",
      "Iter 2441 | Time 62.4673(61.1588) | Bit/dim 3.6804(3.6913) | Xent 0.1524(0.1726) | Loss 3.7566(3.7776) | Error 0.0544(0.0609) Steps 676(670.61) | Grad Norm 1.2207(2.6451) | Total Time 14.00(14.00)\n",
      "Iter 2442 | Time 57.9676(61.0631) | Bit/dim 3.6894(3.6913) | Xent 0.1485(0.1719) | Loss 3.7637(3.7772) | Error 0.0507(0.0606) Steps 670(670.59) | Grad Norm 1.5801(2.6132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 24.4630, Epoch Time 400.7815(404.4056), Bit/dim 3.7056(best: 3.7060), Xent 2.3309, Loss 4.8710, Error 0.4276(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 63.2361(61.1283) | Bit/dim 3.6885(3.6912) | Xent 0.1416(0.1710) | Loss 3.7593(3.7767) | Error 0.0515(0.0603) Steps 664(670.39) | Grad Norm 1.0230(2.5654) | Total Time 14.00(14.00)\n",
      "Iter 2444 | Time 60.6255(61.1132) | Bit/dim 3.6852(3.6910) | Xent 0.1444(0.1702) | Loss 3.7574(3.7761) | Error 0.0483(0.0599) Steps 664(670.20) | Grad Norm 0.7815(2.5119) | Total Time 14.00(14.00)\n",
      "Iter 2445 | Time 58.6305(61.0387) | Bit/dim 3.6841(3.6908) | Xent 0.1551(0.1697) | Loss 3.7616(3.7757) | Error 0.0546(0.0598) Steps 670(670.19) | Grad Norm 0.7851(2.4601) | Total Time 14.00(14.00)\n",
      "Iter 2446 | Time 59.5452(60.9939) | Bit/dim 3.6874(3.6907) | Xent 0.1408(0.1689) | Loss 3.7578(3.7751) | Error 0.0485(0.0594) Steps 670(670.19) | Grad Norm 1.0462(2.4177) | Total Time 14.00(14.00)\n",
      "Iter 2447 | Time 61.0194(60.9947) | Bit/dim 3.6904(3.6907) | Xent 0.1462(0.1682) | Loss 3.7635(3.7748) | Error 0.0490(0.0591) Steps 670(670.18) | Grad Norm 1.2558(2.3828) | Total Time 14.00(14.00)\n",
      "Iter 2448 | Time 60.5900(60.9825) | Bit/dim 3.6907(3.6907) | Xent 0.1460(0.1675) | Loss 3.7637(3.7744) | Error 0.0510(0.0589) Steps 670(670.18) | Grad Norm 1.2088(2.3476) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 24.0789, Epoch Time 403.8991(404.3904), Bit/dim 3.7067(best: 3.7056), Xent 2.3524, Loss 4.8829, Error 0.4251(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 61.6540(61.0027) | Bit/dim 3.6795(3.6903) | Xent 0.1427(0.1668) | Loss 3.7509(3.7737) | Error 0.0503(0.0586) Steps 670(670.17) | Grad Norm 0.7990(2.3012) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 61.2710(61.0107) | Bit/dim 3.6879(3.6903) | Xent 0.1463(0.1662) | Loss 3.7610(3.7734) | Error 0.0491(0.0583) Steps 670(670.16) | Grad Norm 0.7361(2.2542) | Total Time 14.00(14.00)\n",
      "Iter 2451 | Time 59.9923(60.9802) | Bit/dim 3.6874(3.6902) | Xent 0.1383(0.1653) | Loss 3.7566(3.7728) | Error 0.0480(0.0580) Steps 670(670.16) | Grad Norm 0.7600(2.2094) | Total Time 14.00(14.00)\n",
      "Iter 2452 | Time 61.2909(60.9895) | Bit/dim 3.6910(3.6902) | Xent 0.1541(0.1650) | Loss 3.7681(3.7727) | Error 0.0515(0.0578) Steps 676(670.34) | Grad Norm 0.9280(2.1709) | Total Time 14.00(14.00)\n",
      "Iter 2453 | Time 58.3099(60.9091) | Bit/dim 3.6867(3.6901) | Xent 0.1491(0.1645) | Loss 3.7612(3.7724) | Error 0.0529(0.0577) Steps 664(670.15) | Grad Norm 1.1523(2.1404) | Total Time 14.00(14.00)\n",
      "Iter 2454 | Time 62.6772(60.9622) | Bit/dim 3.6930(3.6902) | Xent 0.1389(0.1638) | Loss 3.7625(3.7721) | Error 0.0449(0.0573) Steps 676(670.32) | Grad Norm 1.0656(2.1081) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 24.4178, Epoch Time 405.8283(404.4336), Bit/dim 3.7061(best: 3.7056), Xent 2.3311, Loss 4.8717, Error 0.4247(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 60.6898(60.9540) | Bit/dim 3.6887(3.6901) | Xent 0.1424(0.1631) | Loss 3.7600(3.7717) | Error 0.0509(0.0571) Steps 670(670.31) | Grad Norm 1.3885(2.0866) | Total Time 14.00(14.00)\n",
      "Iter 2456 | Time 60.8316(60.9503) | Bit/dim 3.6841(3.6900) | Xent 0.1510(0.1627) | Loss 3.7596(3.7713) | Error 0.0535(0.0570) Steps 676(670.48) | Grad Norm 1.0504(2.0555) | Total Time 14.00(14.00)\n",
      "Iter 2457 | Time 59.9592(60.9206) | Bit/dim 3.6826(3.6897) | Xent 0.1387(0.1620) | Loss 3.7520(3.7708) | Error 0.0466(0.0567) Steps 670(670.47) | Grad Norm 1.1852(2.0294) | Total Time 14.00(14.00)\n",
      "Iter 2458 | Time 61.2987(60.9319) | Bit/dim 3.6913(3.6898) | Xent 0.1308(0.1611) | Loss 3.7567(3.7703) | Error 0.0469(0.0564) Steps 670(670.45) | Grad Norm 1.0853(2.0010) | Total Time 14.00(14.00)\n",
      "Iter 2459 | Time 61.6608(60.9538) | Bit/dim 3.6951(3.6899) | Xent 0.1450(0.1606) | Loss 3.7675(3.7703) | Error 0.0506(0.0562) Steps 664(670.26) | Grad Norm 1.0097(1.9713) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 62.9733(61.0144) | Bit/dim 3.6799(3.6896) | Xent 0.1444(0.1601) | Loss 3.7521(3.7697) | Error 0.0477(0.0560) Steps 676(670.43) | Grad Norm 1.1138(1.9456) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 23.9924, Epoch Time 407.4915(404.5253), Bit/dim 3.7051(best: 3.7056), Xent 2.3703, Loss 4.8903, Error 0.4294(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 60.9143(61.0114) | Bit/dim 3.6883(3.6896) | Xent 0.1414(0.1596) | Loss 3.7590(3.7694) | Error 0.0481(0.0557) Steps 676(670.60) | Grad Norm 0.7517(1.9098) | Total Time 14.00(14.00)\n",
      "Iter 2462 | Time 60.4491(60.9945) | Bit/dim 3.6891(3.6896) | Xent 0.1498(0.1593) | Loss 3.7640(3.7692) | Error 0.0523(0.0556) Steps 676(670.76) | Grad Norm 0.7673(1.8755) | Total Time 14.00(14.00)\n",
      "Iter 2463 | Time 59.9122(60.9620) | Bit/dim 3.6916(3.6897) | Xent 0.1433(0.1588) | Loss 3.7632(3.7690) | Error 0.0471(0.0554) Steps 664(670.56) | Grad Norm 0.7746(1.8425) | Total Time 14.00(14.00)\n",
      "Iter 2464 | Time 59.0006(60.9032) | Bit/dim 3.6913(3.6897) | Xent 0.1402(0.1582) | Loss 3.7614(3.7688) | Error 0.0506(0.0552) Steps 676(670.72) | Grad Norm 0.9711(1.8163) | Total Time 14.00(14.00)\n",
      "Iter 2465 | Time 62.9508(60.9646) | Bit/dim 3.6794(3.6894) | Xent 0.1406(0.1577) | Loss 3.7497(3.7682) | Error 0.0490(0.0550) Steps 682(671.06) | Grad Norm 0.9012(1.7889) | Total Time 14.00(14.00)\n",
      "Iter 2466 | Time 61.3095(60.9750) | Bit/dim 3.6867(3.6893) | Xent 0.1454(0.1573) | Loss 3.7593(3.7680) | Error 0.0531(0.0550) Steps 664(670.85) | Grad Norm 1.2414(1.7724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 24.0884, Epoch Time 404.8351(404.5346), Bit/dim 3.7061(best: 3.7051), Xent 2.3536, Loss 4.8829, Error 0.4261(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 59.1833(60.9212) | Bit/dim 3.6889(3.6893) | Xent 0.1502(0.1571) | Loss 3.7640(3.7679) | Error 0.0529(0.0549) Steps 676(671.00) | Grad Norm 0.8213(1.7439) | Total Time 14.00(14.00)\n",
      "Iter 2468 | Time 60.1562(60.8983) | Bit/dim 3.6873(3.6892) | Xent 0.1475(0.1568) | Loss 3.7611(3.7677) | Error 0.0517(0.0548) Steps 676(671.15) | Grad Norm 0.8082(1.7158) | Total Time 14.00(14.00)\n",
      "Iter 2469 | Time 59.6954(60.8622) | Bit/dim 3.6992(3.6895) | Xent 0.1388(0.1563) | Loss 3.7686(3.7677) | Error 0.0476(0.0546) Steps 670(671.12) | Grad Norm 0.8532(1.6900) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 60.2806(60.8447) | Bit/dim 3.6813(3.6893) | Xent 0.1486(0.1561) | Loss 3.7556(3.7673) | Error 0.0507(0.0545) Steps 670(671.08) | Grad Norm 1.2234(1.6760) | Total Time 14.00(14.00)\n",
      "Iter 2471 | Time 60.9950(60.8493) | Bit/dim 3.6946(3.6894) | Xent 0.1433(0.1557) | Loss 3.7662(3.7673) | Error 0.0479(0.0543) Steps 664(670.87) | Grad Norm 1.1666(1.6607) | Total Time 14.00(14.00)\n",
      "Iter 2472 | Time 62.0671(60.8858) | Bit/dim 3.6703(3.6889) | Xent 0.1378(0.1551) | Loss 3.7392(3.7664) | Error 0.0485(0.0541) Steps 676(671.03) | Grad Norm 0.7237(1.6326) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 24.2313, Epoch Time 402.9502(404.4871), Bit/dim 3.7058(best: 3.7051), Xent 2.3541, Loss 4.8828, Error 0.4234(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 61.7083(60.9105) | Bit/dim 3.6843(3.6887) | Xent 0.1461(0.1549) | Loss 3.7574(3.7662) | Error 0.0501(0.0540) Steps 676(671.17) | Grad Norm 1.0081(1.6138) | Total Time 14.00(14.00)\n",
      "Iter 2474 | Time 61.5188(60.9287) | Bit/dim 3.6981(3.6890) | Xent 0.1380(0.1544) | Loss 3.7670(3.7662) | Error 0.0450(0.0537) Steps 682(671.50) | Grad Norm 0.8138(1.5898) | Total Time 14.00(14.00)\n",
      "Iter 2475 | Time 61.9212(60.9585) | Bit/dim 3.6861(3.6889) | Xent 0.1367(0.1538) | Loss 3.7544(3.7658) | Error 0.0461(0.0535) Steps 658(671.09) | Grad Norm 1.1275(1.5760) | Total Time 14.00(14.00)\n",
      "Iter 2476 | Time 58.9687(60.8988) | Bit/dim 3.6803(3.6887) | Xent 0.1377(0.1533) | Loss 3.7491(3.7653) | Error 0.0471(0.0533) Steps 676(671.24) | Grad Norm 1.0543(1.5603) | Total Time 14.00(14.00)\n",
      "Iter 2477 | Time 61.3647(60.9128) | Bit/dim 3.6888(3.6887) | Xent 0.1390(0.1529) | Loss 3.7583(3.7651) | Error 0.0470(0.0531) Steps 670(671.20) | Grad Norm 1.2785(1.5519) | Total Time 14.00(14.00)\n",
      "Iter 2478 | Time 60.9727(60.9146) | Bit/dim 3.6868(3.6886) | Xent 0.1404(0.1525) | Loss 3.7571(3.7649) | Error 0.0470(0.0529) Steps 670(671.17) | Grad Norm 1.0680(1.5373) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 24.3715, Epoch Time 406.9222(404.5601), Bit/dim 3.7064(best: 3.7051), Xent 2.3958, Loss 4.9043, Error 0.4316(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2479 | Time 63.0026(60.9772) | Bit/dim 3.6976(3.6889) | Xent 0.1325(0.1519) | Loss 3.7638(3.7649) | Error 0.0447(0.0527) Steps 676(671.31) | Grad Norm 1.1448(1.5256) | Total Time 14.00(14.00)\n",
      "Iter 2480 | Time 60.9800(60.9773) | Bit/dim 3.6721(3.6884) | Xent 0.1429(0.1517) | Loss 3.7436(3.7642) | Error 0.0516(0.0527) Steps 658(670.91) | Grad Norm 0.8985(1.5068) | Total Time 14.00(14.00)\n",
      "Iter 2481 | Time 62.7579(61.0307) | Bit/dim 3.6908(3.6885) | Xent 0.1449(0.1515) | Loss 3.7632(3.7642) | Error 0.0499(0.0526) Steps 670(670.89) | Grad Norm 1.1868(1.4972) | Total Time 14.00(14.00)\n",
      "Iter 2482 | Time 58.6356(60.9589) | Bit/dim 3.6890(3.6885) | Xent 0.1356(0.1510) | Loss 3.7568(3.7640) | Error 0.0474(0.0524) Steps 676(671.04) | Grad Norm 1.5092(1.4975) | Total Time 14.00(14.00)\n",
      "Iter 2483 | Time 59.7220(60.9218) | Bit/dim 3.6920(3.6886) | Xent 0.1346(0.1505) | Loss 3.7593(3.7638) | Error 0.0466(0.0522) Steps 670(671.01) | Grad Norm 0.7937(1.4764) | Total Time 14.00(14.00)\n",
      "Iter 2484 | Time 59.8884(60.8908) | Bit/dim 3.6755(3.6882) | Xent 0.1427(0.1503) | Loss 3.7469(3.7633) | Error 0.0504(0.0522) Steps 676(671.16) | Grad Norm 0.7631(1.4550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 24.3998, Epoch Time 405.0619(404.5752), Bit/dim 3.7061(best: 3.7051), Xent 2.3641, Loss 4.8882, Error 0.4280(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2485 | Time 60.2157(60.8705) | Bit/dim 3.6816(3.6880) | Xent 0.1524(0.1503) | Loss 3.7578(3.7632) | Error 0.0511(0.0522) Steps 670(671.12) | Grad Norm 0.8926(1.4381) | Total Time 14.00(14.00)\n",
      "Iter 2486 | Time 59.1852(60.8199) | Bit/dim 3.6824(3.6878) | Xent 0.1400(0.1500) | Loss 3.7524(3.7628) | Error 0.0484(0.0520) Steps 664(670.91) | Grad Norm 1.1874(1.4306) | Total Time 14.00(14.00)\n",
      "Iter 2487 | Time 58.1868(60.7409) | Bit/dim 3.6862(3.6878) | Xent 0.1363(0.1496) | Loss 3.7543(3.7626) | Error 0.0489(0.0520) Steps 670(670.88) | Grad Norm 0.6818(1.4081) | Total Time 14.00(14.00)\n",
      "Iter 2488 | Time 61.2065(60.7549) | Bit/dim 3.6895(3.6878) | Xent 0.1348(0.1492) | Loss 3.7568(3.7624) | Error 0.0463(0.0518) Steps 670(670.86) | Grad Norm 0.7814(1.3893) | Total Time 14.00(14.00)\n",
      "Iter 2489 | Time 61.9777(60.7916) | Bit/dim 3.6983(3.6881) | Xent 0.1394(0.1489) | Loss 3.7679(3.7626) | Error 0.0523(0.0518) Steps 664(670.65) | Grad Norm 0.9002(1.3747) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 59.5585(60.7546) | Bit/dim 3.6827(3.6880) | Xent 0.1426(0.1487) | Loss 3.7541(3.7623) | Error 0.0490(0.0517) Steps 676(670.81) | Grad Norm 0.7477(1.3559) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 24.5284, Epoch Time 400.9701(404.4670), Bit/dim 3.7060(best: 3.7051), Xent 2.3965, Loss 4.9043, Error 0.4272(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2491 | Time 61.1035(60.7651) | Bit/dim 3.6903(3.6880) | Xent 0.1368(0.1483) | Loss 3.7587(3.7622) | Error 0.0466(0.0516) Steps 658(670.43) | Grad Norm 0.9060(1.3424) | Total Time 14.00(14.00)\n",
      "Iter 2492 | Time 60.3396(60.7523) | Bit/dim 3.6790(3.6878) | Xent 0.1290(0.1477) | Loss 3.7436(3.7616) | Error 0.0444(0.0513) Steps 676(670.59) | Grad Norm 0.8790(1.3285) | Total Time 14.00(14.00)\n",
      "Iter 2493 | Time 60.2499(60.7372) | Bit/dim 3.6849(3.6877) | Xent 0.1336(0.1473) | Loss 3.7518(3.7614) | Error 0.0443(0.0511) Steps 670(670.58) | Grad Norm 0.8724(1.3148) | Total Time 14.00(14.00)\n",
      "Iter 2494 | Time 60.7393(60.7373) | Bit/dim 3.6893(3.6877) | Xent 0.1358(0.1470) | Loss 3.7571(3.7612) | Error 0.0473(0.0510) Steps 664(670.38) | Grad Norm 0.7887(1.2990) | Total Time 14.00(14.00)\n",
      "Iter 2495 | Time 60.7881(60.7388) | Bit/dim 3.6943(3.6879) | Xent 0.1346(0.1466) | Loss 3.7616(3.7612) | Error 0.0451(0.0508) Steps 676(670.55) | Grad Norm 0.9442(1.2884) | Total Time 14.00(14.00)\n",
      "Iter 2496 | Time 60.5093(60.7319) | Bit/dim 3.6823(3.6878) | Xent 0.1387(0.1464) | Loss 3.7517(3.7609) | Error 0.0463(0.0507) Steps 670(670.53) | Grad Norm 0.7056(1.2709) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 24.2331, Epoch Time 403.4396(404.4362), Bit/dim 3.7059(best: 3.7051), Xent 2.3542, Loss 4.8830, Error 0.4284(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2497 | Time 60.0638(60.7119) | Bit/dim 3.6798(3.6875) | Xent 0.1447(0.1463) | Loss 3.7522(3.7607) | Error 0.0495(0.0507) Steps 670(670.52) | Grad Norm 0.8069(1.2570) | Total Time 14.00(14.00)\n",
      "Iter 2498 | Time 59.6180(60.6791) | Bit/dim 3.6908(3.6876) | Xent 0.1401(0.1461) | Loss 3.7608(3.7607) | Error 0.0487(0.0506) Steps 682(670.86) | Grad Norm 0.7355(1.2413) | Total Time 14.00(14.00)\n",
      "Iter 2499 | Time 58.8657(60.6247) | Bit/dim 3.6933(3.6878) | Xent 0.1449(0.1461) | Loss 3.7658(3.7608) | Error 0.0503(0.0506) Steps 664(670.65) | Grad Norm 0.8636(1.2300) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 60.1637(60.6108) | Bit/dim 3.6916(3.6879) | Xent 0.1358(0.1458) | Loss 3.7595(3.7608) | Error 0.0501(0.0506) Steps 676(670.81) | Grad Norm 0.8005(1.2171) | Total Time 14.00(14.00)\n",
      "Iter 2501 | Time 60.6532(60.6121) | Bit/dim 3.6825(3.6877) | Xent 0.1460(0.1458) | Loss 3.7555(3.7606) | Error 0.0499(0.0506) Steps 676(670.97) | Grad Norm 1.0619(1.2124) | Total Time 14.00(14.00)\n",
      "Iter 2502 | Time 60.4943(60.6086) | Bit/dim 3.6873(3.6877) | Xent 0.1433(0.1457) | Loss 3.7589(3.7606) | Error 0.0476(0.0505) Steps 676(671.12) | Grad Norm 0.7942(1.1999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 24.2757, Epoch Time 400.1176(404.3067), Bit/dim 3.7050(best: 3.7051), Xent 2.3744, Loss 4.8922, Error 0.4227(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2503 | Time 58.1757(60.5356) | Bit/dim 3.6799(3.6875) | Xent 0.1324(0.1453) | Loss 3.7461(3.7602) | Error 0.0453(0.0503) Steps 664(670.91) | Grad Norm 0.9100(1.1912) | Total Time 14.00(14.00)\n",
      "Iter 2504 | Time 60.4949(60.5344) | Bit/dim 3.6949(3.6877) | Xent 0.1316(0.1449) | Loss 3.7607(3.7602) | Error 0.0489(0.0503) Steps 670(670.88) | Grad Norm 0.8117(1.1798) | Total Time 14.00(14.00)\n",
      "Iter 2505 | Time 60.2971(60.5272) | Bit/dim 3.6882(3.6877) | Xent 0.1390(0.1447) | Loss 3.7577(3.7601) | Error 0.0506(0.0503) Steps 664(670.67) | Grad Norm 0.8825(1.1709) | Total Time 14.00(14.00)\n",
      "Iter 2506 | Time 60.1154(60.5149) | Bit/dim 3.6859(3.6877) | Xent 0.1472(0.1448) | Loss 3.7596(3.7601) | Error 0.0519(0.0503) Steps 682(671.01) | Grad Norm 0.8228(1.1604) | Total Time 14.00(14.00)\n",
      "Iter 2507 | Time 60.4743(60.5137) | Bit/dim 3.6813(3.6875) | Xent 0.1407(0.1447) | Loss 3.7516(3.7598) | Error 0.0495(0.0503) Steps 670(670.98) | Grad Norm 0.8161(1.1501) | Total Time 14.00(14.00)\n",
      "Iter 2508 | Time 60.2352(60.5053) | Bit/dim 3.6854(3.6874) | Xent 0.1274(0.1442) | Loss 3.7491(3.7595) | Error 0.0443(0.0501) Steps 664(670.77) | Grad Norm 0.9332(1.1436) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 24.3468, Epoch Time 399.8794(404.1738), Bit/dim 3.7049(best: 3.7050), Xent 2.3946, Loss 4.9022, Error 0.4294(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2509 | Time 62.6801(60.5706) | Bit/dim 3.6933(3.6876) | Xent 0.1266(0.1436) | Loss 3.7566(3.7594) | Error 0.0424(0.0499) Steps 676(670.93) | Grad Norm 0.8565(1.1350) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 59.8341(60.5485) | Bit/dim 3.6767(3.6873) | Xent 0.1398(0.1435) | Loss 3.7466(3.7590) | Error 0.0493(0.0499) Steps 670(670.90) | Grad Norm 0.7145(1.1224) | Total Time 14.00(14.00)\n",
      "Iter 2511 | Time 59.4826(60.5165) | Bit/dim 3.6909(3.6874) | Xent 0.1341(0.1432) | Loss 3.7580(3.7590) | Error 0.0465(0.0498) Steps 670(670.88) | Grad Norm 0.6743(1.1089) | Total Time 14.00(14.00)\n",
      "Iter 2512 | Time 60.4481(60.5144) | Bit/dim 3.6874(3.6874) | Xent 0.1344(0.1430) | Loss 3.7546(3.7589) | Error 0.0475(0.0497) Steps 676(671.03) | Grad Norm 0.7758(1.0989) | Total Time 14.00(14.00)\n",
      "Iter 2513 | Time 61.2286(60.5359) | Bit/dim 3.6897(3.6875) | Xent 0.1349(0.1427) | Loss 3.7572(3.7588) | Error 0.0466(0.0496) Steps 676(671.18) | Grad Norm 0.8922(1.0927) | Total Time 14.00(14.00)\n",
      "Iter 2514 | Time 63.2942(60.6186) | Bit/dim 3.6845(3.6874) | Xent 0.1385(0.1426) | Loss 3.7538(3.7587) | Error 0.0490(0.0496) Steps 676(671.32) | Grad Norm 1.2663(1.0979) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 23.9780, Epoch Time 406.7699(404.2517), Bit/dim 3.7057(best: 3.7049), Xent 2.4202, Loss 4.9158, Error 0.4261(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2515 | Time 59.9409(60.5983) | Bit/dim 3.6904(3.6875) | Xent 0.1310(0.1423) | Loss 3.7559(3.7586) | Error 0.0445(0.0494) Steps 676(671.46) | Grad Norm 0.9797(1.0944) | Total Time 14.00(14.00)\n",
      "Iter 2516 | Time 58.2812(60.5288) | Bit/dim 3.6797(3.6872) | Xent 0.1273(0.1418) | Loss 3.7434(3.7581) | Error 0.0441(0.0493) Steps 670(671.42) | Grad Norm 0.6531(1.0812) | Total Time 14.00(14.00)\n",
      "Iter 2517 | Time 60.5590(60.5297) | Bit/dim 3.6820(3.6871) | Xent 0.1312(0.1415) | Loss 3.7476(3.7578) | Error 0.0451(0.0492) Steps 670(671.38) | Grad Norm 1.0021(1.0788) | Total Time 14.00(14.00)\n",
      "Iter 2518 | Time 59.6770(60.5041) | Bit/dim 3.6887(3.6871) | Xent 0.1389(0.1414) | Loss 3.7581(3.7578) | Error 0.0494(0.0492) Steps 682(671.70) | Grad Norm 1.3584(1.0872) | Total Time 14.00(14.00)\n",
      "Iter 2519 | Time 61.6643(60.5389) | Bit/dim 3.6939(3.6873) | Xent 0.1439(0.1415) | Loss 3.7658(3.7581) | Error 0.0509(0.0492) Steps 670(671.64) | Grad Norm 1.4520(1.0981) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 61.8984(60.5797) | Bit/dim 3.6835(3.6872) | Xent 0.1395(0.1414) | Loss 3.7533(3.7579) | Error 0.0481(0.0492) Steps 682(671.96) | Grad Norm 0.9109(1.0925) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 24.3847, Epoch Time 402.2059(404.1903), Bit/dim 3.7046(best: 3.7049), Xent 2.4068, Loss 4.9080, Error 0.4292(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2521 | Time 59.1847(60.5378) | Bit/dim 3.6938(3.6874) | Xent 0.1327(0.1412) | Loss 3.7602(3.7580) | Error 0.0474(0.0491) Steps 682(672.26) | Grad Norm 0.9143(1.0872) | Total Time 14.00(14.00)\n",
      "Iter 2522 | Time 59.7191(60.5133) | Bit/dim 3.6820(3.6872) | Xent 0.1427(0.1412) | Loss 3.7533(3.7578) | Error 0.0503(0.0492) Steps 664(672.01) | Grad Norm 1.1941(1.0904) | Total Time 14.00(14.00)\n",
      "Iter 2523 | Time 60.5554(60.5145) | Bit/dim 3.6836(3.6871) | Xent 0.1310(0.1409) | Loss 3.7491(3.7576) | Error 0.0466(0.0491) Steps 676(672.13) | Grad Norm 1.1316(1.0916) | Total Time 14.00(14.00)\n",
      "Iter 2524 | Time 59.4156(60.4816) | Bit/dim 3.6896(3.6872) | Xent 0.1381(0.1408) | Loss 3.7587(3.7576) | Error 0.0479(0.0490) Steps 676(672.24) | Grad Norm 1.1225(1.0925) | Total Time 14.00(14.00)\n",
      "Iter 2525 | Time 63.0750(60.5594) | Bit/dim 3.6837(3.6871) | Xent 0.1418(0.1409) | Loss 3.7546(3.7575) | Error 0.0493(0.0491) Steps 670(672.18) | Grad Norm 0.9460(1.0881) | Total Time 14.00(14.00)\n",
      "Iter 2526 | Time 60.4357(60.5557) | Bit/dim 3.6838(3.6870) | Xent 0.1422(0.1409) | Loss 3.7549(3.7574) | Error 0.0497(0.0491) Steps 670(672.11) | Grad Norm 0.9921(1.0853) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 24.2255, Epoch Time 402.6912(404.1454), Bit/dim 3.7055(best: 3.7046), Xent 2.3948, Loss 4.9029, Error 0.4226(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2527 | Time 62.1814(60.6044) | Bit/dim 3.6897(3.6871) | Xent 0.1403(0.1409) | Loss 3.7599(3.7575) | Error 0.0475(0.0490) Steps 664(671.87) | Grad Norm 1.1734(1.0879) | Total Time 14.00(14.00)\n",
      "Iter 2528 | Time 58.5027(60.5414) | Bit/dim 3.6937(3.6873) | Xent 0.1291(0.1405) | Loss 3.7582(3.7575) | Error 0.0447(0.0489) Steps 676(671.99) | Grad Norm 0.9362(1.0833) | Total Time 14.00(14.00)\n",
      "Iter 2529 | Time 60.7405(60.5474) | Bit/dim 3.6800(3.6871) | Xent 0.1414(0.1406) | Loss 3.7507(3.7573) | Error 0.0494(0.0489) Steps 670(671.93) | Grad Norm 0.9846(1.0804) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 60.0254(60.5317) | Bit/dim 3.6836(3.6870) | Xent 0.1302(0.1402) | Loss 3.7487(3.7571) | Error 0.0440(0.0488) Steps 676(672.05) | Grad Norm 0.6741(1.0682) | Total Time 14.00(14.00)\n",
      "Iter 2531 | Time 59.5704(60.5029) | Bit/dim 3.6865(3.6869) | Xent 0.1306(0.1399) | Loss 3.7518(3.7569) | Error 0.0456(0.0487) Steps 670(671.99) | Grad Norm 0.8177(1.0607) | Total Time 14.00(14.00)\n",
      "Iter 2532 | Time 61.3272(60.5276) | Bit/dim 3.6868(3.6869) | Xent 0.1385(0.1399) | Loss 3.7560(3.7569) | Error 0.0460(0.0486) Steps 670(671.93) | Grad Norm 1.1566(1.0636) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 24.3471, Epoch Time 402.8863(404.1076), Bit/dim 3.7059(best: 3.7046), Xent 2.4126, Loss 4.9122, Error 0.4271(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2533 | Time 60.2085(60.5180) | Bit/dim 3.6871(3.6869) | Xent 0.1353(0.1398) | Loss 3.7548(3.7568) | Error 0.0461(0.0485) Steps 676(672.06) | Grad Norm 0.8664(1.0576) | Total Time 14.00(14.00)\n",
      "Iter 2534 | Time 62.6090(60.5807) | Bit/dim 3.6810(3.6868) | Xent 0.1372(0.1397) | Loss 3.7496(3.7566) | Error 0.0485(0.0485) Steps 676(672.17) | Grad Norm 1.2108(1.0622) | Total Time 14.00(14.00)\n",
      "Iter 2535 | Time 59.6857(60.5539) | Bit/dim 3.6843(3.6867) | Xent 0.1370(0.1396) | Loss 3.7528(3.7565) | Error 0.0471(0.0485) Steps 682(672.47) | Grad Norm 1.6790(1.0807) | Total Time 14.00(14.00)\n",
      "Iter 2536 | Time 61.9020(60.5943) | Bit/dim 3.6817(3.6865) | Xent 0.1447(0.1398) | Loss 3.7541(3.7564) | Error 0.0520(0.0486) Steps 670(672.39) | Grad Norm 0.9402(1.0765) | Total Time 14.00(14.00)\n",
      "Iter 2537 | Time 60.4596(60.5903) | Bit/dim 3.6953(3.6868) | Xent 0.1303(0.1395) | Loss 3.7605(3.7565) | Error 0.0437(0.0484) Steps 676(672.50) | Grad Norm 0.7228(1.0659) | Total Time 14.00(14.00)\n",
      "Iter 2538 | Time 60.2448(60.5799) | Bit/dim 3.6881(3.6868) | Xent 0.1346(0.1393) | Loss 3.7554(3.7565) | Error 0.0451(0.0483) Steps 670(672.43) | Grad Norm 1.1291(1.0678) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 24.2361, Epoch Time 405.6101(404.1527), Bit/dim 3.7063(best: 3.7046), Xent 2.4128, Loss 4.9127, Error 0.4259(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2539 | Time 62.1051(60.6257) | Bit/dim 3.6869(3.6868) | Xent 0.1321(0.1391) | Loss 3.7529(3.7564) | Error 0.0447(0.0482) Steps 664(672.17) | Grad Norm 0.9163(1.0633) | Total Time 14.00(14.00)\n",
      "Iter 2540 | Time 61.6051(60.6551) | Bit/dim 3.6889(3.6869) | Xent 0.1321(0.1389) | Loss 3.7549(3.7564) | Error 0.0444(0.0481) Steps 670(672.11) | Grad Norm 1.4835(1.0759) | Total Time 14.00(14.00)\n",
      "Iter 2541 | Time 63.6433(60.7447) | Bit/dim 3.6884(3.6870) | Xent 0.1323(0.1387) | Loss 3.7546(3.7563) | Error 0.0461(0.0481) Steps 664(671.87) | Grad Norm 1.6891(1.0943) | Total Time 14.00(14.00)\n",
      "Iter 2542 | Time 60.5967(60.7403) | Bit/dim 3.6878(3.6870) | Xent 0.1313(0.1385) | Loss 3.7535(3.7562) | Error 0.0439(0.0479) Steps 676(671.99) | Grad Norm 1.0869(1.0940) | Total Time 14.00(14.00)\n",
      "Iter 2543 | Time 60.6550(60.7377) | Bit/dim 3.6772(3.6867) | Xent 0.1364(0.1384) | Loss 3.7454(3.7559) | Error 0.0464(0.0479) Steps 682(672.29) | Grad Norm 1.3385(1.1014) | Total Time 14.00(14.00)\n",
      "Iter 2544 | Time 58.4137(60.6680) | Bit/dim 3.6862(3.6867) | Xent 0.1314(0.1382) | Loss 3.7519(3.7558) | Error 0.0471(0.0479) Steps 670(672.22) | Grad Norm 1.2656(1.1063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 24.5239, Epoch Time 407.4169(404.2506), Bit/dim 3.7060(best: 3.7046), Xent 2.4188, Loss 4.9154, Error 0.4289(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2545 | Time 60.4885(60.6626) | Bit/dim 3.6812(3.6865) | Xent 0.1428(0.1383) | Loss 3.7525(3.7557) | Error 0.0495(0.0479) Steps 664(671.98) | Grad Norm 1.1470(1.1075) | Total Time 14.00(14.00)\n",
      "Iter 2546 | Time 58.6642(60.6027) | Bit/dim 3.6823(3.6864) | Xent 0.1359(0.1383) | Loss 3.7503(3.7555) | Error 0.0474(0.0479) Steps 658(671.56) | Grad Norm 0.8276(1.0991) | Total Time 14.00(14.00)\n",
      "Iter 2547 | Time 61.0863(60.6172) | Bit/dim 3.6930(3.6866) | Xent 0.1277(0.1380) | Loss 3.7569(3.7556) | Error 0.0463(0.0478) Steps 670(671.51) | Grad Norm 1.2920(1.1049) | Total Time 14.00(14.00)\n",
      "Iter 2548 | Time 62.2328(60.6656) | Bit/dim 3.6703(3.6861) | Xent 0.1337(0.1378) | Loss 3.7371(3.7550) | Error 0.0440(0.0477) Steps 664(671.28) | Grad Norm 1.2720(1.1099) | Total Time 14.00(14.00)\n",
      "Iter 2549 | Time 60.0815(60.6481) | Bit/dim 3.6939(3.6863) | Xent 0.1446(0.1380) | Loss 3.7662(3.7553) | Error 0.0510(0.0478) Steps 664(671.07) | Grad Norm 0.9453(1.1050) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 58.9303(60.5966) | Bit/dim 3.6934(3.6865) | Xent 0.1356(0.1380) | Loss 3.7612(3.7555) | Error 0.0483(0.0478) Steps 676(671.21) | Grad Norm 1.4550(1.1155) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 24.2906, Epoch Time 401.6055(404.1712), Bit/dim 3.7052(best: 3.7046), Xent 2.4069, Loss 4.9087, Error 0.4241(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2551 | Time 58.0882(60.5213) | Bit/dim 3.6904(3.6867) | Xent 0.1384(0.1380) | Loss 3.7596(3.7556) | Error 0.0470(0.0478) Steps 664(671.00) | Grad Norm 1.5368(1.1281) | Total Time 14.00(14.00)\n",
      "Iter 2552 | Time 60.4016(60.5177) | Bit/dim 3.6859(3.6866) | Xent 0.1329(0.1378) | Loss 3.7523(3.7555) | Error 0.0451(0.0477) Steps 670(670.97) | Grad Norm 1.3535(1.1349) | Total Time 14.00(14.00)\n",
      "Iter 2553 | Time 60.0590(60.5040) | Bit/dim 3.6868(3.6866) | Xent 0.1321(0.1376) | Loss 3.7529(3.7555) | Error 0.0431(0.0476) Steps 664(670.76) | Grad Norm 0.6645(1.1208) | Total Time 14.00(14.00)\n",
      "Iter 2554 | Time 59.3720(60.4700) | Bit/dim 3.6745(3.6863) | Xent 0.1333(0.1375) | Loss 3.7412(3.7550) | Error 0.0474(0.0476) Steps 670(670.74) | Grad Norm 0.9801(1.1166) | Total Time 14.00(14.00)\n",
      "Iter 2555 | Time 60.7665(60.4789) | Bit/dim 3.6904(3.6864) | Xent 0.1419(0.1377) | Loss 3.7614(3.7552) | Error 0.0466(0.0476) Steps 682(671.07) | Grad Norm 1.3639(1.1240) | Total Time 14.00(14.00)\n",
      "Iter 2556 | Time 60.6078(60.4828) | Bit/dim 3.6891(3.6865) | Xent 0.1245(0.1373) | Loss 3.7513(3.7551) | Error 0.0437(0.0474) Steps 658(670.68) | Grad Norm 1.2842(1.1288) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 24.1883, Epoch Time 399.2024(404.0222), Bit/dim 3.7064(best: 3.7046), Xent 2.4276, Loss 4.9201, Error 0.4294(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2557 | Time 61.8322(60.5233) | Bit/dim 3.6756(3.6862) | Xent 0.1331(0.1371) | Loss 3.7421(3.7547) | Error 0.0483(0.0475) Steps 676(670.84) | Grad Norm 0.9719(1.1241) | Total Time 14.00(14.00)\n",
      "Iter 2558 | Time 59.4617(60.4914) | Bit/dim 3.6915(3.6863) | Xent 0.1373(0.1371) | Loss 3.7602(3.7549) | Error 0.0475(0.0475) Steps 664(670.64) | Grad Norm 1.0918(1.1231) | Total Time 14.00(14.00)\n",
      "Iter 2559 | Time 60.3774(60.4880) | Bit/dim 3.6903(3.6864) | Xent 0.1323(0.1370) | Loss 3.7565(3.7549) | Error 0.0467(0.0474) Steps 676(670.80) | Grad Norm 0.9380(1.1176) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 60.4625(60.4872) | Bit/dim 3.6905(3.6866) | Xent 0.1308(0.1368) | Loss 3.7559(3.7550) | Error 0.0471(0.0474) Steps 676(670.95) | Grad Norm 1.1843(1.1196) | Total Time 14.00(14.00)\n",
      "Iter 2561 | Time 60.2934(60.4814) | Bit/dim 3.6696(3.6860) | Xent 0.1334(0.1367) | Loss 3.7363(3.7544) | Error 0.0466(0.0474) Steps 670(670.92) | Grad Norm 1.5452(1.1323) | Total Time 14.00(14.00)\n",
      "Iter 2562 | Time 60.6948(60.4878) | Bit/dim 3.6943(3.6863) | Xent 0.1300(0.1365) | Loss 3.7592(3.7545) | Error 0.0447(0.0473) Steps 676(671.08) | Grad Norm 1.5737(1.1456) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 24.4082, Epoch Time 403.4319(404.0045), Bit/dim 3.7072(best: 3.7046), Xent 2.4167, Loss 4.9156, Error 0.4255(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2563 | Time 60.8690(60.4992) | Bit/dim 3.6908(3.6864) | Xent 0.1271(0.1362) | Loss 3.7544(3.7545) | Error 0.0454(0.0473) Steps 664(670.86) | Grad Norm 0.9153(1.1387) | Total Time 14.00(14.00)\n",
      "Iter 2564 | Time 62.7804(60.5677) | Bit/dim 3.6893(3.6865) | Xent 0.1254(0.1359) | Loss 3.7520(3.7545) | Error 0.0446(0.0472) Steps 676(671.02) | Grad Norm 1.1696(1.1396) | Total Time 14.00(14.00)\n",
      "Iter 2565 | Time 60.3460(60.5610) | Bit/dim 3.6800(3.6863) | Xent 0.1317(0.1358) | Loss 3.7458(3.7542) | Error 0.0444(0.0471) Steps 676(671.17) | Grad Norm 1.8035(1.1595) | Total Time 14.00(14.00)\n",
      "Iter 2566 | Time 59.7108(60.5355) | Bit/dim 3.6863(3.6863) | Xent 0.1326(0.1357) | Loss 3.7526(3.7542) | Error 0.0467(0.0471) Steps 676(671.31) | Grad Norm 1.3845(1.1663) | Total Time 14.00(14.00)\n",
      "Iter 2567 | Time 59.6620(60.5093) | Bit/dim 3.6828(3.6862) | Xent 0.1241(0.1353) | Loss 3.7449(3.7539) | Error 0.0421(0.0470) Steps 664(671.09) | Grad Norm 1.6051(1.1794) | Total Time 14.00(14.00)\n",
      "Iter 2568 | Time 59.3283(60.4739) | Bit/dim 3.6906(3.6863) | Xent 0.1275(0.1351) | Loss 3.7543(3.7539) | Error 0.0444(0.0469) Steps 664(670.88) | Grad Norm 1.0002(1.1740) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 24.4125, Epoch Time 402.9235(403.9720), Bit/dim 3.7046(best: 3.7046), Xent 2.4312, Loss 4.9203, Error 0.4224(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2569 | Time 60.6317(60.4786) | Bit/dim 3.6826(3.6862) | Xent 0.1284(0.1349) | Loss 3.7468(3.7537) | Error 0.0450(0.0468) Steps 676(671.03) | Grad Norm 0.9936(1.1686) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 60.8587(60.4900) | Bit/dim 3.6870(3.6863) | Xent 0.1277(0.1347) | Loss 3.7508(3.7536) | Error 0.0445(0.0467) Steps 658(670.64) | Grad Norm 1.0960(1.1664) | Total Time 14.00(14.00)\n",
      "Iter 2571 | Time 60.5044(60.4905) | Bit/dim 3.6818(3.6861) | Xent 0.1334(0.1346) | Loss 3.7485(3.7534) | Error 0.0460(0.0467) Steps 676(670.80) | Grad Norm 1.2826(1.1699) | Total Time 14.00(14.00)\n",
      "Iter 2572 | Time 60.7822(60.4992) | Bit/dim 3.6917(3.6863) | Xent 0.1270(0.1344) | Loss 3.7552(3.7535) | Error 0.0473(0.0467) Steps 658(670.42) | Grad Norm 1.7134(1.1862) | Total Time 14.00(14.00)\n",
      "Iter 2573 | Time 62.6505(60.5637) | Bit/dim 3.6785(3.6861) | Xent 0.1284(0.1342) | Loss 3.7427(3.7532) | Error 0.0447(0.0467) Steps 676(670.59) | Grad Norm 1.5283(1.1965) | Total Time 14.00(14.00)\n",
      "Iter 2574 | Time 60.8885(60.5735) | Bit/dim 3.6874(3.6861) | Xent 0.1321(0.1342) | Loss 3.7534(3.7532) | Error 0.0450(0.0466) Steps 670(670.57) | Grad Norm 1.2587(1.1984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 24.7503, Epoch Time 406.9987(404.0628), Bit/dim 3.7049(best: 3.7046), Xent 2.4276, Loss 4.9187, Error 0.4269(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2575 | Time 61.1525(60.5909) | Bit/dim 3.6922(3.6863) | Xent 0.1203(0.1337) | Loss 3.7524(3.7531) | Error 0.0434(0.0465) Steps 676(670.73) | Grad Norm 1.0685(1.1945) | Total Time 14.00(14.00)\n",
      "Iter 2576 | Time 60.7604(60.5959) | Bit/dim 3.6753(3.6859) | Xent 0.1236(0.1334) | Loss 3.7371(3.7527) | Error 0.0411(0.0464) Steps 670(670.71) | Grad Norm 0.8448(1.1840) | Total Time 14.00(14.00)\n",
      "Iter 2577 | Time 61.5490(60.6245) | Bit/dim 3.6961(3.6863) | Xent 0.1270(0.1332) | Loss 3.7596(3.7529) | Error 0.0447(0.0463) Steps 676(670.87) | Grad Norm 0.8914(1.1752) | Total Time 14.00(14.00)\n",
      "Iter 2578 | Time 61.4536(60.6494) | Bit/dim 3.6879(3.6863) | Xent 0.1320(0.1332) | Loss 3.7539(3.7529) | Error 0.0467(0.0463) Steps 670(670.84) | Grad Norm 1.3230(1.1796) | Total Time 14.00(14.00)\n",
      "Iter 2579 | Time 59.8535(60.6255) | Bit/dim 3.6819(3.6862) | Xent 0.1361(0.1333) | Loss 3.7500(3.7528) | Error 0.0477(0.0464) Steps 670(670.82) | Grad Norm 0.9266(1.1720) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 61.3388(60.6469) | Bit/dim 3.6787(3.6859) | Xent 0.1330(0.1333) | Loss 3.7452(3.7526) | Error 0.0444(0.0463) Steps 670(670.79) | Grad Norm 0.9243(1.1646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 24.2387, Epoch Time 406.1424(404.1252), Bit/dim 3.7058(best: 3.7046), Xent 2.4584, Loss 4.9350, Error 0.4284(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2581 | Time 60.3279(60.6374) | Bit/dim 3.6860(3.6860) | Xent 0.1312(0.1332) | Loss 3.7517(3.7526) | Error 0.0471(0.0463) Steps 670(670.77) | Grad Norm 0.6463(1.1491) | Total Time 14.00(14.00)\n",
      "Iter 2582 | Time 60.8568(60.6439) | Bit/dim 3.6752(3.6856) | Xent 0.1227(0.1329) | Loss 3.7366(3.7521) | Error 0.0421(0.0462) Steps 670(670.75) | Grad Norm 0.8450(1.1399) | Total Time 14.00(14.00)\n",
      "Iter 2583 | Time 58.8574(60.5903) | Bit/dim 3.6951(3.6859) | Xent 0.1279(0.1328) | Loss 3.7591(3.7523) | Error 0.0424(0.0461) Steps 664(670.54) | Grad Norm 0.9314(1.1337) | Total Time 14.00(14.00)\n",
      "Iter 2584 | Time 59.3851(60.5542) | Bit/dim 3.6821(3.6858) | Xent 0.1299(0.1327) | Loss 3.7470(3.7521) | Error 0.0475(0.0461) Steps 670(670.53) | Grad Norm 0.8317(1.1246) | Total Time 14.00(14.00)\n",
      "Iter 2585 | Time 61.8554(60.5932) | Bit/dim 3.6846(3.6858) | Xent 0.1235(0.1324) | Loss 3.7464(3.7520) | Error 0.0413(0.0460) Steps 664(670.33) | Grad Norm 0.7969(1.1148) | Total Time 14.00(14.00)\n",
      "Iter 2586 | Time 60.4773(60.5897) | Bit/dim 3.6907(3.6859) | Xent 0.1225(0.1321) | Loss 3.7520(3.7520) | Error 0.0411(0.0458) Steps 676(670.50) | Grad Norm 0.9430(1.1096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 24.3303, Epoch Time 401.9927(404.0613), Bit/dim 3.7045(best: 3.7046), Xent 2.4402, Loss 4.9246, Error 0.4252(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2587 | Time 59.9475(60.5705) | Bit/dim 3.6772(3.6857) | Xent 0.1321(0.1321) | Loss 3.7432(3.7517) | Error 0.0460(0.0459) Steps 670(670.49) | Grad Norm 0.7206(1.0980) | Total Time 14.00(14.00)\n",
      "Iter 2588 | Time 60.2059(60.5595) | Bit/dim 3.6819(3.6855) | Xent 0.1270(0.1319) | Loss 3.7454(3.7515) | Error 0.0444(0.0458) Steps 664(670.29) | Grad Norm 0.8682(1.0911) | Total Time 14.00(14.00)\n",
      "Iter 2589 | Time 61.8221(60.5974) | Bit/dim 3.6914(3.6857) | Xent 0.1253(0.1317) | Loss 3.7540(3.7516) | Error 0.0419(0.0457) Steps 670(670.28) | Grad Norm 0.8207(1.0830) | Total Time 14.00(14.00)\n",
      "Iter 2590 | Time 60.5311(60.5954) | Bit/dim 3.6951(3.6860) | Xent 0.1292(0.1317) | Loss 3.7597(3.7518) | Error 0.0437(0.0456) Steps 670(670.27) | Grad Norm 1.0148(1.0809) | Total Time 14.00(14.00)\n",
      "Iter 2591 | Time 61.7396(60.6298) | Bit/dim 3.6875(3.6860) | Xent 0.1295(0.1316) | Loss 3.7522(3.7518) | Error 0.0445(0.0456) Steps 670(670.27) | Grad Norm 0.9549(1.0771) | Total Time 14.00(14.00)\n",
      "Iter 2592 | Time 60.5353(60.6269) | Bit/dim 3.6766(3.6858) | Xent 0.1292(0.1315) | Loss 3.7412(3.7515) | Error 0.0420(0.0455) Steps 670(670.26) | Grad Norm 0.7707(1.0679) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 24.1777, Epoch Time 404.9004(404.0864), Bit/dim 3.7059(best: 3.7045), Xent 2.4657, Loss 4.9387, Error 0.4282(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2593 | Time 59.9016(60.6052) | Bit/dim 3.6960(3.6861) | Xent 0.1322(0.1316) | Loss 3.7622(3.7518) | Error 0.0455(0.0455) Steps 670(670.25) | Grad Norm 1.5687(1.0830) | Total Time 14.00(14.00)\n",
      "Iter 2594 | Time 62.0146(60.6474) | Bit/dim 3.6779(3.6858) | Xent 0.1377(0.1317) | Loss 3.7467(3.7517) | Error 0.0457(0.0455) Steps 676(670.42) | Grad Norm 0.8527(1.0761) | Total Time 14.00(14.00)\n",
      "Iter 2595 | Time 59.3229(60.6077) | Bit/dim 3.6790(3.6856) | Xent 0.1268(0.1316) | Loss 3.7424(3.7514) | Error 0.0444(0.0455) Steps 664(670.23) | Grad Norm 1.0705(1.0759) | Total Time 14.00(14.00)\n",
      "Iter 2596 | Time 60.8157(60.6140) | Bit/dim 3.6904(3.6858) | Xent 0.1238(0.1314) | Loss 3.7523(3.7514) | Error 0.0426(0.0454) Steps 676(670.40) | Grad Norm 1.0514(1.0752) | Total Time 14.00(14.00)\n",
      "Iter 2597 | Time 61.2657(60.6335) | Bit/dim 3.6837(3.6857) | Xent 0.1284(0.1313) | Loss 3.7479(3.7513) | Error 0.0419(0.0453) Steps 670(670.39) | Grad Norm 1.0275(1.0737) | Total Time 14.00(14.00)\n",
      "Iter 2598 | Time 60.9918(60.6443) | Bit/dim 3.6834(3.6856) | Xent 0.1224(0.1310) | Loss 3.7446(3.7511) | Error 0.0420(0.0452) Steps 676(670.56) | Grad Norm 1.1008(1.0745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 24.1411, Epoch Time 404.1505(404.0884), Bit/dim 3.7061(best: 3.7045), Xent 2.4573, Loss 4.9347, Error 0.4328(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2599 | Time 59.0741(60.5971) | Bit/dim 3.6909(3.6858) | Xent 0.1331(0.1311) | Loss 3.7574(3.7513) | Error 0.0446(0.0452) Steps 676(670.72) | Grad Norm 0.8745(1.0685) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 59.3430(60.5595) | Bit/dim 3.6834(3.6857) | Xent 0.1341(0.1312) | Loss 3.7505(3.7513) | Error 0.0459(0.0452) Steps 664(670.52) | Grad Norm 0.8233(1.0612) | Total Time 14.00(14.00)\n",
      "Iter 2601 | Time 60.3411(60.5530) | Bit/dim 3.6927(3.6859) | Xent 0.1327(0.1312) | Loss 3.7590(3.7515) | Error 0.0445(0.0452) Steps 676(670.69) | Grad Norm 0.8580(1.0551) | Total Time 14.00(14.00)\n",
      "Iter 2602 | Time 60.9239(60.5641) | Bit/dim 3.6735(3.6856) | Xent 0.1284(0.1311) | Loss 3.7376(3.7511) | Error 0.0464(0.0452) Steps 670(670.67) | Grad Norm 0.8479(1.0489) | Total Time 14.00(14.00)\n",
      "Iter 2603 | Time 59.7120(60.5385) | Bit/dim 3.6843(3.6855) | Xent 0.1288(0.1310) | Loss 3.7487(3.7510) | Error 0.0445(0.0452) Steps 664(670.47) | Grad Norm 0.9688(1.0465) | Total Time 14.00(14.00)\n",
      "Iter 2604 | Time 60.4515(60.5359) | Bit/dim 3.6860(3.6855) | Xent 0.1262(0.1309) | Loss 3.7492(3.7510) | Error 0.0434(0.0451) Steps 670(670.45) | Grad Norm 0.9498(1.0436) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 24.0281, Epoch Time 399.6455(403.9551), Bit/dim 3.7057(best: 3.7045), Xent 2.4653, Loss 4.9384, Error 0.4279(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2605 | Time 59.9540(60.5185) | Bit/dim 3.6873(3.6856) | Xent 0.1190(0.1305) | Loss 3.7468(3.7509) | Error 0.0434(0.0451) Steps 670(670.44) | Grad Norm 0.7699(1.0354) | Total Time 14.00(14.00)\n",
      "Iter 2606 | Time 63.2906(60.6016) | Bit/dim 3.6994(3.6860) | Xent 0.1293(0.1305) | Loss 3.7640(3.7513) | Error 0.0459(0.0451) Steps 670(670.42) | Grad Norm 0.7234(1.0260) | Total Time 14.00(14.00)\n",
      "Iter 2607 | Time 60.2770(60.5919) | Bit/dim 3.6769(3.6857) | Xent 0.1360(0.1307) | Loss 3.7449(3.7511) | Error 0.0475(0.0452) Steps 658(670.05) | Grad Norm 1.6557(1.0449) | Total Time 14.00(14.00)\n",
      "Iter 2608 | Time 64.5830(60.7116) | Bit/dim 3.6793(3.6855) | Xent 0.1328(0.1307) | Loss 3.7457(3.7509) | Error 0.0479(0.0452) Steps 682(670.41) | Grad Norm 0.9920(1.0433) | Total Time 14.00(14.00)\n",
      "Iter 2609 | Time 60.0992(60.6932) | Bit/dim 3.6866(3.6856) | Xent 0.1286(0.1307) | Loss 3.7509(3.7509) | Error 0.0459(0.0453) Steps 676(670.58) | Grad Norm 0.9772(1.0413) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 59.8759(60.6687) | Bit/dim 3.6806(3.6854) | Xent 0.1292(0.1306) | Loss 3.7451(3.7507) | Error 0.0441(0.0452) Steps 670(670.56) | Grad Norm 0.8394(1.0353) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 24.0833, Epoch Time 408.2216(404.0831), Bit/dim 3.7070(best: 3.7045), Xent 2.4693, Loss 4.9417, Error 0.4316(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2611 | Time 59.8764(60.6450) | Bit/dim 3.6870(3.6855) | Xent 0.1287(0.1306) | Loss 3.7513(3.7507) | Error 0.0457(0.0452) Steps 670(670.54) | Grad Norm 0.7848(1.0278) | Total Time 14.00(14.00)\n",
      "Iter 2612 | Time 59.6225(60.6143) | Bit/dim 3.6789(3.6853) | Xent 0.1308(0.1306) | Loss 3.7443(3.7505) | Error 0.0445(0.0452) Steps 670(670.53) | Grad Norm 1.0546(1.0286) | Total Time 14.00(14.00)\n",
      "Iter 2613 | Time 59.7395(60.5880) | Bit/dim 3.6919(3.6855) | Xent 0.1307(0.1306) | Loss 3.7572(3.7507) | Error 0.0486(0.0453) Steps 670(670.51) | Grad Norm 1.0794(1.0301) | Total Time 14.00(14.00)\n",
      "Iter 2614 | Time 59.2506(60.5479) | Bit/dim 3.6792(3.6853) | Xent 0.1318(0.1306) | Loss 3.7452(3.7506) | Error 0.0476(0.0454) Steps 676(670.68) | Grad Norm 0.7281(1.0210) | Total Time 14.00(14.00)\n",
      "Iter 2615 | Time 61.3148(60.5709) | Bit/dim 3.6922(3.6855) | Xent 0.1317(0.1306) | Loss 3.7580(3.7508) | Error 0.0435(0.0453) Steps 658(670.30) | Grad Norm 0.8201(1.0150) | Total Time 14.00(14.00)\n",
      "Iter 2616 | Time 60.6617(60.5736) | Bit/dim 3.6859(3.6855) | Xent 0.1195(0.1303) | Loss 3.7457(3.7507) | Error 0.0403(0.0452) Steps 676(670.47) | Grad Norm 0.6919(1.0053) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 24.1448, Epoch Time 400.5269(403.9764), Bit/dim 3.7051(best: 3.7045), Xent 2.4660, Loss 4.9381, Error 0.4302(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2617 | Time 59.1156(60.5299) | Bit/dim 3.6845(3.6855) | Xent 0.1319(0.1304) | Loss 3.7505(3.7506) | Error 0.0453(0.0452) Steps 658(670.09) | Grad Norm 1.0484(1.0066) | Total Time 14.00(14.00)\n",
      "Iter 2618 | Time 59.9791(60.5134) | Bit/dim 3.6785(3.6853) | Xent 0.1293(0.1303) | Loss 3.7431(3.7504) | Error 0.0444(0.0452) Steps 664(669.91) | Grad Norm 0.7801(0.9998) | Total Time 14.00(14.00)\n",
      "Iter 2619 | Time 62.7180(60.5795) | Bit/dim 3.6834(3.6852) | Xent 0.1268(0.1302) | Loss 3.7468(3.7503) | Error 0.0414(0.0451) Steps 676(670.09) | Grad Norm 0.7340(0.9918) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 59.9528(60.5607) | Bit/dim 3.6819(3.6851) | Xent 0.1329(0.1303) | Loss 3.7483(3.7503) | Error 0.0471(0.0451) Steps 658(669.73) | Grad Norm 1.1075(0.9953) | Total Time 14.00(14.00)\n",
      "Iter 2621 | Time 61.5100(60.5892) | Bit/dim 3.6921(3.6853) | Xent 0.1255(0.1302) | Loss 3.7548(3.7504) | Error 0.0407(0.0450) Steps 670(669.74) | Grad Norm 1.1302(0.9993) | Total Time 14.00(14.00)\n",
      "Iter 2622 | Time 61.3948(60.6134) | Bit/dim 3.6883(3.6854) | Xent 0.1190(0.1298) | Loss 3.7478(3.7503) | Error 0.0423(0.0449) Steps 682(670.11) | Grad Norm 0.8678(0.9954) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 24.5508, Epoch Time 405.0621(404.0090), Bit/dim 3.7051(best: 3.7045), Xent 2.4981, Loss 4.9542, Error 0.4266(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2623 | Time 62.1096(60.6583) | Bit/dim 3.6871(3.6855) | Xent 0.1294(0.1298) | Loss 3.7518(3.7504) | Error 0.0429(0.0448) Steps 664(669.92) | Grad Norm 0.6363(0.9846) | Total Time 14.00(14.00)\n",
      "Iter 2624 | Time 61.2343(60.6755) | Bit/dim 3.6918(3.6856) | Xent 0.1349(0.1300) | Loss 3.7592(3.7506) | Error 0.0489(0.0450) Steps 670(669.93) | Grad Norm 0.8958(0.9820) | Total Time 14.00(14.00)\n",
      "Iter 2625 | Time 58.7799(60.6187) | Bit/dim 3.6905(3.6858) | Xent 0.1294(0.1299) | Loss 3.7552(3.7508) | Error 0.0443(0.0449) Steps 676(670.11) | Grad Norm 0.8306(0.9774) | Total Time 14.00(14.00)\n",
      "Iter 2626 | Time 60.5261(60.6159) | Bit/dim 3.6841(3.6857) | Xent 0.1239(0.1298) | Loss 3.7460(3.7506) | Error 0.0410(0.0448) Steps 676(670.28) | Grad Norm 1.0356(0.9792) | Total Time 14.00(14.00)\n",
      "Iter 2627 | Time 61.5056(60.6426) | Bit/dim 3.6794(3.6855) | Xent 0.1232(0.1296) | Loss 3.7410(3.7503) | Error 0.0455(0.0448) Steps 670(670.28) | Grad Norm 0.9939(0.9796) | Total Time 14.00(14.00)\n",
      "Iter 2628 | Time 62.2048(60.6894) | Bit/dim 3.6815(3.6854) | Xent 0.1246(0.1294) | Loss 3.7438(3.7501) | Error 0.0434(0.0448) Steps 676(670.45) | Grad Norm 0.9929(0.9800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 24.6244, Epoch Time 407.3472(404.1091), Bit/dim 3.7050(best: 3.7045), Xent 2.4790, Loss 4.9445, Error 0.4315(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2629 | Time 59.3960(60.6506) | Bit/dim 3.6882(3.6855) | Xent 0.1330(0.1295) | Loss 3.7547(3.7503) | Error 0.0433(0.0448) Steps 664(670.25) | Grad Norm 1.1980(0.9865) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 61.0224(60.6618) | Bit/dim 3.6913(3.6857) | Xent 0.1248(0.1294) | Loss 3.7537(3.7504) | Error 0.0433(0.0447) Steps 676(670.43) | Grad Norm 0.9873(0.9866) | Total Time 14.00(14.00)\n",
      "Iter 2631 | Time 61.9044(60.6991) | Bit/dim 3.6866(3.6857) | Xent 0.1170(0.1290) | Loss 3.7451(3.7502) | Error 0.0394(0.0445) Steps 670(670.41) | Grad Norm 0.7657(0.9799) | Total Time 14.00(14.00)\n",
      "Iter 2632 | Time 63.1183(60.7716) | Bit/dim 3.6753(3.6854) | Xent 0.1265(0.1289) | Loss 3.7386(3.7499) | Error 0.0436(0.0445) Steps 664(670.22) | Grad Norm 1.0830(0.9830) | Total Time 14.00(14.00)\n",
      "Iter 2633 | Time 61.0881(60.7811) | Bit/dim 3.6835(3.6853) | Xent 0.1202(0.1287) | Loss 3.7437(3.7497) | Error 0.0395(0.0444) Steps 670(670.21) | Grad Norm 1.2932(0.9923) | Total Time 14.00(14.00)\n",
      "Iter 2634 | Time 60.4784(60.7721) | Bit/dim 3.6885(3.6854) | Xent 0.1290(0.1287) | Loss 3.7530(3.7498) | Error 0.0441(0.0444) Steps 664(670.03) | Grad Norm 0.9033(0.9897) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 24.4094, Epoch Time 407.4003(404.2078), Bit/dim 3.7067(best: 3.7045), Xent 2.4888, Loss 4.9511, Error 0.4302(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2635 | Time 59.2289(60.7258) | Bit/dim 3.6813(3.6853) | Xent 0.1265(0.1286) | Loss 3.7446(3.7496) | Error 0.0445(0.0444) Steps 664(669.85) | Grad Norm 0.8734(0.9862) | Total Time 14.00(14.00)\n",
      "Iter 2636 | Time 60.9238(60.7317) | Bit/dim 3.6913(3.6855) | Xent 0.1190(0.1283) | Loss 3.7508(3.7497) | Error 0.0421(0.0443) Steps 670(669.85) | Grad Norm 1.4540(1.0002) | Total Time 14.00(14.00)\n",
      "Iter 2637 | Time 61.8759(60.7660) | Bit/dim 3.6830(3.6854) | Xent 0.1290(0.1284) | Loss 3.7475(3.7496) | Error 0.0454(0.0443) Steps 664(669.68) | Grad Norm 0.9668(0.9992) | Total Time 14.00(14.00)\n",
      "Iter 2638 | Time 62.2374(60.8102) | Bit/dim 3.6860(3.6854) | Xent 0.1301(0.1284) | Loss 3.7510(3.7496) | Error 0.0440(0.0443) Steps 658(669.33) | Grad Norm 1.0450(1.0006) | Total Time 14.00(14.00)\n",
      "Iter 2639 | Time 60.9056(60.8130) | Bit/dim 3.6805(3.6853) | Xent 0.1252(0.1283) | Loss 3.7431(3.7494) | Error 0.0427(0.0443) Steps 670(669.35) | Grad Norm 0.9833(1.0001) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 61.7891(60.8423) | Bit/dim 3.6896(3.6854) | Xent 0.1246(0.1282) | Loss 3.7519(3.7495) | Error 0.0446(0.0443) Steps 664(669.19) | Grad Norm 0.9243(0.9978) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 24.4595, Epoch Time 407.5204(404.3072), Bit/dim 3.7047(best: 3.7045), Xent 2.5015, Loss 4.9554, Error 0.4290(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2641 | Time 62.2706(60.8852) | Bit/dim 3.6833(3.6854) | Xent 0.1236(0.1281) | Loss 3.7451(3.7494) | Error 0.0415(0.0442) Steps 664(669.03) | Grad Norm 1.3523(1.0084) | Total Time 14.00(14.00)\n",
      "Iter 2642 | Time 59.9046(60.8557) | Bit/dim 3.6896(3.6855) | Xent 0.1263(0.1280) | Loss 3.7527(3.7495) | Error 0.0409(0.0441) Steps 664(668.88) | Grad Norm 0.9067(1.0054) | Total Time 14.00(14.00)\n",
      "Iter 2643 | Time 59.0080(60.8003) | Bit/dim 3.6869(3.6855) | Xent 0.1263(0.1280) | Loss 3.7500(3.7495) | Error 0.0443(0.0441) Steps 670(668.91) | Grad Norm 1.1656(1.0102) | Total Time 14.00(14.00)\n",
      "Iter 2644 | Time 61.3945(60.8181) | Bit/dim 3.6863(3.6855) | Xent 0.1224(0.1278) | Loss 3.7475(3.7494) | Error 0.0417(0.0440) Steps 664(668.77) | Grad Norm 0.8833(1.0064) | Total Time 14.00(14.00)\n",
      "Iter 2645 | Time 61.3506(60.8341) | Bit/dim 3.6842(3.6855) | Xent 0.1120(0.1273) | Loss 3.7402(3.7492) | Error 0.0389(0.0439) Steps 658(668.44) | Grad Norm 1.1015(1.0092) | Total Time 14.00(14.00)\n",
      "Iter 2646 | Time 60.2508(60.8166) | Bit/dim 3.6804(3.6854) | Xent 0.1241(0.1272) | Loss 3.7425(3.7490) | Error 0.0441(0.0439) Steps 682(668.85) | Grad Norm 1.3170(1.0185) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 24.1021, Epoch Time 404.1344(404.3020), Bit/dim 3.7063(best: 3.7045), Xent 2.4968, Loss 4.9547, Error 0.4298(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2647 | Time 60.5136(60.8075) | Bit/dim 3.6883(3.6854) | Xent 0.1231(0.1271) | Loss 3.7498(3.7490) | Error 0.0431(0.0439) Steps 658(668.52) | Grad Norm 0.8346(1.0129) | Total Time 14.00(14.00)\n",
      "Iter 2648 | Time 61.3570(60.8240) | Bit/dim 3.6867(3.6855) | Xent 0.1163(0.1268) | Loss 3.7449(3.7489) | Error 0.0396(0.0437) Steps 676(668.75) | Grad Norm 0.9397(1.0108) | Total Time 14.00(14.00)\n",
      "Iter 2649 | Time 61.6503(60.8488) | Bit/dim 3.6792(3.6853) | Xent 0.1353(0.1270) | Loss 3.7469(3.7488) | Error 0.0484(0.0439) Steps 664(668.61) | Grad Norm 1.1183(1.0140) | Total Time 14.00(14.00)\n",
      "Iter 2650 | Time 61.2824(60.8618) | Bit/dim 3.6972(3.6857) | Xent 0.1156(0.1267) | Loss 3.7550(3.7490) | Error 0.0381(0.0437) Steps 676(668.83) | Grad Norm 0.9544(1.0122) | Total Time 14.00(14.00)\n",
      "Iter 2651 | Time 59.8408(60.8312) | Bit/dim 3.6767(3.6854) | Xent 0.1242(0.1266) | Loss 3.7388(3.7487) | Error 0.0416(0.0436) Steps 664(668.68) | Grad Norm 0.9201(1.0094) | Total Time 14.00(14.00)\n",
      "Iter 2652 | Time 61.3021(60.8453) | Bit/dim 3.6829(3.6853) | Xent 0.1211(0.1264) | Loss 3.7434(3.7485) | Error 0.0436(0.0436) Steps 676(668.90) | Grad Norm 0.9701(1.0082) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 24.2571, Epoch Time 406.1706(404.3581), Bit/dim 3.7048(best: 3.7045), Xent 2.4749, Loss 4.9422, Error 0.4252(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2653 | Time 59.8596(60.8157) | Bit/dim 3.6880(3.6854) | Xent 0.1260(0.1264) | Loss 3.7510(3.7486) | Error 0.0451(0.0437) Steps 670(668.94) | Grad Norm 1.1088(1.0113) | Total Time 14.00(14.00)\n",
      "Iter 2654 | Time 61.9779(60.8506) | Bit/dim 3.6847(3.6854) | Xent 0.1299(0.1265) | Loss 3.7496(3.7486) | Error 0.0446(0.0437) Steps 664(668.79) | Grad Norm 1.2025(1.0170) | Total Time 14.00(14.00)\n",
      "Iter 2655 | Time 59.1708(60.8002) | Bit/dim 3.6903(3.6855) | Xent 0.1240(0.1265) | Loss 3.7524(3.7487) | Error 0.0431(0.0437) Steps 658(668.46) | Grad Norm 1.4169(1.0290) | Total Time 14.00(14.00)\n",
      "Iter 2656 | Time 62.8555(60.8619) | Bit/dim 3.6898(3.6856) | Xent 0.1345(0.1267) | Loss 3.7571(3.7490) | Error 0.0433(0.0437) Steps 676(668.69) | Grad Norm 0.9157(1.0256) | Total Time 14.00(14.00)\n",
      "Iter 2657 | Time 62.2893(60.9047) | Bit/dim 3.6784(3.6854) | Xent 0.1152(0.1264) | Loss 3.7360(3.7486) | Error 0.0386(0.0435) Steps 670(668.73) | Grad Norm 1.2362(1.0319) | Total Time 14.00(14.00)\n",
      "Iter 2658 | Time 58.0559(60.8192) | Bit/dim 3.6761(3.6851) | Xent 0.1246(0.1263) | Loss 3.7383(3.7483) | Error 0.0421(0.0435) Steps 670(668.77) | Grad Norm 1.5399(1.0472) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 24.0550, Epoch Time 404.0770(404.3497), Bit/dim 3.7049(best: 3.7045), Xent 2.4841, Loss 4.9470, Error 0.4296(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2659 | Time 59.6067(60.7828) | Bit/dim 3.6996(3.6856) | Xent 0.1207(0.1261) | Loss 3.7599(3.7486) | Error 0.0421(0.0434) Steps 676(668.98) | Grad Norm 1.0342(1.0468) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 59.6795(60.7497) | Bit/dim 3.6789(3.6854) | Xent 0.1333(0.1263) | Loss 3.7456(3.7486) | Error 0.0480(0.0436) Steps 670(669.01) | Grad Norm 1.2154(1.0518) | Total Time 14.00(14.00)\n",
      "Iter 2661 | Time 59.8921(60.7240) | Bit/dim 3.6863(3.6854) | Xent 0.1156(0.1260) | Loss 3.7441(3.7484) | Error 0.0389(0.0434) Steps 676(669.22) | Grad Norm 0.8430(1.0456) | Total Time 14.00(14.00)\n",
      "Iter 2662 | Time 58.9940(60.6721) | Bit/dim 3.6772(3.6852) | Xent 0.1206(0.1259) | Loss 3.7375(3.7481) | Error 0.0415(0.0434) Steps 664(669.07) | Grad Norm 1.3597(1.0550) | Total Time 14.00(14.00)\n",
      "Iter 2663 | Time 58.9280(60.6198) | Bit/dim 3.6806(3.6850) | Xent 0.1242(0.1258) | Loss 3.7427(3.7479) | Error 0.0436(0.0434) Steps 670(669.10) | Grad Norm 1.3812(1.0648) | Total Time 14.00(14.00)\n",
      "Iter 2664 | Time 59.7890(60.5949) | Bit/dim 3.6865(3.6851) | Xent 0.1284(0.1259) | Loss 3.7508(3.7480) | Error 0.0436(0.0434) Steps 670(669.12) | Grad Norm 0.9274(1.0606) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 24.1510, Epoch Time 397.1271(404.1330), Bit/dim 3.7042(best: 3.7045), Xent 2.4933, Loss 4.9509, Error 0.4284(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2665 | Time 58.0094(60.5173) | Bit/dim 3.6798(3.6849) | Xent 0.1209(0.1257) | Loss 3.7402(3.7478) | Error 0.0401(0.0433) Steps 658(668.79) | Grad Norm 1.1104(1.0621) | Total Time 14.00(14.00)\n",
      "Iter 2666 | Time 61.1835(60.5373) | Bit/dim 3.6696(3.6845) | Xent 0.1284(0.1258) | Loss 3.7338(3.7474) | Error 0.0451(0.0434) Steps 658(668.47) | Grad Norm 1.8366(1.0854) | Total Time 14.00(14.00)\n",
      "Iter 2667 | Time 58.0618(60.4630) | Bit/dim 3.6900(3.6846) | Xent 0.1200(0.1256) | Loss 3.7500(3.7474) | Error 0.0399(0.0433) Steps 652(667.97) | Grad Norm 1.3584(1.0936) | Total Time 14.00(14.00)\n",
      "Iter 2668 | Time 61.0905(60.4818) | Bit/dim 3.6948(3.6849) | Xent 0.1198(0.1255) | Loss 3.7547(3.7477) | Error 0.0400(0.0432) Steps 676(668.21) | Grad Norm 0.8876(1.0874) | Total Time 14.00(14.00)\n",
      "Iter 2669 | Time 58.5317(60.4233) | Bit/dim 3.6930(3.6852) | Xent 0.1228(0.1254) | Loss 3.7543(3.7479) | Error 0.0425(0.0431) Steps 664(668.09) | Grad Norm 0.7701(1.0779) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 61.1761(60.4459) | Bit/dim 3.6839(3.6851) | Xent 0.1182(0.1252) | Loss 3.7430(3.7477) | Error 0.0414(0.0431) Steps 664(667.96) | Grad Norm 0.8954(1.0724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 24.1852, Epoch Time 398.0250(403.9497), Bit/dim 3.7048(best: 3.7042), Xent 2.5025, Loss 4.9561, Error 0.4279(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2671 | Time 62.8533(60.5182) | Bit/dim 3.6894(3.6853) | Xent 0.1224(0.1251) | Loss 3.7506(3.7478) | Error 0.0410(0.0430) Steps 658(667.66) | Grad Norm 1.5722(1.0874) | Total Time 14.00(14.00)\n",
      "Iter 2672 | Time 59.2826(60.4811) | Bit/dim 3.6730(3.6849) | Xent 0.1242(0.1251) | Loss 3.7351(3.7474) | Error 0.0445(0.0431) Steps 664(667.55) | Grad Norm 1.5844(1.1023) | Total Time 14.00(14.00)\n",
      "Iter 2673 | Time 60.8192(60.4912) | Bit/dim 3.6852(3.6849) | Xent 0.1254(0.1251) | Loss 3.7479(3.7474) | Error 0.0445(0.0431) Steps 664(667.45) | Grad Norm 1.1267(1.1030) | Total Time 14.00(14.00)\n",
      "Iter 2674 | Time 60.5522(60.4931) | Bit/dim 3.6863(3.6849) | Xent 0.1180(0.1249) | Loss 3.7453(3.7474) | Error 0.0415(0.0431) Steps 670(667.52) | Grad Norm 0.7755(1.0932) | Total Time 14.00(14.00)\n",
      "Iter 2675 | Time 60.4408(60.4915) | Bit/dim 3.6899(3.6851) | Xent 0.1208(0.1247) | Loss 3.7503(3.7475) | Error 0.0385(0.0429) Steps 658(667.24) | Grad Norm 0.9115(1.0878) | Total Time 14.00(14.00)\n",
      "Iter 2676 | Time 59.2755(60.4550) | Bit/dim 3.6848(3.6851) | Xent 0.1190(0.1246) | Loss 3.7443(3.7474) | Error 0.0411(0.0429) Steps 670(667.32) | Grad Norm 2.0145(1.1156) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 24.2965, Epoch Time 403.8097(403.9455), Bit/dim 3.7059(best: 3.7042), Xent 2.4954, Loss 4.9536, Error 0.4271(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2677 | Time 58.3611(60.3922) | Bit/dim 3.6807(3.6849) | Xent 0.1201(0.1244) | Loss 3.7407(3.7472) | Error 0.0439(0.0429) Steps 682(667.76) | Grad Norm 0.9426(1.1104) | Total Time 14.00(14.00)\n",
      "Iter 2678 | Time 59.2278(60.3573) | Bit/dim 3.6930(3.6852) | Xent 0.1285(0.1246) | Loss 3.7572(3.7475) | Error 0.0417(0.0429) Steps 664(667.65) | Grad Norm 0.9544(1.1057) | Total Time 14.00(14.00)\n",
      "Iter 2679 | Time 58.7928(60.3103) | Bit/dim 3.6859(3.6852) | Xent 0.1253(0.1246) | Loss 3.7485(3.7475) | Error 0.0439(0.0429) Steps 658(667.36) | Grad Norm 0.8892(1.0992) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 62.0671(60.3630) | Bit/dim 3.6807(3.6851) | Xent 0.1227(0.1245) | Loss 3.7421(3.7473) | Error 0.0419(0.0429) Steps 664(667.26) | Grad Norm 1.5954(1.1141) | Total Time 14.00(14.00)\n",
      "Iter 2681 | Time 61.1339(60.3862) | Bit/dim 3.6804(3.6849) | Xent 0.1232(0.1245) | Loss 3.7420(3.7472) | Error 0.0421(0.0428) Steps 682(667.70) | Grad Norm 0.8990(1.1076) | Total Time 14.00(14.00)\n",
      "Iter 2682 | Time 62.4582(60.4483) | Bit/dim 3.6875(3.6850) | Xent 0.1133(0.1241) | Loss 3.7442(3.7471) | Error 0.0384(0.0427) Steps 670(667.77) | Grad Norm 0.8431(1.0997) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 24.5396, Epoch Time 402.3651(403.8981), Bit/dim 3.7054(best: 3.7042), Xent 2.5119, Loss 4.9613, Error 0.4258(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2683 | Time 57.9100(60.3722) | Bit/dim 3.6791(3.6848) | Xent 0.1110(0.1237) | Loss 3.7346(3.7467) | Error 0.0394(0.0426) Steps 664(667.66) | Grad Norm 0.8402(1.0919) | Total Time 14.00(14.00)\n",
      "Iter 2684 | Time 60.3002(60.3700) | Bit/dim 3.6838(3.6848) | Xent 0.1230(0.1237) | Loss 3.7453(3.7467) | Error 0.0425(0.0426) Steps 676(667.91) | Grad Norm 0.7985(1.0831) | Total Time 14.00(14.00)\n",
      "Iter 2685 | Time 58.9594(60.3277) | Bit/dim 3.6824(3.6847) | Xent 0.1225(0.1237) | Loss 3.7436(3.7466) | Error 0.0411(0.0426) Steps 670(667.97) | Grad Norm 0.8018(1.0747) | Total Time 14.00(14.00)\n",
      "Iter 2686 | Time 59.5951(60.3057) | Bit/dim 3.6890(3.6849) | Xent 0.1266(0.1238) | Loss 3.7523(3.7467) | Error 0.0426(0.0426) Steps 664(667.85) | Grad Norm 1.0947(1.0753) | Total Time 14.00(14.00)\n",
      "Iter 2687 | Time 59.2654(60.2745) | Bit/dim 3.6902(3.6850) | Xent 0.1254(0.1238) | Loss 3.7529(3.7469) | Error 0.0415(0.0425) Steps 670(667.92) | Grad Norm 0.9532(1.0716) | Total Time 14.00(14.00)\n",
      "Iter 2688 | Time 61.3970(60.3082) | Bit/dim 3.6827(3.6849) | Xent 0.1226(0.1238) | Loss 3.7440(3.7468) | Error 0.0443(0.0426) Steps 670(667.98) | Grad Norm 0.9329(1.0674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 24.0960, Epoch Time 397.2320(403.6981), Bit/dim 3.7049(best: 3.7042), Xent 2.5217, Loss 4.9658, Error 0.4264(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2689 | Time 59.8060(60.2931) | Bit/dim 3.6794(3.6848) | Xent 0.1167(0.1236) | Loss 3.7377(3.7466) | Error 0.0396(0.0425) Steps 676(668.22) | Grad Norm 1.0583(1.0672) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 60.3957(60.2962) | Bit/dim 3.6917(3.6850) | Xent 0.1140(0.1233) | Loss 3.7487(3.7466) | Error 0.0389(0.0424) Steps 664(668.09) | Grad Norm 0.9563(1.0638) | Total Time 14.00(14.00)\n",
      "Iter 2691 | Time 58.2224(60.2340) | Bit/dim 3.6861(3.6850) | Xent 0.1152(0.1230) | Loss 3.7437(3.7465) | Error 0.0394(0.0423) Steps 664(667.97) | Grad Norm 0.9468(1.0603) | Total Time 14.00(14.00)\n",
      "Iter 2692 | Time 59.9186(60.2245) | Bit/dim 3.6816(3.6849) | Xent 0.1319(0.1233) | Loss 3.7476(3.7466) | Error 0.0454(0.0424) Steps 670(668.03) | Grad Norm 1.3937(1.0703) | Total Time 14.00(14.00)\n",
      "Iter 2693 | Time 60.7034(60.2389) | Bit/dim 3.6732(3.6846) | Xent 0.1278(0.1234) | Loss 3.7370(3.7463) | Error 0.0420(0.0424) Steps 676(668.27) | Grad Norm 1.4848(1.0828) | Total Time 14.00(14.00)\n",
      "Iter 2694 | Time 60.3301(60.2416) | Bit/dim 3.6920(3.6848) | Xent 0.1267(0.1235) | Loss 3.7554(3.7466) | Error 0.0427(0.0424) Steps 664(668.14) | Grad Norm 1.0197(1.0809) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 24.2357, Epoch Time 399.3853(403.5688), Bit/dim 3.7055(best: 3.7042), Xent 2.5186, Loss 4.9648, Error 0.4320(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2695 | Time 60.9071(60.2616) | Bit/dim 3.6855(3.6848) | Xent 0.1127(0.1232) | Loss 3.7419(3.7464) | Error 0.0371(0.0422) Steps 664(668.02) | Grad Norm 0.8044(1.0726) | Total Time 14.00(14.00)\n",
      "Iter 2696 | Time 59.5548(60.2404) | Bit/dim 3.6796(3.6847) | Xent 0.1241(0.1232) | Loss 3.7417(3.7463) | Error 0.0425(0.0422) Steps 670(668.08) | Grad Norm 1.3377(1.0805) | Total Time 14.00(14.00)\n",
      "Iter 2697 | Time 61.4268(60.2760) | Bit/dim 3.6831(3.6846) | Xent 0.1148(0.1230) | Loss 3.7405(3.7461) | Error 0.0399(0.0422) Steps 676(668.31) | Grad Norm 1.2224(1.0848) | Total Time 14.00(14.00)\n",
      "Iter 2698 | Time 58.4212(60.2203) | Bit/dim 3.6946(3.6849) | Xent 0.1215(0.1229) | Loss 3.7553(3.7464) | Error 0.0411(0.0421) Steps 670(668.36) | Grad Norm 1.4116(1.0946) | Total Time 14.00(14.00)\n",
      "Iter 2699 | Time 61.2642(60.2516) | Bit/dim 3.6828(3.6848) | Xent 0.1220(0.1229) | Loss 3.7438(3.7463) | Error 0.0397(0.0421) Steps 670(668.41) | Grad Norm 0.9852(1.0913) | Total Time 14.00(14.00)\n",
      "Iter 2700 | Time 59.8099(60.2384) | Bit/dim 3.6852(3.6849) | Xent 0.1262(0.1230) | Loss 3.7483(3.7464) | Error 0.0436(0.0421) Steps 670(668.46) | Grad Norm 1.0521(1.0901) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 23.9290, Epoch Time 401.0594(403.4935), Bit/dim 3.7057(best: 3.7042), Xent 2.5055, Loss 4.9585, Error 0.4279(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2701 | Time 59.3546(60.2119) | Bit/dim 3.6886(3.6850) | Xent 0.1263(0.1231) | Loss 3.7517(3.7465) | Error 0.0441(0.0422) Steps 670(668.51) | Grad Norm 0.7702(1.0805) | Total Time 14.00(14.00)\n",
      "Iter 2702 | Time 61.2511(60.2431) | Bit/dim 3.6827(3.6849) | Xent 0.1289(0.1233) | Loss 3.7472(3.7465) | Error 0.0454(0.0423) Steps 658(668.19) | Grad Norm 1.3161(1.0876) | Total Time 14.00(14.00)\n",
      "Iter 2703 | Time 60.5387(60.2519) | Bit/dim 3.6791(3.6847) | Xent 0.1208(0.1232) | Loss 3.7395(3.7463) | Error 0.0415(0.0422) Steps 670(668.25) | Grad Norm 1.1756(1.0902) | Total Time 14.00(14.00)\n",
      "Iter 2704 | Time 61.0660(60.2764) | Bit/dim 3.6881(3.6848) | Xent 0.1262(0.1233) | Loss 3.7512(3.7465) | Error 0.0420(0.0422) Steps 664(668.12) | Grad Norm 1.2218(1.0942) | Total Time 14.00(14.00)\n",
      "Iter 2705 | Time 60.7187(60.2896) | Bit/dim 3.6972(3.6852) | Xent 0.1232(0.1233) | Loss 3.7588(3.7468) | Error 0.0411(0.0422) Steps 664(668.00) | Grad Norm 0.7778(1.0847) | Total Time 14.00(14.00)\n",
      "Iter 2706 | Time 60.0229(60.2816) | Bit/dim 3.6672(3.6847) | Xent 0.1150(0.1231) | Loss 3.7247(3.7462) | Error 0.0390(0.0421) Steps 658(667.70) | Grad Norm 1.2592(1.0899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 24.4405, Epoch Time 403.1447(403.4830), Bit/dim 3.7050(best: 3.7042), Xent 2.5000, Loss 4.9550, Error 0.4251(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2707 | Time 60.9896(60.3029) | Bit/dim 3.6907(3.6848) | Xent 0.1158(0.1228) | Loss 3.7486(3.7463) | Error 0.0366(0.0419) Steps 670(667.76) | Grad Norm 1.2138(1.0936) | Total Time 14.00(14.00)\n",
      "Iter 2708 | Time 60.8299(60.3187) | Bit/dim 3.6837(3.6848) | Xent 0.1087(0.1224) | Loss 3.7381(3.7460) | Error 0.0356(0.0418) Steps 658(667.47) | Grad Norm 0.9955(1.0907) | Total Time 14.00(14.00)\n",
      "Iter 2709 | Time 60.7775(60.3324) | Bit/dim 3.6822(3.6847) | Xent 0.1236(0.1224) | Loss 3.7440(3.7459) | Error 0.0434(0.0418) Steps 658(667.19) | Grad Norm 0.9479(1.0864) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 57.6658(60.2524) | Bit/dim 3.6869(3.6848) | Xent 0.1186(0.1223) | Loss 3.7462(3.7460) | Error 0.0424(0.0418) Steps 670(667.27) | Grad Norm 1.2850(1.0924) | Total Time 14.00(14.00)\n",
      "Iter 2711 | Time 60.7923(60.2686) | Bit/dim 3.6758(3.6845) | Xent 0.1229(0.1224) | Loss 3.7373(3.7457) | Error 0.0411(0.0418) Steps 664(667.17) | Grad Norm 0.9750(1.0889) | Total Time 14.00(14.00)\n",
      "Iter 2712 | Time 60.9563(60.2893) | Bit/dim 3.6836(3.6845) | Xent 0.1283(0.1225) | Loss 3.7478(3.7458) | Error 0.0444(0.0419) Steps 676(667.44) | Grad Norm 0.9600(1.0850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 24.0365, Epoch Time 401.7045(403.4297), Bit/dim 3.7053(best: 3.7042), Xent 2.5079, Loss 4.9592, Error 0.4276(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2713 | Time 61.3331(60.3206) | Bit/dim 3.6891(3.6846) | Xent 0.1189(0.1224) | Loss 3.7485(3.7458) | Error 0.0410(0.0418) Steps 670(667.52) | Grad Norm 0.9803(1.0818) | Total Time 14.00(14.00)\n",
      "Iter 2714 | Time 61.9873(60.3706) | Bit/dim 3.6902(3.6848) | Xent 0.1153(0.1222) | Loss 3.7479(3.7459) | Error 0.0393(0.0418) Steps 658(667.23) | Grad Norm 1.1150(1.0828) | Total Time 14.00(14.00)\n",
      "Iter 2715 | Time 59.8509(60.3550) | Bit/dim 3.6886(3.6849) | Xent 0.1184(0.1221) | Loss 3.7478(3.7460) | Error 0.0405(0.0417) Steps 658(666.95) | Grad Norm 0.9731(1.0796) | Total Time 14.00(14.00)\n",
      "Iter 2716 | Time 58.8885(60.3110) | Bit/dim 3.6824(3.6848) | Xent 0.1185(0.1220) | Loss 3.7416(3.7458) | Error 0.0401(0.0417) Steps 682(667.40) | Grad Norm 0.9467(1.0756) | Total Time 14.00(14.00)\n",
      "Iter 2717 | Time 58.5225(60.2573) | Bit/dim 3.6848(3.6848) | Xent 0.1164(0.1218) | Loss 3.7430(3.7457) | Error 0.0415(0.0417) Steps 664(667.30) | Grad Norm 1.1978(1.0792) | Total Time 14.00(14.00)\n",
      "Iter 2718 | Time 60.5607(60.2664) | Bit/dim 3.6777(3.6846) | Xent 0.1249(0.1219) | Loss 3.7401(3.7456) | Error 0.0429(0.0417) Steps 664(667.20) | Grad Norm 1.3182(1.0864) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 24.0046, Epoch Time 400.9595(403.3556), Bit/dim 3.7046(best: 3.7042), Xent 2.5069, Loss 4.9581, Error 0.4256(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2719 | Time 59.8463(60.2538) | Bit/dim 3.6854(3.6846) | Xent 0.1186(0.1218) | Loss 3.7447(3.7456) | Error 0.0394(0.0416) Steps 664(667.11) | Grad Norm 0.9855(1.0834) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 60.2373(60.2533) | Bit/dim 3.6975(3.6850) | Xent 0.1237(0.1219) | Loss 3.7594(3.7460) | Error 0.0430(0.0417) Steps 670(667.19) | Grad Norm 0.8597(1.0767) | Total Time 14.00(14.00)\n",
      "Iter 2721 | Time 59.3197(60.2253) | Bit/dim 3.6767(3.6848) | Xent 0.1180(0.1218) | Loss 3.7357(3.7457) | Error 0.0440(0.0418) Steps 670(667.28) | Grad Norm 1.2281(1.0812) | Total Time 14.00(14.00)\n",
      "Iter 2722 | Time 60.4441(60.2319) | Bit/dim 3.6810(3.6847) | Xent 0.1173(0.1216) | Loss 3.7397(3.7455) | Error 0.0387(0.0417) Steps 664(667.18) | Grad Norm 0.9184(1.0763) | Total Time 14.00(14.00)\n",
      "Iter 2723 | Time 60.2409(60.2322) | Bit/dim 3.6865(3.6847) | Xent 0.1177(0.1215) | Loss 3.7453(3.7455) | Error 0.0370(0.0415) Steps 670(667.26) | Grad Norm 0.9580(1.0728) | Total Time 14.00(14.00)\n",
      "Iter 2724 | Time 61.5412(60.2714) | Bit/dim 3.6818(3.6846) | Xent 0.1221(0.1215) | Loss 3.7429(3.7454) | Error 0.0420(0.0415) Steps 664(667.17) | Grad Norm 0.8876(1.0672) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 24.4738, Epoch Time 402.0214(403.3155), Bit/dim 3.7060(best: 3.7042), Xent 2.5203, Loss 4.9661, Error 0.4274(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2725 | Time 58.8725(60.2295) | Bit/dim 3.6852(3.6847) | Xent 0.1153(0.1213) | Loss 3.7429(3.7453) | Error 0.0405(0.0415) Steps 670(667.25) | Grad Norm 1.1704(1.0703) | Total Time 14.00(14.00)\n",
      "Iter 2726 | Time 60.5978(60.2405) | Bit/dim 3.6843(3.6846) | Xent 0.1116(0.1210) | Loss 3.7401(3.7452) | Error 0.0365(0.0414) Steps 658(666.97) | Grad Norm 1.3653(1.0792) | Total Time 14.00(14.00)\n",
      "Iter 2727 | Time 62.1830(60.2988) | Bit/dim 3.6826(3.6846) | Xent 0.1162(0.1209) | Loss 3.7407(3.7450) | Error 0.0363(0.0412) Steps 676(667.24) | Grad Norm 0.8511(1.0723) | Total Time 14.00(14.00)\n",
      "Iter 2728 | Time 58.1817(60.2353) | Bit/dim 3.6858(3.6846) | Xent 0.1162(0.1208) | Loss 3.7439(3.7450) | Error 0.0371(0.0411) Steps 664(667.15) | Grad Norm 1.2656(1.0781) | Total Time 14.00(14.00)\n",
      "Iter 2729 | Time 61.5416(60.2745) | Bit/dim 3.6906(3.6848) | Xent 0.1205(0.1207) | Loss 3.7509(3.7452) | Error 0.0420(0.0411) Steps 664(667.05) | Grad Norm 0.8778(1.0721) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 59.9704(60.2653) | Bit/dim 3.6775(3.6846) | Xent 0.1228(0.1208) | Loss 3.7389(3.7450) | Error 0.0421(0.0411) Steps 682(667.50) | Grad Norm 0.8624(1.0658) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 24.3190, Epoch Time 401.5515(403.2626), Bit/dim 3.7058(best: 3.7042), Xent 2.5508, Loss 4.9812, Error 0.4303(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2731 | Time 60.2609(60.2652) | Bit/dim 3.6764(3.6843) | Xent 0.1231(0.1209) | Loss 3.7380(3.7448) | Error 0.0425(0.0412) Steps 676(667.76) | Grad Norm 1.1075(1.0671) | Total Time 14.00(14.00)\n",
      "Iter 2732 | Time 59.3776(60.2386) | Bit/dim 3.6922(3.6846) | Xent 0.1068(0.1205) | Loss 3.7457(3.7448) | Error 0.0356(0.0410) Steps 670(667.82) | Grad Norm 1.0406(1.0663) | Total Time 14.00(14.00)\n",
      "Iter 2733 | Time 59.6905(60.2221) | Bit/dim 3.6871(3.6846) | Xent 0.1120(0.1202) | Loss 3.7430(3.7447) | Error 0.0369(0.0409) Steps 670(667.89) | Grad Norm 1.0004(1.0643) | Total Time 14.00(14.00)\n",
      "Iter 2734 | Time 62.0410(60.2767) | Bit/dim 3.6857(3.6847) | Xent 0.1187(0.1202) | Loss 3.7450(3.7448) | Error 0.0405(0.0409) Steps 658(667.59) | Grad Norm 0.8943(1.0592) | Total Time 14.00(14.00)\n",
      "Iter 2735 | Time 60.3605(60.2792) | Bit/dim 3.6835(3.6846) | Xent 0.1129(0.1199) | Loss 3.7399(3.7446) | Error 0.0389(0.0408) Steps 670(667.66) | Grad Norm 0.7457(1.0498) | Total Time 14.00(14.00)\n",
      "Iter 2736 | Time 60.9567(60.2995) | Bit/dim 3.6800(3.6845) | Xent 0.1110(0.1197) | Loss 3.7355(3.7443) | Error 0.0409(0.0408) Steps 664(667.55) | Grad Norm 0.8624(1.0442) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 24.2756, Epoch Time 403.0571(403.2564), Bit/dim 3.7055(best: 3.7042), Xent 2.5524, Loss 4.9817, Error 0.4310(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2737 | Time 59.6855(60.2811) | Bit/dim 3.6822(3.6844) | Xent 0.1154(0.1195) | Loss 3.7399(3.7442) | Error 0.0391(0.0408) Steps 676(667.81) | Grad Norm 0.9252(1.0406) | Total Time 14.00(14.00)\n",
      "Iter 2738 | Time 59.8386(60.2679) | Bit/dim 3.6858(3.6845) | Xent 0.1157(0.1194) | Loss 3.7437(3.7442) | Error 0.0376(0.0407) Steps 664(667.69) | Grad Norm 0.7926(1.0332) | Total Time 14.00(14.00)\n",
      "Iter 2739 | Time 60.5616(60.2767) | Bit/dim 3.6845(3.6845) | Xent 0.1184(0.1194) | Loss 3.7437(3.7442) | Error 0.0421(0.0407) Steps 670(667.76) | Grad Norm 0.8950(1.0290) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 58.8142(60.2328) | Bit/dim 3.6968(3.6848) | Xent 0.1049(0.1190) | Loss 3.7493(3.7443) | Error 0.0384(0.0406) Steps 658(667.47) | Grad Norm 1.0853(1.0307) | Total Time 14.00(14.00)\n",
      "Iter 2741 | Time 57.7994(60.1598) | Bit/dim 3.6831(3.6848) | Xent 0.1148(0.1188) | Loss 3.7405(3.7442) | Error 0.0391(0.0406) Steps 664(667.37) | Grad Norm 0.9873(1.0294) | Total Time 14.00(14.00)\n",
      "Iter 2742 | Time 61.1004(60.1880) | Bit/dim 3.6670(3.6843) | Xent 0.1220(0.1189) | Loss 3.7280(3.7437) | Error 0.0399(0.0406) Steps 652(666.90) | Grad Norm 1.2126(1.0349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 24.2121, Epoch Time 398.3646(403.1097), Bit/dim 3.7042(best: 3.7042), Xent 2.5277, Loss 4.9681, Error 0.4311(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2743 | Time 60.0804(60.1848) | Bit/dim 3.6842(3.6843) | Xent 0.1172(0.1189) | Loss 3.7428(3.7437) | Error 0.0406(0.0406) Steps 652(666.46) | Grad Norm 0.7129(1.0252) | Total Time 14.00(14.00)\n",
      "Iter 2744 | Time 61.0488(60.2107) | Bit/dim 3.6807(3.6841) | Xent 0.1234(0.1190) | Loss 3.7424(3.7437) | Error 0.0431(0.0407) Steps 676(666.74) | Grad Norm 1.0209(1.0251) | Total Time 14.00(14.00)\n",
      "Iter 2745 | Time 61.4838(60.2489) | Bit/dim 3.6730(3.6838) | Xent 0.1063(0.1186) | Loss 3.7261(3.7431) | Error 0.0386(0.0406) Steps 670(666.84) | Grad Norm 1.1081(1.0276) | Total Time 14.00(14.00)\n",
      "Iter 2746 | Time 59.5264(60.2272) | Bit/dim 3.6900(3.6840) | Xent 0.1145(0.1185) | Loss 3.7472(3.7433) | Error 0.0406(0.0406) Steps 670(666.94) | Grad Norm 1.1174(1.0303) | Total Time 14.00(14.00)\n",
      "Iter 2747 | Time 61.4915(60.2651) | Bit/dim 3.6885(3.6841) | Xent 0.1144(0.1184) | Loss 3.7457(3.7433) | Error 0.0391(0.0406) Steps 670(667.03) | Grad Norm 0.8828(1.0259) | Total Time 14.00(14.00)\n",
      "Iter 2748 | Time 59.7108(60.2485) | Bit/dim 3.6804(3.6840) | Xent 0.1131(0.1182) | Loss 3.7370(3.7431) | Error 0.0377(0.0405) Steps 664(666.94) | Grad Norm 1.0110(1.0254) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 24.4833, Epoch Time 404.0133(403.1368), Bit/dim 3.7054(best: 3.7042), Xent 2.5685, Loss 4.9897, Error 0.4279(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2749 | Time 60.2678(60.2491) | Bit/dim 3.6756(3.6838) | Xent 0.1220(0.1183) | Loss 3.7366(3.7429) | Error 0.0401(0.0405) Steps 664(666.85) | Grad Norm 1.3728(1.0358) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 61.0314(60.2726) | Bit/dim 3.6842(3.6838) | Xent 0.1139(0.1182) | Loss 3.7412(3.7429) | Error 0.0377(0.0404) Steps 670(666.94) | Grad Norm 1.0403(1.0360) | Total Time 14.00(14.00)\n",
      "Iter 2751 | Time 59.8402(60.2596) | Bit/dim 3.6853(3.6838) | Xent 0.1182(0.1182) | Loss 3.7444(3.7429) | Error 0.0387(0.0403) Steps 658(666.68) | Grad Norm 0.9585(1.0337) | Total Time 14.00(14.00)\n",
      "Iter 2752 | Time 61.9233(60.3095) | Bit/dim 3.6790(3.6837) | Xent 0.1130(0.1181) | Loss 3.7356(3.7427) | Error 0.0387(0.0403) Steps 670(666.78) | Grad Norm 1.1871(1.0383) | Total Time 14.00(14.00)\n",
      "Iter 2753 | Time 60.9782(60.3296) | Bit/dim 3.6924(3.6839) | Xent 0.1206(0.1181) | Loss 3.7527(3.7430) | Error 0.0445(0.0404) Steps 658(666.51) | Grad Norm 1.3101(1.0464) | Total Time 14.00(14.00)\n",
      "Iter 2754 | Time 58.9003(60.2867) | Bit/dim 3.6869(3.6840) | Xent 0.1110(0.1179) | Loss 3.7424(3.7430) | Error 0.0357(0.0403) Steps 664(666.44) | Grad Norm 1.1910(1.0508) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 24.1269, Epoch Time 403.3454(403.1431), Bit/dim 3.7060(best: 3.7042), Xent 2.5465, Loss 4.9793, Error 0.4296(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2755 | Time 61.7961(60.3320) | Bit/dim 3.6823(3.6840) | Xent 0.1118(0.1177) | Loss 3.7382(3.7428) | Error 0.0374(0.0402) Steps 670(666.54) | Grad Norm 1.1428(1.0535) | Total Time 14.00(14.00)\n",
      "Iter 2756 | Time 58.0858(60.2646) | Bit/dim 3.6798(3.6839) | Xent 0.1091(0.1175) | Loss 3.7343(3.7426) | Error 0.0376(0.0401) Steps 676(666.83) | Grad Norm 1.3971(1.0638) | Total Time 14.00(14.00)\n",
      "Iter 2757 | Time 58.1026(60.1997) | Bit/dim 3.6833(3.6838) | Xent 0.1128(0.1173) | Loss 3.7398(3.7425) | Error 0.0409(0.0401) Steps 658(666.56) | Grad Norm 0.9092(1.0592) | Total Time 14.00(14.00)\n",
      "Iter 2758 | Time 61.7115(60.2451) | Bit/dim 3.6953(3.6842) | Xent 0.1145(0.1172) | Loss 3.7525(3.7428) | Error 0.0420(0.0402) Steps 652(666.13) | Grad Norm 1.0617(1.0593) | Total Time 14.00(14.00)\n",
      "Iter 2759 | Time 59.9184(60.2353) | Bit/dim 3.6858(3.6842) | Xent 0.1121(0.1171) | Loss 3.7418(3.7428) | Error 0.0395(0.0402) Steps 664(666.06) | Grad Norm 1.0423(1.0587) | Total Time 14.00(14.00)\n",
      "Iter 2760 | Time 60.0926(60.2310) | Bit/dim 3.6793(3.6841) | Xent 0.1144(0.1170) | Loss 3.7365(3.7426) | Error 0.0401(0.0402) Steps 658(665.82) | Grad Norm 0.9588(1.0557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 24.1414, Epoch Time 400.1574(403.0535), Bit/dim 3.7050(best: 3.7042), Xent 2.5694, Loss 4.9897, Error 0.4269(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2761 | Time 59.2317(60.2010) | Bit/dim 3.6817(3.6840) | Xent 0.1124(0.1169) | Loss 3.7380(3.7425) | Error 0.0376(0.0401) Steps 664(665.77) | Grad Norm 0.6646(1.0440) | Total Time 14.00(14.00)\n",
      "Iter 2762 | Time 59.2298(60.1719) | Bit/dim 3.6842(3.6840) | Xent 0.1072(0.1166) | Loss 3.7378(3.7423) | Error 0.0379(0.0400) Steps 670(665.89) | Grad Norm 0.8209(1.0373) | Total Time 14.00(14.00)\n",
      "Iter 2763 | Time 59.3017(60.1458) | Bit/dim 3.6826(3.6840) | Xent 0.1142(0.1165) | Loss 3.7397(3.7422) | Error 0.0377(0.0400) Steps 658(665.66) | Grad Norm 0.9522(1.0348) | Total Time 14.00(14.00)\n",
      "Iter 2764 | Time 59.9235(60.1391) | Bit/dim 3.6873(3.6841) | Xent 0.1091(0.1163) | Loss 3.7418(3.7422) | Error 0.0385(0.0399) Steps 664(665.61) | Grad Norm 1.0144(1.0342) | Total Time 14.00(14.00)\n",
      "Iter 2765 | Time 59.8377(60.1301) | Bit/dim 3.6870(3.6842) | Xent 0.1124(0.1162) | Loss 3.7432(3.7423) | Error 0.0364(0.0398) Steps 670(665.74) | Grad Norm 1.0735(1.0353) | Total Time 14.00(14.00)\n",
      "Iter 2766 | Time 60.1128(60.1295) | Bit/dim 3.6846(3.6842) | Xent 0.1244(0.1164) | Loss 3.7468(3.7424) | Error 0.0387(0.0398) Steps 676(666.05) | Grad Norm 1.0737(1.0365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 24.2647, Epoch Time 397.7814(402.8953), Bit/dim 3.7054(best: 3.7042), Xent 2.5560, Loss 4.9834, Error 0.4262(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2767 | Time 61.2323(60.1626) | Bit/dim 3.6904(3.6844) | Xent 0.1185(0.1165) | Loss 3.7496(3.7426) | Error 0.0405(0.0398) Steps 670(666.16) | Grad Norm 0.8554(1.0311) | Total Time 14.00(14.00)\n",
      "Iter 2768 | Time 60.0003(60.1578) | Bit/dim 3.6806(3.6842) | Xent 0.1147(0.1164) | Loss 3.7380(3.7425) | Error 0.0385(0.0398) Steps 670(666.28) | Grad Norm 0.8679(1.0262) | Total Time 14.00(14.00)\n",
      "Iter 2769 | Time 61.8892(60.2097) | Bit/dim 3.6825(3.6842) | Xent 0.1078(0.1162) | Loss 3.7364(3.7423) | Error 0.0371(0.0397) Steps 670(666.39) | Grad Norm 1.0372(1.0265) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 59.9270(60.2012) | Bit/dim 3.6785(3.6840) | Xent 0.1177(0.1162) | Loss 3.7373(3.7421) | Error 0.0393(0.0397) Steps 670(666.50) | Grad Norm 1.8122(1.0501) | Total Time 14.00(14.00)\n",
      "Iter 2771 | Time 61.2727(60.2334) | Bit/dim 3.6806(3.6839) | Xent 0.1110(0.1161) | Loss 3.7361(3.7420) | Error 0.0393(0.0396) Steps 670(666.60) | Grad Norm 1.2980(1.0575) | Total Time 14.00(14.00)\n",
      "Iter 2772 | Time 62.5960(60.3042) | Bit/dim 3.6910(3.6841) | Xent 0.1148(0.1160) | Loss 3.7484(3.7421) | Error 0.0384(0.0396) Steps 664(666.53) | Grad Norm 0.9743(1.0550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 24.2404, Epoch Time 407.2279(403.0253), Bit/dim 3.7047(best: 3.7042), Xent 2.5576, Loss 4.9835, Error 0.4322(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2773 | Time 60.9456(60.3235) | Bit/dim 3.6811(3.6840) | Xent 0.1150(0.1160) | Loss 3.7386(3.7420) | Error 0.0390(0.0396) Steps 670(666.63) | Grad Norm 0.8911(1.0501) | Total Time 14.00(14.00)\n",
      "Iter 2774 | Time 59.9957(60.3137) | Bit/dim 3.6747(3.6838) | Xent 0.1213(0.1161) | Loss 3.7354(3.7418) | Error 0.0409(0.0396) Steps 676(666.91) | Grad Norm 1.7423(1.0709) | Total Time 14.00(14.00)\n",
      "Iter 2775 | Time 60.3399(60.3144) | Bit/dim 3.6862(3.6838) | Xent 0.1149(0.1161) | Loss 3.7437(3.7419) | Error 0.0384(0.0396) Steps 664(666.82) | Grad Norm 1.8103(1.0930) | Total Time 14.00(14.00)\n",
      "Iter 2776 | Time 61.6642(60.3549) | Bit/dim 3.6931(3.6841) | Xent 0.1189(0.1162) | Loss 3.7525(3.7422) | Error 0.0407(0.0396) Steps 682(667.28) | Grad Norm 1.3336(1.1003) | Total Time 14.00(14.00)\n",
      "Iter 2777 | Time 58.7218(60.3059) | Bit/dim 3.6876(3.6842) | Xent 0.1210(0.1163) | Loss 3.7482(3.7424) | Error 0.0405(0.0397) Steps 670(667.36) | Grad Norm 1.0632(1.0991) | Total Time 14.00(14.00)\n",
      "Iter 2778 | Time 59.1371(60.2709) | Bit/dim 3.6828(3.6842) | Xent 0.1232(0.1165) | Loss 3.7444(3.7425) | Error 0.0407(0.0397) Steps 682(667.80) | Grad Norm 1.0303(1.0971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 24.4403, Epoch Time 400.8161(402.9590), Bit/dim 3.7058(best: 3.7042), Xent 2.5752, Loss 4.9934, Error 0.4288(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2779 | Time 60.4864(60.2773) | Bit/dim 3.6795(3.6840) | Xent 0.1274(0.1169) | Loss 3.7432(3.7425) | Error 0.0465(0.0399) Steps 664(667.69) | Grad Norm 2.1926(1.1299) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 58.6128(60.2274) | Bit/dim 3.6858(3.6841) | Xent 0.1221(0.1170) | Loss 3.7468(3.7426) | Error 0.0423(0.0400) Steps 670(667.76) | Grad Norm 1.5775(1.1434) | Total Time 14.00(14.00)\n",
      "Iter 2781 | Time 60.8022(60.2446) | Bit/dim 3.6840(3.6841) | Xent 0.1203(0.1171) | Loss 3.7442(3.7427) | Error 0.0433(0.0401) Steps 658(667.46) | Grad Norm 0.7739(1.1323) | Total Time 14.00(14.00)\n",
      "Iter 2782 | Time 59.8249(60.2321) | Bit/dim 3.6814(3.6840) | Xent 0.1210(0.1172) | Loss 3.7419(3.7426) | Error 0.0397(0.0401) Steps 670(667.54) | Grad Norm 0.7480(1.1208) | Total Time 14.00(14.00)\n",
      "Iter 2783 | Time 59.0620(60.1970) | Bit/dim 3.6883(3.6841) | Xent 0.1238(0.1174) | Loss 3.7502(3.7429) | Error 0.0437(0.0402) Steps 670(667.61) | Grad Norm 1.0321(1.1181) | Total Time 14.00(14.00)\n",
      "Iter 2784 | Time 61.3894(60.2327) | Bit/dim 3.6824(3.6841) | Xent 0.1276(0.1177) | Loss 3.7463(3.7430) | Error 0.0441(0.0403) Steps 670(667.68) | Grad Norm 1.6670(1.1346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 24.2121, Epoch Time 400.4855(402.8848), Bit/dim 3.7057(best: 3.7042), Xent 2.5682, Loss 4.9898, Error 0.4328(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2785 | Time 60.6067(60.2439) | Bit/dim 3.6799(3.6840) | Xent 0.1283(0.1181) | Loss 3.7441(3.7430) | Error 0.0454(0.0404) Steps 664(667.57) | Grad Norm 1.7152(1.1520) | Total Time 14.00(14.00)\n",
      "Iter 2786 | Time 58.7836(60.2001) | Bit/dim 3.6905(3.6842) | Xent 0.1219(0.1182) | Loss 3.7514(3.7432) | Error 0.0423(0.0405) Steps 670(667.65) | Grad Norm 0.8440(1.1427) | Total Time 14.00(14.00)\n",
      "Iter 2787 | Time 63.6822(60.3046) | Bit/dim 3.6894(3.6843) | Xent 0.1209(0.1183) | Loss 3.7499(3.7434) | Error 0.0415(0.0405) Steps 670(667.72) | Grad Norm 0.8298(1.1334) | Total Time 14.00(14.00)\n",
      "Iter 2788 | Time 60.7253(60.3172) | Bit/dim 3.6825(3.6843) | Xent 0.1222(0.1184) | Loss 3.7436(3.7434) | Error 0.0425(0.0406) Steps 652(667.25) | Grad Norm 1.5649(1.1463) | Total Time 14.00(14.00)\n",
      "Iter 2789 | Time 61.5643(60.3546) | Bit/dim 3.6852(3.6843) | Xent 0.1180(0.1184) | Loss 3.7441(3.7435) | Error 0.0417(0.0406) Steps 664(667.15) | Grad Norm 1.0968(1.1448) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 59.4006(60.3260) | Bit/dim 3.6786(3.6841) | Xent 0.1241(0.1185) | Loss 3.7407(3.7434) | Error 0.0435(0.0407) Steps 676(667.41) | Grad Norm 0.9027(1.1376) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 24.1927, Epoch Time 404.8579(402.9440), Bit/dim 3.7054(best: 3.7042), Xent 2.5450, Loss 4.9778, Error 0.4293(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2791 | Time 57.1117(60.2296) | Bit/dim 3.6832(3.6841) | Xent 0.1262(0.1188) | Loss 3.7463(3.7435) | Error 0.0429(0.0408) Steps 664(667.31) | Grad Norm 1.2057(1.1396) | Total Time 14.00(14.00)\n",
      "Iter 2792 | Time 60.1755(60.2280) | Bit/dim 3.6793(3.6839) | Xent 0.1213(0.1188) | Loss 3.7399(3.7434) | Error 0.0437(0.0409) Steps 664(667.21) | Grad Norm 1.1446(1.1398) | Total Time 14.00(14.00)\n",
      "Iter 2793 | Time 61.7878(60.2747) | Bit/dim 3.6857(3.6840) | Xent 0.1235(0.1190) | Loss 3.7475(3.7435) | Error 0.0427(0.0409) Steps 670(667.30) | Grad Norm 1.0625(1.1374) | Total Time 14.00(14.00)\n",
      "Iter 2794 | Time 59.8283(60.2614) | Bit/dim 3.6872(3.6841) | Xent 0.1200(0.1190) | Loss 3.7472(3.7436) | Error 0.0401(0.0409) Steps 664(667.20) | Grad Norm 1.2060(1.1395) | Total Time 14.00(14.00)\n",
      "Iter 2795 | Time 62.0737(60.3157) | Bit/dim 3.6846(3.6841) | Xent 0.1172(0.1190) | Loss 3.7431(3.7436) | Error 0.0425(0.0409) Steps 670(667.28) | Grad Norm 0.9138(1.1327) | Total Time 14.00(14.00)\n",
      "Iter 2796 | Time 58.1381(60.2504) | Bit/dim 3.6778(3.6839) | Xent 0.1232(0.1191) | Loss 3.7394(3.7435) | Error 0.0424(0.0410) Steps 664(667.18) | Grad Norm 1.2608(1.1366) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 24.1872, Epoch Time 399.0829(402.8282), Bit/dim 3.7058(best: 3.7042), Xent 2.5513, Loss 4.9815, Error 0.4278(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2797 | Time 58.8068(60.2071) | Bit/dim 3.6852(3.6840) | Xent 0.1128(0.1189) | Loss 3.7416(3.7434) | Error 0.0406(0.0410) Steps 670(667.27) | Grad Norm 0.9232(1.1302) | Total Time 14.00(14.00)\n",
      "Iter 2798 | Time 59.7605(60.1937) | Bit/dim 3.6801(3.6838) | Xent 0.1240(0.1190) | Loss 3.7421(3.7434) | Error 0.0440(0.0411) Steps 670(667.35) | Grad Norm 1.0821(1.1287) | Total Time 14.00(14.00)\n",
      "Iter 2799 | Time 62.0555(60.2495) | Bit/dim 3.6781(3.6837) | Xent 0.1138(0.1189) | Loss 3.7350(3.7431) | Error 0.0390(0.0410) Steps 682(667.79) | Grad Norm 0.8250(1.1196) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 61.1031(60.2752) | Bit/dim 3.6841(3.6837) | Xent 0.1264(0.1191) | Loss 3.7473(3.7432) | Error 0.0444(0.0411) Steps 664(667.67) | Grad Norm 1.2689(1.1241) | Total Time 14.00(14.00)\n",
      "Iter 2801 | Time 58.7755(60.2302) | Bit/dim 3.6814(3.6836) | Xent 0.1218(0.1192) | Loss 3.7423(3.7432) | Error 0.0411(0.0411) Steps 664(667.56) | Grad Norm 0.9732(1.1196) | Total Time 14.00(14.00)\n",
      "Iter 2802 | Time 61.5910(60.2710) | Bit/dim 3.6928(3.6839) | Xent 0.1188(0.1192) | Loss 3.7522(3.7435) | Error 0.0407(0.0411) Steps 670(667.64) | Grad Norm 1.1616(1.1208) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 24.3034, Epoch Time 402.6875(402.8240), Bit/dim 3.7050(best: 3.7042), Xent 2.5473, Loss 4.9787, Error 0.4286(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2803 | Time 59.2803(60.2413) | Bit/dim 3.6824(3.6838) | Xent 0.1205(0.1192) | Loss 3.7426(3.7435) | Error 0.0434(0.0412) Steps 670(667.71) | Grad Norm 0.9094(1.1145) | Total Time 14.00(14.00)\n",
      "Iter 2804 | Time 60.8179(60.2586) | Bit/dim 3.6981(3.6843) | Xent 0.1158(0.1191) | Loss 3.7560(3.7438) | Error 0.0395(0.0411) Steps 664(667.60) | Grad Norm 1.1024(1.1141) | Total Time 14.00(14.00)\n",
      "Iter 2805 | Time 60.6225(60.2695) | Bit/dim 3.6835(3.6843) | Xent 0.1164(0.1190) | Loss 3.7417(3.7438) | Error 0.0413(0.0411) Steps 664(667.49) | Grad Norm 0.9491(1.1092) | Total Time 14.00(14.00)\n",
      "Iter 2806 | Time 62.6073(60.3396) | Bit/dim 3.6858(3.6843) | Xent 0.1160(0.1189) | Loss 3.7437(3.7438) | Error 0.0414(0.0411) Steps 670(667.56) | Grad Norm 0.8353(1.1010) | Total Time 14.00(14.00)\n",
      "Iter 2807 | Time 59.3889(60.3111) | Bit/dim 3.6762(3.6841) | Xent 0.1201(0.1190) | Loss 3.7363(3.7435) | Error 0.0395(0.0411) Steps 658(667.28) | Grad Norm 0.8625(1.0938) | Total Time 14.00(14.00)\n",
      "Iter 2808 | Time 58.8498(60.2673) | Bit/dim 3.6794(3.6839) | Xent 0.1173(0.1189) | Loss 3.7380(3.7434) | Error 0.0404(0.0411) Steps 664(667.18) | Grad Norm 1.2369(1.0981) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 24.6162, Epoch Time 402.1853(402.8048), Bit/dim 3.7053(best: 3.7042), Xent 2.5882, Loss 4.9994, Error 0.4296(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2809 | Time 58.4850(60.2138) | Bit/dim 3.6824(3.6839) | Xent 0.1238(0.1191) | Loss 3.7442(3.7434) | Error 0.0419(0.0411) Steps 676(667.44) | Grad Norm 0.9415(1.0934) | Total Time 14.00(14.00)\n",
      "Iter 2810 | Time 60.6370(60.2265) | Bit/dim 3.6873(3.6840) | Xent 0.1295(0.1194) | Loss 3.7520(3.7437) | Error 0.0447(0.0412) Steps 676(667.70) | Grad Norm 1.0165(1.0911) | Total Time 14.00(14.00)\n",
      "Iter 2811 | Time 59.1772(60.1950) | Bit/dim 3.6767(3.6838) | Xent 0.1210(0.1194) | Loss 3.7372(3.7435) | Error 0.0420(0.0412) Steps 664(667.59) | Grad Norm 0.9910(1.0881) | Total Time 14.00(14.00)\n",
      "Iter 2812 | Time 61.2192(60.2257) | Bit/dim 3.6884(3.6839) | Xent 0.1165(0.1193) | Loss 3.7466(3.7436) | Error 0.0400(0.0412) Steps 664(667.48) | Grad Norm 0.9274(1.0833) | Total Time 14.00(14.00)\n",
      "Iter 2813 | Time 58.6684(60.1790) | Bit/dim 3.6823(3.6838) | Xent 0.1231(0.1195) | Loss 3.7439(3.7436) | Error 0.0427(0.0412) Steps 670(667.56) | Grad Norm 1.0726(1.0829) | Total Time 14.00(14.00)\n",
      "Iter 2814 | Time 59.6574(60.1634) | Bit/dim 3.6878(3.6840) | Xent 0.1225(0.1195) | Loss 3.7490(3.7437) | Error 0.0443(0.0413) Steps 682(667.99) | Grad Norm 1.0331(1.0815) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 24.7017, Epoch Time 398.4927(402.6754), Bit/dim 3.7062(best: 3.7042), Xent 2.5626, Loss 4.9875, Error 0.4285(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2815 | Time 59.2603(60.1363) | Bit/dim 3.6758(3.6837) | Xent 0.1223(0.1196) | Loss 3.7369(3.7435) | Error 0.0426(0.0414) Steps 658(667.69) | Grad Norm 0.8599(1.0748) | Total Time 14.00(14.00)\n",
      "Iter 2816 | Time 60.2878(60.1408) | Bit/dim 3.6859(3.6838) | Xent 0.1175(0.1196) | Loss 3.7447(3.7436) | Error 0.0414(0.0414) Steps 664(667.58) | Grad Norm 0.8716(1.0687) | Total Time 14.00(14.00)\n",
      "Iter 2817 | Time 59.9684(60.1357) | Bit/dim 3.6946(3.6841) | Xent 0.1118(0.1193) | Loss 3.7506(3.7438) | Error 0.0381(0.0413) Steps 664(667.47) | Grad Norm 1.1685(1.0717) | Total Time 14.00(14.00)\n",
      "Iter 2818 | Time 61.6145(60.1800) | Bit/dim 3.6814(3.6840) | Xent 0.1311(0.1197) | Loss 3.7470(3.7439) | Error 0.0463(0.0414) Steps 670(667.55) | Grad Norm 1.4420(1.0828) | Total Time 14.00(14.00)\n",
      "Iter 2819 | Time 60.0516(60.1762) | Bit/dim 3.6912(3.6842) | Xent 0.1103(0.1194) | Loss 3.7464(3.7439) | Error 0.0379(0.0413) Steps 658(667.26) | Grad Norm 1.1442(1.0847) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 61.0229(60.2016) | Bit/dim 3.6775(3.6840) | Xent 0.1225(0.1195) | Loss 3.7387(3.7438) | Error 0.0443(0.0414) Steps 676(667.52) | Grad Norm 1.1376(1.0862) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 24.5835, Epoch Time 402.7357(402.6772), Bit/dim 3.7057(best: 3.7042), Xent 2.5650, Loss 4.9882, Error 0.4288(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2821 | Time 60.9048(60.2227) | Bit/dim 3.6869(3.6841) | Xent 0.1190(0.1195) | Loss 3.7464(3.7439) | Error 0.0426(0.0414) Steps 664(667.42) | Grad Norm 1.1782(1.0890) | Total Time 14.00(14.00)\n",
      "Iter 2822 | Time 60.0687(60.2180) | Bit/dim 3.6914(3.6843) | Xent 0.1170(0.1194) | Loss 3.7499(3.7440) | Error 0.0401(0.0414) Steps 670(667.50) | Grad Norm 0.9099(1.0836) | Total Time 14.00(14.00)\n",
      "Iter 2823 | Time 59.0419(60.1828) | Bit/dim 3.6876(3.6844) | Xent 0.1209(0.1195) | Loss 3.7481(3.7442) | Error 0.0433(0.0414) Steps 670(667.57) | Grad Norm 1.1655(1.0861) | Total Time 14.00(14.00)\n",
      "Iter 2824 | Time 61.5742(60.2245) | Bit/dim 3.6781(3.6843) | Xent 0.1217(0.1195) | Loss 3.7389(3.7440) | Error 0.0406(0.0414) Steps 664(667.46) | Grad Norm 1.1441(1.0878) | Total Time 14.00(14.00)\n",
      "Iter 2825 | Time 61.2344(60.2548) | Bit/dim 3.6786(3.6841) | Xent 0.1200(0.1195) | Loss 3.7386(3.7439) | Error 0.0423(0.0414) Steps 676(667.72) | Grad Norm 0.9943(1.0850) | Total Time 14.00(14.00)\n",
      "Iter 2826 | Time 58.9116(60.2145) | Bit/dim 3.6805(3.6840) | Xent 0.1195(0.1195) | Loss 3.7402(3.7437) | Error 0.0413(0.0414) Steps 664(667.61) | Grad Norm 1.7643(1.1054) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 24.7095, Epoch Time 402.2557(402.6646), Bit/dim 3.7060(best: 3.7042), Xent 2.5770, Loss 4.9944, Error 0.4271(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2827 | Time 61.7126(60.2594) | Bit/dim 3.6907(3.6842) | Xent 0.1147(0.1194) | Loss 3.7481(3.7439) | Error 0.0411(0.0414) Steps 670(667.68) | Grad Norm 1.2636(1.1101) | Total Time 14.00(14.00)\n",
      "Iter 2828 | Time 58.6295(60.2106) | Bit/dim 3.6850(3.6842) | Xent 0.1141(0.1192) | Loss 3.7420(3.7438) | Error 0.0390(0.0414) Steps 664(667.57) | Grad Norm 0.8201(1.1014) | Total Time 14.00(14.00)\n",
      "Iter 2829 | Time 61.1125(60.2376) | Bit/dim 3.6882(3.6843) | Xent 0.1111(0.1190) | Loss 3.7437(3.7438) | Error 0.0361(0.0412) Steps 670(667.64) | Grad Norm 0.8801(1.0948) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 59.5130(60.2159) | Bit/dim 3.6723(3.6840) | Xent 0.1243(0.1191) | Loss 3.7344(3.7435) | Error 0.0423(0.0412) Steps 670(667.71) | Grad Norm 1.6359(1.1110) | Total Time 14.00(14.00)\n",
      "Iter 2831 | Time 59.8794(60.2058) | Bit/dim 3.6781(3.6838) | Xent 0.1143(0.1190) | Loss 3.7352(3.7433) | Error 0.0400(0.0412) Steps 664(667.60) | Grad Norm 1.3763(1.1190) | Total Time 14.00(14.00)\n",
      "Iter 2832 | Time 60.3981(60.2115) | Bit/dim 3.6906(3.6840) | Xent 0.1142(0.1189) | Loss 3.7478(3.7434) | Error 0.0386(0.0411) Steps 670(667.67) | Grad Norm 1.2526(1.1230) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 24.3540, Epoch Time 401.3674(402.6257), Bit/dim 3.7057(best: 3.7042), Xent 2.5700, Loss 4.9908, Error 0.4282(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2833 | Time 60.9596(60.2340) | Bit/dim 3.6924(3.6842) | Xent 0.1157(0.1188) | Loss 3.7503(3.7436) | Error 0.0389(0.0410) Steps 664(667.56) | Grad Norm 1.1917(1.1251) | Total Time 14.00(14.00)\n",
      "Iter 2834 | Time 57.7200(60.1586) | Bit/dim 3.6863(3.6843) | Xent 0.1169(0.1187) | Loss 3.7447(3.7437) | Error 0.0413(0.0411) Steps 670(667.64) | Grad Norm 0.9764(1.1206) | Total Time 14.00(14.00)\n",
      "Iter 2835 | Time 61.2610(60.1916) | Bit/dim 3.6750(3.6840) | Xent 0.1226(0.1188) | Loss 3.7363(3.7434) | Error 0.0430(0.0411) Steps 676(667.89) | Grad Norm 1.6660(1.1370) | Total Time 14.00(14.00)\n",
      "Iter 2836 | Time 61.2604(60.2237) | Bit/dim 3.6761(3.6838) | Xent 0.1083(0.1185) | Loss 3.7303(3.7430) | Error 0.0360(0.0410) Steps 670(667.95) | Grad Norm 1.1808(1.1383) | Total Time 14.00(14.00)\n",
      "Iter 2837 | Time 61.4181(60.2595) | Bit/dim 3.6846(3.6838) | Xent 0.1187(0.1185) | Loss 3.7440(3.7431) | Error 0.0416(0.0410) Steps 664(667.83) | Grad Norm 0.8932(1.1309) | Total Time 14.00(14.00)\n",
      "Iter 2838 | Time 59.6569(60.2415) | Bit/dim 3.6838(3.6838) | Xent 0.1136(0.1184) | Loss 3.7406(3.7430) | Error 0.0395(0.0409) Steps 670(667.90) | Grad Norm 1.5526(1.1436) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 24.3667, Epoch Time 402.5063(402.6221), Bit/dim 3.7049(best: 3.7042), Xent 2.5669, Loss 4.9883, Error 0.4307(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2839 | Time 62.4145(60.3067) | Bit/dim 3.6937(3.6841) | Xent 0.1162(0.1183) | Loss 3.7518(3.7433) | Error 0.0409(0.0409) Steps 664(667.78) | Grad Norm 1.0255(1.1400) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 60.5523(60.3140) | Bit/dim 3.6836(3.6841) | Xent 0.1245(0.1185) | Loss 3.7459(3.7433) | Error 0.0423(0.0410) Steps 670(667.85) | Grad Norm 1.2323(1.1428) | Total Time 14.00(14.00)\n",
      "Iter 2841 | Time 59.8015(60.2986) | Bit/dim 3.6838(3.6841) | Xent 0.1187(0.1185) | Loss 3.7431(3.7433) | Error 0.0415(0.0410) Steps 664(667.73) | Grad Norm 1.0364(1.1396) | Total Time 14.00(14.00)\n",
      "Iter 2842 | Time 59.4520(60.2732) | Bit/dim 3.6742(3.6838) | Xent 0.1158(0.1184) | Loss 3.7321(3.7430) | Error 0.0410(0.0410) Steps 682(668.16) | Grad Norm 1.3559(1.1461) | Total Time 14.00(14.00)\n",
      "Iter 2843 | Time 61.0025(60.2951) | Bit/dim 3.6815(3.6837) | Xent 0.1276(0.1187) | Loss 3.7453(3.7431) | Error 0.0451(0.0411) Steps 664(668.04) | Grad Norm 2.3063(1.1809) | Total Time 14.00(14.00)\n",
      "Iter 2844 | Time 59.2506(60.2638) | Bit/dim 3.6854(3.6838) | Xent 0.1194(0.1187) | Loss 3.7451(3.7431) | Error 0.0414(0.0411) Steps 670(668.09) | Grad Norm 1.1293(1.1793) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 24.6778, Epoch Time 403.0108(402.6338), Bit/dim 3.7039(best: 3.7042), Xent 2.5621, Loss 4.9849, Error 0.4322(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2845 | Time 58.7293(60.2178) | Bit/dim 3.6862(3.6838) | Xent 0.1127(0.1185) | Loss 3.7426(3.7431) | Error 0.0397(0.0411) Steps 664(667.97) | Grad Norm 1.0971(1.1769) | Total Time 14.00(14.00)\n",
      "Iter 2846 | Time 60.5836(60.2287) | Bit/dim 3.6815(3.6838) | Xent 0.1208(0.1186) | Loss 3.7419(3.7431) | Error 0.0426(0.0411) Steps 670(668.03) | Grad Norm 1.0024(1.1716) | Total Time 14.00(14.00)\n",
      "Iter 2847 | Time 58.4603(60.1757) | Bit/dim 3.6787(3.6836) | Xent 0.1276(0.1189) | Loss 3.7424(3.7431) | Error 0.0445(0.0412) Steps 676(668.27) | Grad Norm 1.1466(1.1709) | Total Time 14.00(14.00)\n",
      "Iter 2848 | Time 59.4507(60.1539) | Bit/dim 3.6855(3.6837) | Xent 0.1179(0.1188) | Loss 3.7444(3.7431) | Error 0.0411(0.0412) Steps 670(668.32) | Grad Norm 0.9670(1.1648) | Total Time 14.00(14.00)\n",
      "Iter 2849 | Time 61.7753(60.2026) | Bit/dim 3.6890(3.6838) | Xent 0.1297(0.1192) | Loss 3.7539(3.7434) | Error 0.0463(0.0414) Steps 664(668.19) | Grad Norm 1.2185(1.1664) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 60.1349(60.2005) | Bit/dim 3.6780(3.6837) | Xent 0.1134(0.1190) | Loss 3.7347(3.7432) | Error 0.0416(0.0414) Steps 670(668.25) | Grad Norm 0.7610(1.1542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 24.1385, Epoch Time 399.3661(402.5357), Bit/dim 3.7054(best: 3.7039), Xent 2.5631, Loss 4.9869, Error 0.4277(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2851 | Time 58.2949(60.1434) | Bit/dim 3.6814(3.6836) | Xent 0.1106(0.1187) | Loss 3.7367(3.7430) | Error 0.0380(0.0413) Steps 670(668.30) | Grad Norm 0.8986(1.1466) | Total Time 14.00(14.00)\n",
      "Iter 2852 | Time 58.8725(60.1052) | Bit/dim 3.6851(3.6836) | Xent 0.1110(0.1185) | Loss 3.7406(3.7429) | Error 0.0364(0.0411) Steps 670(668.35) | Grad Norm 1.5019(1.1572) | Total Time 14.00(14.00)\n",
      "Iter 2853 | Time 59.3098(60.0814) | Bit/dim 3.6768(3.6834) | Xent 0.1123(0.1183) | Loss 3.7329(3.7426) | Error 0.0387(0.0411) Steps 670(668.40) | Grad Norm 0.9452(1.1509) | Total Time 14.00(14.00)\n",
      "Iter 2854 | Time 61.4615(60.1228) | Bit/dim 3.6864(3.6835) | Xent 0.1247(0.1185) | Loss 3.7487(3.7428) | Error 0.0410(0.0411) Steps 676(668.63) | Grad Norm 1.0457(1.1477) | Total Time 14.00(14.00)\n",
      "Iter 2855 | Time 61.0072(60.1493) | Bit/dim 3.6870(3.6836) | Xent 0.1158(0.1184) | Loss 3.7449(3.7428) | Error 0.0406(0.0410) Steps 670(668.67) | Grad Norm 0.9751(1.1425) | Total Time 14.00(14.00)\n",
      "Iter 2856 | Time 59.6162(60.1333) | Bit/dim 3.6895(3.6838) | Xent 0.1166(0.1184) | Loss 3.7478(3.7430) | Error 0.0386(0.0410) Steps 658(668.35) | Grad Norm 0.9511(1.1368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 24.3727, Epoch Time 398.9703(402.4288), Bit/dim 3.7051(best: 3.7039), Xent 2.5719, Loss 4.9910, Error 0.4282(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2857 | Time 58.5566(60.0860) | Bit/dim 3.6811(3.6837) | Xent 0.1148(0.1183) | Loss 3.7385(3.7429) | Error 0.0390(0.0409) Steps 670(668.40) | Grad Norm 0.8346(1.1277) | Total Time 14.00(14.00)\n",
      "Iter 2858 | Time 60.4148(60.0959) | Bit/dim 3.6890(3.6839) | Xent 0.1104(0.1180) | Loss 3.7442(3.7429) | Error 0.0369(0.0408) Steps 664(668.27) | Grad Norm 1.0196(1.1245) | Total Time 14.00(14.00)\n",
      "Iter 2859 | Time 62.9368(60.1811) | Bit/dim 3.6794(3.6837) | Xent 0.1208(0.1181) | Loss 3.7398(3.7428) | Error 0.0440(0.0409) Steps 670(668.32) | Grad Norm 0.8013(1.1148) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 59.8072(60.1699) | Bit/dim 3.6746(3.6835) | Xent 0.1134(0.1180) | Loss 3.7313(3.7425) | Error 0.0379(0.0408) Steps 676(668.55) | Grad Norm 0.9562(1.1100) | Total Time 14.00(14.00)\n",
      "Iter 2861 | Time 57.7238(60.0965) | Bit/dim 3.6855(3.6835) | Xent 0.1225(0.1181) | Loss 3.7468(3.7426) | Error 0.0413(0.0408) Steps 664(668.41) | Grad Norm 1.3366(1.1168) | Total Time 14.00(14.00)\n",
      "Iter 2862 | Time 62.9076(60.1808) | Bit/dim 3.6847(3.6836) | Xent 0.1229(0.1183) | Loss 3.7461(3.7427) | Error 0.0423(0.0409) Steps 664(668.28) | Grad Norm 1.4364(1.1264) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 24.3332, Epoch Time 402.5458(402.4323), Bit/dim 3.7045(best: 3.7039), Xent 2.5960, Loss 5.0025, Error 0.4306(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2863 | Time 59.3688(60.1565) | Bit/dim 3.6913(3.6838) | Xent 0.1184(0.1183) | Loss 3.7505(3.7429) | Error 0.0416(0.0409) Steps 664(668.15) | Grad Norm 1.1614(1.1275) | Total Time 14.00(14.00)\n",
      "Iter 2864 | Time 60.1961(60.1577) | Bit/dim 3.6876(3.6839) | Xent 0.1181(0.1183) | Loss 3.7466(3.7430) | Error 0.0394(0.0408) Steps 664(668.03) | Grad Norm 0.8833(1.1201) | Total Time 14.00(14.00)\n",
      "Iter 2865 | Time 62.5706(60.2301) | Bit/dim 3.6825(3.6839) | Xent 0.1212(0.1183) | Loss 3.7431(3.7430) | Error 0.0400(0.0408) Steps 664(667.91) | Grad Norm 1.2821(1.1250) | Total Time 14.00(14.00)\n",
      "Iter 2866 | Time 59.5737(60.2104) | Bit/dim 3.6784(3.6837) | Xent 0.1116(0.1181) | Loss 3.7342(3.7428) | Error 0.0383(0.0407) Steps 670(667.97) | Grad Norm 1.1063(1.1244) | Total Time 14.00(14.00)\n",
      "Iter 2867 | Time 60.0260(60.2048) | Bit/dim 3.6912(3.6839) | Xent 0.1220(0.1183) | Loss 3.7522(3.7431) | Error 0.0421(0.0408) Steps 664(667.85) | Grad Norm 1.1994(1.1267) | Total Time 14.00(14.00)\n",
      "Iter 2868 | Time 59.1347(60.1727) | Bit/dim 3.6690(3.6835) | Xent 0.1174(0.1182) | Loss 3.7277(3.7426) | Error 0.0389(0.0407) Steps 670(667.92) | Grad Norm 0.9729(1.1221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 24.3495, Epoch Time 401.2885(402.3980), Bit/dim 3.7058(best: 3.7039), Xent 2.5811, Loss 4.9963, Error 0.4269(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2869 | Time 58.8813(60.1340) | Bit/dim 3.6876(3.6836) | Xent 0.1205(0.1183) | Loss 3.7478(3.7428) | Error 0.0419(0.0408) Steps 676(668.16) | Grad Norm 1.1380(1.1225) | Total Time 14.00(14.00)\n",
      "Iter 2870 | Time 58.8612(60.0958) | Bit/dim 3.6753(3.6834) | Xent 0.1207(0.1184) | Loss 3.7357(3.7425) | Error 0.0430(0.0408) Steps 664(668.03) | Grad Norm 1.3402(1.1291) | Total Time 14.00(14.00)\n",
      "Iter 2871 | Time 61.8967(60.1498) | Bit/dim 3.6881(3.6835) | Xent 0.1112(0.1182) | Loss 3.7437(3.7426) | Error 0.0377(0.0407) Steps 676(668.27) | Grad Norm 1.0183(1.1257) | Total Time 14.00(14.00)\n",
      "Iter 2872 | Time 59.3472(60.1258) | Bit/dim 3.6838(3.6835) | Xent 0.1125(0.1180) | Loss 3.7401(3.7425) | Error 0.0400(0.0407) Steps 670(668.32) | Grad Norm 0.9954(1.1218) | Total Time 14.00(14.00)\n",
      "Iter 2873 | Time 61.6877(60.1726) | Bit/dim 3.6828(3.6835) | Xent 0.1147(0.1179) | Loss 3.7401(3.7424) | Error 0.0381(0.0406) Steps 670(668.37) | Grad Norm 1.1495(1.1227) | Total Time 14.00(14.00)\n",
      "Iter 2874 | Time 59.7713(60.1606) | Bit/dim 3.6866(3.6836) | Xent 0.1224(0.1180) | Loss 3.7478(3.7426) | Error 0.0420(0.0407) Steps 670(668.42) | Grad Norm 1.2111(1.1253) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 24.5694, Epoch Time 400.8153(402.3505), Bit/dim 3.7061(best: 3.7039), Xent 2.6058, Loss 5.0090, Error 0.4320(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2875 | Time 59.7639(60.1487) | Bit/dim 3.6820(3.6835) | Xent 0.1077(0.1177) | Loss 3.7358(3.7424) | Error 0.0363(0.0405) Steps 664(668.29) | Grad Norm 1.0294(1.1224) | Total Time 14.00(14.00)\n",
      "Iter 2876 | Time 58.3489(60.0947) | Bit/dim 3.6925(3.6838) | Xent 0.1099(0.1175) | Loss 3.7475(3.7425) | Error 0.0354(0.0404) Steps 688(668.88) | Grad Norm 1.1167(1.1223) | Total Time 14.00(14.00)\n",
      "Iter 2877 | Time 60.2475(60.0993) | Bit/dim 3.6864(3.6839) | Xent 0.1125(0.1173) | Loss 3.7427(3.7425) | Error 0.0391(0.0403) Steps 664(668.73) | Grad Norm 1.0093(1.1189) | Total Time 14.00(14.00)\n",
      "Iter 2878 | Time 60.5138(60.1117) | Bit/dim 3.6817(3.6838) | Xent 0.1181(0.1173) | Loss 3.7408(3.7425) | Error 0.0409(0.0404) Steps 676(668.95) | Grad Norm 1.1122(1.1187) | Total Time 14.00(14.00)\n",
      "Iter 2879 | Time 61.6506(60.1579) | Bit/dim 3.6802(3.6837) | Xent 0.1189(0.1174) | Loss 3.7396(3.7424) | Error 0.0389(0.0403) Steps 664(668.80) | Grad Norm 1.2344(1.1222) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 61.2789(60.1915) | Bit/dim 3.6805(3.6836) | Xent 0.1157(0.1173) | Loss 3.7384(3.7423) | Error 0.0406(0.0403) Steps 670(668.84) | Grad Norm 1.3223(1.1282) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 24.3153, Epoch Time 402.1539(402.3446), Bit/dim 3.7064(best: 3.7039), Xent 2.6019, Loss 5.0074, Error 0.4331(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2881 | Time 60.8691(60.2118) | Bit/dim 3.6833(3.6836) | Xent 0.1144(0.1173) | Loss 3.7405(3.7422) | Error 0.0390(0.0403) Steps 670(668.88) | Grad Norm 1.1164(1.1278) | Total Time 14.00(14.00)\n",
      "Iter 2882 | Time 59.1223(60.1791) | Bit/dim 3.6834(3.6836) | Xent 0.1114(0.1171) | Loss 3.7391(3.7421) | Error 0.0369(0.0402) Steps 658(668.55) | Grad Norm 0.8580(1.1197) | Total Time 14.00(14.00)\n",
      "Iter 2883 | Time 59.9443(60.1721) | Bit/dim 3.6801(3.6835) | Xent 0.1079(0.1168) | Loss 3.7341(3.7419) | Error 0.0359(0.0401) Steps 658(668.23) | Grad Norm 0.8747(1.1124) | Total Time 14.00(14.00)\n",
      "Iter 2884 | Time 58.4047(60.1191) | Bit/dim 3.6904(3.6837) | Xent 0.1231(0.1170) | Loss 3.7519(3.7422) | Error 0.0414(0.0401) Steps 670(668.29) | Grad Norm 1.3996(1.1210) | Total Time 14.00(14.00)\n",
      "Iter 2885 | Time 60.7076(60.1367) | Bit/dim 3.6751(3.6834) | Xent 0.1221(0.1171) | Loss 3.7362(3.7420) | Error 0.0417(0.0401) Steps 652(667.80) | Grad Norm 1.3042(1.1265) | Total Time 14.00(14.00)\n",
      "Iter 2886 | Time 59.6142(60.1211) | Bit/dim 3.6887(3.6836) | Xent 0.1154(0.1171) | Loss 3.7464(3.7421) | Error 0.0411(0.0402) Steps 670(667.86) | Grad Norm 0.9108(1.1200) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 24.4753, Epoch Time 399.0663(402.2462), Bit/dim 3.7055(best: 3.7039), Xent 2.6048, Loss 5.0079, Error 0.4331(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2887 | Time 59.8101(60.1117) | Bit/dim 3.6910(3.6838) | Xent 0.1185(0.1171) | Loss 3.7503(3.7424) | Error 0.0413(0.0402) Steps 664(667.75) | Grad Norm 1.6794(1.1368) | Total Time 14.00(14.00)\n",
      "Iter 2888 | Time 60.9781(60.1377) | Bit/dim 3.6852(3.6839) | Xent 0.1076(0.1168) | Loss 3.7391(3.7423) | Error 0.0345(0.0400) Steps 664(667.63) | Grad Norm 0.9845(1.1322) | Total Time 14.00(14.00)\n",
      "Iter 2889 | Time 60.7766(60.1569) | Bit/dim 3.6914(3.6841) | Xent 0.1187(0.1169) | Loss 3.7507(3.7425) | Error 0.0409(0.0401) Steps 670(667.71) | Grad Norm 0.8843(1.1248) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 60.5218(60.1678) | Bit/dim 3.6759(3.6838) | Xent 0.1079(0.1166) | Loss 3.7298(3.7422) | Error 0.0394(0.0400) Steps 664(667.59) | Grad Norm 0.8522(1.1166) | Total Time 14.00(14.00)\n",
      "Iter 2891 | Time 59.7672(60.1558) | Bit/dim 3.6832(3.6838) | Xent 0.1165(0.1166) | Loss 3.7414(3.7421) | Error 0.0409(0.0401) Steps 658(667.31) | Grad Norm 1.3700(1.1242) | Total Time 14.00(14.00)\n",
      "Iter 2892 | Time 58.9643(60.1201) | Bit/dim 3.6743(3.6835) | Xent 0.1195(0.1167) | Loss 3.7340(3.7419) | Error 0.0431(0.0402) Steps 652(666.85) | Grad Norm 1.0089(1.1207) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 24.2953, Epoch Time 400.8895(402.2055), Bit/dim 3.7047(best: 3.7039), Xent 2.6063, Loss 5.0079, Error 0.4285(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2893 | Time 58.7514(60.0790) | Bit/dim 3.6717(3.6832) | Xent 0.1159(0.1167) | Loss 3.7297(3.7415) | Error 0.0419(0.0402) Steps 658(666.58) | Grad Norm 1.2894(1.1258) | Total Time 14.00(14.00)\n",
      "Iter 2894 | Time 60.4198(60.0892) | Bit/dim 3.6913(3.6834) | Xent 0.1109(0.1165) | Loss 3.7468(3.7417) | Error 0.0363(0.0401) Steps 664(666.50) | Grad Norm 1.3082(1.1313) | Total Time 14.00(14.00)\n",
      "Iter 2895 | Time 62.9445(60.1749) | Bit/dim 3.6912(3.6837) | Xent 0.1177(0.1166) | Loss 3.7500(3.7419) | Error 0.0419(0.0401) Steps 676(666.79) | Grad Norm 1.5480(1.1438) | Total Time 14.00(14.00)\n",
      "Iter 2896 | Time 57.9395(60.1078) | Bit/dim 3.6832(3.6836) | Xent 0.1152(0.1165) | Loss 3.7408(3.7419) | Error 0.0383(0.0401) Steps 670(666.89) | Grad Norm 1.0986(1.1424) | Total Time 14.00(14.00)\n",
      "Iter 2897 | Time 61.0046(60.1347) | Bit/dim 3.6877(3.6838) | Xent 0.1088(0.1163) | Loss 3.7421(3.7419) | Error 0.0383(0.0400) Steps 664(666.80) | Grad Norm 1.1293(1.1420) | Total Time 14.00(14.00)\n",
      "Iter 2898 | Time 61.0472(60.1621) | Bit/dim 3.6783(3.6836) | Xent 0.1147(0.1162) | Loss 3.7357(3.7417) | Error 0.0391(0.0400) Steps 658(666.54) | Grad Norm 1.3337(1.1478) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 24.5391, Epoch Time 402.3944(402.2112), Bit/dim 3.7054(best: 3.7039), Xent 2.6437, Loss 5.0272, Error 0.4327(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2899 | Time 58.7631(60.1201) | Bit/dim 3.6836(3.6836) | Xent 0.1101(0.1160) | Loss 3.7387(3.7416) | Error 0.0394(0.0400) Steps 664(666.46) | Grad Norm 1.1280(1.1472) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 59.1793(60.0919) | Bit/dim 3.6808(3.6835) | Xent 0.1205(0.1162) | Loss 3.7411(3.7416) | Error 0.0429(0.0401) Steps 670(666.57) | Grad Norm 1.3060(1.1520) | Total Time 14.00(14.00)\n",
      "Iter 2901 | Time 60.4249(60.1019) | Bit/dim 3.6763(3.6833) | Xent 0.1227(0.1164) | Loss 3.7377(3.7415) | Error 0.0420(0.0401) Steps 682(667.03) | Grad Norm 1.1002(1.1504) | Total Time 14.00(14.00)\n",
      "Iter 2902 | Time 59.5965(60.0867) | Bit/dim 3.6846(3.6833) | Xent 0.1199(0.1165) | Loss 3.7445(3.7416) | Error 0.0423(0.0402) Steps 670(667.12) | Grad Norm 1.5264(1.1617) | Total Time 14.00(14.00)\n",
      "Iter 2903 | Time 61.1629(60.1190) | Bit/dim 3.6880(3.6835) | Xent 0.1151(0.1164) | Loss 3.7456(3.7417) | Error 0.0410(0.0402) Steps 664(667.02) | Grad Norm 1.6239(1.1755) | Total Time 14.00(14.00)\n",
      "Iter 2904 | Time 58.9101(60.0828) | Bit/dim 3.6795(3.6834) | Xent 0.1159(0.1164) | Loss 3.7375(3.7416) | Error 0.0409(0.0402) Steps 658(666.75) | Grad Norm 1.2078(1.1765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 24.5900, Epoch Time 398.5645(402.1018), Bit/dim 3.7040(best: 3.7039), Xent 2.6056, Loss 5.0067, Error 0.4348(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2905 | Time 59.6523(60.0698) | Bit/dim 3.6803(3.6833) | Xent 0.1210(0.1166) | Loss 3.7408(3.7416) | Error 0.0425(0.0403) Steps 664(666.67) | Grad Norm 1.2144(1.1776) | Total Time 14.00(14.00)\n",
      "Iter 2906 | Time 61.0201(60.0984) | Bit/dim 3.6838(3.6833) | Xent 0.1141(0.1165) | Loss 3.7408(3.7415) | Error 0.0389(0.0403) Steps 676(666.95) | Grad Norm 0.9364(1.1704) | Total Time 14.00(14.00)\n",
      "Iter 2907 | Time 62.6660(60.1754) | Bit/dim 3.6853(3.6833) | Xent 0.1160(0.1165) | Loss 3.7432(3.7416) | Error 0.0377(0.0402) Steps 658(666.68) | Grad Norm 1.7967(1.1892) | Total Time 14.00(14.00)\n",
      "Iter 2908 | Time 60.0291(60.1710) | Bit/dim 3.6798(3.6832) | Xent 0.1233(0.1167) | Loss 3.7415(3.7416) | Error 0.0419(0.0402) Steps 676(666.96) | Grad Norm 1.4928(1.1983) | Total Time 14.00(14.00)\n",
      "Iter 2909 | Time 60.5170(60.1814) | Bit/dim 3.6881(3.6834) | Xent 0.1078(0.1164) | Loss 3.7420(3.7416) | Error 0.0370(0.0401) Steps 664(666.87) | Grad Norm 1.0542(1.1940) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 59.1036(60.1490) | Bit/dim 3.6765(3.6832) | Xent 0.1214(0.1166) | Loss 3.7372(3.7415) | Error 0.0436(0.0402) Steps 664(666.79) | Grad Norm 0.9781(1.1875) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 24.4453, Epoch Time 403.3675(402.1398), Bit/dim 3.7042(best: 3.7039), Xent 2.6170, Loss 5.0127, Error 0.4306(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2911 | Time 60.3338(60.1546) | Bit/dim 3.6853(3.6832) | Xent 0.1178(0.1166) | Loss 3.7442(3.7415) | Error 0.0416(0.0403) Steps 664(666.70) | Grad Norm 1.4444(1.1952) | Total Time 14.00(14.00)\n",
      "Iter 2912 | Time 62.1982(60.2159) | Bit/dim 3.6806(3.6832) | Xent 0.1078(0.1163) | Loss 3.7345(3.7413) | Error 0.0379(0.0402) Steps 670(666.80) | Grad Norm 0.8649(1.1853) | Total Time 14.00(14.00)\n",
      "Iter 2913 | Time 60.9520(60.2380) | Bit/dim 3.6786(3.6830) | Xent 0.1142(0.1163) | Loss 3.7357(3.7412) | Error 0.0399(0.0402) Steps 664(666.72) | Grad Norm 1.3923(1.1915) | Total Time 14.00(14.00)\n",
      "Iter 2914 | Time 59.3209(60.2105) | Bit/dim 3.6750(3.6828) | Xent 0.1142(0.1162) | Loss 3.7321(3.7409) | Error 0.0416(0.0402) Steps 676(667.00) | Grad Norm 1.3834(1.1973) | Total Time 14.00(14.00)\n",
      "Iter 2915 | Time 59.7518(60.1967) | Bit/dim 3.6827(3.6828) | Xent 0.1175(0.1162) | Loss 3.7415(3.7409) | Error 0.0396(0.0402) Steps 682(667.45) | Grad Norm 1.0554(1.1930) | Total Time 14.00(14.00)\n",
      "Iter 2916 | Time 62.0243(60.2515) | Bit/dim 3.6937(3.6831) | Xent 0.1223(0.1164) | Loss 3.7548(3.7413) | Error 0.0431(0.0403) Steps 676(667.70) | Grad Norm 1.0288(1.1881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 24.5481, Epoch Time 404.9646(402.2245), Bit/dim 3.7049(best: 3.7039), Xent 2.6178, Loss 5.0138, Error 0.4327(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2917 | Time 60.8259(60.2688) | Bit/dim 3.6795(3.6830) | Xent 0.1124(0.1163) | Loss 3.7357(3.7412) | Error 0.0395(0.0403) Steps 670(667.77) | Grad Norm 1.3952(1.1943) | Total Time 14.00(14.00)\n",
      "Iter 2918 | Time 61.9534(60.3193) | Bit/dim 3.6889(3.6832) | Xent 0.1092(0.1161) | Loss 3.7435(3.7412) | Error 0.0383(0.0402) Steps 670(667.84) | Grad Norm 0.9856(1.1880) | Total Time 14.00(14.00)\n",
      "Iter 2919 | Time 62.6617(60.3896) | Bit/dim 3.6876(3.6833) | Xent 0.1110(0.1159) | Loss 3.7431(3.7413) | Error 0.0387(0.0402) Steps 664(667.72) | Grad Norm 1.0591(1.1842) | Total Time 14.00(14.00)\n",
      "Iter 2920 | Time 58.7594(60.3407) | Bit/dim 3.6640(3.6827) | Xent 0.1128(0.1158) | Loss 3.7204(3.7407) | Error 0.0371(0.0401) Steps 664(667.61) | Grad Norm 0.8905(1.1754) | Total Time 14.00(14.00)\n",
      "Iter 2921 | Time 63.0768(60.4227) | Bit/dim 3.6936(3.6831) | Xent 0.1143(0.1158) | Loss 3.7507(3.7410) | Error 0.0384(0.0400) Steps 682(668.04) | Grad Norm 1.1815(1.1755) | Total Time 14.00(14.00)\n",
      "Iter 2922 | Time 60.7635(60.4330) | Bit/dim 3.6838(3.6831) | Xent 0.1209(0.1160) | Loss 3.7443(3.7411) | Error 0.0440(0.0402) Steps 664(667.92) | Grad Norm 1.5543(1.1869) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 24.6049, Epoch Time 408.7160(402.4193), Bit/dim 3.7055(best: 3.7039), Xent 2.6058, Loss 5.0084, Error 0.4321(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2923 | Time 60.7150(60.4414) | Bit/dim 3.6912(3.6833) | Xent 0.1125(0.1159) | Loss 3.7474(3.7412) | Error 0.0389(0.0401) Steps 670(667.98) | Grad Norm 0.8368(1.1764) | Total Time 14.00(14.00)\n",
      "Iter 2924 | Time 60.5760(60.4455) | Bit/dim 3.6728(3.6830) | Xent 0.1158(0.1159) | Loss 3.7308(3.7409) | Error 0.0379(0.0401) Steps 652(667.50) | Grad Norm 1.0658(1.1731) | Total Time 14.00(14.00)\n",
      "Iter 2925 | Time 58.1249(60.3759) | Bit/dim 3.6781(3.6829) | Xent 0.1098(0.1157) | Loss 3.7330(3.7407) | Error 0.0366(0.0400) Steps 670(667.58) | Grad Norm 1.1409(1.1721) | Total Time 14.00(14.00)\n",
      "Iter 2926 | Time 60.4325(60.3776) | Bit/dim 3.6857(3.6829) | Xent 0.1187(0.1158) | Loss 3.7451(3.7408) | Error 0.0419(0.0400) Steps 670(667.65) | Grad Norm 1.1452(1.1713) | Total Time 14.00(14.00)\n",
      "Iter 2927 | Time 61.6082(60.4145) | Bit/dim 3.6838(3.6830) | Xent 0.1113(0.1156) | Loss 3.7394(3.7408) | Error 0.0383(0.0400) Steps 664(667.54) | Grad Norm 1.0215(1.1668) | Total Time 14.00(14.00)\n",
      "Iter 2928 | Time 60.4785(60.4164) | Bit/dim 3.6871(3.6831) | Xent 0.1112(0.1155) | Loss 3.7427(3.7408) | Error 0.0389(0.0399) Steps 670(667.62) | Grad Norm 1.5050(1.1770) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 24.5113, Epoch Time 402.2542(402.4143), Bit/dim 3.7053(best: 3.7039), Xent 2.5928, Loss 5.0017, Error 0.4299(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2929 | Time 59.3953(60.3858) | Bit/dim 3.6803(3.6830) | Xent 0.1091(0.1153) | Loss 3.7349(3.7407) | Error 0.0377(0.0399) Steps 670(667.69) | Grad Norm 0.8313(1.1666) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 58.9068(60.3414) | Bit/dim 3.6865(3.6831) | Xent 0.1063(0.1150) | Loss 3.7397(3.7406) | Error 0.0361(0.0397) Steps 676(667.94) | Grad Norm 1.1584(1.1663) | Total Time 14.00(14.00)\n",
      "Iter 2931 | Time 58.0135(60.2716) | Bit/dim 3.6925(3.6834) | Xent 0.1145(0.1150) | Loss 3.7497(3.7409) | Error 0.0393(0.0397) Steps 658(667.64) | Grad Norm 0.9400(1.1596) | Total Time 14.00(14.00)\n",
      "Iter 2932 | Time 60.9226(60.2911) | Bit/dim 3.6798(3.6833) | Xent 0.1147(0.1150) | Loss 3.7371(3.7408) | Error 0.0409(0.0398) Steps 670(667.71) | Grad Norm 1.0037(1.1549) | Total Time 14.00(14.00)\n",
      "Iter 2933 | Time 61.0684(60.3144) | Bit/dim 3.6840(3.6833) | Xent 0.1117(0.1149) | Loss 3.7399(3.7408) | Error 0.0397(0.0398) Steps 676(667.96) | Grad Norm 1.0603(1.1520) | Total Time 14.00(14.00)\n",
      "Iter 2934 | Time 60.4343(60.3180) | Bit/dim 3.6819(3.6833) | Xent 0.1112(0.1148) | Loss 3.7375(3.7407) | Error 0.0385(0.0397) Steps 664(667.84) | Grad Norm 1.4052(1.1596) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 24.8743, Epoch Time 399.5733(402.3291), Bit/dim 3.7047(best: 3.7039), Xent 2.6005, Loss 5.0050, Error 0.4314(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2935 | Time 58.2510(60.2560) | Bit/dim 3.6776(3.6831) | Xent 0.1096(0.1146) | Loss 3.7324(3.7404) | Error 0.0397(0.0397) Steps 676(668.08) | Grad Norm 1.0533(1.1565) | Total Time 14.00(14.00)\n",
      "Iter 2936 | Time 61.4089(60.2906) | Bit/dim 3.6795(3.6830) | Xent 0.1110(0.1145) | Loss 3.7350(3.7403) | Error 0.0389(0.0397) Steps 670(668.14) | Grad Norm 0.7851(1.1453) | Total Time 14.00(14.00)\n",
      "Iter 2937 | Time 60.8641(60.3078) | Bit/dim 3.6853(3.6831) | Xent 0.1050(0.1142) | Loss 3.7378(3.7402) | Error 0.0364(0.0396) Steps 670(668.20) | Grad Norm 1.2528(1.1485) | Total Time 14.00(14.00)\n",
      "Iter 2938 | Time 59.7926(60.2923) | Bit/dim 3.6800(3.6830) | Xent 0.1222(0.1145) | Loss 3.7411(3.7402) | Error 0.0420(0.0397) Steps 676(668.43) | Grad Norm 1.3801(1.1555) | Total Time 14.00(14.00)\n",
      "Iter 2939 | Time 59.2957(60.2624) | Bit/dim 3.6841(3.6830) | Xent 0.1145(0.1145) | Loss 3.7413(3.7402) | Error 0.0389(0.0397) Steps 670(668.48) | Grad Norm 0.8660(1.1468) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 60.1720(60.2597) | Bit/dim 3.6905(3.6832) | Xent 0.1086(0.1143) | Loss 3.7449(3.7404) | Error 0.0369(0.0396) Steps 670(668.52) | Grad Norm 1.2325(1.1494) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0490 | Time 24.0204, Epoch Time 399.7696(402.2523), Bit/dim 3.7047(best: 3.7039), Xent 2.6046, Loss 5.0070, Error 0.4276(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2941 | Time 61.3615(60.2928) | Bit/dim 3.6873(3.6833) | Xent 0.1119(0.1142) | Loss 3.7433(3.7405) | Error 0.0376(0.0395) Steps 670(668.57) | Grad Norm 0.9361(1.1430) | Total Time 14.00(14.00)\n",
      "Iter 2942 | Time 58.8948(60.2508) | Bit/dim 3.6775(3.6832) | Xent 0.1112(0.1141) | Loss 3.7331(3.7402) | Error 0.0386(0.0395) Steps 664(668.43) | Grad Norm 1.1077(1.1419) | Total Time 14.00(14.00)\n",
      "Iter 2943 | Time 61.6206(60.2919) | Bit/dim 3.6829(3.6832) | Xent 0.1109(0.1140) | Loss 3.7383(3.7402) | Error 0.0393(0.0395) Steps 670(668.48) | Grad Norm 0.9211(1.1353) | Total Time 14.00(14.00)\n",
      "Iter 2944 | Time 62.2903(60.3519) | Bit/dim 3.6896(3.6834) | Xent 0.1135(0.1140) | Loss 3.7464(3.7404) | Error 0.0385(0.0394) Steps 670(668.52) | Grad Norm 0.9380(1.1294) | Total Time 14.00(14.00)\n",
      "Iter 2945 | Time 60.9129(60.3687) | Bit/dim 3.6733(3.6831) | Xent 0.1116(0.1140) | Loss 3.7291(3.7400) | Error 0.0397(0.0395) Steps 670(668.57) | Grad Norm 1.3661(1.1365) | Total Time 14.00(14.00)\n",
      "Iter 2946 | Time 59.1968(60.3335) | Bit/dim 3.6782(3.6829) | Xent 0.1156(0.1140) | Loss 3.7360(3.7399) | Error 0.0417(0.0395) Steps 664(668.43) | Grad Norm 1.5336(1.1484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0491 | Time 24.2263, Epoch Time 404.2162(402.3112), Bit/dim 3.7055(best: 3.7039), Xent 2.6269, Loss 5.0189, Error 0.4286(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2947 | Time 60.9301(60.3514) | Bit/dim 3.6819(3.6829) | Xent 0.1118(0.1139) | Loss 3.7378(3.7399) | Error 0.0364(0.0394) Steps 670(668.48) | Grad Norm 1.1808(1.1494) | Total Time 14.00(14.00)\n",
      "Iter 2948 | Time 59.9888(60.3406) | Bit/dim 3.6804(3.6828) | Xent 0.1090(0.1138) | Loss 3.7348(3.7397) | Error 0.0389(0.0394) Steps 664(668.34) | Grad Norm 0.8466(1.1403) | Total Time 14.00(14.00)\n",
      "Iter 2949 | Time 60.7886(60.3540) | Bit/dim 3.6777(3.6827) | Xent 0.1145(0.1138) | Loss 3.7349(3.7396) | Error 0.0403(0.0394) Steps 664(668.21) | Grad Norm 1.2386(1.1432) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 58.5800(60.3008) | Bit/dim 3.6837(3.6827) | Xent 0.1117(0.1138) | Loss 3.7395(3.7396) | Error 0.0421(0.0395) Steps 670(668.27) | Grad Norm 1.0827(1.1414) | Total Time 14.00(14.00)\n",
      "Iter 2951 | Time 61.4692(60.3358) | Bit/dim 3.6801(3.6826) | Xent 0.1185(0.1139) | Loss 3.7393(3.7396) | Error 0.0386(0.0395) Steps 676(668.50) | Grad Norm 1.1190(1.1407) | Total Time 14.00(14.00)\n",
      "Iter 2952 | Time 57.7471(60.2582) | Bit/dim 3.6898(3.6828) | Xent 0.1090(0.1137) | Loss 3.7443(3.7397) | Error 0.0367(0.0394) Steps 664(668.36) | Grad Norm 0.9941(1.1363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 24.3063, Epoch Time 736.5929(412.3397), Bit/dim 3.7049(best: 3.7039), Xent 2.6247, Loss 5.0173, Error 0.4297(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2953 | Time 63.4480(60.3539) | Bit/dim 3.6891(3.6830) | Xent 0.1109(0.1137) | Loss 3.7446(3.7398) | Error 0.0387(0.0394) Steps 670(668.41) | Grad Norm 0.8623(1.1281) | Total Time 14.00(14.00)\n",
      "Iter 2954 | Time 60.3701(60.3544) | Bit/dim 3.6861(3.6831) | Xent 0.1148(0.1137) | Loss 3.7435(3.7399) | Error 0.0386(0.0394) Steps 664(668.28) | Grad Norm 1.2643(1.1322) | Total Time 14.00(14.00)\n",
      "Iter 2955 | Time 58.7726(60.3069) | Bit/dim 3.6834(3.6831) | Xent 0.1018(0.1133) | Loss 3.7343(3.7398) | Error 0.0355(0.0393) Steps 670(668.33) | Grad Norm 0.9103(1.1255) | Total Time 14.00(14.00)\n",
      "Iter 2956 | Time 57.6763(60.2280) | Bit/dim 3.6784(3.6830) | Xent 0.1101(0.1132) | Loss 3.7335(3.7396) | Error 0.0383(0.0392) Steps 658(668.02) | Grad Norm 1.0692(1.1239) | Total Time 14.00(14.00)\n",
      "Iter 2957 | Time 60.0719(60.2233) | Bit/dim 3.6873(3.6831) | Xent 0.1047(0.1130) | Loss 3.7396(3.7396) | Error 0.0360(0.0391) Steps 682(668.44) | Grad Norm 1.1923(1.1259) | Total Time 14.00(14.00)\n",
      "Iter 2958 | Time 60.1607(60.2214) | Bit/dim 3.6831(3.6831) | Xent 0.1073(0.1128) | Loss 3.7368(3.7395) | Error 0.0354(0.0390) Steps 670(668.49) | Grad Norm 0.9149(1.1196) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 24.5258, Epoch Time 400.7859(411.9931), Bit/dim 3.7042(best: 3.7039), Xent 2.6419, Loss 5.0251, Error 0.4290(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2959 | Time 59.2042(60.1909) | Bit/dim 3.6804(3.6830) | Xent 0.1158(0.1129) | Loss 3.7383(3.7395) | Error 0.0374(0.0390) Steps 658(668.17) | Grad Norm 1.1230(1.1197) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 62.0651(60.2471) | Bit/dim 3.6829(3.6830) | Xent 0.1165(0.1130) | Loss 3.7411(3.7395) | Error 0.0397(0.0390) Steps 670(668.23) | Grad Norm 1.2234(1.1228) | Total Time 14.00(14.00)\n",
      "Iter 2961 | Time 60.8930(60.2665) | Bit/dim 3.6779(3.6829) | Xent 0.1070(0.1128) | Loss 3.7314(3.7393) | Error 0.0370(0.0389) Steps 664(668.10) | Grad Norm 1.0795(1.1215) | Total Time 14.00(14.00)\n",
      "Iter 2962 | Time 60.8973(60.2854) | Bit/dim 3.6883(3.6830) | Xent 0.1122(0.1128) | Loss 3.7444(3.7394) | Error 0.0395(0.0389) Steps 658(667.80) | Grad Norm 0.8554(1.1135) | Total Time 14.00(14.00)\n",
      "Iter 2963 | Time 61.3227(60.3166) | Bit/dim 3.6906(3.6833) | Xent 0.1108(0.1127) | Loss 3.7461(3.7396) | Error 0.0391(0.0389) Steps 664(667.69) | Grad Norm 1.1217(1.1138) | Total Time 14.00(14.00)\n",
      "Iter 2964 | Time 60.8045(60.3312) | Bit/dim 3.6746(3.6830) | Xent 0.1169(0.1129) | Loss 3.7331(3.7394) | Error 0.0423(0.0390) Steps 670(667.75) | Grad Norm 1.0177(1.1109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 24.6440, Epoch Time 405.6445(411.8026), Bit/dim 3.7051(best: 3.7039), Xent 2.6443, Loss 5.0272, Error 0.4303(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2965 | Time 58.3776(60.2726) | Bit/dim 3.6744(3.6827) | Xent 0.1094(0.1128) | Loss 3.7291(3.7391) | Error 0.0374(0.0390) Steps 676(668.00) | Grad Norm 1.1508(1.1121) | Total Time 14.00(14.00)\n",
      "Iter 2966 | Time 61.5973(60.3123) | Bit/dim 3.6852(3.6828) | Xent 0.1125(0.1128) | Loss 3.7415(3.7392) | Error 0.0370(0.0389) Steps 658(667.70) | Grad Norm 0.8036(1.1028) | Total Time 14.00(14.00)\n",
      "Iter 2967 | Time 60.4151(60.3154) | Bit/dim 3.6863(3.6829) | Xent 0.1020(0.1124) | Loss 3.7373(3.7391) | Error 0.0356(0.0388) Steps 664(667.59) | Grad Norm 0.8878(1.0964) | Total Time 14.00(14.00)\n",
      "Iter 2968 | Time 58.0079(60.2462) | Bit/dim 3.6818(3.6829) | Xent 0.1157(0.1125) | Loss 3.7396(3.7391) | Error 0.0406(0.0389) Steps 676(667.84) | Grad Norm 1.2093(1.0998) | Total Time 14.00(14.00)\n",
      "Iter 2969 | Time 59.5752(60.2261) | Bit/dim 3.6848(3.6829) | Xent 0.1103(0.1125) | Loss 3.7399(3.7392) | Error 0.0391(0.0389) Steps 664(667.73) | Grad Norm 0.9487(1.0952) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 59.4600(60.2031) | Bit/dim 3.6843(3.6830) | Xent 0.1201(0.1127) | Loss 3.7443(3.7393) | Error 0.0420(0.0390) Steps 652(667.26) | Grad Norm 1.3313(1.1023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 24.4033, Epoch Time 397.5942(411.3763), Bit/dim 3.7039(best: 3.7039), Xent 2.6175, Loss 5.0126, Error 0.4308(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2971 | Time 60.1133(60.2004) | Bit/dim 3.6871(3.6831) | Xent 0.1067(0.1125) | Loss 3.7405(3.7394) | Error 0.0366(0.0389) Steps 676(667.52) | Grad Norm 0.8898(1.0959) | Total Time 14.00(14.00)\n",
      "Iter 2972 | Time 57.9500(60.1329) | Bit/dim 3.6853(3.6832) | Xent 0.1194(0.1127) | Loss 3.7451(3.7395) | Error 0.0415(0.0390) Steps 670(667.59) | Grad Norm 1.0282(1.0939) | Total Time 14.00(14.00)\n",
      "Iter 2973 | Time 61.1161(60.1624) | Bit/dim 3.6741(3.6829) | Xent 0.1121(0.1127) | Loss 3.7301(3.7392) | Error 0.0377(0.0390) Steps 664(667.48) | Grad Norm 0.7472(1.0835) | Total Time 14.00(14.00)\n",
      "Iter 2974 | Time 59.6202(60.1461) | Bit/dim 3.6797(3.6828) | Xent 0.1066(0.1125) | Loss 3.7331(3.7391) | Error 0.0373(0.0389) Steps 670(667.56) | Grad Norm 1.2988(1.0900) | Total Time 14.00(14.00)\n",
      "Iter 2975 | Time 58.0873(60.0843) | Bit/dim 3.6872(3.6829) | Xent 0.1097(0.1124) | Loss 3.7421(3.7392) | Error 0.0360(0.0388) Steps 670(667.63) | Grad Norm 1.4964(1.1022) | Total Time 14.00(14.00)\n",
      "Iter 2976 | Time 60.4028(60.0939) | Bit/dim 3.6841(3.6830) | Xent 0.1101(0.1124) | Loss 3.7391(3.7392) | Error 0.0400(0.0389) Steps 664(667.52) | Grad Norm 1.1015(1.1021) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 24.5654, Epoch Time 397.6454(410.9644), Bit/dim 3.7057(best: 3.7039), Xent 2.6412, Loss 5.0263, Error 0.4276(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2977 | Time 61.2976(60.1300) | Bit/dim 3.6972(3.6834) | Xent 0.1112(0.1123) | Loss 3.7529(3.7396) | Error 0.0375(0.0388) Steps 664(667.42) | Grad Norm 0.9696(1.0982) | Total Time 14.00(14.00)\n",
      "Iter 2978 | Time 57.4399(60.0493) | Bit/dim 3.6773(3.6832) | Xent 0.1141(0.1124) | Loss 3.7343(3.7394) | Error 0.0421(0.0389) Steps 664(667.32) | Grad Norm 1.0492(1.0967) | Total Time 14.00(14.00)\n",
      "Iter 2979 | Time 61.0558(60.0795) | Bit/dim 3.6674(3.6827) | Xent 0.1156(0.1125) | Loss 3.7252(3.7390) | Error 0.0393(0.0389) Steps 670(667.40) | Grad Norm 1.2071(1.1000) | Total Time 14.00(14.00)\n",
      "Iter 2980 | Time 60.0863(60.0797) | Bit/dim 3.6887(3.6829) | Xent 0.1081(0.1123) | Loss 3.7427(3.7391) | Error 0.0393(0.0389) Steps 664(667.29) | Grad Norm 0.9718(1.0962) | Total Time 14.00(14.00)\n",
      "Iter 2981 | Time 62.4321(60.1503) | Bit/dim 3.6839(3.6830) | Xent 0.1175(0.1125) | Loss 3.7427(3.7392) | Error 0.0380(0.0389) Steps 658(667.02) | Grad Norm 0.9169(1.0908) | Total Time 14.00(14.00)\n",
      "Iter 2982 | Time 63.4537(60.2494) | Bit/dim 3.6878(3.6831) | Xent 0.1111(0.1125) | Loss 3.7433(3.7393) | Error 0.0383(0.0389) Steps 670(667.11) | Grad Norm 1.3640(1.0990) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 24.7571, Epoch Time 406.3767(410.8268), Bit/dim 3.7045(best: 3.7039), Xent 2.6417, Loss 5.0253, Error 0.4320(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2983 | Time 59.8332(60.2369) | Bit/dim 3.6864(3.6832) | Xent 0.1019(0.1121) | Loss 3.7373(3.7393) | Error 0.0346(0.0388) Steps 670(667.19) | Grad Norm 1.1345(1.1000) | Total Time 14.00(14.00)\n",
      "Iter 2984 | Time 58.9112(60.1971) | Bit/dim 3.6727(3.6829) | Xent 0.1212(0.1124) | Loss 3.7333(3.7391) | Error 0.0426(0.0389) Steps 658(666.92) | Grad Norm 1.4331(1.1100) | Total Time 14.00(14.00)\n",
      "Iter 2985 | Time 61.1876(60.2268) | Bit/dim 3.6792(3.6828) | Xent 0.1040(0.1122) | Loss 3.7312(3.7389) | Error 0.0360(0.0388) Steps 670(667.01) | Grad Norm 0.9552(1.1054) | Total Time 14.00(14.00)\n",
      "Iter 2986 | Time 59.6104(60.2083) | Bit/dim 3.6877(3.6829) | Xent 0.1085(0.1121) | Loss 3.7420(3.7389) | Error 0.0387(0.0388) Steps 670(667.10) | Grad Norm 1.5636(1.1191) | Total Time 14.00(14.00)\n",
      "Iter 2987 | Time 62.1907(60.2678) | Bit/dim 3.6710(3.6826) | Xent 0.1069(0.1119) | Loss 3.7244(3.7385) | Error 0.0369(0.0387) Steps 664(667.01) | Grad Norm 1.0758(1.1178) | Total Time 14.00(14.00)\n",
      "Iter 2988 | Time 61.6235(60.3085) | Bit/dim 3.6859(3.6827) | Xent 0.1125(0.1119) | Loss 3.7421(3.7386) | Error 0.0373(0.0387) Steps 670(667.10) | Grad Norm 1.0028(1.1144) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 24.4934, Epoch Time 403.6489(410.6114), Bit/dim 3.7050(best: 3.7039), Xent 2.6677, Loss 5.0388, Error 0.4303(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2989 | Time 61.2110(60.3356) | Bit/dim 3.6868(3.6828) | Xent 0.1082(0.1118) | Loss 3.7409(3.7387) | Error 0.0381(0.0387) Steps 664(667.00) | Grad Norm 1.2365(1.1180) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 61.4543(60.3691) | Bit/dim 3.6781(3.6826) | Xent 0.1163(0.1119) | Loss 3.7363(3.7386) | Error 0.0411(0.0387) Steps 670(667.09) | Grad Norm 1.4759(1.1288) | Total Time 14.00(14.00)\n",
      "Iter 2991 | Time 61.7959(60.4119) | Bit/dim 3.6842(3.6827) | Xent 0.1082(0.1118) | Loss 3.7383(3.7386) | Error 0.0353(0.0386) Steps 670(667.18) | Grad Norm 0.8114(1.1193) | Total Time 14.00(14.00)\n",
      "Iter 2992 | Time 60.4262(60.4124) | Bit/dim 3.6924(3.6830) | Xent 0.1081(0.1117) | Loss 3.7464(3.7388) | Error 0.0357(0.0386) Steps 670(667.26) | Grad Norm 0.9472(1.1141) | Total Time 14.00(14.00)\n",
      "Iter 2993 | Time 60.2612(60.4078) | Bit/dim 3.6821(3.6830) | Xent 0.1133(0.1118) | Loss 3.7388(3.7388) | Error 0.0400(0.0386) Steps 676(667.53) | Grad Norm 1.5074(1.1259) | Total Time 14.00(14.00)\n",
      "Iter 2994 | Time 57.9911(60.3353) | Bit/dim 3.6731(3.6827) | Xent 0.1098(0.1117) | Loss 3.7280(3.7385) | Error 0.0394(0.0386) Steps 664(667.42) | Grad Norm 1.2253(1.1289) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 24.6196, Epoch Time 403.2873(410.3917), Bit/dim 3.7049(best: 3.7039), Xent 2.6370, Loss 5.0234, Error 0.4325(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2995 | Time 60.5135(60.3407) | Bit/dim 3.6789(3.6825) | Xent 0.1080(0.1116) | Loss 3.7328(3.7383) | Error 0.0370(0.0386) Steps 664(667.32) | Grad Norm 0.8349(1.1201) | Total Time 14.00(14.00)\n",
      "Iter 2996 | Time 58.8628(60.2963) | Bit/dim 3.6798(3.6825) | Xent 0.1149(0.1117) | Loss 3.7373(3.7383) | Error 0.0415(0.0387) Steps 670(667.40) | Grad Norm 1.0363(1.1175) | Total Time 14.00(14.00)\n",
      "Iter 2997 | Time 58.8465(60.2528) | Bit/dim 3.6776(3.6823) | Xent 0.1072(0.1116) | Loss 3.7312(3.7381) | Error 0.0373(0.0386) Steps 676(667.66) | Grad Norm 1.6646(1.1340) | Total Time 14.00(14.00)\n",
      "Iter 2998 | Time 60.9355(60.2733) | Bit/dim 3.6827(3.6823) | Xent 0.1055(0.1114) | Loss 3.7354(3.7380) | Error 0.0380(0.0386) Steps 670(667.73) | Grad Norm 0.9418(1.1282) | Total Time 14.00(14.00)\n",
      "Iter 2999 | Time 60.5583(60.2819) | Bit/dim 3.6770(3.6822) | Xent 0.1118(0.1114) | Loss 3.7329(3.7379) | Error 0.0380(0.0386) Steps 670(667.80) | Grad Norm 0.9585(1.1231) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 59.5685(60.2605) | Bit/dim 3.6956(3.6826) | Xent 0.1137(0.1115) | Loss 3.7525(3.7383) | Error 0.0400(0.0386) Steps 664(667.68) | Grad Norm 0.9643(1.1183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0500 | Time 24.4419, Epoch Time 399.6908(410.0707), Bit/dim 3.7047(best: 3.7039), Xent 2.6352, Loss 5.0223, Error 0.4323(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3001 | Time 60.2741(60.2609) | Bit/dim 3.6781(3.6824) | Xent 0.1120(0.1115) | Loss 3.7341(3.7382) | Error 0.0399(0.0387) Steps 670(667.75) | Grad Norm 1.0112(1.1151) | Total Time 14.00(14.00)\n",
      "Iter 3002 | Time 59.4954(60.2379) | Bit/dim 3.6808(3.6824) | Xent 0.1065(0.1113) | Loss 3.7340(3.7381) | Error 0.0347(0.0385) Steps 676(668.00) | Grad Norm 0.8708(1.1078) | Total Time 14.00(14.00)\n",
      "Iter 3003 | Time 61.7989(60.2847) | Bit/dim 3.6900(3.6826) | Xent 0.1039(0.1111) | Loss 3.7419(3.7382) | Error 0.0370(0.0385) Steps 682(668.42) | Grad Norm 1.0448(1.1059) | Total Time 14.00(14.00)\n",
      "Iter 3004 | Time 61.2168(60.3127) | Bit/dim 3.6810(3.6826) | Xent 0.1111(0.1111) | Loss 3.7366(3.7381) | Error 0.0375(0.0385) Steps 658(668.11) | Grad Norm 1.0765(1.1050) | Total Time 14.00(14.00)\n",
      "Iter 3005 | Time 59.1915(60.2791) | Bit/dim 3.6747(3.6823) | Xent 0.1156(0.1112) | Loss 3.7325(3.7380) | Error 0.0397(0.0385) Steps 670(668.16) | Grad Norm 1.2283(1.1087) | Total Time 14.00(14.00)\n",
      "Iter 3006 | Time 59.3147(60.2501) | Bit/dim 3.6805(3.6823) | Xent 0.1066(0.1111) | Loss 3.7338(3.7378) | Error 0.0356(0.0384) Steps 670(668.22) | Grad Norm 1.0532(1.1071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0501 | Time 24.4945, Epoch Time 401.8010(409.8226), Bit/dim 3.7049(best: 3.7039), Xent 2.6270, Loss 5.0184, Error 0.4330(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3007 | Time 61.6915(60.2934) | Bit/dim 3.6756(3.6821) | Xent 0.0968(0.1107) | Loss 3.7240(3.7374) | Error 0.0329(0.0383) Steps 658(667.91) | Grad Norm 1.3548(1.1145) | Total Time 14.00(14.00)\n",
      "Iter 3008 | Time 60.8149(60.3090) | Bit/dim 3.6832(3.6821) | Xent 0.1161(0.1108) | Loss 3.7413(3.7375) | Error 0.0384(0.0383) Steps 676(668.15) | Grad Norm 1.2092(1.1173) | Total Time 14.00(14.00)\n",
      "Iter 3009 | Time 61.5449(60.3461) | Bit/dim 3.6851(3.6822) | Xent 0.1140(0.1109) | Loss 3.7421(3.7377) | Error 0.0405(0.0383) Steps 670(668.21) | Grad Norm 1.2680(1.1218) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 61.7474(60.3881) | Bit/dim 3.6923(3.6825) | Xent 0.1125(0.1110) | Loss 3.7486(3.7380) | Error 0.0385(0.0383) Steps 670(668.26) | Grad Norm 1.2956(1.1271) | Total Time 14.00(14.00)\n",
      "Iter 3011 | Time 59.7996(60.3705) | Bit/dim 3.6782(3.6824) | Xent 0.1085(0.1109) | Loss 3.7324(3.7378) | Error 0.0371(0.0383) Steps 676(668.50) | Grad Norm 1.0694(1.1253) | Total Time 14.00(14.00)\n",
      "Iter 3012 | Time 60.1845(60.3649) | Bit/dim 3.6758(3.6822) | Xent 0.1095(0.1109) | Loss 3.7305(3.7376) | Error 0.0405(0.0384) Steps 664(668.36) | Grad Norm 1.0707(1.1237) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0502 | Time 24.5197, Epoch Time 406.1828(409.7134), Bit/dim 3.7056(best: 3.7039), Xent 2.6566, Loss 5.0338, Error 0.4322(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3013 | Time 61.3424(60.3942) | Bit/dim 3.6824(3.6822) | Xent 0.1154(0.1110) | Loss 3.7401(3.7377) | Error 0.0389(0.0384) Steps 658(668.05) | Grad Norm 1.2638(1.1279) | Total Time 14.00(14.00)\n",
      "Iter 3014 | Time 60.1492(60.3869) | Bit/dim 3.6796(3.6821) | Xent 0.1059(0.1108) | Loss 3.7325(3.7375) | Error 0.0375(0.0383) Steps 670(668.11) | Grad Norm 1.0125(1.1244) | Total Time 14.00(14.00)\n",
      "Iter 3015 | Time 61.5158(60.4207) | Bit/dim 3.6782(3.6820) | Xent 0.1172(0.1110) | Loss 3.7368(3.7375) | Error 0.0413(0.0384) Steps 676(668.34) | Grad Norm 1.6537(1.1403) | Total Time 14.00(14.00)\n",
      "Iter 3016 | Time 60.1189(60.4117) | Bit/dim 3.6783(3.6819) | Xent 0.1069(0.1109) | Loss 3.7317(3.7373) | Error 0.0350(0.0383) Steps 676(668.57) | Grad Norm 0.9719(1.1353) | Total Time 14.00(14.00)\n",
      "Iter 3017 | Time 58.2573(60.3471) | Bit/dim 3.6825(3.6819) | Xent 0.1042(0.1107) | Loss 3.7346(3.7373) | Error 0.0367(0.0383) Steps 670(668.62) | Grad Norm 1.1188(1.1348) | Total Time 14.00(14.00)\n",
      "Iter 3018 | Time 59.4727(60.3208) | Bit/dim 3.6848(3.6820) | Xent 0.1078(0.1106) | Loss 3.7388(3.7373) | Error 0.0379(0.0383) Steps 676(668.84) | Grad Norm 1.5207(1.1463) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0503 | Time 24.2609, Epoch Time 401.0180(409.4525), Bit/dim 3.7044(best: 3.7039), Xent 2.6636, Loss 5.0362, Error 0.4307(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3019 | Time 58.2214(60.2578) | Bit/dim 3.6727(3.6817) | Xent 0.0959(0.1102) | Loss 3.7207(3.7368) | Error 0.0337(0.0381) Steps 682(669.23) | Grad Norm 1.2004(1.1480) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 59.8153(60.2446) | Bit/dim 3.6851(3.6818) | Xent 0.1003(0.1099) | Loss 3.7353(3.7368) | Error 0.0308(0.0379) Steps 664(669.08) | Grad Norm 0.8597(1.1393) | Total Time 14.00(14.00)\n",
      "Iter 3021 | Time 61.4018(60.2793) | Bit/dim 3.6832(3.6819) | Xent 0.1106(0.1099) | Loss 3.7385(3.7368) | Error 0.0387(0.0379) Steps 676(669.28) | Grad Norm 1.0443(1.1365) | Total Time 14.00(14.00)\n",
      "Iter 3022 | Time 57.4793(60.1953) | Bit/dim 3.6737(3.6816) | Xent 0.1120(0.1100) | Loss 3.7297(3.7366) | Error 0.0384(0.0380) Steps 676(669.49) | Grad Norm 1.7456(1.1547) | Total Time 14.00(14.00)\n",
      "Iter 3023 | Time 58.0297(60.1303) | Bit/dim 3.6804(3.6816) | Xent 0.1090(0.1099) | Loss 3.7349(3.7365) | Error 0.0363(0.0379) Steps 670(669.50) | Grad Norm 1.3366(1.1602) | Total Time 14.00(14.00)\n",
      "Iter 3024 | Time 57.0302(60.0373) | Bit/dim 3.6951(3.6820) | Xent 0.1035(0.1097) | Loss 3.7468(3.7368) | Error 0.0349(0.0378) Steps 676(669.70) | Grad Norm 1.1151(1.1588) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0504 | Time 24.4080, Epoch Time 392.1677(408.9340), Bit/dim 3.7050(best: 3.7039), Xent 2.6788, Loss 5.0444, Error 0.4280(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3025 | Time 58.9062(60.0034) | Bit/dim 3.6867(3.6821) | Xent 0.1079(0.1097) | Loss 3.7406(3.7370) | Error 0.0346(0.0377) Steps 676(669.89) | Grad Norm 1.2283(1.1609) | Total Time 14.00(14.00)\n",
      "Iter 3026 | Time 60.8312(60.0282) | Bit/dim 3.6735(3.6819) | Xent 0.1118(0.1098) | Loss 3.7294(3.7367) | Error 0.0384(0.0377) Steps 658(669.53) | Grad Norm 1.4190(1.1687) | Total Time 14.00(14.00)\n",
      "Iter 3027 | Time 60.3685(60.0384) | Bit/dim 3.6765(3.6817) | Xent 0.1038(0.1096) | Loss 3.7284(3.7365) | Error 0.0350(0.0377) Steps 670(669.54) | Grad Norm 1.3302(1.1735) | Total Time 14.00(14.00)\n",
      "Iter 3028 | Time 59.8421(60.0325) | Bit/dim 3.6864(3.6818) | Xent 0.1121(0.1097) | Loss 3.7425(3.7367) | Error 0.0385(0.0377) Steps 664(669.38) | Grad Norm 1.1016(1.1714) | Total Time 14.00(14.00)\n",
      "Iter 3029 | Time 60.3289(60.0414) | Bit/dim 3.6845(3.6819) | Xent 0.1020(0.1094) | Loss 3.7355(3.7366) | Error 0.0353(0.0376) Steps 670(669.40) | Grad Norm 1.5230(1.1819) | Total Time 14.00(14.00)\n",
      "Iter 3030 | Time 59.7578(60.0329) | Bit/dim 3.6800(3.6819) | Xent 0.1021(0.1092) | Loss 3.7311(3.7365) | Error 0.0357(0.0376) Steps 670(669.41) | Grad Norm 1.3422(1.1867) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0505 | Time 24.2005, Epoch Time 400.0172(408.6665), Bit/dim 3.7040(best: 3.7039), Xent 2.6781, Loss 5.0431, Error 0.4303(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3031 | Time 61.8801(60.0883) | Bit/dim 3.6925(3.6822) | Xent 0.1166(0.1094) | Loss 3.7508(3.7369) | Error 0.0409(0.0376) Steps 670(669.43) | Grad Norm 1.1497(1.1856) | Total Time 14.00(14.00)\n",
      "Iter 3032 | Time 61.3987(60.1276) | Bit/dim 3.6806(3.6821) | Xent 0.1077(0.1094) | Loss 3.7345(3.7368) | Error 0.0363(0.0376) Steps 664(669.27) | Grad Norm 1.1384(1.1842) | Total Time 14.00(14.00)\n",
      "Iter 3033 | Time 59.2764(60.1021) | Bit/dim 3.6778(3.6820) | Xent 0.1090(0.1094) | Loss 3.7323(3.7367) | Error 0.0381(0.0376) Steps 676(669.47) | Grad Norm 1.5827(1.1961) | Total Time 14.00(14.00)\n",
      "Iter 3034 | Time 58.9588(60.0678) | Bit/dim 3.6790(3.6819) | Xent 0.1056(0.1093) | Loss 3.7319(3.7365) | Error 0.0374(0.0376) Steps 670(669.49) | Grad Norm 0.8786(1.1866) | Total Time 14.00(14.00)\n",
      "Iter 3035 | Time 63.4841(60.1703) | Bit/dim 3.6803(3.6819) | Xent 0.1132(0.1094) | Loss 3.7370(3.7366) | Error 0.0394(0.0377) Steps 670(669.50) | Grad Norm 1.1647(1.1860) | Total Time 14.00(14.00)\n",
      "Iter 3036 | Time 58.7143(60.1266) | Bit/dim 3.6795(3.6818) | Xent 0.1058(0.1093) | Loss 3.7324(3.7364) | Error 0.0384(0.0377) Steps 676(669.70) | Grad Norm 1.0970(1.1833) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0506 | Time 24.3382, Epoch Time 403.8497(408.5220), Bit/dim 3.7039(best: 3.7039), Xent 2.6674, Loss 5.0376, Error 0.4310(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3037 | Time 57.3845(60.0443) | Bit/dim 3.6858(3.6819) | Xent 0.1105(0.1093) | Loss 3.7411(3.7366) | Error 0.0384(0.0377) Steps 664(669.53) | Grad Norm 1.4092(1.1901) | Total Time 14.00(14.00)\n",
      "Iter 3038 | Time 59.0527(60.0146) | Bit/dim 3.6830(3.6819) | Xent 0.1135(0.1094) | Loss 3.7397(3.7367) | Error 0.0346(0.0376) Steps 664(669.36) | Grad Norm 1.1009(1.1874) | Total Time 14.00(14.00)\n",
      "Iter 3039 | Time 62.2726(60.0823) | Bit/dim 3.6832(3.6820) | Xent 0.1077(0.1094) | Loss 3.7371(3.7367) | Error 0.0365(0.0376) Steps 670(669.38) | Grad Norm 1.2568(1.1895) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 57.7640(60.0128) | Bit/dim 3.6766(3.6818) | Xent 0.1074(0.1093) | Loss 3.7303(3.7365) | Error 0.0366(0.0376) Steps 670(669.40) | Grad Norm 1.1640(1.1887) | Total Time 14.00(14.00)\n",
      "Iter 3041 | Time 58.7901(59.9761) | Bit/dim 3.6687(3.6814) | Xent 0.1089(0.1093) | Loss 3.7232(3.7361) | Error 0.0390(0.0376) Steps 670(669.42) | Grad Norm 1.1138(1.1865) | Total Time 14.00(14.00)\n",
      "Iter 3042 | Time 60.2678(59.9849) | Bit/dim 3.6896(3.6817) | Xent 0.1120(0.1094) | Loss 3.7456(3.7364) | Error 0.0387(0.0376) Steps 670(669.43) | Grad Norm 1.0443(1.1822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0507 | Time 24.3300, Epoch Time 395.5537(408.1329), Bit/dim 3.7044(best: 3.7039), Xent 2.6692, Loss 5.0390, Error 0.4317(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3043 | Time 61.6679(60.0354) | Bit/dim 3.6792(3.6816) | Xent 0.1060(0.1093) | Loss 3.7322(3.7362) | Error 0.0359(0.0376) Steps 670(669.45) | Grad Norm 1.0373(1.1779) | Total Time 14.00(14.00)\n",
      "Iter 3044 | Time 60.8577(60.0600) | Bit/dim 3.6837(3.6817) | Xent 0.1142(0.1094) | Loss 3.7408(3.7364) | Error 0.0405(0.0377) Steps 658(669.11) | Grad Norm 1.3826(1.1840) | Total Time 14.00(14.00)\n",
      "Iter 3045 | Time 60.8144(60.0827) | Bit/dim 3.6824(3.6817) | Xent 0.1104(0.1095) | Loss 3.7376(3.7364) | Error 0.0390(0.0377) Steps 658(668.77) | Grad Norm 1.2915(1.1872) | Total Time 14.00(14.00)\n",
      "Iter 3046 | Time 60.9759(60.1095) | Bit/dim 3.6864(3.6818) | Xent 0.0982(0.1091) | Loss 3.7355(3.7364) | Error 0.0327(0.0376) Steps 664(668.63) | Grad Norm 1.2619(1.1895) | Total Time 14.00(14.00)\n",
      "Iter 3047 | Time 62.2060(60.1723) | Bit/dim 3.6752(3.6816) | Xent 0.1110(0.1092) | Loss 3.7307(3.7362) | Error 0.0390(0.0376) Steps 676(668.85) | Grad Norm 0.9668(1.1828) | Total Time 14.00(14.00)\n",
      "Iter 3048 | Time 61.3231(60.2069) | Bit/dim 3.6870(3.6818) | Xent 0.1064(0.1091) | Loss 3.7402(3.7363) | Error 0.0366(0.0376) Steps 676(669.07) | Grad Norm 0.9887(1.1770) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0508 | Time 24.3640, Epoch Time 408.1894(408.1346), Bit/dim 3.7037(best: 3.7039), Xent 2.6486, Loss 5.0280, Error 0.4298(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3049 | Time 61.7082(60.2519) | Bit/dim 3.6930(3.6821) | Xent 0.1084(0.1091) | Loss 3.7472(3.7367) | Error 0.0377(0.0376) Steps 664(668.91) | Grad Norm 1.1033(1.1748) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 60.9742(60.2736) | Bit/dim 3.6808(3.6821) | Xent 0.1122(0.1092) | Loss 3.7369(3.7367) | Error 0.0383(0.0376) Steps 658(668.59) | Grad Norm 1.2303(1.1764) | Total Time 14.00(14.00)\n",
      "Iter 3051 | Time 57.8973(60.2023) | Bit/dim 3.6874(3.6822) | Xent 0.0995(0.1089) | Loss 3.7372(3.7367) | Error 0.0340(0.0375) Steps 664(668.45) | Grad Norm 0.9033(1.1682) | Total Time 14.00(14.00)\n",
      "Iter 3052 | Time 60.3164(60.2057) | Bit/dim 3.6780(3.6821) | Xent 0.1103(0.1089) | Loss 3.7331(3.7366) | Error 0.0370(0.0375) Steps 670(668.50) | Grad Norm 1.0947(1.1660) | Total Time 14.00(14.00)\n",
      "Iter 3053 | Time 61.4511(60.2431) | Bit/dim 3.6648(3.6816) | Xent 0.1061(0.1088) | Loss 3.7178(3.7360) | Error 0.0366(0.0375) Steps 664(668.36) | Grad Norm 1.0290(1.1619) | Total Time 14.00(14.00)\n",
      "Iter 3054 | Time 60.0532(60.2374) | Bit/dim 3.6874(3.6818) | Xent 0.1041(0.1087) | Loss 3.7395(3.7361) | Error 0.0337(0.0373) Steps 664(668.23) | Grad Norm 0.8715(1.1532) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0509 | Time 24.3404, Epoch Time 402.8376(407.9757), Bit/dim 3.7036(best: 3.7037), Xent 2.6661, Loss 5.0366, Error 0.4316(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3055 | Time 60.5933(60.2481) | Bit/dim 3.6813(3.6818) | Xent 0.1091(0.1087) | Loss 3.7359(3.7361) | Error 0.0394(0.0374) Steps 664(668.10) | Grad Norm 1.0446(1.1499) | Total Time 14.00(14.00)\n",
      "Iter 3056 | Time 60.8445(60.2660) | Bit/dim 3.6852(3.6819) | Xent 0.1065(0.1086) | Loss 3.7385(3.7362) | Error 0.0370(0.0374) Steps 670(668.16) | Grad Norm 0.9932(1.1452) | Total Time 14.00(14.00)\n",
      "Iter 3057 | Time 59.6220(60.2466) | Bit/dim 3.6835(3.6819) | Xent 0.1123(0.1087) | Loss 3.7396(3.7363) | Error 0.0381(0.0374) Steps 670(668.21) | Grad Norm 1.0209(1.1415) | Total Time 14.00(14.00)\n",
      "Iter 3058 | Time 60.3758(60.2505) | Bit/dim 3.6792(3.6818) | Xent 0.1068(0.1087) | Loss 3.7326(3.7362) | Error 0.0356(0.0374) Steps 658(667.91) | Grad Norm 0.7444(1.1296) | Total Time 14.00(14.00)\n",
      "Iter 3059 | Time 59.2751(60.2212) | Bit/dim 3.6715(3.6815) | Xent 0.1085(0.1087) | Loss 3.7258(3.7359) | Error 0.0364(0.0373) Steps 670(667.97) | Grad Norm 1.0648(1.1276) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 59.5056(60.1998) | Bit/dim 3.6834(3.6816) | Xent 0.0935(0.1082) | Loss 3.7301(3.7357) | Error 0.0317(0.0372) Steps 664(667.85) | Grad Norm 1.0219(1.1245) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0510 | Time 24.6878, Epoch Time 400.8088(407.7607), Bit/dim 3.7032(best: 3.7036), Xent 2.6490, Loss 5.0277, Error 0.4298(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3061 | Time 58.8175(60.1583) | Bit/dim 3.6899(3.6818) | Xent 0.1082(0.1082) | Loss 3.7440(3.7359) | Error 0.0370(0.0372) Steps 670(667.92) | Grad Norm 1.1606(1.1256) | Total Time 14.00(14.00)\n",
      "Iter 3062 | Time 58.8599(60.1194) | Bit/dim 3.6815(3.6818) | Xent 0.1071(0.1082) | Loss 3.7350(3.7359) | Error 0.0375(0.0372) Steps 670(667.98) | Grad Norm 1.0596(1.1236) | Total Time 14.00(14.00)\n",
      "Iter 3063 | Time 59.8931(60.1126) | Bit/dim 3.6767(3.6817) | Xent 0.1092(0.1082) | Loss 3.7313(3.7358) | Error 0.0367(0.0372) Steps 670(668.04) | Grad Norm 1.3767(1.1312) | Total Time 14.00(14.00)\n",
      "Iter 3064 | Time 60.6060(60.1274) | Bit/dim 3.6728(3.6814) | Xent 0.1037(0.1081) | Loss 3.7246(3.7354) | Error 0.0351(0.0371) Steps 658(667.74) | Grad Norm 1.0811(1.1297) | Total Time 14.00(14.00)\n",
      "Iter 3065 | Time 59.3866(60.1051) | Bit/dim 3.6810(3.6814) | Xent 0.1073(0.1081) | Loss 3.7346(3.7354) | Error 0.0364(0.0371) Steps 670(667.81) | Grad Norm 0.9571(1.1245) | Total Time 14.00(14.00)\n",
      "Iter 3066 | Time 58.6079(60.0602) | Bit/dim 3.6829(3.6814) | Xent 0.1007(0.1078) | Loss 3.7332(3.7354) | Error 0.0355(0.0370) Steps 652(667.33) | Grad Norm 1.3903(1.1325) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0511 | Time 24.1730, Epoch Time 396.0354(407.4090), Bit/dim 3.7043(best: 3.7032), Xent 2.6652, Loss 5.0369, Error 0.4323(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3067 | Time 60.7372(60.0805) | Bit/dim 3.6811(3.6814) | Xent 0.1087(0.1079) | Loss 3.7354(3.7354) | Error 0.0370(0.0370) Steps 664(667.23) | Grad Norm 1.0198(1.1291) | Total Time 14.00(14.00)\n",
      "Iter 3068 | Time 61.3304(60.1180) | Bit/dim 3.6942(3.6818) | Xent 0.0986(0.1076) | Loss 3.7435(3.7356) | Error 0.0334(0.0369) Steps 664(667.14) | Grad Norm 1.1984(1.1312) | Total Time 14.00(14.00)\n",
      "Iter 3069 | Time 60.0315(60.1154) | Bit/dim 3.6826(3.6818) | Xent 0.1042(0.1075) | Loss 3.7346(3.7356) | Error 0.0366(0.0369) Steps 658(666.86) | Grad Norm 1.0927(1.1300) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 61.5055(60.1571) | Bit/dim 3.6853(3.6819) | Xent 0.1118(0.1076) | Loss 3.7412(3.7357) | Error 0.0385(0.0370) Steps 670(666.96) | Grad Norm 1.2311(1.1330) | Total Time 14.00(14.00)\n",
      "Iter 3071 | Time 58.5062(60.1076) | Bit/dim 3.6678(3.6815) | Xent 0.1064(0.1076) | Loss 3.7210(3.7353) | Error 0.0373(0.0370) Steps 664(666.87) | Grad Norm 1.3570(1.1398) | Total Time 14.00(14.00)\n",
      "Iter 3072 | Time 59.5844(60.0919) | Bit/dim 3.6787(3.6814) | Xent 0.1038(0.1075) | Loss 3.7306(3.7352) | Error 0.0359(0.0369) Steps 676(667.14) | Grad Norm 1.2793(1.1440) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0512 | Time 24.4518, Epoch Time 401.8951(407.2435), Bit/dim 3.7037(best: 3.7032), Xent 2.6883, Loss 5.0478, Error 0.4319(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3073 | Time 58.7837(60.0527) | Bit/dim 3.6869(3.6816) | Xent 0.1078(0.1075) | Loss 3.7408(3.7353) | Error 0.0381(0.0370) Steps 670(667.23) | Grad Norm 0.8649(1.1356) | Total Time 14.00(14.00)\n",
      "Iter 3074 | Time 60.2931(60.0599) | Bit/dim 3.6764(3.6814) | Xent 0.1036(0.1074) | Loss 3.7282(3.7351) | Error 0.0354(0.0369) Steps 670(667.31) | Grad Norm 1.2552(1.1392) | Total Time 14.00(14.00)\n",
      "Iter 3075 | Time 60.2534(60.0657) | Bit/dim 3.6714(3.6811) | Xent 0.1036(0.1072) | Loss 3.7232(3.7348) | Error 0.0340(0.0368) Steps 676(667.57) | Grad Norm 0.8659(1.1310) | Total Time 14.00(14.00)\n",
      "Iter 3076 | Time 59.5061(60.0489) | Bit/dim 3.6842(3.6812) | Xent 0.1016(0.1071) | Loss 3.7350(3.7348) | Error 0.0345(0.0368) Steps 664(667.46) | Grad Norm 0.9625(1.1259) | Total Time 14.00(14.00)\n",
      "Iter 3077 | Time 58.0501(59.9889) | Bit/dim 3.6884(3.6814) | Xent 0.1038(0.1070) | Loss 3.7403(3.7349) | Error 0.0363(0.0367) Steps 664(667.36) | Grad Norm 0.9320(1.1201) | Total Time 14.00(14.00)\n",
      "Iter 3078 | Time 60.8368(60.0144) | Bit/dim 3.6802(3.6814) | Xent 0.1129(0.1072) | Loss 3.7366(3.7350) | Error 0.0384(0.0368) Steps 664(667.26) | Grad Norm 1.0842(1.1190) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0513 | Time 24.6398, Epoch Time 398.7126(406.9876), Bit/dim 3.7046(best: 3.7032), Xent 2.6803, Loss 5.0448, Error 0.4300(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3079 | Time 60.3711(60.0251) | Bit/dim 3.6844(3.6815) | Xent 0.1031(0.1070) | Loss 3.7359(3.7350) | Error 0.0359(0.0368) Steps 658(666.98) | Grad Norm 1.1278(1.1193) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 59.5032(60.0094) | Bit/dim 3.6903(3.6818) | Xent 0.1004(0.1068) | Loss 3.7405(3.7352) | Error 0.0343(0.0367) Steps 676(667.25) | Grad Norm 0.8548(1.1114) | Total Time 14.00(14.00)\n",
      "Iter 3081 | Time 60.2244(60.0159) | Bit/dim 3.6748(3.6815) | Xent 0.1174(0.1071) | Loss 3.7335(3.7351) | Error 0.0406(0.0368) Steps 658(666.97) | Grad Norm 1.1910(1.1137) | Total Time 14.00(14.00)\n",
      "Iter 3082 | Time 59.7224(60.0071) | Bit/dim 3.6818(3.6816) | Xent 0.0990(0.1069) | Loss 3.7313(3.7350) | Error 0.0329(0.0367) Steps 670(667.06) | Grad Norm 0.8664(1.1063) | Total Time 14.00(14.00)\n",
      "Iter 3083 | Time 58.9586(59.9756) | Bit/dim 3.6766(3.6814) | Xent 0.1127(0.1071) | Loss 3.7330(3.7349) | Error 0.0365(0.0367) Steps 670(667.15) | Grad Norm 0.9411(1.1014) | Total Time 14.00(14.00)\n",
      "Iter 3084 | Time 61.3754(60.0176) | Bit/dim 3.6835(3.6815) | Xent 0.1058(0.1070) | Loss 3.7364(3.7350) | Error 0.0391(0.0368) Steps 664(667.06) | Grad Norm 1.4149(1.1108) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0514 | Time 24.5869, Epoch Time 400.7532(406.8006), Bit/dim 3.7035(best: 3.7032), Xent 2.6876, Loss 5.0473, Error 0.4287(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3085 | Time 59.7225(60.0087) | Bit/dim 3.6645(3.6810) | Xent 0.1068(0.1070) | Loss 3.7179(3.7345) | Error 0.0383(0.0368) Steps 664(666.97) | Grad Norm 1.5525(1.1240) | Total Time 14.00(14.00)\n",
      "Iter 3086 | Time 58.9007(59.9755) | Bit/dim 3.6876(3.6812) | Xent 0.1051(0.1070) | Loss 3.7402(3.7346) | Error 0.0353(0.0368) Steps 670(667.06) | Grad Norm 1.2427(1.1276) | Total Time 14.00(14.00)\n",
      "Iter 3087 | Time 58.7431(59.9385) | Bit/dim 3.6732(3.6809) | Xent 0.1106(0.1071) | Loss 3.7285(3.7345) | Error 0.0366(0.0368) Steps 664(666.97) | Grad Norm 1.5082(1.1390) | Total Time 14.00(14.00)\n",
      "Iter 3088 | Time 61.2905(59.9791) | Bit/dim 3.6894(3.6812) | Xent 0.0997(0.1069) | Loss 3.7393(3.7346) | Error 0.0323(0.0366) Steps 670(667.06) | Grad Norm 1.5116(1.1502) | Total Time 14.00(14.00)\n",
      "Iter 3089 | Time 60.5253(59.9955) | Bit/dim 3.6831(3.6812) | Xent 0.1057(0.1068) | Loss 3.7359(3.7346) | Error 0.0363(0.0366) Steps 670(667.14) | Grad Norm 1.3565(1.1564) | Total Time 14.00(14.00)\n",
      "Iter 3090 | Time 62.2535(60.0632) | Bit/dim 3.6874(3.6814) | Xent 0.1073(0.1068) | Loss 3.7411(3.7348) | Error 0.0369(0.0366) Steps 670(667.23) | Grad Norm 1.6818(1.1721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0515 | Time 24.5432, Epoch Time 401.8366(406.6517), Bit/dim 3.7047(best: 3.7032), Xent 2.6879, Loss 5.0486, Error 0.4293(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3091 | Time 59.6366(60.0504) | Bit/dim 3.6861(3.6816) | Xent 0.1045(0.1068) | Loss 3.7384(3.7349) | Error 0.0353(0.0366) Steps 670(667.31) | Grad Norm 1.4690(1.1810) | Total Time 14.00(14.00)\n",
      "Iter 3092 | Time 63.7542(60.1615) | Bit/dim 3.6809(3.6815) | Xent 0.1093(0.1068) | Loss 3.7356(3.7350) | Error 0.0369(0.0366) Steps 670(667.39) | Grad Norm 1.2720(1.1838) | Total Time 14.00(14.00)\n",
      "Iter 3093 | Time 61.8129(60.2111) | Bit/dim 3.6813(3.6815) | Xent 0.1000(0.1066) | Loss 3.7313(3.7349) | Error 0.0354(0.0365) Steps 670(667.47) | Grad Norm 0.7942(1.1721) | Total Time 14.00(14.00)\n",
      "Iter 3094 | Time 59.7125(60.1961) | Bit/dim 3.6775(3.6814) | Xent 0.1061(0.1066) | Loss 3.7306(3.7347) | Error 0.0365(0.0365) Steps 664(667.37) | Grad Norm 1.5148(1.1824) | Total Time 14.00(14.00)\n",
      "Iter 3095 | Time 60.5425(60.2065) | Bit/dim 3.6820(3.6814) | Xent 0.0948(0.1063) | Loss 3.7294(3.7346) | Error 0.0334(0.0364) Steps 670(667.45) | Grad Norm 1.4782(1.1912) | Total Time 14.00(14.00)\n",
      "Iter 3096 | Time 61.6626(60.2502) | Bit/dim 3.6810(3.6814) | Xent 0.1036(0.1062) | Loss 3.7328(3.7345) | Error 0.0357(0.0364) Steps 676(667.70) | Grad Norm 1.8778(1.2118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0516 | Time 24.4045, Epoch Time 407.3671(406.6731), Bit/dim 3.7038(best: 3.7032), Xent 2.6736, Loss 5.0405, Error 0.4305(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3097 | Time 60.2494(60.2502) | Bit/dim 3.6769(3.6813) | Xent 0.1057(0.1062) | Loss 3.7297(3.7344) | Error 0.0365(0.0364) Steps 670(667.77) | Grad Norm 1.5256(1.2212) | Total Time 14.00(14.00)\n",
      "Iter 3098 | Time 62.4787(60.3170) | Bit/dim 3.6790(3.6812) | Xent 0.1047(0.1061) | Loss 3.7313(3.7343) | Error 0.0334(0.0363) Steps 670(667.84) | Grad Norm 2.0390(1.2458) | Total Time 14.00(14.00)\n",
      "Iter 3099 | Time 62.1912(60.3732) | Bit/dim 3.6810(3.6812) | Xent 0.1047(0.1061) | Loss 3.7333(3.7343) | Error 0.0354(0.0363) Steps 670(667.90) | Grad Norm 0.8929(1.2352) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 58.5474(60.3185) | Bit/dim 3.6831(3.6813) | Xent 0.1065(0.1061) | Loss 3.7364(3.7343) | Error 0.0391(0.0364) Steps 658(667.61) | Grad Norm 1.3211(1.2378) | Total Time 14.00(14.00)\n",
      "Iter 3101 | Time 58.3254(60.2587) | Bit/dim 3.6870(3.6814) | Xent 0.1018(0.1060) | Loss 3.7379(3.7344) | Error 0.0354(0.0364) Steps 682(668.04) | Grad Norm 0.9909(1.2304) | Total Time 14.00(14.00)\n",
      "Iter 3102 | Time 60.3692(60.2620) | Bit/dim 3.6767(3.6813) | Xent 0.0962(0.1057) | Loss 3.7248(3.7341) | Error 0.0316(0.0362) Steps 658(667.74) | Grad Norm 1.0285(1.2243) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0517 | Time 24.1687, Epoch Time 402.4008(406.5450), Bit/dim 3.7045(best: 3.7032), Xent 2.7292, Loss 5.0691, Error 0.4330(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3103 | Time 59.2491(60.2316) | Bit/dim 3.6866(3.6815) | Xent 0.1059(0.1057) | Loss 3.7395(3.7343) | Error 0.0385(0.0363) Steps 670(667.81) | Grad Norm 1.4764(1.2319) | Total Time 14.00(14.00)\n",
      "Iter 3104 | Time 57.8724(60.1608) | Bit/dim 3.6783(3.6814) | Xent 0.1056(0.1057) | Loss 3.7311(3.7342) | Error 0.0340(0.0362) Steps 676(668.05) | Grad Norm 0.9406(1.2231) | Total Time 14.00(14.00)\n",
      "Iter 3105 | Time 58.5214(60.1116) | Bit/dim 3.6797(3.6813) | Xent 0.1004(0.1055) | Loss 3.7299(3.7341) | Error 0.0340(0.0362) Steps 664(667.93) | Grad Norm 1.5047(1.2316) | Total Time 14.00(14.00)\n",
      "Iter 3106 | Time 61.5441(60.1546) | Bit/dim 3.6787(3.6812) | Xent 0.1003(0.1054) | Loss 3.7288(3.7339) | Error 0.0349(0.0361) Steps 664(667.81) | Grad Norm 1.2118(1.2310) | Total Time 14.00(14.00)\n",
      "Iter 3107 | Time 58.6696(60.1101) | Bit/dim 3.6804(3.6812) | Xent 0.1061(0.1054) | Loss 3.7335(3.7339) | Error 0.0364(0.0361) Steps 664(667.70) | Grad Norm 1.1753(1.2293) | Total Time 14.00(14.00)\n",
      "Iter 3108 | Time 61.2511(60.1443) | Bit/dim 3.6883(3.6814) | Xent 0.1061(0.1054) | Loss 3.7414(3.7341) | Error 0.0381(0.0362) Steps 664(667.59) | Grad Norm 0.8795(1.2188) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0518 | Time 24.4801, Epoch Time 397.5802(406.2760), Bit/dim 3.7038(best: 3.7032), Xent 2.6886, Loss 5.0481, Error 0.4352(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3109 | Time 61.0420(60.1712) | Bit/dim 3.6847(3.6815) | Xent 0.1055(0.1054) | Loss 3.7375(3.7342) | Error 0.0360(0.0362) Steps 664(667.48) | Grad Norm 0.9847(1.2118) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 59.5907(60.1538) | Bit/dim 3.6837(3.6816) | Xent 0.1030(0.1053) | Loss 3.7352(3.7343) | Error 0.0344(0.0361) Steps 676(667.73) | Grad Norm 1.1395(1.2096) | Total Time 14.00(14.00)\n",
      "Iter 3111 | Time 59.3732(60.1304) | Bit/dim 3.6810(3.6816) | Xent 0.1043(0.1053) | Loss 3.7331(3.7342) | Error 0.0367(0.0361) Steps 676(667.98) | Grad Norm 1.2358(1.2104) | Total Time 14.00(14.00)\n",
      "Iter 3112 | Time 58.5089(60.0818) | Bit/dim 3.6793(3.6815) | Xent 0.0999(0.1051) | Loss 3.7293(3.7341) | Error 0.0360(0.0361) Steps 670(668.04) | Grad Norm 0.9719(1.2033) | Total Time 14.00(14.00)\n",
      "Iter 3113 | Time 59.7873(60.0729) | Bit/dim 3.6850(3.6816) | Xent 0.0997(0.1050) | Loss 3.7348(3.7341) | Error 0.0356(0.0361) Steps 664(667.92) | Grad Norm 1.0847(1.1997) | Total Time 14.00(14.00)\n",
      "Iter 3114 | Time 60.3675(60.0818) | Bit/dim 3.6815(3.6816) | Xent 0.1097(0.1051) | Loss 3.7364(3.7342) | Error 0.0365(0.0361) Steps 670(667.98) | Grad Norm 1.4680(1.2077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0519 | Time 24.4914, Epoch Time 398.9543(406.0564), Bit/dim 3.7036(best: 3.7032), Xent 2.6724, Loss 5.0398, Error 0.4281(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3115 | Time 60.0987(60.0823) | Bit/dim 3.6848(3.6817) | Xent 0.1047(0.1051) | Loss 3.7371(3.7342) | Error 0.0359(0.0361) Steps 670(668.04) | Grad Norm 1.3940(1.2133) | Total Time 14.00(14.00)\n",
      "Iter 3116 | Time 60.8270(60.1046) | Bit/dim 3.6756(3.6815) | Xent 0.1035(0.1051) | Loss 3.7274(3.7340) | Error 0.0385(0.0362) Steps 670(668.10) | Grad Norm 1.5261(1.2227) | Total Time 14.00(14.00)\n",
      "Iter 3117 | Time 58.7432(60.0638) | Bit/dim 3.6853(3.6816) | Xent 0.0992(0.1049) | Loss 3.7349(3.7341) | Error 0.0317(0.0361) Steps 664(667.98) | Grad Norm 1.3710(1.2272) | Total Time 14.00(14.00)\n",
      "Iter 3118 | Time 59.9170(60.0594) | Bit/dim 3.6817(3.6816) | Xent 0.1059(0.1049) | Loss 3.7346(3.7341) | Error 0.0363(0.0361) Steps 664(667.86) | Grad Norm 1.5648(1.2373) | Total Time 14.00(14.00)\n",
      "Iter 3119 | Time 60.4799(60.0720) | Bit/dim 3.6783(3.6815) | Xent 0.1057(0.1049) | Loss 3.7311(3.7340) | Error 0.0375(0.0361) Steps 664(667.75) | Grad Norm 0.7630(1.2231) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 62.0315(60.1308) | Bit/dim 3.6856(3.6817) | Xent 0.1034(0.1049) | Loss 3.7373(3.7341) | Error 0.0369(0.0361) Steps 676(667.99) | Grad Norm 1.2860(1.2250) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0520 | Time 24.6139, Epoch Time 402.6040(405.9528), Bit/dim 3.7044(best: 3.7032), Xent 2.6901, Loss 5.0494, Error 0.4323(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3121 | Time 61.3290(60.1667) | Bit/dim 3.6791(3.6816) | Xent 0.1100(0.1050) | Loss 3.7340(3.7341) | Error 0.0367(0.0362) Steps 670(668.05) | Grad Norm 1.5027(1.2333) | Total Time 14.00(14.00)\n",
      "Iter 3122 | Time 59.6868(60.1523) | Bit/dim 3.6858(3.6817) | Xent 0.1059(0.1051) | Loss 3.7388(3.7342) | Error 0.0350(0.0361) Steps 676(668.29) | Grad Norm 1.5025(1.2414) | Total Time 14.00(14.00)\n",
      "Iter 3123 | Time 60.7227(60.1694) | Bit/dim 3.6798(3.6816) | Xent 0.1006(0.1049) | Loss 3.7301(3.7341) | Error 0.0326(0.0360) Steps 670(668.34) | Grad Norm 1.0750(1.2364) | Total Time 14.00(14.00)\n",
      "Iter 3124 | Time 62.4209(60.2370) | Bit/dim 3.6742(3.6814) | Xent 0.1052(0.1049) | Loss 3.7268(3.7339) | Error 0.0374(0.0361) Steps 670(668.39) | Grad Norm 1.3453(1.2396) | Total Time 14.00(14.00)\n",
      "Iter 3125 | Time 58.6327(60.1888) | Bit/dim 3.6811(3.6814) | Xent 0.1020(0.1049) | Loss 3.7321(3.7338) | Error 0.0350(0.0360) Steps 664(668.26) | Grad Norm 1.6768(1.2528) | Total Time 14.00(14.00)\n",
      "Iter 3126 | Time 62.8028(60.2673) | Bit/dim 3.6839(3.6815) | Xent 0.1015(0.1048) | Loss 3.7347(3.7339) | Error 0.0343(0.0360) Steps 670(668.31) | Grad Norm 0.9707(1.2443) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0521 | Time 24.7898, Epoch Time 406.5590(405.9710), Bit/dim 3.7024(best: 3.7032), Xent 2.6833, Loss 5.0440, Error 0.4320(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3127 | Time 61.8554(60.3149) | Bit/dim 3.6840(3.6816) | Xent 0.1104(0.1049) | Loss 3.7392(3.7340) | Error 0.0371(0.0360) Steps 670(668.36) | Grad Norm 1.0101(1.2373) | Total Time 14.00(14.00)\n",
      "Iter 3128 | Time 62.1842(60.3710) | Bit/dim 3.6813(3.6816) | Xent 0.1003(0.1048) | Loss 3.7314(3.7339) | Error 0.0337(0.0359) Steps 664(668.23) | Grad Norm 1.4417(1.2434) | Total Time 14.00(14.00)\n",
      "Iter 3129 | Time 59.4498(60.3433) | Bit/dim 3.6799(3.6815) | Xent 0.1066(0.1048) | Loss 3.7332(3.7339) | Error 0.0340(0.0359) Steps 670(668.29) | Grad Norm 1.4547(1.2497) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 58.9393(60.3012) | Bit/dim 3.6787(3.6814) | Xent 0.1035(0.1048) | Loss 3.7305(3.7338) | Error 0.0340(0.0358) Steps 682(668.70) | Grad Norm 1.2561(1.2499) | Total Time 14.00(14.00)\n",
      "Iter 3131 | Time 58.3971(60.2441) | Bit/dim 3.6783(3.6813) | Xent 0.0982(0.1046) | Loss 3.7274(3.7336) | Error 0.0330(0.0357) Steps 670(668.74) | Grad Norm 0.8421(1.2377) | Total Time 14.00(14.00)\n",
      "Iter 3132 | Time 61.2012(60.2728) | Bit/dim 3.6777(3.6812) | Xent 0.1019(0.1045) | Loss 3.7286(3.7335) | Error 0.0334(0.0357) Steps 664(668.59) | Grad Norm 1.2661(1.2385) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0522 | Time 24.5837, Epoch Time 402.4107(405.8642), Bit/dim 3.7048(best: 3.7024), Xent 2.7347, Loss 5.0721, Error 0.4306(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3133 | Time 60.1235(60.2683) | Bit/dim 3.6805(3.6812) | Xent 0.1031(0.1045) | Loss 3.7320(3.7334) | Error 0.0353(0.0357) Steps 670(668.64) | Grad Norm 1.5471(1.2478) | Total Time 14.00(14.00)\n",
      "Iter 3134 | Time 63.0160(60.3508) | Bit/dim 3.6907(3.6815) | Xent 0.0939(0.1042) | Loss 3.7376(3.7336) | Error 0.0334(0.0356) Steps 676(668.86) | Grad Norm 1.1032(1.2435) | Total Time 14.00(14.00)\n",
      "Iter 3135 | Time 59.5485(60.3267) | Bit/dim 3.6697(3.6811) | Xent 0.1061(0.1042) | Loss 3.7227(3.7332) | Error 0.0365(0.0356) Steps 670(668.89) | Grad Norm 0.9893(1.2358) | Total Time 14.00(14.00)\n",
      "Iter 3136 | Time 60.8235(60.3416) | Bit/dim 3.6817(3.6811) | Xent 0.0969(0.1040) | Loss 3.7301(3.7331) | Error 0.0337(0.0356) Steps 670(668.92) | Grad Norm 1.2876(1.2374) | Total Time 14.00(14.00)\n",
      "Iter 3137 | Time 59.0474(60.3028) | Bit/dim 3.6716(3.6809) | Xent 0.1031(0.1040) | Loss 3.7232(3.7328) | Error 0.0369(0.0356) Steps 664(668.78) | Grad Norm 1.8591(1.2560) | Total Time 14.00(14.00)\n",
      "Iter 3138 | Time 59.2133(60.2701) | Bit/dim 3.6882(3.6811) | Xent 0.1078(0.1041) | Loss 3.7421(3.7331) | Error 0.0371(0.0356) Steps 670(668.81) | Grad Norm 0.9731(1.2476) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0523 | Time 24.2972, Epoch Time 402.0244(405.7490), Bit/dim 3.7049(best: 3.7024), Xent 2.7315, Loss 5.0706, Error 0.4326(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3139 | Time 59.6995(60.2530) | Bit/dim 3.6747(3.6809) | Xent 0.1004(0.1040) | Loss 3.7249(3.7329) | Error 0.0319(0.0355) Steps 670(668.85) | Grad Norm 1.1510(1.2447) | Total Time 14.00(14.00)\n",
      "Iter 3140 | Time 60.9860(60.2750) | Bit/dim 3.6819(3.6809) | Xent 0.0998(0.1039) | Loss 3.7318(3.7328) | Error 0.0334(0.0355) Steps 670(668.88) | Grad Norm 0.9670(1.2363) | Total Time 14.00(14.00)\n",
      "Iter 3141 | Time 62.4260(60.3395) | Bit/dim 3.6803(3.6809) | Xent 0.1028(0.1038) | Loss 3.7317(3.7328) | Error 0.0344(0.0354) Steps 670(668.92) | Grad Norm 1.0799(1.2316) | Total Time 14.00(14.00)\n",
      "Iter 3142 | Time 61.1844(60.3648) | Bit/dim 3.6853(3.6810) | Xent 0.1031(0.1038) | Loss 3.7369(3.7329) | Error 0.0356(0.0354) Steps 670(668.95) | Grad Norm 1.1952(1.2305) | Total Time 14.00(14.00)\n",
      "Iter 3143 | Time 61.8634(60.4098) | Bit/dim 3.6832(3.6811) | Xent 0.1029(0.1038) | Loss 3.7346(3.7330) | Error 0.0355(0.0354) Steps 676(669.16) | Grad Norm 0.8685(1.2197) | Total Time 14.00(14.00)\n",
      "Iter 3144 | Time 60.5086(60.4128) | Bit/dim 3.6783(3.6810) | Xent 0.1007(0.1037) | Loss 3.7287(3.7329) | Error 0.0356(0.0354) Steps 664(669.01) | Grad Norm 1.0096(1.2134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0524 | Time 24.4989, Epoch Time 407.1019(405.7896), Bit/dim 3.7031(best: 3.7024), Xent 2.7179, Loss 5.0621, Error 0.4320(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3145 | Time 61.1338(60.4344) | Bit/dim 3.6814(3.6810) | Xent 0.0966(0.1035) | Loss 3.7297(3.7328) | Error 0.0331(0.0354) Steps 664(668.86) | Grad Norm 0.9492(1.2055) | Total Time 14.00(14.00)\n",
      "Iter 3146 | Time 59.4602(60.4052) | Bit/dim 3.6722(3.6808) | Xent 0.0984(0.1033) | Loss 3.7214(3.7324) | Error 0.0349(0.0354) Steps 664(668.71) | Grad Norm 1.5100(1.2146) | Total Time 14.00(14.00)\n",
      "Iter 3147 | Time 60.4252(60.4058) | Bit/dim 3.6786(3.6807) | Xent 0.1018(0.1033) | Loss 3.7295(3.7323) | Error 0.0344(0.0353) Steps 670(668.75) | Grad Norm 1.2200(1.2148) | Total Time 14.00(14.00)\n",
      "Iter 3148 | Time 60.5470(60.4100) | Bit/dim 3.6828(3.6808) | Xent 0.0934(0.1030) | Loss 3.7296(3.7322) | Error 0.0299(0.0352) Steps 670(668.79) | Grad Norm 0.9288(1.2062) | Total Time 14.00(14.00)\n",
      "Iter 3149 | Time 59.7540(60.3903) | Bit/dim 3.6856(3.6809) | Xent 0.1078(0.1031) | Loss 3.7395(3.7325) | Error 0.0387(0.0353) Steps 676(669.00) | Grad Norm 2.1629(1.2349) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 60.3483(60.3891) | Bit/dim 3.6773(3.6808) | Xent 0.1032(0.1031) | Loss 3.7289(3.7324) | Error 0.0364(0.0353) Steps 670(669.03) | Grad Norm 1.6015(1.2459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0525 | Time 24.8108, Epoch Time 402.5515(405.6924), Bit/dim 3.7028(best: 3.7024), Xent 2.7148, Loss 5.0602, Error 0.4283(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3151 | Time 60.1287(60.3813) | Bit/dim 3.6905(3.6811) | Xent 0.0935(0.1028) | Loss 3.7373(3.7325) | Error 0.0331(0.0352) Steps 670(669.06) | Grad Norm 0.8494(1.2340) | Total Time 14.00(14.00)\n",
      "Iter 3152 | Time 58.8443(60.3351) | Bit/dim 3.6738(3.6809) | Xent 0.0995(0.1027) | Loss 3.7235(3.7322) | Error 0.0343(0.0352) Steps 670(669.09) | Grad Norm 1.2162(1.2334) | Total Time 14.00(14.00)\n",
      "Iter 3153 | Time 61.0256(60.3559) | Bit/dim 3.6742(3.6807) | Xent 0.1097(0.1029) | Loss 3.7290(3.7321) | Error 0.0365(0.0353) Steps 676(669.30) | Grad Norm 1.2274(1.2333) | Total Time 14.00(14.00)\n",
      "Iter 3154 | Time 58.7931(60.3090) | Bit/dim 3.6785(3.6806) | Xent 0.1096(0.1031) | Loss 3.7333(3.7322) | Error 0.0380(0.0353) Steps 664(669.14) | Grad Norm 1.4873(1.2409) | Total Time 14.00(14.00)\n",
      "Iter 3155 | Time 61.3522(60.3403) | Bit/dim 3.6955(3.6810) | Xent 0.0968(0.1030) | Loss 3.7439(3.7325) | Error 0.0327(0.0353) Steps 670(669.16) | Grad Norm 1.1700(1.2388) | Total Time 14.00(14.00)\n",
      "Iter 3156 | Time 58.9584(60.2988) | Bit/dim 3.6692(3.6807) | Xent 0.1021(0.1029) | Loss 3.7203(3.7322) | Error 0.0367(0.0353) Steps 670(669.19) | Grad Norm 1.7235(1.2533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0526 | Time 24.6189, Epoch Time 399.6982(405.5126), Bit/dim 3.7048(best: 3.7024), Xent 2.6934, Loss 5.0515, Error 0.4333(best: 0.4184)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1/epoch_400_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
