{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run3/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 24.8619(25.3788) | Bit/dim 3.5354(3.5494) | Xent 0.0207(0.0465) | Loss 3.5458(3.5726) | Error 0.0100(0.0140) Steps 976(970.59) | Grad Norm 1.2641(2.0681) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 24.3962(25.2194) | Bit/dim 3.5590(3.5456) | Xent 0.0321(0.0412) | Loss 3.5750(3.5662) | Error 0.0067(0.0121) Steps 976(972.13) | Grad Norm 1.0672(1.7767) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 24.8410(25.1059) | Bit/dim 3.5085(3.5395) | Xent 0.0158(0.0353) | Loss 3.5164(3.5572) | Error 0.0044(0.0104) Steps 982(971.54) | Grad Norm 0.7960(1.5123) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 24.4072(25.0196) | Bit/dim 3.5123(3.5370) | Xent 0.0095(0.0302) | Loss 3.5170(3.5521) | Error 0.0022(0.0087) Steps 964(968.89) | Grad Norm 0.5740(1.3082) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 24.4841(24.8908) | Bit/dim 3.5361(3.5323) | Xent 0.0150(0.0269) | Loss 3.5436(3.5457) | Error 0.0044(0.0079) Steps 952(967.51) | Grad Norm 0.7809(1.1884) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 119.4682, Epoch Time 1523.5085(1420.2961), Bit/dim 3.5441(best: inf), Xent 1.8474, Loss 4.4678, Error 0.3262(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 25.1340(24.9160) | Bit/dim 3.5265(3.5302) | Xent 0.0135(0.0242) | Loss 3.5333(3.5423) | Error 0.0056(0.0071) Steps 958(966.47) | Grad Norm 0.9406(1.0911) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 24.3621(24.8648) | Bit/dim 3.5034(3.5275) | Xent 0.0123(0.0202) | Loss 3.5096(3.5377) | Error 0.0033(0.0058) Steps 964(965.12) | Grad Norm 0.5630(0.9748) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 24.3118(24.7714) | Bit/dim 3.4700(3.5247) | Xent 0.0099(0.0183) | Loss 3.4749(3.5339) | Error 0.0033(0.0052) Steps 958(963.44) | Grad Norm 0.8228(0.9022) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 24.5704(24.7307) | Bit/dim 3.5247(3.5264) | Xent 0.0102(0.0166) | Loss 3.5297(3.5346) | Error 0.0022(0.0046) Steps 964(963.38) | Grad Norm 0.5983(0.8267) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 25.5340(24.6939) | Bit/dim 3.5347(3.5244) | Xent 0.0170(0.0148) | Loss 3.5432(3.5318) | Error 0.0056(0.0041) Steps 988(963.29) | Grad Norm 0.7810(0.7645) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 23.6677(24.6715) | Bit/dim 3.5292(3.5228) | Xent 0.0141(0.0138) | Loss 3.5362(3.5297) | Error 0.0022(0.0039) Steps 940(962.05) | Grad Norm 0.5772(0.7321) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 114.9197, Epoch Time 1489.8675(1422.3833), Bit/dim 3.5414(best: 3.5441), Xent 1.9098, Loss 4.4964, Error 0.3290(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 23.5162(24.5756) | Bit/dim 3.5709(3.5230) | Xent 0.0064(0.0133) | Loss 3.5741(3.5296) | Error 0.0011(0.0038) Steps 934(959.30) | Grad Norm 0.4263(0.7180) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 24.2672(24.5198) | Bit/dim 3.5095(3.5241) | Xent 0.0067(0.0119) | Loss 3.5128(3.5300) | Error 0.0011(0.0033) Steps 964(957.46) | Grad Norm 0.4907(0.6635) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 24.3724(24.5697) | Bit/dim 3.5129(3.5213) | Xent 0.0165(0.0110) | Loss 3.5211(3.5268) | Error 0.0044(0.0031) Steps 970(957.30) | Grad Norm 0.7831(0.6377) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 24.4708(24.5487) | Bit/dim 3.4952(3.5188) | Xent 0.0051(0.0104) | Loss 3.4977(3.5240) | Error 0.0011(0.0029) Steps 940(957.46) | Grad Norm 0.4429(0.6061) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 24.7014(24.5843) | Bit/dim 3.5345(3.5178) | Xent 0.0046(0.0102) | Loss 3.5368(3.5229) | Error 0.0011(0.0026) Steps 940(958.52) | Grad Norm 0.3236(0.5820) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 113.3015, Epoch Time 1478.8438(1424.0771), Bit/dim 3.5396(best: 3.5414), Xent 1.9559, Loss 4.5175, Error 0.3288(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 24.0079(24.5326) | Bit/dim 3.5072(3.5164) | Xent 0.0148(0.0103) | Loss 3.5145(3.5215) | Error 0.0033(0.0027) Steps 964(957.33) | Grad Norm 0.6375(0.5684) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 24.8142(24.5291) | Bit/dim 3.5448(3.5179) | Xent 0.0190(0.0104) | Loss 3.5543(3.5231) | Error 0.0067(0.0027) Steps 952(956.29) | Grad Norm 1.0211(0.5963) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 24.5516(24.5348) | Bit/dim 3.5125(3.5166) | Xent 0.0145(0.0099) | Loss 3.5197(3.5215) | Error 0.0044(0.0026) Steps 952(956.51) | Grad Norm 0.7183(0.5858) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 23.8559(24.4524) | Bit/dim 3.5095(3.5156) | Xent 0.0160(0.0098) | Loss 3.5175(3.5206) | Error 0.0033(0.0025) Steps 946(956.34) | Grad Norm 0.7155(0.6023) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 25.2990(24.4438) | Bit/dim 3.5184(3.5177) | Xent 0.0047(0.0098) | Loss 3.5208(3.5226) | Error 0.0000(0.0024) Steps 958(955.65) | Grad Norm 0.4180(0.6021) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 23.3204(24.3587) | Bit/dim 3.4994(3.5173) | Xent 0.0060(0.0093) | Loss 3.5024(3.5219) | Error 0.0022(0.0023) Steps 940(955.09) | Grad Norm 0.5460(0.5936) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 114.9997, Epoch Time 1473.9818(1425.5742), Bit/dim 3.5395(best: 3.5396), Xent 1.9692, Loss 4.5241, Error 0.3288(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 24.7410(24.3806) | Bit/dim 3.5083(3.5163) | Xent 0.0061(0.0086) | Loss 3.5113(3.5206) | Error 0.0022(0.0022) Steps 958(954.01) | Grad Norm 0.4296(0.5735) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 24.3742(24.3521) | Bit/dim 3.5476(3.5176) | Xent 0.0038(0.0080) | Loss 3.5495(3.5216) | Error 0.0000(0.0019) Steps 946(952.73) | Grad Norm 0.2509(0.5368) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 24.7211(24.3785) | Bit/dim 3.5018(3.5183) | Xent 0.0069(0.0076) | Loss 3.5053(3.5221) | Error 0.0022(0.0018) Steps 952(952.41) | Grad Norm 0.3311(0.5243) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 24.7469(24.3743) | Bit/dim 3.4848(3.5156) | Xent 0.0116(0.0073) | Loss 3.4906(3.5192) | Error 0.0044(0.0017) Steps 958(953.50) | Grad Norm 0.7163(0.5298) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 24.7237(24.4036) | Bit/dim 3.5428(3.5150) | Xent 0.0099(0.0078) | Loss 3.5477(3.5189) | Error 0.0022(0.0020) Steps 946(952.44) | Grad Norm 0.4555(0.5408) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 114.6872, Epoch Time 1473.3869(1427.0086), Bit/dim 3.5382(best: 3.5395), Xent 2.0048, Loss 4.5406, Error 0.3301(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 22.8828(24.3066) | Bit/dim 3.5142(3.5143) | Xent 0.0091(0.0078) | Loss 3.5187(3.5182) | Error 0.0011(0.0020) Steps 958(951.39) | Grad Norm 0.6511(0.5529) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 24.4460(24.3248) | Bit/dim 3.5194(3.5138) | Xent 0.0038(0.0074) | Loss 3.5213(3.5175) | Error 0.0000(0.0018) Steps 970(951.74) | Grad Norm 0.2974(0.5207) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 23.3105(24.2770) | Bit/dim 3.5105(3.5153) | Xent 0.0085(0.0083) | Loss 3.5147(3.5194) | Error 0.0022(0.0021) Steps 940(950.71) | Grad Norm 0.6116(0.5304) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 24.5259(24.2547) | Bit/dim 3.5157(3.5128) | Xent 0.0055(0.0080) | Loss 3.5184(3.5168) | Error 0.0011(0.0019) Steps 940(950.64) | Grad Norm 0.4931(0.5347) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 24.5847(24.2242) | Bit/dim 3.5295(3.5168) | Xent 0.0061(0.0081) | Loss 3.5325(3.5208) | Error 0.0011(0.0020) Steps 952(950.40) | Grad Norm 0.6281(0.5581) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 25.0029(24.2309) | Bit/dim 3.5225(3.5146) | Xent 0.0049(0.0082) | Loss 3.5249(3.5187) | Error 0.0011(0.0022) Steps 946(950.32) | Grad Norm 0.4600(0.5914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 113.6745, Epoch Time 1463.0006(1428.0884), Bit/dim 3.5379(best: 3.5382), Xent 2.0027, Loss 4.5393, Error 0.3300(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 24.4640(24.2401) | Bit/dim 3.5475(3.5176) | Xent 0.0091(0.0082) | Loss 3.5520(3.5217) | Error 0.0011(0.0021) Steps 934(949.11) | Grad Norm 0.4646(0.6088) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 23.7588(24.2189) | Bit/dim 3.4918(3.5146) | Xent 0.0028(0.0073) | Loss 3.4932(3.5183) | Error 0.0000(0.0018) Steps 946(948.87) | Grad Norm 0.2187(0.5733) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 23.8376(24.1582) | Bit/dim 3.4903(3.5132) | Xent 0.0065(0.0074) | Loss 3.4935(3.5169) | Error 0.0022(0.0019) Steps 940(948.07) | Grad Norm 1.0567(0.5879) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 25.2003(24.1757) | Bit/dim 3.5281(3.5120) | Xent 0.0063(0.0073) | Loss 3.5313(3.5156) | Error 0.0011(0.0019) Steps 952(949.49) | Grad Norm 0.5353(0.5701) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 23.5640(24.1723) | Bit/dim 3.5274(3.5114) | Xent 0.0056(0.0070) | Loss 3.5302(3.5149) | Error 0.0011(0.0018) Steps 922(948.54) | Grad Norm 0.8082(0.5609) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 114.2340, Epoch Time 1462.8402(1429.1309), Bit/dim 3.5380(best: 3.5379), Xent 2.0547, Loss 4.5654, Error 0.3328(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 24.7403(24.2212) | Bit/dim 3.4724(3.5111) | Xent 0.0085(0.0070) | Loss 3.4767(3.5146) | Error 0.0044(0.0018) Steps 958(949.98) | Grad Norm 0.5940(0.5488) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 24.3917(24.2364) | Bit/dim 3.5204(3.5101) | Xent 0.0070(0.0070) | Loss 3.5239(3.5136) | Error 0.0022(0.0017) Steps 958(948.81) | Grad Norm 0.7399(0.5531) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 23.9901(24.2265) | Bit/dim 3.5080(3.5112) | Xent 0.0064(0.0067) | Loss 3.5112(3.5146) | Error 0.0022(0.0017) Steps 946(948.86) | Grad Norm 0.4755(0.5300) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 23.7424(24.1823) | Bit/dim 3.5469(3.5123) | Xent 0.0050(0.0066) | Loss 3.5494(3.5156) | Error 0.0011(0.0017) Steps 934(947.17) | Grad Norm 0.3634(0.5049) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 24.6597(24.2510) | Bit/dim 3.5145(3.5123) | Xent 0.0031(0.0066) | Loss 3.5160(3.5157) | Error 0.0000(0.0016) Steps 946(950.09) | Grad Norm 0.3462(0.5267) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 24.5008(24.2230) | Bit/dim 3.5116(3.5140) | Xent 0.0037(0.0063) | Loss 3.5135(3.5171) | Error 0.0000(0.0016) Steps 934(948.81) | Grad Norm 0.5473(0.5196) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 113.0202, Epoch Time 1464.0698(1430.1791), Bit/dim 3.5373(best: 3.5379), Xent 2.0778, Loss 4.5762, Error 0.3318(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 24.8071(24.2148) | Bit/dim 3.5191(3.5141) | Xent 0.0029(0.0061) | Loss 3.5205(3.5171) | Error 0.0011(0.0016) Steps 946(948.07) | Grad Norm 0.2307(0.5318) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 24.6629(24.2089) | Bit/dim 3.5107(3.5137) | Xent 0.0087(0.0060) | Loss 3.5151(3.5167) | Error 0.0011(0.0016) Steps 946(949.09) | Grad Norm 0.5526(0.5367) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 24.2727(24.2317) | Bit/dim 3.5306(3.5153) | Xent 0.0107(0.0062) | Loss 3.5359(3.5185) | Error 0.0033(0.0016) Steps 952(948.32) | Grad Norm 0.7650(0.5321) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 23.8066(24.2498) | Bit/dim 3.5115(3.5130) | Xent 0.0075(0.0063) | Loss 3.5152(3.5161) | Error 0.0011(0.0016) Steps 958(947.39) | Grad Norm 0.5216(0.5634) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 24.4670(24.2476) | Bit/dim 3.4840(3.5094) | Xent 0.0056(0.0061) | Loss 3.4868(3.5125) | Error 0.0011(0.0014) Steps 964(948.92) | Grad Norm 0.6156(0.5532) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 113.4406, Epoch Time 1465.6197(1431.2423), Bit/dim 3.5374(best: 3.5373), Xent 2.1115, Loss 4.5931, Error 0.3329(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 24.6459(24.2706) | Bit/dim 3.4823(3.5103) | Xent 0.0053(0.0062) | Loss 3.4850(3.5134) | Error 0.0022(0.0015) Steps 952(949.80) | Grad Norm 0.5390(0.5484) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 24.4427(24.3026) | Bit/dim 3.5205(3.5130) | Xent 0.0033(0.0061) | Loss 3.5221(3.5160) | Error 0.0011(0.0015) Steps 952(948.72) | Grad Norm 0.3983(0.5426) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 23.9120(24.2021) | Bit/dim 3.5094(3.5133) | Xent 0.0027(0.0054) | Loss 3.5108(3.5160) | Error 0.0000(0.0012) Steps 958(949.78) | Grad Norm 0.3368(0.5038) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 23.4754(24.2062) | Bit/dim 3.5026(3.5118) | Xent 0.0022(0.0055) | Loss 3.5037(3.5146) | Error 0.0000(0.0013) Steps 952(951.52) | Grad Norm 0.2091(0.5268) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 24.4521(24.1846) | Bit/dim 3.4809(3.5109) | Xent 0.0025(0.0054) | Loss 3.4822(3.5135) | Error 0.0000(0.0014) Steps 934(950.18) | Grad Norm 0.2540(0.5113) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 24.6178(24.2099) | Bit/dim 3.5195(3.5101) | Xent 0.0104(0.0055) | Loss 3.5247(3.5129) | Error 0.0033(0.0015) Steps 946(950.42) | Grad Norm 0.7907(0.5110) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 114.0175, Epoch Time 1463.1263(1432.1988), Bit/dim 3.5369(best: 3.5373), Xent 2.1281, Loss 4.6010, Error 0.3319(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 23.3345(24.1930) | Bit/dim 3.5022(3.5099) | Xent 0.0046(0.0052) | Loss 3.5045(3.5125) | Error 0.0000(0.0013) Steps 952(951.24) | Grad Norm 0.5478(0.5067) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 24.9222(24.2386) | Bit/dim 3.5116(3.5108) | Xent 0.0063(0.0054) | Loss 3.5147(3.5135) | Error 0.0022(0.0015) Steps 952(952.12) | Grad Norm 0.4728(0.5259) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 23.5077(24.2120) | Bit/dim 3.5290(3.5110) | Xent 0.0029(0.0056) | Loss 3.5304(3.5138) | Error 0.0000(0.0016) Steps 952(950.64) | Grad Norm 0.3163(0.5522) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 24.5990(24.1676) | Bit/dim 3.4751(3.5094) | Xent 0.0050(0.0055) | Loss 3.4776(3.5122) | Error 0.0022(0.0016) Steps 952(951.70) | Grad Norm 0.8298(0.5591) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 24.0275(24.1154) | Bit/dim 3.5327(3.5085) | Xent 0.0041(0.0060) | Loss 3.5348(3.5115) | Error 0.0022(0.0016) Steps 952(950.35) | Grad Norm 0.4378(0.5600) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 113.5503, Epoch Time 1461.3675(1433.0739), Bit/dim 3.5379(best: 3.5369), Xent 2.1351, Loss 4.6055, Error 0.3330(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 24.2306(24.1806) | Bit/dim 3.5290(3.5116) | Xent 0.0017(0.0055) | Loss 3.5298(3.5144) | Error 0.0000(0.0014) Steps 952(951.12) | Grad Norm 0.2770(0.5461) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 24.6189(24.2078) | Bit/dim 3.5394(3.5121) | Xent 0.0099(0.0052) | Loss 3.5444(3.5147) | Error 0.0033(0.0013) Steps 964(952.09) | Grad Norm 1.1016(0.5274) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 24.2541(24.2007) | Bit/dim 3.4758(3.5112) | Xent 0.0035(0.0053) | Loss 3.4775(3.5138) | Error 0.0000(0.0012) Steps 940(951.21) | Grad Norm 0.4820(0.5109) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 23.5215(24.1468) | Bit/dim 3.5094(3.5096) | Xent 0.0081(0.0052) | Loss 3.5134(3.5122) | Error 0.0011(0.0012) Steps 970(952.92) | Grad Norm 0.5740(0.4991) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 23.3085(24.1701) | Bit/dim 3.4914(3.5076) | Xent 0.0080(0.0053) | Loss 3.4954(3.5103) | Error 0.0011(0.0012) Steps 940(952.10) | Grad Norm 0.6073(0.4997) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 23.6044(24.1960) | Bit/dim 3.5153(3.5079) | Xent 0.0115(0.0054) | Loss 3.5210(3.5106) | Error 0.0011(0.0013) Steps 958(952.74) | Grad Norm 0.5642(0.5181) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 113.3118, Epoch Time 1462.7558(1433.9643), Bit/dim 3.5370(best: 3.5369), Xent 2.1684, Loss 4.6212, Error 0.3368(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 24.3812(24.2647) | Bit/dim 3.5198(3.5086) | Xent 0.0043(0.0055) | Loss 3.5220(3.5113) | Error 0.0011(0.0013) Steps 952(952.28) | Grad Norm 0.4457(0.5091) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 23.9372(24.2332) | Bit/dim 3.5100(3.5110) | Xent 0.0030(0.0052) | Loss 3.5115(3.5136) | Error 0.0011(0.0012) Steps 970(952.54) | Grad Norm 0.6434(0.5230) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 23.9748(24.1500) | Bit/dim 3.4992(3.5082) | Xent 0.0044(0.0049) | Loss 3.5014(3.5107) | Error 0.0011(0.0011) Steps 952(951.48) | Grad Norm 0.4060(0.5036) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 24.0813(24.1116) | Bit/dim 3.4958(3.5101) | Xent 0.0069(0.0050) | Loss 3.4992(3.5125) | Error 0.0011(0.0011) Steps 928(949.90) | Grad Norm 0.5456(0.5031) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 23.6824(24.1358) | Bit/dim 3.5093(3.5089) | Xent 0.0025(0.0054) | Loss 3.5105(3.5116) | Error 0.0000(0.0012) Steps 934(948.96) | Grad Norm 0.2836(0.5039) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 113.0269, Epoch Time 1459.1616(1434.7203), Bit/dim 3.5358(best: 3.5369), Xent 2.2017, Loss 4.6366, Error 0.3358(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 24.2951(24.1944) | Bit/dim 3.5340(3.5075) | Xent 0.0042(0.0057) | Loss 3.5361(3.5103) | Error 0.0011(0.0013) Steps 952(950.34) | Grad Norm 0.4638(0.5360) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 23.5122(24.1903) | Bit/dim 3.4803(3.5055) | Xent 0.0093(0.0052) | Loss 3.4849(3.5081) | Error 0.0022(0.0011) Steps 946(948.27) | Grad Norm 1.2324(0.5275) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 23.9279(24.2206) | Bit/dim 3.4951(3.5077) | Xent 0.0043(0.0055) | Loss 3.4973(3.5104) | Error 0.0022(0.0013) Steps 952(947.28) | Grad Norm 0.6247(0.5284) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 23.6756(24.1886) | Bit/dim 3.5167(3.5082) | Xent 0.0036(0.0055) | Loss 3.5185(3.5110) | Error 0.0000(0.0013) Steps 946(947.47) | Grad Norm 0.4000(0.5280) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 24.6694(24.2527) | Bit/dim 3.4864(3.5082) | Xent 0.0088(0.0056) | Loss 3.4909(3.5110) | Error 0.0033(0.0013) Steps 970(949.68) | Grad Norm 0.8907(0.5380) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 23.6565(24.2317) | Bit/dim 3.5253(3.5088) | Xent 0.0045(0.0059) | Loss 3.5275(3.5118) | Error 0.0011(0.0013) Steps 958(950.53) | Grad Norm 0.4573(0.5667) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 114.2549, Epoch Time 1467.5394(1435.7048), Bit/dim 3.5367(best: 3.5358), Xent 2.1949, Loss 4.6342, Error 0.3336(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 24.0279(24.3131) | Bit/dim 3.4937(3.5082) | Xent 0.0034(0.0055) | Loss 3.4953(3.5109) | Error 0.0000(0.0012) Steps 946(950.41) | Grad Norm 0.4709(0.5490) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 24.5861(24.3626) | Bit/dim 3.5002(3.5085) | Xent 0.0053(0.0051) | Loss 3.5029(3.5111) | Error 0.0022(0.0013) Steps 976(950.50) | Grad Norm 0.4516(0.5525) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 24.0031(24.3599) | Bit/dim 3.4918(3.5087) | Xent 0.0060(0.0052) | Loss 3.4948(3.5113) | Error 0.0022(0.0014) Steps 940(950.75) | Grad Norm 0.7350(0.5893) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 24.5881(24.3919) | Bit/dim 3.5117(3.5085) | Xent 0.0018(0.0050) | Loss 3.5126(3.5110) | Error 0.0000(0.0012) Steps 958(951.61) | Grad Norm 0.3221(0.5480) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 24.6379(24.4097) | Bit/dim 3.5097(3.5088) | Xent 0.0024(0.0049) | Loss 3.5109(3.5112) | Error 0.0000(0.0013) Steps 946(951.20) | Grad Norm 0.3093(0.5774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 113.3988, Epoch Time 1477.4249(1436.9564), Bit/dim 3.5366(best: 3.5358), Xent 2.2185, Loss 4.6459, Error 0.3352(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 24.4811(24.3560) | Bit/dim 3.4952(3.5075) | Xent 0.0032(0.0048) | Loss 3.4968(3.5099) | Error 0.0000(0.0013) Steps 964(951.43) | Grad Norm 0.4016(0.5574) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 24.4152(24.3213) | Bit/dim 3.5078(3.5064) | Xent 0.0039(0.0048) | Loss 3.5098(3.5088) | Error 0.0000(0.0013) Steps 952(952.54) | Grad Norm 0.4067(0.5651) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 24.5012(24.3270) | Bit/dim 3.4762(3.5062) | Xent 0.0020(0.0044) | Loss 3.4772(3.5084) | Error 0.0000(0.0012) Steps 952(950.66) | Grad Norm 0.2204(0.5279) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 24.4126(24.2519) | Bit/dim 3.5122(3.5054) | Xent 0.0040(0.0044) | Loss 3.5143(3.5076) | Error 0.0011(0.0012) Steps 958(950.70) | Grad Norm 0.5162(0.5367) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 23.7454(24.2301) | Bit/dim 3.5232(3.5079) | Xent 0.0018(0.0050) | Loss 3.5242(3.5104) | Error 0.0000(0.0012) Steps 964(951.24) | Grad Norm 0.2299(0.5650) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 23.8845(24.1890) | Bit/dim 3.5194(3.5107) | Xent 0.0150(0.0055) | Loss 3.5269(3.5135) | Error 0.0056(0.0014) Steps 946(952.09) | Grad Norm 1.1290(0.6509) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 111.6660, Epoch Time 1458.3628(1437.5986), Bit/dim 3.5374(best: 3.5358), Xent 2.2411, Loss 4.6579, Error 0.3348(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 24.2269(24.1897) | Bit/dim 3.5338(3.5097) | Xent 0.0034(0.0058) | Loss 3.5355(3.5126) | Error 0.0011(0.0014) Steps 940(951.40) | Grad Norm 0.5443(0.6472) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 24.6141(24.2161) | Bit/dim 3.4870(3.5073) | Xent 0.0097(0.0055) | Loss 3.4918(3.5100) | Error 0.0044(0.0015) Steps 946(952.93) | Grad Norm 0.7905(0.6170) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 23.8625(24.2349) | Bit/dim 3.5057(3.5058) | Xent 0.0036(0.0050) | Loss 3.5075(3.5083) | Error 0.0011(0.0013) Steps 952(955.30) | Grad Norm 0.4681(0.5706) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 23.8170(24.1197) | Bit/dim 3.4891(3.5104) | Xent 0.0026(0.0053) | Loss 3.4905(3.5130) | Error 0.0011(0.0014) Steps 958(953.49) | Grad Norm 0.4241(0.5631) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 24.4832(24.2337) | Bit/dim 3.4791(3.5090) | Xent 0.0026(0.0051) | Loss 3.4804(3.5115) | Error 0.0011(0.0014) Steps 946(953.37) | Grad Norm 0.3566(0.5526) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 113.3999, Epoch Time 1465.2060(1438.4268), Bit/dim 3.5353(best: 3.5358), Xent 2.2500, Loss 4.6603, Error 0.3346(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 24.1405(24.3116) | Bit/dim 3.4953(3.5086) | Xent 0.0085(0.0047) | Loss 3.4995(3.5109) | Error 0.0033(0.0014) Steps 946(953.60) | Grad Norm 1.8667(0.5892) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 24.4721(24.3759) | Bit/dim 3.5198(3.5071) | Xent 0.0037(0.0045) | Loss 3.5217(3.5093) | Error 0.0022(0.0013) Steps 952(953.54) | Grad Norm 0.5435(0.5750) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 24.4970(24.3529) | Bit/dim 3.4903(3.5052) | Xent 0.0027(0.0045) | Loss 3.4917(3.5075) | Error 0.0000(0.0012) Steps 970(952.27) | Grad Norm 0.4650(0.5880) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 25.0594(24.3450) | Bit/dim 3.4937(3.5067) | Xent 0.0051(0.0042) | Loss 3.4963(3.5088) | Error 0.0022(0.0011) Steps 946(951.88) | Grad Norm 1.2409(0.5741) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 24.1349(24.3380) | Bit/dim 3.4887(3.5053) | Xent 0.0040(0.0041) | Loss 3.4907(3.5074) | Error 0.0011(0.0010) Steps 940(952.03) | Grad Norm 1.0142(0.5796) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 24.4355(24.3341) | Bit/dim 3.5130(3.5098) | Xent 0.0062(0.0044) | Loss 3.5161(3.5119) | Error 0.0022(0.0011) Steps 946(952.79) | Grad Norm 0.4285(0.5882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 112.1756, Epoch Time 1470.5582(1439.3908), Bit/dim 3.5354(best: 3.5353), Xent 2.2878, Loss 4.6793, Error 0.3354(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 24.3865(24.2828) | Bit/dim 3.5223(3.5097) | Xent 0.0065(0.0043) | Loss 3.5255(3.5119) | Error 0.0022(0.0011) Steps 940(952.64) | Grad Norm 0.7832(0.5689) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 23.8017(24.2080) | Bit/dim 3.5043(3.5082) | Xent 0.0023(0.0042) | Loss 3.5054(3.5103) | Error 0.0011(0.0011) Steps 964(953.17) | Grad Norm 0.3171(0.5679) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 22.8940(24.2386) | Bit/dim 3.5181(3.5069) | Xent 0.0032(0.0043) | Loss 3.5197(3.5091) | Error 0.0011(0.0010) Steps 958(952.43) | Grad Norm 0.7349(0.5838) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 24.4827(24.2417) | Bit/dim 3.5274(3.5058) | Xent 0.0021(0.0043) | Loss 3.5284(3.5079) | Error 0.0000(0.0010) Steps 952(953.09) | Grad Norm 0.3104(0.5695) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 25.5408(24.3010) | Bit/dim 3.5297(3.5060) | Xent 0.0029(0.0043) | Loss 3.5311(3.5082) | Error 0.0011(0.0010) Steps 952(951.25) | Grad Norm 0.5434(0.5566) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 112.5521, Epoch Time 1465.0831(1440.1616), Bit/dim 3.5365(best: 3.5353), Xent 2.2880, Loss 4.6805, Error 0.3362(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 24.3597(24.3279) | Bit/dim 3.5208(3.5083) | Xent 0.0034(0.0044) | Loss 3.5225(3.5106) | Error 0.0011(0.0011) Steps 946(949.95) | Grad Norm 0.7131(0.5750) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 24.2901(24.3606) | Bit/dim 3.5027(3.5069) | Xent 0.0024(0.0047) | Loss 3.5039(3.5092) | Error 0.0000(0.0012) Steps 964(951.46) | Grad Norm 0.5776(0.6062) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 24.0094(24.3123) | Bit/dim 3.5088(3.5052) | Xent 0.0016(0.0050) | Loss 3.5096(3.5076) | Error 0.0000(0.0012) Steps 958(951.78) | Grad Norm 0.3313(0.6125) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 24.8195(24.3070) | Bit/dim 3.4912(3.5042) | Xent 0.0144(0.0051) | Loss 3.4984(3.5068) | Error 0.0056(0.0013) Steps 958(952.95) | Grad Norm 1.0523(0.6372) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 24.4034(24.2787) | Bit/dim 3.5147(3.5048) | Xent 0.0011(0.0049) | Loss 3.5153(3.5073) | Error 0.0000(0.0013) Steps 928(951.93) | Grad Norm 0.2454(0.6137) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 24.1096(24.3246) | Bit/dim 3.5084(3.5082) | Xent 0.0030(0.0049) | Loss 3.5099(3.5106) | Error 0.0011(0.0013) Steps 964(953.37) | Grad Norm 0.4510(0.5787) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 113.2454, Epoch Time 1468.2648(1441.0047), Bit/dim 3.5349(best: 3.5353), Xent 2.3007, Loss 4.6853, Error 0.3378(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 24.9265(24.3088) | Bit/dim 3.5060(3.5078) | Xent 0.0028(0.0049) | Loss 3.5074(3.5103) | Error 0.0000(0.0013) Steps 940(951.80) | Grad Norm 0.4761(0.5711) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 24.3949(24.3419) | Bit/dim 3.5111(3.5060) | Xent 0.0010(0.0049) | Loss 3.5116(3.5084) | Error 0.0000(0.0013) Steps 952(950.63) | Grad Norm 0.2266(0.5644) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 24.4219(24.3785) | Bit/dim 3.5254(3.5062) | Xent 0.0015(0.0049) | Loss 3.5261(3.5086) | Error 0.0000(0.0012) Steps 946(950.36) | Grad Norm 0.2122(0.5651) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 25.8750(24.3716) | Bit/dim 3.4797(3.5060) | Xent 0.0065(0.0049) | Loss 3.4830(3.5084) | Error 0.0022(0.0014) Steps 946(951.91) | Grad Norm 0.7064(0.6102) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 24.9381(24.3886) | Bit/dim 3.5145(3.5070) | Xent 0.0026(0.0047) | Loss 3.5158(3.5094) | Error 0.0000(0.0014) Steps 958(951.88) | Grad Norm 0.4761(0.6346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 113.3674, Epoch Time 1472.6449(1441.9539), Bit/dim 3.5356(best: 3.5349), Xent 2.3223, Loss 4.6967, Error 0.3359(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 23.9642(24.3837) | Bit/dim 3.5013(3.5069) | Xent 0.0011(0.0051) | Loss 3.5019(3.5094) | Error 0.0000(0.0014) Steps 946(952.94) | Grad Norm 0.2953(0.6315) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 23.6308(24.3007) | Bit/dim 3.4784(3.5070) | Xent 0.0037(0.0047) | Loss 3.4803(3.5094) | Error 0.0022(0.0014) Steps 970(952.94) | Grad Norm 0.4847(0.6196) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 23.4936(24.3216) | Bit/dim 3.5469(3.5066) | Xent 0.0027(0.0045) | Loss 3.5483(3.5088) | Error 0.0000(0.0012) Steps 940(952.09) | Grad Norm 0.4251(0.5919) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 23.8086(24.3164) | Bit/dim 3.5410(3.5049) | Xent 0.0029(0.0045) | Loss 3.5425(3.5072) | Error 0.0011(0.0012) Steps 946(952.77) | Grad Norm 0.2153(0.5815) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 24.4773(24.3183) | Bit/dim 3.5071(3.5050) | Xent 0.0085(0.0047) | Loss 3.5114(3.5073) | Error 0.0022(0.0013) Steps 958(953.82) | Grad Norm 0.7776(0.6209) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 24.4134(24.3416) | Bit/dim 3.5333(3.5076) | Xent 0.0021(0.0047) | Loss 3.5344(3.5099) | Error 0.0000(0.0012) Steps 958(955.25) | Grad Norm 0.4195(0.6093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 112.9390, Epoch Time 1467.8618(1442.7311), Bit/dim 3.5352(best: 3.5349), Xent 2.3000, Loss 4.6852, Error 0.3337(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 23.5091(24.2298) | Bit/dim 3.5219(3.5080) | Xent 0.0030(0.0047) | Loss 3.5234(3.5103) | Error 0.0011(0.0013) Steps 952(955.38) | Grad Norm 0.7432(0.6541) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 24.5828(24.2514) | Bit/dim 3.5268(3.5113) | Xent 0.0019(0.0042) | Loss 3.5278(3.5134) | Error 0.0000(0.0011) Steps 952(954.32) | Grad Norm 0.3575(0.6357) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 24.4593(24.2500) | Bit/dim 3.4787(3.5081) | Xent 0.0112(0.0043) | Loss 3.4843(3.5103) | Error 0.0033(0.0011) Steps 940(953.56) | Grad Norm 1.0747(0.6693) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 24.0297(24.2774) | Bit/dim 3.4667(3.5052) | Xent 0.0062(0.0046) | Loss 3.4698(3.5075) | Error 0.0011(0.0011) Steps 958(953.03) | Grad Norm 1.5763(0.7238) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 23.3104(24.2292) | Bit/dim 3.5154(3.5043) | Xent 0.0016(0.0043) | Loss 3.5162(3.5065) | Error 0.0000(0.0011) Steps 946(953.39) | Grad Norm 0.3292(0.6675) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 111.8205, Epoch Time 1461.3894(1443.2908), Bit/dim 3.5352(best: 3.5349), Xent 2.3410, Loss 4.7056, Error 0.3375(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 24.0628(24.3087) | Bit/dim 3.5268(3.5065) | Xent 0.0048(0.0045) | Loss 3.5293(3.5087) | Error 0.0011(0.0012) Steps 958(953.01) | Grad Norm 0.5547(0.6505) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 24.3188(24.2262) | Bit/dim 3.4730(3.5055) | Xent 0.0185(0.0047) | Loss 3.4823(3.5078) | Error 0.0056(0.0013) Steps 946(952.35) | Grad Norm 1.6031(0.6472) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 23.5752(24.1521) | Bit/dim 3.5080(3.5051) | Xent 0.0031(0.0043) | Loss 3.5096(3.5072) | Error 0.0011(0.0012) Steps 952(952.82) | Grad Norm 0.4177(0.6098) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 23.7961(24.1173) | Bit/dim 3.5001(3.5042) | Xent 0.0034(0.0045) | Loss 3.5018(3.5064) | Error 0.0011(0.0012) Steps 946(953.70) | Grad Norm 1.1227(0.6340) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 25.0260(24.0905) | Bit/dim 3.5298(3.5029) | Xent 0.0046(0.0045) | Loss 3.5321(3.5052) | Error 0.0022(0.0012) Steps 952(953.41) | Grad Norm 0.6396(0.6209) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 23.6285(24.1855) | Bit/dim 3.5192(3.5056) | Xent 0.0021(0.0043) | Loss 3.5203(3.5077) | Error 0.0000(0.0012) Steps 946(951.15) | Grad Norm 0.4070(0.6290) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 112.9322, Epoch Time 1459.0504(1443.7636), Bit/dim 3.5346(best: 3.5349), Xent 2.3323, Loss 4.7008, Error 0.3363(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 23.2311(24.2191) | Bit/dim 3.5172(3.5068) | Xent 0.0109(0.0042) | Loss 3.5227(3.5089) | Error 0.0022(0.0011) Steps 952(951.66) | Grad Norm 1.5124(0.6131) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 24.7382(24.2721) | Bit/dim 3.5127(3.5047) | Xent 0.0042(0.0049) | Loss 3.5148(3.5072) | Error 0.0011(0.0014) Steps 940(952.81) | Grad Norm 0.6722(0.6657) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 24.6714(24.3547) | Bit/dim 3.4938(3.5043) | Xent 0.0034(0.0045) | Loss 3.4955(3.5065) | Error 0.0011(0.0012) Steps 958(951.03) | Grad Norm 0.4926(0.6475) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 24.1751(24.3349) | Bit/dim 3.5246(3.5063) | Xent 0.0093(0.0046) | Loss 3.5293(3.5086) | Error 0.0022(0.0013) Steps 952(951.45) | Grad Norm 0.9736(0.6404) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 23.8550(24.3514) | Bit/dim 3.4890(3.5033) | Xent 0.0031(0.0045) | Loss 3.4906(3.5056) | Error 0.0000(0.0013) Steps 958(953.01) | Grad Norm 0.5461(0.6414) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 112.7119, Epoch Time 1472.0720(1444.6129), Bit/dim 3.5348(best: 3.5346), Xent 2.3358, Loss 4.7027, Error 0.3348(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 24.7568(24.3074) | Bit/dim 3.5168(3.5030) | Xent 0.0018(0.0046) | Loss 3.5177(3.5053) | Error 0.0000(0.0014) Steps 964(953.86) | Grad Norm 0.4512(0.6606) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 24.7229(24.3966) | Bit/dim 3.4721(3.5018) | Xent 0.0012(0.0040) | Loss 3.4728(3.5037) | Error 0.0000(0.0011) Steps 946(952.63) | Grad Norm 0.3234(0.5955) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 24.3828(24.4485) | Bit/dim 3.4997(3.5041) | Xent 0.0030(0.0040) | Loss 3.5011(3.5061) | Error 0.0011(0.0012) Steps 958(953.28) | Grad Norm 0.5701(0.6268) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 23.8252(24.4528) | Bit/dim 3.4622(3.5043) | Xent 0.0057(0.0043) | Loss 3.4651(3.5064) | Error 0.0022(0.0013) Steps 958(953.04) | Grad Norm 0.7880(0.6432) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 24.4547(24.4652) | Bit/dim 3.5071(3.5019) | Xent 0.0121(0.0044) | Loss 3.5131(3.5041) | Error 0.0044(0.0013) Steps 952(952.66) | Grad Norm 1.5382(0.6664) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 24.5465(24.4838) | Bit/dim 3.5106(3.5042) | Xent 0.0054(0.0048) | Loss 3.5133(3.5066) | Error 0.0022(0.0013) Steps 964(951.73) | Grad Norm 0.6446(0.6848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 112.2136, Epoch Time 1479.1189(1445.6481), Bit/dim 3.5348(best: 3.5346), Xent 2.3378, Loss 4.7037, Error 0.3341(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 24.3625(24.4200) | Bit/dim 3.4961(3.5052) | Xent 0.0021(0.0045) | Loss 3.4972(3.5074) | Error 0.0000(0.0012) Steps 946(950.66) | Grad Norm 0.3876(0.6616) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 24.6721(24.4604) | Bit/dim 3.4964(3.5050) | Xent 0.0030(0.0045) | Loss 3.4979(3.5073) | Error 0.0011(0.0012) Steps 946(951.06) | Grad Norm 0.5133(0.6839) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 24.8264(24.4407) | Bit/dim 3.4977(3.5055) | Xent 0.0028(0.0041) | Loss 3.4991(3.5075) | Error 0.0011(0.0010) Steps 940(950.72) | Grad Norm 0.5081(0.6488) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 24.0578(24.4046) | Bit/dim 3.4913(3.5035) | Xent 0.0054(0.0044) | Loss 3.4940(3.5057) | Error 0.0011(0.0011) Steps 958(951.65) | Grad Norm 0.6483(0.6512) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 24.8324(24.3610) | Bit/dim 3.5110(3.5016) | Xent 0.0074(0.0046) | Loss 3.5147(3.5039) | Error 0.0022(0.0012) Steps 946(953.81) | Grad Norm 0.8307(0.7100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 112.3166, Epoch Time 1468.4681(1446.3327), Bit/dim 3.5371(best: 3.5346), Xent 2.3648, Loss 4.7195, Error 0.3333(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 24.6327(24.3555) | Bit/dim 3.5025(3.5053) | Xent 0.0072(0.0053) | Loss 3.5061(3.5079) | Error 0.0011(0.0014) Steps 946(953.12) | Grad Norm 1.2116(0.7965) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 23.5627(24.3441) | Bit/dim 3.5126(3.5064) | Xent 0.0011(0.0052) | Loss 3.5131(3.5090) | Error 0.0000(0.0014) Steps 952(953.65) | Grad Norm 0.4971(0.7816) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 24.3188(24.3125) | Bit/dim 3.4936(3.5066) | Xent 0.0124(0.0053) | Loss 3.4998(3.5093) | Error 0.0033(0.0014) Steps 958(953.51) | Grad Norm 0.8616(0.7528) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 22.8512(24.2803) | Bit/dim 3.4638(3.5027) | Xent 0.0050(0.0056) | Loss 3.4663(3.5055) | Error 0.0022(0.0015) Steps 940(952.99) | Grad Norm 0.9843(0.7915) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 23.8794(24.2573) | Bit/dim 3.5244(3.5020) | Xent 0.0024(0.0050) | Loss 3.5256(3.5045) | Error 0.0011(0.0014) Steps 964(953.04) | Grad Norm 0.9492(0.7682) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 22.9932(24.1512) | Bit/dim 3.5369(3.5042) | Xent 0.0038(0.0047) | Loss 3.5388(3.5066) | Error 0.0011(0.0013) Steps 958(953.81) | Grad Norm 0.6824(0.7181) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 112.1321, Epoch Time 1461.8216(1446.7973), Bit/dim 3.5337(best: 3.5346), Xent 2.3575, Loss 4.7124, Error 0.3340(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 24.1030(24.1330) | Bit/dim 3.5029(3.5024) | Xent 0.0014(0.0045) | Loss 3.5036(3.5046) | Error 0.0000(0.0012) Steps 952(954.49) | Grad Norm 0.2734(0.6911) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 24.2359(24.2236) | Bit/dim 3.5177(3.5001) | Xent 0.0071(0.0043) | Loss 3.5213(3.5022) | Error 0.0011(0.0010) Steps 958(954.52) | Grad Norm 0.7201(0.6637) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 24.2349(24.2085) | Bit/dim 3.5011(3.5024) | Xent 0.0031(0.0042) | Loss 3.5027(3.5045) | Error 0.0011(0.0010) Steps 970(954.31) | Grad Norm 0.6794(0.6585) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 24.0526(24.3454) | Bit/dim 3.5304(3.5037) | Xent 0.0014(0.0040) | Loss 3.5311(3.5057) | Error 0.0000(0.0010) Steps 958(955.51) | Grad Norm 0.2984(0.6458) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 24.8435(24.3752) | Bit/dim 3.5289(3.5047) | Xent 0.0040(0.0039) | Loss 3.5309(3.5066) | Error 0.0011(0.0010) Steps 958(957.39) | Grad Norm 0.5829(0.6368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 114.3886, Epoch Time 1473.6964(1447.6043), Bit/dim 3.5335(best: 3.5337), Xent 2.3732, Loss 4.7201, Error 0.3349(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 24.5935(24.3467) | Bit/dim 3.5012(3.5057) | Xent 0.0025(0.0039) | Loss 3.5025(3.5076) | Error 0.0000(0.0010) Steps 952(958.09) | Grad Norm 0.3116(0.6084) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 24.4820(24.3473) | Bit/dim 3.4928(3.5047) | Xent 0.0014(0.0038) | Loss 3.4935(3.5066) | Error 0.0000(0.0010) Steps 976(957.62) | Grad Norm 0.2829(0.6071) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 23.8822(24.2869) | Bit/dim 3.4935(3.5058) | Xent 0.0010(0.0043) | Loss 3.4940(3.5080) | Error 0.0000(0.0010) Steps 958(957.56) | Grad Norm 0.3421(0.6447) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 24.1343(24.2613) | Bit/dim 3.5178(3.5038) | Xent 0.0022(0.0039) | Loss 3.5189(3.5057) | Error 0.0000(0.0009) Steps 946(955.25) | Grad Norm 0.5057(0.6084) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 24.0604(24.3099) | Bit/dim 3.5010(3.5047) | Xent 0.0034(0.0040) | Loss 3.5027(3.5067) | Error 0.0011(0.0010) Steps 946(953.35) | Grad Norm 0.3778(0.6101) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 24.3576(24.2963) | Bit/dim 3.5074(3.5025) | Xent 0.0024(0.0036) | Loss 3.5086(3.5043) | Error 0.0000(0.0009) Steps 964(955.19) | Grad Norm 0.6139(0.5986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 114.1769, Epoch Time 1466.9026(1448.1833), Bit/dim 3.5346(best: 3.5335), Xent 2.4212, Loss 4.7452, Error 0.3402(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 23.5843(24.2313) | Bit/dim 3.4992(3.5026) | Xent 0.0078(0.0042) | Loss 3.5031(3.5046) | Error 0.0033(0.0012) Steps 952(954.23) | Grad Norm 1.2082(0.6908) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 24.6784(24.2587) | Bit/dim 3.5376(3.5034) | Xent 0.0024(0.0046) | Loss 3.5388(3.5057) | Error 0.0011(0.0014) Steps 952(954.19) | Grad Norm 0.6727(0.7081) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 23.7415(24.1963) | Bit/dim 3.5028(3.5028) | Xent 0.0048(0.0049) | Loss 3.5052(3.5053) | Error 0.0022(0.0015) Steps 946(953.73) | Grad Norm 0.8226(0.7664) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 24.4454(24.1989) | Bit/dim 3.5103(3.5022) | Xent 0.0062(0.0048) | Loss 3.5134(3.5046) | Error 0.0011(0.0014) Steps 940(953.54) | Grad Norm 0.4471(0.7265) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 22.9180(24.1222) | Bit/dim 3.4941(3.5018) | Xent 0.0034(0.0042) | Loss 3.4958(3.5039) | Error 0.0011(0.0012) Steps 946(952.84) | Grad Norm 0.6349(0.6782) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 112.5137, Epoch Time 1457.3722(1448.4589), Bit/dim 3.5322(best: 3.5335), Xent 2.3895, Loss 4.7270, Error 0.3343(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 24.5682(24.1232) | Bit/dim 3.4895(3.5019) | Xent 0.0064(0.0041) | Loss 3.4927(3.5039) | Error 0.0033(0.0012) Steps 946(952.28) | Grad Norm 1.3375(0.6758) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 24.2909(24.0876) | Bit/dim 3.4728(3.5004) | Xent 0.0087(0.0045) | Loss 3.4772(3.5027) | Error 0.0022(0.0013) Steps 952(952.52) | Grad Norm 1.0429(0.7239) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 23.8188(24.1373) | Bit/dim 3.5245(3.5043) | Xent 0.0054(0.0041) | Loss 3.5273(3.5064) | Error 0.0022(0.0012) Steps 964(953.62) | Grad Norm 0.7608(0.7208) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 24.2766(24.1958) | Bit/dim 3.4672(3.5048) | Xent 0.0014(0.0039) | Loss 3.4679(3.5067) | Error 0.0000(0.0011) Steps 946(953.05) | Grad Norm 0.3068(0.6824) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 25.3424(24.2741) | Bit/dim 3.5034(3.5018) | Xent 0.0010(0.0040) | Loss 3.5039(3.5038) | Error 0.0000(0.0011) Steps 964(953.00) | Grad Norm 0.3254(0.6977) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 24.8584(24.2868) | Bit/dim 3.4891(3.5017) | Xent 0.0009(0.0038) | Loss 3.4895(3.5036) | Error 0.0000(0.0010) Steps 964(952.87) | Grad Norm 0.4776(0.6617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 112.8279, Epoch Time 1465.7161(1448.9766), Bit/dim 3.5337(best: 3.5322), Xent 2.4402, Loss 4.7538, Error 0.3372(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 23.7226(24.2907) | Bit/dim 3.5032(3.5013) | Xent 0.0023(0.0039) | Loss 3.5043(3.5033) | Error 0.0011(0.0009) Steps 934(952.19) | Grad Norm 0.7558(0.6769) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 24.1516(24.3520) | Bit/dim 3.5025(3.5064) | Xent 0.0023(0.0036) | Loss 3.5036(3.5082) | Error 0.0000(0.0008) Steps 964(952.50) | Grad Norm 0.6751(0.6753) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 24.2973(24.3612) | Bit/dim 3.4836(3.5026) | Xent 0.0062(0.0036) | Loss 3.4867(3.5044) | Error 0.0022(0.0010) Steps 958(951.62) | Grad Norm 0.5255(0.6644) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 24.0107(24.4010) | Bit/dim 3.4793(3.5032) | Xent 0.0074(0.0035) | Loss 3.4830(3.5050) | Error 0.0022(0.0010) Steps 958(952.90) | Grad Norm 0.8094(0.6481) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 24.4611(24.4012) | Bit/dim 3.5057(3.5011) | Xent 0.0029(0.0035) | Loss 3.5071(3.5029) | Error 0.0011(0.0010) Steps 964(955.50) | Grad Norm 0.4191(0.6397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 115.6906, Epoch Time 1478.2929(1449.8561), Bit/dim 3.5334(best: 3.5322), Xent 2.4362, Loss 4.7515, Error 0.3376(best: 0.3262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 24.9056(24.4354) | Bit/dim 3.5041(3.5025) | Xent 0.0017(0.0037) | Loss 3.5049(3.5044) | Error 0.0000(0.0011) Steps 952(957.58) | Grad Norm 0.5446(0.6439) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 23.9725(24.2667) | Bit/dim 3.4851(3.5023) | Xent 0.0009(0.0043) | Loss 3.4855(3.5045) | Error 0.0000(0.0011) Steps 940(957.12) | Grad Norm 0.2908(0.7572) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 24.8078(24.3105) | Bit/dim 3.5033(3.5000) | Xent 0.0014(0.0044) | Loss 3.5040(3.5022) | Error 0.0000(0.0012) Steps 976(958.52) | Grad Norm 0.2838(0.7679) | Total Time 14.00(14.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run3 --resume ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run3/epoch_250_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
