{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1_temp', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.3478(33.9367) | Bit/dim 9.0579(9.2256) | Xent 2.3026(2.3026) | Loss 9.0579(9.2256) | Error 0.8889(0.9055) Steps 574(574.00) | Grad Norm 12.6728(17.0634) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.4539(28.4961) | Bit/dim 8.5456(9.1005) | Xent 2.3026(2.3026) | Loss 8.5456(9.1005) | Error 0.9122(0.9047) Steps 574(574.00) | Grad Norm 4.6958(14.6764) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 13.5727(24.5834) | Bit/dim 8.3427(8.9316) | Xent 2.3026(2.3026) | Loss 8.3427(8.9316) | Error 0.8956(0.9025) Steps 574(574.00) | Grad Norm 3.9852(11.8121) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.2098(21.7134) | Bit/dim 8.1906(8.7620) | Xent 2.3026(2.3026) | Loss 8.1906(8.7620) | Error 0.9156(0.9018) Steps 574(574.00) | Grad Norm 2.8375(9.5343) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.2261(19.5666) | Bit/dim 7.9086(8.5799) | Xent 2.3026(2.3026) | Loss 7.9086(8.5799) | Error 0.8811(0.9013) Steps 574(574.00) | Grad Norm 2.2735(7.7682) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 81.1552, Epoch Time 865.3321(865.3321), Bit/dim 7.7931(best: inf), Xent 2.3026, Loss 7.7931, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.0767(17.9558) | Bit/dim 7.6689(8.3771) | Xent 2.3026(2.3026) | Loss 7.6689(8.3771) | Error 0.8911(0.8997) Steps 574(574.00) | Grad Norm 2.4695(6.4241) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.8848(16.7770) | Bit/dim 7.3701(8.1468) | Xent 2.3026(2.3026) | Loss 7.3701(8.1468) | Error 0.8944(0.8994) Steps 574(574.00) | Grad Norm 2.0775(5.3400) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 13.6572(15.8773) | Bit/dim 7.1668(7.9103) | Xent 2.3026(2.3026) | Loss 7.1668(7.9103) | Error 0.8900(0.8992) Steps 574(574.00) | Grad Norm 1.4636(4.3787) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.3884(15.3017) | Bit/dim 7.0556(7.6986) | Xent 2.3026(2.3026) | Loss 7.0556(7.6986) | Error 0.9033(0.9003) Steps 592(577.72) | Grad Norm 1.0212(3.5093) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.3017(15.2964) | Bit/dim 7.0179(7.5245) | Xent 2.3026(2.3026) | Loss 7.0179(7.5245) | Error 0.8878(0.8998) Steps 610(585.50) | Grad Norm 0.5303(2.7592) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.7065(15.3131) | Bit/dim 6.9890(7.3848) | Xent 2.3026(2.3026) | Loss 6.9890(7.3848) | Error 0.8878(0.9005) Steps 622(593.44) | Grad Norm 0.4507(2.1573) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 82.1449, Epoch Time 879.2993(865.7511), Bit/dim 6.9754(best: 7.7931), Xent 2.3026, Loss 6.9754, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 15.7576(15.4460) | Bit/dim 6.9354(7.2709) | Xent 2.3026(2.3026) | Loss 6.9354(7.2709) | Error 0.9100(0.9007) Steps 628(602.06) | Grad Norm 0.7352(1.7223) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.6501(15.6373) | Bit/dim 6.8692(7.1768) | Xent 2.3026(2.3026) | Loss 6.8692(7.1768) | Error 0.9122(0.9008) Steps 628(608.87) | Grad Norm 0.4663(1.4274) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 16.6768(15.9128) | Bit/dim 6.8129(7.0923) | Xent 2.3026(2.3026) | Loss 6.8129(7.0923) | Error 0.8789(0.8998) Steps 634(615.33) | Grad Norm 0.4967(1.2092) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.4815(16.0979) | Bit/dim 6.7425(7.0129) | Xent 2.3026(2.3026) | Loss 6.7425(7.0129) | Error 0.9011(0.9005) Steps 640(620.76) | Grad Norm 0.6887(1.0365) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 16.3832(16.1965) | Bit/dim 6.6237(6.9215) | Xent 2.3026(2.3026) | Loss 6.6237(6.9215) | Error 0.8889(0.9001) Steps 640(625.81) | Grad Norm 7.3240(1.3268) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 82.5941, Epoch Time 1002.4037(869.8506), Bit/dim 6.4769(best: 6.9754), Xent 2.3026, Loss 6.4769, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 16.9463(16.3406) | Bit/dim 6.3581(6.8058) | Xent 2.3026(2.3026) | Loss 6.3581(6.8058) | Error 0.9078(0.9002) Steps 640(629.36) | Grad Norm 3.6118(3.9394) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 16.5439(16.3935) | Bit/dim 6.1953(6.6655) | Xent 2.3026(2.3026) | Loss 6.1953(6.6655) | Error 0.9044(0.9010) Steps 646(632.33) | Grad Norm 29.3879(8.3056) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.5852(16.6584) | Bit/dim 6.0260(6.5087) | Xent 2.3026(2.3026) | Loss 6.0260(6.5087) | Error 0.9056(0.9004) Steps 670(639.09) | Grad Norm 30.1133(12.1223) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.3993(16.9639) | Bit/dim 5.8392(6.3491) | Xent 2.3026(2.3026) | Loss 5.8392(6.3491) | Error 0.8967(0.8998) Steps 652(646.49) | Grad Norm 2.6877(12.8215) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.2452(17.1179) | Bit/dim 5.7363(6.1958) | Xent 2.3026(2.3026) | Loss 5.7363(6.1958) | Error 0.8978(0.9002) Steps 670(650.45) | Grad Norm 11.2880(12.1136) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 16.9978(17.2037) | Bit/dim 5.6572(6.0606) | Xent 2.3026(2.3026) | Loss 5.6572(6.0606) | Error 0.8989(0.8993) Steps 670(655.58) | Grad Norm 6.9986(10.8442) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 88.3562, Epoch Time 1057.2773(875.4734), Bit/dim 5.6558(best: 6.4769), Xent 2.3026, Loss 5.6558, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.1725(17.2699) | Bit/dim 5.5973(5.9498) | Xent 2.3026(2.3026) | Loss 5.5973(5.9498) | Error 0.8911(0.9001) Steps 676(660.79) | Grad Norm 8.3048(11.2323) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 18.1032(17.3385) | Bit/dim 5.5906(5.8578) | Xent 2.3026(2.3026) | Loss 5.5906(5.8578) | Error 0.9011(0.9003) Steps 682(664.86) | Grad Norm 5.6147(9.8899) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.9097(17.3688) | Bit/dim 5.5418(5.7833) | Xent 2.3026(2.3026) | Loss 5.5418(5.7833) | Error 0.8944(0.8998) Steps 682(664.38) | Grad Norm 3.7220(8.3680) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 18.3895(17.4881) | Bit/dim 5.5399(5.7224) | Xent 2.3026(2.3026) | Loss 5.5399(5.7224) | Error 0.9144(0.9004) Steps 688(668.18) | Grad Norm 5.9116(7.3168) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.9524(17.5318) | Bit/dim 5.5758(5.6807) | Xent 2.3026(2.3026) | Loss 5.5758(5.6807) | Error 0.9033(0.8996) Steps 664(670.89) | Grad Norm 9.1865(8.9281) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 87.7809, Epoch Time 1072.1268(881.3730), Bit/dim 5.5213(best: 5.6558), Xent 2.3026, Loss 5.5213, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.8574(17.4164) | Bit/dim 5.5155(5.6408) | Xent 2.3026(2.3026) | Loss 5.5155(5.6408) | Error 0.8956(0.9003) Steps 670(670.20) | Grad Norm 3.4159(7.7033) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.9911(17.3836) | Bit/dim 5.4634(5.6028) | Xent 2.3026(2.3026) | Loss 5.4634(5.6028) | Error 0.8867(0.8999) Steps 670(670.94) | Grad Norm 10.4078(7.4179) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 16.9109(17.3477) | Bit/dim 5.4549(5.5661) | Xent 2.3026(2.3026) | Loss 5.4549(5.5661) | Error 0.9100(0.8995) Steps 676(671.81) | Grad Norm 4.9551(7.4450) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.5395(17.3950) | Bit/dim 5.3561(5.5298) | Xent 2.3026(2.3026) | Loss 5.3561(5.5298) | Error 0.9000(0.8982) Steps 676(672.91) | Grad Norm 1.5908(6.6015) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 18.1372(17.3782) | Bit/dim 5.3837(5.4967) | Xent 2.3026(2.3026) | Loss 5.3837(5.4967) | Error 0.9056(0.8992) Steps 676(673.71) | Grad Norm 13.4958(7.3952) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.9629(17.3479) | Bit/dim 5.3638(5.4619) | Xent 2.3026(2.3026) | Loss 5.3638(5.4619) | Error 0.8978(0.9006) Steps 688(674.71) | Grad Norm 6.6891(8.3870) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 89.3694, Epoch Time 1058.2717(886.6800), Bit/dim 5.3358(best: 5.5213), Xent 2.3026, Loss 5.3358, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 17.5865(17.3142) | Bit/dim 5.4185(5.4302) | Xent 2.3026(2.3026) | Loss 5.4185(5.4302) | Error 0.8978(0.9001) Steps 676(675.78) | Grad Norm 32.6772(10.7077) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 16.9113(17.2963) | Bit/dim 5.2488(5.3914) | Xent 2.3026(2.3026) | Loss 5.2488(5.3914) | Error 0.9000(0.9000) Steps 676(676.12) | Grad Norm 7.7494(10.3739) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 16.8014(17.2604) | Bit/dim 5.2282(5.3431) | Xent 2.3026(2.3026) | Loss 5.2282(5.3431) | Error 0.8800(0.9000) Steps 694(679.19) | Grad Norm 11.7375(9.3747) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 17.6506(17.3112) | Bit/dim 5.1207(5.2949) | Xent 2.3026(2.3026) | Loss 5.1207(5.2949) | Error 0.9100(0.8996) Steps 700(683.28) | Grad Norm 7.2417(8.8659) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.3030(17.2629) | Bit/dim 5.0774(5.2500) | Xent 2.3026(2.3026) | Loss 5.0774(5.2500) | Error 0.9011(0.9002) Steps 682(684.19) | Grad Norm 3.2629(8.1957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 92.2554, Epoch Time 1060.0550(891.8813), Bit/dim 5.0713(best: 5.3358), Xent 2.3026, Loss 5.0713, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 17.0299(17.2908) | Bit/dim 5.0596(5.2063) | Xent 2.3026(2.3026) | Loss 5.0596(5.2063) | Error 0.8911(0.9000) Steps 676(685.51) | Grad Norm 4.1291(7.7611) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.7813(17.1820) | Bit/dim 5.0608(5.1650) | Xent 2.3026(2.3026) | Loss 5.0608(5.1650) | Error 0.8944(0.8998) Steps 676(683.45) | Grad Norm 7.5972(7.9888) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 17.1232(17.0881) | Bit/dim 5.1037(5.1352) | Xent 2.3026(2.3026) | Loss 5.1037(5.1352) | Error 0.8967(0.8998) Steps 670(680.83) | Grad Norm 24.6978(9.3794) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 16.7832(17.0160) | Bit/dim 4.9384(5.0924) | Xent 2.3026(2.3026) | Loss 4.9384(5.0924) | Error 0.9100(0.9003) Steps 670(678.13) | Grad Norm 6.3658(8.7693) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 16.6288(16.9991) | Bit/dim 4.8549(5.0433) | Xent 2.3026(2.3026) | Loss 4.8549(5.0433) | Error 0.8944(0.9006) Steps 670(675.99) | Grad Norm 1.8751(7.4716) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 16.8925(16.9518) | Bit/dim 4.8609(5.0040) | Xent 2.3026(2.3026) | Loss 4.8609(5.0040) | Error 0.9056(0.9002) Steps 664(674.38) | Grad Norm 2.3316(7.1071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 88.5753, Epoch Time 1035.3445(896.1852), Bit/dim 4.8752(best: 5.0713), Xent 2.3026, Loss 4.8752, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 17.1307(16.9659) | Bit/dim 4.8544(4.9721) | Xent 2.3026(2.3026) | Loss 4.8544(4.9721) | Error 0.8900(0.8992) Steps 658(671.64) | Grad Norm 8.9125(7.7719) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 18.3750(17.0499) | Bit/dim 4.9631(4.9686) | Xent 2.3026(2.3026) | Loss 4.9631(4.9686) | Error 0.9133(0.8997) Steps 688(671.87) | Grad Norm 20.1450(10.8397) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 18.3172(17.2976) | Bit/dim 4.7931(4.9377) | Xent 2.3026(2.3026) | Loss 4.7931(4.9377) | Error 0.8967(0.9004) Steps 694(675.01) | Grad Norm 9.4868(10.5259) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 18.1565(17.6240) | Bit/dim 4.7894(4.9009) | Xent 2.3026(2.3026) | Loss 4.7894(4.9009) | Error 0.9067(0.8999) Steps 694(681.36) | Grad Norm 4.5417(9.2232) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.3043(17.8322) | Bit/dim 4.7673(4.8627) | Xent 2.3026(2.3026) | Loss 4.7673(4.8627) | Error 0.9100(0.8993) Steps 712(687.31) | Grad Norm 4.9637(8.0513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 93.7498, Epoch Time 1096.4166(902.1921), Bit/dim 4.7354(best: 4.8752), Xent 2.3026, Loss 4.7354, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.4875(18.0114) | Bit/dim 4.7366(4.8307) | Xent 2.3026(2.3026) | Loss 4.7366(4.8307) | Error 0.8900(0.8999) Steps 706(692.40) | Grad Norm 7.2065(7.3167) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 18.2024(18.0937) | Bit/dim 4.7181(4.8033) | Xent 2.3026(2.3026) | Loss 4.7181(4.8033) | Error 0.9078(0.9008) Steps 706(696.87) | Grad Norm 6.0180(8.0066) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 18.1054(18.1647) | Bit/dim 4.7232(4.7829) | Xent 2.3026(2.3026) | Loss 4.7232(4.7829) | Error 0.8878(0.8998) Steps 706(700.98) | Grad Norm 11.8618(9.5228) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.7900(18.2382) | Bit/dim 4.6665(4.7643) | Xent 2.3026(2.3026) | Loss 4.6665(4.7643) | Error 0.9089(0.8997) Steps 712(704.19) | Grad Norm 2.5213(8.8869) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.3299(18.3594) | Bit/dim 4.6427(4.7379) | Xent 2.3026(2.3026) | Loss 4.6427(4.7379) | Error 0.8878(0.8996) Steps 718(710.27) | Grad Norm 4.6273(7.7613) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 19.1184(18.4657) | Bit/dim 4.7043(4.7167) | Xent 2.3026(2.3026) | Loss 4.7043(4.7167) | Error 0.8944(0.9002) Steps 742(713.93) | Grad Norm 17.9414(7.9648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 93.8893, Epoch Time 1130.4710(909.0405), Bit/dim 4.7145(best: 4.7354), Xent 2.3026, Loss 4.7145, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.5716(18.5316) | Bit/dim 4.6401(4.6930) | Xent 2.3026(2.3026) | Loss 4.6401(4.6930) | Error 0.9011(0.8998) Steps 706(715.97) | Grad Norm 4.5002(8.3932) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 19.7586(18.6639) | Bit/dim 4.5931(4.6716) | Xent 2.3026(2.3026) | Loss 4.5931(4.6716) | Error 0.9078(0.8994) Steps 730(718.72) | Grad Norm 7.4532(8.6595) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.5084(18.7234) | Bit/dim 4.5535(4.6458) | Xent 2.3026(2.3026) | Loss 4.5535(4.6458) | Error 0.8922(0.9006) Steps 730(719.62) | Grad Norm 4.8381(7.8503) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.1396(18.7013) | Bit/dim 5.4487(4.7271) | Xent 2.3026(2.3026) | Loss 5.4487(4.7271) | Error 0.9156(0.9019) Steps 700(719.55) | Grad Norm 13.6981(12.5288) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 17.8176(18.5553) | Bit/dim 4.8957(4.8312) | Xent 2.3026(2.3026) | Loss 4.8957(4.8312) | Error 0.8933(0.9005) Steps 670(714.91) | Grad Norm 9.1171(11.7533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 88.3074, Epoch Time 1125.4675(915.5333), Bit/dim 4.8067(best: 4.7145), Xent 2.3026, Loss 4.8067, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 17.5216(18.2026) | Bit/dim 4.7338(4.8293) | Xent 2.3026(2.3026) | Loss 4.7338(4.8293) | Error 0.9011(0.8992) Steps 682(703.54) | Grad Norm 3.0353(9.9341) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 16.9488(17.9802) | Bit/dim 4.6003(4.7860) | Xent 2.3026(2.3026) | Loss 4.6003(4.7860) | Error 0.8989(0.8998) Steps 676(694.89) | Grad Norm 1.8799(7.8908) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 17.2949(17.7864) | Bit/dim 4.5909(4.7372) | Xent 2.3026(2.3026) | Loss 4.5909(4.7372) | Error 0.8978(0.8992) Steps 688(689.85) | Grad Norm 1.0339(6.1590) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 17.9718(17.7553) | Bit/dim 4.5706(4.6927) | Xent 2.3026(2.3026) | Loss 4.5706(4.6927) | Error 0.8867(0.8993) Steps 688(689.07) | Grad Norm 0.8460(4.7991) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 18.9467(17.9268) | Bit/dim 4.5285(4.6509) | Xent 2.3026(2.3026) | Loss 4.5285(4.6509) | Error 0.8978(0.8997) Steps 694(689.67) | Grad Norm 1.9214(3.8796) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.5189(18.0128) | Bit/dim 4.4831(4.6120) | Xent 2.3026(2.3026) | Loss 4.4831(4.6120) | Error 0.8989(0.9004) Steps 688(691.24) | Grad Norm 3.9355(3.4184) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 93.4308, Epoch Time 1087.8621(920.7031), Bit/dim 4.5005(best: 4.7145), Xent 2.3026, Loss 4.5005, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 18.5702(18.1479) | Bit/dim 4.4666(4.5789) | Xent 2.3026(2.3026) | Loss 4.4666(4.5789) | Error 0.9033(0.9007) Steps 712(695.32) | Grad Norm 1.2509(2.9998) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 18.2801(18.2470) | Bit/dim 4.4892(4.5531) | Xent 2.3026(2.3026) | Loss 4.4892(4.5531) | Error 0.8844(0.9009) Steps 712(696.77) | Grad Norm 1.5350(2.6168) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 18.1125(18.2712) | Bit/dim 4.4608(4.5309) | Xent 2.3026(2.3026) | Loss 4.4608(4.5309) | Error 0.8978(0.9014) Steps 694(697.71) | Grad Norm 4.4535(2.9224) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 17.1264(18.2485) | Bit/dim 4.4263(4.5101) | Xent 2.3026(2.3026) | Loss 4.4263(4.5101) | Error 0.8822(0.9003) Steps 706(699.21) | Grad Norm 2.4481(2.7588) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 18.3156(18.2526) | Bit/dim 4.4260(4.4905) | Xent 2.3026(2.3026) | Loss 4.4260(4.4905) | Error 0.8978(0.9007) Steps 712(702.46) | Grad Norm 1.9570(2.5279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 93.1629, Epoch Time 1122.0073(926.7423), Bit/dim 4.4185(best: 4.5005), Xent 2.3026, Loss 4.4185, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 18.9188(18.3273) | Bit/dim 4.3797(4.4736) | Xent 2.3026(2.3026) | Loss 4.3797(4.4736) | Error 0.8933(0.9000) Steps 724(704.63) | Grad Norm 4.0484(2.5735) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 18.7128(18.4231) | Bit/dim 4.4048(4.4552) | Xent 2.3026(2.3026) | Loss 4.4048(4.4552) | Error 0.9022(0.8997) Steps 730(705.87) | Grad Norm 0.7532(2.6644) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 19.3796(18.4679) | Bit/dim 4.3745(4.4394) | Xent 2.3026(2.3026) | Loss 4.3745(4.4394) | Error 0.9033(0.8996) Steps 718(706.91) | Grad Norm 1.5487(3.6788) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 19.4851(18.5758) | Bit/dim 4.3825(4.4248) | Xent 2.3026(2.3026) | Loss 4.3825(4.4248) | Error 0.8989(0.8995) Steps 706(709.04) | Grad Norm 2.1459(3.7255) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.6091(18.6685) | Bit/dim 4.3333(4.4047) | Xent 2.3026(2.3026) | Loss 4.3333(4.4047) | Error 0.9022(0.8990) Steps 724(709.35) | Grad Norm 5.3322(3.3616) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 19.0710(18.7650) | Bit/dim 4.5133(4.4292) | Xent 2.3026(2.3026) | Loss 4.5133(4.4292) | Error 0.8811(0.8999) Steps 736(713.12) | Grad Norm 10.7013(5.9257) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 95.3754, Epoch Time 1147.8012(933.3740), Bit/dim 4.4723(best: 4.4185), Xent 2.3026, Loss 4.4723, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.7063(18.8812) | Bit/dim 4.3924(4.4287) | Xent 2.3026(2.3026) | Loss 4.3924(4.4287) | Error 0.9011(0.8988) Steps 724(720.30) | Grad Norm 3.8252(5.7873) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 17.9283(18.7214) | Bit/dim 4.3272(4.4123) | Xent 2.3026(2.3026) | Loss 4.3272(4.4123) | Error 0.9111(0.8999) Steps 682(711.81) | Grad Norm 1.6096(4.9489) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 18.7779(18.6525) | Bit/dim 4.3333(4.3907) | Xent 2.3026(2.3026) | Loss 4.3333(4.3907) | Error 0.8911(0.9007) Steps 718(708.22) | Grad Norm 1.6604(4.1708) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 19.2255(18.7697) | Bit/dim 4.3039(4.3673) | Xent 2.3026(2.3026) | Loss 4.3039(4.3673) | Error 0.9022(0.8996) Steps 724(712.55) | Grad Norm 1.3699(3.4778) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 19.8400(18.9538) | Bit/dim 4.3018(4.3464) | Xent 2.3026(2.3026) | Loss 4.3018(4.3464) | Error 0.8956(0.8990) Steps 766(721.88) | Grad Norm 4.0273(3.1019) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 93.4563, Epoch Time 1154.9474(940.0212), Bit/dim 4.3337(best: 4.4185), Xent 2.3026, Loss 4.3337, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 19.1054(19.0137) | Bit/dim 4.3009(4.3409) | Xent 2.3026(2.3026) | Loss 4.3009(4.3409) | Error 0.9022(0.9011) Steps 724(726.69) | Grad Norm 3.5324(3.9720) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 18.9611(19.0579) | Bit/dim 4.2217(4.3203) | Xent 2.3026(2.3026) | Loss 4.2217(4.3203) | Error 0.8856(0.9001) Steps 748(727.33) | Grad Norm 3.5977(3.7981) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 19.3508(19.0449) | Bit/dim 4.2307(4.3040) | Xent 2.3026(2.3026) | Loss 4.2307(4.3040) | Error 0.8800(0.9007) Steps 736(730.77) | Grad Norm 2.6467(3.7353) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 19.1084(19.0608) | Bit/dim 4.2863(4.2887) | Xent 2.3026(2.3026) | Loss 4.2863(4.2887) | Error 0.8956(0.8995) Steps 730(734.53) | Grad Norm 10.3865(4.1189) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 18.4562(19.0909) | Bit/dim 4.2669(4.2796) | Xent 2.3026(2.3026) | Loss 4.2669(4.2796) | Error 0.9067(0.8995) Steps 754(738.50) | Grad Norm 4.3245(4.2366) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 19.4861(19.2045) | Bit/dim 4.2363(4.2665) | Xent 2.3026(2.3026) | Loss 4.2363(4.2665) | Error 0.8822(0.9001) Steps 772(746.34) | Grad Norm 1.6961(3.9103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 100.0693, Epoch Time 1172.0921(946.9834), Bit/dim 4.2043(best: 4.3337), Xent 2.3026, Loss 4.2043, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 19.1011(19.3035) | Bit/dim 4.2616(4.2521) | Xent 2.3026(2.3026) | Loss 4.2616(4.2521) | Error 0.8978(0.9007) Steps 748(752.53) | Grad Norm 11.8348(4.2419) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 20.6663(19.4134) | Bit/dim 4.2078(4.2444) | Xent 2.3026(2.3026) | Loss 4.2078(4.2444) | Error 0.9011(0.8993) Steps 784(756.19) | Grad Norm 4.8256(4.9581) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 19.8354(19.5148) | Bit/dim 4.1357(4.2288) | Xent 2.3026(2.3026) | Loss 4.1357(4.2288) | Error 0.9189(0.9002) Steps 784(761.30) | Grad Norm 3.4662(4.7901) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 19.8293(19.6042) | Bit/dim 4.1686(4.2149) | Xent 2.3026(2.3026) | Loss 4.1686(4.2149) | Error 0.8956(0.9007) Steps 784(766.71) | Grad Norm 4.4721(4.8466) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 20.0188(19.7569) | Bit/dim 4.1734(4.2022) | Xent 2.3026(2.3026) | Loss 4.1734(4.2022) | Error 0.8933(0.8999) Steps 796(774.63) | Grad Norm 8.1375(5.2321) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 102.6683, Epoch Time 1213.0553(954.9655), Bit/dim 4.1618(best: 4.2043), Xent 2.3026, Loss 4.1618, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 20.3259(19.9036) | Bit/dim 4.1306(4.1896) | Xent 2.3026(2.3026) | Loss 4.1306(4.1896) | Error 0.9011(0.8989) Steps 802(778.39) | Grad Norm 7.0243(5.1865) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 19.8682(20.0073) | Bit/dim 4.0994(4.1770) | Xent 2.3026(2.3026) | Loss 4.0994(4.1770) | Error 0.8911(0.8978) Steps 802(782.59) | Grad Norm 2.1939(5.0562) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 21.1788(20.1455) | Bit/dim 4.1296(4.1615) | Xent 2.3026(2.3026) | Loss 4.1296(4.1615) | Error 0.8989(0.8986) Steps 808(786.65) | Grad Norm 6.6267(4.5270) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 20.5637(20.2688) | Bit/dim 4.1061(4.1497) | Xent 2.3026(2.3026) | Loss 4.1061(4.1497) | Error 0.8944(0.8993) Steps 820(790.29) | Grad Norm 6.6580(4.8137) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 20.4662(20.4330) | Bit/dim 4.1153(4.1391) | Xent 2.3026(2.3026) | Loss 4.1153(4.1391) | Error 0.9000(0.8996) Steps 790(792.39) | Grad Norm 4.4785(5.0495) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 20.2710(20.4872) | Bit/dim 4.1093(4.1284) | Xent 2.3026(2.3026) | Loss 4.1093(4.1284) | Error 0.9078(0.9018) Steps 790(793.49) | Grad Norm 4.1373(4.6812) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 102.0583, Epoch Time 1252.2795(963.8849), Bit/dim 4.0781(best: 4.1618), Xent 2.3026, Loss 4.0781, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 20.6129(20.6264) | Bit/dim 4.0585(4.1129) | Xent 2.3026(2.3026) | Loss 4.0585(4.1129) | Error 0.9033(0.9010) Steps 808(797.59) | Grad Norm 3.0570(4.2669) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 21.5462(20.8079) | Bit/dim 4.0492(4.1018) | Xent 2.3026(2.3026) | Loss 4.0492(4.1018) | Error 0.8944(0.9015) Steps 802(801.98) | Grad Norm 0.9455(4.2320) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 21.5612(20.9330) | Bit/dim 4.0812(4.0903) | Xent 2.3026(2.3026) | Loss 4.0812(4.0903) | Error 0.8933(0.9014) Steps 820(806.78) | Grad Norm 6.2837(4.3124) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 21.1665(20.9637) | Bit/dim 4.0267(4.0806) | Xent 2.3026(2.3026) | Loss 4.0267(4.0806) | Error 0.8844(0.9011) Steps 802(809.53) | Grad Norm 3.8201(4.5124) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 21.5974(21.0160) | Bit/dim 4.0131(4.0675) | Xent 2.3026(2.3026) | Loss 4.0131(4.0675) | Error 0.8933(0.8996) Steps 802(810.46) | Grad Norm 5.9935(4.3994) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 101.9913, Epoch Time 1283.7436(973.4807), Bit/dim 4.0231(best: 4.0781), Xent 2.3026, Loss 4.0231, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 21.8296(21.0720) | Bit/dim 3.9886(4.0552) | Xent 2.3026(2.3026) | Loss 3.9886(4.0552) | Error 0.8844(0.8992) Steps 832(813.52) | Grad Norm 2.8649(4.1268) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 21.1188(21.0995) | Bit/dim 4.0446(4.0422) | Xent 2.3026(2.3026) | Loss 4.0446(4.0422) | Error 0.9044(0.9001) Steps 808(815.43) | Grad Norm 3.8727(3.7110) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 21.1891(21.1415) | Bit/dim 4.0559(4.0347) | Xent 2.3026(2.3026) | Loss 4.0559(4.0347) | Error 0.9144(0.9003) Steps 832(818.45) | Grad Norm 7.5701(3.9194) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 20.7666(21.0719) | Bit/dim 4.0329(4.0279) | Xent 2.3026(2.3026) | Loss 4.0329(4.0279) | Error 0.9000(0.9000) Steps 826(820.49) | Grad Norm 3.6394(4.2315) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 20.4335(21.0129) | Bit/dim 3.9603(4.0176) | Xent 2.3026(2.3026) | Loss 3.9603(4.0176) | Error 0.8800(0.8997) Steps 820(819.82) | Grad Norm 1.6638(4.0711) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 21.3475(20.9714) | Bit/dim 3.9747(4.0054) | Xent 2.3026(2.3026) | Loss 3.9747(4.0054) | Error 0.9022(0.9000) Steps 820(819.74) | Grad Norm 2.5119(3.6444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 102.6214, Epoch Time 1277.2754(982.5945), Bit/dim 3.9690(best: 4.0231), Xent 2.3026, Loss 3.9690, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 21.5950(20.9817) | Bit/dim 3.9601(3.9943) | Xent 2.3026(2.3026) | Loss 3.9601(3.9943) | Error 0.8967(0.9003) Steps 826(821.27) | Grad Norm 1.2115(3.5701) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 21.4211(21.0436) | Bit/dim 3.9877(3.9863) | Xent 2.3026(2.3026) | Loss 3.9877(3.9863) | Error 0.9167(0.9000) Steps 832(823.30) | Grad Norm 6.2724(3.5891) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 20.7245(21.0140) | Bit/dim 3.9723(3.9812) | Xent 2.3026(2.3026) | Loss 3.9723(3.9812) | Error 0.8989(0.9007) Steps 826(822.94) | Grad Norm 2.0528(4.1692) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 21.0475(21.0328) | Bit/dim 3.9255(3.9725) | Xent 2.3026(2.3026) | Loss 3.9255(3.9725) | Error 0.9089(0.8998) Steps 814(821.19) | Grad Norm 2.2851(3.9161) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 21.0455(21.0728) | Bit/dim 3.9687(3.9671) | Xent 2.3026(2.3026) | Loss 3.9687(3.9671) | Error 0.9100(0.9002) Steps 808(820.18) | Grad Norm 7.3529(4.4297) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 101.9709, Epoch Time 1279.4976(991.5016), Bit/dim 3.9444(best: 3.9690), Xent 2.3026, Loss 3.9444, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 20.4062(21.0028) | Bit/dim 3.9421(3.9619) | Xent 2.3026(2.3026) | Loss 3.9421(3.9619) | Error 0.8922(0.9001) Steps 814(820.05) | Grad Norm 3.0113(4.2420) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 20.7773(20.9251) | Bit/dim 3.8941(3.9521) | Xent 2.3026(2.3026) | Loss 3.8941(3.9521) | Error 0.8911(0.8991) Steps 808(818.75) | Grad Norm 2.6191(3.9666) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 21.2497(20.9048) | Bit/dim 3.9093(3.9419) | Xent 2.3026(2.3026) | Loss 3.9093(3.9419) | Error 0.8867(0.8994) Steps 832(818.90) | Grad Norm 2.4804(3.5157) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.9329(20.9363) | Bit/dim 3.8925(3.9338) | Xent 2.3026(2.3026) | Loss 3.8925(3.9338) | Error 0.9122(0.9000) Steps 826(819.62) | Grad Norm 3.9452(3.7652) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 20.8999(20.9702) | Bit/dim 3.8930(3.9292) | Xent 2.3026(2.3026) | Loss 3.8930(3.9292) | Error 0.8944(0.8992) Steps 826(820.81) | Grad Norm 2.6591(4.0453) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 20.8864(21.0014) | Bit/dim 3.8723(3.9203) | Xent 2.3026(2.3026) | Loss 3.8723(3.9203) | Error 0.9056(0.9006) Steps 820(821.27) | Grad Norm 3.8441(3.8030) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 102.6724, Epoch Time 1270.9668(999.8856), Bit/dim 3.8988(best: 3.9444), Xent 2.3026, Loss 3.8988, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 20.9656(20.9980) | Bit/dim 3.8716(3.9121) | Xent 2.3026(2.3026) | Loss 3.8716(3.9121) | Error 0.8911(0.9005) Steps 814(820.18) | Grad Norm 2.0934(3.5437) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 21.4640(21.0235) | Bit/dim 3.8860(3.9077) | Xent 2.3026(2.3026) | Loss 3.8860(3.9077) | Error 0.9033(0.9020) Steps 844(821.68) | Grad Norm 6.3447(4.0695) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 19.9460(20.8676) | Bit/dim 3.8956(3.9033) | Xent 2.3026(2.3026) | Loss 3.8956(3.9033) | Error 0.9000(0.9006) Steps 784(816.38) | Grad Norm 5.1628(4.2467) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 20.2195(20.7717) | Bit/dim 3.8906(3.8962) | Xent 2.3026(2.3026) | Loss 3.8906(3.8962) | Error 0.9044(0.8999) Steps 796(812.03) | Grad Norm 2.0980(3.8228) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 20.3528(20.7435) | Bit/dim 3.8859(3.8908) | Xent 2.3026(2.3026) | Loss 3.8859(3.8908) | Error 0.9022(0.9003) Steps 796(807.91) | Grad Norm 2.7766(4.0301) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 100.3919, Epoch Time 1260.0602(1007.6908), Bit/dim 3.8694(best: 3.8988), Xent 2.3026, Loss 3.8694, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 19.8623(20.7184) | Bit/dim 3.8554(3.8852) | Xent 2.3026(2.3026) | Loss 3.8554(3.8852) | Error 0.9111(0.8991) Steps 778(805.90) | Grad Norm 2.6463(3.7962) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 20.1670(20.6736) | Bit/dim 3.8306(3.8763) | Xent 2.3026(2.3026) | Loss 3.8306(3.8763) | Error 0.9000(0.8987) Steps 790(802.90) | Grad Norm 5.9416(3.5393) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 20.0000(20.6778) | Bit/dim 3.8763(3.8792) | Xent 2.3026(2.3026) | Loss 3.8763(3.8792) | Error 0.8856(0.8993) Steps 808(803.39) | Grad Norm 7.6300(4.4766) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 20.3634(20.6324) | Bit/dim 3.8790(3.8732) | Xent 2.3026(2.3026) | Loss 3.8790(3.8732) | Error 0.8967(0.8996) Steps 766(798.42) | Grad Norm 4.2795(4.4686) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 21.1589(20.6313) | Bit/dim 3.8691(3.8685) | Xent 2.3026(2.3026) | Loss 3.8691(3.8685) | Error 0.8989(0.9000) Steps 826(798.21) | Grad Norm 2.5825(4.0027) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 21.1758(20.6133) | Bit/dim 3.8506(3.8614) | Xent 2.3026(2.3026) | Loss 3.8506(3.8614) | Error 0.9078(0.9007) Steps 820(797.31) | Grad Norm 5.2809(3.7172) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 99.1241, Epoch Time 1248.2468(1014.9075), Bit/dim 3.8548(best: 3.8694), Xent 2.3026, Loss 3.8548, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 20.9977(20.6229) | Bit/dim 3.8600(3.8573) | Xent 2.3026(2.3026) | Loss 3.8600(3.8573) | Error 0.9044(0.9002) Steps 808(797.34) | Grad Norm 5.1365(4.3207) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 20.9640(20.6855) | Bit/dim 3.8540(3.8535) | Xent 2.3026(2.3026) | Loss 3.8540(3.8535) | Error 0.8967(0.9008) Steps 796(797.20) | Grad Norm 2.5234(3.9753) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 20.4183(20.6301) | Bit/dim 3.8438(3.8473) | Xent 2.3026(2.3026) | Loss 3.8438(3.8473) | Error 0.8878(0.8996) Steps 784(797.50) | Grad Norm 2.0428(3.4265) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 20.7821(20.6358) | Bit/dim 3.8499(3.8441) | Xent 2.3026(2.3026) | Loss 3.8499(3.8441) | Error 0.9022(0.8995) Steps 796(797.74) | Grad Norm 3.7878(3.9953) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 21.0206(20.7281) | Bit/dim 3.8063(3.8399) | Xent 2.3026(2.3026) | Loss 3.8063(3.8399) | Error 0.8867(0.9000) Steps 832(800.08) | Grad Norm 4.8041(3.9660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 98.6619, Epoch Time 1255.5502(1022.1268), Bit/dim 3.8231(best: 3.8548), Xent 2.3026, Loss 3.8231, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 20.0646(20.6884) | Bit/dim 3.8433(3.8364) | Xent 2.3026(2.3026) | Loss 3.8433(3.8364) | Error 0.8944(0.9002) Steps 790(800.02) | Grad Norm 3.9220(3.9919) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 20.9485(20.7306) | Bit/dim 3.8608(3.8342) | Xent 2.3026(2.3026) | Loss 3.8608(3.8342) | Error 0.8989(0.8998) Steps 790(801.12) | Grad Norm 6.7444(4.2480) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 20.4118(20.7979) | Bit/dim 3.8206(3.8308) | Xent 2.3026(2.3026) | Loss 3.8206(3.8308) | Error 0.9011(0.8999) Steps 808(800.85) | Grad Norm 2.6142(3.9622) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 21.1437(20.8637) | Bit/dim 3.8104(3.8263) | Xent 2.3026(2.3026) | Loss 3.8104(3.8263) | Error 0.9011(0.9013) Steps 832(805.00) | Grad Norm 6.9721(4.4825) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 21.5641(20.7649) | Bit/dim 3.7672(3.8183) | Xent 2.3026(2.3026) | Loss 3.7672(3.8183) | Error 0.8933(0.9014) Steps 802(804.24) | Grad Norm 2.7601(4.1904) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 20.8334(20.7267) | Bit/dim 3.7754(3.8131) | Xent 2.3026(2.3026) | Loss 3.7754(3.8131) | Error 0.9089(0.8999) Steps 796(802.42) | Grad Norm 2.7778(3.7798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 99.5185, Epoch Time 1260.3039(1029.2721), Bit/dim 3.7979(best: 3.8231), Xent 2.3026, Loss 3.7979, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 21.0922(20.7170) | Bit/dim 3.8090(3.8087) | Xent 2.3026(2.3026) | Loss 3.8090(3.8087) | Error 0.9056(0.9000) Steps 814(801.72) | Grad Norm 4.2826(4.0654) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 20.5111(20.7192) | Bit/dim 3.8085(3.8055) | Xent 2.3026(2.3026) | Loss 3.8085(3.8055) | Error 0.9200(0.9002) Steps 820(801.52) | Grad Norm 2.0888(3.7644) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 20.8870(20.7882) | Bit/dim 3.7711(3.8009) | Xent 2.3026(2.3026) | Loss 3.7711(3.8009) | Error 0.9033(0.8996) Steps 820(803.97) | Grad Norm 2.4807(3.9727) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 20.8686(20.7157) | Bit/dim 3.7889(3.7991) | Xent 2.3026(2.3026) | Loss 3.7889(3.7991) | Error 0.9122(0.9000) Steps 802(803.94) | Grad Norm 1.9737(4.0308) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 20.6179(20.6873) | Bit/dim 3.7917(3.7962) | Xent 2.3026(2.3026) | Loss 3.7917(3.7962) | Error 0.9233(0.9007) Steps 820(804.77) | Grad Norm 2.9225(3.9464) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 99.5190, Epoch Time 1256.8644(1036.0999), Bit/dim 3.7884(best: 3.7979), Xent 2.3026, Loss 3.7884, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 20.3558(20.6781) | Bit/dim 3.8007(3.7950) | Xent 2.3026(2.3026) | Loss 3.8007(3.7950) | Error 0.8967(0.9006) Steps 802(804.54) | Grad Norm 5.9137(4.1780) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 20.2044(20.6526) | Bit/dim 3.7921(3.7928) | Xent 2.3026(2.3026) | Loss 3.7921(3.7928) | Error 0.8989(0.9009) Steps 796(805.34) | Grad Norm 3.9777(3.9064) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 20.5761(20.6451) | Bit/dim 3.7838(3.7871) | Xent 2.3026(2.3026) | Loss 3.7838(3.7871) | Error 0.8922(0.8998) Steps 814(806.17) | Grad Norm 2.5536(3.9946) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 20.8219(20.6187) | Bit/dim 3.7733(3.7873) | Xent 2.3026(2.3026) | Loss 3.7733(3.7873) | Error 0.9022(0.8993) Steps 814(806.49) | Grad Norm 5.3045(4.1495) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 20.7043(20.6117) | Bit/dim 3.7858(3.7827) | Xent 2.3026(2.3026) | Loss 3.7858(3.7827) | Error 0.8944(0.9003) Steps 796(806.08) | Grad Norm 8.2461(4.1728) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 21.2693(20.6087) | Bit/dim 3.7448(3.7770) | Xent 2.3026(2.3026) | Loss 3.7448(3.7770) | Error 0.9022(0.8999) Steps 814(806.33) | Grad Norm 3.3841(4.0119) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 99.1867, Epoch Time 1249.8274(1042.5117), Bit/dim 3.7672(best: 3.7884), Xent 2.3026, Loss 3.7672, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 21.2736(20.6179) | Bit/dim 3.7944(3.7744) | Xent 2.3026(2.3026) | Loss 3.7944(3.7744) | Error 0.9144(0.9009) Steps 802(805.89) | Grad Norm 2.7428(3.5364) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 21.1450(20.6664) | Bit/dim 3.7746(3.7730) | Xent 2.3026(2.3026) | Loss 3.7746(3.7730) | Error 0.8933(0.8986) Steps 820(808.75) | Grad Norm 3.7443(3.9146) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 20.7686(20.6884) | Bit/dim 3.7430(3.7690) | Xent 2.3026(2.3026) | Loss 3.7430(3.7690) | Error 0.9011(0.8993) Steps 802(808.02) | Grad Norm 3.0629(4.0653) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 20.9200(20.6641) | Bit/dim 3.7797(3.7657) | Xent 2.3026(2.3026) | Loss 3.7797(3.7657) | Error 0.9000(0.9006) Steps 814(808.77) | Grad Norm 6.9007(4.1764) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 20.5002(20.6634) | Bit/dim 3.7366(3.7625) | Xent 2.3026(2.3026) | Loss 3.7366(3.7625) | Error 0.8933(0.8998) Steps 814(809.22) | Grad Norm 2.1918(4.0293) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 100.6466, Epoch Time 1258.4335(1048.9893), Bit/dim 3.7496(best: 3.7672), Xent 2.3026, Loss 3.7496, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 21.4812(20.7118) | Bit/dim 3.7650(3.7597) | Xent 2.3026(2.3026) | Loss 3.7650(3.7597) | Error 0.9089(0.8993) Steps 826(810.65) | Grad Norm 7.9514(3.9944) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 21.0524(20.7049) | Bit/dim 3.7757(3.7589) | Xent 2.3026(2.3026) | Loss 3.7757(3.7589) | Error 0.9156(0.8997) Steps 814(812.07) | Grad Norm 1.7856(3.9415) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 20.8082(20.6500) | Bit/dim 3.7712(3.7558) | Xent 2.3026(2.3026) | Loss 3.7712(3.7558) | Error 0.9156(0.8991) Steps 808(812.32) | Grad Norm 5.1025(3.9807) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 20.1134(20.6440) | Bit/dim 3.7578(3.7534) | Xent 2.3026(2.3026) | Loss 3.7578(3.7534) | Error 0.8878(0.8982) Steps 802(810.96) | Grad Norm 6.3292(4.1459) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 20.6138(20.6140) | Bit/dim 3.7694(3.7536) | Xent 2.3026(2.3026) | Loss 3.7694(3.7536) | Error 0.9156(0.8991) Steps 808(810.83) | Grad Norm 3.8439(4.3515) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 20.9816(20.5899) | Bit/dim 3.7310(3.7503) | Xent 2.3026(2.3026) | Loss 3.7310(3.7503) | Error 0.8956(0.9009) Steps 826(810.32) | Grad Norm 6.4389(4.1861) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 98.9949, Epoch Time 1248.9544(1054.9883), Bit/dim 3.7502(best: 3.7496), Xent 2.3026, Loss 3.7502, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 20.4693(20.6130) | Bit/dim 3.7427(3.7479) | Xent 2.3026(2.3026) | Loss 3.7427(3.7479) | Error 0.9022(0.9013) Steps 820(811.11) | Grad Norm 3.4945(4.1784) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 20.3338(20.6066) | Bit/dim 3.7457(3.7482) | Xent 2.3026(2.3026) | Loss 3.7457(3.7482) | Error 0.9078(0.9022) Steps 808(812.60) | Grad Norm 2.3207(3.7581) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 20.9133(20.5875) | Bit/dim 3.7364(3.7459) | Xent 2.3026(2.3026) | Loss 3.7364(3.7459) | Error 0.9044(0.9022) Steps 796(813.19) | Grad Norm 5.0819(4.0308) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 20.8600(20.5783) | Bit/dim 3.6896(3.7411) | Xent 2.3026(2.3026) | Loss 3.6896(3.7411) | Error 0.8844(0.9012) Steps 820(812.98) | Grad Norm 2.6636(4.1609) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 20.4306(20.5168) | Bit/dim 3.7226(3.7353) | Xent 2.3026(2.3026) | Loss 3.7226(3.7353) | Error 0.8933(0.8986) Steps 802(811.29) | Grad Norm 2.1291(4.1947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 99.5406, Epoch Time 1246.9918(1060.7484), Bit/dim 3.7269(best: 3.7496), Xent 2.3026, Loss 3.7269, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 21.0891(20.5598) | Bit/dim 3.7527(3.7353) | Xent 2.3026(2.3026) | Loss 3.7527(3.7353) | Error 0.9167(0.8997) Steps 808(810.37) | Grad Norm 6.6030(4.1453) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 20.8411(20.5486) | Bit/dim 3.7096(3.7322) | Xent 2.3026(2.3026) | Loss 3.7096(3.7322) | Error 0.9000(0.8987) Steps 814(810.24) | Grad Norm 5.1127(4.2153) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 20.5533(20.5934) | Bit/dim 3.7606(3.7315) | Xent 2.3026(2.3026) | Loss 3.7606(3.7315) | Error 0.9033(0.8995) Steps 802(810.51) | Grad Norm 5.3200(4.1132) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 20.4172(20.5786) | Bit/dim 3.7196(3.7285) | Xent 2.3026(2.3026) | Loss 3.7196(3.7285) | Error 0.9011(0.9001) Steps 808(810.49) | Grad Norm 6.8950(4.0154) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 20.1435(20.5425) | Bit/dim 3.7347(3.7275) | Xent 2.3026(2.3026) | Loss 3.7347(3.7275) | Error 0.8989(0.9007) Steps 802(811.24) | Grad Norm 4.1276(4.0174) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 19.9896(20.5414) | Bit/dim 3.6915(3.7222) | Xent 2.3026(2.3026) | Loss 3.6915(3.7222) | Error 0.9156(0.8996) Steps 814(812.09) | Grad Norm 4.5947(3.7582) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 99.4734, Epoch Time 1250.0693(1066.4280), Bit/dim 3.7228(best: 3.7269), Xent 2.3026, Loss 3.7228, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 20.0912(20.5675) | Bit/dim 3.7158(3.7205) | Xent 2.3026(2.3026) | Loss 3.7158(3.7205) | Error 0.9156(0.9003) Steps 814(812.77) | Grad Norm 5.3843(4.0553) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 20.2256(20.5481) | Bit/dim 3.6984(3.7163) | Xent 2.3026(2.3026) | Loss 3.6984(3.7163) | Error 0.8944(0.9002) Steps 808(813.25) | Grad Norm 2.0140(3.5721) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 20.0047(20.5517) | Bit/dim 3.6994(3.7138) | Xent 2.3026(2.3026) | Loss 3.6994(3.7138) | Error 0.8878(0.8992) Steps 820(814.60) | Grad Norm 1.4214(3.7678) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 20.8127(20.5743) | Bit/dim 3.6950(3.7141) | Xent 2.3026(2.3026) | Loss 3.6950(3.7141) | Error 0.8933(0.8990) Steps 820(814.06) | Grad Norm 3.2679(3.9724) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 20.4045(20.5472) | Bit/dim 3.7311(3.7175) | Xent 2.3026(2.3026) | Loss 3.7311(3.7175) | Error 0.8978(0.9003) Steps 802(811.69) | Grad Norm 4.8834(4.1282) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 99.1201, Epoch Time 1249.1239(1071.9089), Bit/dim 3.7077(best: 3.7228), Xent 2.3026, Loss 3.7077, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 20.3302(20.5144) | Bit/dim 3.7236(3.7145) | Xent 2.3026(2.3026) | Loss 3.7236(3.7145) | Error 0.9078(0.8998) Steps 796(809.66) | Grad Norm 1.7548(3.8031) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 20.3300(20.5001) | Bit/dim 3.7126(3.7110) | Xent 2.3026(2.3026) | Loss 3.7126(3.7110) | Error 0.9211(0.9017) Steps 808(810.47) | Grad Norm 4.5178(3.4059) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 20.1542(20.4621) | Bit/dim 3.6999(3.7093) | Xent 2.3026(2.3026) | Loss 3.6999(3.7093) | Error 0.9011(0.9008) Steps 814(810.93) | Grad Norm 4.5623(3.5899) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 21.1156(20.5611) | Bit/dim 3.7050(3.7089) | Xent 2.3026(2.3026) | Loss 3.7050(3.7089) | Error 0.9044(0.9008) Steps 820(812.92) | Grad Norm 6.9061(3.9094) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 21.0494(20.5589) | Bit/dim 3.7053(3.7059) | Xent 2.3026(2.3026) | Loss 3.7053(3.7059) | Error 0.8833(0.8996) Steps 814(813.51) | Grad Norm 4.2324(3.8954) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 19.9647(20.5550) | Bit/dim 3.6773(3.7030) | Xent 2.3026(2.3026) | Loss 3.6773(3.7030) | Error 0.8956(0.8988) Steps 826(814.83) | Grad Norm 1.6106(3.8982) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 100.5148, Epoch Time 1247.2676(1077.1697), Bit/dim 3.6948(best: 3.7077), Xent 2.3026, Loss 3.6948, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 21.5532(20.6403) | Bit/dim 3.7239(3.7002) | Xent 2.3026(2.3026) | Loss 3.7239(3.7002) | Error 0.9000(0.8978) Steps 814(814.95) | Grad Norm 3.2307(3.8705) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 20.7181(20.6694) | Bit/dim 3.6955(3.6991) | Xent 2.3026(2.3026) | Loss 3.6955(3.6991) | Error 0.9122(0.8997) Steps 826(815.33) | Grad Norm 1.7054(3.5654) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 21.4119(20.6972) | Bit/dim 3.6931(3.6972) | Xent 2.3026(2.3026) | Loss 3.6931(3.6972) | Error 0.9022(0.8995) Steps 814(816.79) | Grad Norm 3.2498(3.8044) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 20.2047(20.6319) | Bit/dim 3.6750(3.6939) | Xent 2.3026(2.3026) | Loss 3.6750(3.6939) | Error 0.8978(0.8994) Steps 820(817.38) | Grad Norm 2.9039(3.7981) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 20.3140(20.6093) | Bit/dim 3.7179(3.6950) | Xent 2.3026(2.3026) | Loss 3.7179(3.6950) | Error 0.9067(0.9004) Steps 826(817.32) | Grad Norm 6.5301(3.8170) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 99.6519, Epoch Time 1255.9367(1082.5327), Bit/dim 3.6941(best: 3.6948), Xent 2.3026, Loss 3.6941, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 20.8761(20.6588) | Bit/dim 3.7190(3.6946) | Xent 2.3026(2.3026) | Loss 3.7190(3.6946) | Error 0.9011(0.9001) Steps 826(817.59) | Grad Norm 4.2702(4.0681) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 20.6125(20.7211) | Bit/dim 3.7105(3.6932) | Xent 2.3026(2.3026) | Loss 3.7105(3.6932) | Error 0.9033(0.8995) Steps 814(818.60) | Grad Norm 5.0120(4.2251) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 19.8015(20.7143) | Bit/dim 3.7045(3.6939) | Xent 2.3026(2.3026) | Loss 3.7045(3.6939) | Error 0.8933(0.9001) Steps 814(815.97) | Grad Norm 2.2212(4.0291) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 20.6378(20.6354) | Bit/dim 3.6955(3.6922) | Xent 2.3026(2.3026) | Loss 3.6955(3.6922) | Error 0.9022(0.9001) Steps 802(815.74) | Grad Norm 1.7080(3.6332) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 21.0091(20.6319) | Bit/dim 3.6594(3.6874) | Xent 2.3026(2.3026) | Loss 3.6594(3.6874) | Error 0.9056(0.8998) Steps 826(815.99) | Grad Norm 1.3629(3.6203) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 20.7979(20.7022) | Bit/dim 3.7117(3.6850) | Xent 2.3026(2.3026) | Loss 3.7117(3.6850) | Error 0.8978(0.9002) Steps 814(817.09) | Grad Norm 1.6561(3.6363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 100.2564, Epoch Time 1257.1089(1087.7700), Bit/dim 3.6788(best: 3.6941), Xent 2.3026, Loss 3.6788, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 20.1208(20.7034) | Bit/dim 3.6472(3.6825) | Xent 2.3026(2.3026) | Loss 3.6472(3.6825) | Error 0.8967(0.9001) Steps 814(818.37) | Grad Norm 3.1315(3.5411) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 20.1408(20.7257) | Bit/dim 3.6894(3.6803) | Xent 2.3026(2.3026) | Loss 3.6894(3.6803) | Error 0.9189(0.9006) Steps 826(819.05) | Grad Norm 5.5869(3.4983) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 20.0574(20.7399) | Bit/dim 3.6860(3.6766) | Xent 2.3026(2.3026) | Loss 3.6860(3.6766) | Error 0.9233(0.9008) Steps 826(820.06) | Grad Norm 5.6115(3.6429) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 20.5483(20.7623) | Bit/dim 3.6494(3.6760) | Xent 2.3026(2.3026) | Loss 3.6494(3.6760) | Error 0.8811(0.8999) Steps 832(821.02) | Grad Norm 5.5422(3.8895) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 21.0218(20.7368) | Bit/dim 3.6605(3.6767) | Xent 2.3026(2.3026) | Loss 3.6605(3.6767) | Error 0.8967(0.9001) Steps 808(819.85) | Grad Norm 4.2274(4.0112) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 100.0199, Epoch Time 1259.8753(1092.9331), Bit/dim 3.6706(best: 3.6788), Xent 2.3026, Loss 3.6706, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 20.5087(20.7148) | Bit/dim 3.6711(3.6765) | Xent 2.3026(2.3026) | Loss 3.6711(3.6765) | Error 0.9000(0.8986) Steps 814(819.91) | Grad Norm 1.4574(3.5648) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 20.5271(20.7860) | Bit/dim 3.7018(3.6763) | Xent 2.3026(2.3026) | Loss 3.7018(3.6763) | Error 0.9044(0.8989) Steps 808(818.80) | Grad Norm 4.4787(3.6495) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 21.0655(20.7530) | Bit/dim 3.6919(3.6757) | Xent 2.3026(2.3026) | Loss 3.6919(3.6757) | Error 0.9122(0.8992) Steps 802(817.90) | Grad Norm 2.6547(3.5488) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 20.6320(20.7277) | Bit/dim 3.6350(3.6719) | Xent 2.3026(2.3026) | Loss 3.6350(3.6719) | Error 0.8956(0.9000) Steps 814(816.86) | Grad Norm 3.4984(3.3814) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 20.6472(20.7379) | Bit/dim 3.6557(3.6704) | Xent 2.3026(2.3026) | Loss 3.6557(3.6704) | Error 0.8867(0.9004) Steps 808(815.96) | Grad Norm 3.0560(3.5613) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 20.3029(20.6975) | Bit/dim 3.6557(3.6678) | Xent 2.3026(2.3026) | Loss 3.6557(3.6678) | Error 0.9089(0.9004) Steps 814(815.30) | Grad Norm 3.7659(3.6730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 100.2102, Epoch Time 1257.6673(1097.8752), Bit/dim 3.6668(best: 3.6706), Xent 2.3026, Loss 3.6668, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 20.3703(20.6914) | Bit/dim 3.6865(3.6694) | Xent 2.3026(2.3026) | Loss 3.6865(3.6694) | Error 0.8800(0.9003) Steps 808(815.97) | Grad Norm 1.6227(3.9636) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 20.3035(20.6523) | Bit/dim 3.6823(3.6692) | Xent 2.3026(2.3026) | Loss 3.6823(3.6692) | Error 0.9144(0.9016) Steps 820(815.60) | Grad Norm 2.6085(3.5550) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 20.4394(20.6123) | Bit/dim 3.6981(3.6690) | Xent 2.3026(2.3026) | Loss 3.6981(3.6690) | Error 0.8867(0.9006) Steps 814(816.11) | Grad Norm 2.9105(3.4147) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 20.6834(20.6156) | Bit/dim 3.6522(3.6644) | Xent 2.3026(2.3026) | Loss 3.6522(3.6644) | Error 0.8856(0.8996) Steps 814(814.95) | Grad Norm 2.1141(3.5701) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 20.8447(20.5461) | Bit/dim 3.6617(3.6615) | Xent 2.3026(2.3026) | Loss 3.6617(3.6615) | Error 0.8967(0.8995) Steps 832(814.38) | Grad Norm 2.2558(3.8623) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 100.0965, Epoch Time 1247.3528(1102.3595), Bit/dim 3.6556(best: 3.6668), Xent 2.3026, Loss 3.6556, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 20.3233(20.5207) | Bit/dim 3.6592(3.6588) | Xent 2.3026(2.3026) | Loss 3.6592(3.6588) | Error 0.9033(0.8996) Steps 802(814.56) | Grad Norm 2.6711(3.5988) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 20.7502(20.4585) | Bit/dim 3.6708(3.6580) | Xent 2.3026(2.3026) | Loss 3.6708(3.6580) | Error 0.9033(0.8997) Steps 838(814.55) | Grad Norm 3.4711(3.4979) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 20.8313(20.4690) | Bit/dim 3.6459(3.6585) | Xent 2.3026(2.3026) | Loss 3.6459(3.6585) | Error 0.8967(0.8988) Steps 802(815.11) | Grad Norm 4.2626(3.7450) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 21.3825(20.4590) | Bit/dim 3.6484(3.6546) | Xent 2.3026(2.3026) | Loss 3.6484(3.6546) | Error 0.8889(0.8988) Steps 832(813.70) | Grad Norm 3.4395(3.5068) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 20.6320(20.5309) | Bit/dim 3.6400(3.6550) | Xent 2.3026(2.3026) | Loss 3.6400(3.6550) | Error 0.8989(0.9002) Steps 802(811.46) | Grad Norm 6.4873(3.7701) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 21.0821(20.5658) | Bit/dim 3.6320(3.6536) | Xent 2.3026(2.3026) | Loss 3.6320(3.6536) | Error 0.8700(0.8997) Steps 790(810.32) | Grad Norm 3.4176(3.6965) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 98.4599, Epoch Time 1244.4045(1106.6208), Bit/dim 3.6502(best: 3.6556), Xent 2.3026, Loss 3.6502, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 20.0723(20.5643) | Bit/dim 3.6241(3.6509) | Xent 2.3026(2.3026) | Loss 3.6241(3.6509) | Error 0.9122(0.8997) Steps 808(810.09) | Grad Norm 2.2927(3.3544) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 20.9517(20.6166) | Bit/dim 3.6513(3.6548) | Xent 2.3026(2.3026) | Loss 3.6513(3.6548) | Error 0.8956(0.9000) Steps 826(810.25) | Grad Norm 3.6573(3.7038) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 20.3912(20.5897) | Bit/dim 3.6338(3.6525) | Xent 2.3026(2.3026) | Loss 3.6338(3.6525) | Error 0.8933(0.9007) Steps 814(809.83) | Grad Norm 4.5567(3.7007) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 21.0068(20.5939) | Bit/dim 3.6443(3.6496) | Xent 2.3026(2.3026) | Loss 3.6443(3.6496) | Error 0.8856(0.9006) Steps 796(809.43) | Grad Norm 2.4199(3.4968) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 20.6994(20.6271) | Bit/dim 3.6727(3.6478) | Xent 2.3026(2.3026) | Loss 3.6727(3.6478) | Error 0.8889(0.8999) Steps 826(810.83) | Grad Norm 5.2472(3.3326) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 98.4880, Epoch Time 1250.3500(1110.9327), Bit/dim 3.6512(best: 3.6502), Xent 2.3026, Loss 3.6512, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 20.4327(20.5502) | Bit/dim 3.6197(3.6463) | Xent 2.3026(2.3026) | Loss 3.6197(3.6463) | Error 0.8978(0.8999) Steps 808(811.11) | Grad Norm 4.1954(3.7585) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 19.6312(20.5457) | Bit/dim 3.6753(3.6458) | Xent 2.3026(2.3026) | Loss 3.6753(3.6458) | Error 0.8967(0.8991) Steps 808(811.12) | Grad Norm 3.7847(3.5592) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 20.5136(20.5001) | Bit/dim 3.6485(3.6460) | Xent 2.3026(2.3026) | Loss 3.6485(3.6460) | Error 0.9156(0.8996) Steps 814(810.14) | Grad Norm 2.5805(3.8135) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 20.4215(20.5053) | Bit/dim 3.6355(3.6432) | Xent 2.3026(2.3026) | Loss 3.6355(3.6432) | Error 0.9067(0.8998) Steps 814(810.26) | Grad Norm 4.2044(3.8193) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 20.3251(20.5019) | Bit/dim 3.6461(3.6425) | Xent 2.3026(2.3026) | Loss 3.6461(3.6425) | Error 0.8778(0.8985) Steps 814(809.43) | Grad Norm 2.7012(3.4518) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 20.5117(20.5091) | Bit/dim 3.6708(3.6434) | Xent 2.3026(2.3026) | Loss 3.6708(3.6434) | Error 0.9033(0.9003) Steps 820(809.33) | Grad Norm 3.4115(3.6880) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 98.8460, Epoch Time 1243.1459(1114.8991), Bit/dim 3.6465(best: 3.6502), Xent 2.3026, Loss 3.6465, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 21.3542(20.5719) | Bit/dim 3.6185(3.6417) | Xent 2.3026(2.3026) | Loss 3.6185(3.6417) | Error 0.8956(0.8990) Steps 814(809.21) | Grad Norm 2.7201(3.6898) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 19.7364(20.5976) | Bit/dim 3.5996(3.6384) | Xent 2.3026(2.3026) | Loss 3.5996(3.6384) | Error 0.8833(0.8987) Steps 808(808.74) | Grad Norm 4.5508(3.6530) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 20.8759(20.6758) | Bit/dim 3.6355(3.6373) | Xent 2.3026(2.3026) | Loss 3.6355(3.6373) | Error 0.9033(0.8991) Steps 814(808.77) | Grad Norm 3.4935(3.6560) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 20.5935(20.6937) | Bit/dim 3.6298(3.6379) | Xent 2.3026(2.3026) | Loss 3.6298(3.6379) | Error 0.9022(0.9006) Steps 820(809.73) | Grad Norm 4.1113(3.5806) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 20.8378(20.6760) | Bit/dim 3.6329(3.6378) | Xent 2.3026(2.3026) | Loss 3.6329(3.6378) | Error 0.9000(0.9009) Steps 808(810.59) | Grad Norm 2.0302(3.4676) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 97.7614, Epoch Time 1256.4579(1119.1459), Bit/dim 3.6385(best: 3.6465), Xent 2.3026, Loss 3.6385, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 19.9487(20.6535) | Bit/dim 3.6130(3.6386) | Xent 2.3026(2.3026) | Loss 3.6130(3.6386) | Error 0.8844(0.9004) Steps 802(809.83) | Grad Norm 3.8702(3.5941) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 20.6529(20.6487) | Bit/dim 3.6177(3.6345) | Xent 2.3026(2.3026) | Loss 3.6177(3.6345) | Error 0.8967(0.9008) Steps 808(810.62) | Grad Norm 1.6182(3.5250) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 20.6788(20.6771) | Bit/dim 3.6168(3.6352) | Xent 2.3026(2.3026) | Loss 3.6168(3.6352) | Error 0.9033(0.9004) Steps 802(810.53) | Grad Norm 1.8325(3.2538) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 20.5183(20.6579) | Bit/dim 3.6302(3.6334) | Xent 2.3026(2.3026) | Loss 3.6302(3.6334) | Error 0.8944(0.9003) Steps 796(810.69) | Grad Norm 5.0609(3.4954) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 20.4501(20.6496) | Bit/dim 3.6151(3.6332) | Xent 2.3026(2.3026) | Loss 3.6151(3.6332) | Error 0.9133(0.8999) Steps 814(810.80) | Grad Norm 4.2576(3.5367) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 20.9829(20.6228) | Bit/dim 3.6242(3.6296) | Xent 2.3026(2.3026) | Loss 3.6242(3.6296) | Error 0.9122(0.9002) Steps 808(811.37) | Grad Norm 1.8932(3.3882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 98.5958, Epoch Time 1250.3150(1123.0809), Bit/dim 3.6271(best: 3.6385), Xent 2.3026, Loss 3.6271, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 20.1954(20.6647) | Bit/dim 3.6534(3.6322) | Xent 2.3026(2.3026) | Loss 3.6534(3.6322) | Error 0.9033(0.9018) Steps 814(811.29) | Grad Norm 2.5867(3.4264) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 20.1123(20.6287) | Bit/dim 3.6219(3.6309) | Xent 2.3026(2.3026) | Loss 3.6219(3.6309) | Error 0.8878(0.9008) Steps 802(810.54) | Grad Norm 2.9766(3.4236) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 21.5160(20.6403) | Bit/dim 3.6764(3.6294) | Xent 2.3026(2.3026) | Loss 3.6764(3.6294) | Error 0.9044(0.9003) Steps 826(810.86) | Grad Norm 6.2887(3.5810) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 20.4552(20.6920) | Bit/dim 3.5986(3.6280) | Xent 2.3026(2.3026) | Loss 3.5986(3.6280) | Error 0.8967(0.8995) Steps 814(811.08) | Grad Norm 2.9756(3.6255) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 21.0001(20.7766) | Bit/dim 3.6003(3.6261) | Xent 2.3026(2.3026) | Loss 3.6003(3.6261) | Error 0.8967(0.8994) Steps 808(811.59) | Grad Norm 2.4278(3.2280) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 99.6140, Epoch Time 1259.1476(1127.1629), Bit/dim 3.6252(best: 3.6271), Xent 2.3026, Loss 3.6252, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 19.7927(20.6460) | Bit/dim 3.6278(3.6236) | Xent 2.3026(2.3026) | Loss 3.6278(3.6236) | Error 0.9178(0.9006) Steps 802(810.18) | Grad Norm 3.3125(3.3028) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 20.9828(20.6603) | Bit/dim 3.6477(3.6235) | Xent 2.3026(2.3026) | Loss 3.6477(3.6235) | Error 0.9122(0.9001) Steps 802(808.99) | Grad Norm 4.3741(3.3795) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 21.0101(20.7225) | Bit/dim 3.6271(3.6230) | Xent 2.3026(2.3026) | Loss 3.6271(3.6230) | Error 0.9144(0.9007) Steps 820(809.95) | Grad Norm 2.4856(3.3660) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 20.3451(20.6901) | Bit/dim 3.6128(3.6192) | Xent 2.3026(2.3026) | Loss 3.6128(3.6192) | Error 0.8911(0.8999) Steps 790(809.72) | Grad Norm 1.9360(3.3737) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 20.3489(20.6904) | Bit/dim 3.6088(3.6211) | Xent 2.3026(2.3026) | Loss 3.6088(3.6211) | Error 0.8856(0.8993) Steps 814(810.84) | Grad Norm 2.5761(3.3548) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 20.7711(20.6863) | Bit/dim 3.6030(3.6200) | Xent 2.3026(2.3026) | Loss 3.6030(3.6200) | Error 0.9222(0.8998) Steps 814(811.41) | Grad Norm 2.9321(3.4565) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 20.9739(20.6208) | Bit/dim 3.6190(3.6147) | Xent 2.3026(2.3026) | Loss 3.6190(3.6147) | Error 0.9044(0.8995) Steps 808(810.04) | Grad Norm 3.7732(3.4215) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 20.6752(20.6514) | Bit/dim 3.6260(3.6181) | Xent 2.3026(2.3026) | Loss 3.6260(3.6181) | Error 0.8989(0.8995) Steps 808(809.69) | Grad Norm 4.6428(3.3973) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 20.7904(20.6759) | Bit/dim 3.6145(3.6167) | Xent 2.3026(2.3026) | Loss 3.6145(3.6167) | Error 0.9056(0.8998) Steps 802(808.57) | Grad Norm 2.9598(3.2320) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 20.7263(20.6254) | Bit/dim 3.6166(3.6150) | Xent 2.3026(2.3026) | Loss 3.6166(3.6150) | Error 0.8867(0.8996) Steps 790(807.73) | Grad Norm 3.4447(3.3084) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 20.6626(20.6076) | Bit/dim 3.6122(3.6142) | Xent 2.3026(2.3026) | Loss 3.6122(3.6142) | Error 0.9178(0.8998) Steps 814(807.63) | Grad Norm 2.0075(3.0914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 98.6508, Epoch Time 1250.6972(1134.4994), Bit/dim 3.6146(best: 3.6235), Xent 2.3026, Loss 3.6146, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 21.0325(20.5869) | Bit/dim 3.5863(3.6128) | Xent 2.3026(2.3026) | Loss 3.5863(3.6128) | Error 0.9011(0.9006) Steps 808(808.17) | Grad Norm 4.0138(3.3164) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 21.3750(20.6155) | Bit/dim 3.5978(3.6125) | Xent 2.3026(2.3026) | Loss 3.5978(3.6125) | Error 0.8889(0.8993) Steps 808(808.56) | Grad Norm 3.4296(3.1202) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 19.9193(20.5905) | Bit/dim 3.6386(3.6137) | Xent 2.3026(2.3026) | Loss 3.6386(3.6137) | Error 0.8878(0.9001) Steps 814(808.28) | Grad Norm 6.4992(3.3877) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 20.5531(20.5807) | Bit/dim 3.6022(3.6135) | Xent 2.3026(2.3026) | Loss 3.6022(3.6135) | Error 0.9033(0.8998) Steps 808(808.66) | Grad Norm 2.7706(3.4773) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 21.0510(20.5866) | Bit/dim 3.5907(3.6106) | Xent 2.3026(2.3026) | Loss 3.5907(3.6106) | Error 0.8978(0.8992) Steps 820(807.94) | Grad Norm 2.1018(3.2126) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 20.4663(20.6314) | Bit/dim 3.5958(3.6100) | Xent 2.3026(2.3026) | Loss 3.5958(3.6100) | Error 0.9111(0.9006) Steps 826(810.42) | Grad Norm 2.9689(2.9876) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 100.0321, Epoch Time 1252.8495(1138.0499), Bit/dim 3.6088(best: 3.6146), Xent 2.3026, Loss 3.6088, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 20.9646(20.6690) | Bit/dim 3.6055(3.6098) | Xent 2.3026(2.3026) | Loss 3.6055(3.6098) | Error 0.9000(0.9006) Steps 820(811.37) | Grad Norm 4.7002(3.3514) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 21.0543(20.7027) | Bit/dim 3.6524(3.6110) | Xent 2.3026(2.3026) | Loss 3.6524(3.6110) | Error 0.9156(0.9006) Steps 796(810.32) | Grad Norm 2.4398(3.4137) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 20.0707(20.6780) | Bit/dim 3.6232(3.6083) | Xent 2.3026(2.3026) | Loss 3.6232(3.6083) | Error 0.9211(0.9008) Steps 796(809.80) | Grad Norm 1.6804(3.1158) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 20.1736(20.7395) | Bit/dim 3.6113(3.6072) | Xent 2.3026(2.3026) | Loss 3.6113(3.6072) | Error 0.8933(0.8994) Steps 820(809.53) | Grad Norm 3.5264(3.3583) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 20.1654(20.7281) | Bit/dim 3.6063(3.6063) | Xent 2.3026(2.3026) | Loss 3.6063(3.6063) | Error 0.9022(0.9002) Steps 808(809.31) | Grad Norm 2.8439(3.3928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 100.6025, Epoch Time 1258.8128(1141.6728), Bit/dim 3.6037(best: 3.6088), Xent 2.3026, Loss 3.6037, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 20.2065(20.6797) | Bit/dim 3.6086(3.6040) | Xent 2.3026(2.3026) | Loss 3.6086(3.6040) | Error 0.9044(0.9002) Steps 808(809.74) | Grad Norm 4.0954(3.2998) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 20.0392(20.7543) | Bit/dim 3.5550(3.6023) | Xent 2.3026(2.3026) | Loss 3.5550(3.6023) | Error 0.9067(0.8994) Steps 802(808.10) | Grad Norm 4.5462(3.5467) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 20.1598(20.6280) | Bit/dim 3.5805(3.6016) | Xent 2.3026(2.3026) | Loss 3.5805(3.6016) | Error 0.9089(0.8993) Steps 784(806.36) | Grad Norm 2.1843(3.4073) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 21.0261(20.5668) | Bit/dim 3.5890(3.6026) | Xent 2.3026(2.3026) | Loss 3.5890(3.6026) | Error 0.9067(0.8999) Steps 784(803.87) | Grad Norm 3.4836(3.0700) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 21.0011(20.5849) | Bit/dim 3.6096(3.6055) | Xent 2.3026(2.3026) | Loss 3.6096(3.6055) | Error 0.8856(0.8998) Steps 808(805.09) | Grad Norm 3.0573(3.1732) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 20.3518(20.6749) | Bit/dim 3.6140(3.6042) | Xent 2.3026(2.3026) | Loss 3.6140(3.6042) | Error 0.9022(0.9002) Steps 820(807.38) | Grad Norm 3.6947(3.3169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 101.0380, Epoch Time 1255.5005(1145.0876), Bit/dim 3.5969(best: 3.6037), Xent 2.3026, Loss 3.5969, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 20.5565(20.7862) | Bit/dim 3.5954(3.6034) | Xent 2.3026(2.3026) | Loss 3.5954(3.6034) | Error 0.8978(0.8999) Steps 814(808.85) | Grad Norm 3.8935(3.4547) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 20.6940(20.8043) | Bit/dim 3.5796(3.6022) | Xent 2.3026(2.3026) | Loss 3.5796(3.6022) | Error 0.9211(0.9009) Steps 808(808.96) | Grad Norm 2.6014(3.1549) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 20.4547(20.7765) | Bit/dim 3.6289(3.5972) | Xent 2.3026(2.3026) | Loss 3.6289(3.5972) | Error 0.9122(0.9002) Steps 802(809.64) | Grad Norm 5.2611(3.1291) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 20.8887(20.7725) | Bit/dim 3.6055(3.5984) | Xent 2.3026(2.3026) | Loss 3.6055(3.5984) | Error 0.8956(0.8998) Steps 814(811.74) | Grad Norm 2.8891(3.3275) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 21.5451(20.7392) | Bit/dim 3.5739(3.5978) | Xent 2.3026(2.3026) | Loss 3.5739(3.5978) | Error 0.8867(0.9003) Steps 808(811.62) | Grad Norm 2.3099(3.1046) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 99.7178, Epoch Time 1265.6057(1148.7031), Bit/dim 3.5928(best: 3.5969), Xent 2.3026, Loss 3.5928, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 21.1053(20.8343) | Bit/dim 3.5753(3.5951) | Xent 2.3026(2.3026) | Loss 3.5753(3.5951) | Error 0.8956(0.8998) Steps 814(813.14) | Grad Norm 5.5806(3.0316) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 20.2054(20.8413) | Bit/dim 3.6070(3.5944) | Xent 2.3026(2.3026) | Loss 3.6070(3.5944) | Error 0.8756(0.8993) Steps 796(813.31) | Grad Norm 3.4567(3.1737) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 20.8202(20.8544) | Bit/dim 3.5604(3.5930) | Xent 2.3026(2.3026) | Loss 3.5604(3.5930) | Error 0.9056(0.8987) Steps 814(813.79) | Grad Norm 4.8717(3.0794) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 20.9473(20.8034) | Bit/dim 3.5923(3.5947) | Xent 2.3026(2.3026) | Loss 3.5923(3.5947) | Error 0.8911(0.8990) Steps 814(813.30) | Grad Norm 3.5745(3.2399) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 20.7259(20.8281) | Bit/dim 3.5742(3.5933) | Xent 2.3026(2.3026) | Loss 3.5742(3.5933) | Error 0.8867(0.8990) Steps 814(813.30) | Grad Norm 2.3404(3.0110) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 20.2775(20.7970) | Bit/dim 3.5936(3.5952) | Xent 2.3026(2.3026) | Loss 3.5936(3.5952) | Error 0.9122(0.9012) Steps 826(812.76) | Grad Norm 5.7923(3.2627) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 101.4372, Epoch Time 1263.8094(1152.1563), Bit/dim 3.5942(best: 3.5928), Xent 2.3026, Loss 3.5942, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 20.0475(20.7748) | Bit/dim 3.5937(3.5952) | Xent 2.3026(2.3026) | Loss 3.5937(3.5952) | Error 0.9033(0.9011) Steps 808(812.61) | Grad Norm 2.9273(3.4078) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 20.1911(20.7249) | Bit/dim 3.5950(3.5932) | Xent 2.3026(2.3026) | Loss 3.5950(3.5932) | Error 0.8967(0.9006) Steps 808(812.23) | Grad Norm 2.6604(3.3099) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 21.1915(20.6932) | Bit/dim 3.5792(3.5914) | Xent 2.3026(2.3026) | Loss 3.5792(3.5914) | Error 0.8911(0.9003) Steps 802(811.92) | Grad Norm 3.6740(3.0444) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 21.8493(20.8148) | Bit/dim 3.5809(3.5908) | Xent 2.3026(2.3026) | Loss 3.5809(3.5908) | Error 0.9044(0.9008) Steps 838(814.34) | Grad Norm 2.9846(3.0644) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 20.0421(20.8450) | Bit/dim 3.5979(3.5900) | Xent 2.3026(2.3026) | Loss 3.5979(3.5900) | Error 0.8944(0.8999) Steps 802(813.99) | Grad Norm 4.0403(3.0407) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 101.6094, Epoch Time 1263.6362(1155.5007), Bit/dim 3.5905(best: 3.5928), Xent 2.3026, Loss 3.5905, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 20.5797(20.8292) | Bit/dim 3.5943(3.5872) | Xent 2.3026(2.3026) | Loss 3.5943(3.5872) | Error 0.9100(0.9000) Steps 808(814.42) | Grad Norm 1.7174(3.1414) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 20.9612(20.7987) | Bit/dim 3.5878(3.5888) | Xent 2.3026(2.3026) | Loss 3.5878(3.5888) | Error 0.9056(0.9011) Steps 820(814.83) | Grad Norm 2.9067(3.1419) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 21.2315(20.8326) | Bit/dim 3.5652(3.5885) | Xent 2.3026(2.3026) | Loss 3.5652(3.5885) | Error 0.8978(0.9016) Steps 814(814.08) | Grad Norm 4.1584(3.0193) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 20.2139(20.7488) | Bit/dim 3.5703(3.5895) | Xent 2.3026(2.3026) | Loss 3.5703(3.5895) | Error 0.9056(0.9010) Steps 802(813.79) | Grad Norm 5.9325(3.2297) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 20.6991(20.6989) | Bit/dim 3.5945(3.5859) | Xent 2.3026(2.3026) | Loss 3.5945(3.5859) | Error 0.9056(0.8992) Steps 790(812.19) | Grad Norm 2.0233(3.0951) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 21.3556(20.6856) | Bit/dim 3.5871(3.5841) | Xent 2.3026(2.3026) | Loss 3.5871(3.5841) | Error 0.9033(0.8999) Steps 820(812.86) | Grad Norm 3.1418(2.8063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 101.5370, Epoch Time 1257.2436(1158.5530), Bit/dim 3.5865(best: 3.5905), Xent 2.3026, Loss 3.5865, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 20.5439(20.7542) | Bit/dim 3.5604(3.5826) | Xent 2.3026(2.3026) | Loss 3.5604(3.5826) | Error 0.8878(0.8982) Steps 814(811.31) | Grad Norm 3.6578(3.3475) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 20.6570(20.7902) | Bit/dim 3.5486(3.5812) | Xent 2.3026(2.3026) | Loss 3.5486(3.5812) | Error 0.9011(0.8990) Steps 802(811.55) | Grad Norm 2.3729(3.2854) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 20.6593(20.7500) | Bit/dim 3.5823(3.5811) | Xent 2.3026(2.3026) | Loss 3.5823(3.5811) | Error 0.8989(0.8989) Steps 820(812.02) | Grad Norm 2.7797(3.1314) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 20.8235(20.7290) | Bit/dim 3.6168(3.5848) | Xent 2.3026(2.3026) | Loss 3.6168(3.5848) | Error 0.8989(0.9000) Steps 808(811.63) | Grad Norm 4.4154(2.9806) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 20.2576(20.6842) | Bit/dim 3.5824(3.5830) | Xent 2.3026(2.3026) | Loss 3.5824(3.5830) | Error 0.9033(0.9001) Steps 820(812.83) | Grad Norm 3.6552(3.2063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 101.9865, Epoch Time 1259.5410(1161.5827), Bit/dim 3.5796(best: 3.5865), Xent 2.3026, Loss 3.5796, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 20.6452(20.6516) | Bit/dim 3.6060(3.5831) | Xent 2.3026(2.3026) | Loss 3.6060(3.5831) | Error 0.8900(0.9011) Steps 790(810.19) | Grad Norm 1.6606(3.1444) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 20.5640(20.6483) | Bit/dim 3.5820(3.5821) | Xent 2.3026(2.3026) | Loss 3.5820(3.5821) | Error 0.9311(0.9024) Steps 802(810.56) | Grad Norm 5.7126(3.4039) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 19.6758(20.5208) | Bit/dim 3.6062(3.5823) | Xent 2.3026(2.3026) | Loss 3.6062(3.5823) | Error 0.9033(0.9008) Steps 802(808.45) | Grad Norm 3.0132(3.3126) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 20.6120(20.6182) | Bit/dim 3.5526(3.5794) | Xent 2.3026(2.3026) | Loss 3.5526(3.5794) | Error 0.8989(0.9005) Steps 796(806.86) | Grad Norm 3.0659(3.1324) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 20.2658(20.6007) | Bit/dim 3.5778(3.5777) | Xent 2.3026(2.3026) | Loss 3.5778(3.5777) | Error 0.9089(0.9008) Steps 802(807.04) | Grad Norm 3.8582(3.3055) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 21.3096(20.5354) | Bit/dim 3.5878(3.5775) | Xent 2.3026(2.3026) | Loss 3.5878(3.5775) | Error 0.8967(0.8998) Steps 814(806.94) | Grad Norm 1.8956(3.0998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 101.2386, Epoch Time 1248.9555(1164.2038), Bit/dim 3.5756(best: 3.5796), Xent 2.3026, Loss 3.5756, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 20.6253(20.4916) | Bit/dim 3.5152(3.5760) | Xent 2.3026(2.3026) | Loss 3.5152(3.5760) | Error 0.9022(0.8994) Steps 820(807.67) | Grad Norm 2.2091(3.3009) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 20.2073(20.4611) | Bit/dim 3.5727(3.5754) | Xent 2.3026(2.3026) | Loss 3.5727(3.5754) | Error 0.8978(0.8995) Steps 796(807.00) | Grad Norm 2.6491(3.2402) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 20.5935(20.4664) | Bit/dim 3.5959(3.5766) | Xent 2.3026(2.3026) | Loss 3.5959(3.5766) | Error 0.9078(0.8999) Steps 814(805.85) | Grad Norm 4.7145(3.3135) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 19.9530(20.4433) | Bit/dim 3.6163(3.5777) | Xent 2.3026(2.3026) | Loss 3.6163(3.5777) | Error 0.9089(0.8996) Steps 814(807.17) | Grad Norm 3.2378(3.3103) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 20.2427(20.5570) | Bit/dim 3.5445(3.5740) | Xent 2.3026(2.3026) | Loss 3.5445(3.5740) | Error 0.9044(0.9001) Steps 790(807.40) | Grad Norm 3.0806(3.1083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 102.3191, Epoch Time 1248.1167(1166.7212), Bit/dim 3.5746(best: 3.5756), Xent 2.3026, Loss 3.5746, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 21.6614(20.5239) | Bit/dim 3.5791(3.5766) | Xent 2.3026(2.3026) | Loss 3.5791(3.5766) | Error 0.9033(0.9007) Steps 832(808.83) | Grad Norm 3.6195(3.2238) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 20.3807(20.5252) | Bit/dim 3.5709(3.5783) | Xent 2.3026(2.3026) | Loss 3.5709(3.5783) | Error 0.8911(0.9007) Steps 808(808.86) | Grad Norm 2.5176(3.1278) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 20.9093(20.5943) | Bit/dim 3.5784(3.5750) | Xent 2.3026(2.3026) | Loss 3.5784(3.5750) | Error 0.9222(0.9011) Steps 826(810.46) | Grad Norm 2.4438(2.9838) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 20.0717(20.6333) | Bit/dim 3.5133(3.5723) | Xent 2.3026(2.3026) | Loss 3.5133(3.5723) | Error 0.8922(0.9009) Steps 808(810.37) | Grad Norm 2.1411(3.1240) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 20.0909(20.5866) | Bit/dim 3.5330(3.5707) | Xent 2.3026(2.3026) | Loss 3.5330(3.5707) | Error 0.9178(0.9007) Steps 802(811.05) | Grad Norm 2.6375(3.1192) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 20.0826(20.5303) | Bit/dim 3.5569(3.5693) | Xent 2.3026(2.3026) | Loss 3.5569(3.5693) | Error 0.9044(0.8994) Steps 802(810.01) | Grad Norm 2.0636(3.1225) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 102.1108, Epoch Time 1251.4626(1169.2635), Bit/dim 3.5701(best: 3.5746), Xent 2.3026, Loss 3.5701, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 21.0418(20.5739) | Bit/dim 3.5607(3.5690) | Xent 2.3026(2.3026) | Loss 3.5607(3.5690) | Error 0.8978(0.8986) Steps 814(809.96) | Grad Norm 2.2155(3.2032) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 20.4331(20.5138) | Bit/dim 3.5747(3.5679) | Xent 2.3026(2.3026) | Loss 3.5747(3.5679) | Error 0.9178(0.8980) Steps 790(806.92) | Grad Norm 1.9667(2.9314) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 19.5073(20.5046) | Bit/dim 3.5423(3.5689) | Xent 2.3026(2.3026) | Loss 3.5423(3.5689) | Error 0.8911(0.8986) Steps 808(807.94) | Grad Norm 2.2910(3.0397) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 20.2775(20.4816) | Bit/dim 3.5237(3.5665) | Xent 2.3026(2.3026) | Loss 3.5237(3.5665) | Error 0.8767(0.8986) Steps 796(807.79) | Grad Norm 2.8205(2.9614) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 20.3036(20.4466) | Bit/dim 3.5433(3.5658) | Xent 2.3026(2.3026) | Loss 3.5433(3.5658) | Error 0.8967(0.9004) Steps 820(807.28) | Grad Norm 1.6348(2.9426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 103.2087, Epoch Time 1247.7688(1171.6186), Bit/dim 3.5728(best: 3.5701), Xent 2.3026, Loss 3.5728, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 21.6317(20.5621) | Bit/dim 3.5704(3.5675) | Xent 2.3026(2.3026) | Loss 3.5704(3.5675) | Error 0.9067(0.9014) Steps 838(809.29) | Grad Norm 3.0546(3.0399) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 20.2146(20.5644) | Bit/dim 3.5645(3.5671) | Xent 2.3026(2.3026) | Loss 3.5645(3.5671) | Error 0.9033(0.9011) Steps 796(808.70) | Grad Norm 3.8767(3.2123) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 21.6195(20.5370) | Bit/dim 3.5969(3.5653) | Xent 2.3026(2.3026) | Loss 3.5969(3.5653) | Error 0.9089(0.9010) Steps 826(809.36) | Grad Norm 3.3041(3.2621) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 21.3167(20.5868) | Bit/dim 3.5836(3.5651) | Xent 2.3026(2.3026) | Loss 3.5836(3.5651) | Error 0.9189(0.8993) Steps 826(810.36) | Grad Norm 5.0676(3.2725) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 20.6052(20.5371) | Bit/dim 3.5719(3.5654) | Xent 2.3026(2.3026) | Loss 3.5719(3.5654) | Error 0.9044(0.9005) Steps 808(810.52) | Grad Norm 1.8391(3.0938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 103.2328, Epoch Time 1253.8288(1174.0849), Bit/dim 3.5606(best: 3.5701), Xent 2.3026, Loss 3.5606, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 20.5682(20.4948) | Bit/dim 3.5361(3.5650) | Xent 2.3026(2.3026) | Loss 3.5361(3.5650) | Error 0.9056(0.9004) Steps 796(809.07) | Grad Norm 1.9803(2.9091) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 20.5265(20.5222) | Bit/dim 3.5885(3.5656) | Xent 2.3026(2.3026) | Loss 3.5885(3.5656) | Error 0.8867(0.8994) Steps 808(808.81) | Grad Norm 2.6436(3.0551) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 20.7608(20.5377) | Bit/dim 3.5466(3.5642) | Xent 2.3026(2.3026) | Loss 3.5466(3.5642) | Error 0.9111(0.8995) Steps 826(809.96) | Grad Norm 2.8223(3.0185) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 21.0070(20.5349) | Bit/dim 3.5837(3.5635) | Xent 2.3026(2.3026) | Loss 3.5837(3.5635) | Error 0.9033(0.9001) Steps 808(810.11) | Grad Norm 3.3032(3.1716) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 21.1029(20.5189) | Bit/dim 3.5304(3.5613) | Xent 2.3026(2.3026) | Loss 3.5304(3.5613) | Error 0.8933(0.9003) Steps 820(809.95) | Grad Norm 1.9212(3.0620) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 103.2069, Epoch Time 1249.6978(1176.3533), Bit/dim 3.5599(best: 3.5606), Xent 2.3026, Loss 3.5599, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 19.9318(20.4981) | Bit/dim 3.5439(3.5599) | Xent 2.3026(2.3026) | Loss 3.5439(3.5599) | Error 0.8967(0.9002) Steps 820(811.38) | Grad Norm 4.0070(2.8901) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 21.1114(20.5520) | Bit/dim 3.5549(3.5595) | Xent 2.3026(2.3026) | Loss 3.5549(3.5595) | Error 0.8922(0.9013) Steps 808(812.45) | Grad Norm 3.0580(2.9630) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 20.2803(20.5432) | Bit/dim 3.5540(3.5597) | Xent 2.3026(2.3026) | Loss 3.5540(3.5597) | Error 0.9156(0.9017) Steps 814(813.54) | Grad Norm 2.4999(3.0777) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 20.5796(20.5312) | Bit/dim 3.5445(3.5571) | Xent 2.3026(2.3026) | Loss 3.5445(3.5571) | Error 0.9022(0.8997) Steps 802(812.42) | Grad Norm 2.3272(3.0217) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 20.2297(20.4908) | Bit/dim 3.5668(3.5591) | Xent 2.3026(2.3026) | Loss 3.5668(3.5591) | Error 0.9133(0.9005) Steps 832(812.49) | Grad Norm 3.5530(3.1063) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 20.5770(20.4489) | Bit/dim 3.5745(3.5605) | Xent 2.3026(2.3026) | Loss 3.5745(3.5605) | Error 0.8989(0.9001) Steps 832(813.78) | Grad Norm 3.7232(3.2937) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 103.1001, Epoch Time 1247.1020(1178.4758), Bit/dim 3.5603(best: 3.5599), Xent 2.3026, Loss 3.5603, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 20.3858(20.3743) | Bit/dim 3.5551(3.5582) | Xent 2.3026(2.3026) | Loss 3.5551(3.5582) | Error 0.8978(0.8995) Steps 808(812.99) | Grad Norm 2.7236(3.1948) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 20.4316(20.3481) | Bit/dim 3.5323(3.5562) | Xent 2.3026(2.3026) | Loss 3.5323(3.5562) | Error 0.9011(0.9004) Steps 820(812.95) | Grad Norm 3.5224(3.0893) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 21.4158(20.3951) | Bit/dim 3.5864(3.5592) | Xent 2.3026(2.3026) | Loss 3.5864(3.5592) | Error 0.8889(0.8994) Steps 796(812.66) | Grad Norm 1.8235(3.0939) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 21.1593(20.3881) | Bit/dim 3.5820(3.5586) | Xent 2.3026(2.3026) | Loss 3.5820(3.5586) | Error 0.9100(0.8995) Steps 826(810.41) | Grad Norm 4.2540(3.1318) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 20.3458(20.4689) | Bit/dim 3.5584(3.5566) | Xent 2.3026(2.3026) | Loss 3.5584(3.5566) | Error 0.8922(0.8992) Steps 802(811.01) | Grad Norm 3.1576(2.9952) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 103.5426, Epoch Time 1243.8315(1180.4364), Bit/dim 3.5531(best: 3.5599), Xent 2.3026, Loss 3.5531, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 20.1824(20.4140) | Bit/dim 3.5245(3.5550) | Xent 2.3026(2.3026) | Loss 3.5245(3.5550) | Error 0.8811(0.9001) Steps 802(809.96) | Grad Norm 2.2825(2.8655) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 20.3970(20.3350) | Bit/dim 3.5552(3.5542) | Xent 2.3026(2.3026) | Loss 3.5552(3.5542) | Error 0.9078(0.8992) Steps 802(811.14) | Grad Norm 2.9771(3.0780) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 21.1024(20.3108) | Bit/dim 3.5733(3.5549) | Xent 2.3026(2.3026) | Loss 3.5733(3.5549) | Error 0.8978(0.8997) Steps 820(811.04) | Grad Norm 2.4672(2.9514) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 20.6715(20.3424) | Bit/dim 3.5528(3.5557) | Xent 2.3026(2.3026) | Loss 3.5528(3.5557) | Error 0.8900(0.8997) Steps 820(812.11) | Grad Norm 4.6390(3.0826) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 20.5728(20.3841) | Bit/dim 3.5447(3.5518) | Xent 2.3026(2.3026) | Loss 3.5447(3.5518) | Error 0.9111(0.9000) Steps 814(811.64) | Grad Norm 2.5302(3.1155) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 20.2076(20.2968) | Bit/dim 3.5601(3.5534) | Xent 2.3026(2.3026) | Loss 3.5601(3.5534) | Error 0.9111(0.9003) Steps 796(809.89) | Grad Norm 2.6694(3.0626) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 104.2328, Epoch Time 1236.5597(1182.1201), Bit/dim 3.5511(best: 3.5531), Xent 2.3026, Loss 3.5511, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 21.9640(20.3156) | Bit/dim 3.5828(3.5534) | Xent 2.3026(2.3026) | Loss 3.5828(3.5534) | Error 0.9111(0.9018) Steps 832(810.66) | Grad Norm 4.3148(3.0937) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 21.4139(20.2940) | Bit/dim 3.5702(3.5537) | Xent 2.3026(2.3026) | Loss 3.5702(3.5537) | Error 0.8922(0.9001) Steps 820(812.56) | Grad Norm 3.1531(3.1070) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 19.6045(20.1934) | Bit/dim 3.5712(3.5508) | Xent 2.3026(2.3026) | Loss 3.5712(3.5508) | Error 0.8989(0.8998) Steps 820(810.99) | Grad Norm 4.8105(3.0852) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 20.5328(20.1972) | Bit/dim 3.5377(3.5497) | Xent 2.3026(2.3026) | Loss 3.5377(3.5497) | Error 0.9144(0.8992) Steps 820(813.91) | Grad Norm 1.9848(3.0804) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 20.0386(20.2406) | Bit/dim 3.5576(3.5516) | Xent 2.3026(2.3026) | Loss 3.5576(3.5516) | Error 0.9078(0.9001) Steps 826(815.02) | Grad Norm 5.0581(2.9612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 104.0860, Epoch Time 1234.0523(1183.6781), Bit/dim 3.5511(best: 3.5511), Xent 2.3026, Loss 3.5511, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 19.9615(20.2019) | Bit/dim 3.5501(3.5497) | Xent 2.3026(2.3026) | Loss 3.5501(3.5497) | Error 0.8933(0.8988) Steps 814(815.99) | Grad Norm 2.7298(3.0460) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 19.2130(20.1764) | Bit/dim 3.5136(3.5476) | Xent 2.3026(2.3026) | Loss 3.5136(3.5476) | Error 0.9222(0.8984) Steps 802(816.50) | Grad Norm 2.0236(3.0112) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 19.1730(20.1289) | Bit/dim 3.5423(3.5506) | Xent 2.3026(2.3026) | Loss 3.5423(3.5506) | Error 0.8911(0.9002) Steps 814(814.90) | Grad Norm 2.8037(3.1035) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 19.8753(20.2350) | Bit/dim 3.5357(3.5479) | Xent 2.3026(2.3026) | Loss 3.5357(3.5479) | Error 0.9022(0.9010) Steps 808(816.06) | Grad Norm 2.6109(3.0182) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 20.7491(20.2669) | Bit/dim 3.5604(3.5492) | Xent 2.3026(2.3026) | Loss 3.5604(3.5492) | Error 0.9111(0.9016) Steps 826(816.62) | Grad Norm 2.2514(2.8254) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 20.5653(20.2864) | Bit/dim 3.5156(3.5477) | Xent 2.3026(2.3026) | Loss 3.5156(3.5477) | Error 0.8733(0.9002) Steps 814(817.25) | Grad Norm 4.2587(3.0776) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 102.4970, Epoch Time 1234.1577(1185.1925), Bit/dim 3.5492(best: 3.5511), Xent 2.3026, Loss 3.5492, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 20.1727(20.2315) | Bit/dim 3.5638(3.5481) | Xent 2.3026(2.3026) | Loss 3.5638(3.5481) | Error 0.9011(0.9000) Steps 808(817.22) | Grad Norm 4.6050(3.0642) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 20.4757(20.2805) | Bit/dim 3.5618(3.5466) | Xent 2.3026(2.3026) | Loss 3.5618(3.5466) | Error 0.9000(0.8990) Steps 820(817.22) | Grad Norm 3.3909(3.1145) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 20.6137(20.2559) | Bit/dim 3.5524(3.5444) | Xent 2.3026(2.3026) | Loss 3.5524(3.5444) | Error 0.9100(0.8998) Steps 826(818.59) | Grad Norm 2.6089(2.8237) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 20.0965(20.1425) | Bit/dim 3.5076(3.5441) | Xent 2.3026(2.3026) | Loss 3.5076(3.5441) | Error 0.8767(0.8994) Steps 808(816.41) | Grad Norm 2.2011(2.9487) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 21.2480(20.2616) | Bit/dim 3.5444(3.5426) | Xent 2.3026(2.3026) | Loss 3.5444(3.5426) | Error 0.9022(0.8999) Steps 826(818.41) | Grad Norm 2.0232(2.9516) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 105.3371, Epoch Time 1237.1659(1186.7517), Bit/dim 3.5470(best: 3.5492), Xent 2.3026, Loss 3.5470, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 20.1876(20.2600) | Bit/dim 3.5555(3.5448) | Xent 2.3026(2.3026) | Loss 3.5555(3.5448) | Error 0.9067(0.9010) Steps 808(816.41) | Grad Norm 3.2419(2.9944) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 20.5563(20.2458) | Bit/dim 3.5524(3.5452) | Xent 2.3026(2.3026) | Loss 3.5524(3.5452) | Error 0.9056(0.9003) Steps 820(816.88) | Grad Norm 2.2482(3.0073) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 20.4524(20.1657) | Bit/dim 3.5796(3.5440) | Xent 2.3026(2.3026) | Loss 3.5796(3.5440) | Error 0.9144(0.9009) Steps 838(816.06) | Grad Norm 3.1024(3.1029) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 20.2947(20.1265) | Bit/dim 3.5003(3.5426) | Xent 2.3026(2.3026) | Loss 3.5003(3.5426) | Error 0.8933(0.9020) Steps 820(816.47) | Grad Norm 2.9953(2.9299) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 20.5536(20.1595) | Bit/dim 3.5229(3.5430) | Xent 2.3026(2.3026) | Loss 3.5229(3.5430) | Error 0.8744(0.8994) Steps 814(817.47) | Grad Norm 2.6640(2.9096) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 20.5192(20.2337) | Bit/dim 3.5614(3.5417) | Xent 2.3026(2.3026) | Loss 3.5614(3.5417) | Error 0.9144(0.9000) Steps 826(817.38) | Grad Norm 3.2070(2.9521) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 104.4360, Epoch Time 1231.4551(1188.0928), Bit/dim 3.5399(best: 3.5470), Xent 2.3026, Loss 3.5399, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 20.0219(20.1784) | Bit/dim 3.5387(3.5412) | Xent 2.3026(2.3026) | Loss 3.5387(3.5412) | Error 0.8922(0.9007) Steps 838(818.89) | Grad Norm 3.4008(3.0651) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 20.3875(20.2465) | Bit/dim 3.5488(3.5411) | Xent 2.3026(2.3026) | Loss 3.5488(3.5411) | Error 0.9167(0.9006) Steps 838(819.57) | Grad Norm 1.6944(2.8788) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 20.0300(20.2489) | Bit/dim 3.5353(3.5399) | Xent 2.3026(2.3026) | Loss 3.5353(3.5399) | Error 0.9044(0.8995) Steps 832(820.14) | Grad Norm 4.4263(3.0276) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 20.2483(20.1581) | Bit/dim 3.5226(3.5381) | Xent 2.3026(2.3026) | Loss 3.5226(3.5381) | Error 0.8933(0.9000) Steps 802(817.47) | Grad Norm 2.4930(2.8473) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 20.5527(20.1846) | Bit/dim 3.5388(3.5393) | Xent 2.3026(2.3026) | Loss 3.5388(3.5393) | Error 0.8967(0.8997) Steps 844(817.95) | Grad Norm 3.0434(3.0363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 103.4139, Epoch Time 1229.9389(1189.3482), Bit/dim 3.5402(best: 3.5399), Xent 2.3026, Loss 3.5402, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 19.7698(20.1323) | Bit/dim 3.5395(3.5385) | Xent 2.3026(2.3026) | Loss 3.5395(3.5385) | Error 0.9133(0.8988) Steps 820(818.18) | Grad Norm 2.8490(3.0895) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 20.1601(20.1966) | Bit/dim 3.5119(3.5385) | Xent 2.3026(2.3026) | Loss 3.5119(3.5385) | Error 0.9011(0.8989) Steps 826(818.36) | Grad Norm 3.9324(2.9365) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 20.2617(20.2551) | Bit/dim 3.5007(3.5391) | Xent 2.3026(2.3026) | Loss 3.5007(3.5391) | Error 0.8867(0.8995) Steps 838(821.94) | Grad Norm 1.9023(2.9746) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 20.2668(20.2557) | Bit/dim 3.5344(3.5376) | Xent 2.3026(2.3026) | Loss 3.5344(3.5376) | Error 0.8989(0.9000) Steps 832(820.73) | Grad Norm 2.1448(3.0405) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 20.6984(20.2915) | Bit/dim 3.5401(3.5383) | Xent 2.3026(2.3026) | Loss 3.5401(3.5383) | Error 0.9022(0.9005) Steps 826(821.58) | Grad Norm 2.3835(2.9266) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 21.6083(20.3828) | Bit/dim 3.5090(3.5353) | Xent 2.3026(2.3026) | Loss 3.5090(3.5353) | Error 0.8867(0.8997) Steps 838(823.15) | Grad Norm 2.4465(2.8909) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 107.4066, Epoch Time 1246.2215(1191.0544), Bit/dim 3.5336(best: 3.5399), Xent 2.3026, Loss 3.5336, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 19.6287(20.3356) | Bit/dim 3.5410(3.5354) | Xent 2.3026(2.3026) | Loss 3.5410(3.5354) | Error 0.8900(0.8991) Steps 832(823.26) | Grad Norm 3.1229(2.8830) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 20.7136(20.3147) | Bit/dim 3.5960(3.5362) | Xent 2.3026(2.3026) | Loss 3.5960(3.5362) | Error 0.8989(0.8990) Steps 820(824.49) | Grad Norm 2.6104(2.9944) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 19.7966(20.2999) | Bit/dim 3.4960(3.5341) | Xent 2.3026(2.3026) | Loss 3.4960(3.5341) | Error 0.8856(0.8993) Steps 826(824.54) | Grad Norm 2.1978(3.0140) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 19.5357(20.2307) | Bit/dim 3.5254(3.5341) | Xent 2.3026(2.3026) | Loss 3.5254(3.5341) | Error 0.8967(0.9006) Steps 826(823.17) | Grad Norm 3.5956(2.8047) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 20.0844(20.1895) | Bit/dim 3.5142(3.5332) | Xent 2.3026(2.3026) | Loss 3.5142(3.5332) | Error 0.8922(0.8997) Steps 820(823.19) | Grad Norm 2.6007(2.9121) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 104.9356, Epoch Time 1234.5854(1192.3603), Bit/dim 3.5381(best: 3.5336), Xent 2.3026, Loss 3.5381, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 20.3828(20.2894) | Bit/dim 3.5332(3.5310) | Xent 2.3026(2.3026) | Loss 3.5332(3.5310) | Error 0.9067(0.8995) Steps 826(822.45) | Grad Norm 4.5549(2.9389) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 20.3911(20.2585) | Bit/dim 3.5503(3.5339) | Xent 2.3026(2.3026) | Loss 3.5503(3.5339) | Error 0.9056(0.9009) Steps 814(824.17) | Grad Norm 1.8910(2.7814) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 20.8891(20.3319) | Bit/dim 3.5436(3.5325) | Xent 2.3026(2.3026) | Loss 3.5436(3.5325) | Error 0.8978(0.8999) Steps 820(825.94) | Grad Norm 2.2254(2.8962) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 19.6857(20.2746) | Bit/dim 3.5413(3.5341) | Xent 2.3026(2.3026) | Loss 3.5413(3.5341) | Error 0.8967(0.9011) Steps 820(826.01) | Grad Norm 2.2096(2.9616) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 20.7046(20.3037) | Bit/dim 3.5285(3.5326) | Xent 2.3026(2.3026) | Loss 3.5285(3.5326) | Error 0.9011(0.9000) Steps 832(826.76) | Grad Norm 2.0657(2.8072) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 20.0896(20.2796) | Bit/dim 3.5087(3.5324) | Xent 2.3026(2.3026) | Loss 3.5087(3.5324) | Error 0.9022(0.8997) Steps 832(827.05) | Grad Norm 2.6076(2.9023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 104.7850, Epoch Time 1240.4227(1193.8022), Bit/dim 3.5322(best: 3.5336), Xent 2.3026, Loss 3.5322, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 20.4140(20.3544) | Bit/dim 3.5662(3.5341) | Xent 2.3026(2.3026) | Loss 3.5662(3.5341) | Error 0.9122(0.9010) Steps 832(827.10) | Grad Norm 4.1322(2.8077) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 20.6188(20.4006) | Bit/dim 3.5399(3.5298) | Xent 2.3026(2.3026) | Loss 3.5399(3.5298) | Error 0.8989(0.9002) Steps 838(827.72) | Grad Norm 3.1554(2.8425) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 20.3518(20.4160) | Bit/dim 3.5343(3.5301) | Xent 2.3026(2.3026) | Loss 3.5343(3.5301) | Error 0.9044(0.9012) Steps 838(827.98) | Grad Norm 3.5212(2.9317) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 20.1905(20.3659) | Bit/dim 3.5431(3.5276) | Xent 2.3026(2.3026) | Loss 3.5431(3.5276) | Error 0.9200(0.8999) Steps 808(827.30) | Grad Norm 2.4102(2.9857) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 20.2694(20.3122) | Bit/dim 3.5426(3.5293) | Xent 2.3026(2.3026) | Loss 3.5426(3.5293) | Error 0.8911(0.8993) Steps 838(827.50) | Grad Norm 2.4421(2.8333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 107.1614, Epoch Time 1247.1450(1195.4025), Bit/dim 3.5277(best: 3.5322), Xent 2.3026, Loss 3.5277, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 20.3102(20.3739) | Bit/dim 3.5259(3.5307) | Xent 2.3026(2.3026) | Loss 3.5259(3.5307) | Error 0.9044(0.9003) Steps 838(830.06) | Grad Norm 3.4679(2.9675) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 20.5263(20.3876) | Bit/dim 3.5161(3.5319) | Xent 2.3026(2.3026) | Loss 3.5161(3.5319) | Error 0.9133(0.9005) Steps 844(830.43) | Grad Norm 2.1551(2.9152) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 20.3340(20.3767) | Bit/dim 3.5381(3.5293) | Xent 2.3026(2.3026) | Loss 3.5381(3.5293) | Error 0.8856(0.9002) Steps 844(831.46) | Grad Norm 4.6598(2.9291) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 19.9032(20.3003) | Bit/dim 3.5054(3.5309) | Xent 2.3026(2.3026) | Loss 3.5054(3.5309) | Error 0.8933(0.9005) Steps 826(831.91) | Grad Norm 2.4236(2.8740) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 20.0448(20.3282) | Bit/dim 3.5094(3.5281) | Xent 2.3026(2.3026) | Loss 3.5094(3.5281) | Error 0.8844(0.8990) Steps 844(831.13) | Grad Norm 3.5953(2.8167) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 20.1915(20.3706) | Bit/dim 3.5233(3.5269) | Xent 2.3026(2.3026) | Loss 3.5233(3.5269) | Error 0.9056(0.8988) Steps 838(830.35) | Grad Norm 3.0335(2.8606) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 105.0292, Epoch Time 1244.7744(1196.8836), Bit/dim 3.5282(best: 3.5277), Xent 2.3026, Loss 3.5282, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 20.9139(20.3716) | Bit/dim 3.5208(3.5259) | Xent 2.3026(2.3026) | Loss 3.5208(3.5259) | Error 0.8933(0.8988) Steps 850(832.18) | Grad Norm 1.6056(2.9720) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 20.8377(20.3377) | Bit/dim 3.5368(3.5237) | Xent 2.3026(2.3026) | Loss 3.5368(3.5237) | Error 0.9111(0.8996) Steps 838(829.66) | Grad Norm 3.7032(3.0011) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 20.3646(20.3094) | Bit/dim 3.5125(3.5245) | Xent 2.3026(2.3026) | Loss 3.5125(3.5245) | Error 0.9078(0.8998) Steps 832(829.81) | Grad Norm 1.5776(2.7381) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 20.5279(20.3080) | Bit/dim 3.5547(3.5250) | Xent 2.3026(2.3026) | Loss 3.5547(3.5250) | Error 0.9144(0.8988) Steps 838(830.77) | Grad Norm 3.8470(2.8970) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 19.9825(20.3024) | Bit/dim 3.5296(3.5278) | Xent 2.3026(2.3026) | Loss 3.5296(3.5278) | Error 0.9000(0.8995) Steps 844(829.73) | Grad Norm 2.4075(2.9782) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 106.2933, Epoch Time 1239.3907(1198.1588), Bit/dim 3.5228(best: 3.5277), Xent 2.3026, Loss 3.5228, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 20.3359(20.3105) | Bit/dim 3.4947(3.5250) | Xent 2.3026(2.3026) | Loss 3.4947(3.5250) | Error 0.9000(0.9002) Steps 838(831.60) | Grad Norm 3.3383(2.8847) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 20.7662(20.3510) | Bit/dim 3.5640(3.5250) | Xent 2.3026(2.3026) | Loss 3.5640(3.5250) | Error 0.8956(0.8985) Steps 832(829.87) | Grad Norm 2.6185(2.8326) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 20.7526(20.3486) | Bit/dim 3.4900(3.5225) | Xent 2.3026(2.3026) | Loss 3.4900(3.5225) | Error 0.8933(0.8998) Steps 814(827.25) | Grad Norm 2.8599(2.7754) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 20.4000(20.3120) | Bit/dim 3.5219(3.5227) | Xent 2.3026(2.3026) | Loss 3.5219(3.5227) | Error 0.8856(0.9001) Steps 838(826.25) | Grad Norm 3.6829(2.8332) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 20.8969(20.3640) | Bit/dim 3.5513(3.5219) | Xent 2.3026(2.3026) | Loss 3.5513(3.5219) | Error 0.9078(0.8999) Steps 832(825.92) | Grad Norm 2.8160(2.8002) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 20.6918(20.3687) | Bit/dim 3.5211(3.5235) | Xent 2.3026(2.3026) | Loss 3.5211(3.5235) | Error 0.9100(0.9005) Steps 844(827.29) | Grad Norm 3.0189(2.8854) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 106.7958, Epoch Time 1246.6806(1199.6145), Bit/dim 3.5223(best: 3.5228), Xent 2.3026, Loss 3.5223, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 20.0100(20.3427) | Bit/dim 3.5308(3.5214) | Xent 2.3026(2.3026) | Loss 3.5308(3.5214) | Error 0.9089(0.8998) Steps 832(828.16) | Grad Norm 3.9580(2.7596) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 19.4070(20.3403) | Bit/dim 3.5220(3.5209) | Xent 2.3026(2.3026) | Loss 3.5220(3.5209) | Error 0.9033(0.8994) Steps 820(827.19) | Grad Norm 1.3145(2.7600) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 20.5069(20.3747) | Bit/dim 3.5240(3.5217) | Xent 2.3026(2.3026) | Loss 3.5240(3.5217) | Error 0.8933(0.8985) Steps 814(829.42) | Grad Norm 4.0205(2.7791) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 20.6560(20.3021) | Bit/dim 3.4896(3.5206) | Xent 2.3026(2.3026) | Loss 3.4896(3.5206) | Error 0.8844(0.9001) Steps 826(830.19) | Grad Norm 2.4817(2.7906) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 20.0079(20.3910) | Bit/dim 3.5261(3.5216) | Xent 2.3026(2.3026) | Loss 3.5261(3.5216) | Error 0.9000(0.8996) Steps 820(831.23) | Grad Norm 3.9916(2.7905) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 107.2146, Epoch Time 1246.2498(1201.0136), Bit/dim 3.5185(best: 3.5223), Xent 2.3026, Loss 3.5185, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 21.3911(20.4071) | Bit/dim 3.5377(3.5223) | Xent 2.3026(2.3026) | Loss 3.5377(3.5223) | Error 0.9067(0.8994) Steps 820(829.94) | Grad Norm 2.5328(2.6643) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 19.7495(20.4369) | Bit/dim 3.5519(3.5223) | Xent 2.3026(2.3026) | Loss 3.5519(3.5223) | Error 0.9100(0.8993) Steps 820(832.17) | Grad Norm 4.6039(2.9352) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 20.3664(20.3958) | Bit/dim 3.4887(3.5184) | Xent 2.3026(2.3026) | Loss 3.4887(3.5184) | Error 0.8978(0.8995) Steps 838(830.97) | Grad Norm 1.4520(2.9268) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 19.2944(20.3529) | Bit/dim 3.5135(3.5203) | Xent 2.3026(2.3026) | Loss 3.5135(3.5203) | Error 0.9222(0.9011) Steps 826(829.88) | Grad Norm 2.5597(2.8239) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 19.8309(20.3445) | Bit/dim 3.5251(3.5205) | Xent 2.3026(2.3026) | Loss 3.5251(3.5205) | Error 0.8856(0.9014) Steps 820(827.43) | Grad Norm 2.1665(2.8957) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 21.4686(20.3554) | Bit/dim 3.5233(3.5194) | Xent 2.3026(2.3026) | Loss 3.5233(3.5194) | Error 0.9022(0.9009) Steps 814(825.45) | Grad Norm 2.3065(2.8902) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 107.4880, Epoch Time 1246.1107(1202.3665), Bit/dim 3.5151(best: 3.5185), Xent 2.3026, Loss 3.5151, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 20.1076(20.3830) | Bit/dim 3.5118(3.5168) | Xent 2.3026(2.3026) | Loss 3.5118(3.5168) | Error 0.8922(0.9003) Steps 826(827.05) | Grad Norm 2.6368(2.7678) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 20.5054(20.4353) | Bit/dim 3.5073(3.5142) | Xent 2.3026(2.3026) | Loss 3.5073(3.5142) | Error 0.9067(0.8990) Steps 838(828.27) | Grad Norm 2.6435(2.8230) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 20.5303(20.4706) | Bit/dim 3.5102(3.5172) | Xent 2.3026(2.3026) | Loss 3.5102(3.5172) | Error 0.8944(0.8999) Steps 820(828.17) | Grad Norm 4.1419(2.8885) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 20.4304(20.5016) | Bit/dim 3.5212(3.5168) | Xent 2.3026(2.3026) | Loss 3.5212(3.5168) | Error 0.9078(0.8998) Steps 826(829.28) | Grad Norm 1.8415(2.7681) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 19.6067(20.4379) | Bit/dim 3.5172(3.5182) | Xent 2.3026(2.3026) | Loss 3.5172(3.5182) | Error 0.8933(0.8993) Steps 826(829.44) | Grad Norm 3.2631(2.8428) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 106.3991, Epoch Time 1252.9874(1203.8851), Bit/dim 3.5162(best: 3.5151), Xent 2.3026, Loss 3.5162, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 20.6589(20.4741) | Bit/dim 3.5242(3.5177) | Xent 2.3026(2.3026) | Loss 3.5242(3.5177) | Error 0.9022(0.8997) Steps 826(829.82) | Grad Norm 4.6755(2.9505) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 19.8997(20.4624) | Bit/dim 3.5246(3.5187) | Xent 2.3026(2.3026) | Loss 3.5246(3.5187) | Error 0.9044(0.9002) Steps 814(829.55) | Grad Norm 2.9288(2.9533) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 20.9250(20.4146) | Bit/dim 3.5028(3.5161) | Xent 2.3026(2.3026) | Loss 3.5028(3.5161) | Error 0.9011(0.9006) Steps 838(829.92) | Grad Norm 3.5856(2.8273) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 20.8911(20.3672) | Bit/dim 3.5405(3.5159) | Xent 2.3026(2.3026) | Loss 3.5405(3.5159) | Error 0.9056(0.9007) Steps 820(826.75) | Grad Norm 1.8545(2.8436) | Total Time 14.00(14.00)\n",
      "Iter 4390 | Time 20.9683(20.3436) | Bit/dim 3.4926(3.5141) | Xent 2.3026(2.3026) | Loss 3.4926(3.5141) | Error 0.8856(0.9003) Steps 838(828.71) | Grad Norm 2.9665(2.6892) | Total Time 14.00(14.00)\n",
      "Iter 4400 | Time 21.0064(20.3526) | Bit/dim 3.5588(3.5184) | Xent 2.3026(2.3026) | Loss 3.5588(3.5184) | Error 0.9133(0.9006) Steps 850(829.76) | Grad Norm 4.1704(2.7917) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 106.4289, Epoch Time 1242.2849(1205.0371), Bit/dim 3.5203(best: 3.5151), Xent 2.3026, Loss 3.5203, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 20.2220(20.3105) | Bit/dim 3.4698(3.5157) | Xent 2.3026(2.3026) | Loss 3.4698(3.5157) | Error 0.9011(0.9004) Steps 820(828.53) | Grad Norm 3.1717(2.7908) | Total Time 14.00(14.00)\n",
      "Iter 4420 | Time 20.4998(20.3730) | Bit/dim 3.5052(3.5148) | Xent 2.3026(2.3026) | Loss 3.5052(3.5148) | Error 0.9022(0.9009) Steps 844(830.17) | Grad Norm 1.5856(2.5102) | Total Time 14.00(14.00)\n",
      "Iter 4430 | Time 20.1822(20.3483) | Bit/dim 3.4911(3.5124) | Xent 2.3026(2.3026) | Loss 3.4911(3.5124) | Error 0.8811(0.8998) Steps 844(830.40) | Grad Norm 3.8972(2.7369) | Total Time 14.00(14.00)\n",
      "Iter 4440 | Time 20.2639(20.2995) | Bit/dim 3.5419(3.5129) | Xent 2.3026(2.3026) | Loss 3.5419(3.5129) | Error 0.9044(0.8999) Steps 802(830.76) | Grad Norm 3.6467(2.7683) | Total Time 14.00(14.00)\n",
      "Iter 4450 | Time 21.4241(20.3126) | Bit/dim 3.4832(3.5147) | Xent 2.3026(2.3026) | Loss 3.4832(3.5147) | Error 0.8956(0.9004) Steps 838(828.64) | Grad Norm 2.3752(2.6745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 106.8402, Epoch Time 1242.2278(1206.1528), Bit/dim 3.5143(best: 3.5151), Xent 2.3026, Loss 3.5143, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 19.6766(20.2972) | Bit/dim 3.5288(3.5173) | Xent 2.3026(2.3026) | Loss 3.5288(3.5173) | Error 0.9178(0.9007) Steps 808(828.57) | Grad Norm 4.8269(2.9730) | Total Time 14.00(14.00)\n",
      "Iter 4470 | Time 19.9798(20.2030) | Bit/dim 3.5171(3.5172) | Xent 2.3026(2.3026) | Loss 3.5171(3.5172) | Error 0.9178(0.9016) Steps 832(827.65) | Grad Norm 2.0519(3.0205) | Total Time 14.00(14.00)\n",
      "Iter 4480 | Time 20.4916(20.2738) | Bit/dim 3.5423(3.5163) | Xent 2.3026(2.3026) | Loss 3.5423(3.5163) | Error 0.9111(0.9013) Steps 826(826.26) | Grad Norm 2.1375(2.7868) | Total Time 14.00(14.00)\n",
      "Iter 4490 | Time 20.0578(20.2823) | Bit/dim 3.5100(3.5126) | Xent 2.3026(2.3026) | Loss 3.5100(3.5126) | Error 0.8911(0.9001) Steps 826(825.09) | Grad Norm 4.3665(2.8457) | Total Time 14.00(14.00)\n",
      "Iter 4500 | Time 20.0012(20.2667) | Bit/dim 3.5119(3.5117) | Xent 2.3026(2.3026) | Loss 3.5119(3.5117) | Error 0.9122(0.8998) Steps 832(824.79) | Grad Norm 2.9974(2.8309) | Total Time 14.00(14.00)\n",
      "Iter 4510 | Time 20.1350(20.2407) | Bit/dim 3.4838(3.5093) | Xent 2.3026(2.3026) | Loss 3.4838(3.5093) | Error 0.9144(0.8991) Steps 838(825.28) | Grad Norm 2.5807(2.8120) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 107.2101, Epoch Time 1237.2084(1207.0845), Bit/dim 3.5116(best: 3.5143), Xent 2.3026, Loss 3.5116, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 20.5164(20.2523) | Bit/dim 3.5042(3.5081) | Xent 2.3026(2.3026) | Loss 3.5042(3.5081) | Error 0.9056(0.8996) Steps 826(825.32) | Grad Norm 2.7057(2.8722) | Total Time 14.00(14.00)\n",
      "Iter 4530 | Time 20.9380(20.2929) | Bit/dim 3.4990(3.5095) | Xent 2.3026(2.3026) | Loss 3.4990(3.5095) | Error 0.8856(0.8992) Steps 820(825.91) | Grad Norm 2.1861(2.6442) | Total Time 14.00(14.00)\n",
      "Iter 4540 | Time 19.9181(20.3359) | Bit/dim 3.4851(3.5075) | Xent 2.3026(2.3026) | Loss 3.4851(3.5075) | Error 0.8900(0.9000) Steps 826(827.21) | Grad Norm 4.1873(2.6054) | Total Time 14.00(14.00)\n",
      "Iter 4550 | Time 20.8436(20.3355) | Bit/dim 3.5176(3.5069) | Xent 2.3026(2.3026) | Loss 3.5176(3.5069) | Error 0.9056(0.8994) Steps 820(827.37) | Grad Norm 2.7306(2.7724) | Total Time 14.00(14.00)\n",
      "Iter 4560 | Time 21.0714(20.3454) | Bit/dim 3.5052(3.5078) | Xent 2.3026(2.3026) | Loss 3.5052(3.5078) | Error 0.8967(0.8999) Steps 838(825.15) | Grad Norm 2.7897(2.7066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 109.0374, Epoch Time 1247.3858(1208.2935), Bit/dim 3.5097(best: 3.5116), Xent 2.3026, Loss 3.5097, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 20.1255(20.4077) | Bit/dim 3.4916(3.5086) | Xent 2.3026(2.3026) | Loss 3.4916(3.5086) | Error 0.8989(0.8998) Steps 832(828.32) | Grad Norm 1.7847(2.6842) | Total Time 14.00(14.00)\n",
      "Iter 4580 | Time 19.8945(20.4429) | Bit/dim 3.5236(3.5096) | Xent 2.3026(2.3026) | Loss 3.5236(3.5096) | Error 0.9044(0.8992) Steps 832(828.52) | Grad Norm 1.7312(2.5809) | Total Time 14.00(14.00)\n",
      "Iter 4590 | Time 20.0953(20.3606) | Bit/dim 3.4919(3.5099) | Xent 2.3026(2.3026) | Loss 3.4919(3.5099) | Error 0.8978(0.9001) Steps 820(828.47) | Grad Norm 2.1194(2.7904) | Total Time 14.00(14.00)\n",
      "Iter 4600 | Time 20.1312(20.3773) | Bit/dim 3.4779(3.5086) | Xent 2.3026(2.3026) | Loss 3.4779(3.5086) | Error 0.9144(0.9008) Steps 838(829.82) | Grad Norm 2.9366(2.8375) | Total Time 14.00(14.00)\n",
      "Iter 4610 | Time 20.3412(20.4561) | Bit/dim 3.5159(3.5078) | Xent 2.3026(2.3026) | Loss 3.5159(3.5078) | Error 0.8967(0.9003) Steps 850(833.31) | Grad Norm 4.4539(2.8337) | Total Time 14.00(14.00)\n",
      "Iter 4620 | Time 20.6148(20.5192) | Bit/dim 3.5037(3.5070) | Xent 2.3026(2.3026) | Loss 3.5037(3.5070) | Error 0.8933(0.9001) Steps 838(835.12) | Grad Norm 2.1743(2.8247) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 107.5784, Epoch Time 1254.3840(1209.6762), Bit/dim 3.5061(best: 3.5097), Xent 2.3026, Loss 3.5061, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 21.6359(20.4907) | Bit/dim 3.4936(3.5065) | Xent 2.3026(2.3026) | Loss 3.4936(3.5065) | Error 0.8900(0.9007) Steps 832(832.18) | Grad Norm 2.5608(2.7404) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 20.2294(20.5310) | Bit/dim 3.4954(3.5047) | Xent 2.3026(2.3026) | Loss 3.4954(3.5047) | Error 0.9067(0.8998) Steps 814(830.71) | Grad Norm 3.5209(2.6469) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 20.0365(20.5244) | Bit/dim 3.4838(3.5037) | Xent 2.3026(2.3026) | Loss 3.4838(3.5037) | Error 0.8911(0.8987) Steps 814(829.87) | Grad Norm 2.4165(2.6599) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 20.2711(20.4720) | Bit/dim 3.4763(3.5012) | Xent 2.3026(2.3026) | Loss 3.4763(3.5012) | Error 0.8833(0.8983) Steps 820(830.14) | Grad Norm 2.2215(2.6737) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 20.1936(20.4619) | Bit/dim 3.5258(3.5039) | Xent 2.3026(2.3026) | Loss 3.5258(3.5039) | Error 0.9078(0.8993) Steps 820(830.42) | Grad Norm 4.0879(2.7404) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 106.8774, Epoch Time 1249.7696(1210.8790), Bit/dim 3.5094(best: 3.5061), Xent 2.3026, Loss 3.5094, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 20.8911(20.4507) | Bit/dim 3.5271(3.5100) | Xent 2.3026(2.3026) | Loss 3.5271(3.5100) | Error 0.8956(0.9004) Steps 844(830.44) | Grad Norm 2.7545(2.7241) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 20.3789(20.4454) | Bit/dim 3.5027(3.5079) | Xent 2.3026(2.3026) | Loss 3.5027(3.5079) | Error 0.9033(0.8999) Steps 826(829.72) | Grad Norm 2.9072(2.7057) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 20.8618(20.4697) | Bit/dim 3.5082(3.5061) | Xent 2.3026(2.3026) | Loss 3.5082(3.5061) | Error 0.9111(0.9004) Steps 838(828.52) | Grad Norm 4.7849(2.7607) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 20.1831(20.4790) | Bit/dim 3.5077(3.5050) | Xent 2.3026(2.3026) | Loss 3.5077(3.5050) | Error 0.9078(0.9010) Steps 820(830.92) | Grad Norm 4.0402(2.7969) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 20.3725(20.5093) | Bit/dim 3.4887(3.5030) | Xent 2.3026(2.3026) | Loss 3.4887(3.5030) | Error 0.8867(0.8996) Steps 826(831.19) | Grad Norm 4.1034(2.8749) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 20.9991(20.5408) | Bit/dim 3.4838(3.5039) | Xent 2.3026(2.3026) | Loss 3.4838(3.5039) | Error 0.8989(0.9004) Steps 826(832.21) | Grad Norm 2.7768(2.8553) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 107.7151, Epoch Time 1255.1391(1212.2068), Bit/dim 3.5020(best: 3.5061), Xent 2.3026, Loss 3.5020, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.3223(20.5426) | Bit/dim 3.5244(3.5015) | Xent 2.3026(2.3026) | Loss 3.5244(3.5015) | Error 0.8967(0.8997) Steps 838(833.56) | Grad Norm 3.2927(2.7951) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 21.0824(20.6092) | Bit/dim 3.4948(3.5006) | Xent 2.3026(2.3026) | Loss 3.4948(3.5006) | Error 0.9067(0.8992) Steps 838(833.19) | Grad Norm 2.2216(2.7099) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 20.7257(20.5445) | Bit/dim 3.5093(3.5030) | Xent 2.3026(2.3026) | Loss 3.5093(3.5030) | Error 0.8978(0.8996) Steps 814(831.65) | Grad Norm 1.9891(2.5112) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 21.2161(20.6026) | Bit/dim 3.5195(3.5049) | Xent 2.3026(2.3026) | Loss 3.5195(3.5049) | Error 0.8911(0.8994) Steps 820(831.13) | Grad Norm 3.8383(2.7166) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 20.5421(20.6064) | Bit/dim 3.5044(3.5024) | Xent 2.3026(2.3026) | Loss 3.5044(3.5024) | Error 0.9067(0.9005) Steps 832(832.96) | Grad Norm 2.7793(2.6846) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 108.8683, Epoch Time 1262.1567(1213.7053), Bit/dim 3.5105(best: 3.5020), Xent 2.3026, Loss 3.5105, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 21.4856(20.6963) | Bit/dim 3.5164(3.5030) | Xent 2.3026(2.3026) | Loss 3.5164(3.5030) | Error 0.8956(0.9009) Steps 832(833.00) | Grad Norm 1.4803(2.7627) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 20.4016(20.6704) | Bit/dim 3.4946(3.5027) | Xent 2.3026(2.3026) | Loss 3.4946(3.5027) | Error 0.9122(0.9017) Steps 844(835.67) | Grad Norm 1.9933(2.7126) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 20.9030(20.7362) | Bit/dim 3.5033(3.4994) | Xent 2.3026(2.3026) | Loss 3.5033(3.4994) | Error 0.8900(0.8995) Steps 856(838.69) | Grad Norm 2.1875(2.7698) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 21.0416(20.7195) | Bit/dim 3.5011(3.5000) | Xent 2.3026(2.3026) | Loss 3.5011(3.5000) | Error 0.8911(0.9000) Steps 856(839.43) | Grad Norm 2.9367(2.7887) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 20.6410(20.7074) | Bit/dim 3.5250(3.5006) | Xent 2.3026(2.3026) | Loss 3.5250(3.5006) | Error 0.9133(0.9010) Steps 850(840.64) | Grad Norm 1.7945(2.5677) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 20.0107(20.6886) | Bit/dim 3.4882(3.4992) | Xent 2.3026(2.3026) | Loss 3.4882(3.4992) | Error 0.8944(0.9000) Steps 844(839.36) | Grad Norm 1.3770(2.5602) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 108.4170, Epoch Time 1266.9364(1215.3023), Bit/dim 3.4968(best: 3.5020), Xent 2.3026, Loss 3.4968, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 21.1986(20.6761) | Bit/dim 3.4952(3.4997) | Xent 2.3026(2.3026) | Loss 3.4952(3.4997) | Error 0.9000(0.9013) Steps 832(836.95) | Grad Norm 2.9553(2.6435) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 20.8223(20.6923) | Bit/dim 3.5325(3.5007) | Xent 2.3026(2.3026) | Loss 3.5325(3.5007) | Error 0.8989(0.9008) Steps 838(837.97) | Grad Norm 2.8176(2.7105) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 20.3322(20.6032) | Bit/dim 3.5182(3.5006) | Xent 2.3026(2.3026) | Loss 3.5182(3.5006) | Error 0.8944(0.9001) Steps 850(837.21) | Grad Norm 2.6141(2.5546) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 20.6744(20.6169) | Bit/dim 3.4851(3.4996) | Xent 2.3026(2.3026) | Loss 3.4851(3.4996) | Error 0.9200(0.9001) Steps 844(839.14) | Grad Norm 2.5087(2.7441) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 20.3895(20.5915) | Bit/dim 3.5284(3.4969) | Xent 2.3026(2.3026) | Loss 3.5284(3.4969) | Error 0.9133(0.9000) Steps 838(839.48) | Grad Norm 2.0090(2.6859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 108.0181, Epoch Time 1258.3864(1216.5948), Bit/dim 3.4995(best: 3.4968), Xent 2.3026, Loss 3.4995, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 20.5162(20.6036) | Bit/dim 3.5041(3.4967) | Xent 2.3026(2.3026) | Loss 3.5041(3.4967) | Error 0.8978(0.9001) Steps 838(837.60) | Grad Norm 2.5486(2.6856) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 20.5388(20.6678) | Bit/dim 3.4775(3.4961) | Xent 2.3026(2.3026) | Loss 3.4775(3.4961) | Error 0.8978(0.8999) Steps 838(837.22) | Grad Norm 2.1186(2.7925) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 20.7498(20.7387) | Bit/dim 3.4705(3.4973) | Xent 2.3026(2.3026) | Loss 3.4705(3.4973) | Error 0.8989(0.8996) Steps 838(838.68) | Grad Norm 1.7928(2.7496) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 21.9409(20.8314) | Bit/dim 3.5197(3.4963) | Xent 2.3026(2.3026) | Loss 3.5197(3.4963) | Error 0.9133(0.9000) Steps 850(841.76) | Grad Norm 2.2830(2.5559) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 20.8537(20.8899) | Bit/dim 3.4889(3.4953) | Xent 2.3026(2.3026) | Loss 3.4889(3.4953) | Error 0.9133(0.9006) Steps 844(841.47) | Grad Norm 1.8515(2.6275) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 20.6421(20.8469) | Bit/dim 3.4608(3.4973) | Xent 2.3026(2.3026) | Loss 3.4608(3.4973) | Error 0.9100(0.9005) Steps 838(841.52) | Grad Norm 2.4705(2.6465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 108.2954, Epoch Time 1276.3809(1218.3884), Bit/dim 3.4996(best: 3.4968), Xent 2.3026, Loss 3.4996, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 20.4920(20.8432) | Bit/dim 3.4354(3.4938) | Xent 2.3026(2.3026) | Loss 3.4354(3.4938) | Error 0.8856(0.8999) Steps 856(842.38) | Grad Norm 2.3654(2.5960) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 21.3981(20.8463) | Bit/dim 3.5181(3.4945) | Xent 2.3026(2.3026) | Loss 3.5181(3.4945) | Error 0.9122(0.9005) Steps 850(844.67) | Grad Norm 3.2065(2.8034) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 20.0136(20.8988) | Bit/dim 3.4653(3.4942) | Xent 2.3026(2.3026) | Loss 3.4653(3.4942) | Error 0.8978(0.8994) Steps 844(845.20) | Grad Norm 2.3189(2.7735) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 20.9143(20.9024) | Bit/dim 3.4590(3.4941) | Xent 2.3026(2.3026) | Loss 3.4590(3.4941) | Error 0.8878(0.8993) Steps 820(844.03) | Grad Norm 2.3426(2.7514) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 20.0894(20.8602) | Bit/dim 3.5429(3.4957) | Xent 2.3026(2.3026) | Loss 3.5429(3.4957) | Error 0.9189(0.9005) Steps 850(844.15) | Grad Norm 1.6661(2.5053) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 109.8465, Epoch Time 1279.7641(1220.2296), Bit/dim 3.4941(best: 3.4968), Xent 2.3026, Loss 3.4941, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 21.5263(20.9372) | Bit/dim 3.4618(3.4956) | Xent 2.3026(2.3026) | Loss 3.4618(3.4956) | Error 0.8944(0.9003) Steps 856(848.61) | Grad Norm 1.8134(2.5636) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 20.6313(20.9186) | Bit/dim 3.4909(3.4959) | Xent 2.3026(2.3026) | Loss 3.4909(3.4959) | Error 0.9033(0.9007) Steps 838(846.48) | Grad Norm 4.8834(2.6569) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 21.2459(20.9611) | Bit/dim 3.4731(3.4944) | Xent 2.3026(2.3026) | Loss 3.4731(3.4944) | Error 0.8989(0.8998) Steps 808(843.09) | Grad Norm 2.5098(2.7171) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 20.9323(21.0165) | Bit/dim 3.4656(3.4928) | Xent 2.3026(2.3026) | Loss 3.4656(3.4928) | Error 0.9011(0.8998) Steps 856(843.66) | Grad Norm 2.8810(2.6998) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 21.6778(21.0492) | Bit/dim 3.5423(3.4952) | Xent 2.3026(2.3026) | Loss 3.5423(3.4952) | Error 0.9089(0.9010) Steps 850(843.11) | Grad Norm 2.6494(2.6303) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 20.8896(21.0569) | Bit/dim 3.5130(3.4948) | Xent 2.3026(2.3026) | Loss 3.5130(3.4948) | Error 0.9089(0.9004) Steps 850(846.87) | Grad Norm 2.4216(2.7402) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 108.6523, Epoch Time 1284.7931(1222.1666), Bit/dim 3.4953(best: 3.4941), Xent 2.3026, Loss 3.4953, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.8250(21.0280) | Bit/dim 3.4686(3.4916) | Xent 2.3026(2.3026) | Loss 3.4686(3.4916) | Error 0.8989(0.9001) Steps 832(846.18) | Grad Norm 3.3684(2.6878) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 20.2863(21.0647) | Bit/dim 3.5053(3.4918) | Xent 2.3026(2.3026) | Loss 3.5053(3.4918) | Error 0.8833(0.8990) Steps 850(848.27) | Grad Norm 2.0940(2.7495) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 21.4937(21.0962) | Bit/dim 3.5030(3.4936) | Xent 2.3026(2.3026) | Loss 3.5030(3.4936) | Error 0.8944(0.8995) Steps 868(851.55) | Grad Norm 3.2565(2.7636) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 21.3562(21.1710) | Bit/dim 3.5056(3.4930) | Xent 2.3026(2.3026) | Loss 3.5056(3.4930) | Error 0.9089(0.8999) Steps 856(851.31) | Grad Norm 2.3306(2.7633) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 20.8313(21.1425) | Bit/dim 3.5245(3.4938) | Xent 2.3026(2.3026) | Loss 3.5245(3.4938) | Error 0.9022(0.9004) Steps 862(853.59) | Grad Norm 2.0693(2.7242) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 108.6425, Epoch Time 1290.5738(1224.2188), Bit/dim 3.4888(best: 3.4941), Xent 2.3026, Loss 3.4888, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 21.1277(21.1557) | Bit/dim 3.5042(3.4946) | Xent 2.3026(2.3026) | Loss 3.5042(3.4946) | Error 0.8900(0.9008) Steps 862(853.88) | Grad Norm 4.9826(2.6854) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 21.0314(21.1638) | Bit/dim 3.4930(3.4908) | Xent 2.3026(2.3026) | Loss 3.4930(3.4908) | Error 0.9022(0.9005) Steps 820(852.75) | Grad Norm 2.3287(2.7144) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 22.2424(21.1778) | Bit/dim 3.4826(3.4903) | Xent 2.3026(2.3026) | Loss 3.4826(3.4903) | Error 0.9078(0.9014) Steps 880(854.46) | Grad Norm 2.1265(2.7425) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 21.0182(21.1981) | Bit/dim 3.5042(3.4926) | Xent 2.3026(2.3026) | Loss 3.5042(3.4926) | Error 0.9078(0.8998) Steps 862(853.59) | Grad Norm 2.3264(2.6256) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 21.4963(21.2591) | Bit/dim 3.5006(3.4907) | Xent 2.3026(2.3026) | Loss 3.5006(3.4907) | Error 0.8944(0.8991) Steps 844(852.83) | Grad Norm 1.6500(2.6778) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 20.4871(21.2028) | Bit/dim 3.5123(3.4932) | Xent 2.3026(2.3026) | Loss 3.5123(3.4932) | Error 0.9067(0.9007) Steps 838(852.59) | Grad Norm 2.2382(2.6666) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 108.8429, Epoch Time 1294.6713(1226.3323), Bit/dim 3.4895(best: 3.4888), Xent 2.3026, Loss 3.4895, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 21.4558(21.1953) | Bit/dim 3.4692(3.4957) | Xent 2.3026(2.3026) | Loss 3.4692(3.4957) | Error 0.9089(0.9014) Steps 856(852.88) | Grad Norm 3.0908(2.7106) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 21.1774(21.1997) | Bit/dim 3.4670(3.4948) | Xent 2.3026(2.3026) | Loss 3.4670(3.4948) | Error 0.9033(0.9012) Steps 850(853.45) | Grad Norm 3.1007(2.6738) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 21.6121(21.2016) | Bit/dim 3.5046(3.4904) | Xent 2.3026(2.3026) | Loss 3.5046(3.4904) | Error 0.8911(0.8999) Steps 862(854.15) | Grad Norm 2.2218(2.8106) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 20.9328(21.1603) | Bit/dim 3.4892(3.4912) | Xent 2.3026(2.3026) | Loss 3.4892(3.4912) | Error 0.8989(0.9005) Steps 838(849.83) | Grad Norm 2.8434(2.7698) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 21.9829(21.1694) | Bit/dim 3.5060(3.4891) | Xent 2.3026(2.3026) | Loss 3.5060(3.4891) | Error 0.8989(0.9001) Steps 862(850.39) | Grad Norm 2.4704(2.6736) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 110.4906, Epoch Time 1294.2191(1228.3689), Bit/dim 3.4915(best: 3.4888), Xent 2.3026, Loss 3.4915, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.9587(21.2587) | Bit/dim 3.4720(3.4847) | Xent 2.3026(2.3026) | Loss 3.4720(3.4847) | Error 0.8989(0.8997) Steps 850(853.04) | Grad Norm 2.4725(2.6253) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 21.2568(21.3289) | Bit/dim 3.4943(3.4879) | Xent 2.3026(2.3026) | Loss 3.4943(3.4879) | Error 0.9044(0.9003) Steps 862(854.25) | Grad Norm 3.1436(2.6323) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 21.6036(21.3023) | Bit/dim 3.4754(3.4875) | Xent 2.3026(2.3026) | Loss 3.4754(3.4875) | Error 0.8878(0.8994) Steps 850(855.03) | Grad Norm 3.3556(2.5788) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 21.3618(21.3680) | Bit/dim 3.4857(3.4868) | Xent 2.3026(2.3026) | Loss 3.4857(3.4868) | Error 0.9133(0.9002) Steps 838(854.65) | Grad Norm 3.4364(2.6486) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 21.9132(21.4163) | Bit/dim 3.4763(3.4875) | Xent 2.3026(2.3026) | Loss 3.4763(3.4875) | Error 0.8978(0.8994) Steps 844(855.91) | Grad Norm 1.9336(2.5521) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 21.5981(21.4991) | Bit/dim 3.4893(3.4878) | Xent 2.3026(2.3026) | Loss 3.4893(3.4878) | Error 0.9133(0.9006) Steps 862(857.28) | Grad Norm 2.9063(2.7102) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 110.4888, Epoch Time 1312.3494(1230.8884), Bit/dim 3.4897(best: 3.4888), Xent 2.3026, Loss 3.4897, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 21.2176(21.4230) | Bit/dim 3.5120(3.4887) | Xent 2.3026(2.3026) | Loss 3.5120(3.4887) | Error 0.9156(0.9008) Steps 856(856.67) | Grad Norm 3.3613(2.6972) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 20.9912(21.3466) | Bit/dim 3.4938(3.4889) | Xent 2.3026(2.3026) | Loss 3.4938(3.4889) | Error 0.8811(0.9020) Steps 868(858.82) | Grad Norm 2.2378(2.7386) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 21.6428(21.4061) | Bit/dim 3.4780(3.4857) | Xent 2.3026(2.3026) | Loss 3.4780(3.4857) | Error 0.9078(0.9011) Steps 868(861.10) | Grad Norm 2.7059(2.6483) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 21.7292(21.3914) | Bit/dim 3.4780(3.4877) | Xent 2.3026(2.3026) | Loss 3.4780(3.4877) | Error 0.8933(0.9000) Steps 868(863.22) | Grad Norm 3.1762(2.6215) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 21.6429(21.3771) | Bit/dim 3.4873(3.4863) | Xent 2.3026(2.3026) | Loss 3.4873(3.4863) | Error 0.9078(0.9006) Steps 862(863.14) | Grad Norm 3.0406(2.7537) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 108.6460, Epoch Time 1299.7776(1232.9550), Bit/dim 3.4856(best: 3.4888), Xent 2.3026, Loss 3.4856, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 21.6597(21.4056) | Bit/dim 3.4649(3.4843) | Xent 2.3026(2.3026) | Loss 3.4649(3.4843) | Error 0.9122(0.9006) Steps 838(862.71) | Grad Norm 2.1762(2.5981) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 20.7320(21.3760) | Bit/dim 3.4443(3.4859) | Xent 2.3026(2.3026) | Loss 3.4443(3.4859) | Error 0.9089(0.9012) Steps 868(862.88) | Grad Norm 3.4047(2.6519) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 21.9086(21.4187) | Bit/dim 3.4887(3.4860) | Xent 2.3026(2.3026) | Loss 3.4887(3.4860) | Error 0.8889(0.8993) Steps 874(863.26) | Grad Norm 2.6169(2.6339) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 21.7437(21.4316) | Bit/dim 3.4886(3.4857) | Xent 2.3026(2.3026) | Loss 3.4886(3.4857) | Error 0.8878(0.8992) Steps 868(862.79) | Grad Norm 3.2270(2.6518) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 21.4082(21.4771) | Bit/dim 3.4715(3.4834) | Xent 2.3026(2.3026) | Loss 3.4715(3.4834) | Error 0.9156(0.9002) Steps 868(863.77) | Grad Norm 3.0177(2.5451) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 21.6955(21.4550) | Bit/dim 3.4834(3.4837) | Xent 2.3026(2.3026) | Loss 3.4834(3.4837) | Error 0.8989(0.9001) Steps 838(865.07) | Grad Norm 3.6804(2.6660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 109.3211, Epoch Time 1308.2254(1235.2131), Bit/dim 3.4965(best: 3.4856), Xent 2.3026, Loss 3.4965, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 21.0468(21.4182) | Bit/dim 3.5060(3.4882) | Xent 2.3026(2.3026) | Loss 3.5060(3.4882) | Error 0.9144(0.9010) Steps 838(860.71) | Grad Norm 2.7583(2.6173) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 21.5955(21.4246) | Bit/dim 3.5119(3.4871) | Xent 2.3026(2.3026) | Loss 3.5119(3.4871) | Error 0.9222(0.9009) Steps 874(863.17) | Grad Norm 2.4109(2.6152) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 22.5545(21.4873) | Bit/dim 3.4634(3.4862) | Xent 2.3026(2.3026) | Loss 3.4634(3.4862) | Error 0.8822(0.9016) Steps 880(863.16) | Grad Norm 3.0258(2.6673) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 21.3496(21.5200) | Bit/dim 3.4820(3.4838) | Xent 2.3026(2.3026) | Loss 3.4820(3.4838) | Error 0.9111(0.9012) Steps 874(865.44) | Grad Norm 2.1558(2.5839) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 21.3272(21.5330) | Bit/dim 3.4533(3.4816) | Xent 2.3026(2.3026) | Loss 3.4533(3.4816) | Error 0.8689(0.8992) Steps 874(865.17) | Grad Norm 2.1095(2.7342) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 110.8999, Epoch Time 1310.9785(1237.4861), Bit/dim 3.4879(best: 3.4856), Xent 2.3026, Loss 3.4879, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 22.1056(21.5101) | Bit/dim 3.4842(3.4811) | Xent 2.3026(2.3026) | Loss 3.4842(3.4811) | Error 0.8878(0.8999) Steps 874(865.28) | Grad Norm 3.3525(2.7513) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 22.0702(21.5545) | Bit/dim 3.4594(3.4780) | Xent 2.3026(2.3026) | Loss 3.4594(3.4780) | Error 0.8867(0.8988) Steps 844(865.86) | Grad Norm 2.6652(2.6705) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 21.3402(21.5462) | Bit/dim 3.4898(3.4808) | Xent 2.3026(2.3026) | Loss 3.4898(3.4808) | Error 0.9044(0.8985) Steps 874(865.49) | Grad Norm 2.7364(2.7260) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 21.6423(21.5233) | Bit/dim 3.4786(3.4821) | Xent 2.3026(2.3026) | Loss 3.4786(3.4821) | Error 0.9011(0.8997) Steps 874(868.02) | Grad Norm 1.6929(2.4918) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 21.3170(21.4478) | Bit/dim 3.4618(3.4827) | Xent 2.3026(2.3026) | Loss 3.4618(3.4827) | Error 0.8978(0.8996) Steps 874(868.58) | Grad Norm 1.4894(2.4915) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 21.7774(21.5022) | Bit/dim 3.4523(3.4798) | Xent 2.3026(2.3026) | Loss 3.4523(3.4798) | Error 0.9033(0.9002) Steps 850(867.86) | Grad Norm 3.9409(2.5085) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 110.9229, Epoch Time 1312.4613(1239.7354), Bit/dim 3.4866(best: 3.4856), Xent 2.3026, Loss 3.4866, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 21.7141(21.4930) | Bit/dim 3.4870(3.4823) | Xent 2.3026(2.3026) | Loss 3.4870(3.4823) | Error 0.9067(0.9007) Steps 880(868.29) | Grad Norm 2.8887(2.5147) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 21.7595(21.4984) | Bit/dim 3.5038(3.4806) | Xent 2.3026(2.3026) | Loss 3.5038(3.4806) | Error 0.9011(0.8993) Steps 874(869.50) | Grad Norm 2.7081(2.7116) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 21.6557(21.5331) | Bit/dim 3.4890(3.4826) | Xent 2.3026(2.3026) | Loss 3.4890(3.4826) | Error 0.9167(0.8993) Steps 880(868.85) | Grad Norm 2.2881(2.6781) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 21.4490(21.5186) | Bit/dim 3.4528(3.4778) | Xent 2.3026(2.3026) | Loss 3.4528(3.4778) | Error 0.8989(0.8998) Steps 868(868.51) | Grad Norm 1.8419(2.7168) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 21.3094(21.4854) | Bit/dim 3.4454(3.4783) | Xent 2.3026(2.3026) | Loss 3.4454(3.4783) | Error 0.8889(0.9008) Steps 886(868.04) | Grad Norm 1.5796(2.5918) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 110.9678, Epoch Time 1311.6872(1241.8939), Bit/dim 3.4843(best: 3.4856), Xent 2.3026, Loss 3.4843, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 21.9891(21.5082) | Bit/dim 3.4798(3.4773) | Xent 2.3026(2.3026) | Loss 3.4798(3.4773) | Error 0.9033(0.8999) Steps 874(869.57) | Grad Norm 1.5568(2.6946) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 21.1690(21.4306) | Bit/dim 3.5051(3.4776) | Xent 2.3026(2.3026) | Loss 3.5051(3.4776) | Error 0.9211(0.9012) Steps 862(867.23) | Grad Norm 1.8159(2.6618) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 21.5225(21.3812) | Bit/dim 3.4560(3.4745) | Xent 2.3026(2.3026) | Loss 3.4560(3.4745) | Error 0.8878(0.9005) Steps 868(866.81) | Grad Norm 1.8921(2.6027) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 22.5480(21.4106) | Bit/dim 3.5099(3.4769) | Xent 2.3026(2.3026) | Loss 3.5099(3.4769) | Error 0.8944(0.8996) Steps 874(865.44) | Grad Norm 2.4835(2.4433) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 21.5776(21.4872) | Bit/dim 3.4586(3.4813) | Xent 2.3026(2.3026) | Loss 3.4586(3.4813) | Error 0.9022(0.9005) Steps 874(867.14) | Grad Norm 3.2810(2.4558) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 21.6982(21.5239) | Bit/dim 3.5208(3.4810) | Xent 2.3026(2.3026) | Loss 3.5208(3.4810) | Error 0.9044(0.8996) Steps 868(867.74) | Grad Norm 3.3877(2.7227) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 109.7648, Epoch Time 1308.3414(1243.8873), Bit/dim 3.4801(best: 3.4843), Xent 2.3026, Loss 3.4801, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 21.3108(21.5439) | Bit/dim 3.4907(3.4814) | Xent 2.3026(2.3026) | Loss 3.4907(3.4814) | Error 0.8978(0.8990) Steps 856(866.49) | Grad Norm 2.9527(2.6008) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 21.9662(21.5534) | Bit/dim 3.4799(3.4801) | Xent 2.3026(2.3026) | Loss 3.4799(3.4801) | Error 0.8944(0.8984) Steps 856(867.33) | Grad Norm 2.2437(2.6864) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 22.1543(21.6336) | Bit/dim 3.4990(3.4810) | Xent 2.3026(2.3026) | Loss 3.4990(3.4810) | Error 0.9189(0.9004) Steps 868(867.53) | Grad Norm 3.2384(2.7618) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 21.5292(21.6182) | Bit/dim 3.4805(3.4798) | Xent 2.3026(2.3026) | Loss 3.4805(3.4798) | Error 0.8989(0.9005) Steps 862(866.97) | Grad Norm 1.7344(2.7015) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 21.6596(21.6459) | Bit/dim 3.4844(3.4756) | Xent 2.3026(2.3026) | Loss 3.4844(3.4756) | Error 0.9044(0.8998) Steps 880(868.31) | Grad Norm 1.3307(2.6204) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 110.6183, Epoch Time 1320.4214(1246.1834), Bit/dim 3.4765(best: 3.4801), Xent 2.3026, Loss 3.4765, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 21.2823(21.6262) | Bit/dim 3.4450(3.4761) | Xent 2.3026(2.3026) | Loss 3.4450(3.4761) | Error 0.8967(0.9003) Steps 862(868.99) | Grad Norm 2.6197(2.5123) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 20.6580(21.5880) | Bit/dim 3.4876(3.4753) | Xent 2.3026(2.3026) | Loss 3.4876(3.4753) | Error 0.9089(0.9003) Steps 874(869.83) | Grad Norm 3.1421(2.5919) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 21.4665(21.5657) | Bit/dim 3.4384(3.4755) | Xent 2.3026(2.3026) | Loss 3.4384(3.4755) | Error 0.8933(0.9009) Steps 874(867.42) | Grad Norm 1.5728(2.4122) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 21.7950(21.5576) | Bit/dim 3.4531(3.4753) | Xent 2.3026(2.3026) | Loss 3.4531(3.4753) | Error 0.8878(0.8994) Steps 880(867.46) | Grad Norm 4.1614(2.4512) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 22.7247(21.5967) | Bit/dim 3.4911(3.4746) | Xent 2.3026(2.3026) | Loss 3.4911(3.4746) | Error 0.8767(0.8981) Steps 886(867.83) | Grad Norm 2.3088(2.5227) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 21.4047(21.6470) | Bit/dim 3.5098(3.4763) | Xent 2.3026(2.3026) | Loss 3.5098(3.4763) | Error 0.9178(0.9005) Steps 850(867.69) | Grad Norm 1.7358(2.5972) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 110.2690, Epoch Time 1316.1468(1248.2823), Bit/dim 3.4765(best: 3.4765), Xent 2.3026, Loss 3.4765, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 21.6039(21.6253) | Bit/dim 3.5111(3.4766) | Xent 2.3026(2.3026) | Loss 3.5111(3.4766) | Error 0.9122(0.9005) Steps 874(866.15) | Grad Norm 2.3832(2.5345) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 22.0568(21.6660) | Bit/dim 3.4360(3.4742) | Xent 2.3026(2.3026) | Loss 3.4360(3.4742) | Error 0.8989(0.9007) Steps 868(867.40) | Grad Norm 2.4515(2.5425) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 21.6800(21.6455) | Bit/dim 3.4662(3.4738) | Xent 2.3026(2.3026) | Loss 3.4662(3.4738) | Error 0.8989(0.9000) Steps 874(866.84) | Grad Norm 2.9862(2.5691) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 21.6349(21.7005) | Bit/dim 3.4851(3.4747) | Xent 2.3026(2.3026) | Loss 3.4851(3.4747) | Error 0.8967(0.9000) Steps 868(868.39) | Grad Norm 2.7294(2.4681) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 21.8921(21.7104) | Bit/dim 3.4576(3.4726) | Xent 2.3026(2.3026) | Loss 3.4576(3.4726) | Error 0.9078(0.9001) Steps 862(867.74) | Grad Norm 1.1715(2.3454) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 111.3003, Epoch Time 1324.6520(1250.5734), Bit/dim 3.4779(best: 3.4765), Xent 2.3026, Loss 3.4779, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 21.8683(21.7510) | Bit/dim 3.4952(3.4753) | Xent 2.3026(2.3026) | Loss 3.4952(3.4753) | Error 0.8978(0.9001) Steps 868(867.16) | Grad Norm 3.5851(2.4789) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 22.0295(21.7558) | Bit/dim 3.4673(3.4745) | Xent 2.3026(2.3026) | Loss 3.4673(3.4745) | Error 0.8833(0.8992) Steps 880(869.14) | Grad Norm 2.7959(2.5139) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 22.0326(21.6880) | Bit/dim 3.4722(3.4752) | Xent 2.3026(2.3026) | Loss 3.4722(3.4752) | Error 0.9033(0.8993) Steps 850(868.24) | Grad Norm 2.1559(2.5776) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 22.1377(21.6870) | Bit/dim 3.4654(3.4756) | Xent 2.3026(2.3026) | Loss 3.4654(3.4756) | Error 0.8778(0.8993) Steps 874(866.64) | Grad Norm 1.9428(2.6879) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 21.9731(21.7146) | Bit/dim 3.4619(3.4731) | Xent 2.3026(2.3026) | Loss 3.4619(3.4731) | Error 0.9133(0.8997) Steps 874(867.35) | Grad Norm 2.5262(2.6493) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 21.6875(21.6511) | Bit/dim 3.4791(3.4719) | Xent 2.3026(2.3026) | Loss 3.4791(3.4719) | Error 0.8978(0.9002) Steps 880(866.73) | Grad Norm 2.3197(2.5114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 110.3016, Epoch Time 1320.1961(1252.6620), Bit/dim 3.4756(best: 3.4765), Xent 2.3026, Loss 3.4756, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 22.1602(21.6930) | Bit/dim 3.4946(3.4727) | Xent 2.3026(2.3026) | Loss 3.4946(3.4727) | Error 0.8922(0.8996) Steps 868(867.73) | Grad Norm 3.4762(2.7308) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 21.8444(21.6932) | Bit/dim 3.5112(3.4722) | Xent 2.3026(2.3026) | Loss 3.5112(3.4722) | Error 0.9033(0.8996) Steps 868(867.25) | Grad Norm 3.2151(2.7187) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 21.4919(21.6872) | Bit/dim 3.4975(3.4746) | Xent 2.3026(2.3026) | Loss 3.4975(3.4746) | Error 0.9111(0.8997) Steps 886(867.07) | Grad Norm 1.2370(2.5598) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 22.0859(21.6862) | Bit/dim 3.4566(3.4736) | Xent 2.3026(2.3026) | Loss 3.4566(3.4736) | Error 0.8944(0.9005) Steps 856(866.27) | Grad Norm 2.2458(2.5767) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 21.8003(21.6368) | Bit/dim 3.5066(3.4725) | Xent 2.3026(2.3026) | Loss 3.5066(3.4725) | Error 0.9022(0.8998) Steps 862(866.55) | Grad Norm 3.3327(2.5122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 110.3169, Epoch Time 1321.0936(1254.7150), Bit/dim 3.4765(best: 3.4756), Xent 2.3026, Loss 3.4765, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 21.5767(21.6502) | Bit/dim 3.4839(3.4746) | Xent 2.3026(2.3026) | Loss 3.4839(3.4746) | Error 0.8944(0.9001) Steps 856(868.00) | Grad Norm 1.5628(2.5734) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 21.7414(21.6174) | Bit/dim 3.4581(3.4720) | Xent 2.3026(2.3026) | Loss 3.4581(3.4720) | Error 0.8956(0.8994) Steps 862(867.92) | Grad Norm 3.2645(2.4688) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 21.3138(21.6317) | Bit/dim 3.4704(3.4732) | Xent 2.3026(2.3026) | Loss 3.4704(3.4732) | Error 0.9056(0.9003) Steps 880(868.09) | Grad Norm 2.9575(2.5896) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 21.5814(21.5420) | Bit/dim 3.4855(3.4711) | Xent 2.3026(2.3026) | Loss 3.4855(3.4711) | Error 0.8900(0.8989) Steps 862(867.20) | Grad Norm 1.1239(2.5985) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 21.6570(21.5799) | Bit/dim 3.4823(3.4713) | Xent 2.3026(2.3026) | Loss 3.4823(3.4713) | Error 0.9122(0.8990) Steps 868(867.55) | Grad Norm 3.9494(2.6714) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 22.8432(21.6602) | Bit/dim 3.4636(3.4718) | Xent 2.3026(2.3026) | Loss 3.4636(3.4718) | Error 0.8978(0.9004) Steps 886(868.40) | Grad Norm 1.7893(2.5276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 109.0608, Epoch Time 1315.3491(1256.5340), Bit/dim 3.4732(best: 3.4756), Xent 2.3026, Loss 3.4732, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 21.6606(21.6756) | Bit/dim 3.4772(3.4720) | Xent 2.3026(2.3026) | Loss 3.4772(3.4720) | Error 0.9044(0.9009) Steps 874(866.17) | Grad Norm 3.8503(2.4098) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 21.6916(21.6772) | Bit/dim 3.4795(3.4727) | Xent 2.3026(2.3026) | Loss 3.4795(3.4727) | Error 0.8900(0.9012) Steps 880(869.28) | Grad Norm 3.5882(2.6316) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 21.9299(21.6616) | Bit/dim 3.4635(3.4704) | Xent 2.3026(2.3026) | Loss 3.4635(3.4704) | Error 0.9033(0.8992) Steps 898(871.15) | Grad Norm 1.5110(2.5839) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 22.0234(21.6824) | Bit/dim 3.5060(3.4716) | Xent 2.3026(2.3026) | Loss 3.5060(3.4716) | Error 0.8933(0.8995) Steps 844(868.05) | Grad Norm 1.6938(2.5266) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 20.9832(21.6182) | Bit/dim 3.4581(3.4714) | Xent 2.3026(2.3026) | Loss 3.4581(3.4714) | Error 0.8989(0.9007) Steps 874(867.44) | Grad Norm 1.9110(2.5191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 110.9238, Epoch Time 1317.9593(1258.3768), Bit/dim 3.4687(best: 3.4732), Xent 2.3026, Loss 3.4687, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 21.5385(21.6053) | Bit/dim 3.4653(3.4695) | Xent 2.3026(2.3026) | Loss 3.4653(3.4695) | Error 0.9244(0.9003) Steps 862(867.90) | Grad Norm 1.8238(2.3021) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 21.4778(21.5934) | Bit/dim 3.4886(3.4663) | Xent 2.3026(2.3026) | Loss 3.4886(3.4663) | Error 0.8911(0.8991) Steps 862(866.90) | Grad Norm 3.1391(2.3231) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 22.1184(21.5966) | Bit/dim 3.4291(3.4651) | Xent 2.3026(2.3026) | Loss 3.4291(3.4651) | Error 0.8956(0.8990) Steps 886(868.06) | Grad Norm 1.6259(2.4619) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 21.6208(21.6364) | Bit/dim 3.4592(3.4658) | Xent 2.3026(2.3026) | Loss 3.4592(3.4658) | Error 0.9200(0.8993) Steps 868(869.62) | Grad Norm 1.8252(2.4420) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 21.5672(21.6458) | Bit/dim 3.4663(3.4675) | Xent 2.3026(2.3026) | Loss 3.4663(3.4675) | Error 0.8989(0.8993) Steps 862(868.49) | Grad Norm 3.2747(2.5072) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 21.9376(21.7039) | Bit/dim 3.4977(3.4708) | Xent 2.3026(2.3026) | Loss 3.4977(3.4708) | Error 0.8967(0.9008) Steps 874(868.09) | Grad Norm 1.6128(2.4561) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 110.6609, Epoch Time 1322.3213(1260.2951), Bit/dim 3.4693(best: 3.4687), Xent 2.3026, Loss 3.4693, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 22.3375(21.7374) | Bit/dim 3.5056(3.4694) | Xent 2.3026(2.3026) | Loss 3.5056(3.4694) | Error 0.9056(0.9008) Steps 880(868.89) | Grad Norm 3.1571(2.5501) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 21.9361(21.6959) | Bit/dim 3.4680(3.4702) | Xent 2.3026(2.3026) | Loss 3.4680(3.4702) | Error 0.9122(0.9009) Steps 862(868.32) | Grad Norm 2.0976(2.4314) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 22.0112(21.6884) | Bit/dim 3.4414(3.4689) | Xent 2.3026(2.3026) | Loss 3.4414(3.4689) | Error 0.9033(0.9020) Steps 868(866.25) | Grad Norm 3.2787(2.6002) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 21.4663(21.6501) | Bit/dim 3.4725(3.4675) | Xent 2.3026(2.3026) | Loss 3.4725(3.4675) | Error 0.9256(0.9010) Steps 874(865.98) | Grad Norm 2.3430(2.6130) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 21.9256(21.6809) | Bit/dim 3.4807(3.4672) | Xent 2.3026(2.3026) | Loss 3.4807(3.4672) | Error 0.8978(0.9007) Steps 868(866.42) | Grad Norm 2.6595(2.5134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 109.4154, Epoch Time 1320.9023(1262.1133), Bit/dim 3.4671(best: 3.4687), Xent 2.3026, Loss 3.4671, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 22.1217(21.7337) | Bit/dim 3.4609(3.4681) | Xent 2.3026(2.3026) | Loss 3.4609(3.4681) | Error 0.8822(0.8991) Steps 862(868.81) | Grad Norm 2.0388(2.4995) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 21.8963(21.7385) | Bit/dim 3.4610(3.4691) | Xent 2.3026(2.3026) | Loss 3.4610(3.4691) | Error 0.8989(0.8992) Steps 862(867.52) | Grad Norm 2.4934(2.6393) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 22.1585(21.7328) | Bit/dim 3.4714(3.4664) | Xent 2.3026(2.3026) | Loss 3.4714(3.4664) | Error 0.8944(0.8981) Steps 886(868.44) | Grad Norm 2.8280(2.7823) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 22.0342(21.7552) | Bit/dim 3.4594(3.4667) | Xent 2.3026(2.3026) | Loss 3.4594(3.4667) | Error 0.9089(0.8989) Steps 868(868.11) | Grad Norm 2.8409(2.6857) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 22.2756(21.7721) | Bit/dim 3.4642(3.4655) | Xent 2.3026(2.3026) | Loss 3.4642(3.4655) | Error 0.8911(0.8997) Steps 886(870.09) | Grad Norm 2.7422(2.6760) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 21.4754(21.7550) | Bit/dim 3.4927(3.4678) | Xent 2.3026(2.3026) | Loss 3.4927(3.4678) | Error 0.9144(0.9009) Steps 880(871.37) | Grad Norm 1.3453(2.4113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 110.0172, Epoch Time 1326.0979(1264.0329), Bit/dim 3.4645(best: 3.4671), Xent 2.3026, Loss 3.4645, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 21.6670(21.7738) | Bit/dim 3.4690(3.4674) | Xent 2.3026(2.3026) | Loss 3.4690(3.4674) | Error 0.8978(0.8994) Steps 880(873.07) | Grad Norm 2.7677(2.5150) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 21.5997(21.7258) | Bit/dim 3.4556(3.4660) | Xent 2.3026(2.3026) | Loss 3.4556(3.4660) | Error 0.9033(0.9007) Steps 844(871.70) | Grad Norm 2.8396(2.5145) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 21.2126(21.7437) | Bit/dim 3.4193(3.4643) | Xent 2.3026(2.3026) | Loss 3.4193(3.4643) | Error 0.9089(0.9003) Steps 862(870.39) | Grad Norm 2.8238(2.4618) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 21.4960(21.7189) | Bit/dim 3.4848(3.4672) | Xent 2.3026(2.3026) | Loss 3.4848(3.4672) | Error 0.8889(0.9010) Steps 856(869.99) | Grad Norm 1.5292(2.4985) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 22.4581(21.8099) | Bit/dim 3.4614(3.4657) | Xent 2.3026(2.3026) | Loss 3.4614(3.4657) | Error 0.9100(0.9004) Steps 886(871.26) | Grad Norm 2.2862(2.5614) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 109.6598, Epoch Time 1326.3374(1265.9020), Bit/dim 3.4675(best: 3.4645), Xent 2.3026, Loss 3.4675, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 21.7739(21.8009) | Bit/dim 3.4355(3.4652) | Xent 2.3026(2.3026) | Loss 3.4355(3.4652) | Error 0.9078(0.9004) Steps 862(870.36) | Grad Norm 1.6850(2.4266) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 22.5477(21.8318) | Bit/dim 3.4900(3.4646) | Xent 2.3026(2.3026) | Loss 3.4900(3.4646) | Error 0.9011(0.9005) Steps 868(870.77) | Grad Norm 3.6633(2.4887) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 22.6839(21.8138) | Bit/dim 3.4879(3.4662) | Xent 2.3026(2.3026) | Loss 3.4879(3.4662) | Error 0.8933(0.9008) Steps 868(870.21) | Grad Norm 3.3659(2.4944) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 21.4094(21.8453) | Bit/dim 3.4664(3.4655) | Xent 2.3026(2.3026) | Loss 3.4664(3.4655) | Error 0.9022(0.8996) Steps 880(871.71) | Grad Norm 1.9466(2.6386) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 21.4407(21.8316) | Bit/dim 3.4628(3.4655) | Xent 2.3026(2.3026) | Loss 3.4628(3.4655) | Error 0.9044(0.8991) Steps 880(874.05) | Grad Norm 3.2864(2.5337) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 22.3356(21.8287) | Bit/dim 3.4270(3.4636) | Xent 2.3026(2.3026) | Loss 3.4270(3.4636) | Error 0.9022(0.8997) Steps 880(873.81) | Grad Norm 1.7953(2.4835) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 109.5417, Epoch Time 1329.0119(1267.7953), Bit/dim 3.4644(best: 3.4645), Xent 2.3026, Loss 3.4644, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 21.4687(21.8174) | Bit/dim 3.4497(3.4649) | Xent 2.3026(2.3026) | Loss 3.4497(3.4649) | Error 0.8944(0.9001) Steps 886(874.54) | Grad Norm 3.6766(2.4386) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 22.0127(21.7752) | Bit/dim 3.4469(3.4622) | Xent 2.3026(2.3026) | Loss 3.4469(3.4622) | Error 0.9111(0.8998) Steps 886(875.71) | Grad Norm 1.5370(2.4702) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 21.8772(21.7587) | Bit/dim 3.4679(3.4637) | Xent 2.3026(2.3026) | Loss 3.4679(3.4637) | Error 0.9000(0.9006) Steps 880(876.22) | Grad Norm 2.3391(2.3974) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 21.7280(21.7871) | Bit/dim 3.4748(3.4639) | Xent 2.3026(2.3026) | Loss 3.4748(3.4639) | Error 0.9133(0.9007) Steps 856(874.74) | Grad Norm 1.7946(2.4462) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 21.3012(21.6948) | Bit/dim 3.4614(3.4620) | Xent 2.3026(2.3026) | Loss 3.4614(3.4620) | Error 0.8844(0.8995) Steps 898(874.92) | Grad Norm 1.8866(2.4191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 108.5750, Epoch Time 1320.8260(1269.3862), Bit/dim 3.4610(best: 3.4644), Xent 2.3026, Loss 3.4610, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 21.5173(21.6984) | Bit/dim 3.4704(3.4619) | Xent 2.3026(2.3026) | Loss 3.4704(3.4619) | Error 0.9000(0.8995) Steps 856(874.14) | Grad Norm 4.1408(2.3815) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 21.7231(21.7391) | Bit/dim 3.4686(3.4624) | Xent 2.3026(2.3026) | Loss 3.4686(3.4624) | Error 0.8922(0.8979) Steps 886(875.57) | Grad Norm 2.6042(2.4834) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 22.3930(21.7476) | Bit/dim 3.4596(3.4627) | Xent 2.3026(2.3026) | Loss 3.4596(3.4627) | Error 0.8889(0.8983) Steps 892(876.39) | Grad Norm 3.2546(2.4696) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 22.0245(21.7635) | Bit/dim 3.4722(3.4615) | Xent 2.3026(2.3026) | Loss 3.4722(3.4615) | Error 0.9044(0.8983) Steps 874(875.00) | Grad Norm 2.0415(2.4322) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 22.2685(21.8413) | Bit/dim 3.4773(3.4628) | Xent 2.3026(2.3026) | Loss 3.4773(3.4628) | Error 0.9044(0.9005) Steps 874(876.24) | Grad Norm 2.4852(2.5398) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 21.5854(21.8302) | Bit/dim 3.4544(3.4615) | Xent 2.3026(2.3026) | Loss 3.4544(3.4615) | Error 0.9100(0.9012) Steps 880(877.91) | Grad Norm 1.7478(2.5210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 109.1292, Epoch Time 1328.0926(1271.1474), Bit/dim 3.4602(best: 3.4610), Xent 2.3026, Loss 3.4602, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 22.0093(21.8667) | Bit/dim 3.4659(3.4621) | Xent 2.3026(2.3026) | Loss 3.4659(3.4621) | Error 0.8978(0.9003) Steps 886(879.10) | Grad Norm 2.9095(2.4661) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 22.2537(21.8242) | Bit/dim 3.4538(3.4617) | Xent 2.3026(2.3026) | Loss 3.4538(3.4617) | Error 0.8967(0.8997) Steps 868(877.55) | Grad Norm 2.6969(2.4010) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 21.2228(21.8038) | Bit/dim 3.4386(3.4621) | Xent 2.3026(2.3026) | Loss 3.4386(3.4621) | Error 0.8911(0.9003) Steps 874(878.80) | Grad Norm 3.0416(2.5392) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 21.8106(21.7698) | Bit/dim 3.4904(3.4615) | Xent 2.3026(2.3026) | Loss 3.4904(3.4615) | Error 0.9222(0.9012) Steps 886(878.20) | Grad Norm 2.9622(2.4822) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 21.7532(21.7752) | Bit/dim 3.4445(3.4613) | Xent 2.3026(2.3026) | Loss 3.4445(3.4613) | Error 0.8856(0.9012) Steps 880(877.89) | Grad Norm 2.3615(2.5284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 108.9983, Epoch Time 1325.6118(1272.7813), Bit/dim 3.4686(best: 3.4602), Xent 2.3026, Loss 3.4686, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 21.4399(21.7551) | Bit/dim 3.4892(3.4639) | Xent 2.3026(2.3026) | Loss 3.4892(3.4639) | Error 0.9122(0.9005) Steps 874(875.15) | Grad Norm 2.2769(2.5813) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 22.3147(21.8320) | Bit/dim 3.4451(3.4602) | Xent 2.3026(2.3026) | Loss 3.4451(3.4602) | Error 0.8867(0.8991) Steps 862(874.82) | Grad Norm 2.0960(2.4590) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 21.8130(21.8270) | Bit/dim 3.4419(3.4595) | Xent 2.3026(2.3026) | Loss 3.4419(3.4595) | Error 0.8944(0.8991) Steps 880(876.62) | Grad Norm 3.0032(2.6069) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 22.4683(21.8556) | Bit/dim 3.5204(3.4602) | Xent 2.3026(2.3026) | Loss 3.5204(3.4602) | Error 0.9167(0.9010) Steps 892(876.56) | Grad Norm 2.8519(2.5490) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 21.4209(21.7948) | Bit/dim 3.4433(3.4605) | Xent 2.3026(2.3026) | Loss 3.4433(3.4605) | Error 0.9056(0.9009) Steps 880(876.96) | Grad Norm 1.5225(2.5764) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 22.0236(21.8230) | Bit/dim 3.4295(3.4602) | Xent 2.3026(2.3026) | Loss 3.4295(3.4602) | Error 0.8856(0.9002) Steps 874(877.15) | Grad Norm 1.9526(2.5296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 109.6202, Epoch Time 1329.1091(1274.4712), Bit/dim 3.4601(best: 3.4602), Xent 2.3026, Loss 3.4601, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 21.6339(21.7940) | Bit/dim 3.4741(3.4587) | Xent 2.3026(2.3026) | Loss 3.4741(3.4587) | Error 0.8856(0.9002) Steps 886(878.58) | Grad Norm 1.7910(2.5472) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 22.5988(21.8809) | Bit/dim 3.4514(3.4604) | Xent 2.3026(2.3026) | Loss 3.4514(3.4604) | Error 0.8844(0.8996) Steps 880(878.02) | Grad Norm 2.9873(2.6122) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 22.1773(21.8478) | Bit/dim 3.4578(3.4603) | Xent 2.3026(2.3026) | Loss 3.4578(3.4603) | Error 0.9089(0.8992) Steps 892(878.40) | Grad Norm 2.7101(2.5569) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 21.7840(21.8023) | Bit/dim 3.4495(3.4608) | Xent 2.3026(2.3026) | Loss 3.4495(3.4608) | Error 0.8944(0.8997) Steps 874(877.31) | Grad Norm 2.8563(2.4959) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 21.5711(21.7024) | Bit/dim 3.4605(3.4599) | Xent 2.3026(2.3026) | Loss 3.4605(3.4599) | Error 0.9133(0.9008) Steps 880(876.96) | Grad Norm 2.2065(2.6076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 109.3024, Epoch Time 1325.6460(1276.0064), Bit/dim 3.4576(best: 3.4601), Xent 2.3026, Loss 3.4576, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 22.0443(21.7728) | Bit/dim 3.4438(3.4591) | Xent 2.3026(2.3026) | Loss 3.4438(3.4591) | Error 0.9022(0.9011) Steps 880(876.72) | Grad Norm 2.9851(2.5045) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 22.1085(21.8398) | Bit/dim 3.4447(3.4581) | Xent 2.3026(2.3026) | Loss 3.4447(3.4581) | Error 0.9067(0.9017) Steps 874(877.38) | Grad Norm 1.9213(2.5498) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 21.9932(21.8429) | Bit/dim 3.4430(3.4587) | Xent 2.3026(2.3026) | Loss 3.4430(3.4587) | Error 0.8944(0.9014) Steps 886(878.56) | Grad Norm 2.4172(2.5118) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 22.3265(21.8311) | Bit/dim 3.4689(3.4544) | Xent 2.3026(2.3026) | Loss 3.4689(3.4544) | Error 0.8856(0.9000) Steps 874(877.82) | Grad Norm 2.5006(2.4582) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 22.2706(21.8773) | Bit/dim 3.4765(3.4559) | Xent 2.3026(2.3026) | Loss 3.4765(3.4559) | Error 0.8944(0.8987) Steps 850(876.20) | Grad Norm 2.2340(2.4615) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 21.8911(21.8772) | Bit/dim 3.4554(3.4582) | Xent 2.3026(2.3026) | Loss 3.4554(3.4582) | Error 0.8978(0.8991) Steps 886(876.72) | Grad Norm 2.6169(2.4848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 107.7752, Epoch Time 1330.2260(1277.6330), Bit/dim 3.4614(best: 3.4576), Xent 2.3026, Loss 3.4614, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 21.8068(21.8357) | Bit/dim 3.4266(3.4582) | Xent 2.3026(2.3026) | Loss 3.4266(3.4582) | Error 0.9133(0.8994) Steps 886(876.94) | Grad Norm 1.9301(2.4920) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 21.4787(21.8176) | Bit/dim 3.5070(3.4591) | Xent 2.3026(2.3026) | Loss 3.5070(3.4591) | Error 0.8844(0.8989) Steps 880(878.01) | Grad Norm 2.5095(2.5225) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 21.7964(21.8173) | Bit/dim 3.4603(3.4563) | Xent 2.3026(2.3026) | Loss 3.4603(3.4563) | Error 0.9189(0.8988) Steps 874(878.86) | Grad Norm 1.1923(2.4941) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 21.6689(21.8500) | Bit/dim 3.4903(3.4580) | Xent 2.3026(2.3026) | Loss 3.4903(3.4580) | Error 0.9000(0.8986) Steps 868(879.08) | Grad Norm 1.8707(2.2946) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 21.7300(21.8496) | Bit/dim 3.4676(3.4577) | Xent 2.3026(2.3026) | Loss 3.4676(3.4577) | Error 0.9067(0.8999) Steps 880(880.38) | Grad Norm 3.2844(2.3778) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 109.8615, Epoch Time 1329.5879(1279.1917), Bit/dim 3.4625(best: 3.4576), Xent 2.3026, Loss 3.4625, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 21.5431(21.8647) | Bit/dim 3.4965(3.4565) | Xent 2.3026(2.3026) | Loss 3.4965(3.4565) | Error 0.9100(0.8999) Steps 862(879.58) | Grad Norm 3.6680(2.4043) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 21.9070(21.8535) | Bit/dim 3.4329(3.4577) | Xent 2.3026(2.3026) | Loss 3.4329(3.4577) | Error 0.8900(0.9007) Steps 886(881.02) | Grad Norm 1.5191(2.4803) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 21.8028(21.8601) | Bit/dim 3.4520(3.4559) | Xent 2.3026(2.3026) | Loss 3.4520(3.4559) | Error 0.8933(0.8988) Steps 868(880.39) | Grad Norm 2.4567(2.3494) | Total Time 14.00(14.00)\n",
      "Iter 6690 | Time 22.4698(21.8256) | Bit/dim 3.4520(3.4555) | Xent 2.3026(2.3026) | Loss 3.4520(3.4555) | Error 0.8922(0.8992) Steps 856(879.70) | Grad Norm 2.6509(2.4343) | Total Time 14.00(14.00)\n",
      "Iter 6700 | Time 22.3726(21.8235) | Bit/dim 3.4342(3.4548) | Xent 2.3026(2.3026) | Loss 3.4342(3.4548) | Error 0.8967(0.8998) Steps 880(879.38) | Grad Norm 2.8424(2.4249) | Total Time 14.00(14.00)\n",
      "Iter 6710 | Time 22.0461(21.8221) | Bit/dim 3.4475(3.4559) | Xent 2.3026(2.3026) | Loss 3.4475(3.4559) | Error 0.8922(0.9007) Steps 850(876.91) | Grad Norm 2.1097(2.5839) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 108.4354, Epoch Time 1325.9304(1280.5938), Bit/dim 3.4556(best: 3.4576), Xent 2.3026, Loss 3.4556, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 22.2561(21.8356) | Bit/dim 3.4483(3.4539) | Xent 2.3026(2.3026) | Loss 3.4483(3.4539) | Error 0.9222(0.9006) Steps 886(878.53) | Grad Norm 2.5933(2.4823) | Total Time 14.00(14.00)\n",
      "Iter 6730 | Time 22.3090(21.8701) | Bit/dim 3.4787(3.4565) | Xent 2.3026(2.3026) | Loss 3.4787(3.4565) | Error 0.8944(0.9016) Steps 892(879.37) | Grad Norm 1.1601(2.4807) | Total Time 14.00(14.00)\n",
      "Iter 6740 | Time 22.5208(21.8908) | Bit/dim 3.4609(3.4572) | Xent 2.3026(2.3026) | Loss 3.4609(3.4572) | Error 0.9144(0.9016) Steps 880(878.70) | Grad Norm 1.7540(2.3907) | Total Time 14.00(14.00)\n",
      "Iter 6750 | Time 21.3335(21.9253) | Bit/dim 3.4466(3.4571) | Xent 2.3026(2.3026) | Loss 3.4466(3.4571) | Error 0.9122(0.9016) Steps 874(879.57) | Grad Norm 1.6097(2.2655) | Total Time 14.00(14.00)\n",
      "Iter 6760 | Time 22.0132(21.9498) | Bit/dim 3.4253(3.4553) | Xent 2.3026(2.3026) | Loss 3.4253(3.4553) | Error 0.8956(0.9001) Steps 880(881.09) | Grad Norm 2.9022(2.4283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 109.5668, Epoch Time 1336.0646(1282.2579), Bit/dim 3.4553(best: 3.4556), Xent 2.3026, Loss 3.4553, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 22.3297(21.9889) | Bit/dim 3.4642(3.4552) | Xent 2.3026(2.3026) | Loss 3.4642(3.4552) | Error 0.8800(0.8991) Steps 862(880.18) | Grad Norm 2.5696(2.4029) | Total Time 14.00(14.00)\n",
      "Iter 6780 | Time 22.5859(21.9739) | Bit/dim 3.4364(3.4538) | Xent 2.3026(2.3026) | Loss 3.4364(3.4538) | Error 0.8944(0.8987) Steps 892(880.63) | Grad Norm 2.0782(2.5301) | Total Time 14.00(14.00)\n",
      "Iter 6790 | Time 22.3474(22.0371) | Bit/dim 3.4591(3.4516) | Xent 2.3026(2.3026) | Loss 3.4591(3.4516) | Error 0.9100(0.8991) Steps 886(882.45) | Grad Norm 2.4187(2.4695) | Total Time 14.00(14.00)\n",
      "Iter 6800 | Time 22.0294(22.0781) | Bit/dim 3.4331(3.4511) | Xent 2.3026(2.3026) | Loss 3.4331(3.4511) | Error 0.9078(0.8998) Steps 886(883.75) | Grad Norm 2.1257(2.3367) | Total Time 14.00(14.00)\n",
      "Iter 6810 | Time 22.0578(22.0495) | Bit/dim 3.4737(3.4532) | Xent 2.3026(2.3026) | Loss 3.4737(3.4532) | Error 0.8844(0.9001) Steps 874(883.54) | Grad Norm 2.8407(2.3356) | Total Time 14.00(14.00)\n",
      "Iter 6820 | Time 22.1068(22.0957) | Bit/dim 3.4490(3.4540) | Xent 2.3026(2.3026) | Loss 3.4490(3.4540) | Error 0.9078(0.9001) Steps 886(883.88) | Grad Norm 2.0114(2.3983) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 109.4878, Epoch Time 1344.0506(1284.1117), Bit/dim 3.4553(best: 3.4553), Xent 2.3026, Loss 3.4553, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 22.6841(22.0524) | Bit/dim 3.4336(3.4514) | Xent 2.3026(2.3026) | Loss 3.4336(3.4514) | Error 0.8867(0.9004) Steps 898(883.63) | Grad Norm 1.6892(2.4810) | Total Time 14.00(14.00)\n",
      "Iter 6840 | Time 22.7214(22.1073) | Bit/dim 3.4604(3.4526) | Xent 2.3026(2.3026) | Loss 3.4604(3.4526) | Error 0.9067(0.9007) Steps 880(885.66) | Grad Norm 2.5421(2.3875) | Total Time 14.00(14.00)\n",
      "Iter 6850 | Time 21.9340(22.0898) | Bit/dim 3.4543(3.4519) | Xent 2.3026(2.3026) | Loss 3.4543(3.4519) | Error 0.8967(0.8997) Steps 904(886.44) | Grad Norm 2.1376(2.3933) | Total Time 14.00(14.00)\n",
      "Iter 6860 | Time 21.6214(22.0556) | Bit/dim 3.4525(3.4515) | Xent 2.3026(2.3026) | Loss 3.4525(3.4515) | Error 0.9056(0.9002) Steps 886(886.04) | Grad Norm 2.6845(2.4558) | Total Time 14.00(14.00)\n",
      "Iter 6870 | Time 21.4272(22.1347) | Bit/dim 3.4884(3.4553) | Xent 2.3026(2.3026) | Loss 3.4884(3.4553) | Error 0.9044(0.9004) Steps 874(886.28) | Grad Norm 3.1442(2.4948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 108.4545, Epoch Time 1342.0806(1285.8508), Bit/dim 3.4572(best: 3.4553), Xent 2.3026, Loss 3.4572, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 21.8464(22.0717) | Bit/dim 3.4907(3.4560) | Xent 2.3026(2.3026) | Loss 3.4907(3.4560) | Error 0.9022(0.9000) Steps 880(885.52) | Grad Norm 1.3493(2.5063) | Total Time 14.00(14.00)\n",
      "Iter 6890 | Time 22.1152(22.0236) | Bit/dim 3.4525(3.4552) | Xent 2.3026(2.3026) | Loss 3.4525(3.4552) | Error 0.9022(0.8997) Steps 886(884.24) | Grad Norm 3.2836(2.5222) | Total Time 14.00(14.00)\n",
      "Iter 6900 | Time 22.2491(22.0430) | Bit/dim 3.4767(3.4524) | Xent 2.3026(2.3026) | Loss 3.4767(3.4524) | Error 0.8944(0.9001) Steps 892(885.92) | Grad Norm 2.3789(2.4778) | Total Time 14.00(14.00)\n",
      "Iter 6910 | Time 23.0933(22.0114) | Bit/dim 3.4617(3.4543) | Xent 2.3026(2.3026) | Loss 3.4617(3.4543) | Error 0.9133(0.8997) Steps 904(885.11) | Grad Norm 1.5071(2.3698) | Total Time 14.00(14.00)\n",
      "Iter 6920 | Time 21.8345(22.0073) | Bit/dim 3.4356(3.4547) | Xent 2.3026(2.3026) | Loss 3.4356(3.4547) | Error 0.8967(0.8999) Steps 898(885.27) | Grad Norm 1.6392(2.3552) | Total Time 14.00(14.00)\n",
      "Iter 6930 | Time 22.8436(22.0618) | Bit/dim 3.4661(3.4527) | Xent 2.3026(2.3026) | Loss 3.4661(3.4527) | Error 0.9022(0.9003) Steps 898(885.33) | Grad Norm 3.2784(2.4881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 108.9951, Epoch Time 1337.6273(1287.4041), Bit/dim 3.4513(best: 3.4553), Xent 2.3026, Loss 3.4513, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 22.1774(22.0849) | Bit/dim 3.4288(3.4505) | Xent 2.3026(2.3026) | Loss 3.4288(3.4505) | Error 0.8944(0.9005) Steps 892(886.45) | Grad Norm 2.5232(2.4213) | Total Time 14.00(14.00)\n",
      "Iter 6950 | Time 22.3715(22.0701) | Bit/dim 3.4636(3.4522) | Xent 2.3026(2.3026) | Loss 3.4636(3.4522) | Error 0.9000(0.9006) Steps 916(888.33) | Grad Norm 2.4501(2.5304) | Total Time 14.00(14.00)\n",
      "Iter 6960 | Time 21.6047(22.0780) | Bit/dim 3.4514(3.4497) | Xent 2.3026(2.3026) | Loss 3.4514(3.4497) | Error 0.9111(0.9007) Steps 898(887.16) | Grad Norm 1.1432(2.4084) | Total Time 14.00(14.00)\n",
      "Iter 6970 | Time 21.9734(22.0581) | Bit/dim 3.4727(3.4525) | Xent 2.3026(2.3026) | Loss 3.4727(3.4525) | Error 0.9167(0.9005) Steps 868(887.37) | Grad Norm 2.0138(2.4125) | Total Time 14.00(14.00)\n",
      "Iter 6980 | Time 21.5822(22.0831) | Bit/dim 3.4446(3.4521) | Xent 2.3026(2.3026) | Loss 3.4446(3.4521) | Error 0.8867(0.8998) Steps 886(888.26) | Grad Norm 2.7334(2.3375) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 109.2005, Epoch Time 1344.2103(1289.1083), Bit/dim 3.4541(best: 3.4513), Xent 2.3026, Loss 3.4541, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 22.1272(22.0978) | Bit/dim 3.4264(3.4500) | Xent 2.3026(2.3026) | Loss 3.4264(3.4500) | Error 0.9044(0.8994) Steps 880(888.12) | Grad Norm 2.6774(2.4023) | Total Time 14.00(14.00)\n",
      "Iter 7000 | Time 22.6918(22.0894) | Bit/dim 3.4504(3.4490) | Xent 2.3026(2.3026) | Loss 3.4504(3.4490) | Error 0.9000(0.9008) Steps 880(887.88) | Grad Norm 1.9553(2.3402) | Total Time 14.00(14.00)\n",
      "Iter 7010 | Time 22.0969(22.1205) | Bit/dim 3.4571(3.4510) | Xent 2.3026(2.3026) | Loss 3.4571(3.4510) | Error 0.8956(0.9008) Steps 892(889.80) | Grad Norm 3.1813(2.3560) | Total Time 14.00(14.00)\n",
      "Iter 7020 | Time 21.9630(22.1318) | Bit/dim 3.4288(3.4493) | Xent 2.3026(2.3026) | Loss 3.4288(3.4493) | Error 0.8833(0.8987) Steps 874(887.89) | Grad Norm 2.2515(2.3383) | Total Time 14.00(14.00)\n",
      "Iter 7030 | Time 22.1908(22.1406) | Bit/dim 3.4701(3.4510) | Xent 2.3026(2.3026) | Loss 3.4701(3.4510) | Error 0.8978(0.8991) Steps 892(889.48) | Grad Norm 1.8906(2.4534) | Total Time 14.00(14.00)\n",
      "Iter 7040 | Time 21.5108(22.2014) | Bit/dim 3.4498(3.4542) | Xent 2.3026(2.3026) | Loss 3.4498(3.4542) | Error 0.9044(0.8998) Steps 874(888.62) | Grad Norm 2.5381(2.4926) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 108.3315, Epoch Time 1346.1274(1290.8188), Bit/dim 3.4520(best: 3.4513), Xent 2.3026, Loss 3.4520, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 22.2856(22.1982) | Bit/dim 3.4379(3.4513) | Xent 2.3026(2.3026) | Loss 3.4379(3.4513) | Error 0.8911(0.8992) Steps 874(887.90) | Grad Norm 1.4146(2.3429) | Total Time 14.00(14.00)\n",
      "Iter 7060 | Time 21.8198(22.1951) | Bit/dim 3.4631(3.4520) | Xent 2.3026(2.3026) | Loss 3.4631(3.4520) | Error 0.8822(0.9000) Steps 886(887.08) | Grad Norm 1.9857(2.2458) | Total Time 14.00(14.00)\n",
      "Iter 7070 | Time 22.5970(22.2275) | Bit/dim 3.4713(3.4532) | Xent 2.3026(2.3026) | Loss 3.4713(3.4532) | Error 0.8989(0.9008) Steps 892(887.90) | Grad Norm 1.5239(2.4319) | Total Time 14.00(14.00)\n",
      "Iter 7080 | Time 22.2862(22.2309) | Bit/dim 3.4223(3.4495) | Xent 2.3026(2.3026) | Loss 3.4223(3.4495) | Error 0.9122(0.9000) Steps 892(888.58) | Grad Norm 2.7652(2.3642) | Total Time 14.00(14.00)\n",
      "Iter 7090 | Time 21.7326(22.1684) | Bit/dim 3.4648(3.4500) | Xent 2.3026(2.3026) | Loss 3.4648(3.4500) | Error 0.9056(0.8999) Steps 874(886.79) | Grad Norm 3.1656(2.3829) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 109.9470, Epoch Time 1351.1788(1292.6296), Bit/dim 3.4530(best: 3.4513), Xent 2.3026, Loss 3.4530, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 21.6000(22.2400) | Bit/dim 3.4458(3.4486) | Xent 2.3026(2.3026) | Loss 3.4458(3.4486) | Error 0.8911(0.9004) Steps 892(886.21) | Grad Norm 1.3359(2.3717) | Total Time 14.00(14.00)\n",
      "Iter 7110 | Time 22.2465(22.2524) | Bit/dim 3.4530(3.4500) | Xent 2.3026(2.3026) | Loss 3.4530(3.4500) | Error 0.8978(0.9011) Steps 904(889.24) | Grad Norm 3.2435(2.4034) | Total Time 14.00(14.00)\n",
      "Iter 7120 | Time 22.1129(22.2867) | Bit/dim 3.4277(3.4495) | Xent 2.3026(2.3026) | Loss 3.4277(3.4495) | Error 0.9033(0.8993) Steps 898(890.57) | Grad Norm 3.8270(2.3884) | Total Time 14.00(14.00)\n",
      "Iter 7130 | Time 22.5361(22.2978) | Bit/dim 3.4510(3.4470) | Xent 2.3026(2.3026) | Loss 3.4510(3.4470) | Error 0.9044(0.8986) Steps 898(891.23) | Grad Norm 2.7009(2.4386) | Total Time 14.00(14.00)\n",
      "Iter 7140 | Time 21.8215(22.2728) | Bit/dim 3.4514(3.4477) | Xent 2.3026(2.3026) | Loss 3.4514(3.4477) | Error 0.9022(0.8996) Steps 904(891.14) | Grad Norm 3.4901(2.5318) | Total Time 14.00(14.00)\n",
      "Iter 7150 | Time 22.0093(22.2357) | Bit/dim 3.4288(3.4489) | Xent 2.3026(2.3026) | Loss 3.4288(3.4489) | Error 0.8933(0.9003) Steps 868(888.46) | Grad Norm 1.9002(2.3553) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 110.3839, Epoch Time 1353.7178(1294.4623), Bit/dim 3.4488(best: 3.4513), Xent 2.3026, Loss 3.4488, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 22.2928(22.2163) | Bit/dim 3.4691(3.4477) | Xent 2.3026(2.3026) | Loss 3.4691(3.4477) | Error 0.8922(0.9011) Steps 904(888.81) | Grad Norm 2.7076(2.4507) | Total Time 14.00(14.00)\n",
      "Iter 7170 | Time 21.7636(22.1192) | Bit/dim 3.4475(3.4479) | Xent 2.3026(2.3026) | Loss 3.4475(3.4479) | Error 0.9122(0.9007) Steps 892(889.14) | Grad Norm 1.7439(2.4069) | Total Time 14.00(14.00)\n",
      "Iter 7180 | Time 21.6359(22.0982) | Bit/dim 3.4727(3.4487) | Xent 2.3026(2.3026) | Loss 3.4727(3.4487) | Error 0.9067(0.9004) Steps 874(887.03) | Grad Norm 3.4975(2.3294) | Total Time 14.00(14.00)\n",
      "Iter 7190 | Time 21.8898(22.1793) | Bit/dim 3.4742(3.4508) | Xent 2.3026(2.3026) | Loss 3.4742(3.4508) | Error 0.9000(0.9000) Steps 892(888.21) | Grad Norm 2.1226(2.3970) | Total Time 14.00(14.00)\n",
      "Iter 7200 | Time 22.1829(22.1842) | Bit/dim 3.4714(3.4498) | Xent 2.3026(2.3026) | Loss 3.4714(3.4498) | Error 0.8956(0.8994) Steps 880(889.07) | Grad Norm 1.9805(2.3278) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 109.0344, Epoch Time 1345.5883(1295.9961), Bit/dim 3.4549(best: 3.4488), Xent 2.3026, Loss 3.4549, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 22.7768(22.2000) | Bit/dim 3.4236(3.4478) | Xent 2.3026(2.3026) | Loss 3.4236(3.4478) | Error 0.8989(0.8999) Steps 880(889.96) | Grad Norm 1.9672(2.4298) | Total Time 14.00(14.00)\n",
      "Iter 7220 | Time 21.9608(22.2284) | Bit/dim 3.4498(3.4469) | Xent 2.3026(2.3026) | Loss 3.4498(3.4469) | Error 0.9011(0.9003) Steps 892(890.84) | Grad Norm 1.4837(2.2868) | Total Time 14.00(14.00)\n",
      "Iter 7230 | Time 22.5217(22.1493) | Bit/dim 3.4528(3.4449) | Xent 2.3026(2.3026) | Loss 3.4528(3.4449) | Error 0.8956(0.8999) Steps 886(890.75) | Grad Norm 2.9294(2.2980) | Total Time 14.00(14.00)\n",
      "Iter 7240 | Time 21.6749(22.1723) | Bit/dim 3.4639(3.4474) | Xent 2.3026(2.3026) | Loss 3.4639(3.4474) | Error 0.8967(0.9006) Steps 874(890.82) | Grad Norm 2.8523(2.4030) | Total Time 14.00(14.00)\n",
      "Iter 7250 | Time 22.3306(22.1688) | Bit/dim 3.4349(3.4488) | Xent 2.3026(2.3026) | Loss 3.4349(3.4488) | Error 0.8800(0.8990) Steps 928(892.26) | Grad Norm 1.8939(2.3563) | Total Time 14.00(14.00)\n",
      "Iter 7260 | Time 22.8315(22.1738) | Bit/dim 3.4180(3.4475) | Xent 2.3026(2.3026) | Loss 3.4180(3.4475) | Error 0.8889(0.9000) Steps 916(891.74) | Grad Norm 2.6308(2.3347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 109.2144, Epoch Time 1347.0691(1297.5283), Bit/dim 3.4496(best: 3.4488), Xent 2.3026, Loss 3.4496, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 22.9158(22.2540) | Bit/dim 3.4109(3.4468) | Xent 2.3026(2.3026) | Loss 3.4109(3.4468) | Error 0.8811(0.9005) Steps 892(893.59) | Grad Norm 2.0457(2.4391) | Total Time 14.00(14.00)\n",
      "Iter 7280 | Time 22.5541(22.2112) | Bit/dim 3.4730(3.4450) | Xent 2.3026(2.3026) | Loss 3.4730(3.4450) | Error 0.9000(0.8994) Steps 892(890.89) | Grad Norm 2.1829(2.2351) | Total Time 14.00(14.00)\n",
      "Iter 7290 | Time 21.6960(22.2230) | Bit/dim 3.4312(3.4462) | Xent 2.3026(2.3026) | Loss 3.4312(3.4462) | Error 0.8778(0.8984) Steps 880(891.12) | Grad Norm 3.1610(2.4274) | Total Time 14.00(14.00)\n",
      "Iter 7300 | Time 22.0528(22.2233) | Bit/dim 3.4421(3.4447) | Xent 2.3026(2.3026) | Loss 3.4421(3.4447) | Error 0.9100(0.8993) Steps 886(889.40) | Grad Norm 2.0462(2.5009) | Total Time 14.00(14.00)\n",
      "Iter 7310 | Time 21.9210(22.2322) | Bit/dim 3.4792(3.4472) | Xent 2.3026(2.3026) | Loss 3.4792(3.4472) | Error 0.8989(0.9004) Steps 874(887.97) | Grad Norm 2.4209(2.3581) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 109.1809, Epoch Time 1350.5214(1299.1181), Bit/dim 3.4444(best: 3.4488), Xent 2.3026, Loss 3.4444, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 21.6988(22.1512) | Bit/dim 3.4575(3.4479) | Xent 2.3026(2.3026) | Loss 3.4575(3.4479) | Error 0.9000(0.8999) Steps 892(888.45) | Grad Norm 2.0816(2.4156) | Total Time 14.00(14.00)\n",
      "Iter 7330 | Time 22.5881(22.1080) | Bit/dim 3.4396(3.4448) | Xent 2.3026(2.3026) | Loss 3.4396(3.4448) | Error 0.8878(0.8989) Steps 868(888.16) | Grad Norm 1.8938(2.3719) | Total Time 14.00(14.00)\n",
      "Iter 7340 | Time 22.0581(22.0692) | Bit/dim 3.4656(3.4471) | Xent 2.3026(2.3026) | Loss 3.4656(3.4471) | Error 0.8933(0.8993) Steps 880(886.83) | Grad Norm 2.0484(2.3975) | Total Time 14.00(14.00)\n",
      "Iter 7350 | Time 21.7407(22.0108) | Bit/dim 3.4101(3.4450) | Xent 2.3026(2.3026) | Loss 3.4101(3.4450) | Error 0.8967(0.9005) Steps 886(885.04) | Grad Norm 2.0799(2.3197) | Total Time 14.00(14.00)\n",
      "Iter 7360 | Time 21.9892(22.0022) | Bit/dim 3.4194(3.4473) | Xent 2.3026(2.3026) | Loss 3.4194(3.4473) | Error 0.9100(0.9003) Steps 880(886.56) | Grad Norm 3.1991(2.4390) | Total Time 14.00(14.00)\n",
      "Iter 7370 | Time 21.2225(21.9707) | Bit/dim 3.4145(3.4433) | Xent 2.3026(2.3026) | Loss 3.4145(3.4433) | Error 0.8967(0.9006) Steps 892(887.06) | Grad Norm 1.5330(2.4153) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 108.8712, Epoch Time 1333.2326(1300.1415), Bit/dim 3.4457(best: 3.4444), Xent 2.3026, Loss 3.4457, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 22.3738(22.0441) | Bit/dim 3.4426(3.4419) | Xent 2.3026(2.3026) | Loss 3.4426(3.4419) | Error 0.8911(0.9006) Steps 874(885.98) | Grad Norm 1.7070(2.4368) | Total Time 14.00(14.00)\n",
      "Iter 7390 | Time 22.2227(22.0187) | Bit/dim 3.4365(3.4409) | Xent 2.3026(2.3026) | Loss 3.4365(3.4409) | Error 0.8922(0.9009) Steps 892(886.68) | Grad Norm 1.3960(2.2529) | Total Time 14.00(14.00)\n",
      "Iter 7400 | Time 21.5737(22.0618) | Bit/dim 3.4233(3.4423) | Xent 2.3026(2.3026) | Loss 3.4233(3.4423) | Error 0.8978(0.9012) Steps 886(888.88) | Grad Norm 1.4497(2.2557) | Total Time 14.00(14.00)\n",
      "Iter 7410 | Time 22.1929(22.1216) | Bit/dim 3.4103(3.4431) | Xent 2.3026(2.3026) | Loss 3.4103(3.4431) | Error 0.9022(0.9006) Steps 886(888.32) | Grad Norm 2.3027(2.4542) | Total Time 14.00(14.00)\n",
      "Iter 7420 | Time 22.0121(22.1279) | Bit/dim 3.4523(3.4451) | Xent 2.3026(2.3026) | Loss 3.4523(3.4451) | Error 0.9056(0.8997) Steps 898(889.82) | Grad Norm 2.5671(2.4435) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 109.2737, Epoch Time 1348.0817(1301.5797), Bit/dim 3.4461(best: 3.4444), Xent 2.3026, Loss 3.4461, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 22.0794(22.1649) | Bit/dim 3.4318(3.4466) | Xent 2.3026(2.3026) | Loss 3.4318(3.4466) | Error 0.8867(0.8998) Steps 880(891.57) | Grad Norm 2.2635(2.3156) | Total Time 14.00(14.00)\n",
      "Iter 7440 | Time 22.5846(22.2216) | Bit/dim 3.4720(3.4484) | Xent 2.3026(2.3026) | Loss 3.4720(3.4484) | Error 0.8978(0.9005) Steps 880(891.29) | Grad Norm 2.1156(2.3929) | Total Time 14.00(14.00)\n",
      "Iter 7450 | Time 21.3673(22.2211) | Bit/dim 3.4449(3.4480) | Xent 2.3026(2.3026) | Loss 3.4449(3.4480) | Error 0.9100(0.9011) Steps 886(889.74) | Grad Norm 1.5525(2.2679) | Total Time 14.00(14.00)\n",
      "Iter 7460 | Time 22.7298(22.2746) | Bit/dim 3.4586(3.4473) | Xent 2.3026(2.3026) | Loss 3.4586(3.4473) | Error 0.8900(0.9002) Steps 886(889.86) | Grad Norm 1.6998(2.3736) | Total Time 14.00(14.00)\n",
      "Iter 7470 | Time 22.2940(22.2322) | Bit/dim 3.4285(3.4438) | Xent 2.3026(2.3026) | Loss 3.4285(3.4438) | Error 0.8778(0.8992) Steps 904(892.03) | Grad Norm 2.5463(2.3813) | Total Time 14.00(14.00)\n",
      "Iter 7480 | Time 22.0557(22.1505) | Bit/dim 3.4587(3.4417) | Xent 2.3026(2.3026) | Loss 3.4587(3.4417) | Error 0.9078(0.8993) Steps 886(890.47) | Grad Norm 3.6410(2.3977) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 109.2175, Epoch Time 1348.8191(1302.9969), Bit/dim 3.4519(best: 3.4444), Xent 2.3026, Loss 3.4519, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 21.7701(22.1310) | Bit/dim 3.4525(3.4420) | Xent 2.3026(2.3026) | Loss 3.4525(3.4420) | Error 0.9200(0.8996) Steps 892(888.95) | Grad Norm 2.8104(2.3710) | Total Time 14.00(14.00)\n",
      "Iter 7500 | Time 22.6694(22.1549) | Bit/dim 3.4503(3.4424) | Xent 2.3026(2.3026) | Loss 3.4503(3.4424) | Error 0.9033(0.8990) Steps 904(890.14) | Grad Norm 2.3586(2.2782) | Total Time 14.00(14.00)\n",
      "Iter 7510 | Time 22.6943(22.0965) | Bit/dim 3.4689(3.4418) | Xent 2.3026(2.3026) | Loss 3.4689(3.4418) | Error 0.9044(0.8985) Steps 892(891.07) | Grad Norm 1.7288(2.3580) | Total Time 14.00(14.00)\n",
      "Iter 7520 | Time 22.1249(22.1067) | Bit/dim 3.4914(3.4414) | Xent 2.3026(2.3026) | Loss 3.4914(3.4414) | Error 0.9011(0.8997) Steps 904(890.88) | Grad Norm 2.4407(2.3773) | Total Time 14.00(14.00)\n",
      "Iter 7530 | Time 21.7055(22.0925) | Bit/dim 3.4758(3.4425) | Xent 2.3026(2.3026) | Loss 3.4758(3.4425) | Error 0.8956(0.8993) Steps 880(891.17) | Grad Norm 2.5204(2.4161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 109.9301, Epoch Time 1344.9173(1304.2545), Bit/dim 3.4483(best: 3.4444), Xent 2.3026, Loss 3.4483, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 22.6160(22.1327) | Bit/dim 3.4565(3.4434) | Xent 2.3026(2.3026) | Loss 3.4565(3.4434) | Error 0.9000(0.9000) Steps 922(895.65) | Grad Norm 2.2642(2.4371) | Total Time 14.00(14.00)\n",
      "Iter 7550 | Time 22.2748(22.1387) | Bit/dim 3.4383(3.4435) | Xent 2.3026(2.3026) | Loss 3.4383(3.4435) | Error 0.9022(0.9002) Steps 898(894.78) | Grad Norm 2.4127(2.3805) | Total Time 14.00(14.00)\n",
      "Iter 7560 | Time 22.6687(22.1065) | Bit/dim 3.4605(3.4425) | Xent 2.3026(2.3026) | Loss 3.4605(3.4425) | Error 0.8900(0.9005) Steps 886(894.22) | Grad Norm 1.6670(2.2567) | Total Time 14.00(14.00)\n",
      "Iter 7570 | Time 21.5694(22.1003) | Bit/dim 3.4511(3.4442) | Xent 2.3026(2.3026) | Loss 3.4511(3.4442) | Error 0.9022(0.8995) Steps 880(893.82) | Grad Norm 3.4201(2.2898) | Total Time 14.00(14.00)\n",
      "Iter 7580 | Time 21.8759(22.0535) | Bit/dim 3.4371(3.4443) | Xent 2.3026(2.3026) | Loss 3.4371(3.4443) | Error 0.8900(0.8993) Steps 886(891.60) | Grad Norm 2.2167(2.3724) | Total Time 14.00(14.00)\n",
      "Iter 7590 | Time 21.9550(22.0516) | Bit/dim 3.4290(3.4416) | Xent 2.3026(2.3026) | Loss 3.4290(3.4416) | Error 0.9156(0.9008) Steps 910(894.32) | Grad Norm 2.9006(2.4286) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 108.9855, Epoch Time 1340.3746(1305.3381), Bit/dim 3.4430(best: 3.4444), Xent 2.3026, Loss 3.4430, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 22.2601(22.0576) | Bit/dim 3.4352(3.4412) | Xent 2.3026(2.3026) | Loss 3.4352(3.4412) | Error 0.8978(0.9007) Steps 910(895.09) | Grad Norm 2.4411(2.4453) | Total Time 14.00(14.00)\n",
      "Iter 7610 | Time 21.9304(22.1104) | Bit/dim 3.4218(3.4390) | Xent 2.3026(2.3026) | Loss 3.4218(3.4390) | Error 0.9067(0.9010) Steps 886(894.41) | Grad Norm 1.6834(2.3383) | Total Time 14.00(14.00)\n",
      "Iter 7620 | Time 22.2800(22.1646) | Bit/dim 3.4622(3.4404) | Xent 2.3026(2.3026) | Loss 3.4622(3.4404) | Error 0.9078(0.9015) Steps 910(897.00) | Grad Norm 1.9320(2.3751) | Total Time 14.00(14.00)\n",
      "Iter 7630 | Time 22.8068(22.2198) | Bit/dim 3.4357(3.4421) | Xent 2.3026(2.3026) | Loss 3.4357(3.4421) | Error 0.9011(0.9009) Steps 886(897.59) | Grad Norm 1.8067(2.2047) | Total Time 14.00(14.00)\n",
      "Iter 7640 | Time 22.5815(22.2369) | Bit/dim 3.4421(3.4418) | Xent 2.3026(2.3026) | Loss 3.4421(3.4418) | Error 0.8978(0.8993) Steps 904(898.63) | Grad Norm 1.7457(2.2108) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 108.9860, Epoch Time 1351.0970(1306.7109), Bit/dim 3.4511(best: 3.4430), Xent 2.3026, Loss 3.4511, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 22.9843(22.2580) | Bit/dim 3.4193(3.4414) | Xent 2.3026(2.3026) | Loss 3.4193(3.4414) | Error 0.8956(0.8998) Steps 934(900.83) | Grad Norm 2.1798(2.3177) | Total Time 14.00(14.00)\n",
      "Iter 7660 | Time 21.5233(22.2988) | Bit/dim 3.4345(3.4418) | Xent 2.3026(2.3026) | Loss 3.4345(3.4418) | Error 0.9011(0.9008) Steps 886(901.66) | Grad Norm 2.4004(2.2665) | Total Time 14.00(14.00)\n",
      "Iter 7670 | Time 21.9454(22.2323) | Bit/dim 3.4782(3.4446) | Xent 2.3026(2.3026) | Loss 3.4782(3.4446) | Error 0.9033(0.9010) Steps 898(897.61) | Grad Norm 3.1085(2.3482) | Total Time 14.00(14.00)\n",
      "Iter 7680 | Time 22.2063(22.2093) | Bit/dim 3.4487(3.4418) | Xent 2.3026(2.3026) | Loss 3.4487(3.4418) | Error 0.9044(0.9004) Steps 886(895.44) | Grad Norm 3.1454(2.4047) | Total Time 14.00(14.00)\n",
      "Iter 7690 | Time 22.5143(22.2484) | Bit/dim 3.4370(3.4412) | Xent 2.3026(2.3026) | Loss 3.4370(3.4412) | Error 0.8978(0.9011) Steps 892(896.73) | Grad Norm 3.1257(2.4621) | Total Time 14.00(14.00)\n",
      "Iter 7700 | Time 22.0761(22.1762) | Bit/dim 3.4820(3.4400) | Xent 2.3026(2.3026) | Loss 3.4820(3.4400) | Error 0.8889(0.8995) Steps 892(897.05) | Grad Norm 2.0206(2.4492) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 109.5923, Epoch Time 1350.7065(1308.0307), Bit/dim 3.4405(best: 3.4430), Xent 2.3026, Loss 3.4405, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 22.8500(22.2326) | Bit/dim 3.4364(3.4407) | Xent 2.3026(2.3026) | Loss 3.4364(3.4407) | Error 0.9000(0.8997) Steps 922(898.61) | Grad Norm 2.3142(2.4652) | Total Time 14.00(14.00)\n",
      "Iter 7720 | Time 22.1219(22.2760) | Bit/dim 3.4277(3.4383) | Xent 2.3026(2.3026) | Loss 3.4277(3.4383) | Error 0.9100(0.8992) Steps 904(898.11) | Grad Norm 3.6402(2.4145) | Total Time 14.00(14.00)\n",
      "Iter 7730 | Time 23.1439(22.3049) | Bit/dim 3.4077(3.4403) | Xent 2.3026(2.3026) | Loss 3.4077(3.4403) | Error 0.8978(0.8994) Steps 904(897.69) | Grad Norm 2.9663(2.4777) | Total Time 14.00(14.00)\n",
      "Iter 7740 | Time 22.3445(22.2759) | Bit/dim 3.4307(3.4387) | Xent 2.3026(2.3026) | Loss 3.4307(3.4387) | Error 0.9011(0.9006) Steps 904(897.99) | Grad Norm 2.5084(2.4479) | Total Time 14.00(14.00)\n",
      "Iter 7750 | Time 22.3797(22.2371) | Bit/dim 3.4288(3.4397) | Xent 2.3026(2.3026) | Loss 3.4288(3.4397) | Error 0.8922(0.9007) Steps 910(900.10) | Grad Norm 1.9665(2.2899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 110.0105, Epoch Time 1354.9163(1309.4373), Bit/dim 3.4417(best: 3.4405), Xent 2.3026, Loss 3.4417, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 22.2628(22.2541) | Bit/dim 3.4323(3.4402) | Xent 2.3026(2.3026) | Loss 3.4323(3.4402) | Error 0.9144(0.9004) Steps 910(900.10) | Grad Norm 3.5423(2.3757) | Total Time 14.00(14.00)\n",
      "Iter 7770 | Time 22.1621(22.2532) | Bit/dim 3.4087(3.4394) | Xent 2.3026(2.3026) | Loss 3.4087(3.4394) | Error 0.8867(0.9000) Steps 898(900.05) | Grad Norm 1.7208(2.2133) | Total Time 14.00(14.00)\n",
      "Iter 7780 | Time 22.7262(22.3093) | Bit/dim 3.4669(3.4392) | Xent 2.3026(2.3026) | Loss 3.4669(3.4392) | Error 0.8811(0.8991) Steps 898(902.11) | Grad Norm 1.6919(2.3128) | Total Time 14.00(14.00)\n",
      "Iter 7790 | Time 22.3668(22.3570) | Bit/dim 3.4167(3.4379) | Xent 2.3026(2.3026) | Loss 3.4167(3.4379) | Error 0.9078(0.8991) Steps 928(905.16) | Grad Norm 1.8924(2.2771) | Total Time 14.00(14.00)\n",
      "Iter 7800 | Time 22.1602(22.3121) | Bit/dim 3.4218(3.4386) | Xent 2.3026(2.3026) | Loss 3.4218(3.4386) | Error 0.9144(0.8997) Steps 898(906.12) | Grad Norm 2.2278(2.4038) | Total Time 14.00(14.00)\n",
      "Iter 7810 | Time 21.2461(22.2919) | Bit/dim 3.4314(3.4371) | Xent 2.3026(2.3026) | Loss 3.4314(3.4371) | Error 0.9089(0.9003) Steps 898(903.12) | Grad Norm 2.8603(2.3675) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 111.2759, Epoch Time 1358.0125(1310.8946), Bit/dim 3.4396(best: 3.4405), Xent 2.3026, Loss 3.4396, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 22.0812(22.3209) | Bit/dim 3.4527(3.4386) | Xent 2.3026(2.3026) | Loss 3.4527(3.4386) | Error 0.8944(0.8997) Steps 892(902.30) | Grad Norm 2.2413(2.2672) | Total Time 14.00(14.00)\n",
      "Iter 7830 | Time 21.5864(22.2714) | Bit/dim 3.4558(3.4363) | Xent 2.3026(2.3026) | Loss 3.4558(3.4363) | Error 0.9067(0.8993) Steps 904(902.35) | Grad Norm 2.8910(2.4028) | Total Time 14.00(14.00)\n",
      "Iter 7840 | Time 22.8534(22.3008) | Bit/dim 3.4589(3.4364) | Xent 2.3026(2.3026) | Loss 3.4589(3.4364) | Error 0.9078(0.8998) Steps 904(902.06) | Grad Norm 1.9207(2.3314) | Total Time 14.00(14.00)\n",
      "Iter 7850 | Time 21.8221(22.2536) | Bit/dim 3.4188(3.4379) | Xent 2.3026(2.3026) | Loss 3.4188(3.4379) | Error 0.8989(0.8998) Steps 886(898.57) | Grad Norm 2.5774(2.4291) | Total Time 14.00(14.00)\n",
      "Iter 7860 | Time 22.4071(22.2606) | Bit/dim 3.4518(3.4387) | Xent 2.3026(2.3026) | Loss 3.4518(3.4387) | Error 0.9133(0.9005) Steps 910(900.16) | Grad Norm 2.0668(2.3936) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 111.1663, Epoch Time 1355.9893(1312.2474), Bit/dim 3.4381(best: 3.4396), Xent 2.3026, Loss 3.4381, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 23.4164(22.3295) | Bit/dim 3.4624(3.4385) | Xent 2.3026(2.3026) | Loss 3.4624(3.4385) | Error 0.8922(0.9007) Steps 934(900.83) | Grad Norm 1.6480(2.1661) | Total Time 14.00(14.00)\n",
      "Iter 7880 | Time 22.3793(22.3431) | Bit/dim 3.4338(3.4370) | Xent 2.3026(2.3026) | Loss 3.4338(3.4370) | Error 0.8978(0.9006) Steps 892(901.44) | Grad Norm 2.3464(2.2193) | Total Time 14.00(14.00)\n",
      "Iter 7890 | Time 22.2173(22.3802) | Bit/dim 3.4277(3.4388) | Xent 2.3026(2.3026) | Loss 3.4277(3.4388) | Error 0.8911(0.9015) Steps 880(900.58) | Grad Norm 3.4122(2.2476) | Total Time 14.00(14.00)\n",
      "Iter 7900 | Time 21.4708(22.3424) | Bit/dim 3.4286(3.4371) | Xent 2.3026(2.3026) | Loss 3.4286(3.4371) | Error 0.8922(0.9001) Steps 892(901.59) | Grad Norm 3.3180(2.3395) | Total Time 14.00(14.00)\n",
      "Iter 7910 | Time 23.1232(22.3369) | Bit/dim 3.4340(3.4374) | Xent 2.3026(2.3026) | Loss 3.4340(3.4374) | Error 0.8922(0.8999) Steps 922(899.05) | Grad Norm 1.8664(2.3555) | Total Time 14.00(14.00)\n",
      "Iter 7920 | Time 22.4594(22.3620) | Bit/dim 3.4192(3.4363) | Xent 2.3026(2.3026) | Loss 3.4192(3.4363) | Error 0.8989(0.8999) Steps 898(899.79) | Grad Norm 3.2526(2.3378) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 111.5087, Epoch Time 1360.6532(1313.6996), Bit/dim 3.4438(best: 3.4381), Xent 2.3026, Loss 3.4438, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 23.1262(22.4798) | Bit/dim 3.4308(3.4364) | Xent 2.3026(2.3026) | Loss 3.4308(3.4364) | Error 0.9044(0.9004) Steps 898(905.16) | Grad Norm 2.6740(2.4024) | Total Time 14.00(14.00)\n",
      "Iter 7940 | Time 22.8238(22.4071) | Bit/dim 3.4184(3.4335) | Xent 2.3026(2.3026) | Loss 3.4184(3.4335) | Error 0.8922(0.8983) Steps 922(906.45) | Grad Norm 2.1211(2.3442) | Total Time 14.00(14.00)\n",
      "Iter 7950 | Time 21.7863(22.3970) | Bit/dim 3.4182(3.4347) | Xent 2.3026(2.3026) | Loss 3.4182(3.4347) | Error 0.8889(0.8982) Steps 916(906.70) | Grad Norm 2.3294(2.2010) | Total Time 14.00(14.00)\n",
      "Iter 7960 | Time 22.0718(22.3903) | Bit/dim 3.4312(3.4341) | Xent 2.3026(2.3026) | Loss 3.4312(3.4341) | Error 0.9011(0.8979) Steps 898(906.42) | Grad Norm 4.2181(2.3425) | Total Time 14.00(14.00)\n",
      "Iter 7970 | Time 21.7319(22.4036) | Bit/dim 3.4382(3.4341) | Xent 2.3026(2.3026) | Loss 3.4382(3.4341) | Error 0.9089(0.8993) Steps 910(907.91) | Grad Norm 3.4735(2.3847) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 111.6400, Epoch Time 1364.3241(1315.2183), Bit/dim 3.4364(best: 3.4381), Xent 2.3026, Loss 3.4364, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 21.9068(22.4081) | Bit/dim 3.4316(3.4372) | Xent 2.3026(2.3026) | Loss 3.4316(3.4372) | Error 0.8967(0.9015) Steps 886(906.07) | Grad Norm 1.5652(2.2889) | Total Time 14.00(14.00)\n",
      "Iter 7990 | Time 22.8142(22.4662) | Bit/dim 3.4371(3.4379) | Xent 2.3026(2.3026) | Loss 3.4371(3.4379) | Error 0.9011(0.9015) Steps 910(908.40) | Grad Norm 2.7620(2.3347) | Total Time 14.00(14.00)\n",
      "Iter 8000 | Time 22.6742(22.4491) | Bit/dim 3.4506(3.4373) | Xent 2.3026(2.3026) | Loss 3.4506(3.4373) | Error 0.9211(0.9008) Steps 922(907.79) | Grad Norm 2.5911(2.3961) | Total Time 14.00(14.00)\n",
      "Iter 8010 | Time 22.5995(22.4457) | Bit/dim 3.4449(3.4355) | Xent 2.3026(2.3026) | Loss 3.4449(3.4355) | Error 0.9078(0.9003) Steps 922(907.73) | Grad Norm 2.4054(2.4140) | Total Time 14.00(14.00)\n",
      "Iter 8020 | Time 23.0899(22.4808) | Bit/dim 3.4154(3.4337) | Xent 2.3026(2.3026) | Loss 3.4154(3.4337) | Error 0.9067(0.9006) Steps 928(908.52) | Grad Norm 1.9936(2.2148) | Total Time 14.00(14.00)\n",
      "Iter 8030 | Time 22.3685(22.4440) | Bit/dim 3.4559(3.4365) | Xent 2.3026(2.3026) | Loss 3.4559(3.4365) | Error 0.9078(0.9000) Steps 898(907.74) | Grad Norm 1.8641(2.2206) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 110.9404, Epoch Time 1365.6266(1316.7306), Bit/dim 3.4395(best: 3.4364), Xent 2.3026, Loss 3.4395, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 22.7629(22.5202) | Bit/dim 3.4535(3.4382) | Xent 2.3026(2.3026) | Loss 3.4535(3.4382) | Error 0.8844(0.9001) Steps 910(909.78) | Grad Norm 2.4863(2.2246) | Total Time 14.00(14.00)\n",
      "Iter 8050 | Time 22.2019(22.5459) | Bit/dim 3.4136(3.4359) | Xent 2.3026(2.3026) | Loss 3.4136(3.4359) | Error 0.8933(0.8994) Steps 910(910.10) | Grad Norm 2.0971(2.2295) | Total Time 14.00(14.00)\n",
      "Iter 8060 | Time 22.1163(22.4987) | Bit/dim 3.4488(3.4350) | Xent 2.3026(2.3026) | Loss 3.4488(3.4350) | Error 0.9022(0.9000) Steps 922(911.12) | Grad Norm 3.3898(2.4039) | Total Time 14.00(14.00)\n",
      "Iter 8070 | Time 22.7097(22.4906) | Bit/dim 3.4617(3.4365) | Xent 2.3026(2.3026) | Loss 3.4617(3.4365) | Error 0.8900(0.8992) Steps 916(911.23) | Grad Norm 1.4866(2.3313) | Total Time 14.00(14.00)\n",
      "Iter 8080 | Time 22.6060(22.5850) | Bit/dim 3.4234(3.4336) | Xent 2.3026(2.3026) | Loss 3.4234(3.4336) | Error 0.8978(0.8999) Steps 904(911.05) | Grad Norm 2.1632(2.3292) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 113.0461, Epoch Time 1375.5090(1318.4939), Bit/dim 3.4344(best: 3.4364), Xent 2.3026, Loss 3.4344, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 21.3858(22.5306) | Bit/dim 3.4362(3.4326) | Xent 2.3026(2.3026) | Loss 3.4362(3.4326) | Error 0.8978(0.9001) Steps 916(911.71) | Grad Norm 3.0152(2.3121) | Total Time 14.00(14.00)\n",
      "Iter 8100 | Time 21.9817(22.5318) | Bit/dim 3.3977(3.4307) | Xent 2.3026(2.3026) | Loss 3.3977(3.4307) | Error 0.9011(0.9002) Steps 904(911.53) | Grad Norm 2.2612(2.3917) | Total Time 14.00(14.00)\n",
      "Iter 8110 | Time 22.6534(22.5519) | Bit/dim 3.4456(3.4336) | Xent 2.3026(2.3026) | Loss 3.4456(3.4336) | Error 0.9156(0.9014) Steps 928(911.64) | Grad Norm 2.4829(2.2002) | Total Time 14.00(14.00)\n",
      "Iter 8120 | Time 22.2502(22.5645) | Bit/dim 3.4308(3.4331) | Xent 2.3026(2.3026) | Loss 3.4308(3.4331) | Error 0.9078(0.9006) Steps 868(912.98) | Grad Norm 2.0750(2.2937) | Total Time 14.00(14.00)\n",
      "Iter 8130 | Time 22.5662(22.5764) | Bit/dim 3.4486(3.4352) | Xent 2.3026(2.3026) | Loss 3.4486(3.4352) | Error 0.8956(0.9002) Steps 910(912.77) | Grad Norm 3.0443(2.3178) | Total Time 14.00(14.00)\n",
      "Iter 8140 | Time 22.5266(22.5786) | Bit/dim 3.4501(3.4341) | Xent 2.3026(2.3026) | Loss 3.4501(3.4341) | Error 0.9078(0.8993) Steps 922(913.70) | Grad Norm 2.0553(2.1232) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 112.5006, Epoch Time 1371.6518(1320.0886), Bit/dim 3.4372(best: 3.4344), Xent 2.3026, Loss 3.4372, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 23.0523(22.6395) | Bit/dim 3.4728(3.4359) | Xent 2.3026(2.3026) | Loss 3.4728(3.4359) | Error 0.9000(0.8989) Steps 922(914.42) | Grad Norm 3.0891(2.2622) | Total Time 14.00(14.00)\n",
      "Iter 8160 | Time 22.2046(22.6427) | Bit/dim 3.4238(3.4343) | Xent 2.3026(2.3026) | Loss 3.4238(3.4343) | Error 0.9133(0.8992) Steps 910(913.83) | Grad Norm 2.4584(2.2265) | Total Time 14.00(14.00)\n",
      "Iter 8170 | Time 22.2202(22.6603) | Bit/dim 3.4518(3.4366) | Xent 2.3026(2.3026) | Loss 3.4518(3.4366) | Error 0.9000(0.9000) Steps 904(913.63) | Grad Norm 2.4429(2.2799) | Total Time 14.00(14.00)\n",
      "Iter 8180 | Time 23.0709(22.6890) | Bit/dim 3.3989(3.4344) | Xent 2.3026(2.3026) | Loss 3.3989(3.4344) | Error 0.8956(0.8998) Steps 928(912.96) | Grad Norm 2.3528(2.3725) | Total Time 14.00(14.00)\n",
      "Iter 8190 | Time 22.5522(22.7298) | Bit/dim 3.4063(3.4316) | Xent 2.3026(2.3026) | Loss 3.4063(3.4316) | Error 0.9111(0.8995) Steps 922(915.74) | Grad Norm 1.6013(2.3707) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 112.0875, Epoch Time 1383.1943(1321.9818), Bit/dim 3.4352(best: 3.4344), Xent 2.3026, Loss 3.4352, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 22.9983(22.7500) | Bit/dim 3.4523(3.4322) | Xent 2.3026(2.3026) | Loss 3.4523(3.4322) | Error 0.8989(0.8999) Steps 940(917.82) | Grad Norm 2.3984(2.3104) | Total Time 14.00(14.00)\n",
      "Iter 8210 | Time 22.1955(22.7278) | Bit/dim 3.4194(3.4321) | Xent 2.3026(2.3026) | Loss 3.4194(3.4321) | Error 0.9033(0.9010) Steps 922(916.69) | Grad Norm 1.8893(2.2679) | Total Time 14.00(14.00)\n",
      "Iter 8220 | Time 22.4318(22.7515) | Bit/dim 3.4109(3.4327) | Xent 2.3026(2.3026) | Loss 3.4109(3.4327) | Error 0.8922(0.9007) Steps 916(915.38) | Grad Norm 3.3168(2.3461) | Total Time 14.00(14.00)\n",
      "Iter 8230 | Time 22.6863(22.7810) | Bit/dim 3.4241(3.4316) | Xent 2.3026(2.3026) | Loss 3.4241(3.4316) | Error 0.8844(0.8995) Steps 910(917.48) | Grad Norm 1.5797(2.2531) | Total Time 14.00(14.00)\n",
      "Iter 8240 | Time 22.2380(22.7379) | Bit/dim 3.3848(3.4309) | Xent 2.3026(2.3026) | Loss 3.3848(3.4309) | Error 0.8911(0.9000) Steps 910(916.18) | Grad Norm 2.0841(2.1233) | Total Time 14.00(14.00)\n",
      "Iter 8250 | Time 22.8457(22.6857) | Bit/dim 3.4311(3.4315) | Xent 2.3026(2.3026) | Loss 3.4311(3.4315) | Error 0.8944(0.9000) Steps 922(915.97) | Grad Norm 1.4474(2.3034) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 113.7097, Epoch Time 1381.0730(1323.7546), Bit/dim 3.4358(best: 3.4344), Xent 2.3026, Loss 3.4358, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 23.1115(22.7292) | Bit/dim 3.4308(3.4319) | Xent 2.3026(2.3026) | Loss 3.4308(3.4319) | Error 0.9156(0.9002) Steps 934(916.62) | Grad Norm 1.4730(2.2475) | Total Time 14.00(14.00)\n",
      "Iter 8270 | Time 23.1355(22.7445) | Bit/dim 3.4502(3.4339) | Xent 2.3026(2.3026) | Loss 3.4502(3.4339) | Error 0.8856(0.8992) Steps 928(916.63) | Grad Norm 1.4662(2.3209) | Total Time 14.00(14.00)\n",
      "Iter 8280 | Time 22.9255(22.7623) | Bit/dim 3.4178(3.4328) | Xent 2.3026(2.3026) | Loss 3.4178(3.4328) | Error 0.9067(0.8998) Steps 928(913.32) | Grad Norm 2.6092(2.2512) | Total Time 14.00(14.00)\n",
      "Iter 8290 | Time 23.7235(22.7306) | Bit/dim 3.4336(3.4324) | Xent 2.3026(2.3026) | Loss 3.4336(3.4324) | Error 0.9067(0.9000) Steps 910(912.01) | Grad Norm 1.7064(2.3449) | Total Time 14.00(14.00)\n",
      "Iter 8300 | Time 23.4727(22.7518) | Bit/dim 3.4342(3.4310) | Xent 2.3026(2.3026) | Loss 3.4342(3.4310) | Error 0.9033(0.9000) Steps 958(914.12) | Grad Norm 2.3733(2.3870) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 112.4357, Epoch Time 1383.0423(1325.5332), Bit/dim 3.4299(best: 3.4344), Xent 2.3026, Loss 3.4299, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 22.1056(22.7148) | Bit/dim 3.4401(3.4312) | Xent 2.3026(2.3026) | Loss 3.4401(3.4312) | Error 0.8956(0.8989) Steps 916(914.72) | Grad Norm 2.1551(2.2422) | Total Time 14.00(14.00)\n",
      "Iter 8320 | Time 23.8947(22.7806) | Bit/dim 3.3997(3.4287) | Xent 2.3026(2.3026) | Loss 3.3997(3.4287) | Error 0.9078(0.8980) Steps 952(917.30) | Grad Norm 2.8046(2.1619) | Total Time 14.00(14.00)\n",
      "Iter 8330 | Time 23.0894(22.8368) | Bit/dim 3.4327(3.4292) | Xent 2.3026(2.3026) | Loss 3.4327(3.4292) | Error 0.9067(0.8994) Steps 928(915.91) | Grad Norm 2.1895(2.3488) | Total Time 14.00(14.00)\n",
      "Iter 8340 | Time 22.8299(22.8296) | Bit/dim 3.4617(3.4317) | Xent 2.3026(2.3026) | Loss 3.4617(3.4317) | Error 0.9044(0.9000) Steps 922(916.87) | Grad Norm 2.4420(2.2994) | Total Time 14.00(14.00)\n",
      "Iter 8350 | Time 23.3198(22.8349) | Bit/dim 3.4170(3.4327) | Xent 2.3026(2.3026) | Loss 3.4170(3.4327) | Error 0.8889(0.8995) Steps 910(918.47) | Grad Norm 3.2154(2.3035) | Total Time 14.00(14.00)\n",
      "Iter 8360 | Time 23.1285(22.8450) | Bit/dim 3.4288(3.4317) | Xent 2.3026(2.3026) | Loss 3.4288(3.4317) | Error 0.9067(0.9014) Steps 916(916.87) | Grad Norm 2.3837(2.2914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 112.1413, Epoch Time 1388.4858(1327.4218), Bit/dim 3.4335(best: 3.4299), Xent 2.3026, Loss 3.4335, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 22.0734(22.8194) | Bit/dim 3.3755(3.4303) | Xent 2.3026(2.3026) | Loss 3.3755(3.4303) | Error 0.8933(0.9003) Steps 910(917.22) | Grad Norm 2.1227(2.2783) | Total Time 14.00(14.00)\n",
      "Iter 8380 | Time 23.0154(22.7914) | Bit/dim 3.4248(3.4262) | Xent 2.3026(2.3026) | Loss 3.4248(3.4262) | Error 0.9089(0.8991) Steps 922(917.88) | Grad Norm 2.0145(2.2641) | Total Time 14.00(14.00)\n",
      "Iter 8390 | Time 22.4220(22.7493) | Bit/dim 3.4199(3.4289) | Xent 2.3026(2.3026) | Loss 3.4199(3.4289) | Error 0.9200(0.9009) Steps 928(916.02) | Grad Norm 1.5513(2.2020) | Total Time 14.00(14.00)\n",
      "Iter 8400 | Time 22.2189(22.6863) | Bit/dim 3.4475(3.4290) | Xent 2.3026(2.3026) | Loss 3.4475(3.4290) | Error 0.8744(0.9008) Steps 880(914.92) | Grad Norm 3.4978(2.2690) | Total Time 14.00(14.00)\n",
      "Iter 8410 | Time 22.4256(22.7467) | Bit/dim 3.4239(3.4327) | Xent 2.3026(2.3026) | Loss 3.4239(3.4327) | Error 0.9022(0.9005) Steps 916(916.14) | Grad Norm 1.8574(2.3405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 113.8673, Epoch Time 1382.0838(1329.0616), Bit/dim 3.4325(best: 3.4299), Xent 2.3026, Loss 3.4325, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 22.9637(22.7207) | Bit/dim 3.4147(3.4308) | Xent 2.3026(2.3026) | Loss 3.4147(3.4308) | Error 0.8867(0.8998) Steps 886(914.48) | Grad Norm 4.1051(2.3264) | Total Time 14.00(14.00)\n",
      "Iter 8430 | Time 22.7612(22.7151) | Bit/dim 3.4134(3.4324) | Xent 2.3026(2.3026) | Loss 3.4134(3.4324) | Error 0.8967(0.8983) Steps 904(914.47) | Grad Norm 2.5305(2.3400) | Total Time 14.00(14.00)\n",
      "Iter 8440 | Time 22.6479(22.6881) | Bit/dim 3.4671(3.4312) | Xent 2.3026(2.3026) | Loss 3.4671(3.4312) | Error 0.9144(0.8991) Steps 910(917.45) | Grad Norm 1.9770(2.2204) | Total Time 14.00(14.00)\n",
      "Iter 8450 | Time 23.2235(22.8039) | Bit/dim 3.4370(3.4283) | Xent 2.3026(2.3026) | Loss 3.4370(3.4283) | Error 0.9067(0.8993) Steps 934(919.66) | Grad Norm 1.9465(2.2265) | Total Time 14.00(14.00)\n",
      "Iter 8460 | Time 23.0883(22.8222) | Bit/dim 3.4702(3.4305) | Xent 2.3026(2.3026) | Loss 3.4702(3.4305) | Error 0.9200(0.9004) Steps 898(920.26) | Grad Norm 2.3993(2.2625) | Total Time 14.00(14.00)\n",
      "Iter 8470 | Time 22.5331(22.8493) | Bit/dim 3.4539(3.4307) | Xent 2.3026(2.3026) | Loss 3.4539(3.4307) | Error 0.8933(0.9018) Steps 880(917.73) | Grad Norm 1.6960(2.1850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 113.3639, Epoch Time 1385.9545(1330.7684), Bit/dim 3.4301(best: 3.4299), Xent 2.3026, Loss 3.4301, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 22.7564(22.8027) | Bit/dim 3.4154(3.4297) | Xent 2.3026(2.3026) | Loss 3.4154(3.4297) | Error 0.9011(0.9012) Steps 916(918.70) | Grad Norm 2.4031(2.2993) | Total Time 14.00(14.00)\n",
      "Iter 8490 | Time 22.2552(22.7699) | Bit/dim 3.4549(3.4286) | Xent 2.3026(2.3026) | Loss 3.4549(3.4286) | Error 0.9122(0.9014) Steps 934(920.26) | Grad Norm 2.3757(2.2830) | Total Time 14.00(14.00)\n",
      "Iter 8500 | Time 22.7911(22.7306) | Bit/dim 3.4057(3.4270) | Xent 2.3026(2.3026) | Loss 3.4057(3.4270) | Error 0.8933(0.9012) Steps 904(918.39) | Grad Norm 2.0005(2.2642) | Total Time 14.00(14.00)\n",
      "Iter 8510 | Time 23.0790(22.7860) | Bit/dim 3.4706(3.4287) | Xent 2.3026(2.3026) | Loss 3.4706(3.4287) | Error 0.8922(0.9012) Steps 928(919.14) | Grad Norm 2.6624(2.1236) | Total Time 14.00(14.00)\n",
      "Iter 8520 | Time 22.6341(22.8195) | Bit/dim 3.4430(3.4292) | Xent 2.3026(2.3026) | Loss 3.4430(3.4292) | Error 0.8911(0.8997) Steps 934(920.98) | Grad Norm 2.2603(2.3068) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 112.6862, Epoch Time 1383.8107(1332.3597), Bit/dim 3.4290(best: 3.4299), Xent 2.3026, Loss 3.4290, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 22.3368(22.8297) | Bit/dim 3.4151(3.4282) | Xent 2.3026(2.3026) | Loss 3.4151(3.4282) | Error 0.8911(0.8997) Steps 916(922.72) | Grad Norm 2.7167(2.2249) | Total Time 14.00(14.00)\n",
      "Iter 8540 | Time 22.2249(22.7719) | Bit/dim 3.4333(3.4302) | Xent 2.3026(2.3026) | Loss 3.4333(3.4302) | Error 0.9056(0.8997) Steps 910(923.38) | Grad Norm 1.9519(2.2517) | Total Time 14.00(14.00)\n",
      "Iter 8550 | Time 22.6428(22.7871) | Bit/dim 3.4044(3.4282) | Xent 2.3026(2.3026) | Loss 3.4044(3.4282) | Error 0.8944(0.8994) Steps 928(921.61) | Grad Norm 2.7885(2.2397) | Total Time 14.00(14.00)\n",
      "Iter 8560 | Time 22.1860(22.7927) | Bit/dim 3.3790(3.4299) | Xent 2.3026(2.3026) | Loss 3.3790(3.4299) | Error 0.8878(0.9013) Steps 904(920.73) | Grad Norm 3.6883(2.3339) | Total Time 14.00(14.00)\n",
      "Iter 8570 | Time 22.5334(22.8219) | Bit/dim 3.4219(3.4273) | Xent 2.3026(2.3026) | Loss 3.4219(3.4273) | Error 0.9056(0.9007) Steps 928(922.15) | Grad Norm 1.9072(2.3589) | Total Time 14.00(14.00)\n",
      "Iter 8580 | Time 23.2201(22.8631) | Bit/dim 3.4487(3.4277) | Xent 2.3026(2.3026) | Loss 3.4487(3.4277) | Error 0.9111(0.8999) Steps 934(926.19) | Grad Norm 1.9958(2.2562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 113.2232, Epoch Time 1387.5247(1334.0146), Bit/dim 3.4254(best: 3.4290), Xent 2.3026, Loss 3.4254, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 22.7593(22.9143) | Bit/dim 3.4374(3.4259) | Xent 2.3026(2.3026) | Loss 3.4374(3.4259) | Error 0.8978(0.8997) Steps 910(924.87) | Grad Norm 2.5453(2.2788) | Total Time 14.00(14.00)\n",
      "Iter 8600 | Time 23.0191(23.0050) | Bit/dim 3.3918(3.4246) | Xent 2.3026(2.3026) | Loss 3.3918(3.4246) | Error 0.8967(0.8991) Steps 952(928.99) | Grad Norm 1.7248(2.2943) | Total Time 14.00(14.00)\n",
      "Iter 8610 | Time 23.1430(23.0212) | Bit/dim 3.3961(3.4260) | Xent 2.3026(2.3026) | Loss 3.3961(3.4260) | Error 0.8922(0.9005) Steps 916(928.49) | Grad Norm 2.1282(2.2546) | Total Time 14.00(14.00)\n",
      "Iter 8620 | Time 22.9161(23.0253) | Bit/dim 3.4467(3.4255) | Xent 2.3026(2.3026) | Loss 3.4467(3.4255) | Error 0.8889(0.8993) Steps 946(927.89) | Grad Norm 2.2421(2.2233) | Total Time 14.00(14.00)\n",
      "Iter 8630 | Time 22.7599(23.0746) | Bit/dim 3.4579(3.4295) | Xent 2.3026(2.3026) | Loss 3.4579(3.4295) | Error 0.9000(0.9004) Steps 898(928.45) | Grad Norm 2.1493(2.2577) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 114.0677, Epoch Time 1404.7192(1336.1358), Bit/dim 3.4294(best: 3.4254), Xent 2.3026, Loss 3.4294, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 22.3748(23.0327) | Bit/dim 3.4540(3.4304) | Xent 2.3026(2.3026) | Loss 3.4540(3.4304) | Error 0.8911(0.9008) Steps 910(927.10) | Grad Norm 1.9223(2.2355) | Total Time 14.00(14.00)\n",
      "Iter 8650 | Time 23.1264(23.0268) | Bit/dim 3.3913(3.4278) | Xent 2.3026(2.3026) | Loss 3.3913(3.4278) | Error 0.8756(0.9005) Steps 952(928.26) | Grad Norm 2.7077(2.1919) | Total Time 14.00(14.00)\n",
      "Iter 8660 | Time 22.2274(22.9356) | Bit/dim 3.4106(3.4292) | Xent 2.3026(2.3026) | Loss 3.4106(3.4292) | Error 0.9011(0.9006) Steps 916(926.66) | Grad Norm 1.7013(2.2442) | Total Time 14.00(14.00)\n",
      "Iter 8670 | Time 23.7756(22.9543) | Bit/dim 3.4608(3.4276) | Xent 2.3026(2.3026) | Loss 3.4608(3.4276) | Error 0.9011(0.9001) Steps 946(928.27) | Grad Norm 2.2524(2.3513) | Total Time 14.00(14.00)\n",
      "Iter 8680 | Time 22.9339(22.8886) | Bit/dim 3.4206(3.4250) | Xent 2.3026(2.3026) | Loss 3.4206(3.4250) | Error 0.9011(0.8992) Steps 940(928.77) | Grad Norm 2.1875(2.2666) | Total Time 14.00(14.00)\n",
      "Iter 8690 | Time 22.7176(22.9397) | Bit/dim 3.4324(3.4262) | Xent 2.3026(2.3026) | Loss 3.4324(3.4262) | Error 0.8978(0.8997) Steps 946(929.74) | Grad Norm 2.6955(2.2266) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 112.8206, Epoch Time 1390.3879(1337.7633), Bit/dim 3.4298(best: 3.4254), Xent 2.3026, Loss 3.4298, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 23.1303(22.9297) | Bit/dim 3.4330(3.4257) | Xent 2.3026(2.3026) | Loss 3.4330(3.4257) | Error 0.8933(0.8999) Steps 952(931.89) | Grad Norm 2.6832(2.3494) | Total Time 14.00(14.00)\n",
      "Iter 8710 | Time 23.4218(22.8739) | Bit/dim 3.4694(3.4266) | Xent 2.3026(2.3026) | Loss 3.4694(3.4266) | Error 0.9044(0.9006) Steps 910(928.83) | Grad Norm 2.5966(2.2924) | Total Time 14.00(14.00)\n",
      "Iter 8720 | Time 22.8718(22.8966) | Bit/dim 3.4117(3.4275) | Xent 2.3026(2.3026) | Loss 3.4117(3.4275) | Error 0.8922(0.9007) Steps 916(929.49) | Grad Norm 2.1060(2.2816) | Total Time 14.00(14.00)\n",
      "Iter 8730 | Time 23.3226(22.9715) | Bit/dim 3.3719(3.4250) | Xent 2.3026(2.3026) | Loss 3.3719(3.4250) | Error 0.8922(0.8994) Steps 916(932.36) | Grad Norm 1.8718(2.2643) | Total Time 14.00(14.00)\n",
      "Iter 8740 | Time 23.0683(22.9518) | Bit/dim 3.4207(3.4241) | Xent 2.3026(2.3026) | Loss 3.4207(3.4241) | Error 0.9144(0.8999) Steps 934(934.47) | Grad Norm 2.5513(2.2940) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 114.5936, Epoch Time 1395.4111(1339.4928), Bit/dim 3.4264(best: 3.4254), Xent 2.3026, Loss 3.4264, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 22.1920(22.9592) | Bit/dim 3.4158(3.4246) | Xent 2.3026(2.3026) | Loss 3.4158(3.4246) | Error 0.8767(0.8997) Steps 928(934.50) | Grad Norm 3.6035(2.3330) | Total Time 14.00(14.00)\n",
      "Iter 8760 | Time 23.4867(22.9592) | Bit/dim 3.4054(3.4282) | Xent 2.3026(2.3026) | Loss 3.4054(3.4282) | Error 0.9089(0.9016) Steps 940(933.56) | Grad Norm 2.1502(2.3615) | Total Time 14.00(14.00)\n",
      "Iter 8770 | Time 22.7642(22.9314) | Bit/dim 3.4163(3.4239) | Xent 2.3026(2.3026) | Loss 3.4163(3.4239) | Error 0.8856(0.8987) Steps 952(932.63) | Grad Norm 3.2152(2.3653) | Total Time 14.00(14.00)\n",
      "Iter 8780 | Time 23.3870(22.9208) | Bit/dim 3.4086(3.4251) | Xent 2.3026(2.3026) | Loss 3.4086(3.4251) | Error 0.9133(0.8997) Steps 946(932.22) | Grad Norm 2.5193(2.3949) | Total Time 14.00(14.00)\n",
      "Iter 8790 | Time 23.0982(22.9712) | Bit/dim 3.4193(3.4261) | Xent 2.3026(2.3026) | Loss 3.4193(3.4261) | Error 0.9000(0.9006) Steps 922(930.06) | Grad Norm 1.6307(2.2258) | Total Time 14.00(14.00)\n",
      "Iter 8800 | Time 23.2749(22.9860) | Bit/dim 3.4444(3.4261) | Xent 2.3026(2.3026) | Loss 3.4444(3.4261) | Error 0.9011(0.8996) Steps 922(930.57) | Grad Norm 3.0336(2.4021) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 113.4662, Epoch Time 1395.1492(1341.1625), Bit/dim 3.4277(best: 3.4254), Xent 2.3026, Loss 3.4277, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 22.5503(22.9524) | Bit/dim 3.4220(3.4232) | Xent 2.3026(2.3026) | Loss 3.4220(3.4232) | Error 0.9122(0.8995) Steps 910(932.15) | Grad Norm 3.5938(2.4167) | Total Time 14.00(14.00)\n",
      "Iter 8820 | Time 23.2855(22.9909) | Bit/dim 3.4115(3.4222) | Xent 2.3026(2.3026) | Loss 3.4115(3.4222) | Error 0.9089(0.8993) Steps 928(930.72) | Grad Norm 1.7049(2.3412) | Total Time 14.00(14.00)\n",
      "Iter 8830 | Time 23.4809(23.0980) | Bit/dim 3.4369(3.4242) | Xent 2.3026(2.3026) | Loss 3.4369(3.4242) | Error 0.9022(0.8994) Steps 940(931.46) | Grad Norm 1.4573(2.1879) | Total Time 14.00(14.00)\n",
      "Iter 8840 | Time 22.7913(23.1400) | Bit/dim 3.4797(3.4256) | Xent 2.3026(2.3026) | Loss 3.4797(3.4256) | Error 0.9089(0.9003) Steps 946(932.82) | Grad Norm 2.3635(2.1575) | Total Time 14.00(14.00)\n",
      "Iter 8850 | Time 23.0699(23.1454) | Bit/dim 3.4232(3.4232) | Xent 2.3026(2.3026) | Loss 3.4232(3.4232) | Error 0.9044(0.9003) Steps 922(932.50) | Grad Norm 1.9867(2.2126) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 115.2908, Epoch Time 1409.4162(1343.2101), Bit/dim 3.4235(best: 3.4254), Xent 2.3026, Loss 3.4235, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 23.1668(23.1704) | Bit/dim 3.4398(3.4240) | Xent 2.3026(2.3026) | Loss 3.4398(3.4240) | Error 0.8933(0.9003) Steps 952(935.27) | Grad Norm 3.5596(2.2337) | Total Time 14.00(14.00)\n",
      "Iter 8870 | Time 23.2464(23.1916) | Bit/dim 3.4676(3.4254) | Xent 2.3026(2.3026) | Loss 3.4676(3.4254) | Error 0.9078(0.9007) Steps 952(936.70) | Grad Norm 1.8679(2.2355) | Total Time 14.00(14.00)\n",
      "Iter 8880 | Time 23.6980(23.2041) | Bit/dim 3.4013(3.4230) | Xent 2.3026(2.3026) | Loss 3.4013(3.4230) | Error 0.8833(0.8990) Steps 928(937.68) | Grad Norm 2.1283(2.0880) | Total Time 14.00(14.00)\n",
      "Iter 8890 | Time 23.2627(23.2350) | Bit/dim 3.4693(3.4252) | Xent 2.3026(2.3026) | Loss 3.4693(3.4252) | Error 0.8989(0.8990) Steps 946(938.62) | Grad Norm 2.3901(2.2533) | Total Time 14.00(14.00)\n",
      "Iter 8900 | Time 22.3533(23.2150) | Bit/dim 3.4486(3.4251) | Xent 2.3026(2.3026) | Loss 3.4486(3.4251) | Error 0.9111(0.9004) Steps 946(940.46) | Grad Norm 1.6221(2.1789) | Total Time 14.00(14.00)\n",
      "Iter 8910 | Time 23.1374(23.2157) | Bit/dim 3.4134(3.4228) | Xent 2.3026(2.3026) | Loss 3.4134(3.4228) | Error 0.9100(0.9006) Steps 928(939.85) | Grad Norm 2.0224(2.1223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 114.0058, Epoch Time 1409.7580(1345.2065), Bit/dim 3.4262(best: 3.4235), Xent 2.3026, Loss 3.4262, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 22.6919(23.1917) | Bit/dim 3.4572(3.4222) | Xent 2.3026(2.3026) | Loss 3.4572(3.4222) | Error 0.8944(0.9008) Steps 952(939.97) | Grad Norm 2.6008(2.2476) | Total Time 14.00(14.00)\n",
      "Iter 8930 | Time 23.2994(23.1939) | Bit/dim 3.4349(3.4231) | Xent 2.3026(2.3026) | Loss 3.4349(3.4231) | Error 0.8967(0.9010) Steps 934(940.69) | Grad Norm 1.7868(2.1872) | Total Time 14.00(14.00)\n",
      "Iter 8940 | Time 23.3123(23.2283) | Bit/dim 3.4435(3.4249) | Xent 2.3026(2.3026) | Loss 3.4435(3.4249) | Error 0.8822(0.8996) Steps 946(940.94) | Grad Norm 1.7371(2.2340) | Total Time 14.00(14.00)\n",
      "Iter 8950 | Time 22.8946(23.1503) | Bit/dim 3.4328(3.4235) | Xent 2.3026(2.3026) | Loss 3.4328(3.4235) | Error 0.9167(0.8992) Steps 928(936.27) | Grad Norm 2.1463(2.2660) | Total Time 14.00(14.00)\n",
      "Iter 8960 | Time 23.6612(23.1523) | Bit/dim 3.3972(3.4242) | Xent 2.3026(2.3026) | Loss 3.3972(3.4242) | Error 0.9111(0.8996) Steps 928(932.65) | Grad Norm 1.8535(2.1096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 116.8289, Epoch Time 1408.4568(1347.1040), Bit/dim 3.4236(best: 3.4235), Xent 2.3026, Loss 3.4236, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 23.4854(23.1950) | Bit/dim 3.4327(3.4247) | Xent 2.3026(2.3026) | Loss 3.4327(3.4247) | Error 0.8944(0.9005) Steps 958(935.25) | Grad Norm 2.9510(2.2425) | Total Time 14.00(14.00)\n",
      "Iter 8980 | Time 22.7949(23.1874) | Bit/dim 3.4247(3.4235) | Xent 2.3026(2.3026) | Loss 3.4247(3.4235) | Error 0.8811(0.9005) Steps 928(935.86) | Grad Norm 2.1500(2.2018) | Total Time 14.00(14.00)\n",
      "Iter 8990 | Time 23.4700(23.1973) | Bit/dim 3.3993(3.4198) | Xent 2.3026(2.3026) | Loss 3.3993(3.4198) | Error 0.9000(0.8996) Steps 946(938.58) | Grad Norm 1.8878(2.2521) | Total Time 14.00(14.00)\n",
      "Iter 9000 | Time 23.3773(23.1624) | Bit/dim 3.4429(3.4222) | Xent 2.3026(2.3026) | Loss 3.4429(3.4222) | Error 0.9244(0.9012) Steps 976(940.59) | Grad Norm 1.9247(2.2234) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1_temp --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
