{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    \n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        for i in range(1,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=4, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_4block_run2/current_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_4block_run2', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): SqueezeLayer()\n",
      "        (6): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): SqueezeLayer()\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): SqueezeLayer()\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 2846456\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 1729 | Time 158.8693(121.0013) | Bit/dim 3.6401(3.6364) | Xent 0.8266(0.8462) | Loss 17.8531(15.1194) | Error 0.2974(0.3029) Steps 1136(1102.58) | Grad Norm 8.0424(10.4426) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 104.0366(120.4924) | Bit/dim 3.6391(3.6364) | Xent 0.8400(0.8460) | Loss 14.1680(15.0908) | Error 0.2973(0.3027) Steps 1100(1102.50) | Grad Norm 7.9668(10.3684) | Total Time 0.00(0.00)\n",
      "Iter 1731 | Time 109.2949(120.1565) | Bit/dim 3.6231(3.6360) | Xent 0.8262(0.8454) | Loss 14.2906(15.0668) | Error 0.2983(0.3026) Steps 1088(1102.07) | Grad Norm 7.7789(10.2907) | Total Time 0.00(0.00)\n",
      "Iter 1732 | Time 102.4289(119.6246) | Bit/dim 3.6420(3.6362) | Xent 0.8293(0.8449) | Loss 14.1561(15.0395) | Error 0.3030(0.3026) Steps 1064(1100.92) | Grad Norm 5.3858(10.1435) | Total Time 0.00(0.00)\n",
      "Iter 1733 | Time 106.4442(119.2292) | Bit/dim 3.6358(3.6362) | Xent 0.8351(0.8446) | Loss 13.9280(15.0061) | Error 0.2946(0.3024) Steps 1100(1100.90) | Grad Norm 9.3646(10.1202) | Total Time 0.00(0.00)\n",
      "Iter 1734 | Time 101.3634(118.6932) | Bit/dim 3.6293(3.6360) | Xent 0.8373(0.8444) | Loss 13.6332(14.9650) | Error 0.3039(0.3024) Steps 1088(1100.51) | Grad Norm 9.0978(10.0895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 56.2783, Epoch Time 755.4974(772.0104), Bit/dim 3.6329(best: inf), Xent 0.8544, Loss 4.0601, Error 0.3076(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 117.8835(118.6689) | Bit/dim 3.6392(3.6361) | Xent 0.8239(0.8438) | Loss 20.1996(15.1220) | Error 0.3031(0.3024) Steps 1148(1101.93) | Grad Norm 8.2666(10.0348) | Total Time 0.00(0.00)\n",
      "Iter 1736 | Time 100.7750(118.1321) | Bit/dim 3.6395(3.6362) | Xent 0.8215(0.8431) | Loss 13.6126(15.0767) | Error 0.2884(0.3020) Steps 1076(1101.16) | Grad Norm 8.3720(9.9849) | Total Time 0.00(0.00)\n",
      "Iter 1737 | Time 105.7098(117.7595) | Bit/dim 3.6283(3.6360) | Xent 0.8343(0.8429) | Loss 14.1770(15.0497) | Error 0.2944(0.3018) Steps 1088(1100.76) | Grad Norm 8.9711(9.9545) | Total Time 0.00(0.00)\n",
      "Iter 1738 | Time 104.8130(117.3711) | Bit/dim 3.6334(3.6359) | Xent 0.8530(0.8432) | Loss 13.9697(15.0173) | Error 0.3033(0.3018) Steps 1112(1101.10) | Grad Norm 13.0892(10.0485) | Total Time 0.00(0.00)\n",
      "Iter 1739 | Time 97.6120(116.7783) | Bit/dim 3.6362(3.6359) | Xent 0.8395(0.8431) | Loss 14.1811(14.9922) | Error 0.3045(0.3019) Steps 1100(1101.07) | Grad Norm 9.6959(10.0380) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 103.4270(116.3778) | Bit/dim 3.6300(3.6357) | Xent 0.8254(0.8425) | Loss 14.0296(14.9634) | Error 0.2974(0.3018) Steps 1064(1099.95) | Grad Norm 5.2889(9.8955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 40.3878, Epoch Time 687.2465(769.4675), Bit/dim 3.6411(best: 3.6329), Xent 0.8651, Loss 4.0736, Error 0.3079(best: 0.3076)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 106.0639(116.0683) | Bit/dim 3.6455(3.6360) | Xent 0.8326(0.8422) | Loss 19.9655(15.1134) | Error 0.2991(0.3017) Steps 1088(1099.59) | Grad Norm 10.3571(9.9093) | Total Time 0.00(0.00)\n",
      "Iter 1742 | Time 101.0803(115.6187) | Bit/dim 3.6216(3.6356) | Xent 0.8375(0.8421) | Loss 14.0951(15.0829) | Error 0.2985(0.3016) Steps 1088(1099.25) | Grad Norm 11.3503(9.9526) | Total Time 0.00(0.00)\n",
      "Iter 1743 | Time 102.7928(115.2339) | Bit/dim 3.6356(3.6356) | Xent 0.8174(0.8413) | Loss 14.1142(15.0538) | Error 0.2986(0.3015) Steps 1070(1098.37) | Grad Norm 12.4819(10.0284) | Total Time 0.00(0.00)\n",
      "Iter 1744 | Time 106.6732(114.9771) | Bit/dim 3.6345(3.6355) | Xent 0.8335(0.8411) | Loss 14.2683(15.0302) | Error 0.3006(0.3015) Steps 1088(1098.06) | Grad Norm 10.9199(10.0552) | Total Time 0.00(0.00)\n",
      "Iter 1745 | Time 107.1073(114.7410) | Bit/dim 3.6373(3.6356) | Xent 0.8312(0.8408) | Loss 14.2715(15.0075) | Error 0.3004(0.3014) Steps 1130(1099.02) | Grad Norm 8.0813(9.9960) | Total Time 0.00(0.00)\n",
      "Iter 1746 | Time 100.1742(114.3040) | Bit/dim 3.6348(3.6356) | Xent 0.8597(0.8414) | Loss 14.0313(14.9782) | Error 0.3100(0.3017) Steps 1088(1098.69) | Grad Norm 11.9350(10.0541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 40.4837, Epoch Time 680.8859(766.8100), Bit/dim 3.6327(best: 3.6329), Xent 0.8399, Loss 4.0527, Error 0.2987(best: 0.3076)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 110.0735(114.1771) | Bit/dim 3.6332(3.6355) | Xent 0.8333(0.8411) | Loss 19.9721(15.1280) | Error 0.2967(0.3015) Steps 1124(1099.45) | Grad Norm 4.5395(9.8887) | Total Time 0.00(0.00)\n",
      "Iter 1748 | Time 106.0671(113.9338) | Bit/dim 3.6344(3.6355) | Xent 0.8276(0.8407) | Loss 14.2094(15.1005) | Error 0.2960(0.3014) Steps 1112(1099.82) | Grad Norm 12.5213(9.9677) | Total Time 0.00(0.00)\n",
      "Iter 1749 | Time 103.2162(113.6123) | Bit/dim 3.6276(3.6352) | Xent 0.8638(0.8414) | Loss 14.0301(15.0683) | Error 0.3065(0.3015) Steps 1100(1099.83) | Grad Norm 14.4969(10.1036) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 121.3290(113.8438) | Bit/dim 3.6328(3.6352) | Xent 0.8398(0.8414) | Loss 14.3234(15.0460) | Error 0.3005(0.3015) Steps 1124(1100.55) | Grad Norm 11.4549(10.1441) | Total Time 0.00(0.00)\n",
      "Iter 1751 | Time 120.1429(114.0327) | Bit/dim 3.6379(3.6352) | Xent 0.8368(0.8412) | Loss 14.0902(15.0173) | Error 0.2975(0.3014) Steps 1106(1100.72) | Grad Norm 8.4352(10.0928) | Total Time 0.00(0.00)\n",
      "Iter 1752 | Time 119.0521(114.1833) | Bit/dim 3.6380(3.6353) | Xent 0.8856(0.8426) | Loss 14.3853(14.9984) | Error 0.3170(0.3019) Steps 1100(1100.69) | Grad Norm 12.8297(10.1749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 40.6135, Epoch Time 737.0403(765.9169), Bit/dim 3.6413(best: 3.6327), Xent 0.9404, Loss 4.1114, Error 0.3380(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 118.7190(114.3194) | Bit/dim 3.6504(3.6358) | Xent 0.9015(0.8443) | Loss 19.9700(15.1475) | Error 0.3206(0.3024) Steps 1082(1100.13) | Grad Norm 19.1225(10.4434) | Total Time 0.00(0.00)\n",
      "Iter 1754 | Time 121.0146(114.5202) | Bit/dim 3.6406(3.6359) | Xent 0.8997(0.8460) | Loss 14.4109(15.1254) | Error 0.3254(0.3031) Steps 1100(1100.13) | Grad Norm 22.1044(10.7932) | Total Time 0.00(0.00)\n",
      "Iter 1755 | Time 117.3307(114.6046) | Bit/dim 3.6350(3.6359) | Xent 0.8420(0.8459) | Loss 14.0511(15.0932) | Error 0.2987(0.3030) Steps 1124(1100.85) | Grad Norm 11.3155(10.8089) | Total Time 0.00(0.00)\n",
      "Iter 1756 | Time 117.7634(114.6993) | Bit/dim 3.6385(3.6360) | Xent 0.8644(0.8464) | Loss 14.1511(15.0649) | Error 0.3090(0.3032) Steps 1088(1100.46) | Grad Norm 19.4625(11.0685) | Total Time 0.00(0.00)\n",
      "Iter 1757 | Time 117.0590(114.7701) | Bit/dim 3.6283(3.6357) | Xent 0.8180(0.8456) | Loss 14.2582(15.0407) | Error 0.2933(0.3029) Steps 1088(1100.09) | Grad Norm 7.0815(10.9489) | Total Time 0.00(0.00)\n",
      "Iter 1758 | Time 124.9611(115.0758) | Bit/dim 3.6433(3.6360) | Xent 0.8513(0.8457) | Loss 14.2025(15.0156) | Error 0.3026(0.3028) Steps 1124(1100.80) | Grad Norm 13.9816(11.0398) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 41.4504, Epoch Time 776.1225(766.2231), Bit/dim 3.6388(best: 3.6327), Xent 0.8692, Loss 4.0734, Error 0.3086(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 116.5783(115.1209) | Bit/dim 3.6433(3.6362) | Xent 0.8630(0.8463) | Loss 19.8919(15.1619) | Error 0.3056(0.3029) Steps 1112(1101.14) | Grad Norm 10.5798(11.0260) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 113.3211(115.0669) | Bit/dim 3.6429(3.6364) | Xent 0.8391(0.8461) | Loss 14.0464(15.1284) | Error 0.3017(0.3029) Steps 1088(1100.75) | Grad Norm 13.6281(11.1041) | Total Time 0.00(0.00)\n",
      "Iter 1761 | Time 116.6153(115.1134) | Bit/dim 3.6282(3.6361) | Xent 0.8173(0.8452) | Loss 14.0619(15.0964) | Error 0.2973(0.3027) Steps 1082(1100.18) | Grad Norm 4.8851(10.9175) | Total Time 0.00(0.00)\n",
      "Iter 1762 | Time 118.7256(115.2217) | Bit/dim 3.6396(3.6362) | Xent 0.8274(0.8447) | Loss 14.2160(15.0700) | Error 0.2954(0.3025) Steps 1076(1099.46) | Grad Norm 10.5965(10.9079) | Total Time 0.00(0.00)\n",
      "Iter 1763 | Time 119.8941(115.3619) | Bit/dim 3.6438(3.6365) | Xent 0.8149(0.8438) | Loss 14.2490(15.0454) | Error 0.2961(0.3023) Steps 1118(1100.01) | Grad Norm 5.4097(10.7430) | Total Time 0.00(0.00)\n",
      "Iter 1764 | Time 126.6535(115.7007) | Bit/dim 3.6328(3.6364) | Xent 0.8647(0.8444) | Loss 14.2336(15.0210) | Error 0.3129(0.3026) Steps 1136(1101.09) | Grad Norm 9.5480(10.7071) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 41.0578, Epoch Time 769.1930(766.3122), Bit/dim 3.6414(best: 3.6327), Xent 0.8599, Loss 4.0714, Error 0.3069(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 122.3923(115.9014) | Bit/dim 3.6451(3.6366) | Xent 0.8170(0.8436) | Loss 20.5521(15.1869) | Error 0.3047(0.3027) Steps 1088(1100.70) | Grad Norm 11.0118(10.7163) | Total Time 0.00(0.00)\n",
      "Iter 1766 | Time 123.4269(116.1272) | Bit/dim 3.6349(3.6366) | Xent 0.8172(0.8428) | Loss 14.1840(15.1569) | Error 0.2914(0.3024) Steps 1130(1101.58) | Grad Norm 7.1629(10.6097) | Total Time 0.00(0.00)\n",
      "Iter 1767 | Time 119.5679(116.2304) | Bit/dim 3.6199(3.6361) | Xent 0.8372(0.8426) | Loss 14.1944(15.1280) | Error 0.3029(0.3024) Steps 1100(1101.53) | Grad Norm 9.5297(10.5773) | Total Time 0.00(0.00)\n",
      "Iter 1768 | Time 121.3576(116.3842) | Bit/dim 3.6409(3.6362) | Xent 0.8470(0.8427) | Loss 14.3299(15.1040) | Error 0.3040(0.3024) Steps 1106(1101.67) | Grad Norm 11.8807(10.6164) | Total Time 0.00(0.00)\n",
      "Iter 1769 | Time 120.9048(116.5198) | Bit/dim 3.6428(3.6364) | Xent 0.8663(0.8434) | Loss 14.3086(15.0802) | Error 0.3043(0.3025) Steps 1124(1102.34) | Grad Norm 12.0857(10.6604) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 114.7172(116.4657) | Bit/dim 3.6364(3.6364) | Xent 0.8490(0.8436) | Loss 14.1565(15.0525) | Error 0.3077(0.3026) Steps 1082(1101.73) | Grad Norm 12.9559(10.7293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 40.8843, Epoch Time 779.3914(766.7046), Bit/dim 3.6323(best: 3.6327), Xent 0.8757, Loss 4.0701, Error 0.3127(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 121.8811(116.6282) | Bit/dim 3.6321(3.6363) | Xent 0.8493(0.8438) | Loss 20.1875(15.2065) | Error 0.3045(0.3027) Steps 1100(1101.67) | Grad Norm 11.7387(10.7596) | Total Time 0.00(0.00)\n",
      "Iter 1772 | Time 120.9865(116.7590) | Bit/dim 3.6315(3.6361) | Xent 0.8177(0.8430) | Loss 14.2645(15.1783) | Error 0.2971(0.3025) Steps 1058(1100.36) | Grad Norm 5.0808(10.5892) | Total Time 0.00(0.00)\n",
      "Iter 1773 | Time 115.4405(116.7194) | Bit/dim 3.6378(3.6362) | Xent 0.8450(0.8431) | Loss 14.1648(15.1478) | Error 0.3023(0.3025) Steps 1130(1101.25) | Grad Norm 10.5656(10.5885) | Total Time 0.00(0.00)\n",
      "Iter 1774 | Time 113.2648(116.6158) | Bit/dim 3.6338(3.6361) | Xent 0.8302(0.8427) | Loss 13.7046(15.1046) | Error 0.2945(0.3023) Steps 1076(1100.50) | Grad Norm 8.7606(10.5337) | Total Time 0.00(0.00)\n",
      "Iter 1775 | Time 115.5800(116.5847) | Bit/dim 3.6279(3.6359) | Xent 0.8381(0.8425) | Loss 14.1808(15.0768) | Error 0.2974(0.3021) Steps 1106(1100.66) | Grad Norm 8.0833(10.4602) | Total Time 0.00(0.00)\n",
      "Iter 1776 | Time 124.0072(116.8074) | Bit/dim 3.6305(3.6357) | Xent 0.8127(0.8416) | Loss 14.2925(15.0533) | Error 0.2877(0.3017) Steps 1124(1101.36) | Grad Norm 4.3019(10.2754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 40.4278, Epoch Time 767.9812(766.7429), Bit/dim 3.6344(best: 3.6323), Xent 0.8472, Loss 4.0580, Error 0.3039(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 121.7534(116.9557) | Bit/dim 3.6392(3.6358) | Xent 0.8225(0.8411) | Loss 19.7634(15.1946) | Error 0.2923(0.3014) Steps 1106(1101.50) | Grad Norm 7.1607(10.1820) | Total Time 0.00(0.00)\n",
      "Iter 1778 | Time 122.0506(117.1086) | Bit/dim 3.6387(3.6359) | Xent 0.8144(0.8403) | Loss 14.0169(15.1593) | Error 0.2907(0.3011) Steps 1112(1101.82) | Grad Norm 8.0701(10.1186) | Total Time 0.00(0.00)\n",
      "Iter 1779 | Time 112.9556(116.9840) | Bit/dim 3.6305(3.6357) | Xent 0.8356(0.8401) | Loss 14.1213(15.1281) | Error 0.2990(0.3010) Steps 1094(1101.58) | Grad Norm 7.8920(10.0518) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 124.6424(117.2138) | Bit/dim 3.6345(3.6357) | Xent 0.8097(0.8392) | Loss 14.0217(15.0949) | Error 0.2925(0.3008) Steps 1130(1102.43) | Grad Norm 7.4508(9.9738) | Total Time 0.00(0.00)\n",
      "Iter 1781 | Time 112.2019(117.0634) | Bit/dim 3.6297(3.6355) | Xent 0.7989(0.8380) | Loss 14.1538(15.0667) | Error 0.2849(0.3003) Steps 1094(1102.18) | Grad Norm 4.9180(9.8221) | Total Time 0.00(0.00)\n",
      "Iter 1782 | Time 115.1290(117.0054) | Bit/dim 3.6332(3.6355) | Xent 0.8109(0.8372) | Loss 13.9858(15.0343) | Error 0.2953(0.3001) Steps 1076(1101.40) | Grad Norm 7.6086(9.7557) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 39.6063, Epoch Time 765.2417(766.6978), Bit/dim 3.6361(best: 3.6323), Xent 0.8394, Loss 4.0558, Error 0.2994(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 120.8966(117.1221) | Bit/dim 3.6297(3.6353) | Xent 0.7993(0.8361) | Loss 19.9290(15.1811) | Error 0.2761(0.2994) Steps 1106(1101.53) | Grad Norm 6.9524(9.6716) | Total Time 0.00(0.00)\n",
      "Iter 1784 | Time 116.4739(117.1027) | Bit/dim 3.6392(3.6354) | Xent 0.8158(0.8355) | Loss 14.0730(15.1479) | Error 0.2969(0.2993) Steps 1082(1100.95) | Grad Norm 5.7875(9.5551) | Total Time 0.00(0.00)\n",
      "Iter 1785 | Time 116.9520(117.0981) | Bit/dim 3.6322(3.6353) | Xent 0.8004(0.8344) | Loss 13.9365(15.1115) | Error 0.2886(0.2990) Steps 1082(1100.38) | Grad Norm 2.9355(9.3565) | Total Time 0.00(0.00)\n",
      "Iter 1786 | Time 120.0087(117.1855) | Bit/dim 3.6196(3.6348) | Xent 0.8066(0.8336) | Loss 13.8173(15.0727) | Error 0.2900(0.2988) Steps 1076(1099.65) | Grad Norm 8.2310(9.3227) | Total Time 0.00(0.00)\n",
      "Iter 1787 | Time 121.7483(117.3223) | Bit/dim 3.6333(3.6348) | Xent 0.8416(0.8338) | Loss 14.3800(15.0519) | Error 0.2990(0.2988) Steps 1094(1099.48) | Grad Norm 14.8096(9.4873) | Total Time 0.00(0.00)\n",
      "Iter 1788 | Time 113.2589(117.2004) | Bit/dim 3.6438(3.6351) | Xent 0.8700(0.8349) | Loss 14.4085(15.0326) | Error 0.3040(0.2989) Steps 1088(1099.13) | Grad Norm 17.9147(9.7402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 39.8243, Epoch Time 765.4694(766.6610), Bit/dim 3.6324(best: 3.6323), Xent 0.8887, Loss 4.0767, Error 0.3154(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 117.1469(117.1988) | Bit/dim 3.6267(3.6348) | Xent 0.8533(0.8354) | Loss 19.8992(15.1786) | Error 0.3071(0.2992) Steps 1094(1098.98) | Grad Norm 16.9055(9.9551) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 126.4301(117.4758) | Bit/dim 3.6412(3.6350) | Xent 0.8444(0.8357) | Loss 14.1373(15.1474) | Error 0.3027(0.2993) Steps 1118(1099.55) | Grad Norm 14.1707(10.0816) | Total Time 0.00(0.00)\n",
      "Iter 1791 | Time 119.7225(117.5432) | Bit/dim 3.6385(3.6351) | Xent 0.8394(0.8358) | Loss 14.2041(15.1191) | Error 0.2975(0.2992) Steps 1106(1099.74) | Grad Norm 13.2410(10.1764) | Total Time 0.00(0.00)\n",
      "Iter 1792 | Time 116.4466(117.5103) | Bit/dim 3.6405(3.6353) | Xent 0.8389(0.8359) | Loss 13.9441(15.0838) | Error 0.2970(0.2992) Steps 1118(1100.29) | Grad Norm 12.8269(10.2559) | Total Time 0.00(0.00)\n",
      "Iter 1793 | Time 118.0843(117.5275) | Bit/dim 3.6341(3.6352) | Xent 0.8035(0.8349) | Loss 14.2020(15.0574) | Error 0.2866(0.2988) Steps 1082(1099.74) | Grad Norm 5.1505(10.1027) | Total Time 0.00(0.00)\n",
      "Iter 1794 | Time 116.1030(117.4848) | Bit/dim 3.6241(3.6349) | Xent 0.8112(0.8342) | Loss 14.0292(15.0265) | Error 0.2900(0.2985) Steps 1094(1099.57) | Grad Norm 9.0671(10.0717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 40.0487, Epoch Time 770.6477(766.7806), Bit/dim 3.6341(best: 3.6323), Xent 0.8499, Loss 4.0590, Error 0.3034(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 122.5928(117.6380) | Bit/dim 3.6292(3.6347) | Xent 0.8043(0.8333) | Loss 19.8105(15.1701) | Error 0.2916(0.2983) Steps 1100(1099.58) | Grad Norm 10.4354(10.0826) | Total Time 0.00(0.00)\n",
      "Iter 1796 | Time 124.9562(117.8575) | Bit/dim 3.6305(3.6346) | Xent 0.8017(0.8324) | Loss 14.2176(15.1415) | Error 0.2889(0.2980) Steps 1112(1099.96) | Grad Norm 5.7889(9.9538) | Total Time 0.00(0.00)\n",
      "Iter 1797 | Time 120.1384(117.9260) | Bit/dim 3.6452(3.6349) | Xent 0.8200(0.8320) | Loss 14.1378(15.1114) | Error 0.2927(0.2979) Steps 1082(1099.42) | Grad Norm 8.2143(9.9016) | Total Time 0.00(0.00)\n",
      "Iter 1798 | Time 121.8268(118.0430) | Bit/dim 3.6337(3.6349) | Xent 0.8174(0.8316) | Loss 14.1352(15.0821) | Error 0.2860(0.2975) Steps 1082(1098.89) | Grad Norm 9.5982(9.8925) | Total Time 0.00(0.00)\n",
      "Iter 1799 | Time 119.1228(118.0754) | Bit/dim 3.6230(3.6345) | Xent 0.8075(0.8308) | Loss 14.2629(15.0575) | Error 0.2895(0.2973) Steps 1124(1099.65) | Grad Norm 6.8342(9.8007) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 113.7841(117.9467) | Bit/dim 3.6417(3.6347) | Xent 0.8015(0.8300) | Loss 13.9911(15.0255) | Error 0.2855(0.2969) Steps 1118(1100.20) | Grad Norm 5.0904(9.6594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 40.2290, Epoch Time 779.1174(767.1507), Bit/dim 3.6336(best: 3.6323), Xent 0.8357, Loss 4.0514, Error 0.2975(best: 0.2987)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 119.3566(117.9889) | Bit/dim 3.6319(3.6347) | Xent 0.8150(0.8295) | Loss 20.0005(15.1748) | Error 0.2964(0.2969) Steps 1082(1099.65) | Grad Norm 8.3587(9.6204) | Total Time 0.00(0.00)\n",
      "Iter 1802 | Time 108.7740(117.7125) | Bit/dim 3.6244(3.6343) | Xent 0.7994(0.8286) | Loss 13.9347(15.1376) | Error 0.2866(0.2966) Steps 1070(1098.76) | Grad Norm 5.7703(9.5049) | Total Time 0.00(0.00)\n",
      "Iter 1803 | Time 114.1942(117.6070) | Bit/dim 3.6432(3.6346) | Xent 0.8025(0.8278) | Loss 13.9577(15.1022) | Error 0.2843(0.2962) Steps 1100(1098.80) | Grad Norm 8.4549(9.4734) | Total Time 0.00(0.00)\n",
      "Iter 1804 | Time 118.8502(117.6442) | Bit/dim 3.6403(3.6348) | Xent 0.8212(0.8276) | Loss 13.8098(15.0634) | Error 0.2913(0.2961) Steps 1100(1098.84) | Grad Norm 12.5386(9.5653) | Total Time 0.00(0.00)\n",
      "Iter 1805 | Time 119.2131(117.6913) | Bit/dim 3.6253(3.6345) | Xent 0.8470(0.8282) | Loss 14.2881(15.0401) | Error 0.3030(0.2963) Steps 1136(1099.95) | Grad Norm 15.0225(9.7291) | Total Time 0.00(0.00)\n",
      "Iter 1806 | Time 112.6582(117.5403) | Bit/dim 3.6379(3.6346) | Xent 0.8623(0.8292) | Loss 14.0686(15.0110) | Error 0.3126(0.2968) Steps 1094(1099.77) | Grad Norm 17.7881(9.9708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 40.6571, Epoch Time 750.0623(766.6380), Bit/dim 3.6369(best: 3.6323), Xent 0.8999, Loss 4.0869, Error 0.3242(best: 0.2975)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 120.5726(117.6313) | Bit/dim 3.6357(3.6346) | Xent 0.8572(0.8301) | Loss 19.8002(15.1547) | Error 0.3074(0.2971) Steps 1106(1099.96) | Grad Norm 16.4919(10.1665) | Total Time 0.00(0.00)\n",
      "Iter 1808 | Time 124.0927(117.8251) | Bit/dim 3.6267(3.6344) | Xent 0.8177(0.8297) | Loss 14.0951(15.1229) | Error 0.2931(0.2970) Steps 1094(1099.78) | Grad Norm 9.1966(10.1374) | Total Time 0.00(0.00)\n",
      "Iter 1809 | Time 121.2672(117.9284) | Bit/dim 3.6212(3.6340) | Xent 0.8293(0.8297) | Loss 14.0649(15.0911) | Error 0.2954(0.2969) Steps 1106(1099.97) | Grad Norm 8.5149(10.0887) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 114.3717(117.8217) | Bit/dim 3.6365(3.6341) | Xent 0.8187(0.8294) | Loss 13.8725(15.0546) | Error 0.2911(0.2967) Steps 1112(1100.33) | Grad Norm 9.8968(10.0829) | Total Time 0.00(0.00)\n",
      "Iter 1811 | Time 119.1540(117.8617) | Bit/dim 3.6421(3.6343) | Xent 0.7920(0.8282) | Loss 13.9802(15.0224) | Error 0.2829(0.2963) Steps 1106(1100.50) | Grad Norm 4.6662(9.9204) | Total Time 0.00(0.00)\n",
      "Iter 1812 | Time 121.3523(117.9664) | Bit/dim 3.6245(3.6340) | Xent 0.8383(0.8285) | Loss 14.2328(14.9987) | Error 0.2959(0.2963) Steps 1100(1100.48) | Grad Norm 7.5790(9.8502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 40.1733, Epoch Time 777.5957(766.9668), Bit/dim 3.6367(best: 3.6323), Xent 0.8556, Loss 4.0645, Error 0.3061(best: 0.2975)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 118.4608(117.9812) | Bit/dim 3.6384(3.6342) | Xent 0.7980(0.8276) | Loss 19.9652(15.1477) | Error 0.2804(0.2958) Steps 1100(1100.47) | Grad Norm 8.8568(9.8204) | Total Time 0.00(0.00)\n",
      "Iter 1814 | Time 118.6198(118.0004) | Bit/dim 3.6345(3.6342) | Xent 0.8038(0.8269) | Loss 14.2080(15.1195) | Error 0.2901(0.2957) Steps 1088(1100.09) | Grad Norm 5.4978(9.6907) | Total Time 0.00(0.00)\n",
      "Iter 1815 | Time 119.7882(118.0540) | Bit/dim 3.6276(3.6340) | Xent 0.8007(0.8261) | Loss 14.1899(15.0916) | Error 0.2874(0.2954) Steps 1088(1099.73) | Grad Norm 8.4536(9.6536) | Total Time 0.00(0.00)\n",
      "Iter 1816 | Time 117.0605(118.0242) | Bit/dim 3.6278(3.6338) | Xent 0.8218(0.8260) | Loss 14.0060(15.0590) | Error 0.2944(0.2954) Steps 1070(1098.84) | Grad Norm 10.5032(9.6791) | Total Time 0.00(0.00)\n",
      "Iter 1817 | Time 119.7795(118.0769) | Bit/dim 3.6219(3.6334) | Xent 0.8276(0.8260) | Loss 13.7552(15.0199) | Error 0.2996(0.2955) Steps 1106(1099.05) | Grad Norm 9.3602(9.6695) | Total Time 0.00(0.00)\n",
      "Iter 1818 | Time 119.5142(118.1200) | Bit/dim 3.6208(3.6330) | Xent 0.8166(0.8258) | Loss 14.1760(14.9946) | Error 0.2900(0.2954) Steps 1076(1098.36) | Grad Norm 10.4632(9.6933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 40.2311, Epoch Time 769.9108(767.0551), Bit/dim 3.6325(best: 3.6323), Xent 0.8623, Loss 4.0636, Error 0.3108(best: 0.2975)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 123.0582(118.2681) | Bit/dim 3.6335(3.6331) | Xent 0.8266(0.8258) | Loss 19.8268(15.1396) | Error 0.2997(0.2955) Steps 1118(1098.95) | Grad Norm 13.2987(9.8015) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 118.1782(118.2654) | Bit/dim 3.6346(3.6331) | Xent 0.7954(0.8249) | Loss 14.0548(15.1070) | Error 0.2857(0.2952) Steps 1124(1099.70) | Grad Norm 10.5951(9.8253) | Total Time 0.00(0.00)\n",
      "Iter 1821 | Time 118.0963(118.2604) | Bit/dim 3.6194(3.6327) | Xent 0.8080(0.8244) | Loss 14.1044(15.0769) | Error 0.2851(0.2949) Steps 1082(1099.17) | Grad Norm 2.2072(9.5968) | Total Time 0.00(0.00)\n",
      "Iter 1822 | Time 121.5906(118.3603) | Bit/dim 3.6431(3.6330) | Xent 0.8032(0.8237) | Loss 14.1676(15.0497) | Error 0.2885(0.2947) Steps 1094(1099.02) | Grad Norm 8.6627(9.5687) | Total Time 0.00(0.00)\n",
      "Iter 1823 | Time 118.0351(118.3505) | Bit/dim 3.6368(3.6331) | Xent 0.8126(0.8234) | Loss 14.0170(15.0187) | Error 0.2977(0.2948) Steps 1100(1099.05) | Grad Norm 6.6104(9.4800) | Total Time 0.00(0.00)\n",
      "Iter 1824 | Time 117.2565(118.3177) | Bit/dim 3.6280(3.6330) | Xent 0.8172(0.8232) | Loss 14.1909(14.9938) | Error 0.2949(0.2948) Steps 1082(1098.54) | Grad Norm 5.7798(9.3690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 40.3143, Epoch Time 773.2349(767.2405), Bit/dim 3.6329(best: 3.6323), Xent 0.8344, Loss 4.0501, Error 0.2984(best: 0.2975)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 120.2764(118.3764) | Bit/dim 3.6406(3.6332) | Xent 0.7879(0.8221) | Loss 19.9145(15.1415) | Error 0.2805(0.2944) Steps 1064(1097.50) | Grad Norm 7.0163(9.2984) | Total Time 0.00(0.00)\n",
      "Iter 1826 | Time 119.1130(118.3985) | Bit/dim 3.6286(3.6331) | Xent 0.8157(0.8220) | Loss 13.9045(15.1043) | Error 0.2929(0.2943) Steps 1076(1096.85) | Grad Norm 8.2391(9.2666) | Total Time 0.00(0.00)\n",
      "Iter 1827 | Time 120.4496(118.4601) | Bit/dim 3.6357(3.6331) | Xent 0.8046(0.8214) | Loss 14.1330(15.0752) | Error 0.2891(0.2942) Steps 1112(1097.31) | Grad Norm 8.7113(9.2500) | Total Time 0.00(0.00)\n",
      "Iter 1828 | Time 119.4746(118.4905) | Bit/dim 3.6324(3.6331) | Xent 0.7884(0.8204) | Loss 14.2776(15.0513) | Error 0.2830(0.2938) Steps 1088(1097.03) | Grad Norm 9.0980(9.2454) | Total Time 0.00(0.00)\n",
      "Iter 1829 | Time 118.8716(118.5019) | Bit/dim 3.6345(3.6332) | Xent 0.8012(0.8199) | Loss 14.1249(15.0235) | Error 0.2920(0.2938) Steps 1112(1097.48) | Grad Norm 8.9753(9.2373) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 126.1494(118.7314) | Bit/dim 3.6315(3.6331) | Xent 0.8094(0.8196) | Loss 14.2945(15.0016) | Error 0.2923(0.2937) Steps 1076(1096.83) | Grad Norm 9.9788(9.2595) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 40.5764, Epoch Time 781.4365(767.6664), Bit/dim 3.6357(best: 3.6323), Xent 0.8306, Loss 4.0511, Error 0.2959(best: 0.2975)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1831 | Time 115.9958(118.6493) | Bit/dim 3.6239(3.6328) | Xent 0.8009(0.8190) | Loss 19.9069(15.1488) | Error 0.2854(0.2935) Steps 1106(1097.11) | Grad Norm 7.7955(9.2156) | Total Time 0.00(0.00)\n",
      "Iter 1832 | Time 116.9069(118.5970) | Bit/dim 3.6282(3.6327) | Xent 0.7990(0.8184) | Loss 14.2442(15.1216) | Error 0.2889(0.2933) Steps 1100(1097.20) | Grad Norm 6.8028(9.1432) | Total Time 0.00(0.00)\n",
      "Iter 1833 | Time 115.5231(118.5048) | Bit/dim 3.6225(3.6324) | Xent 0.7927(0.8176) | Loss 13.6444(15.0773) | Error 0.2869(0.2931) Steps 1076(1096.56) | Grad Norm 5.7203(9.0405) | Total Time 0.00(0.00)\n",
      "Iter 1834 | Time 117.6601(118.4795) | Bit/dim 3.6350(3.6325) | Xent 0.7954(0.8170) | Loss 14.1164(15.0485) | Error 0.2846(0.2929) Steps 1124(1097.38) | Grad Norm 5.4518(8.9329) | Total Time 0.00(0.00)\n",
      "Iter 1835 | Time 122.7524(118.6077) | Bit/dim 3.6332(3.6325) | Xent 0.8017(0.8165) | Loss 13.9441(15.0154) | Error 0.2879(0.2927) Steps 1112(1097.82) | Grad Norm 7.2885(8.8836) | Total Time 0.00(0.00)\n",
      "Iter 1836 | Time 116.1271(118.5332) | Bit/dim 3.6295(3.6324) | Xent 0.8148(0.8165) | Loss 14.0404(14.9861) | Error 0.2939(0.2928) Steps 1106(1098.07) | Grad Norm 10.2635(8.9250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 40.4766, Epoch Time 761.8571(767.4921), Bit/dim 3.6373(best: 3.6323), Xent 0.8518, Loss 4.0632, Error 0.3035(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1837 | Time 122.6711(118.6574) | Bit/dim 3.6409(3.6327) | Xent 0.7932(0.8158) | Loss 20.2587(15.1443) | Error 0.2780(0.2923) Steps 1112(1098.49) | Grad Norm 13.2487(9.0547) | Total Time 0.00(0.00)\n",
      "Iter 1838 | Time 115.0405(118.5489) | Bit/dim 3.6427(3.6330) | Xent 0.8638(0.8172) | Loss 14.1188(15.1135) | Error 0.3041(0.2927) Steps 1058(1097.27) | Grad Norm 20.5985(9.4010) | Total Time 0.00(0.00)\n",
      "Iter 1839 | Time 119.9789(118.5918) | Bit/dim 3.6264(3.6328) | Xent 0.9080(0.8199) | Loss 14.3549(15.0908) | Error 0.3167(0.2934) Steps 1100(1097.35) | Grad Norm 22.6180(9.7975) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 119.9436(118.6323) | Bit/dim 3.6250(3.6325) | Xent 0.8573(0.8210) | Loss 14.1125(15.0614) | Error 0.3089(0.2939) Steps 1094(1097.25) | Grad Norm 10.9008(9.8306) | Total Time 0.00(0.00)\n",
      "Iter 1841 | Time 119.4503(118.6569) | Bit/dim 3.6317(3.6325) | Xent 0.8375(0.8215) | Loss 14.2691(15.0376) | Error 0.3071(0.2943) Steps 1088(1096.97) | Grad Norm 15.4697(9.9998) | Total Time 0.00(0.00)\n",
      "Iter 1842 | Time 117.5976(118.6251) | Bit/dim 3.6353(3.6326) | Xent 0.9182(0.8244) | Loss 14.3174(15.0160) | Error 0.3227(0.2951) Steps 1100(1097.07) | Grad Norm 23.5123(10.4051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 39.8862, Epoch Time 771.2775(767.6056), Bit/dim 3.6387(best: 3.6323), Xent 0.8490, Loss 4.0632, Error 0.3025(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1843 | Time 113.7832(118.4798) | Bit/dim 3.6304(3.6325) | Xent 0.8332(0.8247) | Loss 19.8278(15.1604) | Error 0.2941(0.2951) Steps 1076(1096.43) | Grad Norm 12.2999(10.4620) | Total Time 0.00(0.00)\n",
      "Iter 1844 | Time 118.6388(118.4846) | Bit/dim 3.6441(3.6329) | Xent 0.9391(0.8281) | Loss 14.5043(15.1407) | Error 0.3394(0.2964) Steps 1088(1096.18) | Grad Norm 23.8437(10.8634) | Total Time 0.00(0.00)\n",
      "Iter 1845 | Time 118.4370(118.4832) | Bit/dim 3.6267(3.6327) | Xent 0.7993(0.8273) | Loss 13.9662(15.1055) | Error 0.2854(0.2961) Steps 1112(1096.65) | Grad Norm 6.2901(10.7262) | Total Time 0.00(0.00)\n",
      "Iter 1846 | Time 110.8485(118.2541) | Bit/dim 3.6477(3.6331) | Xent 0.9018(0.8295) | Loss 14.0714(15.0745) | Error 0.3204(0.2968) Steps 1058(1095.50) | Grad Norm 22.2490(11.0719) | Total Time 0.00(0.00)\n",
      "Iter 1847 | Time 117.1850(118.2221) | Bit/dim 3.6361(3.6332) | Xent 0.8682(0.8307) | Loss 14.3745(15.0535) | Error 0.3109(0.2972) Steps 1088(1095.27) | Grad Norm 22.7544(11.4224) | Total Time 0.00(0.00)\n",
      "Iter 1848 | Time 116.9440(118.1837) | Bit/dim 3.6381(3.6334) | Xent 0.8834(0.8322) | Loss 14.1387(15.0260) | Error 0.3187(0.2979) Steps 1088(1095.05) | Grad Norm 16.9939(11.5895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 41.3575, Epoch Time 753.2060(767.1737), Bit/dim 3.6402(best: 3.6323), Xent 0.8661, Loss 4.0732, Error 0.3057(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1849 | Time 114.7844(118.0817) | Bit/dim 3.6452(3.6337) | Xent 0.8578(0.8330) | Loss 20.2710(15.1834) | Error 0.2989(0.2979) Steps 1088(1094.84) | Grad Norm 14.2812(11.6703) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 121.2575(118.1770) | Bit/dim 3.6280(3.6336) | Xent 0.8422(0.8333) | Loss 14.0325(15.1488) | Error 0.3017(0.2980) Steps 1094(1094.82) | Grad Norm 10.8367(11.6453) | Total Time 0.00(0.00)\n",
      "Iter 1851 | Time 117.0018(118.1418) | Bit/dim 3.6422(3.6338) | Xent 0.8419(0.8335) | Loss 14.2864(15.1230) | Error 0.3019(0.2981) Steps 1118(1095.51) | Grad Norm 11.7348(11.6480) | Total Time 0.00(0.00)\n",
      "Iter 1852 | Time 116.0906(118.0802) | Bit/dim 3.6344(3.6338) | Xent 0.8246(0.8333) | Loss 14.0834(15.0918) | Error 0.2975(0.2981) Steps 1094(1095.47) | Grad Norm 9.4087(11.5808) | Total Time 0.00(0.00)\n",
      "Iter 1853 | Time 115.1916(117.9936) | Bit/dim 3.6377(3.6339) | Xent 0.8476(0.8337) | Loss 14.1224(15.0627) | Error 0.3011(0.2982) Steps 1082(1095.06) | Grad Norm 11.7349(11.5854) | Total Time 0.00(0.00)\n",
      "Iter 1854 | Time 118.2329(118.0007) | Bit/dim 3.6300(3.6338) | Xent 0.8216(0.8333) | Loss 13.8800(15.0272) | Error 0.2946(0.2981) Steps 1064(1094.13) | Grad Norm 6.1733(11.4230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 40.7387, Epoch Time 760.0227(766.9591), Bit/dim 3.6433(best: 3.6323), Xent 0.8765, Loss 4.0816, Error 0.3143(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1855 | Time 120.8123(118.0851) | Bit/dim 3.6312(3.6337) | Xent 0.8608(0.8342) | Loss 19.5994(15.1644) | Error 0.3104(0.2985) Steps 1094(1094.13) | Grad Norm 11.3884(11.4220) | Total Time 0.00(0.00)\n",
      "Iter 1856 | Time 118.0928(118.0853) | Bit/dim 3.6365(3.6338) | Xent 0.8153(0.8336) | Loss 13.9906(15.1292) | Error 0.2910(0.2983) Steps 1094(1094.12) | Grad Norm 10.0328(11.3803) | Total Time 0.00(0.00)\n",
      "Iter 1857 | Time 120.2650(118.1507) | Bit/dim 3.6337(3.6338) | Xent 0.8370(0.8337) | Loss 14.1157(15.0988) | Error 0.2950(0.2982) Steps 1100(1094.30) | Grad Norm 10.9096(11.3662) | Total Time 0.00(0.00)\n",
      "Iter 1858 | Time 121.7017(118.2572) | Bit/dim 3.6328(3.6338) | Xent 0.8358(0.8338) | Loss 14.2176(15.0723) | Error 0.2990(0.2982) Steps 1118(1095.01) | Grad Norm 8.9650(11.2942) | Total Time 0.00(0.00)\n",
      "Iter 1859 | Time 121.6736(118.3597) | Bit/dim 3.6385(3.6339) | Xent 0.8271(0.8336) | Loss 14.1269(15.0440) | Error 0.2937(0.2980) Steps 1106(1095.34) | Grad Norm 10.5141(11.2708) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 119.4030(118.3910) | Bit/dim 3.6302(3.6338) | Xent 0.8069(0.8328) | Loss 14.1895(15.0183) | Error 0.2933(0.2979) Steps 1118(1096.02) | Grad Norm 9.1973(11.2086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 39.9862, Epoch Time 778.0629(767.2922), Bit/dim 3.6320(best: 3.6323), Xent 0.8386, Loss 4.0513, Error 0.2989(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1861 | Time 120.2245(118.4460) | Bit/dim 3.6440(3.6341) | Xent 0.8127(0.8322) | Loss 19.8791(15.1642) | Error 0.2953(0.2978) Steps 1112(1096.50) | Grad Norm 6.8131(11.0767) | Total Time 0.00(0.00)\n",
      "Iter 1862 | Time 118.2374(118.4398) | Bit/dim 3.6287(3.6340) | Xent 0.8337(0.8322) | Loss 14.1295(15.1331) | Error 0.3040(0.2980) Steps 1088(1096.24) | Grad Norm 14.3753(11.1757) | Total Time 0.00(0.00)\n",
      "Iter 1863 | Time 116.1921(118.3723) | Bit/dim 3.6317(3.6339) | Xent 0.8661(0.8332) | Loss 13.9647(15.0981) | Error 0.3115(0.2984) Steps 1058(1095.10) | Grad Norm 15.2249(11.2971) | Total Time 0.00(0.00)\n",
      "Iter 1864 | Time 118.5602(118.3780) | Bit/dim 3.6151(3.6333) | Xent 0.8494(0.8337) | Loss 14.2125(15.0715) | Error 0.3053(0.2986) Steps 1142(1096.50) | Grad Norm 19.2935(11.5370) | Total Time 0.00(0.00)\n",
      "Iter 1865 | Time 119.0950(118.3995) | Bit/dim 3.6424(3.6336) | Xent 0.8417(0.8340) | Loss 14.0784(15.0417) | Error 0.3013(0.2987) Steps 1064(1095.53) | Grad Norm 15.7830(11.6644) | Total Time 0.00(0.00)\n",
      "Iter 1866 | Time 118.8068(118.4117) | Bit/dim 3.6265(3.6334) | Xent 0.7942(0.8328) | Loss 14.1519(15.0150) | Error 0.2802(0.2981) Steps 1094(1095.48) | Grad Norm 4.5287(11.4503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 41.2748, Epoch Time 768.8060(767.3377), Bit/dim 3.6356(best: 3.6320), Xent 0.8559, Loss 4.0636, Error 0.3029(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1867 | Time 116.2613(118.3472) | Bit/dim 3.6312(3.6333) | Xent 0.8219(0.8324) | Loss 20.0507(15.1661) | Error 0.2849(0.2977) Steps 1112(1095.98) | Grad Norm 12.0468(11.4682) | Total Time 0.00(0.00)\n",
      "Iter 1868 | Time 116.0910(118.2795) | Bit/dim 3.6360(3.6334) | Xent 0.8231(0.8322) | Loss 14.1214(15.1347) | Error 0.2950(0.2977) Steps 1112(1096.46) | Grad Norm 7.3869(11.3458) | Total Time 0.00(0.00)\n",
      "Iter 1869 | Time 123.0268(118.4219) | Bit/dim 3.6295(3.6333) | Xent 0.8070(0.8314) | Loss 14.0890(15.1034) | Error 0.2866(0.2973) Steps 1112(1096.92) | Grad Norm 9.5008(11.2904) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 118.3102(118.4186) | Bit/dim 3.6236(3.6330) | Xent 0.8223(0.8311) | Loss 14.2303(15.0772) | Error 0.2990(0.2974) Steps 1082(1096.48) | Grad Norm 7.2255(11.1685) | Total Time 0.00(0.00)\n",
      "Iter 1871 | Time 118.9445(118.4344) | Bit/dim 3.6259(3.6328) | Xent 0.8003(0.8302) | Loss 13.8980(15.0418) | Error 0.2910(0.2972) Steps 1112(1096.94) | Grad Norm 6.8604(11.0392) | Total Time 0.00(0.00)\n",
      "Iter 1872 | Time 118.8860(118.4479) | Bit/dim 3.6356(3.6329) | Xent 0.8154(0.8298) | Loss 14.0654(15.0125) | Error 0.2994(0.2973) Steps 1094(1096.85) | Grad Norm 9.2103(10.9844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 40.2951, Epoch Time 768.3344(767.3676), Bit/dim 3.6319(best: 3.6320), Xent 0.8342, Loss 4.0490, Error 0.2954(best: 0.2959)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1873 | Time 111.0166(118.2250) | Bit/dim 3.6355(3.6329) | Xent 0.7983(0.8288) | Loss 20.1181(15.1657) | Error 0.2871(0.2970) Steps 1082(1096.41) | Grad Norm 5.8422(10.8301) | Total Time 0.00(0.00)\n",
      "Iter 1874 | Time 115.2357(118.1353) | Bit/dim 3.6293(3.6328) | Xent 0.8032(0.8280) | Loss 14.1162(15.1342) | Error 0.2906(0.2968) Steps 1124(1097.24) | Grad Norm 9.5611(10.7920) | Total Time 0.00(0.00)\n",
      "Iter 1875 | Time 110.9226(117.9189) | Bit/dim 3.6295(3.6327) | Xent 0.7964(0.8271) | Loss 14.0519(15.1017) | Error 0.2850(0.2964) Steps 1076(1096.60) | Grad Norm 7.1013(10.6813) | Total Time 0.00(0.00)\n",
      "Iter 1876 | Time 123.5563(118.0880) | Bit/dim 3.6281(3.6326) | Xent 0.7990(0.8263) | Loss 14.1926(15.0744) | Error 0.2863(0.2961) Steps 1112(1097.06) | Grad Norm 5.0908(10.5136) | Total Time 0.00(0.00)\n",
      "Iter 1877 | Time 111.9080(117.9026) | Bit/dim 3.6335(3.6326) | Xent 0.7928(0.8252) | Loss 13.9392(15.0404) | Error 0.2894(0.2959) Steps 1058(1095.89) | Grad Norm 6.9730(10.4074) | Total Time 0.00(0.00)\n",
      "Iter 1878 | Time 117.9717(117.9047) | Bit/dim 3.6286(3.6325) | Xent 0.7768(0.8238) | Loss 14.1553(15.0138) | Error 0.2781(0.2954) Steps 1100(1096.01) | Grad Norm 7.9160(10.3327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 40.5461, Epoch Time 747.0657(766.7585), Bit/dim 3.6284(best: 3.6319), Xent 0.8286, Loss 4.0427, Error 0.2952(best: 0.2954)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1879 | Time 118.4149(117.9200) | Bit/dim 3.6306(3.6325) | Xent 0.8016(0.8231) | Loss 19.9835(15.1629) | Error 0.2871(0.2951) Steps 1112(1096.49) | Grad Norm 6.6799(10.2231) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 115.6750(117.8527) | Bit/dim 3.6375(3.6326) | Xent 0.7864(0.8220) | Loss 14.1469(15.1324) | Error 0.2811(0.2947) Steps 1106(1096.78) | Grad Norm 8.7812(10.1798) | Total Time 0.00(0.00)\n",
      "Iter 1881 | Time 122.3817(117.9885) | Bit/dim 3.6354(3.6327) | Xent 0.7944(0.8212) | Loss 14.1593(15.1033) | Error 0.2815(0.2943) Steps 1118(1097.41) | Grad Norm 9.1143(10.1478) | Total Time 0.00(0.00)\n",
      "Iter 1882 | Time 121.7322(118.1008) | Bit/dim 3.6261(3.6325) | Xent 0.7769(0.8199) | Loss 13.9394(15.0683) | Error 0.2748(0.2937) Steps 1100(1097.49) | Grad Norm 5.5163(10.0089) | Total Time 0.00(0.00)\n",
      "Iter 1883 | Time 115.6627(118.0277) | Bit/dim 3.6434(3.6328) | Xent 0.7835(0.8188) | Loss 14.1761(15.0416) | Error 0.2771(0.2932) Steps 1088(1097.21) | Grad Norm 5.1185(9.8622) | Total Time 0.00(0.00)\n",
      "Iter 1884 | Time 118.9397(118.0551) | Bit/dim 3.6183(3.6324) | Xent 0.7747(0.8175) | Loss 14.1058(15.0135) | Error 0.2741(0.2926) Steps 1064(1096.21) | Grad Norm 6.6485(9.7658) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 39.6858, Epoch Time 768.6385(766.8149), Bit/dim 3.6309(best: 3.6284), Xent 0.8235, Loss 4.0426, Error 0.2908(best: 0.2952)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1885 | Time 112.1415(117.8776) | Bit/dim 3.6298(3.6323) | Xent 0.7884(0.8166) | Loss 19.4127(15.1455) | Error 0.2836(0.2924) Steps 1106(1096.50) | Grad Norm 5.0559(9.6245) | Total Time 0.00(0.00)\n",
      "Iter 1886 | Time 122.4034(118.0134) | Bit/dim 3.6331(3.6323) | Xent 0.7772(0.8154) | Loss 13.8653(15.1071) | Error 0.2798(0.2920) Steps 1076(1095.89) | Grad Norm 3.5671(9.4428) | Total Time 0.00(0.00)\n",
      "Iter 1887 | Time 122.8185(118.1576) | Bit/dim 3.6256(3.6321) | Xent 0.8002(0.8149) | Loss 14.0159(15.0743) | Error 0.2846(0.2918) Steps 1142(1097.27) | Grad Norm 9.3623(9.4403) | Total Time 0.00(0.00)\n",
      "Iter 1888 | Time 117.9701(118.1519) | Bit/dim 3.6262(3.6319) | Xent 0.7925(0.8143) | Loss 14.0640(15.0440) | Error 0.2810(0.2915) Steps 1118(1097.89) | Grad Norm 10.1007(9.4602) | Total Time 0.00(0.00)\n",
      "Iter 1889 | Time 116.6599(118.1072) | Bit/dim 3.6291(3.6319) | Xent 0.8151(0.8143) | Loss 14.0944(15.0155) | Error 0.2907(0.2914) Steps 1058(1096.70) | Grad Norm 13.7406(9.5886) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 122.1317(118.2279) | Bit/dim 3.6420(3.6322) | Xent 0.8620(0.8157) | Loss 14.3827(14.9966) | Error 0.3104(0.2920) Steps 1112(1097.16) | Grad Norm 18.3987(9.8529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 41.3621, Epoch Time 771.4193(766.9530), Bit/dim 3.6347(best: 3.6284), Xent 0.8629, Loss 4.0661, Error 0.3072(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1891 | Time 121.7359(118.3332) | Bit/dim 3.6382(3.6323) | Xent 0.8247(0.8160) | Loss 19.9924(15.1464) | Error 0.2934(0.2920) Steps 1100(1097.24) | Grad Norm 13.3785(9.9586) | Total Time 0.00(0.00)\n",
      "Iter 1892 | Time 117.8988(118.3201) | Bit/dim 3.6231(3.6321) | Xent 0.7686(0.8146) | Loss 13.9286(15.1099) | Error 0.2775(0.2916) Steps 1088(1096.96) | Grad Norm 4.1502(9.7844) | Total Time 0.00(0.00)\n",
      "Iter 1893 | Time 119.0648(118.3425) | Bit/dim 3.6331(3.6321) | Xent 0.8241(0.8149) | Loss 14.1463(15.0810) | Error 0.2980(0.2918) Steps 1076(1096.34) | Grad Norm 10.1460(9.7952) | Total Time 0.00(0.00)\n",
      "Iter 1894 | Time 123.8569(118.5079) | Bit/dim 3.6277(3.6320) | Xent 0.8290(0.8153) | Loss 14.1555(15.0532) | Error 0.2999(0.2920) Steps 1076(1095.73) | Grad Norm 9.3541(9.7820) | Total Time 0.00(0.00)\n",
      "Iter 1895 | Time 118.4914(118.5074) | Bit/dim 3.6163(3.6315) | Xent 0.7845(0.8144) | Loss 13.9786(15.0210) | Error 0.2781(0.2916) Steps 1088(1095.49) | Grad Norm 5.9523(9.6671) | Total Time 0.00(0.00)\n",
      "Iter 1896 | Time 124.5134(118.6876) | Bit/dim 3.6281(3.6314) | Xent 0.8016(0.8140) | Loss 13.9435(14.9887) | Error 0.2874(0.2915) Steps 1094(1095.45) | Grad Norm 7.4824(9.6016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 41.7753, Epoch Time 783.7811(767.4579), Bit/dim 3.6291(best: 3.6284), Xent 0.8259, Loss 4.0420, Error 0.2945(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1897 | Time 120.7082(118.7482) | Bit/dim 3.6232(3.6311) | Xent 0.7974(0.8135) | Loss 19.8873(15.1356) | Error 0.2794(0.2911) Steps 1070(1094.69) | Grad Norm 3.5166(9.4190) | Total Time 0.00(0.00)\n",
      "Iter 1898 | Time 122.7270(118.8676) | Bit/dim 3.6297(3.6311) | Xent 0.7978(0.8130) | Loss 13.9746(15.1008) | Error 0.2816(0.2908) Steps 1106(1095.03) | Grad Norm 9.9908(9.4362) | Total Time 0.00(0.00)\n",
      "Iter 1899 | Time 119.0465(118.8729) | Bit/dim 3.6214(3.6308) | Xent 0.7832(0.8121) | Loss 14.0253(15.0685) | Error 0.2886(0.2908) Steps 1082(1094.63) | Grad Norm 8.3183(9.4026) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 118.3994(118.8587) | Bit/dim 3.6173(3.6304) | Xent 0.7981(0.8117) | Loss 14.0101(15.0368) | Error 0.2841(0.2906) Steps 1070(1093.90) | Grad Norm 6.7058(9.3217) | Total Time 0.00(0.00)\n",
      "Iter 1901 | Time 120.1738(118.8982) | Bit/dim 3.6316(3.6304) | Xent 0.8054(0.8115) | Loss 14.0765(15.0080) | Error 0.2849(0.2904) Steps 1118(1094.62) | Grad Norm 12.1030(9.4052) | Total Time 0.00(0.00)\n",
      "Iter 1902 | Time 118.2417(118.8785) | Bit/dim 3.6275(3.6304) | Xent 0.7758(0.8104) | Loss 13.8385(14.9729) | Error 0.2785(0.2901) Steps 1082(1094.24) | Grad Norm 7.5160(9.3485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 39.6529, Epoch Time 775.4139(767.6966), Bit/dim 3.6320(best: 3.6284), Xent 0.8376, Loss 4.0508, Error 0.2983(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1903 | Time 122.5095(118.9874) | Bit/dim 3.6328(3.6304) | Xent 0.8015(0.8102) | Loss 19.5140(15.1091) | Error 0.2894(0.2900) Steps 1088(1094.05) | Grad Norm 9.8244(9.3628) | Total Time 0.00(0.00)\n",
      "Iter 1904 | Time 115.1136(118.8712) | Bit/dim 3.6204(3.6301) | Xent 0.7917(0.8096) | Loss 14.1089(15.0791) | Error 0.2863(0.2899) Steps 1112(1094.59) | Grad Norm 14.3228(9.5116) | Total Time 0.00(0.00)\n",
      "Iter 1905 | Time 119.9236(118.9028) | Bit/dim 3.6319(3.6302) | Xent 0.7626(0.8082) | Loss 14.1397(15.0509) | Error 0.2765(0.2895) Steps 1106(1094.93) | Grad Norm 8.2028(9.4723) | Total Time 0.00(0.00)\n",
      "Iter 1906 | Time 117.0547(118.8473) | Bit/dim 3.6266(3.6301) | Xent 0.7862(0.8075) | Loss 14.0421(15.0207) | Error 0.2795(0.2892) Steps 1070(1094.19) | Grad Norm 8.0489(9.4296) | Total Time 0.00(0.00)\n",
      "Iter 1907 | Time 110.5587(118.5987) | Bit/dim 3.6273(3.6300) | Xent 0.7937(0.8071) | Loss 13.9428(14.9883) | Error 0.2859(0.2891) Steps 1088(1094.00) | Grad Norm 11.7114(9.4981) | Total Time 0.00(0.00)\n",
      "Iter 1908 | Time 120.8884(118.6674) | Bit/dim 3.6306(3.6300) | Xent 0.8015(0.8070) | Loss 14.1513(14.9632) | Error 0.2879(0.2891) Steps 1094(1094.00) | Grad Norm 8.0372(9.4542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 40.4315, Epoch Time 762.4937(767.5405), Bit/dim 3.6294(best: 3.6284), Xent 0.8590, Loss 4.0589, Error 0.3058(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1909 | Time 121.5266(118.7531) | Bit/dim 3.6361(3.6302) | Xent 0.8018(0.8068) | Loss 20.0945(15.1171) | Error 0.2867(0.2890) Steps 1112(1094.54) | Grad Norm 10.7328(9.4926) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 119.1955(118.7664) | Bit/dim 3.6084(3.6295) | Xent 0.7789(0.8060) | Loss 13.9514(15.0822) | Error 0.2759(0.2886) Steps 1106(1094.88) | Grad Norm 6.5170(9.4033) | Total Time 0.00(0.00)\n",
      "Iter 1911 | Time 121.4260(118.8462) | Bit/dim 3.6300(3.6296) | Xent 0.7795(0.8052) | Loss 14.0692(15.0518) | Error 0.2833(0.2885) Steps 1088(1094.68) | Grad Norm 7.5768(9.3485) | Total Time 0.00(0.00)\n",
      "Iter 1912 | Time 120.3740(118.8920) | Bit/dim 3.6352(3.6297) | Xent 0.8011(0.8051) | Loss 13.9505(15.0187) | Error 0.2900(0.2885) Steps 1112(1095.20) | Grad Norm 8.7493(9.3305) | Total Time 0.00(0.00)\n",
      "Iter 1913 | Time 123.7146(119.0367) | Bit/dim 3.6186(3.6294) | Xent 0.7958(0.8048) | Loss 13.9875(14.9878) | Error 0.2841(0.2884) Steps 1106(1095.52) | Grad Norm 9.8393(9.3458) | Total Time 0.00(0.00)\n",
      "Iter 1914 | Time 121.1111(119.0989) | Bit/dim 3.6348(3.6296) | Xent 0.8532(0.8062) | Loss 14.0030(14.9583) | Error 0.3019(0.2888) Steps 1076(1094.94) | Grad Norm 16.4901(9.5601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 39.9734, Epoch Time 783.1355(768.0083), Bit/dim 3.6333(best: 3.6284), Xent 0.8942, Loss 4.0804, Error 0.3169(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1915 | Time 122.8517(119.2115) | Bit/dim 3.6179(3.6292) | Xent 0.8479(0.8075) | Loss 19.8493(15.1050) | Error 0.3045(0.2892) Steps 1118(1095.63) | Grad Norm 17.7772(9.8066) | Total Time 0.00(0.00)\n",
      "Iter 1916 | Time 118.0861(119.1778) | Bit/dim 3.6342(3.6294) | Xent 0.9045(0.8104) | Loss 14.1447(15.0762) | Error 0.3289(0.2904) Steps 1106(1095.94) | Grad Norm 18.4019(10.0645) | Total Time 0.00(0.00)\n",
      "Iter 1917 | Time 117.7641(119.1354) | Bit/dim 3.6270(3.6293) | Xent 0.8223(0.8107) | Loss 14.1101(15.0472) | Error 0.2947(0.2906) Steps 1106(1096.24) | Grad Norm 10.1160(10.0660) | Total Time 0.00(0.00)\n",
      "Iter 1918 | Time 120.3678(119.1723) | Bit/dim 3.6336(3.6294) | Xent 0.8255(0.8112) | Loss 14.1444(15.0201) | Error 0.2946(0.2907) Steps 1088(1095.99) | Grad Norm 11.9655(10.1230) | Total Time 0.00(0.00)\n",
      "Iter 1919 | Time 122.1467(119.2616) | Bit/dim 3.6200(3.6291) | Xent 0.8397(0.8120) | Loss 13.8273(14.9843) | Error 0.2981(0.2909) Steps 1142(1097.37) | Grad Norm 11.6783(10.1697) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 120.6923(119.3045) | Bit/dim 3.6328(3.6292) | Xent 0.8384(0.8128) | Loss 13.8730(14.9510) | Error 0.2973(0.2911) Steps 1118(1097.99) | Grad Norm 11.0729(10.1968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 40.2547, Epoch Time 778.6195(768.3267), Bit/dim 3.6337(best: 3.6284), Xent 0.8693, Loss 4.0684, Error 0.3102(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1921 | Time 119.4887(119.3100) | Bit/dim 3.6248(3.6291) | Xent 0.8615(0.8143) | Loss 20.1157(15.1059) | Error 0.3054(0.2915) Steps 1130(1098.95) | Grad Norm 10.8228(10.2156) | Total Time 0.00(0.00)\n",
      "Iter 1922 | Time 124.3990(119.4627) | Bit/dim 3.6292(3.6291) | Xent 0.8275(0.8147) | Loss 13.8728(15.0689) | Error 0.2959(0.2917) Steps 1136(1100.06) | Grad Norm 10.9249(10.2368) | Total Time 0.00(0.00)\n",
      "Iter 1923 | Time 121.5917(119.5265) | Bit/dim 3.6407(3.6295) | Xent 0.8061(0.8144) | Loss 13.9942(15.0367) | Error 0.2887(0.2916) Steps 1118(1100.60) | Grad Norm 13.2631(10.3276) | Total Time 0.00(0.00)\n",
      "Iter 1924 | Time 115.2727(119.3989) | Bit/dim 3.6327(3.6296) | Xent 0.7831(0.8135) | Loss 13.9767(15.0049) | Error 0.2840(0.2913) Steps 1088(1100.22) | Grad Norm 5.2966(10.1767) | Total Time 0.00(0.00)\n",
      "Iter 1925 | Time 118.5275(119.3728) | Bit/dim 3.6354(3.6297) | Xent 0.8175(0.8136) | Loss 14.2138(14.9812) | Error 0.2891(0.2913) Steps 1100(1100.22) | Grad Norm 14.2303(10.2983) | Total Time 0.00(0.00)\n",
      "Iter 1926 | Time 116.5345(119.2876) | Bit/dim 3.6370(3.6300) | Xent 0.8002(0.8132) | Loss 14.1953(14.9576) | Error 0.2844(0.2911) Steps 1094(1100.03) | Grad Norm 13.4314(10.3923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 40.7474, Epoch Time 772.5299(768.4528), Bit/dim 3.6356(best: 3.6284), Xent 0.8610, Loss 4.0661, Error 0.3064(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1927 | Time 120.3226(119.3187) | Bit/dim 3.6202(3.6297) | Xent 0.8251(0.8136) | Loss 19.7468(15.1013) | Error 0.2975(0.2913) Steps 1124(1100.75) | Grad Norm 10.9296(10.4084) | Total Time 0.00(0.00)\n",
      "Iter 1928 | Time 118.2452(119.2865) | Bit/dim 3.6319(3.6297) | Xent 0.8215(0.8138) | Loss 14.2952(15.0771) | Error 0.2893(0.2912) Steps 1112(1101.09) | Grad Norm 14.9781(10.5455) | Total Time 0.00(0.00)\n",
      "Iter 1929 | Time 125.4362(119.4710) | Bit/dim 3.6242(3.6296) | Xent 0.8341(0.8144) | Loss 14.2279(15.0516) | Error 0.2990(0.2914) Steps 1136(1102.13) | Grad Norm 16.8817(10.7356) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 114.2509(119.3144) | Bit/dim 3.6283(3.6295) | Xent 0.8418(0.8152) | Loss 14.1472(15.0245) | Error 0.2970(0.2916) Steps 1088(1101.71) | Grad Norm 14.5075(10.8488) | Total Time 0.00(0.00)\n",
      "Iter 1931 | Time 113.2997(119.1339) | Bit/dim 3.6434(3.6299) | Xent 0.7871(0.8144) | Loss 13.7575(14.9865) | Error 0.2826(0.2913) Steps 1100(1101.66) | Grad Norm 5.6690(10.6934) | Total Time 0.00(0.00)\n",
      "Iter 1932 | Time 111.2272(118.8967) | Bit/dim 3.6384(3.6302) | Xent 0.8365(0.8151) | Loss 14.3524(14.9674) | Error 0.3034(0.2917) Steps 1088(1101.25) | Grad Norm 14.5439(10.8089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 41.2741, Epoch Time 760.6243(768.2179), Bit/dim 3.6259(best: 3.6284), Xent 0.8365, Loss 4.0442, Error 0.2961(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1933 | Time 117.7127(118.8612) | Bit/dim 3.6315(3.6302) | Xent 0.7927(0.8144) | Loss 19.5645(15.1054) | Error 0.2840(0.2915) Steps 1100(1101.21) | Grad Norm 7.8313(10.7196) | Total Time 0.00(0.00)\n",
      "Iter 1934 | Time 115.1387(118.7495) | Bit/dim 3.6263(3.6301) | Xent 0.8172(0.8145) | Loss 14.2444(15.0795) | Error 0.2897(0.2914) Steps 1088(1100.82) | Grad Norm 8.9553(10.6666) | Total Time 0.00(0.00)\n",
      "Iter 1935 | Time 119.7767(118.7803) | Bit/dim 3.6321(3.6302) | Xent 0.8285(0.8149) | Loss 13.9934(15.0469) | Error 0.2966(0.2916) Steps 1112(1101.15) | Grad Norm 6.3813(10.5381) | Total Time 0.00(0.00)\n",
      "Iter 1936 | Time 114.5626(118.6538) | Bit/dim 3.6323(3.6302) | Xent 0.7913(0.8142) | Loss 13.7982(15.0095) | Error 0.2819(0.2913) Steps 1094(1100.94) | Grad Norm 6.1067(10.4051) | Total Time 0.00(0.00)\n",
      "Iter 1937 | Time 120.6690(118.7143) | Bit/dim 3.6344(3.6304) | Xent 0.7965(0.8136) | Loss 14.2696(14.9873) | Error 0.2826(0.2910) Steps 1124(1101.63) | Grad Norm 7.3451(10.3133) | Total Time 0.00(0.00)\n",
      "Iter 1938 | Time 114.2276(118.5797) | Bit/dim 3.6258(3.6302) | Xent 0.7837(0.8127) | Loss 13.9734(14.9569) | Error 0.2816(0.2907) Steps 1124(1102.30) | Grad Norm 8.5734(10.2611) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 41.3706, Epoch Time 759.7560(767.9640), Bit/dim 3.6359(best: 3.6259), Xent 0.8386, Loss 4.0552, Error 0.2974(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1939 | Time 119.0216(118.5929) | Bit/dim 3.6314(3.6303) | Xent 0.8066(0.8126) | Loss 19.7058(15.0993) | Error 0.2867(0.2906) Steps 1112(1102.59) | Grad Norm 11.5566(10.3000) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 117.7542(118.5678) | Bit/dim 3.6224(3.6300) | Xent 0.7634(0.8111) | Loss 14.0911(15.0691) | Error 0.2735(0.2901) Steps 1112(1102.87) | Grad Norm 5.7497(10.1635) | Total Time 0.00(0.00)\n",
      "Iter 1941 | Time 116.8744(118.5170) | Bit/dim 3.6306(3.6300) | Xent 0.7831(0.8102) | Loss 14.0502(15.0385) | Error 0.2806(0.2898) Steps 1124(1103.51) | Grad Norm 10.2419(10.1658) | Total Time 0.00(0.00)\n",
      "Iter 1942 | Time 115.7525(118.4340) | Bit/dim 3.6311(3.6301) | Xent 0.7893(0.8096) | Loss 14.2463(15.0148) | Error 0.2871(0.2897) Steps 1082(1102.86) | Grad Norm 6.7246(10.0626) | Total Time 0.00(0.00)\n",
      "Iter 1943 | Time 120.5443(118.4973) | Bit/dim 3.6250(3.6299) | Xent 0.7836(0.8088) | Loss 14.1533(14.9889) | Error 0.2808(0.2895) Steps 1112(1103.14) | Grad Norm 5.6835(9.9312) | Total Time 0.00(0.00)\n",
      "Iter 1944 | Time 121.6080(118.5907) | Bit/dim 3.6349(3.6301) | Xent 0.7821(0.8080) | Loss 14.1172(14.9628) | Error 0.2796(0.2892) Steps 1118(1103.58) | Grad Norm 6.0973(9.8162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 39.9042, Epoch Time 767.5245(767.9509), Bit/dim 3.6363(best: 3.6259), Xent 0.8180, Loss 4.0453, Error 0.2899(best: 0.2908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1945 | Time 122.7587(118.7157) | Bit/dim 3.6221(3.6298) | Xent 0.7774(0.8071) | Loss 19.5183(15.0994) | Error 0.2775(0.2888) Steps 1106(1103.65) | Grad Norm 6.7264(9.7235) | Total Time 0.00(0.00)\n",
      "Iter 1946 | Time 116.8732(118.6604) | Bit/dim 3.6342(3.6300) | Xent 0.7734(0.8061) | Loss 13.9436(15.0648) | Error 0.2805(0.2886) Steps 1076(1102.82) | Grad Norm 7.0192(9.6424) | Total Time 0.00(0.00)\n",
      "Iter 1947 | Time 118.3472(118.6510) | Bit/dim 3.6222(3.6297) | Xent 0.7846(0.8055) | Loss 13.8158(15.0273) | Error 0.2855(0.2885) Steps 1100(1102.74) | Grad Norm 8.0598(9.5949) | Total Time 0.00(0.00)\n",
      "Iter 1948 | Time 119.6019(118.6796) | Bit/dim 3.6281(3.6297) | Xent 0.8049(0.8054) | Loss 13.9191(14.9940) | Error 0.2901(0.2885) Steps 1130(1103.56) | Grad Norm 11.1016(9.6401) | Total Time 0.00(0.00)\n",
      "Iter 1949 | Time 124.0165(118.8397) | Bit/dim 3.6289(3.6297) | Xent 0.7786(0.8046) | Loss 13.9082(14.9615) | Error 0.2786(0.2882) Steps 1112(1103.81) | Grad Norm 11.9875(9.7105) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 122.7657(118.9574) | Bit/dim 3.6327(3.6297) | Xent 0.7958(0.8044) | Loss 14.0105(14.9329) | Error 0.2802(0.2880) Steps 1100(1103.70) | Grad Norm 11.3699(9.7603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 41.2157, Epoch Time 781.7903(768.3660), Bit/dim 3.6297(best: 3.6259), Xent 0.8545, Loss 4.0570, Error 0.3051(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1951 | Time 114.2349(118.8158) | Bit/dim 3.6348(3.6299) | Xent 0.8134(0.8046) | Loss 19.4341(15.0680) | Error 0.2891(0.2880) Steps 1100(1103.59) | Grad Norm 12.4405(9.8407) | Total Time 0.00(0.00)\n",
      "Iter 1952 | Time 121.6753(118.9016) | Bit/dim 3.6179(3.6295) | Xent 0.7891(0.8042) | Loss 13.6540(15.0256) | Error 0.2847(0.2879) Steps 1118(1104.02) | Grad Norm 8.1536(9.7901) | Total Time 0.00(0.00)\n",
      "Iter 1953 | Time 121.7224(118.9862) | Bit/dim 3.6279(3.6295) | Xent 0.7667(0.8031) | Loss 14.0986(14.9978) | Error 0.2731(0.2875) Steps 1154(1105.52) | Grad Norm 4.2189(9.6230) | Total Time 0.00(0.00)\n",
      "Iter 1954 | Time 113.3359(118.8167) | Bit/dim 3.6305(3.6295) | Xent 0.8123(0.8033) | Loss 13.9668(14.9668) | Error 0.2914(0.2876) Steps 1100(1105.35) | Grad Norm 11.3442(9.6746) | Total Time 0.00(0.00)\n",
      "Iter 1955 | Time 115.4925(118.7169) | Bit/dim 3.6253(3.6294) | Xent 0.7854(0.8028) | Loss 14.0987(14.9408) | Error 0.2790(0.2873) Steps 1130(1106.09) | Grad Norm 11.2757(9.7226) | Total Time 0.00(0.00)\n",
      "Iter 1956 | Time 120.9978(118.7854) | Bit/dim 3.6126(3.6289) | Xent 0.7876(0.8023) | Loss 14.2332(14.9196) | Error 0.2814(0.2872) Steps 1130(1106.81) | Grad Norm 4.5988(9.5689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 40.1523, Epoch Time 763.6539(768.2247), Bit/dim 3.6288(best: 3.6259), Xent 0.8241, Loss 4.0408, Error 0.2904(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1957 | Time 114.8676(118.6678) | Bit/dim 3.6219(3.6287) | Xent 0.7727(0.8014) | Loss 19.5283(15.0578) | Error 0.2745(0.2868) Steps 1094(1106.42) | Grad Norm 7.6871(9.5125) | Total Time 0.00(0.00)\n",
      "Iter 1958 | Time 121.2467(118.7452) | Bit/dim 3.6238(3.6285) | Xent 0.7729(0.8006) | Loss 13.9473(15.0245) | Error 0.2779(0.2865) Steps 1112(1106.59) | Grad Norm 10.4147(9.5395) | Total Time 0.00(0.00)\n",
      "Iter 1959 | Time 123.7907(118.8966) | Bit/dim 3.6272(3.6285) | Xent 0.7838(0.8001) | Loss 13.9972(14.9937) | Error 0.2762(0.2862) Steps 1106(1106.57) | Grad Norm 8.1231(9.4970) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 118.5882(118.8873) | Bit/dim 3.6265(3.6284) | Xent 0.7692(0.7992) | Loss 14.0570(14.9656) | Error 0.2719(0.2858) Steps 1082(1105.84) | Grad Norm 10.6933(9.5329) | Total Time 0.00(0.00)\n",
      "Iter 1961 | Time 119.8131(118.9151) | Bit/dim 3.6283(3.6284) | Xent 0.7922(0.7990) | Loss 14.1754(14.9419) | Error 0.2880(0.2858) Steps 1088(1105.30) | Grad Norm 10.7989(9.5709) | Total Time 0.00(0.00)\n",
      "Iter 1962 | Time 115.4138(118.8101) | Bit/dim 3.6320(3.6285) | Xent 0.7629(0.7979) | Loss 13.9825(14.9131) | Error 0.2751(0.2855) Steps 1100(1105.14) | Grad Norm 9.2875(9.5624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 39.8488, Epoch Time 769.9405(768.2762), Bit/dim 3.6257(best: 3.6259), Xent 0.8168, Loss 4.0341, Error 0.2901(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1963 | Time 118.6006(118.8038) | Bit/dim 3.6190(3.6283) | Xent 0.7715(0.7971) | Loss 19.4255(15.0485) | Error 0.2750(0.2852) Steps 1124(1105.71) | Grad Norm 8.1941(9.5214) | Total Time 0.00(0.00)\n",
      "Iter 1964 | Time 117.9508(118.7782) | Bit/dim 3.6200(3.6280) | Xent 0.8007(0.7972) | Loss 13.8096(15.0113) | Error 0.2900(0.2854) Steps 1100(1105.54) | Grad Norm 12.9564(9.6244) | Total Time 0.00(0.00)\n",
      "Iter 1965 | Time 114.3630(118.6457) | Bit/dim 3.6224(3.6278) | Xent 0.7734(0.7965) | Loss 13.9163(14.9784) | Error 0.2820(0.2853) Steps 1088(1105.01) | Grad Norm 11.2236(9.6724) | Total Time 0.00(0.00)\n",
      "Iter 1966 | Time 115.5236(118.5521) | Bit/dim 3.6229(3.6277) | Xent 0.7550(0.7952) | Loss 14.0973(14.9520) | Error 0.2704(0.2848) Steps 1088(1104.50) | Grad Norm 6.6457(9.5816) | Total Time 0.00(0.00)\n",
      "Iter 1967 | Time 122.4220(118.6682) | Bit/dim 3.6351(3.6279) | Xent 0.7846(0.7949) | Loss 14.2152(14.9299) | Error 0.2811(0.2847) Steps 1100(1104.37) | Grad Norm 8.9873(9.5638) | Total Time 0.00(0.00)\n",
      "Iter 1968 | Time 123.3314(118.8081) | Bit/dim 3.6395(3.6283) | Xent 0.7932(0.7949) | Loss 14.1240(14.9057) | Error 0.2827(0.2846) Steps 1100(1104.23) | Grad Norm 14.8502(9.7223) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 40.0271, Epoch Time 769.0008(768.2979), Bit/dim 3.6282(best: 3.6257), Xent 0.8249, Loss 4.0406, Error 0.2973(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1969 | Time 119.4370(118.8269) | Bit/dim 3.6253(3.6282) | Xent 0.7751(0.7943) | Loss 19.8334(15.0536) | Error 0.2834(0.2846) Steps 1088(1103.75) | Grad Norm 8.4936(9.6855) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 124.5948(119.0000) | Bit/dim 3.6299(3.6282) | Xent 0.7634(0.7933) | Loss 13.9086(15.0192) | Error 0.2719(0.2842) Steps 1106(1103.82) | Grad Norm 5.6988(9.5659) | Total Time 0.00(0.00)\n",
      "Iter 1971 | Time 114.8072(118.8742) | Bit/dim 3.6235(3.6281) | Xent 0.7567(0.7922) | Loss 14.1425(14.9929) | Error 0.2719(0.2838) Steps 1118(1104.24) | Grad Norm 8.2434(9.5262) | Total Time 0.00(0.00)\n",
      "Iter 1972 | Time 120.0860(118.9105) | Bit/dim 3.6232(3.6279) | Xent 0.7683(0.7915) | Loss 14.2423(14.9704) | Error 0.2770(0.2836) Steps 1100(1104.11) | Grad Norm 6.6194(9.4390) | Total Time 0.00(0.00)\n",
      "Iter 1973 | Time 121.9254(119.0010) | Bit/dim 3.6244(3.6278) | Xent 0.7816(0.7912) | Loss 14.0584(14.9430) | Error 0.2806(0.2836) Steps 1124(1104.71) | Grad Norm 9.2662(9.4338) | Total Time 0.00(0.00)\n",
      "Iter 1974 | Time 120.7721(119.0541) | Bit/dim 3.6315(3.6279) | Xent 0.8002(0.7915) | Loss 14.0566(14.9164) | Error 0.2863(0.2836) Steps 1130(1105.47) | Grad Norm 13.2651(9.5488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 40.6523, Epoch Time 778.6138(768.6074), Bit/dim 3.6272(best: 3.6257), Xent 0.8360, Loss 4.0452, Error 0.2945(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1975 | Time 115.2997(118.9415) | Bit/dim 3.6181(3.6276) | Xent 0.7836(0.7913) | Loss 19.7737(15.0622) | Error 0.2834(0.2836) Steps 1088(1104.94) | Grad Norm 12.6799(9.6427) | Total Time 0.00(0.00)\n",
      "Iter 1976 | Time 117.8626(118.9091) | Bit/dim 3.6238(3.6275) | Xent 0.7767(0.7908) | Loss 14.0194(15.0309) | Error 0.2799(0.2835) Steps 1118(1105.34) | Grad Norm 9.5694(9.6405) | Total Time 0.00(0.00)\n",
      "Iter 1977 | Time 123.8613(119.0577) | Bit/dim 3.6226(3.6274) | Xent 0.7829(0.7906) | Loss 13.9504(14.9985) | Error 0.2823(0.2835) Steps 1136(1106.26) | Grad Norm 4.7886(9.4949) | Total Time 0.00(0.00)\n",
      "Iter 1978 | Time 125.7407(119.2582) | Bit/dim 3.6191(3.6271) | Xent 0.7807(0.7903) | Loss 14.2111(14.9748) | Error 0.2769(0.2833) Steps 1088(1105.71) | Grad Norm 10.8990(9.5371) | Total Time 0.00(0.00)\n",
      "Iter 1979 | Time 118.1438(119.2247) | Bit/dim 3.6418(3.6276) | Xent 0.8002(0.7906) | Loss 14.0455(14.9470) | Error 0.2939(0.2836) Steps 1106(1105.72) | Grad Norm 15.2827(9.7094) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 119.5873(119.2356) | Bit/dim 3.6335(3.6277) | Xent 0.7891(0.7905) | Loss 14.0609(14.9204) | Error 0.2821(0.2836) Steps 1100(1105.55) | Grad Norm 11.2993(9.7571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 40.5880, Epoch Time 777.5296(768.8750), Bit/dim 3.6306(best: 3.6257), Xent 0.8104, Loss 4.0358, Error 0.2902(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1981 | Time 122.5688(119.3356) | Bit/dim 3.6300(3.6278) | Xent 0.7653(0.7898) | Loss 20.1078(15.0760) | Error 0.2755(0.2833) Steps 1106(1105.56) | Grad Norm 6.5418(9.6607) | Total Time 0.00(0.00)\n",
      "Iter 1982 | Time 121.8155(119.4100) | Bit/dim 3.6172(3.6275) | Xent 0.7705(0.7892) | Loss 13.8857(15.0403) | Error 0.2759(0.2831) Steps 1088(1105.03) | Grad Norm 11.1611(9.7057) | Total Time 0.00(0.00)\n",
      "Iter 1983 | Time 122.9530(119.5163) | Bit/dim 3.6369(3.6278) | Xent 0.8096(0.7898) | Loss 14.2081(15.0153) | Error 0.2915(0.2833) Steps 1082(1104.34) | Grad Norm 10.5898(9.7322) | Total Time 0.00(0.00)\n",
      "Iter 1984 | Time 124.2278(119.6577) | Bit/dim 3.6107(3.6273) | Xent 0.7548(0.7888) | Loss 13.9455(14.9832) | Error 0.2686(0.2829) Steps 1106(1104.39) | Grad Norm 9.1714(9.7154) | Total Time 0.00(0.00)\n",
      "Iter 1985 | Time 120.9553(119.6966) | Bit/dim 3.6278(3.6273) | Xent 0.8007(0.7891) | Loss 14.0870(14.9563) | Error 0.2788(0.2828) Steps 1112(1104.62) | Grad Norm 15.0027(9.8740) | Total Time 0.00(0.00)\n",
      "Iter 1986 | Time 123.6763(119.8160) | Bit/dim 3.6250(3.6272) | Xent 0.7846(0.7890) | Loss 13.7540(14.9203) | Error 0.2817(0.2827) Steps 1082(1103.94) | Grad Norm 13.0189(9.9683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 41.4822, Epoch Time 793.9607(769.6276), Bit/dim 3.6261(best: 3.6257), Xent 0.8274, Loss 4.0398, Error 0.2961(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 112.5316(119.5974) | Bit/dim 3.6195(3.6270) | Xent 0.7741(0.7885) | Loss 19.7480(15.0651) | Error 0.2786(0.2826) Steps 1106(1104.00) | Grad Norm 9.4696(9.9534) | Total Time 0.00(0.00)\n",
      "Iter 1988 | Time 118.1706(119.5546) | Bit/dim 3.6298(3.6271) | Xent 0.7808(0.7883) | Loss 14.0870(15.0358) | Error 0.2850(0.2827) Steps 1100(1103.88) | Grad Norm 10.4825(9.9693) | Total Time 0.00(0.00)\n",
      "Iter 1989 | Time 129.6816(119.8584) | Bit/dim 3.6284(3.6271) | Xent 0.7743(0.7879) | Loss 13.7775(14.9980) | Error 0.2758(0.2825) Steps 1142(1105.03) | Grad Norm 9.2741(9.9484) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 113.9021(119.6798) | Bit/dim 3.6261(3.6271) | Xent 0.7636(0.7872) | Loss 13.9623(14.9669) | Error 0.2761(0.2823) Steps 1100(1104.88) | Grad Norm 4.0782(9.7723) | Total Time 0.00(0.00)\n",
      "Iter 1991 | Time 116.8000(119.5934) | Bit/dim 3.6195(3.6268) | Xent 0.7771(0.7869) | Loss 13.9282(14.9358) | Error 0.2771(0.2821) Steps 1094(1104.55) | Grad Norm 7.6625(9.7090) | Total Time 0.00(0.00)\n",
      "Iter 1992 | Time 124.5214(119.7412) | Bit/dim 3.6214(3.6267) | Xent 0.7837(0.7868) | Loss 14.0092(14.9080) | Error 0.2760(0.2820) Steps 1124(1105.13) | Grad Norm 9.5363(9.7038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 40.4368, Epoch Time 772.3401(769.7090), Bit/dim 3.6289(best: 3.6257), Xent 0.8121, Loss 4.0350, Error 0.2903(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 117.5796(119.6764) | Bit/dim 3.6248(3.6266) | Xent 0.7431(0.7855) | Loss 19.9042(15.0579) | Error 0.2658(0.2815) Steps 1106(1105.16) | Grad Norm 5.7057(9.5839) | Total Time 0.00(0.00)\n",
      "Iter 1994 | Time 120.3849(119.6976) | Bit/dim 3.6107(3.6261) | Xent 0.7833(0.7854) | Loss 14.0440(15.0275) | Error 0.2794(0.2814) Steps 1100(1105.00) | Grad Norm 9.6269(9.5852) | Total Time 0.00(0.00)\n",
      "Iter 1995 | Time 118.3574(119.6574) | Bit/dim 3.6268(3.6262) | Xent 0.7899(0.7855) | Loss 13.8587(14.9924) | Error 0.2819(0.2814) Steps 1088(1104.49) | Grad Norm 11.3896(9.6393) | Total Time 0.00(0.00)\n",
      "Iter 1996 | Time 117.6402(119.5969) | Bit/dim 3.6182(3.6259) | Xent 0.7541(0.7846) | Loss 13.9227(14.9603) | Error 0.2689(0.2810) Steps 1106(1104.54) | Grad Norm 7.6580(9.5799) | Total Time 0.00(0.00)\n",
      "Iter 1997 | Time 117.8658(119.5450) | Bit/dim 3.6406(3.6264) | Xent 0.7691(0.7841) | Loss 14.2620(14.9394) | Error 0.2750(0.2809) Steps 1136(1105.48) | Grad Norm 5.9620(9.4713) | Total Time 0.00(0.00)\n",
      "Iter 1998 | Time 115.9278(119.4364) | Bit/dim 3.6290(3.6264) | Xent 0.7818(0.7840) | Loss 14.2207(14.9178) | Error 0.2806(0.2809) Steps 1106(1105.50) | Grad Norm 8.5165(9.4427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 39.2652, Epoch Time 763.3252(769.5175), Bit/dim 3.6311(best: 3.6257), Xent 0.8249, Loss 4.0435, Error 0.2907(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 120.6778(119.4737) | Bit/dim 3.6202(3.6263) | Xent 0.7576(0.7833) | Loss 19.4308(15.0532) | Error 0.2674(0.2804) Steps 1142(1106.59) | Grad Norm 8.8046(9.4235) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 118.8875(119.4561) | Bit/dim 3.6262(3.6263) | Xent 0.7766(0.7831) | Loss 14.0443(15.0229) | Error 0.2785(0.2804) Steps 1094(1106.22) | Grad Norm 8.0464(9.3822) | Total Time 0.00(0.00)\n",
      "Iter 2001 | Time 114.1553(119.2971) | Bit/dim 3.6254(3.6262) | Xent 0.7907(0.7833) | Loss 13.9326(14.9902) | Error 0.2825(0.2805) Steps 1082(1105.49) | Grad Norm 10.8888(9.4274) | Total Time 0.00(0.00)\n",
      "Iter 2002 | Time 125.9464(119.4966) | Bit/dim 3.6276(3.6263) | Xent 0.8004(0.7838) | Loss 14.1154(14.9640) | Error 0.2873(0.2807) Steps 1088(1104.96) | Grad Norm 16.7574(9.6473) | Total Time 0.00(0.00)\n",
      "Iter 2003 | Time 122.6533(119.5913) | Bit/dim 3.6388(3.6266) | Xent 0.8184(0.7848) | Loss 14.1928(14.9408) | Error 0.2961(0.2811) Steps 1076(1104.10) | Grad Norm 15.7889(9.8316) | Total Time 0.00(0.00)\n",
      "Iter 2004 | Time 116.7410(119.5057) | Bit/dim 3.6141(3.6263) | Xent 0.7681(0.7843) | Loss 14.0230(14.9133) | Error 0.2756(0.2810) Steps 1112(1104.33) | Grad Norm 6.3111(9.7260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 40.4899, Epoch Time 775.5255(769.6977), Bit/dim 3.6231(best: 3.6257), Xent 0.8163, Loss 4.0313, Error 0.2877(best: 0.2899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 116.5907(119.4183) | Bit/dim 3.6200(3.6261) | Xent 0.7922(0.7846) | Loss 20.0203(15.0665) | Error 0.2798(0.2809) Steps 1094(1104.02) | Grad Norm 8.3095(9.6835) | Total Time 0.00(0.00)\n",
      "Iter 2006 | Time 117.5253(119.3615) | Bit/dim 3.6216(3.6259) | Xent 0.7974(0.7850) | Loss 13.9775(15.0338) | Error 0.2812(0.2809) Steps 1142(1105.16) | Grad Norm 11.5019(9.7380) | Total Time 0.00(0.00)\n",
      "Iter 2007 | Time 124.2943(119.5095) | Bit/dim 3.6304(3.6261) | Xent 0.7758(0.7847) | Loss 13.7878(14.9965) | Error 0.2775(0.2808) Steps 1088(1104.65) | Grad Norm 9.3678(9.7269) | Total Time 0.00(0.00)\n",
      "Iter 2008 | Time 117.9493(119.4627) | Bit/dim 3.6287(3.6262) | Xent 0.7760(0.7844) | Loss 13.9252(14.9643) | Error 0.2781(0.2807) Steps 1112(1104.87) | Grad Norm 8.9785(9.7045) | Total Time 0.00(0.00)\n",
      "Iter 2009 | Time 114.7261(119.3206) | Bit/dim 3.6224(3.6260) | Xent 0.7998(0.7849) | Loss 14.0765(14.9377) | Error 0.2866(0.2809) Steps 1070(1103.82) | Grad Norm 15.4929(9.8781) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 115.7398(119.2132) | Bit/dim 3.6274(3.6261) | Xent 0.8097(0.7856) | Loss 14.0913(14.9123) | Error 0.2920(0.2813) Steps 1106(1103.89) | Grad Norm 15.5427(10.0480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 40.5563, Epoch Time 763.3126(769.5062), Bit/dim 3.6326(best: 3.6231), Xent 0.8256, Loss 4.0454, Error 0.2897(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 121.8608(119.2926) | Bit/dim 3.6285(3.6262) | Xent 0.7652(0.7850) | Loss 19.8357(15.0600) | Error 0.2726(0.2810) Steps 1130(1104.67) | Grad Norm 9.1518(10.0212) | Total Time 0.00(0.00)\n",
      "Iter 2012 | Time 117.6512(119.2433) | Bit/dim 3.6245(3.6261) | Xent 0.7718(0.7846) | Loss 13.9267(15.0260) | Error 0.2761(0.2808) Steps 1100(1104.53) | Grad Norm 10.9828(10.0500) | Total Time 0.00(0.00)\n",
      "Iter 2013 | Time 117.0571(119.1778) | Bit/dim 3.6240(3.6260) | Xent 0.8067(0.7853) | Loss 13.8701(14.9913) | Error 0.2855(0.2810) Steps 1118(1104.93) | Grad Norm 11.5594(10.0953) | Total Time 0.00(0.00)\n",
      "Iter 2014 | Time 121.7267(119.2542) | Bit/dim 3.6250(3.6260) | Xent 0.8021(0.7858) | Loss 14.4030(14.9737) | Error 0.2935(0.2814) Steps 1130(1105.69) | Grad Norm 8.9897(10.0621) | Total Time 0.00(0.00)\n",
      "Iter 2015 | Time 119.0682(119.2486) | Bit/dim 3.6254(3.6260) | Xent 0.8284(0.7871) | Loss 14.2302(14.9514) | Error 0.2993(0.2819) Steps 1106(1105.70) | Grad Norm 14.9021(10.2073) | Total Time 0.00(0.00)\n",
      "Iter 2016 | Time 121.2321(119.3082) | Bit/dim 3.6294(3.6261) | Xent 0.8174(0.7880) | Loss 14.3109(14.9321) | Error 0.2927(0.2822) Steps 1112(1105.88) | Grad Norm 22.4731(10.5753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 40.4190, Epoch Time 774.7756(769.6642), Bit/dim 3.6348(best: 3.6231), Xent 0.8617, Loss 4.0656, Error 0.3044(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 121.3664(119.3699) | Bit/dim 3.6226(3.6260) | Xent 0.8238(0.7890) | Loss 20.3466(15.0946) | Error 0.2943(0.2826) Steps 1100(1105.71) | Grad Norm 17.8877(10.7947) | Total Time 0.00(0.00)\n",
      "Iter 2018 | Time 118.9762(119.3581) | Bit/dim 3.6255(3.6260) | Xent 0.8924(0.7921) | Loss 14.2779(15.0701) | Error 0.3243(0.2838) Steps 1112(1105.90) | Grad Norm 17.2869(10.9894) | Total Time 0.00(0.00)\n",
      "Iter 2019 | Time 122.7004(119.4584) | Bit/dim 3.6256(3.6260) | Xent 0.8106(0.7927) | Loss 13.9840(15.0375) | Error 0.2836(0.2838) Steps 1118(1106.26) | Grad Norm 16.5154(11.1552) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 123.0437(119.5659) | Bit/dim 3.6422(3.6265) | Xent 0.9566(0.7976) | Loss 14.4951(15.0212) | Error 0.3373(0.2854) Steps 1076(1105.35) | Grad Norm 26.5973(11.6185) | Total Time 0.00(0.00)\n",
      "Iter 2021 | Time 125.0713(119.7311) | Bit/dim 3.6332(3.6267) | Xent 0.7722(0.7969) | Loss 13.8969(14.9875) | Error 0.2765(0.2852) Steps 1124(1105.91) | Grad Norm 6.6510(11.4695) | Total Time 0.00(0.00)\n",
      "Iter 2022 | Time 117.3484(119.6596) | Bit/dim 3.6338(3.6269) | Xent 0.8619(0.7988) | Loss 14.2463(14.9653) | Error 0.3117(0.2860) Steps 1070(1104.83) | Grad Norm 16.7940(11.6292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 40.7912, Epoch Time 784.8202(770.1189), Bit/dim 3.6338(best: 3.6231), Xent 0.8631, Loss 4.0654, Error 0.3117(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 115.7003(119.5408) | Bit/dim 3.6335(3.6271) | Xent 0.8181(0.7994) | Loss 19.8352(15.1114) | Error 0.2929(0.2862) Steps 1076(1103.97) | Grad Norm 12.1176(11.6438) | Total Time 0.00(0.00)\n",
      "Iter 2024 | Time 117.6859(119.4852) | Bit/dim 3.6421(3.6275) | Xent 0.8637(0.8013) | Loss 14.5095(15.0933) | Error 0.3074(0.2868) Steps 1094(1103.67) | Grad Norm 18.4188(11.8471) | Total Time 0.00(0.00)\n",
      "Iter 2025 | Time 116.9708(119.4097) | Bit/dim 3.6318(3.6277) | Xent 0.7733(0.8005) | Loss 14.0468(15.0619) | Error 0.2808(0.2866) Steps 1118(1104.10) | Grad Norm 10.8657(11.8176) | Total Time 0.00(0.00)\n",
      "Iter 2026 | Time 118.6002(119.3855) | Bit/dim 3.6303(3.6277) | Xent 0.8452(0.8018) | Loss 14.1978(15.0360) | Error 0.3044(0.2872) Steps 1082(1103.44) | Grad Norm 12.7379(11.8453) | Total Time 0.00(0.00)\n",
      "Iter 2027 | Time 118.6275(119.3627) | Bit/dim 3.6221(3.6276) | Xent 0.8000(0.8018) | Loss 13.9177(15.0024) | Error 0.2874(0.2872) Steps 1094(1103.15) | Grad Norm 9.9548(11.7885) | Total Time 0.00(0.00)\n",
      "Iter 2028 | Time 117.9279(119.3197) | Bit/dim 3.6359(3.6278) | Xent 0.8051(0.8019) | Loss 14.3967(14.9843) | Error 0.2881(0.2872) Steps 1100(1103.06) | Grad Norm 10.6912(11.7556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 40.7557, Epoch Time 762.1313(769.8793), Bit/dim 3.6310(best: 3.6231), Xent 0.8274, Loss 4.0447, Error 0.2949(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 123.5549(119.4467) | Bit/dim 3.6301(3.6279) | Xent 0.7936(0.8016) | Loss 19.6914(15.1255) | Error 0.2864(0.2872) Steps 1136(1104.05) | Grad Norm 5.8253(11.5777) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 120.3093(119.4726) | Bit/dim 3.6327(3.6280) | Xent 0.7751(0.8008) | Loss 14.1586(15.0965) | Error 0.2806(0.2870) Steps 1094(1103.75) | Grad Norm 10.9115(11.5577) | Total Time 0.00(0.00)\n",
      "Iter 2031 | Time 123.7529(119.6010) | Bit/dim 3.6278(3.6280) | Xent 0.7942(0.8006) | Loss 14.2100(15.0699) | Error 0.2831(0.2869) Steps 1130(1104.53) | Grad Norm 6.7058(11.4122) | Total Time 0.00(0.00)\n",
      "Iter 2032 | Time 118.5615(119.5698) | Bit/dim 3.6372(3.6283) | Xent 0.7692(0.7997) | Loss 13.9692(15.0369) | Error 0.2765(0.2865) Steps 1136(1105.48) | Grad Norm 7.9191(11.3074) | Total Time 0.00(0.00)\n",
      "Iter 2033 | Time 115.5403(119.4489) | Bit/dim 3.6268(3.6282) | Xent 0.7680(0.7987) | Loss 13.9927(15.0055) | Error 0.2770(0.2863) Steps 1070(1104.41) | Grad Norm 6.3265(11.1579) | Total Time 0.00(0.00)\n",
      "Iter 2034 | Time 122.4173(119.5380) | Bit/dim 3.6316(3.6283) | Xent 0.7620(0.7976) | Loss 13.9763(14.9747) | Error 0.2715(0.2858) Steps 1136(1105.36) | Grad Norm 6.6618(11.0231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 39.8982, Epoch Time 780.0414(770.1842), Bit/dim 3.6273(best: 3.6231), Xent 0.8255, Loss 4.0400, Error 0.2928(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 123.4734(119.6561) | Bit/dim 3.6432(3.6288) | Xent 0.7818(0.7972) | Loss 19.7384(15.1176) | Error 0.2802(0.2856) Steps 1112(1105.56) | Grad Norm 7.4512(10.9159) | Total Time 0.00(0.00)\n",
      "Iter 2036 | Time 119.8327(119.6614) | Bit/dim 3.6235(3.6286) | Xent 0.7758(0.7965) | Loss 13.8748(15.0803) | Error 0.2731(0.2853) Steps 1106(1105.57) | Grad Norm 8.3350(10.8385) | Total Time 0.00(0.00)\n",
      "Iter 2037 | Time 116.7405(119.5737) | Bit/dim 3.6284(3.6286) | Xent 0.7717(0.7958) | Loss 13.9137(15.0453) | Error 0.2774(0.2850) Steps 1094(1105.23) | Grad Norm 11.5489(10.8598) | Total Time 0.00(0.00)\n",
      "Iter 2038 | Time 117.7605(119.5193) | Bit/dim 3.6168(3.6283) | Xent 0.7721(0.7951) | Loss 13.9413(15.0122) | Error 0.2760(0.2848) Steps 1094(1104.89) | Grad Norm 8.7875(10.7976) | Total Time 0.00(0.00)\n",
      "Iter 2039 | Time 111.8646(119.2897) | Bit/dim 3.6215(3.6281) | Xent 0.7721(0.7944) | Loss 14.0976(14.9847) | Error 0.2730(0.2844) Steps 1076(1104.02) | Grad Norm 8.9959(10.7436) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 117.6081(119.2392) | Bit/dim 3.6201(3.6278) | Xent 0.7972(0.7945) | Loss 14.0288(14.9561) | Error 0.2843(0.2844) Steps 1118(1104.44) | Grad Norm 13.4254(10.8240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 40.7639, Epoch Time 763.8244(769.9934), Bit/dim 3.6303(best: 3.6231), Xent 0.8323, Loss 4.0464, Error 0.2952(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 122.5086(119.3373) | Bit/dim 3.6361(3.6281) | Xent 0.7852(0.7942) | Loss 20.3831(15.1189) | Error 0.2798(0.2843) Steps 1124(1105.03) | Grad Norm 12.8581(10.8851) | Total Time 0.00(0.00)\n",
      "Iter 2042 | Time 118.8581(119.3230) | Bit/dim 3.6055(3.6274) | Xent 0.7891(0.7940) | Loss 14.0600(15.0871) | Error 0.2772(0.2841) Steps 1124(1105.60) | Grad Norm 6.1971(10.7444) | Total Time 0.00(0.00)\n",
      "Iter 2043 | Time 123.0344(119.4343) | Bit/dim 3.6332(3.6276) | Xent 0.7794(0.7936) | Loss 14.0284(15.0553) | Error 0.2845(0.2841) Steps 1106(1105.61) | Grad Norm 7.3587(10.6428) | Total Time 0.00(0.00)\n",
      "Iter 2044 | Time 117.5865(119.3789) | Bit/dim 3.6224(3.6274) | Xent 0.8055(0.7939) | Loss 13.8996(15.0207) | Error 0.2874(0.2842) Steps 1076(1104.72) | Grad Norm 10.6314(10.6425) | Total Time 0.00(0.00)\n",
      "Iter 2045 | Time 124.3328(119.5275) | Bit/dim 3.6264(3.6274) | Xent 0.7738(0.7933) | Loss 14.3938(15.0019) | Error 0.2800(0.2840) Steps 1130(1105.48) | Grad Norm 10.2193(10.6298) | Total Time 0.00(0.00)\n",
      "Iter 2046 | Time 123.9336(119.6597) | Bit/dim 3.6318(3.6275) | Xent 0.7878(0.7932) | Loss 14.0065(14.9720) | Error 0.2837(0.2840) Steps 1130(1106.22) | Grad Norm 10.2280(10.6177) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 40.6617, Epoch Time 786.5776(770.4909), Bit/dim 3.6298(best: 3.6231), Xent 0.8215, Loss 4.0406, Error 0.2891(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 120.3395(119.6801) | Bit/dim 3.6182(3.6272) | Xent 0.7651(0.7923) | Loss 19.7897(15.1165) | Error 0.2710(0.2836) Steps 1118(1106.57) | Grad Norm 7.7418(10.5315) | Total Time 0.00(0.00)\n",
      "Iter 2048 | Time 120.4253(119.7024) | Bit/dim 3.6283(3.6273) | Xent 0.7906(0.7923) | Loss 13.8279(15.0779) | Error 0.2840(0.2837) Steps 1100(1106.37) | Grad Norm 12.0063(10.5757) | Total Time 0.00(0.00)\n",
      "Iter 2049 | Time 118.7766(119.6746) | Bit/dim 3.6336(3.6275) | Xent 0.7669(0.7915) | Loss 13.8932(15.0423) | Error 0.2710(0.2833) Steps 1082(1105.64) | Grad Norm 9.2689(10.5365) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 117.4595(119.6082) | Bit/dim 3.6172(3.6272) | Xent 0.7611(0.7906) | Loss 14.0339(15.0121) | Error 0.2731(0.2830) Steps 1082(1104.93) | Grad Norm 9.2970(10.4993) | Total Time 0.00(0.00)\n",
      "Iter 2051 | Time 122.3921(119.6917) | Bit/dim 3.6320(3.6273) | Xent 0.7633(0.7898) | Loss 13.8606(14.9775) | Error 0.2722(0.2826) Steps 1100(1104.78) | Grad Norm 8.5344(10.4404) | Total Time 0.00(0.00)\n",
      "Iter 2052 | Time 117.5709(119.6281) | Bit/dim 3.6334(3.6275) | Xent 0.7782(0.7894) | Loss 13.7430(14.9405) | Error 0.2784(0.2825) Steps 1100(1104.64) | Grad Norm 6.0393(10.3083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 40.3262, Epoch Time 773.0221(770.5668), Bit/dim 3.6252(best: 3.6231), Xent 0.8232, Loss 4.0369, Error 0.2952(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 120.8392(119.6644) | Bit/dim 3.6217(3.6273) | Xent 0.7593(0.7885) | Loss 19.8682(15.0883) | Error 0.2741(0.2823) Steps 1082(1103.96) | Grad Norm 7.1889(10.2148) | Total Time 0.00(0.00)\n",
      "Iter 2054 | Time 124.9665(119.8235) | Bit/dim 3.6147(3.6269) | Xent 0.7682(0.7879) | Loss 13.9842(15.0552) | Error 0.2728(0.2820) Steps 1148(1105.28) | Grad Norm 11.8642(10.2642) | Total Time 0.00(0.00)\n",
      "Iter 2055 | Time 123.6494(119.9383) | Bit/dim 3.6338(3.6271) | Xent 0.7923(0.7881) | Loss 13.9251(15.0213) | Error 0.2849(0.2821) Steps 1124(1105.84) | Grad Norm 12.9686(10.3454) | Total Time 0.00(0.00)\n",
      "Iter 2056 | Time 113.0405(119.7313) | Bit/dim 3.6247(3.6271) | Xent 0.7633(0.7873) | Loss 14.1465(14.9951) | Error 0.2755(0.2819) Steps 1094(1105.49) | Grad Norm 11.9194(10.3926) | Total Time 0.00(0.00)\n",
      "Iter 2057 | Time 114.8485(119.5848) | Bit/dim 3.6297(3.6271) | Xent 0.7905(0.7874) | Loss 14.2663(14.9732) | Error 0.2824(0.2819) Steps 1136(1106.40) | Grad Norm 12.7547(10.4635) | Total Time 0.00(0.00)\n",
      "Iter 2058 | Time 118.6527(119.5569) | Bit/dim 3.6239(3.6271) | Xent 0.7760(0.7871) | Loss 13.7342(14.9360) | Error 0.2810(0.2819) Steps 1082(1105.67) | Grad Norm 12.7008(10.5306) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 40.4675, Epoch Time 772.2812(770.6183), Bit/dim 3.6237(best: 3.6231), Xent 0.8174, Loss 4.0324, Error 0.2889(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 115.6829(119.4407) | Bit/dim 3.6215(3.6269) | Xent 0.7599(0.7863) | Loss 19.7990(15.0819) | Error 0.2756(0.2817) Steps 1100(1105.50) | Grad Norm 7.6975(10.4456) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 116.5032(119.3525) | Bit/dim 3.6309(3.6270) | Xent 0.7737(0.7859) | Loss 13.7703(15.0426) | Error 0.2754(0.2815) Steps 1100(1105.34) | Grad Norm 12.4544(10.5058) | Total Time 0.00(0.00)\n",
      "Iter 2061 | Time 120.4272(119.3848) | Bit/dim 3.6249(3.6269) | Xent 0.7840(0.7858) | Loss 13.8154(15.0057) | Error 0.2790(0.2814) Steps 1076(1104.46) | Grad Norm 12.0667(10.5527) | Total Time 0.00(0.00)\n",
      "Iter 2062 | Time 114.4834(119.2377) | Bit/dim 3.6187(3.6267) | Xent 0.7725(0.7854) | Loss 13.6950(14.9664) | Error 0.2709(0.2811) Steps 1076(1103.60) | Grad Norm 9.5987(10.5240) | Total Time 0.00(0.00)\n",
      "Iter 2063 | Time 121.8868(119.3172) | Bit/dim 3.6307(3.6268) | Xent 0.7756(0.7851) | Loss 14.1563(14.9421) | Error 0.2790(0.2810) Steps 1118(1104.03) | Grad Norm 11.0961(10.5412) | Total Time 0.00(0.00)\n",
      "Iter 2064 | Time 116.0002(119.2177) | Bit/dim 3.6240(3.6267) | Xent 0.7826(0.7851) | Loss 14.0347(14.9149) | Error 0.2827(0.2811) Steps 1112(1104.27) | Grad Norm 12.6342(10.6040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 41.0830, Epoch Time 762.0247(770.3605), Bit/dim 3.6249(best: 3.6231), Xent 0.8189, Loss 4.0344, Error 0.2877(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 121.1745(119.2764) | Bit/dim 3.6263(3.6267) | Xent 0.7740(0.7847) | Loss 19.6882(15.0581) | Error 0.2776(0.2810) Steps 1142(1105.41) | Grad Norm 12.4388(10.6590) | Total Time 0.00(0.00)\n",
      "Iter 2066 | Time 121.1065(119.3313) | Bit/dim 3.6337(3.6269) | Xent 0.7511(0.7837) | Loss 14.2202(15.0330) | Error 0.2702(0.2807) Steps 1130(1106.14) | Grad Norm 4.6395(10.4785) | Total Time 0.00(0.00)\n",
      "Iter 2067 | Time 125.8467(119.5268) | Bit/dim 3.6187(3.6267) | Xent 0.7569(0.7829) | Loss 13.8592(14.9977) | Error 0.2651(0.2802) Steps 1136(1107.04) | Grad Norm 9.1881(10.4397) | Total Time 0.00(0.00)\n",
      "Iter 2068 | Time 121.6762(119.5912) | Bit/dim 3.6216(3.6265) | Xent 0.7638(0.7823) | Loss 14.0617(14.9697) | Error 0.2710(0.2799) Steps 1106(1107.01) | Grad Norm 12.7014(10.5076) | Total Time 0.00(0.00)\n",
      "Iter 2069 | Time 121.4671(119.6475) | Bit/dim 3.6167(3.6262) | Xent 0.7762(0.7821) | Loss 13.7563(14.9333) | Error 0.2744(0.2798) Steps 1142(1108.06) | Grad Norm 10.5153(10.5078) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 121.3797(119.6995) | Bit/dim 3.6195(3.6260) | Xent 0.7474(0.7811) | Loss 14.0725(14.9074) | Error 0.2674(0.2794) Steps 1130(1108.72) | Grad Norm 3.1276(10.2864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 40.2804, Epoch Time 795.9010(771.1267), Bit/dim 3.6298(best: 3.6231), Xent 0.8274, Loss 4.0435, Error 0.2902(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 115.9682(119.5875) | Bit/dim 3.6210(3.6259) | Xent 0.7789(0.7810) | Loss 19.8217(15.0549) | Error 0.2745(0.2792) Steps 1112(1108.81) | Grad Norm 12.7394(10.3600) | Total Time 0.00(0.00)\n",
      "Iter 2072 | Time 118.1241(119.5436) | Bit/dim 3.6350(3.6262) | Xent 0.7636(0.7805) | Loss 13.9211(15.0209) | Error 0.2691(0.2789) Steps 1106(1108.73) | Grad Norm 10.8360(10.3743) | Total Time 0.00(0.00)\n",
      "Iter 2073 | Time 115.3645(119.4183) | Bit/dim 3.6239(3.6261) | Xent 0.7414(0.7793) | Loss 13.7470(14.9826) | Error 0.2646(0.2785) Steps 1088(1108.11) | Grad Norm 3.6542(10.1727) | Total Time 0.00(0.00)\n",
      "Iter 2074 | Time 115.3220(119.2954) | Bit/dim 3.6235(3.6260) | Xent 0.7494(0.7784) | Loss 13.9797(14.9526) | Error 0.2675(0.2782) Steps 1112(1108.22) | Grad Norm 11.9397(10.2257) | Total Time 0.00(0.00)\n",
      "Iter 2075 | Time 124.7637(119.4594) | Bit/dim 3.6459(3.6266) | Xent 0.7715(0.7782) | Loss 14.0820(14.9264) | Error 0.2741(0.2780) Steps 1130(1108.88) | Grad Norm 13.2799(10.3173) | Total Time 0.00(0.00)\n",
      "Iter 2076 | Time 124.4732(119.6098) | Bit/dim 3.6116(3.6262) | Xent 0.7692(0.7780) | Loss 13.9772(14.8980) | Error 0.2758(0.2780) Steps 1124(1109.33) | Grad Norm 11.8447(10.3631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 41.0745, Epoch Time 770.8021(771.1169), Bit/dim 3.6284(best: 3.6231), Xent 0.8370, Loss 4.0469, Error 0.2970(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 117.1561(119.5362) | Bit/dim 3.6305(3.6263) | Xent 0.7771(0.7779) | Loss 20.0091(15.0513) | Error 0.2758(0.2779) Steps 1106(1109.23) | Grad Norm 13.5429(10.4585) | Total Time 0.00(0.00)\n",
      "Iter 2078 | Time 123.3567(119.6508) | Bit/dim 3.6221(3.6262) | Xent 0.7581(0.7773) | Loss 14.1131(15.0232) | Error 0.2724(0.2777) Steps 1106(1109.13) | Grad Norm 13.3753(10.5460) | Total Time 0.00(0.00)\n",
      "Iter 2079 | Time 118.4719(119.6155) | Bit/dim 3.6195(3.6260) | Xent 0.7558(0.7767) | Loss 13.9844(14.9920) | Error 0.2728(0.2776) Steps 1100(1108.86) | Grad Norm 8.8483(10.4951) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 120.5580(119.6438) | Bit/dim 3.6256(3.6259) | Xent 0.7403(0.7756) | Loss 13.9078(14.9595) | Error 0.2666(0.2773) Steps 1112(1108.95) | Grad Norm 4.1482(10.3047) | Total Time 0.00(0.00)\n",
      "Iter 2081 | Time 116.7152(119.5559) | Bit/dim 3.6128(3.6256) | Xent 0.7644(0.7753) | Loss 13.8365(14.9258) | Error 0.2738(0.2772) Steps 1136(1109.77) | Grad Norm 10.4736(10.3098) | Total Time 0.00(0.00)\n",
      "Iter 2082 | Time 124.5237(119.7049) | Bit/dim 3.6214(3.6254) | Xent 0.7711(0.7751) | Loss 13.8022(14.8921) | Error 0.2741(0.2771) Steps 1136(1110.55) | Grad Norm 11.1701(10.3356) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 41.4741, Epoch Time 785.2500(771.5409), Bit/dim 3.6243(best: 3.6231), Xent 0.8314, Loss 4.0400, Error 0.2936(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 115.8341(119.5888) | Bit/dim 3.6282(3.6255) | Xent 0.7648(0.7748) | Loss 19.9598(15.0441) | Error 0.2710(0.2769) Steps 1112(1110.60) | Grad Norm 10.6294(10.3444) | Total Time 0.00(0.00)\n",
      "Iter 2084 | Time 117.0695(119.5132) | Bit/dim 3.6181(3.6253) | Xent 0.7814(0.7750) | Loss 14.1390(15.0169) | Error 0.2825(0.2771) Steps 1088(1109.92) | Grad Norm 12.6593(10.4138) | Total Time 0.00(0.00)\n",
      "Iter 2085 | Time 121.2628(119.5657) | Bit/dim 3.6170(3.6250) | Xent 0.7526(0.7744) | Loss 14.1857(14.9920) | Error 0.2645(0.2767) Steps 1124(1110.34) | Grad Norm 8.8563(10.3671) | Total Time 0.00(0.00)\n",
      "Iter 2086 | Time 120.3153(119.5882) | Bit/dim 3.6295(3.6252) | Xent 0.7419(0.7734) | Loss 14.1951(14.9681) | Error 0.2646(0.2763) Steps 1124(1110.75) | Grad Norm 5.7450(10.2284) | Total Time 0.00(0.00)\n",
      "Iter 2087 | Time 121.8867(119.6571) | Bit/dim 3.6185(3.6250) | Xent 0.7652(0.7731) | Loss 13.8310(14.9340) | Error 0.2710(0.2762) Steps 1100(1110.43) | Grad Norm 12.2277(10.2884) | Total Time 0.00(0.00)\n",
      "Iter 2088 | Time 119.3370(119.6475) | Bit/dim 3.6150(3.6247) | Xent 0.7767(0.7732) | Loss 13.9066(14.9032) | Error 0.2758(0.2761) Steps 1112(1110.48) | Grad Norm 11.7636(10.3327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 41.2115, Epoch Time 772.5347(771.5707), Bit/dim 3.6243(best: 3.6231), Xent 0.8091, Loss 4.0289, Error 0.2896(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 116.2237(119.5448) | Bit/dim 3.6259(3.6247) | Xent 0.7503(0.7726) | Loss 19.4539(15.0397) | Error 0.2660(0.2758) Steps 1082(1109.62) | Grad Norm 6.0726(10.2049) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 115.6020(119.4265) | Bit/dim 3.6210(3.6246) | Xent 0.7737(0.7726) | Loss 14.0555(15.0102) | Error 0.2796(0.2760) Steps 1094(1109.15) | Grad Norm 11.5998(10.2467) | Total Time 0.00(0.00)\n",
      "Iter 2091 | Time 119.5610(119.4306) | Bit/dim 3.6163(3.6243) | Xent 0.7458(0.7718) | Loss 14.0377(14.9810) | Error 0.2679(0.2757) Steps 1136(1109.96) | Grad Norm 9.2426(10.2166) | Total Time 0.00(0.00)\n",
      "Iter 2092 | Time 122.1268(119.5115) | Bit/dim 3.6320(3.6246) | Xent 0.7339(0.7707) | Loss 14.0099(14.9519) | Error 0.2636(0.2754) Steps 1124(1110.38) | Grad Norm 5.9298(10.0880) | Total Time 0.00(0.00)\n",
      "Iter 2093 | Time 113.7561(119.3388) | Bit/dim 3.6147(3.6243) | Xent 0.7761(0.7708) | Loss 14.2081(14.9295) | Error 0.2750(0.2753) Steps 1094(1109.89) | Grad Norm 13.9484(10.2038) | Total Time 0.00(0.00)\n",
      "Iter 2094 | Time 116.7746(119.2619) | Bit/dim 3.6392(3.6247) | Xent 0.7829(0.7712) | Loss 14.0234(14.9024) | Error 0.2805(0.2755) Steps 1100(1109.59) | Grad Norm 15.8585(10.3735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 40.6800, Epoch Time 760.4714(771.2378), Bit/dim 3.6224(best: 3.6231), Xent 0.8597, Loss 4.0523, Error 0.3015(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 121.4775(119.3283) | Bit/dim 3.6177(3.6245) | Xent 0.8044(0.7722) | Loss 19.5927(15.0431) | Error 0.2869(0.2758) Steps 1136(1110.38) | Grad Norm 15.8074(10.5365) | Total Time 0.00(0.00)\n",
      "Iter 2096 | Time 122.7950(119.4323) | Bit/dim 3.6318(3.6247) | Xent 0.8257(0.7738) | Loss 14.2313(15.0187) | Error 0.2923(0.2763) Steps 1148(1111.51) | Grad Norm 17.1396(10.7346) | Total Time 0.00(0.00)\n",
      "Iter 2097 | Time 120.9137(119.4768) | Bit/dim 3.6166(3.6245) | Xent 0.7948(0.7744) | Loss 13.7764(14.9815) | Error 0.2859(0.2766) Steps 1136(1112.25) | Grad Norm 11.4852(10.7571) | Total Time 0.00(0.00)\n",
      "Iter 2098 | Time 116.5789(119.3898) | Bit/dim 3.6213(3.6244) | Xent 0.7817(0.7746) | Loss 14.1884(14.9577) | Error 0.2812(0.2768) Steps 1094(1111.70) | Grad Norm 12.4220(10.8070) | Total Time 0.00(0.00)\n",
      "Iter 2099 | Time 114.7723(119.2513) | Bit/dim 3.6292(3.6245) | Xent 0.7530(0.7740) | Loss 13.9330(14.9269) | Error 0.2662(0.2764) Steps 1094(1111.17) | Grad Norm 8.8883(10.7495) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 123.7947(119.3876) | Bit/dim 3.6185(3.6244) | Xent 0.7683(0.7738) | Loss 14.0275(14.8999) | Error 0.2776(0.2765) Steps 1112(1111.19) | Grad Norm 12.0421(10.7883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 39.9979, Epoch Time 776.2661(771.3886), Bit/dim 3.6232(best: 3.6224), Xent 0.8409, Loss 4.0437, Error 0.3011(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 119.9865(119.4056) | Bit/dim 3.6133(3.6240) | Xent 0.7841(0.7741) | Loss 20.1974(15.0589) | Error 0.2764(0.2765) Steps 1100(1110.86) | Grad Norm 11.2403(10.8018) | Total Time 0.00(0.00)\n",
      "Iter 2102 | Time 123.0932(119.5162) | Bit/dim 3.6221(3.6240) | Xent 0.7476(0.7733) | Loss 14.1716(15.0322) | Error 0.2692(0.2763) Steps 1088(1110.17) | Grad Norm 8.7007(10.7388) | Total Time 0.00(0.00)\n",
      "Iter 2103 | Time 117.1646(119.4457) | Bit/dim 3.6312(3.6242) | Xent 0.7464(0.7725) | Loss 14.1452(15.0056) | Error 0.2675(0.2760) Steps 1106(1110.05) | Grad Norm 8.0432(10.6579) | Total Time 0.00(0.00)\n",
      "Iter 2104 | Time 117.1173(119.3758) | Bit/dim 3.6245(3.6242) | Xent 0.7802(0.7727) | Loss 13.9150(14.9729) | Error 0.2841(0.2762) Steps 1082(1109.21) | Grad Norm 10.3618(10.6490) | Total Time 0.00(0.00)\n",
      "Iter 2105 | Time 117.4197(119.3171) | Bit/dim 3.6299(3.6244) | Xent 0.7524(0.7721) | Loss 14.0453(14.9451) | Error 0.2662(0.2759) Steps 1070(1108.03) | Grad Norm 6.5023(10.5246) | Total Time 0.00(0.00)\n",
      "Iter 2106 | Time 124.1735(119.4628) | Bit/dim 3.6251(3.6244) | Xent 0.7477(0.7714) | Loss 13.9202(14.9143) | Error 0.2692(0.2757) Steps 1118(1108.33) | Grad Norm 6.0034(10.3890) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 40.8596, Epoch Time 775.6522(771.5165), Bit/dim 3.6257(best: 3.6224), Xent 0.8080, Loss 4.0297, Error 0.2868(best: 0.2877)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 122.3667(119.5499) | Bit/dim 3.6167(3.6242) | Xent 0.7492(0.7707) | Loss 19.6554(15.0566) | Error 0.2711(0.2756) Steps 1124(1108.80) | Grad Norm 5.1179(10.2309) | Total Time 0.00(0.00)\n",
      "Iter 2108 | Time 121.8681(119.6195) | Bit/dim 3.6242(3.6242) | Xent 0.7397(0.7698) | Loss 14.3934(15.0367) | Error 0.2668(0.2753) Steps 1136(1109.61) | Grad Norm 4.8677(10.0700) | Total Time 0.00(0.00)\n",
      "Iter 2109 | Time 125.5731(119.7981) | Bit/dim 3.6199(3.6240) | Xent 0.7193(0.7683) | Loss 13.9149(15.0030) | Error 0.2596(0.2749) Steps 1142(1110.59) | Grad Norm 6.7115(9.9692) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 123.4364(119.9072) | Bit/dim 3.6216(3.6240) | Xent 0.7408(0.7675) | Loss 13.9510(14.9715) | Error 0.2672(0.2746) Steps 1082(1109.73) | Grad Norm 6.2849(9.8587) | Total Time 0.00(0.00)\n",
      "Iter 2111 | Time 113.9912(119.7298) | Bit/dim 3.6182(3.6238) | Xent 0.7436(0.7668) | Loss 13.6269(14.9311) | Error 0.2705(0.2745) Steps 1088(1109.08) | Grad Norm 5.3589(9.7237) | Total Time 0.00(0.00)\n",
      "Iter 2112 | Time 121.7996(119.7919) | Bit/dim 3.6187(3.6236) | Xent 0.7485(0.7662) | Loss 14.0798(14.9056) | Error 0.2712(0.2744) Steps 1106(1108.98) | Grad Norm 6.8664(9.6380) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 40.2917, Epoch Time 791.2069(772.1072), Bit/dim 3.6254(best: 3.6224), Xent 0.8218, Loss 4.0363, Error 0.2897(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 123.8485(119.9136) | Bit/dim 3.6276(3.6238) | Xent 0.7578(0.7660) | Loss 19.9287(15.0563) | Error 0.2710(0.2743) Steps 1154(1110.33) | Grad Norm 13.2313(9.7458) | Total Time 0.00(0.00)\n",
      "Iter 2114 | Time 119.5315(119.9021) | Bit/dim 3.6206(3.6237) | Xent 0.7570(0.7657) | Loss 13.8991(15.0216) | Error 0.2739(0.2743) Steps 1100(1110.02) | Grad Norm 14.5261(9.8892) | Total Time 0.00(0.00)\n",
      "Iter 2115 | Time 115.3447(119.7654) | Bit/dim 3.6262(3.6237) | Xent 0.7505(0.7652) | Loss 13.7901(14.9846) | Error 0.2684(0.2741) Steps 1100(1109.72) | Grad Norm 9.5265(9.8783) | Total Time 0.00(0.00)\n",
      "Iter 2116 | Time 118.7405(119.7346) | Bit/dim 3.6185(3.6236) | Xent 0.7439(0.7646) | Loss 14.0184(14.9556) | Error 0.2679(0.2739) Steps 1082(1108.89) | Grad Norm 9.5370(9.8681) | Total Time 0.00(0.00)\n",
      "Iter 2117 | Time 115.4199(119.6052) | Bit/dim 3.6215(3.6235) | Xent 0.7752(0.7649) | Loss 14.0540(14.9286) | Error 0.2709(0.2738) Steps 1094(1108.45) | Grad Norm 12.9214(9.9597) | Total Time 0.00(0.00)\n",
      "Iter 2118 | Time 119.1938(119.5928) | Bit/dim 3.6319(3.6238) | Xent 0.8042(0.7661) | Loss 13.9685(14.8998) | Error 0.2839(0.2741) Steps 1118(1108.73) | Grad Norm 16.4455(10.1542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 41.4171, Epoch Time 769.2849(772.0226), Bit/dim 3.6220(best: 3.6224), Xent 0.8146, Loss 4.0293, Error 0.2884(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 118.9846(119.5746) | Bit/dim 3.6190(3.6236) | Xent 0.7712(0.7662) | Loss 19.8596(15.0486) | Error 0.2831(0.2744) Steps 1100(1108.47) | Grad Norm 11.8602(10.2054) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 123.3070(119.6866) | Bit/dim 3.6083(3.6232) | Xent 0.7404(0.7655) | Loss 13.9574(15.0158) | Error 0.2668(0.2742) Steps 1112(1108.58) | Grad Norm 4.7282(10.0411) | Total Time 0.00(0.00)\n",
      "Iter 2121 | Time 117.3680(119.6170) | Bit/dim 3.6041(3.6226) | Xent 0.7751(0.7658) | Loss 13.8337(14.9804) | Error 0.2808(0.2744) Steps 1130(1109.22) | Grad Norm 13.6434(10.1492) | Total Time 0.00(0.00)\n",
      "Iter 2122 | Time 123.5080(119.7337) | Bit/dim 3.6266(3.6227) | Xent 0.7818(0.7662) | Loss 13.8843(14.9475) | Error 0.2779(0.2745) Steps 1136(1110.02) | Grad Norm 14.5656(10.2817) | Total Time 0.00(0.00)\n",
      "Iter 2123 | Time 117.4577(119.6655) | Bit/dim 3.6309(3.6230) | Xent 0.7268(0.7651) | Loss 14.1144(14.9225) | Error 0.2565(0.2739) Steps 1082(1109.18) | Grad Norm 6.6031(10.1713) | Total Time 0.00(0.00)\n",
      "Iter 2124 | Time 123.5369(119.7816) | Bit/dim 3.6319(3.6232) | Xent 0.7767(0.7654) | Loss 14.0616(14.8967) | Error 0.2729(0.2739) Steps 1124(1109.63) | Grad Norm 19.7239(10.4579) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 40.6134, Epoch Time 780.5663(772.2789), Bit/dim 3.6329(best: 3.6220), Xent 0.8958, Loss 4.0808, Error 0.3194(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 121.3920(119.8299) | Bit/dim 3.6399(3.6237) | Xent 0.8109(0.7668) | Loss 20.2662(15.0578) | Error 0.2877(0.2743) Steps 1094(1109.16) | Grad Norm 21.7515(10.7967) | Total Time 0.00(0.00)\n",
      "Iter 2126 | Time 118.6147(119.7935) | Bit/dim 3.6286(3.6239) | Xent 0.8762(0.7701) | Loss 14.1377(15.0302) | Error 0.3066(0.2753) Steps 1094(1108.70) | Grad Norm 20.5663(11.0898) | Total Time 0.00(0.00)\n",
      "Iter 2127 | Time 116.1926(119.6854) | Bit/dim 3.6246(3.6239) | Xent 0.8887(0.7736) | Loss 14.2464(15.0066) | Error 0.3163(0.2765) Steps 1088(1108.08) | Grad Norm 25.5850(11.5246) | Total Time 0.00(0.00)\n",
      "Iter 2128 | Time 121.4650(119.7388) | Bit/dim 3.6377(3.6243) | Xent 0.8214(0.7750) | Loss 14.2083(14.9827) | Error 0.2925(0.2770) Steps 1100(1107.84) | Grad Norm 11.0483(11.5103) | Total Time 0.00(0.00)\n",
      "Iter 2129 | Time 120.2685(119.7547) | Bit/dim 3.6189(3.6241) | Xent 0.9223(0.7795) | Loss 14.4400(14.9664) | Error 0.3276(0.2785) Steps 1100(1107.60) | Grad Norm 24.2674(11.8931) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 125.4515(119.9256) | Bit/dim 3.6220(3.6241) | Xent 0.7743(0.7793) | Loss 13.9528(14.9360) | Error 0.2841(0.2787) Steps 1118(1107.92) | Grad Norm 8.5775(11.7936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 41.0506, Epoch Time 780.1513(772.5150), Bit/dim 3.6280(best: 3.6220), Xent 0.9035, Loss 4.0797, Error 0.3133(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 119.6984(119.9188) | Bit/dim 3.6351(3.6244) | Xent 0.8491(0.7814) | Loss 19.9822(15.0874) | Error 0.3001(0.2793) Steps 1112(1108.04) | Grad Norm 16.5064(11.9350) | Total Time 0.00(0.00)\n",
      "Iter 2132 | Time 122.8690(120.0073) | Bit/dim 3.6278(3.6245) | Xent 0.7938(0.7818) | Loss 14.0517(15.0563) | Error 0.2927(0.2797) Steps 1154(1109.42) | Grad Norm 16.0033(12.0570) | Total Time 0.00(0.00)\n",
      "Iter 2133 | Time 120.1911(120.0128) | Bit/dim 3.6269(3.6246) | Xent 0.7995(0.7823) | Loss 14.3281(15.0345) | Error 0.2831(0.2798) Steps 1100(1109.13) | Grad Norm 10.6823(12.0158) | Total Time 0.00(0.00)\n",
      "Iter 2134 | Time 113.5580(119.8192) | Bit/dim 3.6258(3.6246) | Xent 0.7872(0.7825) | Loss 14.1260(15.0072) | Error 0.2782(0.2798) Steps 1136(1109.94) | Grad Norm 13.7810(12.0687) | Total Time 0.00(0.00)\n",
      "Iter 2135 | Time 119.4812(119.8090) | Bit/dim 3.6274(3.6247) | Xent 0.7753(0.7822) | Loss 13.8844(14.9735) | Error 0.2794(0.2798) Steps 1136(1110.72) | Grad Norm 10.5603(12.0235) | Total Time 0.00(0.00)\n",
      "Iter 2136 | Time 120.4123(119.8271) | Bit/dim 3.6372(3.6251) | Xent 0.7862(0.7824) | Loss 13.8664(14.9403) | Error 0.2774(0.2797) Steps 1100(1110.40) | Grad Norm 10.0333(11.9638) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 41.0764, Epoch Time 772.9686(772.5286), Bit/dim 3.6358(best: 3.6220), Xent 0.8178, Loss 4.0447, Error 0.2919(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 119.8115(119.8267) | Bit/dim 3.6313(3.6253) | Xent 0.7620(0.7817) | Loss 19.9020(15.0892) | Error 0.2744(0.2795) Steps 1100(1110.09) | Grad Norm 7.0984(11.8178) | Total Time 0.00(0.00)\n",
      "Iter 2138 | Time 120.8567(119.8576) | Bit/dim 3.6297(3.6254) | Xent 0.7650(0.7812) | Loss 14.1562(15.0612) | Error 0.2794(0.2795) Steps 1088(1109.43) | Grad Norm 10.7427(11.7856) | Total Time 0.00(0.00)\n",
      "Iter 2139 | Time 118.5531(119.8184) | Bit/dim 3.6222(3.6253) | Xent 0.7511(0.7803) | Loss 13.8957(15.0262) | Error 0.2704(0.2793) Steps 1112(1109.50) | Grad Norm 9.6849(11.7225) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 113.5301(119.6298) | Bit/dim 3.6316(3.6255) | Xent 0.7835(0.7804) | Loss 14.1066(14.9986) | Error 0.2796(0.2793) Steps 1100(1109.22) | Grad Norm 11.3951(11.7127) | Total Time 0.00(0.00)\n",
      "Iter 2141 | Time 118.6418(119.6001) | Bit/dim 3.6342(3.6257) | Xent 0.7714(0.7802) | Loss 14.1699(14.9738) | Error 0.2781(0.2792) Steps 1106(1109.12) | Grad Norm 13.0547(11.7530) | Total Time 0.00(0.00)\n",
      "Iter 2142 | Time 121.1168(119.6456) | Bit/dim 3.6159(3.6255) | Xent 0.7722(0.7799) | Loss 13.9122(14.9419) | Error 0.2739(0.2791) Steps 1112(1109.21) | Grad Norm 7.2328(11.6174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 40.4565, Epoch Time 768.6762(772.4131), Bit/dim 3.6263(best: 3.6220), Xent 0.8194, Loss 4.0360, Error 0.2858(best: 0.2868)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 123.1363(119.7504) | Bit/dim 3.6255(3.6255) | Xent 0.7612(0.7794) | Loss 19.9832(15.0932) | Error 0.2756(0.2790) Steps 1112(1109.29) | Grad Norm 9.1750(11.5441) | Total Time 0.00(0.00)\n",
      "Iter 2144 | Time 124.2616(119.8857) | Bit/dim 3.6210(3.6253) | Xent 0.7516(0.7785) | Loss 13.7294(15.0522) | Error 0.2732(0.2788) Steps 1106(1109.19) | Grad Norm 10.2517(11.5053) | Total Time 0.00(0.00)\n",
      "Iter 2145 | Time 123.9651(120.0081) | Bit/dim 3.6170(3.6251) | Xent 0.7785(0.7785) | Loss 14.1321(15.0246) | Error 0.2798(0.2788) Steps 1082(1108.38) | Grad Norm 15.7350(11.6322) | Total Time 0.00(0.00)\n",
      "Iter 2146 | Time 123.7970(120.1217) | Bit/dim 3.6213(3.6250) | Xent 0.7985(0.7791) | Loss 14.2567(15.0016) | Error 0.2780(0.2788) Steps 1106(1108.31) | Grad Norm 15.8283(11.7581) | Total Time 0.00(0.00)\n",
      "Iter 2147 | Time 122.0221(120.1788) | Bit/dim 3.6216(3.6249) | Xent 0.7799(0.7792) | Loss 14.0349(14.9726) | Error 0.2825(0.2789) Steps 1124(1108.78) | Grad Norm 13.9147(11.8228) | Total Time 0.00(0.00)\n",
      "Iter 2148 | Time 117.5900(120.1011) | Bit/dim 3.6326(3.6251) | Xent 0.7429(0.7781) | Loss 14.1153(14.9469) | Error 0.2682(0.2786) Steps 1118(1109.05) | Grad Norm 10.5541(11.7847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 41.1849, Epoch Time 791.6661(772.9907), Bit/dim 3.6240(best: 3.6220), Xent 0.8092, Loss 4.0286, Error 0.2894(best: 0.2858)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 119.5045(120.0832) | Bit/dim 3.6265(3.6251) | Xent 0.7433(0.7770) | Loss 19.7607(15.0913) | Error 0.2684(0.2783) Steps 1124(1109.50) | Grad Norm 6.2713(11.6193) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 119.3439(120.0610) | Bit/dim 3.6276(3.6252) | Xent 0.7475(0.7761) | Loss 14.0258(15.0593) | Error 0.2706(0.2781) Steps 1100(1109.22) | Grad Norm 10.7211(11.5924) | Total Time 0.00(0.00)\n",
      "Iter 2151 | Time 120.8320(120.0841) | Bit/dim 3.6257(3.6252) | Xent 0.7518(0.7754) | Loss 13.7253(15.0193) | Error 0.2691(0.2778) Steps 1136(1110.02) | Grad Norm 5.5423(11.4109) | Total Time 0.00(0.00)\n",
      "Iter 2152 | Time 120.2801(120.0900) | Bit/dim 3.6193(3.6250) | Xent 0.7234(0.7738) | Loss 13.7688(14.9818) | Error 0.2612(0.2773) Steps 1100(1109.72) | Grad Norm 7.7387(11.3007) | Total Time 0.00(0.00)\n",
      "Iter 2153 | Time 119.8635(120.0832) | Bit/dim 3.6261(3.6251) | Xent 0.7413(0.7729) | Loss 14.1093(14.9556) | Error 0.2675(0.2770) Steps 1130(1110.33) | Grad Norm 5.0702(11.1138) | Total Time 0.00(0.00)\n",
      "Iter 2154 | Time 119.5328(120.0667) | Bit/dim 3.6193(3.6249) | Xent 0.7522(0.7723) | Loss 13.6953(14.9178) | Error 0.2711(0.2768) Steps 1088(1109.66) | Grad Norm 6.9135(10.9878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 40.5634, Epoch Time 775.8809(773.0774), Bit/dim 3.6209(best: 3.6220), Xent 0.7935, Loss 4.0177, Error 0.2783(best: 0.2858)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 121.7279(120.1166) | Bit/dim 3.6156(3.6246) | Xent 0.7305(0.7710) | Loss 20.0391(15.0714) | Error 0.2652(0.2765) Steps 1112(1109.73) | Grad Norm 4.8320(10.8031) | Total Time 0.00(0.00)\n",
      "Iter 2156 | Time 123.3709(120.2142) | Bit/dim 3.6223(3.6245) | Xent 0.7216(0.7695) | Loss 13.8511(15.0348) | Error 0.2582(0.2759) Steps 1118(1109.98) | Grad Norm 5.3636(10.6399) | Total Time 0.00(0.00)\n",
      "Iter 2157 | Time 123.9868(120.3274) | Bit/dim 3.6252(3.6246) | Xent 0.7496(0.7689) | Loss 13.9189(15.0014) | Error 0.2770(0.2760) Steps 1148(1111.12) | Grad Norm 7.3332(10.5407) | Total Time 0.00(0.00)\n",
      "Iter 2158 | Time 119.0578(120.2893) | Bit/dim 3.6253(3.6246) | Xent 0.7601(0.7687) | Loss 13.8537(14.9669) | Error 0.2662(0.2757) Steps 1130(1111.68) | Grad Norm 11.8641(10.5804) | Total Time 0.00(0.00)\n",
      "Iter 2159 | Time 113.4676(120.0846) | Bit/dim 3.6129(3.6242) | Xent 0.7689(0.7687) | Loss 14.1239(14.9416) | Error 0.2755(0.2757) Steps 1124(1112.05) | Grad Norm 12.6787(10.6434) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 129.9170(120.3796) | Bit/dim 3.6208(3.6241) | Xent 0.7685(0.7687) | Loss 13.9294(14.9113) | Error 0.2754(0.2757) Steps 1142(1112.95) | Grad Norm 14.2826(10.7526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 41.0726, Epoch Time 793.7955(773.6989), Bit/dim 3.6255(best: 3.6209), Xent 0.8067, Loss 4.0289, Error 0.2880(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 125.9345(120.5462) | Bit/dim 3.6147(3.6239) | Xent 0.7320(0.7676) | Loss 19.5934(15.0517) | Error 0.2572(0.2751) Steps 1136(1113.64) | Grad Norm 11.7315(10.7819) | Total Time 0.00(0.00)\n",
      "Iter 2162 | Time 121.9838(120.5894) | Bit/dim 3.6236(3.6238) | Xent 0.7284(0.7664) | Loss 13.9776(15.0195) | Error 0.2611(0.2747) Steps 1118(1113.77) | Grad Norm 6.4052(10.6506) | Total Time 0.00(0.00)\n",
      "Iter 2163 | Time 122.9419(120.6599) | Bit/dim 3.6098(3.6234) | Xent 0.7403(0.7656) | Loss 14.0862(14.9915) | Error 0.2596(0.2742) Steps 1106(1113.54) | Grad Norm 8.1079(10.5743) | Total Time 0.00(0.00)\n",
      "Iter 2164 | Time 118.3060(120.5893) | Bit/dim 3.6288(3.6236) | Xent 0.7233(0.7643) | Loss 14.1242(14.9655) | Error 0.2638(0.2739) Steps 1112(1113.49) | Grad Norm 5.6164(10.4256) | Total Time 0.00(0.00)\n",
      "Iter 2165 | Time 118.5390(120.5278) | Bit/dim 3.6243(3.6236) | Xent 0.7310(0.7633) | Loss 13.9364(14.9346) | Error 0.2619(0.2736) Steps 1094(1112.91) | Grad Norm 7.2285(10.3297) | Total Time 0.00(0.00)\n",
      "Iter 2166 | Time 122.1143(120.5754) | Bit/dim 3.6214(3.6235) | Xent 0.7291(0.7623) | Loss 14.0408(14.9078) | Error 0.2604(0.2732) Steps 1130(1113.42) | Grad Norm 5.4874(10.1844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 40.1385, Epoch Time 796.5301(774.3838), Bit/dim 3.6184(best: 3.6209), Xent 0.7921, Loss 4.0145, Error 0.2812(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 123.8527(120.6737) | Bit/dim 3.6147(3.6233) | Xent 0.7245(0.7612) | Loss 19.8494(15.0561) | Error 0.2634(0.2729) Steps 1130(1113.92) | Grad Norm 4.8581(10.0246) | Total Time 0.00(0.00)\n",
      "Iter 2168 | Time 125.5461(120.8199) | Bit/dim 3.6224(3.6232) | Xent 0.7375(0.7605) | Loss 14.1392(15.0285) | Error 0.2591(0.2725) Steps 1124(1114.22) | Grad Norm 7.2824(9.9424) | Total Time 0.00(0.00)\n",
      "Iter 2169 | Time 123.8298(120.9102) | Bit/dim 3.6328(3.6235) | Xent 0.7396(0.7598) | Loss 14.0859(15.0003) | Error 0.2656(0.2723) Steps 1130(1114.70) | Grad Norm 8.9567(9.9128) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 120.9909(120.9126) | Bit/dim 3.6058(3.6230) | Xent 0.7411(0.7593) | Loss 13.7792(14.9636) | Error 0.2611(0.2719) Steps 1082(1113.71) | Grad Norm 4.9242(9.7631) | Total Time 0.00(0.00)\n",
      "Iter 2171 | Time 118.2380(120.8324) | Bit/dim 3.6108(3.6226) | Xent 0.7258(0.7583) | Loss 14.0704(14.9368) | Error 0.2616(0.2716) Steps 1100(1113.30) | Grad Norm 4.0620(9.5921) | Total Time 0.00(0.00)\n",
      "Iter 2172 | Time 121.1219(120.8411) | Bit/dim 3.6271(3.6228) | Xent 0.7324(0.7575) | Loss 14.0517(14.9103) | Error 0.2634(0.2714) Steps 1130(1113.80) | Grad Norm 6.9490(9.5128) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 41.4441, Epoch Time 791.4892(774.8970), Bit/dim 3.6235(best: 3.6184), Xent 0.8105, Loss 4.0287, Error 0.2870(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 122.8858(120.9024) | Bit/dim 3.6052(3.6222) | Xent 0.7416(0.7570) | Loss 20.3077(15.0722) | Error 0.2618(0.2711) Steps 1130(1114.29) | Grad Norm 10.0403(9.5286) | Total Time 0.00(0.00)\n",
      "Iter 2174 | Time 120.7344(120.8974) | Bit/dim 3.6219(3.6222) | Xent 0.7538(0.7569) | Loss 13.8392(15.0352) | Error 0.2720(0.2711) Steps 1118(1114.40) | Grad Norm 11.9943(9.6026) | Total Time 0.00(0.00)\n",
      "Iter 2175 | Time 120.8123(120.8948) | Bit/dim 3.6225(3.6222) | Xent 0.7583(0.7570) | Loss 13.8008(14.9982) | Error 0.2719(0.2711) Steps 1130(1114.87) | Grad Norm 15.3607(9.7753) | Total Time 0.00(0.00)\n",
      "Iter 2176 | Time 122.4538(120.9416) | Bit/dim 3.6262(3.6224) | Xent 0.7694(0.7573) | Loss 14.3335(14.9782) | Error 0.2782(0.2713) Steps 1136(1115.50) | Grad Norm 16.5880(9.9797) | Total Time 0.00(0.00)\n",
      "Iter 2177 | Time 119.6445(120.9027) | Bit/dim 3.6266(3.6225) | Xent 0.7438(0.7569) | Loss 13.7599(14.9417) | Error 0.2688(0.2713) Steps 1094(1114.86) | Grad Norm 11.4813(10.0248) | Total Time 0.00(0.00)\n",
      "Iter 2178 | Time 123.1987(120.9715) | Bit/dim 3.6073(3.6220) | Xent 0.7220(0.7559) | Loss 13.7331(14.9054) | Error 0.2569(0.2708) Steps 1124(1115.13) | Grad Norm 3.8729(9.8402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 40.1642, Epoch Time 786.2637(775.2380), Bit/dim 3.6207(best: 3.6184), Xent 0.8091, Loss 4.0253, Error 0.2889(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 121.7849(120.9959) | Bit/dim 3.6165(3.6219) | Xent 0.7462(0.7556) | Loss 19.9036(15.0554) | Error 0.2654(0.2707) Steps 1136(1115.76) | Grad Norm 8.3920(9.7968) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 123.1566(121.0608) | Bit/dim 3.6244(3.6219) | Xent 0.7403(0.7551) | Loss 13.9108(15.0210) | Error 0.2635(0.2705) Steps 1124(1116.01) | Grad Norm 7.3266(9.7227) | Total Time 0.00(0.00)\n",
      "Iter 2181 | Time 123.4590(121.1327) | Bit/dim 3.6120(3.6216) | Xent 0.7351(0.7545) | Loss 14.1034(14.9935) | Error 0.2688(0.2704) Steps 1124(1116.25) | Grad Norm 6.6450(9.6303) | Total Time 0.00(0.00)\n",
      "Iter 2182 | Time 125.0615(121.2506) | Bit/dim 3.6180(3.6215) | Xent 0.7585(0.7546) | Loss 14.0329(14.9647) | Error 0.2719(0.2704) Steps 1118(1116.30) | Grad Norm 8.3940(9.5932) | Total Time 0.00(0.00)\n",
      "Iter 2183 | Time 116.8820(121.1195) | Bit/dim 3.6183(3.6214) | Xent 0.7247(0.7537) | Loss 14.0441(14.9371) | Error 0.2538(0.2699) Steps 1112(1116.17) | Grad Norm 6.0442(9.4868) | Total Time 0.00(0.00)\n",
      "Iter 2184 | Time 120.7615(121.1088) | Bit/dim 3.6244(3.6215) | Xent 0.7248(0.7529) | Loss 14.0312(14.9099) | Error 0.2619(0.2697) Steps 1124(1116.40) | Grad Norm 5.7136(9.3736) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 41.4771, Epoch Time 788.7792(775.6442), Bit/dim 3.6207(best: 3.6184), Xent 0.8076, Loss 4.0245, Error 0.2875(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 120.5349(121.0916) | Bit/dim 3.6229(3.6216) | Xent 0.7357(0.7524) | Loss 19.7796(15.0560) | Error 0.2658(0.2696) Steps 1094(1115.73) | Grad Norm 9.1597(9.3672) | Total Time 0.00(0.00)\n",
      "Iter 2186 | Time 109.5526(120.7454) | Bit/dim 3.6247(3.6217) | Xent 0.7227(0.7515) | Loss 13.8599(15.0201) | Error 0.2602(0.2693) Steps 1112(1115.62) | Grad Norm 7.4121(9.3085) | Total Time 0.00(0.00)\n",
      "Iter 2187 | Time 121.5108(120.7684) | Bit/dim 3.6023(3.6211) | Xent 0.7032(0.7500) | Loss 14.0021(14.9896) | Error 0.2524(0.2688) Steps 1094(1114.97) | Grad Norm 5.0491(9.1807) | Total Time 0.00(0.00)\n",
      "Iter 2188 | Time 118.4412(120.6985) | Bit/dim 3.6170(3.6210) | Xent 0.7390(0.7497) | Loss 13.9775(14.9592) | Error 0.2671(0.2687) Steps 1106(1114.70) | Grad Norm 8.3531(9.1559) | Total Time 0.00(0.00)\n",
      "Iter 2189 | Time 120.3448(120.6879) | Bit/dim 3.6258(3.6211) | Xent 0.7770(0.7505) | Loss 13.9036(14.9275) | Error 0.2809(0.2691) Steps 1106(1114.44) | Grad Norm 20.0831(9.4837) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 117.7195(120.5989) | Bit/dim 3.6192(3.6210) | Xent 0.9219(0.7557) | Loss 14.2563(14.9074) | Error 0.3226(0.2707) Steps 1106(1114.19) | Grad Norm 27.7508(10.0317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 40.9127, Epoch Time 765.4698(775.3390), Bit/dim 3.6158(best: 3.6184), Xent 0.8077, Loss 4.0196, Error 0.2839(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 118.9209(120.5485) | Bit/dim 3.6217(3.6211) | Xent 0.7504(0.7555) | Loss 20.1950(15.0660) | Error 0.2650(0.2705) Steps 1076(1113.04) | Grad Norm 9.6293(10.0197) | Total Time 0.00(0.00)\n",
      "Iter 2192 | Time 118.1722(120.4772) | Bit/dim 3.6218(3.6211) | Xent 0.8183(0.7574) | Loss 14.2783(15.0424) | Error 0.2954(0.2713) Steps 1100(1112.65) | Grad Norm 19.0268(10.2899) | Total Time 0.00(0.00)\n",
      "Iter 2193 | Time 123.9302(120.5808) | Bit/dim 3.6211(3.6211) | Xent 0.8500(0.7602) | Loss 13.8986(15.0081) | Error 0.3016(0.2722) Steps 1142(1113.53) | Grad Norm 15.9475(10.4596) | Total Time 0.00(0.00)\n",
      "Iter 2194 | Time 124.1957(120.6893) | Bit/dim 3.6262(3.6212) | Xent 0.8025(0.7614) | Loss 13.9892(14.9775) | Error 0.2850(0.2726) Steps 1142(1114.39) | Grad Norm 15.8262(10.6206) | Total Time 0.00(0.00)\n",
      "Iter 2195 | Time 123.2577(120.7663) | Bit/dim 3.6335(3.6216) | Xent 0.8273(0.7634) | Loss 14.0672(14.9502) | Error 0.2943(0.2732) Steps 1106(1114.13) | Grad Norm 13.8587(10.7177) | Total Time 0.00(0.00)\n",
      "Iter 2196 | Time 120.3353(120.7534) | Bit/dim 3.6410(3.6222) | Xent 0.7862(0.7641) | Loss 14.2621(14.9296) | Error 0.2800(0.2734) Steps 1112(1114.07) | Grad Norm 14.3440(10.8265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 40.5206, Epoch Time 785.6949(775.6497), Bit/dim 3.6335(best: 3.6158), Xent 0.8364, Loss 4.0517, Error 0.2980(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 121.2713(120.7689) | Bit/dim 3.6227(3.6222) | Xent 0.7736(0.7644) | Loss 19.8552(15.0773) | Error 0.2771(0.2735) Steps 1106(1113.83) | Grad Norm 11.1751(10.8370) | Total Time 0.00(0.00)\n",
      "Iter 2198 | Time 120.9701(120.7750) | Bit/dim 3.6226(3.6222) | Xent 0.7716(0.7646) | Loss 13.8278(15.0398) | Error 0.2775(0.2737) Steps 1130(1114.31) | Grad Norm 7.8678(10.7479) | Total Time 0.00(0.00)\n",
      "Iter 2199 | Time 123.2014(120.8478) | Bit/dim 3.6302(3.6225) | Xent 0.7607(0.7645) | Loss 13.9502(15.0072) | Error 0.2719(0.2736) Steps 1148(1115.32) | Grad Norm 9.3253(10.7052) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 117.5201(120.7479) | Bit/dim 3.6210(3.6224) | Xent 0.7383(0.7637) | Loss 14.0231(14.9776) | Error 0.2639(0.2733) Steps 1094(1114.68) | Grad Norm 8.7510(10.6466) | Total Time 0.00(0.00)\n",
      "Iter 2201 | Time 124.3645(120.8564) | Bit/dim 3.6289(3.6226) | Xent 0.7410(0.7630) | Loss 14.0382(14.9495) | Error 0.2656(0.2731) Steps 1124(1114.96) | Grad Norm 14.4913(10.7619) | Total Time 0.00(0.00)\n",
      "Iter 2202 | Time 123.8750(120.9470) | Bit/dim 3.6287(3.6228) | Xent 0.7710(0.7632) | Loss 14.1897(14.9267) | Error 0.2719(0.2731) Steps 1118(1115.05) | Grad Norm 6.8675(10.6451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 40.5845, Epoch Time 788.3551(776.0309), Bit/dim 3.6256(best: 3.6158), Xent 0.8026, Loss 4.0269, Error 0.2864(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 116.3025(120.8077) | Bit/dim 3.6234(3.6228) | Xent 0.7502(0.7629) | Loss 19.6946(15.0697) | Error 0.2704(0.2730) Steps 1112(1114.96) | Grad Norm 8.9460(10.5941) | Total Time 0.00(0.00)\n",
      "Iter 2204 | Time 122.0356(120.8445) | Bit/dim 3.6195(3.6227) | Xent 0.7634(0.7629) | Loss 14.1118(15.0410) | Error 0.2749(0.2730) Steps 1142(1115.77) | Grad Norm 10.5077(10.5915) | Total Time 0.00(0.00)\n",
      "Iter 2205 | Time 127.8578(121.0549) | Bit/dim 3.6337(3.6230) | Xent 0.7255(0.7618) | Loss 14.1275(15.0136) | Error 0.2612(0.2727) Steps 1130(1116.20) | Grad Norm 8.2786(10.5221) | Total Time 0.00(0.00)\n",
      "Iter 2206 | Time 118.4399(120.9764) | Bit/dim 3.6243(3.6231) | Xent 0.7238(0.7606) | Loss 13.9626(14.9820) | Error 0.2569(0.2722) Steps 1118(1116.25) | Grad Norm 8.8508(10.4720) | Total Time 0.00(0.00)\n",
      "Iter 2207 | Time 114.5648(120.7841) | Bit/dim 3.6263(3.6232) | Xent 0.7555(0.7605) | Loss 14.0873(14.9552) | Error 0.2729(0.2722) Steps 1148(1117.21) | Grad Norm 9.5664(10.4448) | Total Time 0.00(0.00)\n",
      "Iter 2208 | Time 120.0579(120.7623) | Bit/dim 3.6261(3.6233) | Xent 0.7639(0.7606) | Loss 14.2140(14.9330) | Error 0.2722(0.2722) Steps 1142(1117.95) | Grad Norm 12.1644(10.4964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 39.8611, Epoch Time 775.8285(776.0248), Bit/dim 3.6259(best: 3.6158), Xent 0.8035, Loss 4.0277, Error 0.2853(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 123.7566(120.8521) | Bit/dim 3.6302(3.6235) | Xent 0.7495(0.7602) | Loss 19.2455(15.0623) | Error 0.2665(0.2721) Steps 1124(1118.13) | Grad Norm 7.9010(10.4186) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 124.0987(120.9495) | Bit/dim 3.6235(3.6235) | Xent 0.7297(0.7593) | Loss 14.0646(15.0324) | Error 0.2628(0.2718) Steps 1130(1118.49) | Grad Norm 7.8340(10.3410) | Total Time 0.00(0.00)\n",
      "Iter 2211 | Time 124.5921(121.0588) | Bit/dim 3.6182(3.6233) | Xent 0.7334(0.7585) | Loss 14.0519(15.0030) | Error 0.2612(0.2715) Steps 1124(1118.65) | Grad Norm 8.6452(10.2902) | Total Time 0.00(0.00)\n",
      "Iter 2212 | Time 123.7054(121.1382) | Bit/dim 3.6247(3.6234) | Xent 0.7269(0.7576) | Loss 14.1676(14.9779) | Error 0.2624(0.2712) Steps 1136(1119.17) | Grad Norm 6.5601(10.1783) | Total Time 0.00(0.00)\n",
      "Iter 2213 | Time 126.5345(121.3001) | Bit/dim 3.6158(3.6231) | Xent 0.7162(0.7563) | Loss 13.4822(14.9330) | Error 0.2551(0.2707) Steps 1142(1119.86) | Grad Norm 4.7153(10.0144) | Total Time 0.00(0.00)\n",
      "Iter 2214 | Time 120.5236(121.2768) | Bit/dim 3.6237(3.6231) | Xent 0.7200(0.7553) | Loss 13.8547(14.9007) | Error 0.2555(0.2702) Steps 1094(1119.08) | Grad Norm 7.9776(9.9533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 40.7856, Epoch Time 800.3790(776.7554), Bit/dim 3.6233(best: 3.6158), Xent 0.7991, Loss 4.0229, Error 0.2878(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 117.1401(121.1527) | Bit/dim 3.6155(3.6229) | Xent 0.7508(0.7551) | Loss 20.0499(15.0552) | Error 0.2598(0.2699) Steps 1142(1119.77) | Grad Norm 9.2532(9.9323) | Total Time 0.00(0.00)\n",
      "Iter 2216 | Time 121.8765(121.1744) | Bit/dim 3.6143(3.6227) | Xent 0.7578(0.7552) | Loss 14.0694(15.0256) | Error 0.2704(0.2699) Steps 1136(1120.26) | Grad Norm 17.5081(10.1595) | Total Time 0.00(0.00)\n",
      "Iter 2217 | Time 118.0316(121.0801) | Bit/dim 3.6231(3.6227) | Xent 0.8057(0.7567) | Loss 14.1348(14.9989) | Error 0.2829(0.2703) Steps 1106(1119.83) | Grad Norm 21.6997(10.5057) | Total Time 0.00(0.00)\n",
      "Iter 2218 | Time 122.5582(121.1245) | Bit/dim 3.6211(3.6226) | Xent 0.7597(0.7568) | Loss 14.1987(14.9749) | Error 0.2719(0.2704) Steps 1100(1119.23) | Grad Norm 10.9132(10.5180) | Total Time 0.00(0.00)\n",
      "Iter 2219 | Time 115.0413(120.9420) | Bit/dim 3.6278(3.6228) | Xent 0.7357(0.7562) | Loss 13.9878(14.9453) | Error 0.2670(0.2703) Steps 1100(1118.66) | Grad Norm 5.9193(10.3800) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 122.0114(120.9741) | Bit/dim 3.6269(3.6229) | Xent 0.7388(0.7557) | Loss 13.9964(14.9168) | Error 0.2650(0.2701) Steps 1124(1118.82) | Grad Norm 13.0995(10.4616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 41.4376, Epoch Time 774.4018(776.6848), Bit/dim 3.6200(best: 3.6158), Xent 0.7988, Loss 4.0195, Error 0.2822(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 116.7944(120.8487) | Bit/dim 3.6314(3.6232) | Xent 0.7134(0.7544) | Loss 19.8022(15.0634) | Error 0.2535(0.2696) Steps 1118(1118.79) | Grad Norm 7.4159(10.3702) | Total Time 0.00(0.00)\n",
      "Iter 2222 | Time 122.8872(120.9098) | Bit/dim 3.6138(3.6229) | Xent 0.7386(0.7539) | Loss 14.1051(15.0346) | Error 0.2654(0.2695) Steps 1130(1119.13) | Grad Norm 8.8761(10.3254) | Total Time 0.00(0.00)\n",
      "Iter 2223 | Time 117.7976(120.8165) | Bit/dim 3.6111(3.6225) | Xent 0.7561(0.7540) | Loss 13.8658(14.9995) | Error 0.2751(0.2697) Steps 1082(1118.02) | Grad Norm 13.1173(10.4091) | Total Time 0.00(0.00)\n",
      "Iter 2224 | Time 121.5425(120.8382) | Bit/dim 3.6275(3.6227) | Xent 0.7106(0.7527) | Loss 14.0886(14.9722) | Error 0.2521(0.2691) Steps 1130(1118.38) | Grad Norm 5.1577(10.2516) | Total Time 0.00(0.00)\n",
      "Iter 2225 | Time 123.0283(120.9039) | Bit/dim 3.6002(3.6220) | Xent 0.7369(0.7522) | Loss 13.8046(14.9372) | Error 0.2622(0.2689) Steps 1136(1118.90) | Grad Norm 7.0479(10.1555) | Total Time 0.00(0.00)\n",
      "Iter 2226 | Time 123.3592(120.9776) | Bit/dim 3.6193(3.6219) | Xent 0.7286(0.7515) | Loss 14.0753(14.9113) | Error 0.2602(0.2687) Steps 1106(1118.52) | Grad Norm 9.3421(10.1311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 41.2077, Epoch Time 783.5204(776.8899), Bit/dim 3.6209(best: 3.6158), Xent 0.7966, Loss 4.0192, Error 0.2823(best: 0.2783)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 122.7051(121.0294) | Bit/dim 3.6105(3.6216) | Xent 0.7171(0.7505) | Loss 20.0415(15.0652) | Error 0.2575(0.2683) Steps 1112(1118.32) | Grad Norm 7.8669(10.0632) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 4 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_4block_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_4block_run2/current_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
