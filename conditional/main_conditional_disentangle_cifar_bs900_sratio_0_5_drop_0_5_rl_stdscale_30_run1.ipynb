{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_30_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 9.7102(26.4739) | Bit/dim 8.6904(8.9519) | Xent 2.2806(2.3001) | Loss 19.1850(19.8709) | Error 0.7989(0.8599) Steps 0(0.00) | Grad Norm 20.5312(26.6079) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 10.7412(22.2445) | Bit/dim 8.4875(8.8631) | Xent 2.2266(2.2874) | Loss 19.0382(19.6798) | Error 0.7233(0.8323) Steps 0(0.00) | Grad Norm 8.6616(23.0366) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 10.8561(19.0993) | Bit/dim 8.3923(8.7514) | Xent 2.1743(2.2634) | Loss 18.3877(19.4405) | Error 0.7544(0.8089) Steps 0(0.00) | Grad Norm 8.3989(18.9160) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 10.7057(16.8449) | Bit/dim 8.1927(8.6234) | Xent 2.1132(2.2346) | Loss 18.4083(19.1822) | Error 0.7267(0.7905) Steps 0(0.00) | Grad Norm 5.3809(15.5266) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 10.4754(15.0948) | Bit/dim 7.9745(8.4738) | Xent 2.1015(2.2020) | Loss 17.7915(18.8632) | Error 0.7044(0.7746) Steps 0(0.00) | Grad Norm 5.1975(12.8938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 64.9135, Epoch Time 671.2409(671.2409), Bit/dim 7.7699(best: inf), Xent 2.0785, Loss 8.8091, Error 0.6996(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 10.5825(13.8922) | Bit/dim 7.6553(8.2922) | Xent 2.0890(2.1718) | Loss 17.2242(18.8811) | Error 0.7078(0.7582) Steps 0(0.00) | Grad Norm 4.7842(10.8655) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 10.1279(13.0226) | Bit/dim 7.3662(8.0807) | Xent 2.0765(2.1452) | Loss 16.5248(18.3465) | Error 0.7000(0.7411) Steps 0(0.00) | Grad Norm 4.1065(9.1844) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 10.6749(12.3890) | Bit/dim 7.1814(7.8642) | Xent 2.0835(2.1248) | Loss 16.3778(17.8554) | Error 0.6733(0.7256) Steps 0(0.00) | Grad Norm 2.9832(7.6739) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 10.8007(11.9589) | Bit/dim 7.0769(7.6684) | Xent 2.0845(2.1140) | Loss 16.1747(17.4135) | Error 0.6800(0.7182) Steps 0(0.00) | Grad Norm 2.2965(6.3024) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 11.5781(11.7174) | Bit/dim 7.0201(7.5058) | Xent 2.0805(2.1033) | Loss 15.9376(17.0511) | Error 0.6978(0.7134) Steps 0(0.00) | Grad Norm 2.4578(5.1932) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 11.3684(11.6368) | Bit/dim 6.9998(7.3758) | Xent 2.0651(2.0946) | Loss 16.0594(16.7893) | Error 0.7189(0.7128) Steps 0(0.00) | Grad Norm 6.1355(4.7115) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 62.7904, Epoch Time 677.6489(671.4332), Bit/dim 6.9918(best: 7.7699), Xent 2.0575, Loss 8.0206, Error 0.6957(best: 0.6996)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 11.2828(11.5643) | Bit/dim 6.9381(7.2706) | Xent 2.0479(2.0852) | Loss 15.8223(16.9111) | Error 0.6767(0.7108) Steps 0(0.00) | Grad Norm 3.5488(4.6244) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 11.6025(11.6141) | Bit/dim 6.9079(7.1832) | Xent 2.0652(2.0760) | Loss 15.7872(16.6344) | Error 0.7189(0.7078) Steps 0(0.00) | Grad Norm 4.6498(4.2706) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 10.9736(11.6472) | Bit/dim 6.8746(7.1068) | Xent 2.0518(2.0702) | Loss 15.6473(16.3968) | Error 0.6944(0.7046) Steps 0(0.00) | Grad Norm 13.7217(4.9019) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 11.3684(11.7121) | Bit/dim 6.8471(7.0375) | Xent 1.9903(2.0586) | Loss 15.7603(16.1900) | Error 0.6778(0.7001) Steps 0(0.00) | Grad Norm 2.0261(5.0399) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 12.2770(11.7331) | Bit/dim 6.7211(6.9634) | Xent 1.9773(2.0479) | Loss 15.4346(15.9908) | Error 0.6800(0.6965) Steps 0(0.00) | Grad Norm 13.8444(5.9570) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 63.9143, Epoch Time 727.6022(673.1182), Bit/dim 6.6411(best: 6.9918), Xent 2.0307, Loss 7.6565, Error 0.6974(best: 0.6957)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 11.9058(11.8451) | Bit/dim 6.5805(6.8812) | Xent 2.0164(2.0447) | Loss 15.1249(16.2120) | Error 0.6744(0.6978) Steps 0(0.00) | Grad Norm 5.0720(11.4855) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 11.4755(11.8661) | Bit/dim 6.4532(6.7792) | Xent 2.1790(2.0462) | Loss 15.1439(15.9011) | Error 0.7789(0.7025) Steps 0(0.00) | Grad Norm 98.4957(21.2652) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 11.8902(11.8611) | Bit/dim 6.2483(6.6591) | Xent 2.0267(2.0644) | Loss 14.6620(15.6097) | Error 0.7033(0.7185) Steps 0(0.00) | Grad Norm 19.7288(31.0921) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 11.6824(11.8689) | Bit/dim 6.1746(6.5322) | Xent 2.1629(2.0624) | Loss 14.3301(15.2854) | Error 0.7789(0.7182) Steps 0(0.00) | Grad Norm 123.2390(35.8100) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 12.1001(11.9396) | Bit/dim 6.0784(6.4592) | Xent 2.0793(2.1013) | Loss 14.0811(15.1233) | Error 0.7556(0.7295) Steps 0(0.00) | Grad Norm 18.7984(53.2605) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 11.8593(11.9098) | Bit/dim 5.9497(6.3462) | Xent 2.0721(2.0933) | Loss 14.0061(14.8642) | Error 0.6867(0.7280) Steps 0(0.00) | Grad Norm 29.6672(45.1428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 64.8679, Epoch Time 739.5253(675.1105), Bit/dim 5.9271(best: 6.6411), Xent 2.0564, Loss 6.9553, Error 0.7092(best: 0.6957)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 11.4560(11.8736) | Bit/dim 5.8609(6.2228) | Xent 2.0189(2.0814) | Loss 13.5904(14.9591) | Error 0.7078(0.7250) Steps 0(0.00) | Grad Norm 41.8799(39.7090) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 11.2117(11.8061) | Bit/dim 5.7511(6.1102) | Xent 2.0395(2.0689) | Loss 13.6151(14.6305) | Error 0.7333(0.7222) Steps 0(0.00) | Grad Norm 58.8716(37.6981) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 11.0330(11.7322) | Bit/dim 5.6910(6.0123) | Xent 1.9775(2.0618) | Loss 13.3782(14.3273) | Error 0.7078(0.7214) Steps 0(0.00) | Grad Norm 30.0140(39.1759) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 11.0123(11.7257) | Bit/dim 5.7019(5.9269) | Xent 2.0499(2.0493) | Loss 13.2976(14.0931) | Error 0.6811(0.7129) Steps 0(0.00) | Grad Norm 4.5544(34.3258) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 11.6310(11.6567) | Bit/dim 5.6494(5.8582) | Xent 1.9985(2.0380) | Loss 13.4231(13.9085) | Error 0.6867(0.7033) Steps 0(0.00) | Grad Norm 16.0918(29.1472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 64.6491, Epoch Time 721.8119(676.5115), Bit/dim 5.6367(best: 5.9271), Xent 1.9549, Loss 6.6141, Error 0.6421(best: 0.6957)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 10.8138(11.6330) | Bit/dim 5.6345(5.8014) | Xent 1.9646(2.0220) | Loss 13.2426(14.1715) | Error 0.6856(0.6953) Steps 0(0.00) | Grad Norm 9.4023(25.3017) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 11.7036(11.5973) | Bit/dim 5.6037(5.7518) | Xent 1.9407(2.0129) | Loss 13.2268(13.9237) | Error 0.6856(0.6920) Steps 0(0.00) | Grad Norm 50.8489(28.7664) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 11.8752(11.6420) | Bit/dim 5.6063(5.7076) | Xent 1.9230(1.9964) | Loss 12.9334(13.7230) | Error 0.6467(0.6854) Steps 0(0.00) | Grad Norm 7.8152(26.7907) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 11.2749(11.6454) | Bit/dim 5.5795(5.6727) | Xent 1.9480(1.9819) | Loss 13.1883(13.5683) | Error 0.6656(0.6804) Steps 0(0.00) | Grad Norm 28.1461(25.2059) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 11.4379(11.5793) | Bit/dim 5.5911(5.6449) | Xent 2.0632(1.9814) | Loss 13.2331(13.4528) | Error 0.7222(0.6817) Steps 0(0.00) | Grad Norm 63.4422(29.2721) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 11.8144(11.5863) | Bit/dim 5.5780(5.6267) | Xent 2.0533(2.0136) | Loss 13.3624(13.4243) | Error 0.7233(0.6984) Steps 0(0.00) | Grad Norm 23.1320(38.3466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 64.4400, Epoch Time 718.6979(677.7771), Bit/dim 5.5250(best: 5.6367), Xent 2.0135, Loss 6.5317, Error 0.6949(best: 0.6421)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 11.2778(11.6997) | Bit/dim 5.4956(5.5963) | Xent 1.9885(2.0120) | Loss 13.0682(13.7035) | Error 0.6844(0.6962) Steps 0(0.00) | Grad Norm 15.9354(32.4957) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 11.8462(11.7679) | Bit/dim 5.4223(5.5662) | Xent 1.9685(2.0003) | Loss 12.9437(13.5154) | Error 0.6689(0.6910) Steps 0(0.00) | Grad Norm 4.0602(25.8497) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 11.1678(11.7724) | Bit/dim 5.4049(5.5280) | Xent 1.9313(1.9831) | Loss 12.8277(13.3390) | Error 0.6533(0.6829) Steps 0(0.00) | Grad Norm 14.3063(20.9557) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 11.5854(11.8119) | Bit/dim 5.3765(5.4973) | Xent 1.9362(1.9670) | Loss 12.7331(13.2036) | Error 0.6533(0.6766) Steps 0(0.00) | Grad Norm 7.6981(18.6891) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 12.6000(11.9262) | Bit/dim 5.3709(5.4586) | Xent 1.8637(1.9522) | Loss 12.8215(13.0908) | Error 0.6289(0.6721) Steps 0(0.00) | Grad Norm 5.0930(18.2743) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 65.8565, Epoch Time 743.1344(679.7378), Bit/dim 5.3084(best: 5.5250), Xent 1.8768, Loss 6.2468, Error 0.6319(best: 0.6421)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 12.9493(11.9792) | Bit/dim 5.3434(5.4205) | Xent 1.9254(1.9365) | Loss 12.6811(13.4169) | Error 0.6656(0.6692) Steps 0(0.00) | Grad Norm 8.9327(18.1070) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 11.6113(12.0288) | Bit/dim 5.2860(5.3860) | Xent 1.9253(1.9248) | Loss 12.5342(13.1913) | Error 0.6556(0.6664) Steps 0(0.00) | Grad Norm 28.6662(20.7131) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 12.1757(12.1229) | Bit/dim 5.2770(5.3534) | Xent 2.0297(1.9278) | Loss 12.6340(13.0300) | Error 0.7389(0.6700) Steps 0(0.00) | Grad Norm 67.5667(27.0467) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 12.0753(12.1668) | Bit/dim 5.3017(5.3462) | Xent 2.1776(1.9640) | Loss 12.8513(12.9630) | Error 0.7967(0.6905) Steps 0(0.00) | Grad Norm 76.1820(33.3191) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 12.4617(12.1162) | Bit/dim 5.2310(5.3212) | Xent 1.9699(1.9787) | Loss 12.5950(12.8793) | Error 0.7156(0.6976) Steps 0(0.00) | Grad Norm 14.1158(32.1117) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 11.8054(12.0648) | Bit/dim 5.1686(5.2866) | Xent 1.9205(1.9710) | Loss 12.3189(12.7465) | Error 0.6789(0.6934) Steps 0(0.00) | Grad Norm 4.9290(27.2179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 65.3017, Epoch Time 751.4845(681.8902), Bit/dim 5.1642(best: 5.3084), Xent 1.9223, Loss 6.1253, Error 0.6667(best: 0.6319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 12.6491(12.0776) | Bit/dim 5.1276(5.2523) | Xent 1.9034(1.9585) | Loss 12.3686(13.0078) | Error 0.6733(0.6859) Steps 0(0.00) | Grad Norm 16.2391(23.2558) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 12.5373(12.1409) | Bit/dim 5.1117(5.2194) | Xent 1.8558(1.9464) | Loss 12.1826(12.8299) | Error 0.6456(0.6801) Steps 0(0.00) | Grad Norm 4.5295(20.5950) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 12.1883(12.2199) | Bit/dim 5.1147(5.1872) | Xent 1.9073(1.9316) | Loss 12.2755(12.6673) | Error 0.6533(0.6750) Steps 0(0.00) | Grad Norm 5.1233(19.1006) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 11.7115(12.2245) | Bit/dim 5.1158(5.1573) | Xent 1.8840(1.9167) | Loss 12.2726(12.5250) | Error 0.6589(0.6696) Steps 0(0.00) | Grad Norm 16.3931(18.4339) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 12.5683(12.3186) | Bit/dim 5.0225(5.1289) | Xent 1.9250(1.9127) | Loss 12.0355(12.4178) | Error 0.6589(0.6678) Steps 0(0.00) | Grad Norm 34.0557(23.0644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 68.4159, Epoch Time 766.2656(684.4215), Bit/dim 5.0043(best: 5.1642), Xent 1.8407, Loss 5.9246, Error 0.6351(best: 0.6319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 12.2408(12.3617) | Bit/dim 4.9653(5.0972) | Xent 1.8246(1.9065) | Loss 11.8342(12.7437) | Error 0.6278(0.6652) Steps 0(0.00) | Grad Norm 14.5408(22.9449) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 12.4883(12.4131) | Bit/dim 4.9516(5.0650) | Xent 1.8435(1.8916) | Loss 11.8240(12.5273) | Error 0.6322(0.6603) Steps 0(0.00) | Grad Norm 15.5328(21.0078) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 12.8513(12.4061) | Bit/dim 4.9500(5.0378) | Xent 1.8638(1.8860) | Loss 11.9698(12.3634) | Error 0.6444(0.6574) Steps 0(0.00) | Grad Norm 40.2427(23.8218) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 12.8086(12.4527) | Bit/dim 4.9264(5.0278) | Xent 1.8873(1.8949) | Loss 11.7435(12.2829) | Error 0.6767(0.6619) Steps 0(0.00) | Grad Norm 14.4260(29.8236) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 12.8333(12.5958) | Bit/dim 4.9011(4.9981) | Xent 1.9196(1.8978) | Loss 11.9101(12.1811) | Error 0.6944(0.6657) Steps 0(0.00) | Grad Norm 9.2167(26.7734) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 12.8617(12.6152) | Bit/dim 4.8319(4.9670) | Xent 1.8037(1.8843) | Loss 11.6606(12.0686) | Error 0.6589(0.6616) Steps 0(0.00) | Grad Norm 9.7254(23.3697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 68.8439, Epoch Time 780.1561(687.2935), Bit/dim 4.8588(best: 5.0043), Xent 1.7939, Loss 5.7558, Error 0.6199(best: 0.6319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 13.2213(12.6074) | Bit/dim 4.8669(4.9383) | Xent 1.7492(1.8628) | Loss 11.6693(12.3412) | Error 0.6233(0.6537) Steps 0(0.00) | Grad Norm 8.6397(21.1503) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 13.2384(12.6935) | Bit/dim 4.8567(4.9181) | Xent 1.9534(1.8645) | Loss 11.8473(12.1975) | Error 0.6811(0.6541) Steps 0(0.00) | Grad Norm 45.6603(27.3395) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 13.3852(12.8431) | Bit/dim 4.8765(4.8983) | Xent 1.8702(1.8648) | Loss 11.7568(12.0697) | Error 0.6411(0.6550) Steps 0(0.00) | Grad Norm 38.8467(27.9548) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 12.7793(12.8473) | Bit/dim 4.7676(4.8720) | Xent 1.8598(1.8541) | Loss 11.4790(11.9427) | Error 0.6522(0.6516) Steps 0(0.00) | Grad Norm 42.0391(26.7160) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 14.0533(12.9462) | Bit/dim 4.7982(4.8529) | Xent 1.8190(1.8393) | Loss 11.6958(11.8515) | Error 0.6522(0.6471) Steps 0(0.00) | Grad Norm 33.5965(26.5574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 69.6528, Epoch Time 801.2100(690.7110), Bit/dim 4.8271(best: 4.8588), Xent 1.8056, Loss 5.7299, Error 0.6342(best: 0.6199)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 13.7884(12.9625) | Bit/dim 4.8506(4.8383) | Xent 2.2131(1.8495) | Loss 12.1793(12.2423) | Error 0.7422(0.6507) Steps 0(0.00) | Grad Norm 85.7622(31.2887) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 13.6269(13.0356) | Bit/dim 4.7471(4.8326) | Xent 1.8660(1.8667) | Loss 11.6243(12.1192) | Error 0.6833(0.6595) Steps 0(0.00) | Grad Norm 22.9876(32.4030) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 13.3804(13.0964) | Bit/dim 4.7417(4.8118) | Xent 1.7814(1.8556) | Loss 11.5683(11.9661) | Error 0.6433(0.6546) Steps 0(0.00) | Grad Norm 14.0454(27.7655) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 13.9459(13.1295) | Bit/dim 4.7858(4.7938) | Xent 1.8593(1.8424) | Loss 11.7881(11.8398) | Error 0.6422(0.6497) Steps 0(0.00) | Grad Norm 40.2164(25.7318) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 13.3099(13.1971) | Bit/dim 4.7070(4.7730) | Xent 1.7843(1.8216) | Loss 11.3617(11.7202) | Error 0.6178(0.6425) Steps 0(0.00) | Grad Norm 27.0540(24.5027) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 13.4803(13.1705) | Bit/dim 4.7956(4.7776) | Xent 2.1309(1.8336) | Loss 11.9908(11.7056) | Error 0.7500(0.6460) Steps 0(0.00) | Grad Norm 67.4786(30.5341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 71.3614, Epoch Time 815.7349(694.4617), Bit/dim 4.8805(best: 4.8271), Xent 1.8352, Loss 5.7982, Error 0.6471(best: 0.6199)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 13.0569(13.1611) | Bit/dim 4.7700(4.7934) | Xent 1.8311(1.8407) | Loss 11.5078(12.0937) | Error 0.6756(0.6493) Steps 0(0.00) | Grad Norm 16.3958(29.3413) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 12.8188(13.2470) | Bit/dim 4.7068(4.7762) | Xent 1.7829(1.8270) | Loss 11.2903(11.9178) | Error 0.6200(0.6431) Steps 0(0.00) | Grad Norm 17.9532(25.8026) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 12.2902(13.2803) | Bit/dim 4.6420(4.7478) | Xent 1.7573(1.8098) | Loss 11.1519(11.7584) | Error 0.6167(0.6390) Steps 0(0.00) | Grad Norm 11.3257(22.6170) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 13.2216(13.2904) | Bit/dim 4.6590(4.7246) | Xent 1.7353(1.7890) | Loss 11.4014(11.6369) | Error 0.6278(0.6330) Steps 0(0.00) | Grad Norm 22.6801(22.3845) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 13.2173(13.3775) | Bit/dim 4.6215(4.7001) | Xent 1.7229(1.7665) | Loss 11.3403(11.5118) | Error 0.6100(0.6259) Steps 0(0.00) | Grad Norm 36.1591(20.7488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 73.8354, Epoch Time 829.2030(698.5040), Bit/dim 4.6536(best: 4.8271), Xent 1.6904, Loss 5.4988, Error 0.5994(best: 0.6199)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 13.9682(13.4589) | Bit/dim 4.6140(4.6811) | Xent 1.7937(1.7596) | Loss 11.4508(11.9088) | Error 0.6300(0.6235) Steps 0(0.00) | Grad Norm 45.0721(24.4963) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 13.9548(13.6736) | Bit/dim 4.6295(4.6614) | Xent 1.7031(1.7458) | Loss 11.2838(11.7140) | Error 0.5933(0.6201) Steps 0(0.00) | Grad Norm 19.7215(23.7592) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 13.5038(13.6817) | Bit/dim 4.5852(4.6449) | Xent 1.7081(1.7336) | Loss 11.1692(11.5623) | Error 0.6011(0.6133) Steps 0(0.00) | Grad Norm 18.2002(22.6046) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.3675(13.6724) | Bit/dim 4.6507(4.6412) | Xent 1.9993(1.7474) | Loss 11.5162(11.4946) | Error 0.6956(0.6175) Steps 0(0.00) | Grad Norm 70.6549(27.4123) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 13.0808(13.6298) | Bit/dim 4.5719(4.6299) | Xent 1.7494(1.7548) | Loss 11.2374(11.4318) | Error 0.6256(0.6215) Steps 0(0.00) | Grad Norm 18.8474(25.7748) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 13.3878(13.5551) | Bit/dim 4.5848(4.6143) | Xent 1.6940(1.7447) | Loss 11.1942(11.3543) | Error 0.6033(0.6193) Steps 0(0.00) | Grad Norm 5.8507(22.0026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 71.0461, Epoch Time 843.1700(702.8439), Bit/dim 4.5577(best: 4.6536), Xent 1.6116, Loss 5.3635, Error 0.5751(best: 0.5994)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 13.1357(13.5453) | Bit/dim 4.5153(4.5968) | Xent 1.6102(1.7182) | Loss 10.7756(11.6561) | Error 0.5867(0.6116) Steps 0(0.00) | Grad Norm 10.1826(19.2481) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 13.7988(13.5846) | Bit/dim 4.6625(4.5981) | Xent 1.8355(1.7396) | Loss 11.3690(11.5507) | Error 0.6589(0.6178) Steps 0(0.00) | Grad Norm 27.1495(23.4710) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 12.9527(13.5883) | Bit/dim 4.5530(4.5896) | Xent 1.7481(1.7542) | Loss 11.0609(11.4520) | Error 0.6400(0.6238) Steps 0(0.00) | Grad Norm 9.2213(21.3679) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 13.5963(13.5993) | Bit/dim 4.5143(4.5743) | Xent 1.6320(1.7429) | Loss 10.9491(11.3410) | Error 0.5756(0.6217) Steps 0(0.00) | Grad Norm 6.4257(18.2978) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.2444(13.6823) | Bit/dim 4.5465(4.5613) | Xent 1.6880(1.7242) | Loss 11.0992(11.2559) | Error 0.6144(0.6150) Steps 0(0.00) | Grad Norm 18.3558(16.0289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 74.1646, Epoch Time 844.9070(707.1058), Bit/dim 4.4927(best: 4.5577), Xent 1.6183, Loss 5.3018, Error 0.5790(best: 0.5751)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 14.2511(13.8142) | Bit/dim 4.5192(4.5432) | Xent 1.6525(1.7027) | Loss 10.8468(11.6514) | Error 0.6056(0.6098) Steps 0(0.00) | Grad Norm 31.7173(17.1602) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 14.9443(13.8910) | Bit/dim 4.5134(4.5392) | Xent 1.6750(1.7206) | Loss 11.1441(11.5149) | Error 0.5689(0.6120) Steps 0(0.00) | Grad Norm 19.3041(23.2619) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 14.2072(13.9708) | Bit/dim 4.4495(4.5236) | Xent 1.6538(1.7135) | Loss 10.8914(11.3604) | Error 0.5956(0.6120) Steps 0(0.00) | Grad Norm 12.8300(21.5270) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 13.8098(13.9440) | Bit/dim 4.4683(4.5089) | Xent 1.5838(1.6936) | Loss 10.8208(11.2252) | Error 0.5556(0.6050) Steps 0(0.00) | Grad Norm 18.8019(19.7339) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.2092(13.9783) | Bit/dim 4.4431(4.4975) | Xent 1.6549(1.6784) | Loss 10.9121(11.1360) | Error 0.6200(0.6024) Steps 0(0.00) | Grad Norm 23.8288(19.2090) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 13.3747(13.9793) | Bit/dim 4.4149(4.4815) | Xent 1.6372(1.6648) | Loss 10.6775(11.0355) | Error 0.5967(0.5982) Steps 0(0.00) | Grad Norm 20.6693(17.8877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 74.6589, Epoch Time 866.2959(711.8815), Bit/dim 4.4291(best: 4.4927), Xent 1.5345, Loss 5.1964, Error 0.5521(best: 0.5751)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 13.7281(13.9883) | Bit/dim 4.4522(4.4672) | Xent 1.5956(1.6484) | Loss 10.8855(11.3675) | Error 0.5811(0.5915) Steps 0(0.00) | Grad Norm 12.1726(17.7698) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.4480(14.0867) | Bit/dim 4.5857(4.4672) | Xent 1.7299(1.6537) | Loss 10.9533(11.2435) | Error 0.6389(0.5949) Steps 0(0.00) | Grad Norm 32.7487(20.5960) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 14.0615(14.1501) | Bit/dim 4.4341(4.4681) | Xent 1.6095(1.6460) | Loss 10.7188(11.1405) | Error 0.6067(0.5929) Steps 0(0.00) | Grad Norm 12.2621(19.4899) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 14.1507(14.0583) | Bit/dim 4.4109(4.4553) | Xent 1.6260(1.6460) | Loss 10.7163(11.0345) | Error 0.5900(0.5945) Steps 0(0.00) | Grad Norm 14.1559(18.3211) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 14.6865(14.1117) | Bit/dim 4.4158(4.4395) | Xent 1.6050(1.6400) | Loss 10.7410(10.9569) | Error 0.5811(0.5922) Steps 0(0.00) | Grad Norm 17.1041(17.2344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 76.9494, Epoch Time 877.2224(716.8418), Bit/dim 4.4044(best: 4.4291), Xent 1.5321, Loss 5.1705, Error 0.5493(best: 0.5521)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 14.5008(14.1773) | Bit/dim 4.3860(4.4232) | Xent 1.5509(1.6278) | Loss 10.6066(11.3600) | Error 0.5489(0.5869) Steps 0(0.00) | Grad Norm 16.7782(17.4041) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 15.2527(14.2065) | Bit/dim 4.3757(4.4153) | Xent 1.5846(1.6262) | Loss 10.7185(11.1904) | Error 0.5844(0.5855) Steps 0(0.00) | Grad Norm 13.6129(19.1106) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 13.6040(14.1672) | Bit/dim 4.3648(4.4012) | Xent 1.6424(1.6281) | Loss 10.6799(11.0519) | Error 0.5733(0.5853) Steps 0(0.00) | Grad Norm 25.0850(19.9484) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 12.8160(14.1232) | Bit/dim 4.3368(4.3828) | Xent 1.6118(1.6200) | Loss 10.4689(10.9266) | Error 0.5756(0.5848) Steps 0(0.00) | Grad Norm 15.8309(18.4969) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 13.7233(14.0710) | Bit/dim 4.3430(4.3740) | Xent 1.5644(1.6061) | Loss 10.5571(10.8288) | Error 0.5589(0.5801) Steps 0(0.00) | Grad Norm 9.4964(17.3081) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 13.8643(14.0273) | Bit/dim 4.3519(4.3638) | Xent 1.5896(1.5992) | Loss 10.5182(10.7486) | Error 0.5822(0.5784) Steps 0(0.00) | Grad Norm 21.9509(16.7902) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 76.1948, Epoch Time 865.8099(721.3108), Bit/dim 4.3345(best: 4.4044), Xent 1.5373, Loss 5.1032, Error 0.5496(best: 0.5493)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 14.3391(14.0063) | Bit/dim 4.3100(4.3528) | Xent 1.6385(1.5910) | Loss 10.5688(11.1061) | Error 0.5856(0.5741) Steps 0(0.00) | Grad Norm 23.9274(16.9700) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.3093(14.0400) | Bit/dim 4.2948(4.3355) | Xent 1.5308(1.5778) | Loss 10.4241(10.9244) | Error 0.5667(0.5694) Steps 0(0.00) | Grad Norm 10.0991(15.2871) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 13.9226(14.0388) | Bit/dim 4.2562(4.3241) | Xent 1.5405(1.5653) | Loss 10.2945(10.7978) | Error 0.5578(0.5656) Steps 0(0.00) | Grad Norm 17.6757(14.6488) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 14.6221(14.1393) | Bit/dim 4.3486(4.3156) | Xent 1.7169(1.5636) | Loss 10.5972(10.7043) | Error 0.6033(0.5653) Steps 0(0.00) | Grad Norm 47.1773(15.5034) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 13.3884(14.1231) | Bit/dim 4.2972(4.3240) | Xent 1.5790(1.5897) | Loss 10.3792(10.6886) | Error 0.5644(0.5752) Steps 0(0.00) | Grad Norm 7.0611(16.9556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 77.6121, Epoch Time 871.3499(725.8120), Bit/dim 4.2778(best: 4.3345), Xent 1.4710, Loss 5.0134, Error 0.5317(best: 0.5493)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 13.4069(14.0982) | Bit/dim 4.2531(4.3124) | Xent 1.5175(1.5803) | Loss 10.3357(11.1066) | Error 0.5556(0.5724) Steps 0(0.00) | Grad Norm 7.1460(15.0266) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 14.1402(14.0647) | Bit/dim 4.2476(4.2966) | Xent 1.5303(1.5619) | Loss 10.3919(10.9088) | Error 0.5733(0.5669) Steps 0(0.00) | Grad Norm 9.6744(13.1366) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 14.0435(14.0741) | Bit/dim 4.2025(4.2795) | Xent 1.5041(1.5477) | Loss 10.2002(10.7547) | Error 0.5644(0.5608) Steps 0(0.00) | Grad Norm 11.4317(12.4097) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 14.0007(14.0925) | Bit/dim 4.2388(4.2670) | Xent 1.5017(1.5338) | Loss 10.2427(10.6234) | Error 0.5589(0.5566) Steps 0(0.00) | Grad Norm 4.4203(11.6231) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 13.7543(14.1723) | Bit/dim 4.2084(4.2592) | Xent 1.6044(1.5391) | Loss 10.4744(10.5736) | Error 0.5600(0.5570) Steps 0(0.00) | Grad Norm 21.3874(14.0469) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 14.6894(14.1855) | Bit/dim 4.1866(4.2476) | Xent 1.4069(1.5239) | Loss 10.0495(10.4854) | Error 0.5233(0.5531) Steps 0(0.00) | Grad Norm 14.5846(14.4825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 76.8531, Epoch Time 873.2765(730.2359), Bit/dim 4.2036(best: 4.2778), Xent 1.3983, Loss 4.9027, Error 0.5135(best: 0.5317)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 14.7048(14.2447) | Bit/dim 4.1873(4.2337) | Xent 1.4480(1.5079) | Loss 10.3104(10.8211) | Error 0.5311(0.5456) Steps 0(0.00) | Grad Norm 5.5532(12.8304) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 14.7580(14.2761) | Bit/dim 4.1877(4.2249) | Xent 1.5294(1.5036) | Loss 10.1768(10.6625) | Error 0.5433(0.5439) Steps 0(0.00) | Grad Norm 21.0247(13.4197) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 13.2317(14.1978) | Bit/dim 4.2417(4.2347) | Xent 1.4513(1.5005) | Loss 10.3230(10.5865) | Error 0.5344(0.5432) Steps 0(0.00) | Grad Norm 24.4302(15.9937) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 13.7146(14.1618) | Bit/dim 4.2289(4.2296) | Xent 1.4239(1.4998) | Loss 10.2636(10.5093) | Error 0.5233(0.5420) Steps 0(0.00) | Grad Norm 6.6628(14.3426) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 14.3835(14.1291) | Bit/dim 4.1649(4.2197) | Xent 1.4312(1.4872) | Loss 10.1832(10.4174) | Error 0.5400(0.5382) Steps 0(0.00) | Grad Norm 7.3439(12.2855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 77.3586, Epoch Time 874.1676(734.5539), Bit/dim 4.1813(best: 4.2036), Xent 1.3675, Loss 4.8650, Error 0.5006(best: 0.5135)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 13.6005(14.1272) | Bit/dim 4.1632(4.2081) | Xent 1.4120(1.4727) | Loss 10.1193(10.8264) | Error 0.5033(0.5323) Steps 0(0.00) | Grad Norm 13.5921(11.4775) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.1544(14.0579) | Bit/dim 4.1453(4.1991) | Xent 1.4003(1.4584) | Loss 10.0102(10.6349) | Error 0.5178(0.5264) Steps 0(0.00) | Grad Norm 7.0015(10.9527) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.3169(14.1819) | Bit/dim 4.1683(4.1892) | Xent 1.4382(1.4559) | Loss 10.1788(10.5087) | Error 0.5089(0.5245) Steps 0(0.00) | Grad Norm 13.5365(12.6390) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.0964(14.3905) | Bit/dim 4.1514(4.1786) | Xent 1.4546(1.4593) | Loss 10.1384(10.4063) | Error 0.5156(0.5264) Steps 0(0.00) | Grad Norm 9.3795(12.4199) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.2416(14.5104) | Bit/dim 4.1196(4.1650) | Xent 1.4148(1.4521) | Loss 9.9754(10.3065) | Error 0.5267(0.5258) Steps 0(0.00) | Grad Norm 14.1398(12.8807) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.0008(14.6499) | Bit/dim 4.1719(4.1580) | Xent 1.4145(1.4486) | Loss 10.1168(10.2458) | Error 0.5122(0.5253) Steps 0(0.00) | Grad Norm 13.0917(12.8622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 77.9246, Epoch Time 898.6284(739.4761), Bit/dim 4.1280(best: 4.1813), Xent 1.3249, Loss 4.7905, Error 0.4759(best: 0.5006)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 14.6740(14.6933) | Bit/dim 4.1612(4.1532) | Xent 1.3739(1.4350) | Loss 10.0578(10.6243) | Error 0.4800(0.5204) Steps 0(0.00) | Grad Norm 8.5276(12.7548) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.0249(14.5879) | Bit/dim 4.1287(4.1452) | Xent 1.4168(1.4287) | Loss 9.9771(10.4568) | Error 0.5278(0.5176) Steps 0(0.00) | Grad Norm 19.8573(13.2621) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.8427(14.6623) | Bit/dim 4.1102(4.1380) | Xent 1.3251(1.4200) | Loss 10.0414(10.3323) | Error 0.4756(0.5135) Steps 0(0.00) | Grad Norm 8.5559(12.9855) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.1086(14.7557) | Bit/dim 4.1511(4.1322) | Xent 1.3754(1.4165) | Loss 9.9225(10.2511) | Error 0.5111(0.5144) Steps 0(0.00) | Grad Norm 13.8631(13.4308) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.7418(14.8827) | Bit/dim 4.1240(4.1258) | Xent 1.3877(1.4074) | Loss 10.1034(10.1928) | Error 0.5022(0.5117) Steps 0(0.00) | Grad Norm 9.6187(12.8407) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 75.3381, Epoch Time 907.6564(744.5215), Bit/dim 4.1444(best: 4.1280), Xent 1.3932, Loss 4.8410, Error 0.5040(best: 0.4759)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 15.4777(14.8441) | Bit/dim 4.1068(4.1239) | Xent 1.4762(1.4105) | Loss 10.0940(10.6731) | Error 0.5211(0.5121) Steps 0(0.00) | Grad Norm 17.6168(14.2030) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 13.9429(14.7711) | Bit/dim 4.1031(4.1204) | Xent 1.3994(1.4157) | Loss 9.8758(10.5024) | Error 0.5122(0.5149) Steps 0(0.00) | Grad Norm 11.1456(14.4178) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.0941(14.8164) | Bit/dim 4.1186(4.1179) | Xent 1.3385(1.4081) | Loss 9.9600(10.3731) | Error 0.4844(0.5115) Steps 0(0.00) | Grad Norm 5.5071(13.4178) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.7257(14.7638) | Bit/dim 4.1015(4.1100) | Xent 1.3434(1.4014) | Loss 9.9799(10.2511) | Error 0.5089(0.5085) Steps 0(0.00) | Grad Norm 14.6045(12.6772) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.2183(14.8329) | Bit/dim 4.0986(4.1011) | Xent 1.4564(1.3867) | Loss 10.0412(10.1484) | Error 0.5233(0.5027) Steps 0(0.00) | Grad Norm 7.0663(11.7510) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 14.8077(14.8989) | Bit/dim 4.1335(4.0987) | Xent 1.4024(1.3815) | Loss 10.1465(10.1012) | Error 0.5089(0.5004) Steps 0(0.00) | Grad Norm 25.8976(12.8471) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 75.7785, Epoch Time 910.6782(749.5062), Bit/dim 4.0961(best: 4.1280), Xent 1.3797, Loss 4.7859, Error 0.5051(best: 0.4759)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.0388(14.8714) | Bit/dim 4.0655(4.0926) | Xent 1.4086(1.3747) | Loss 9.8595(10.4743) | Error 0.5011(0.4964) Steps 0(0.00) | Grad Norm 11.3528(12.8075) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 14.3603(14.9365) | Bit/dim 4.0471(4.0824) | Xent 1.3736(1.3685) | Loss 9.8695(10.3009) | Error 0.5133(0.4948) Steps 0(0.00) | Grad Norm 9.2145(12.0955) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 14.1167(14.9297) | Bit/dim 4.0654(4.0810) | Xent 1.3606(1.3577) | Loss 9.8295(10.1904) | Error 0.4822(0.4899) Steps 0(0.00) | Grad Norm 4.9885(11.4290) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.1127(14.8694) | Bit/dim 4.0508(4.0730) | Xent 1.3164(1.3562) | Loss 9.7467(10.0927) | Error 0.4811(0.4906) Steps 0(0.00) | Grad Norm 7.2183(12.0417) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 13.9935(14.7575) | Bit/dim 4.0299(4.0690) | Xent 1.3583(1.3510) | Loss 9.8030(10.0181) | Error 0.4956(0.4917) Steps 0(0.00) | Grad Norm 5.5198(11.7570) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 74.4673, Epoch Time 904.9792(754.1704), Bit/dim 4.0433(best: 4.0961), Xent 1.2635, Loss 4.6750, Error 0.4563(best: 0.4759)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.4081(14.6965) | Bit/dim 4.0272(4.0588) | Xent 1.2349(1.3403) | Loss 9.6387(10.4374) | Error 0.4478(0.4880) Steps 0(0.00) | Grad Norm 11.0986(11.3842) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 15.0384(14.6873) | Bit/dim 4.0609(4.0628) | Xent 1.3253(1.3530) | Loss 9.7804(10.2875) | Error 0.4689(0.4911) Steps 0(0.00) | Grad Norm 14.9587(14.2211) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 14.3975(14.7104) | Bit/dim 4.0608(4.0632) | Xent 1.3201(1.3510) | Loss 9.8914(10.1730) | Error 0.4689(0.4905) Steps 0(0.00) | Grad Norm 7.8452(13.9562) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 13.8438(14.6900) | Bit/dim 4.0660(4.0584) | Xent 1.3344(1.3464) | Loss 9.8274(10.0681) | Error 0.5011(0.4895) Steps 0(0.00) | Grad Norm 8.2838(13.2491) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.0152(14.7130) | Bit/dim 4.0102(4.0527) | Xent 1.3191(1.3433) | Loss 9.7074(9.9839) | Error 0.4767(0.4879) Steps 0(0.00) | Grad Norm 19.5092(13.5519) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 15.0678(14.8052) | Bit/dim 4.0478(4.0505) | Xent 1.3074(1.3411) | Loss 9.6881(9.9287) | Error 0.4867(0.4855) Steps 0(0.00) | Grad Norm 10.7661(13.6965) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 76.0402, Epoch Time 906.0213(758.7259), Bit/dim 4.0294(best: 4.0433), Xent 1.2449, Loss 4.6519, Error 0.4579(best: 0.4563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 15.2258(14.8282) | Bit/dim 4.0304(4.0445) | Xent 1.2911(1.3290) | Loss 9.8679(10.3182) | Error 0.4633(0.4801) Steps 0(0.00) | Grad Norm 9.8396(12.7442) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.9508(14.7816) | Bit/dim 4.0478(4.0369) | Xent 1.3377(1.3217) | Loss 9.8470(10.1642) | Error 0.5056(0.4781) Steps 0(0.00) | Grad Norm 9.8630(12.3364) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 14.6052(14.7790) | Bit/dim 3.9758(4.0289) | Xent 1.2844(1.3144) | Loss 9.5731(10.0282) | Error 0.4522(0.4753) Steps 0(0.00) | Grad Norm 9.9725(11.8697) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 14.6770(14.7548) | Bit/dim 3.9990(4.0238) | Xent 1.3040(1.3081) | Loss 9.7080(9.9334) | Error 0.4911(0.4742) Steps 0(0.00) | Grad Norm 5.5381(10.4207) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.1129(14.7475) | Bit/dim 4.0246(4.0186) | Xent 1.3588(1.3001) | Loss 9.7629(9.8608) | Error 0.5011(0.4716) Steps 0(0.00) | Grad Norm 6.1703(9.3533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 75.5681, Epoch Time 904.7920(763.1079), Bit/dim 4.0004(best: 4.0294), Xent 1.2093, Loss 4.6051, Error 0.4436(best: 0.4563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.8303(14.7082) | Bit/dim 3.9411(4.0140) | Xent 1.2009(1.2869) | Loss 9.4751(10.2852) | Error 0.4356(0.4656) Steps 0(0.00) | Grad Norm 9.4982(9.0326) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.2614(14.7513) | Bit/dim 3.9972(4.0090) | Xent 1.2216(1.2827) | Loss 9.6886(10.1251) | Error 0.4244(0.4623) Steps 0(0.00) | Grad Norm 11.1403(9.6839) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 14.3819(14.7764) | Bit/dim 4.0327(4.0069) | Xent 1.2155(1.2778) | Loss 9.6161(10.0019) | Error 0.4389(0.4610) Steps 0(0.00) | Grad Norm 13.5823(10.2954) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 13.0914(14.6724) | Bit/dim 3.9921(4.0035) | Xent 1.3103(1.2739) | Loss 9.6461(9.9091) | Error 0.4556(0.4592) Steps 0(0.00) | Grad Norm 14.4575(10.8578) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 14.9679(14.6746) | Bit/dim 4.0004(4.0017) | Xent 1.3193(1.2884) | Loss 9.8100(9.8590) | Error 0.4733(0.4635) Steps 0(0.00) | Grad Norm 19.3739(12.5912) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.0032(14.6713) | Bit/dim 3.9647(3.9982) | Xent 1.3176(1.2944) | Loss 9.4962(9.8127) | Error 0.4744(0.4674) Steps 0(0.00) | Grad Norm 11.5705(12.4422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 76.3257, Epoch Time 901.1069(767.2479), Bit/dim 3.9792(best: 4.0004), Xent 1.2371, Loss 4.5978, Error 0.4499(best: 0.4436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 13.8778(14.6810) | Bit/dim 3.9858(3.9950) | Xent 1.2360(1.2873) | Loss 9.6549(10.2218) | Error 0.4444(0.4647) Steps 0(0.00) | Grad Norm 14.2934(12.3183) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 14.3229(14.7493) | Bit/dim 3.9611(3.9918) | Xent 1.2619(1.2756) | Loss 9.5348(10.0652) | Error 0.4544(0.4607) Steps 0(0.00) | Grad Norm 10.3047(11.7939) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 14.5339(14.6937) | Bit/dim 3.9809(3.9865) | Xent 1.2236(1.2677) | Loss 9.4943(9.9501) | Error 0.4500(0.4569) Steps 0(0.00) | Grad Norm 9.2833(11.3391) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 14.5834(14.7880) | Bit/dim 3.9647(3.9830) | Xent 1.2774(1.2659) | Loss 9.6909(9.8739) | Error 0.4667(0.4557) Steps 0(0.00) | Grad Norm 6.4516(11.4108) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 14.1908(14.7311) | Bit/dim 3.9806(3.9794) | Xent 1.2434(1.2601) | Loss 9.5046(9.8004) | Error 0.4367(0.4548) Steps 0(0.00) | Grad Norm 7.9049(10.7307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 76.1840, Epoch Time 905.0298(771.3813), Bit/dim 3.9679(best: 3.9792), Xent 1.1989, Loss 4.5674, Error 0.4370(best: 0.4436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.9225(14.6384) | Bit/dim 3.9786(3.9776) | Xent 1.2552(1.2565) | Loss 9.6203(10.2430) | Error 0.4544(0.4534) Steps 0(0.00) | Grad Norm 6.6940(10.8609) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 14.3409(14.6146) | Bit/dim 3.9796(3.9725) | Xent 1.2568(1.2505) | Loss 9.6331(10.0681) | Error 0.4467(0.4505) Steps 0(0.00) | Grad Norm 18.7378(10.9292) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.9294(14.6215) | Bit/dim 3.9814(3.9705) | Xent 1.1764(1.2476) | Loss 9.5593(9.9471) | Error 0.4433(0.4501) Steps 0(0.00) | Grad Norm 10.2060(11.6661) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.3024(14.6413) | Bit/dim 3.9329(3.9649) | Xent 1.3237(1.2407) | Loss 9.6907(9.8455) | Error 0.4800(0.4473) Steps 0(0.00) | Grad Norm 10.8088(11.6074) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.5405(14.6129) | Bit/dim 3.9614(3.9662) | Xent 1.2370(1.2450) | Loss 9.5824(9.7714) | Error 0.4500(0.4479) Steps 0(0.00) | Grad Norm 5.4796(11.5475) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 14.8227(14.5616) | Bit/dim 3.9732(3.9603) | Xent 1.3370(1.2509) | Loss 9.7472(9.7213) | Error 0.4756(0.4496) Steps 0(0.00) | Grad Norm 25.5169(12.1858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 77.5023, Epoch Time 895.2493(775.0974), Bit/dim 3.9523(best: 3.9679), Xent 1.2068, Loss 4.5557, Error 0.4401(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 14.7677(14.5678) | Bit/dim 3.9616(3.9599) | Xent 1.1754(1.2471) | Loss 9.5584(10.0922) | Error 0.4356(0.4500) Steps 0(0.00) | Grad Norm 14.3826(12.5015) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.6153(14.5586) | Bit/dim 3.9720(3.9587) | Xent 1.2965(1.2435) | Loss 9.6927(9.9491) | Error 0.4611(0.4478) Steps 0(0.00) | Grad Norm 19.9062(12.8855) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.5617(14.5468) | Bit/dim 3.9328(3.9555) | Xent 1.2526(1.2392) | Loss 9.4920(9.8218) | Error 0.4511(0.4461) Steps 0(0.00) | Grad Norm 20.5607(12.5616) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 13.6123(14.5486) | Bit/dim 3.9474(3.9534) | Xent 1.2224(1.2381) | Loss 9.3815(9.7473) | Error 0.4589(0.4467) Steps 0(0.00) | Grad Norm 8.8233(12.1863) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 13.5823(14.5373) | Bit/dim 3.9902(3.9500) | Xent 1.2660(1.2323) | Loss 9.7442(9.6865) | Error 0.4533(0.4453) Steps 0(0.00) | Grad Norm 16.3650(11.4907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 77.5082, Epoch Time 895.4041(778.7066), Bit/dim 3.9451(best: 3.9523), Xent 1.1433, Loss 4.5168, Error 0.4111(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.2076(14.5854) | Bit/dim 3.9568(3.9468) | Xent 1.2214(1.2294) | Loss 9.5979(10.1714) | Error 0.4311(0.4434) Steps 0(0.00) | Grad Norm 24.1581(11.5276) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 14.3094(14.5201) | Bit/dim 3.9594(3.9452) | Xent 1.1781(1.2260) | Loss 9.4630(9.9988) | Error 0.4122(0.4425) Steps 0(0.00) | Grad Norm 10.7384(11.9357) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 14.7522(14.5428) | Bit/dim 3.9382(3.9408) | Xent 1.1501(1.2182) | Loss 9.4571(9.8745) | Error 0.4156(0.4412) Steps 0(0.00) | Grad Norm 14.4059(11.9149) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 13.9313(14.5584) | Bit/dim 3.9332(3.9410) | Xent 1.2678(1.2257) | Loss 9.5552(9.8009) | Error 0.4667(0.4436) Steps 0(0.00) | Grad Norm 21.7313(13.4255) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 14.6942(14.5473) | Bit/dim 3.9269(3.9391) | Xent 1.1935(1.2215) | Loss 9.4284(9.7311) | Error 0.4278(0.4432) Steps 0(0.00) | Grad Norm 8.1733(12.7061) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 14.6648(14.5642) | Bit/dim 3.9291(3.9355) | Xent 1.1864(1.2138) | Loss 9.3266(9.6602) | Error 0.4422(0.4399) Steps 0(0.00) | Grad Norm 11.2666(11.5079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 78.0029, Epoch Time 896.8770(782.2517), Bit/dim 3.9211(best: 3.9451), Xent 1.1368, Loss 4.4895, Error 0.4070(best: 0.4111)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 14.6804(14.6136) | Bit/dim 3.9530(3.9369) | Xent 1.2196(1.2075) | Loss 9.5713(10.0614) | Error 0.4378(0.4368) Steps 0(0.00) | Grad Norm 9.8948(11.7202) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.1981(14.6320) | Bit/dim 3.9033(3.9299) | Xent 1.1817(1.2001) | Loss 9.5143(9.9031) | Error 0.4244(0.4329) Steps 0(0.00) | Grad Norm 6.1461(11.2636) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 14.6310(14.6121) | Bit/dim 3.8837(3.9250) | Xent 1.2260(1.2025) | Loss 9.4481(9.7957) | Error 0.4589(0.4331) Steps 0(0.00) | Grad Norm 13.8896(11.9080) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.1898(14.6309) | Bit/dim 3.8809(3.9227) | Xent 1.1830(1.1961) | Loss 9.3973(9.7029) | Error 0.4544(0.4320) Steps 0(0.00) | Grad Norm 9.1682(11.6475) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.6533(14.6565) | Bit/dim 3.9203(3.9206) | Xent 1.1640(1.1943) | Loss 9.4138(9.6429) | Error 0.4122(0.4310) Steps 0(0.00) | Grad Norm 10.6500(11.2071) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 78.4592, Epoch Time 904.5504(785.9207), Bit/dim 3.9290(best: 3.9211), Xent 1.1864, Loss 4.5222, Error 0.4252(best: 0.4070)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 14.1964(14.6299) | Bit/dim 3.9247(3.9213) | Xent 1.1803(1.1966) | Loss 9.4472(10.1123) | Error 0.4111(0.4308) Steps 0(0.00) | Grad Norm 16.6657(11.8333) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 14.5126(14.6126) | Bit/dim 3.9022(3.9200) | Xent 1.2038(1.1901) | Loss 9.5009(9.9436) | Error 0.4267(0.4264) Steps 0(0.00) | Grad Norm 21.2247(11.6512) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 14.1741(14.5725) | Bit/dim 3.9075(3.9175) | Xent 1.1712(1.1898) | Loss 9.3159(9.8142) | Error 0.4189(0.4264) Steps 0(0.00) | Grad Norm 20.0294(12.1127) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 15.0019(14.6581) | Bit/dim 3.9265(3.9155) | Xent 1.2133(1.1940) | Loss 9.4176(9.7221) | Error 0.4333(0.4291) Steps 0(0.00) | Grad Norm 5.7137(12.4369) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 14.6757(14.7717) | Bit/dim 3.8832(3.9120) | Xent 1.1821(1.1887) | Loss 9.3819(9.6424) | Error 0.4378(0.4273) Steps 0(0.00) | Grad Norm 6.3050(11.9864) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.3197(14.7215) | Bit/dim 3.8853(3.9095) | Xent 1.1983(1.1781) | Loss 9.4844(9.5753) | Error 0.4322(0.4236) Steps 0(0.00) | Grad Norm 7.9831(11.5958) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 77.9297, Epoch Time 903.0016(789.4331), Bit/dim 3.9042(best: 3.9211), Xent 1.1545, Loss 4.4815, Error 0.4168(best: 0.4070)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 15.0173(14.7509) | Bit/dim 3.9087(3.9092) | Xent 1.2144(1.1760) | Loss 9.5823(9.9657) | Error 0.4378(0.4215) Steps 0(0.00) | Grad Norm 8.6565(12.0535) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 14.7201(14.7254) | Bit/dim 3.8612(3.9031) | Xent 1.0865(1.1685) | Loss 9.1985(9.8007) | Error 0.3822(0.4183) Steps 0(0.00) | Grad Norm 11.8980(11.6233) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.3243(14.7142) | Bit/dim 3.8998(3.9028) | Xent 1.0970(1.1569) | Loss 9.1940(9.6781) | Error 0.3744(0.4141) Steps 0(0.00) | Grad Norm 7.1858(11.0729) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 14.3245(14.7415) | Bit/dim 3.8931(3.8991) | Xent 1.1441(1.1586) | Loss 9.3017(9.5988) | Error 0.4211(0.4154) Steps 0(0.00) | Grad Norm 10.2392(11.6371) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 13.7731(14.7528) | Bit/dim 3.9107(3.9012) | Xent 1.2435(1.1615) | Loss 9.4223(9.5508) | Error 0.4333(0.4151) Steps 0(0.00) | Grad Norm 11.5899(11.4008) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 77.7296, Epoch Time 907.6409(792.9793), Bit/dim 3.8878(best: 3.9042), Xent 1.0934, Loss 4.4345, Error 0.3932(best: 0.4070)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.8185(14.8105) | Bit/dim 3.8724(3.9010) | Xent 1.1171(1.1581) | Loss 9.3742(10.0235) | Error 0.3933(0.4151) Steps 0(0.00) | Grad Norm 13.3569(11.8013) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.2922(14.7385) | Bit/dim 3.8864(3.9006) | Xent 1.1760(1.1497) | Loss 9.3414(9.8398) | Error 0.4144(0.4125) Steps 0(0.00) | Grad Norm 8.7258(12.0319) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 14.3057(14.7617) | Bit/dim 3.8915(3.8983) | Xent 1.0973(1.1462) | Loss 9.3669(9.7080) | Error 0.4044(0.4100) Steps 0(0.00) | Grad Norm 11.4839(11.5258) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.3291(14.7810) | Bit/dim 3.8769(3.8955) | Xent 1.1630(1.1431) | Loss 9.4063(9.6231) | Error 0.4078(0.4097) Steps 0(0.00) | Grad Norm 10.8942(10.5141) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 14.7861(14.8645) | Bit/dim 3.8454(3.8903) | Xent 1.1210(1.1296) | Loss 9.3050(9.5441) | Error 0.3933(0.4054) Steps 0(0.00) | Grad Norm 11.5941(10.3139) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 14.6337(14.7666) | Bit/dim 3.8504(3.8894) | Xent 1.1453(1.1320) | Loss 9.3793(9.5032) | Error 0.3867(0.4055) Steps 0(0.00) | Grad Norm 9.4298(11.2065) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 77.1898, Epoch Time 908.4294(796.4428), Bit/dim 3.8952(best: 3.8878), Xent 1.0645, Loss 4.4275, Error 0.3813(best: 0.3932)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.7049(14.7500) | Bit/dim 3.8860(3.8866) | Xent 1.0647(1.1215) | Loss 9.3124(9.8924) | Error 0.3822(0.4022) Steps 0(0.00) | Grad Norm 7.0560(10.7394) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 16.1084(14.8166) | Bit/dim 3.8994(3.8832) | Xent 1.0660(1.1189) | Loss 9.4405(9.7470) | Error 0.3800(0.4011) Steps 0(0.00) | Grad Norm 8.7977(10.5706) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 14.7220(14.8087) | Bit/dim 3.9400(3.8850) | Xent 1.0797(1.1118) | Loss 9.3888(9.6345) | Error 0.4056(0.3983) Steps 0(0.00) | Grad Norm 12.5474(10.3998) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.5757(14.8017) | Bit/dim 3.8573(3.8818) | Xent 1.1669(1.1092) | Loss 9.2757(9.5433) | Error 0.4256(0.3969) Steps 0(0.00) | Grad Norm 10.3739(9.7561) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 14.3904(14.7219) | Bit/dim 3.9380(3.8846) | Xent 1.3470(1.1268) | Loss 9.6574(9.5045) | Error 0.4856(0.4032) Steps 0(0.00) | Grad Norm 35.9984(11.5369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 77.9927, Epoch Time 907.0945(799.7624), Bit/dim 3.8821(best: 3.8878), Xent 1.1344, Loss 4.4493, Error 0.4089(best: 0.3813)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.2622(14.7588) | Bit/dim 3.8863(3.8880) | Xent 1.1908(1.1539) | Loss 9.4458(10.0217) | Error 0.3989(0.4103) Steps 0(0.00) | Grad Norm 8.1666(12.1889) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.1433(14.8096) | Bit/dim 3.8823(3.8845) | Xent 1.0805(1.1452) | Loss 9.4608(9.8409) | Error 0.3533(0.4072) Steps 0(0.00) | Grad Norm 7.0922(11.4502) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 14.5613(14.7600) | Bit/dim 3.8804(3.8840) | Xent 1.0743(1.1262) | Loss 9.3964(9.6993) | Error 0.3700(0.4008) Steps 0(0.00) | Grad Norm 4.1358(10.8841) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 14.2350(14.7011) | Bit/dim 3.8542(3.8789) | Xent 1.0172(1.1122) | Loss 9.1220(9.5816) | Error 0.3611(0.3953) Steps 0(0.00) | Grad Norm 5.0728(10.1379) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 15.5048(14.8108) | Bit/dim 3.8884(3.8751) | Xent 1.1180(1.1102) | Loss 9.2295(9.4867) | Error 0.4122(0.3959) Steps 0(0.00) | Grad Norm 20.5245(10.5398) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 14.9639(14.8309) | Bit/dim 3.8787(3.8714) | Xent 1.1302(1.1096) | Loss 9.3155(9.4357) | Error 0.4133(0.3963) Steps 0(0.00) | Grad Norm 12.7817(10.5539) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 77.3008, Epoch Time 910.9637(803.0984), Bit/dim 3.8686(best: 3.8821), Xent 1.0111, Loss 4.3742, Error 0.3634(best: 0.3813)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.5671(14.7693) | Bit/dim 3.8846(3.8728) | Xent 1.0674(1.0970) | Loss 9.3308(9.8292) | Error 0.3833(0.3914) Steps 0(0.00) | Grad Norm 13.2777(10.7337) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 14.7441(14.7795) | Bit/dim 3.8781(3.8693) | Xent 1.1020(1.0931) | Loss 9.3124(9.6790) | Error 0.3922(0.3889) Steps 0(0.00) | Grad Norm 5.2742(10.5242) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 15.2228(14.7594) | Bit/dim 3.8550(3.8674) | Xent 1.1057(1.0879) | Loss 9.2389(9.5647) | Error 0.3911(0.3871) Steps 0(0.00) | Grad Norm 11.8144(10.7202) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.0173(14.8281) | Bit/dim 3.8521(3.8632) | Xent 1.0304(1.0875) | Loss 9.1408(9.4880) | Error 0.3500(0.3861) Steps 0(0.00) | Grad Norm 12.8844(10.8300) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 14.3539(14.7886) | Bit/dim 3.8448(3.8597) | Xent 1.0451(1.0817) | Loss 9.0580(9.4141) | Error 0.3700(0.3847) Steps 0(0.00) | Grad Norm 4.0869(10.9937) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 77.3873, Epoch Time 905.4469(806.1689), Bit/dim 3.8550(best: 3.8686), Xent 0.9947, Loss 4.3523, Error 0.3575(best: 0.3634)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.5925(14.7713) | Bit/dim 3.8393(3.8586) | Xent 1.0481(1.0735) | Loss 9.1724(9.8414) | Error 0.3678(0.3821) Steps 0(0.00) | Grad Norm 9.0434(10.7102) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 14.1968(14.7868) | Bit/dim 3.8739(3.8613) | Xent 1.0651(1.0666) | Loss 9.0849(9.6747) | Error 0.3533(0.3801) Steps 0(0.00) | Grad Norm 6.7266(10.2760) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 14.8999(14.7738) | Bit/dim 3.8658(3.8590) | Xent 0.9881(1.0555) | Loss 9.2633(9.5515) | Error 0.3578(0.3766) Steps 0(0.00) | Grad Norm 12.3458(10.0516) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 14.9779(14.9156) | Bit/dim 3.8650(3.8578) | Xent 1.0626(1.0534) | Loss 9.2001(9.4674) | Error 0.3856(0.3769) Steps 0(0.00) | Grad Norm 12.0779(10.0194) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.4566(14.9559) | Bit/dim 3.8477(3.8558) | Xent 1.0543(1.0527) | Loss 9.3084(9.3962) | Error 0.3967(0.3762) Steps 0(0.00) | Grad Norm 8.5993(10.2123) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.3167(14.9442) | Bit/dim 3.8213(3.8534) | Xent 1.0726(1.0648) | Loss 9.2789(9.3559) | Error 0.3789(0.3800) Steps 0(0.00) | Grad Norm 6.1102(11.6180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 77.0084, Epoch Time 918.5212(809.5394), Bit/dim 3.8549(best: 3.8550), Xent 1.0637, Loss 4.3867, Error 0.3766(best: 0.3575)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.1971(14.9039) | Bit/dim 3.8339(3.8537) | Xent 1.0255(1.0668) | Loss 9.1622(9.7469) | Error 0.3767(0.3815) Steps 0(0.00) | Grad Norm 5.0197(11.4356) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.9219(14.9239) | Bit/dim 3.8523(3.8533) | Xent 1.0521(1.0590) | Loss 9.2757(9.5947) | Error 0.3800(0.3788) Steps 0(0.00) | Grad Norm 13.8867(11.4806) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.0189(14.9704) | Bit/dim 3.8682(3.8521) | Xent 1.0189(1.0640) | Loss 9.2411(9.5028) | Error 0.3744(0.3800) Steps 0(0.00) | Grad Norm 8.1146(11.5441) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 13.9450(14.8474) | Bit/dim 3.8018(3.8504) | Xent 1.0857(1.0599) | Loss 9.0509(9.4139) | Error 0.4011(0.3789) Steps 0(0.00) | Grad Norm 5.0399(11.0855) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 14.5645(14.7556) | Bit/dim 3.8394(3.8474) | Xent 1.1251(1.0594) | Loss 9.3064(9.3596) | Error 0.4000(0.3791) Steps 0(0.00) | Grad Norm 15.6316(11.3552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 76.8333, Epoch Time 907.9090(812.4905), Bit/dim 3.8379(best: 3.8549), Xent 1.0082, Loss 4.3420, Error 0.3627(best: 0.3575)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 14.9771(14.8274) | Bit/dim 3.8598(3.8465) | Xent 0.9892(1.0522) | Loss 9.1993(9.8030) | Error 0.3644(0.3756) Steps 0(0.00) | Grad Norm 9.9011(11.0201) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.2874(14.7857) | Bit/dim 3.8642(3.8453) | Xent 1.0410(1.0455) | Loss 9.2661(9.6275) | Error 0.3689(0.3725) Steps 0(0.00) | Grad Norm 10.1326(11.1301) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 15.1130(14.8088) | Bit/dim 3.8554(3.8413) | Xent 1.0089(1.0483) | Loss 9.2399(9.5080) | Error 0.3800(0.3733) Steps 0(0.00) | Grad Norm 3.7983(10.9013) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 14.3190(14.7631) | Bit/dim 3.8493(3.8399) | Xent 1.0748(1.0418) | Loss 9.3021(9.4092) | Error 0.4011(0.3719) Steps 0(0.00) | Grad Norm 14.1358(10.7100) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.2884(14.8634) | Bit/dim 3.8122(3.8386) | Xent 1.0435(1.0459) | Loss 9.1010(9.3446) | Error 0.3733(0.3740) Steps 0(0.00) | Grad Norm 5.7348(10.7619) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 14.8047(14.8647) | Bit/dim 3.8469(3.8400) | Xent 1.0497(1.0428) | Loss 9.1706(9.2994) | Error 0.3989(0.3730) Steps 0(0.00) | Grad Norm 7.4761(10.2405) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 77.2247, Epoch Time 912.8318(815.5008), Bit/dim 3.8337(best: 3.8379), Xent 1.0207, Loss 4.3440, Error 0.3680(best: 0.3575)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 14.8317(14.8156) | Bit/dim 3.8228(3.8353) | Xent 0.9775(1.0336) | Loss 9.0477(9.6708) | Error 0.3544(0.3707) Steps 0(0.00) | Grad Norm 6.7336(9.9063) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 14.7567(14.7978) | Bit/dim 3.8263(3.8360) | Xent 1.0126(1.0249) | Loss 8.9936(9.5198) | Error 0.3556(0.3669) Steps 0(0.00) | Grad Norm 11.0102(10.0025) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 14.3416(14.8395) | Bit/dim 3.8183(3.8355) | Xent 1.0251(1.0188) | Loss 9.1765(9.4177) | Error 0.3711(0.3654) Steps 0(0.00) | Grad Norm 22.6196(10.5520) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 14.8043(14.9030) | Bit/dim 3.8436(3.8370) | Xent 1.0126(1.0226) | Loss 9.0764(9.3518) | Error 0.3556(0.3666) Steps 0(0.00) | Grad Norm 11.6790(10.9222) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.4335(14.9436) | Bit/dim 3.8275(3.8321) | Xent 1.0797(1.0178) | Loss 9.1012(9.2769) | Error 0.3956(0.3643) Steps 0(0.00) | Grad Norm 18.1995(10.7848) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 79.2181, Epoch Time 916.0017(818.5158), Bit/dim 3.8342(best: 3.8337), Xent 1.0041, Loss 4.3362, Error 0.3594(best: 0.3575)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 14.8402(14.9651) | Bit/dim 3.8189(3.8336) | Xent 1.0059(1.0217) | Loss 9.0610(9.7626) | Error 0.3522(0.3653) Steps 0(0.00) | Grad Norm 8.3323(11.4127) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.1744(14.9356) | Bit/dim 3.8442(3.8308) | Xent 1.0594(1.0176) | Loss 9.2353(9.5906) | Error 0.3733(0.3637) Steps 0(0.00) | Grad Norm 21.2849(11.1561) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 15.2371(15.0088) | Bit/dim 3.8431(3.8296) | Xent 1.0100(1.0155) | Loss 9.2370(9.4677) | Error 0.3633(0.3623) Steps 0(0.00) | Grad Norm 18.3005(11.1337) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.3992(15.0846) | Bit/dim 3.8428(3.8315) | Xent 1.0462(1.0110) | Loss 9.2241(9.3852) | Error 0.3911(0.3604) Steps 0(0.00) | Grad Norm 14.4450(10.8693) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 14.9824(15.1056) | Bit/dim 3.8079(3.8246) | Xent 0.9826(0.9999) | Loss 9.0938(9.3005) | Error 0.3489(0.3568) Steps 0(0.00) | Grad Norm 5.4901(9.7566) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.5734(15.2039) | Bit/dim 3.8310(3.8242) | Xent 1.0667(1.0056) | Loss 9.1689(9.2658) | Error 0.3967(0.3589) Steps 0(0.00) | Grad Norm 9.8760(10.2413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 79.5371, Epoch Time 933.0695(821.9524), Bit/dim 3.8200(best: 3.8337), Xent 0.9619, Loss 4.3009, Error 0.3398(best: 0.3575)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.2560(15.2745) | Bit/dim 3.8198(3.8248) | Xent 0.9493(0.9985) | Loss 9.0604(9.6736) | Error 0.3389(0.3583) Steps 0(0.00) | Grad Norm 7.1689(9.8875) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 14.5450(15.2380) | Bit/dim 3.8486(3.8247) | Xent 1.0489(0.9979) | Loss 9.2356(9.5238) | Error 0.3733(0.3568) Steps 0(0.00) | Grad Norm 11.7141(10.0744) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.3591(15.2709) | Bit/dim 3.8305(3.8221) | Xent 0.9895(0.9967) | Loss 9.0480(9.4174) | Error 0.3656(0.3555) Steps 0(0.00) | Grad Norm 11.0759(9.9647) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 14.7562(15.2402) | Bit/dim 3.8453(3.8195) | Xent 1.0763(1.0026) | Loss 9.2162(9.3283) | Error 0.3889(0.3578) Steps 0(0.00) | Grad Norm 9.6579(10.3911) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 15.3434(15.2679) | Bit/dim 3.8125(3.8214) | Xent 1.0796(1.0089) | Loss 9.2060(9.2723) | Error 0.3867(0.3584) Steps 0(0.00) | Grad Norm 15.0292(11.0966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 78.0928, Epoch Time 938.0344(825.4349), Bit/dim 3.8162(best: 3.8200), Xent 0.9754, Loss 4.3040, Error 0.3473(best: 0.3398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 16.1148(15.2889) | Bit/dim 3.8106(3.8219) | Xent 1.0475(1.0086) | Loss 9.2224(9.7775) | Error 0.3889(0.3583) Steps 0(0.00) | Grad Norm 15.4290(11.5872) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 14.7042(15.3135) | Bit/dim 3.8419(3.8225) | Xent 1.0098(1.0076) | Loss 9.2082(9.6021) | Error 0.3633(0.3596) Steps 0(0.00) | Grad Norm 8.4130(11.4215) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 14.4075(15.2532) | Bit/dim 3.8196(3.8226) | Xent 0.9569(0.9953) | Loss 9.0021(9.4694) | Error 0.3333(0.3551) Steps 0(0.00) | Grad Norm 6.7980(10.5069) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.6811(15.2577) | Bit/dim 3.7820(3.8182) | Xent 0.9758(0.9893) | Loss 8.9597(9.3547) | Error 0.3444(0.3527) Steps 0(0.00) | Grad Norm 12.7728(9.8889) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.4443(15.2449) | Bit/dim 3.7406(3.8169) | Xent 1.0646(0.9942) | Loss 8.9601(9.2851) | Error 0.3756(0.3537) Steps 0(0.00) | Grad Norm 11.8753(10.7085) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 15.3685(15.2997) | Bit/dim 3.8231(3.8173) | Xent 0.9814(0.9917) | Loss 9.0383(9.2359) | Error 0.3522(0.3539) Steps 0(0.00) | Grad Norm 11.8131(10.5462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 78.1670, Epoch Time 936.1336(828.7558), Bit/dim 3.8055(best: 3.8162), Xent 0.9381, Loss 4.2745, Error 0.3363(best: 0.3398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.7233(15.3234) | Bit/dim 3.8260(3.8147) | Xent 0.9578(0.9838) | Loss 8.9630(9.6121) | Error 0.3589(0.3503) Steps 0(0.00) | Grad Norm 17.4265(10.5064) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 15.0225(15.2998) | Bit/dim 3.7902(3.8129) | Xent 0.9095(0.9755) | Loss 8.9552(9.4566) | Error 0.3289(0.3480) Steps 0(0.00) | Grad Norm 14.6625(10.2246) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.5840(15.3916) | Bit/dim 3.8090(3.8110) | Xent 0.9483(0.9699) | Loss 8.9904(9.3489) | Error 0.3522(0.3454) Steps 0(0.00) | Grad Norm 8.5426(9.7219) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 15.9325(15.4738) | Bit/dim 3.8081(3.8101) | Xent 0.9069(0.9707) | Loss 9.1472(9.2766) | Error 0.3300(0.3447) Steps 0(0.00) | Grad Norm 14.4645(9.8157) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 14.8713(15.4557) | Bit/dim 3.7932(3.8096) | Xent 0.8680(0.9668) | Loss 8.9440(9.2056) | Error 0.3011(0.3432) Steps 0(0.00) | Grad Norm 9.8374(9.9287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 78.3931, Epoch Time 947.2369(832.3103), Bit/dim 3.8042(best: 3.8055), Xent 0.9428, Loss 4.2756, Error 0.3350(best: 0.3363)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.4023(15.4347) | Bit/dim 3.7818(3.8080) | Xent 0.9633(0.9638) | Loss 8.8854(9.6775) | Error 0.3378(0.3419) Steps 0(0.00) | Grad Norm 12.0924(10.0963) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.8831(15.4182) | Bit/dim 3.8379(3.8079) | Xent 0.9830(0.9646) | Loss 9.0423(9.4999) | Error 0.3378(0.3420) Steps 0(0.00) | Grad Norm 14.0944(10.7713) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.0875(15.3984) | Bit/dim 3.7853(3.8061) | Xent 0.9665(0.9696) | Loss 8.8932(9.3829) | Error 0.3522(0.3440) Steps 0(0.00) | Grad Norm 9.8594(10.9933) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 15.5623(15.3740) | Bit/dim 3.8311(3.8043) | Xent 0.9891(0.9645) | Loss 9.0749(9.2845) | Error 0.3600(0.3433) Steps 0(0.00) | Grad Norm 12.8600(10.3982) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.7538(15.3833) | Bit/dim 3.8002(3.8058) | Xent 1.1076(0.9752) | Loss 9.1850(9.2316) | Error 0.3889(0.3472) Steps 0(0.00) | Grad Norm 11.7278(10.6987) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.1722(15.3883) | Bit/dim 3.7758(3.8055) | Xent 0.9199(0.9749) | Loss 8.8977(9.1763) | Error 0.3378(0.3481) Steps 0(0.00) | Grad Norm 10.0227(10.7379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 80.0087, Epoch Time 941.9356(835.5990), Bit/dim 3.8107(best: 3.8042), Xent 0.9597, Loss 4.2906, Error 0.3427(best: 0.3350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.1373(15.3528) | Bit/dim 3.7620(3.8037) | Xent 0.8968(0.9675) | Loss 8.9304(9.6077) | Error 0.3067(0.3448) Steps 0(0.00) | Grad Norm 7.2242(10.6639) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 15.2059(15.3938) | Bit/dim 3.7971(3.8037) | Xent 0.9225(0.9590) | Loss 8.9454(9.4574) | Error 0.3211(0.3414) Steps 0(0.00) | Grad Norm 6.6088(9.7330) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.9751(15.4105) | Bit/dim 3.8082(3.8011) | Xent 0.9199(0.9535) | Loss 9.0373(9.3429) | Error 0.3444(0.3393) Steps 0(0.00) | Grad Norm 8.9162(9.3504) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 15.7145(15.4436) | Bit/dim 3.7776(3.8018) | Xent 0.9002(0.9459) | Loss 8.9882(9.2658) | Error 0.3200(0.3380) Steps 0(0.00) | Grad Norm 11.9150(9.0437) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 14.3214(15.4366) | Bit/dim 3.8477(3.8023) | Xent 0.9799(0.9502) | Loss 9.1348(9.2125) | Error 0.3400(0.3388) Steps 0(0.00) | Grad Norm 14.6187(10.1941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 77.8938, Epoch Time 947.2138(838.9475), Bit/dim 3.7967(best: 3.8042), Xent 0.9090, Loss 4.2511, Error 0.3211(best: 0.3350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.5295(15.4821) | Bit/dim 3.8252(3.7993) | Xent 0.9112(0.9464) | Loss 9.0494(9.6718) | Error 0.3167(0.3367) Steps 0(0.00) | Grad Norm 16.1672(10.2941) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.6711(15.5630) | Bit/dim 3.7976(3.7994) | Xent 0.8999(0.9423) | Loss 9.0164(9.4996) | Error 0.3011(0.3349) Steps 0(0.00) | Grad Norm 5.6255(10.4161) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.6681(15.6010) | Bit/dim 3.7977(3.7985) | Xent 0.9617(0.9462) | Loss 9.1025(9.3753) | Error 0.3456(0.3368) Steps 0(0.00) | Grad Norm 15.6511(11.5356) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.5882(15.5986) | Bit/dim 3.8097(3.8020) | Xent 1.0262(0.9748) | Loss 9.0268(9.3095) | Error 0.3778(0.3466) Steps 0(0.00) | Grad Norm 10.6670(12.8679) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.9977(15.6675) | Bit/dim 3.7669(3.8015) | Xent 0.9503(0.9740) | Loss 8.9911(9.2356) | Error 0.3589(0.3469) Steps 0(0.00) | Grad Norm 8.2167(12.4509) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.0324(15.6724) | Bit/dim 3.7662(3.7987) | Xent 0.9610(0.9690) | Loss 9.0229(9.1816) | Error 0.3267(0.3454) Steps 0(0.00) | Grad Norm 12.1067(11.3055) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 78.7928, Epoch Time 960.0261(842.5798), Bit/dim 3.7934(best: 3.7967), Xent 0.8947, Loss 4.2407, Error 0.3205(best: 0.3211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 16.3919(15.7217) | Bit/dim 3.7884(3.7973) | Xent 0.8946(0.9484) | Loss 8.9108(9.5808) | Error 0.3156(0.3376) Steps 0(0.00) | Grad Norm 9.4797(10.7832) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 16.4587(15.7904) | Bit/dim 3.8019(3.7933) | Xent 0.8953(0.9425) | Loss 8.9164(9.4226) | Error 0.3122(0.3349) Steps 0(0.00) | Grad Norm 10.0574(10.4997) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 15.1171(15.7778) | Bit/dim 3.8134(3.7937) | Xent 0.9439(0.9469) | Loss 8.9980(9.3115) | Error 0.3400(0.3364) Steps 0(0.00) | Grad Norm 13.5273(11.4138) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.7716(15.7334) | Bit/dim 3.7965(3.7964) | Xent 1.0175(0.9561) | Loss 9.1511(9.2455) | Error 0.3722(0.3393) Steps 0(0.00) | Grad Norm 10.0668(11.5174) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.0031(15.7168) | Bit/dim 3.7740(3.7956) | Xent 0.9008(0.9484) | Loss 9.0463(9.1768) | Error 0.3244(0.3363) Steps 0(0.00) | Grad Norm 8.8687(10.5863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 77.7493, Epoch Time 961.4191(846.1450), Bit/dim 3.7924(best: 3.7934), Xent 0.9011, Loss 4.2429, Error 0.3194(best: 0.3205)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 14.9541(15.6767) | Bit/dim 3.7692(3.7942) | Xent 0.9242(0.9377) | Loss 8.8786(9.6338) | Error 0.3167(0.3323) Steps 0(0.00) | Grad Norm 7.6852(9.7963) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.5374(15.6714) | Bit/dim 3.7774(3.7921) | Xent 0.8648(0.9312) | Loss 8.8096(9.4517) | Error 0.3000(0.3307) Steps 0(0.00) | Grad Norm 4.1712(9.5808) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.9494(15.6740) | Bit/dim 3.7787(3.7908) | Xent 0.9581(0.9310) | Loss 8.8202(9.3224) | Error 0.3356(0.3293) Steps 0(0.00) | Grad Norm 12.4982(10.8174) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.6718(15.6434) | Bit/dim 3.8014(3.7925) | Xent 0.9315(0.9301) | Loss 9.0076(9.2430) | Error 0.3411(0.3298) Steps 0(0.00) | Grad Norm 15.0785(10.6760) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 16.2266(15.7240) | Bit/dim 3.7732(3.7917) | Xent 0.9312(0.9313) | Loss 8.8971(9.1732) | Error 0.3367(0.3312) Steps 0(0.00) | Grad Norm 11.8169(11.2181) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.4283(15.6874) | Bit/dim 3.7719(3.7881) | Xent 0.8854(0.9292) | Loss 8.8570(9.1093) | Error 0.3267(0.3325) Steps 0(0.00) | Grad Norm 8.2713(10.7249) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 78.8830, Epoch Time 959.1290(849.5345), Bit/dim 3.7860(best: 3.7924), Xent 0.8792, Loss 4.2256, Error 0.3139(best: 0.3194)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.9232(15.7463) | Bit/dim 3.7698(3.7855) | Xent 0.9281(0.9270) | Loss 8.8957(9.4933) | Error 0.3433(0.3304) Steps 0(0.00) | Grad Norm 11.0549(10.2578) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 15.7135(15.8259) | Bit/dim 3.7978(3.7863) | Xent 1.0200(0.9404) | Loss 8.8846(9.3629) | Error 0.3667(0.3341) Steps 0(0.00) | Grad Norm 8.0195(10.4444) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.7615(15.8990) | Bit/dim 3.7614(3.7855) | Xent 0.9409(0.9388) | Loss 8.9481(9.2527) | Error 0.3389(0.3337) Steps 0(0.00) | Grad Norm 6.6022(9.7449) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.0124(15.8898) | Bit/dim 3.7861(3.7854) | Xent 0.8377(0.9243) | Loss 8.8468(9.1717) | Error 0.2811(0.3288) Steps 0(0.00) | Grad Norm 15.1124(10.3108) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 16.2852(15.9124) | Bit/dim 3.7986(3.7866) | Xent 0.9359(0.9262) | Loss 8.9782(9.1215) | Error 0.3367(0.3292) Steps 0(0.00) | Grad Norm 5.9006(10.2820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 79.5890, Epoch Time 976.6395(853.3477), Bit/dim 3.7859(best: 3.7860), Xent 0.9170, Loss 4.2444, Error 0.3291(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.7989(15.9122) | Bit/dim 3.7496(3.7847) | Xent 0.9014(0.9143) | Loss 8.7684(9.5869) | Error 0.3056(0.3243) Steps 0(0.00) | Grad Norm 7.4630(10.0920) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 16.4070(15.8922) | Bit/dim 3.7394(3.7827) | Xent 0.8149(0.9042) | Loss 8.7911(9.4023) | Error 0.2922(0.3210) Steps 0(0.00) | Grad Norm 12.7501(10.1641) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 16.1539(15.8934) | Bit/dim 3.7436(3.7823) | Xent 0.8082(0.8978) | Loss 8.9280(9.2750) | Error 0.2844(0.3193) Steps 0(0.00) | Grad Norm 7.5830(9.7733) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 16.3052(15.9401) | Bit/dim 3.7694(3.7807) | Xent 0.9211(0.9047) | Loss 8.9008(9.1822) | Error 0.3467(0.3222) Steps 0(0.00) | Grad Norm 11.3965(10.2158) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.7421(15.8600) | Bit/dim 3.7954(3.7814) | Xent 0.9383(0.9117) | Loss 9.0471(9.1347) | Error 0.3211(0.3235) Steps 0(0.00) | Grad Norm 8.1727(10.9920) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.4981(15.7486) | Bit/dim 3.7512(3.7804) | Xent 0.9560(0.9111) | Loss 8.8350(9.0780) | Error 0.3489(0.3240) Steps 0(0.00) | Grad Norm 16.5715(10.7559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 77.5824, Epoch Time 963.5640(856.6541), Bit/dim 3.7808(best: 3.7859), Xent 0.8962, Loss 4.2289, Error 0.3206(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.8261(15.6210) | Bit/dim 3.8034(3.7790) | Xent 0.8722(0.9096) | Loss 9.0079(9.4575) | Error 0.2944(0.3238) Steps 0(0.00) | Grad Norm 12.3281(10.5036) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.7482(15.4913) | Bit/dim 3.7831(3.7789) | Xent 0.9497(0.9041) | Loss 8.8339(9.3029) | Error 0.3411(0.3221) Steps 0(0.00) | Grad Norm 16.4591(10.5072) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.5058(15.4474) | Bit/dim 3.8222(3.7783) | Xent 0.9251(0.9097) | Loss 9.0070(9.2045) | Error 0.3278(0.3242) Steps 0(0.00) | Grad Norm 10.7840(10.5015) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.8828(15.5147) | Bit/dim 3.7821(3.7783) | Xent 0.8730(0.9078) | Loss 8.9274(9.1326) | Error 0.3056(0.3232) Steps 0(0.00) | Grad Norm 5.0518(9.7867) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 14.9555(15.5759) | Bit/dim 3.7783(3.7739) | Xent 0.9340(0.8999) | Loss 9.0322(9.0623) | Error 0.3322(0.3217) Steps 0(0.00) | Grad Norm 8.3484(9.3470) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 78.8151, Epoch Time 945.0967(859.3074), Bit/dim 3.7801(best: 3.7808), Xent 0.9049, Loss 4.2325, Error 0.3214(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.2751(15.5276) | Bit/dim 3.7725(3.7765) | Xent 0.8676(0.8941) | Loss 8.8537(9.5354) | Error 0.2989(0.3180) Steps 0(0.00) | Grad Norm 11.4356(9.6098) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 16.1857(15.4707) | Bit/dim 3.7753(3.7768) | Xent 0.8794(0.8862) | Loss 8.9869(9.3719) | Error 0.3222(0.3155) Steps 0(0.00) | Grad Norm 4.9150(9.2539) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.6472(15.4628) | Bit/dim 3.7547(3.7728) | Xent 0.9088(0.8860) | Loss 8.8924(9.2453) | Error 0.3211(0.3154) Steps 0(0.00) | Grad Norm 9.2415(9.9423) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 14.9380(15.5355) | Bit/dim 3.7776(3.7735) | Xent 0.9899(0.8987) | Loss 9.0091(9.1601) | Error 0.3511(0.3199) Steps 0(0.00) | Grad Norm 9.9051(10.4865) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 16.0597(15.5277) | Bit/dim 3.7717(3.7719) | Xent 0.8874(0.9006) | Loss 8.8131(9.0880) | Error 0.3256(0.3204) Steps 0(0.00) | Grad Norm 8.9541(10.2575) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 15.0251(15.5294) | Bit/dim 3.7697(3.7692) | Xent 0.8211(0.8912) | Loss 8.7869(9.0227) | Error 0.2800(0.3178) Steps 0(0.00) | Grad Norm 6.1032(9.6573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 80.1524, Epoch Time 950.5721(862.0454), Bit/dim 3.7730(best: 3.7801), Xent 0.8659, Loss 4.2060, Error 0.3037(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 15.7873(15.5195) | Bit/dim 3.7639(3.7723) | Xent 0.8133(0.8769) | Loss 8.7043(9.4114) | Error 0.2789(0.3133) Steps 0(0.00) | Grad Norm 5.3135(9.3847) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 16.0243(15.6079) | Bit/dim 3.7436(3.7717) | Xent 0.8191(0.8755) | Loss 8.7977(9.2704) | Error 0.2822(0.3123) Steps 0(0.00) | Grad Norm 6.7242(8.9105) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.3405(15.5014) | Bit/dim 3.7921(3.7686) | Xent 0.9086(0.8777) | Loss 8.8054(9.1464) | Error 0.3122(0.3121) Steps 0(0.00) | Grad Norm 18.1210(8.8658) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.6700(15.5868) | Bit/dim 3.7327(3.7639) | Xent 0.9120(0.8700) | Loss 8.7414(9.0573) | Error 0.3189(0.3099) Steps 0(0.00) | Grad Norm 8.5337(8.8694) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.7325(15.5631) | Bit/dim 3.7771(3.7628) | Xent 0.8720(0.8759) | Loss 8.9993(9.0150) | Error 0.3111(0.3119) Steps 0(0.00) | Grad Norm 11.5647(9.4308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 79.0078, Epoch Time 953.5310(864.7899), Bit/dim 3.7693(best: 3.7730), Xent 0.8735, Loss 4.2061, Error 0.3071(best: 0.3037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.5612(15.5834) | Bit/dim 3.7739(3.7636) | Xent 0.9902(0.8772) | Loss 8.9039(9.4911) | Error 0.3422(0.3118) Steps 0(0.00) | Grad Norm 21.9498(10.3975) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.9390(15.5918) | Bit/dim 3.7771(3.7644) | Xent 0.8166(0.8737) | Loss 8.8768(9.3254) | Error 0.3156(0.3112) Steps 0(0.00) | Grad Norm 6.6904(10.3208) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.3598(15.6525) | Bit/dim 3.7722(3.7647) | Xent 0.9268(0.8776) | Loss 8.9002(9.2163) | Error 0.3311(0.3128) Steps 0(0.00) | Grad Norm 10.2932(9.9711) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.4102(15.6264) | Bit/dim 3.7453(3.7624) | Xent 0.8896(0.8721) | Loss 8.8474(9.1161) | Error 0.3200(0.3118) Steps 0(0.00) | Grad Norm 10.4492(9.5792) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.4313(15.6075) | Bit/dim 3.8021(3.7644) | Xent 0.8994(0.8705) | Loss 8.9259(9.0454) | Error 0.3289(0.3098) Steps 0(0.00) | Grad Norm 15.4908(9.7350) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.9141(15.6299) | Bit/dim 3.7523(3.7603) | Xent 0.9405(0.8831) | Loss 8.8530(8.9863) | Error 0.3467(0.3148) Steps 0(0.00) | Grad Norm 10.7196(10.6814) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 76.5480, Epoch Time 954.7539(867.4889), Bit/dim 3.7656(best: 3.7693), Xent 0.8616, Loss 4.1964, Error 0.3036(best: 0.3037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.1102(15.6417) | Bit/dim 3.7542(3.7615) | Xent 0.8437(0.8780) | Loss 8.7326(9.3875) | Error 0.3011(0.3128) Steps 0(0.00) | Grad Norm 7.4698(10.9035) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.6583(15.6456) | Bit/dim 3.7670(3.7616) | Xent 0.8277(0.8780) | Loss 8.9206(9.2482) | Error 0.2967(0.3117) Steps 0(0.00) | Grad Norm 8.0612(10.6201) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.5675(15.6761) | Bit/dim 3.7514(3.7590) | Xent 0.8499(0.8696) | Loss 8.9364(9.1255) | Error 0.3156(0.3101) Steps 0(0.00) | Grad Norm 14.7237(10.4354) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.0122(15.7797) | Bit/dim 3.7522(3.7577) | Xent 0.9126(0.8636) | Loss 8.9844(9.0523) | Error 0.3133(0.3075) Steps 0(0.00) | Grad Norm 16.8991(10.3759) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.4723(15.7589) | Bit/dim 3.7763(3.7605) | Xent 0.8866(0.8711) | Loss 8.8865(9.0062) | Error 0.3100(0.3112) Steps 0(0.00) | Grad Norm 5.6712(10.5017) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 77.7711, Epoch Time 961.1820(870.2996), Bit/dim 3.7725(best: 3.7656), Xent 0.9757, Loss 4.2603, Error 0.3393(best: 0.3036)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 16.0100(15.7738) | Bit/dim 3.7502(3.7608) | Xent 0.9420(0.8821) | Loss 8.7944(9.4849) | Error 0.3444(0.3142) Steps 0(0.00) | Grad Norm 12.1238(11.7194) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.6649(15.7872) | Bit/dim 3.7583(3.7671) | Xent 0.8713(0.8761) | Loss 8.7345(9.3152) | Error 0.3122(0.3109) Steps 0(0.00) | Grad Norm 10.6530(11.5686) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.4947(15.7352) | Bit/dim 3.7433(3.7664) | Xent 0.8514(0.8697) | Loss 8.7779(9.1866) | Error 0.3100(0.3096) Steps 0(0.00) | Grad Norm 6.1521(10.7046) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 16.3146(15.7135) | Bit/dim 3.7835(3.7644) | Xent 0.9152(0.8665) | Loss 8.9679(9.0876) | Error 0.3356(0.3082) Steps 0(0.00) | Grad Norm 14.4373(10.6435) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.2674(15.7073) | Bit/dim 3.7065(3.7579) | Xent 0.8959(0.8642) | Loss 8.8252(9.0123) | Error 0.3211(0.3076) Steps 0(0.00) | Grad Norm 15.7099(10.7778) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 16.1492(15.7190) | Bit/dim 3.7488(3.7550) | Xent 0.8571(0.8644) | Loss 8.7223(8.9581) | Error 0.3100(0.3079) Steps 0(0.00) | Grad Norm 11.5063(10.8976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 78.3091, Epoch Time 962.0958(873.0535), Bit/dim 3.7518(best: 3.7656), Xent 0.8391, Loss 4.1714, Error 0.2982(best: 0.3036)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.6297(15.6627) | Bit/dim 3.7440(3.7525) | Xent 0.9274(0.8608) | Loss 8.9306(9.3585) | Error 0.3267(0.3076) Steps 0(0.00) | Grad Norm 11.3054(11.0050) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.8505(15.6891) | Bit/dim 3.7443(3.7497) | Xent 0.8007(0.8544) | Loss 8.8086(9.2154) | Error 0.2789(0.3047) Steps 0(0.00) | Grad Norm 7.0140(10.5716) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.1932(15.6963) | Bit/dim 3.7897(3.7546) | Xent 0.8272(0.8461) | Loss 8.8286(9.1007) | Error 0.2822(0.3016) Steps 0(0.00) | Grad Norm 5.7004(9.5859) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 16.3743(15.7535) | Bit/dim 3.7437(3.7522) | Xent 0.8544(0.8399) | Loss 8.8166(9.0237) | Error 0.3133(0.3000) Steps 0(0.00) | Grad Norm 9.0832(9.1078) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.4848(15.7841) | Bit/dim 3.7274(3.7498) | Xent 0.8208(0.8372) | Loss 8.7918(8.9618) | Error 0.2822(0.2987) Steps 0(0.00) | Grad Norm 5.2429(9.1241) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 77.0728, Epoch Time 960.9215(875.6896), Bit/dim 3.7536(best: 3.7518), Xent 0.8845, Loss 4.1958, Error 0.3083(best: 0.2982)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 16.2091(15.7592) | Bit/dim 3.7354(3.7472) | Xent 0.8241(0.8381) | Loss 8.7378(9.4113) | Error 0.2778(0.2989) Steps 0(0.00) | Grad Norm 12.9606(9.4596) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.6567(15.8448) | Bit/dim 3.7282(3.7470) | Xent 0.8197(0.8366) | Loss 8.7787(9.2584) | Error 0.2967(0.2986) Steps 0(0.00) | Grad Norm 5.5398(8.9779) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.9328(15.9358) | Bit/dim 3.7374(3.7466) | Xent 0.8648(0.8377) | Loss 8.8794(9.1481) | Error 0.3033(0.2975) Steps 0(0.00) | Grad Norm 12.9274(9.5959) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.7691(15.9575) | Bit/dim 3.7431(3.7447) | Xent 0.8516(0.8403) | Loss 8.8475(9.0576) | Error 0.3000(0.2981) Steps 0(0.00) | Grad Norm 11.7608(9.9335) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.1803(15.9464) | Bit/dim 3.7105(3.7443) | Xent 0.9105(0.8415) | Loss 8.8277(8.9945) | Error 0.3300(0.2995) Steps 0(0.00) | Grad Norm 7.6593(9.5479) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.6119(15.8938) | Bit/dim 3.8005(3.7464) | Xent 0.8377(0.8429) | Loss 8.9098(8.9572) | Error 0.2978(0.3005) Steps 0(0.00) | Grad Norm 11.4641(9.8459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 78.3509, Epoch Time 974.6196(878.6575), Bit/dim 3.7553(best: 3.7518), Xent 0.9108, Loss 4.2107, Error 0.3219(best: 0.2982)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.8934(15.9027) | Bit/dim 3.7430(3.7495) | Xent 0.8599(0.8497) | Loss 8.8035(9.3688) | Error 0.3056(0.3036) Steps 0(0.00) | Grad Norm 11.5383(10.8174) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.9924(15.9134) | Bit/dim 3.7550(3.7493) | Xent 0.7848(0.8465) | Loss 8.8269(9.2209) | Error 0.2833(0.3013) Steps 0(0.00) | Grad Norm 4.5554(10.4668) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 16.0087(15.9293) | Bit/dim 3.7541(3.7465) | Xent 0.8538(0.8441) | Loss 8.7393(9.1055) | Error 0.3011(0.3012) Steps 0(0.00) | Grad Norm 5.9896(9.6040) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.7199(15.8998) | Bit/dim 3.7060(3.7445) | Xent 0.8666(0.8397) | Loss 8.7940(9.0210) | Error 0.3067(0.2992) Steps 0(0.00) | Grad Norm 7.6664(8.8230) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 16.9116(16.0207) | Bit/dim 3.7607(3.7436) | Xent 0.9284(0.8406) | Loss 8.8196(8.9661) | Error 0.3378(0.2992) Steps 0(0.00) | Grad Norm 12.4439(9.0056) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 80.4400, Epoch Time 978.8818(881.6642), Bit/dim 3.7368(best: 3.7518), Xent 0.8177, Loss 4.1457, Error 0.2856(best: 0.2982)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.8692(16.0250) | Bit/dim 3.7608(3.7461) | Xent 0.7927(0.8350) | Loss 8.7327(9.4526) | Error 0.2844(0.2978) Steps 0(0.00) | Grad Norm 4.7467(8.8199) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.9229(16.0631) | Bit/dim 3.7171(3.7449) | Xent 0.7739(0.8234) | Loss 8.7169(9.2613) | Error 0.2789(0.2932) Steps 0(0.00) | Grad Norm 5.8863(8.3914) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.5052(16.1076) | Bit/dim 3.7207(3.7423) | Xent 0.8137(0.8160) | Loss 8.6422(9.1256) | Error 0.2833(0.2911) Steps 0(0.00) | Grad Norm 10.2987(8.6028) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.2358(16.1129) | Bit/dim 3.7707(3.7403) | Xent 0.8675(0.8239) | Loss 8.8927(9.0326) | Error 0.3133(0.2958) Steps 0(0.00) | Grad Norm 11.0310(9.5472) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 16.4227(16.1083) | Bit/dim 3.7478(3.7433) | Xent 0.8172(0.8269) | Loss 8.7873(8.9748) | Error 0.2800(0.2952) Steps 0(0.00) | Grad Norm 9.8333(9.7588) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.8274(16.1231) | Bit/dim 3.7451(3.7420) | Xent 0.8321(0.8248) | Loss 8.8758(8.9267) | Error 0.3211(0.2960) Steps 0(0.00) | Grad Norm 7.9602(10.0727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 80.7327, Epoch Time 986.1384(884.7984), Bit/dim 3.7373(best: 3.7368), Xent 0.8036, Loss 4.1391, Error 0.2827(best: 0.2856)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 16.2656(16.1277) | Bit/dim 3.7585(3.7412) | Xent 0.8390(0.8125) | Loss 8.8285(9.3364) | Error 0.2933(0.2901) Steps 0(0.00) | Grad Norm 8.1195(9.3941) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.9881(16.2370) | Bit/dim 3.7712(3.7389) | Xent 0.8368(0.8129) | Loss 8.8698(9.1967) | Error 0.2767(0.2900) Steps 0(0.00) | Grad Norm 10.2079(9.4378) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.2815(16.2733) | Bit/dim 3.7675(3.7393) | Xent 0.8046(0.8124) | Loss 8.6486(9.0851) | Error 0.2889(0.2907) Steps 0(0.00) | Grad Norm 7.7626(10.0548) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.2231(16.2617) | Bit/dim 3.7577(3.7386) | Xent 0.8613(0.8120) | Loss 8.9082(9.0094) | Error 0.2944(0.2902) Steps 0(0.00) | Grad Norm 9.4381(9.7086) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 16.3787(16.3267) | Bit/dim 3.7326(3.7358) | Xent 0.7632(0.8112) | Loss 8.7199(8.9426) | Error 0.2789(0.2904) Steps 0(0.00) | Grad Norm 6.9111(9.6813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 78.8383, Epoch Time 994.9673(888.1035), Bit/dim 3.7427(best: 3.7368), Xent 0.8363, Loss 4.1608, Error 0.2929(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 17.5011(16.2655) | Bit/dim 3.6827(3.7383) | Xent 0.8151(0.8150) | Loss 8.6840(9.4069) | Error 0.2878(0.2907) Steps 0(0.00) | Grad Norm 8.1204(10.9246) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.7842(16.3923) | Bit/dim 3.7269(3.7387) | Xent 0.7766(0.8123) | Loss 8.7467(9.2452) | Error 0.2833(0.2897) Steps 0(0.00) | Grad Norm 6.6135(10.2262) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 17.0868(16.4326) | Bit/dim 3.7458(3.7359) | Xent 0.8451(0.8122) | Loss 8.8059(9.1138) | Error 0.2856(0.2882) Steps 0(0.00) | Grad Norm 10.2624(9.9464) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 16.0003(16.4165) | Bit/dim 3.7154(3.7348) | Xent 0.8176(0.8186) | Loss 8.6894(9.0258) | Error 0.3100(0.2916) Steps 0(0.00) | Grad Norm 8.1928(9.8495) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 16.8737(16.4392) | Bit/dim 3.7549(3.7372) | Xent 0.7271(0.8063) | Loss 8.5998(8.9427) | Error 0.2822(0.2880) Steps 0(0.00) | Grad Norm 7.1819(9.5426) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.9395(16.3832) | Bit/dim 3.7417(3.7354) | Xent 0.7361(0.8036) | Loss 8.7320(8.8928) | Error 0.2567(0.2872) Steps 0(0.00) | Grad Norm 9.7878(9.3144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 81.7386, Epoch Time 1004.0452(891.5817), Bit/dim 3.7310(best: 3.7368), Xent 0.8020, Loss 4.1320, Error 0.2852(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 17.2543(16.4754) | Bit/dim 3.7291(3.7305) | Xent 0.7413(0.7949) | Loss 8.6988(9.2792) | Error 0.2778(0.2841) Steps 0(0.00) | Grad Norm 8.5493(8.5161) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.5983(16.5466) | Bit/dim 3.7359(3.7312) | Xent 0.7392(0.7873) | Loss 8.8226(9.1399) | Error 0.2822(0.2822) Steps 0(0.00) | Grad Norm 6.0541(8.3485) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.1130(16.4956) | Bit/dim 3.7283(3.7309) | Xent 0.9369(0.8099) | Loss 8.8527(9.0414) | Error 0.3278(0.2899) Steps 0(0.00) | Grad Norm 10.8145(9.4563) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.7583(16.5210) | Bit/dim 3.7213(3.7331) | Xent 0.8256(0.8121) | Loss 8.8392(8.9769) | Error 0.2989(0.2902) Steps 0(0.00) | Grad Norm 7.5095(9.3360) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 17.4408(16.6436) | Bit/dim 3.7482(3.7354) | Xent 0.8396(0.8169) | Loss 8.8505(8.9473) | Error 0.2956(0.2920) Steps 0(0.00) | Grad Norm 19.0055(9.5352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 82.2797, Epoch Time 1019.2266(895.4111), Bit/dim 3.7430(best: 3.7310), Xent 0.8909, Loss 4.1885, Error 0.3122(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 17.2847(16.7271) | Bit/dim 3.7000(3.7372) | Xent 0.7609(0.8140) | Loss 8.6976(9.4562) | Error 0.2778(0.2912) Steps 0(0.00) | Grad Norm 6.4582(9.6383) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 17.3394(16.7323) | Bit/dim 3.7417(3.7378) | Xent 0.7910(0.7995) | Loss 8.7933(9.2720) | Error 0.2956(0.2853) Steps 0(0.00) | Grad Norm 7.8588(9.1213) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 16.9025(16.8094) | Bit/dim 3.7537(3.7343) | Xent 0.8151(0.7969) | Loss 8.8140(9.1331) | Error 0.2833(0.2844) Steps 0(0.00) | Grad Norm 8.5102(8.6133) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 16.0977(16.8182) | Bit/dim 3.7393(3.7328) | Xent 0.8082(0.7928) | Loss 8.8402(9.0285) | Error 0.2711(0.2830) Steps 0(0.00) | Grad Norm 10.8610(8.6173) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.8541(16.7842) | Bit/dim 3.7285(3.7325) | Xent 0.8605(0.7967) | Loss 8.9069(8.9561) | Error 0.3078(0.2846) Steps 0(0.00) | Grad Norm 8.1656(8.8850) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 17.3072(16.8622) | Bit/dim 3.7435(3.7326) | Xent 0.9005(0.8063) | Loss 8.9340(8.9117) | Error 0.3144(0.2882) Steps 0(0.00) | Grad Norm 8.8167(9.5219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 83.0976, Epoch Time 1029.4710(899.4329), Bit/dim 3.7324(best: 3.7310), Xent 0.8622, Loss 4.1635, Error 0.3030(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.4033(16.8660) | Bit/dim 3.7375(3.7334) | Xent 0.7346(0.8001) | Loss 8.6786(9.3185) | Error 0.2522(0.2866) Steps 0(0.00) | Grad Norm 9.5674(9.6630) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 16.5366(16.8431) | Bit/dim 3.7482(3.7338) | Xent 0.9086(0.7944) | Loss 8.7464(9.1628) | Error 0.3156(0.2844) Steps 0(0.00) | Grad Norm 16.0245(9.9889) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 16.9419(16.9267) | Bit/dim 3.7428(3.7353) | Xent 0.8287(0.7987) | Loss 8.7363(9.0657) | Error 0.2856(0.2839) Steps 0(0.00) | Grad Norm 11.1604(10.1073) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.3985(16.8591) | Bit/dim 3.7095(3.7343) | Xent 0.8041(0.7949) | Loss 8.7720(8.9845) | Error 0.2989(0.2833) Steps 0(0.00) | Grad Norm 9.8595(10.0200) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 17.2856(16.9009) | Bit/dim 3.7201(3.7315) | Xent 0.7784(0.7917) | Loss 8.7751(8.9239) | Error 0.2833(0.2814) Steps 0(0.00) | Grad Norm 8.3981(9.7874) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 83.5201, Epoch Time 1032.0353(903.4110), Bit/dim 3.7255(best: 3.7310), Xent 0.8005, Loss 4.1258, Error 0.2783(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 17.0409(17.0070) | Bit/dim 3.7554(3.7320) | Xent 0.7847(0.7865) | Loss 8.8485(9.4495) | Error 0.2867(0.2793) Steps 0(0.00) | Grad Norm 12.3775(9.4683) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 16.7871(17.0239) | Bit/dim 3.7241(3.7309) | Xent 0.7438(0.7857) | Loss 8.6594(9.2777) | Error 0.2733(0.2807) Steps 0(0.00) | Grad Norm 7.6504(9.2824) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.7215(17.0255) | Bit/dim 3.7427(3.7292) | Xent 0.7473(0.7835) | Loss 8.8152(9.1437) | Error 0.2722(0.2804) Steps 0(0.00) | Grad Norm 9.4903(9.0627) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.9249(17.0518) | Bit/dim 3.7411(3.7286) | Xent 0.7916(0.7800) | Loss 8.8269(9.0378) | Error 0.2833(0.2790) Steps 0(0.00) | Grad Norm 9.1480(8.3477) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 16.6815(17.0041) | Bit/dim 3.7226(3.7280) | Xent 0.7822(0.7793) | Loss 8.7910(8.9689) | Error 0.2833(0.2783) Steps 0(0.00) | Grad Norm 8.0568(8.5915) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 17.2256(16.9560) | Bit/dim 3.7541(3.7286) | Xent 0.8190(0.7846) | Loss 8.9227(8.9342) | Error 0.2844(0.2804) Steps 0(0.00) | Grad Norm 16.3023(9.4788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 83.5143, Epoch Time 1037.4805(907.4330), Bit/dim 3.7264(best: 3.7255), Xent 0.7792, Loss 4.1160, Error 0.2749(best: 0.2783)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.6422(16.9432) | Bit/dim 3.6781(3.7293) | Xent 0.7364(0.7794) | Loss 8.6869(9.3322) | Error 0.2600(0.2783) Steps 0(0.00) | Grad Norm 8.7181(9.3028) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.8250(16.9614) | Bit/dim 3.6845(3.7279) | Xent 0.7062(0.7690) | Loss 8.7084(9.1650) | Error 0.2389(0.2743) Steps 0(0.00) | Grad Norm 5.7922(8.6870) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 17.0266(16.9141) | Bit/dim 3.7469(3.7271) | Xent 0.8123(0.7681) | Loss 8.7959(9.0552) | Error 0.2733(0.2733) Steps 0(0.00) | Grad Norm 7.9421(8.4316) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 16.7804(16.9827) | Bit/dim 3.6991(3.7261) | Xent 0.8312(0.7795) | Loss 8.7658(8.9805) | Error 0.3089(0.2783) Steps 0(0.00) | Grad Norm 7.2583(8.7966) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 17.7986(16.9969) | Bit/dim 3.7313(3.7228) | Xent 0.7925(0.7806) | Loss 8.8591(8.9202) | Error 0.2911(0.2785) Steps 0(0.00) | Grad Norm 14.0363(8.9929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 83.0680, Epoch Time 1036.6174(911.3086), Bit/dim 3.7271(best: 3.7255), Xent 0.8002, Loss 4.1272, Error 0.2822(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 16.8190(17.0472) | Bit/dim 3.7567(3.7238) | Xent 0.7921(0.7805) | Loss 8.9062(9.4303) | Error 0.2867(0.2777) Steps 0(0.00) | Grad Norm 10.1341(9.3551) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 17.6384(17.0867) | Bit/dim 3.7283(3.7228) | Xent 0.7816(0.7796) | Loss 8.8986(9.2626) | Error 0.2744(0.2770) Steps 0(0.00) | Grad Norm 7.8737(9.3843) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.2999(17.0537) | Bit/dim 3.7225(3.7243) | Xent 0.7291(0.7757) | Loss 8.7206(9.1270) | Error 0.2711(0.2766) Steps 0(0.00) | Grad Norm 5.7158(9.5324) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 17.9643(17.1266) | Bit/dim 3.7334(3.7276) | Xent 0.7741(0.7739) | Loss 8.7638(9.0313) | Error 0.2811(0.2761) Steps 0(0.00) | Grad Norm 5.8962(9.0125) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.8997(17.1792) | Bit/dim 3.7267(3.7246) | Xent 0.7449(0.7744) | Loss 8.7376(8.9575) | Error 0.2689(0.2756) Steps 0(0.00) | Grad Norm 4.4119(8.6356) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 17.2531(17.1911) | Bit/dim 3.7042(3.7203) | Xent 0.8037(0.7762) | Loss 8.6424(8.8884) | Error 0.2889(0.2759) Steps 0(0.00) | Grad Norm 9.7514(9.1641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 84.9349, Epoch Time 1048.8530(915.4349), Bit/dim 3.7213(best: 3.7255), Xent 0.8135, Loss 4.1281, Error 0.2851(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 17.6545(17.2324) | Bit/dim 3.7221(3.7229) | Xent 0.7605(0.7718) | Loss 8.7940(9.3301) | Error 0.2744(0.2755) Steps 0(0.00) | Grad Norm 9.2644(9.1862) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 17.1061(17.2545) | Bit/dim 3.7134(3.7216) | Xent 0.6730(0.7602) | Loss 8.5654(9.1709) | Error 0.2378(0.2710) Steps 0(0.00) | Grad Norm 8.2872(8.8140) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 17.5813(17.2886) | Bit/dim 3.7374(3.7213) | Xent 0.6884(0.7581) | Loss 8.8160(9.0599) | Error 0.2378(0.2704) Steps 0(0.00) | Grad Norm 5.5126(8.6656) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 18.3995(17.3234) | Bit/dim 3.7025(3.7187) | Xent 0.7960(0.7614) | Loss 8.8368(8.9787) | Error 0.2800(0.2711) Steps 0(0.00) | Grad Norm 14.4891(9.1065) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 17.7806(17.3641) | Bit/dim 3.7487(3.7210) | Xent 0.7698(0.7671) | Loss 8.7440(8.9183) | Error 0.2644(0.2724) Steps 0(0.00) | Grad Norm 9.4890(10.0806) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 84.0207, Epoch Time 1057.7349(919.7039), Bit/dim 3.7202(best: 3.7213), Xent 0.7674, Loss 4.1039, Error 0.2712(best: 0.2749)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 18.8263(17.4421) | Bit/dim 3.7190(3.7237) | Xent 0.6618(0.7559) | Loss 8.7815(9.4137) | Error 0.2411(0.2683) Steps 0(0.00) | Grad Norm 5.8320(9.1362) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 17.6470(17.4756) | Bit/dim 3.7480(3.7219) | Xent 0.6949(0.7513) | Loss 8.8005(9.2379) | Error 0.2567(0.2663) Steps 0(0.00) | Grad Norm 8.4920(8.5876) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 17.7649(17.5012) | Bit/dim 3.6930(3.7193) | Xent 0.7802(0.7529) | Loss 8.8267(9.1082) | Error 0.2756(0.2656) Steps 0(0.00) | Grad Norm 9.3488(8.4971) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 18.6822(17.5642) | Bit/dim 3.7287(3.7197) | Xent 0.7774(0.7519) | Loss 8.8528(9.0187) | Error 0.2633(0.2671) Steps 0(0.00) | Grad Norm 13.3056(8.9435) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 17.5373(17.5331) | Bit/dim 3.7001(3.7192) | Xent 0.7486(0.7573) | Loss 8.6676(8.9439) | Error 0.2689(0.2703) Steps 0(0.00) | Grad Norm 8.0418(8.8461) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 17.3265(17.5319) | Bit/dim 3.7272(3.7199) | Xent 0.7470(0.7553) | Loss 8.6865(8.8864) | Error 0.2644(0.2689) Steps 0(0.00) | Grad Norm 9.2230(8.8127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 84.9802, Epoch Time 1070.7218(924.2344), Bit/dim 3.7238(best: 3.7202), Xent 0.7675, Loss 4.1076, Error 0.2666(best: 0.2712)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 17.5130(17.5397) | Bit/dim 3.7407(3.7186) | Xent 0.7859(0.7506) | Loss 8.9211(9.3211) | Error 0.2800(0.2665) Steps 0(0.00) | Grad Norm 13.6007(8.8384) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.7348(17.5538) | Bit/dim 3.7427(3.7210) | Xent 0.7725(0.7527) | Loss 8.7582(9.1809) | Error 0.2744(0.2681) Steps 0(0.00) | Grad Norm 12.3390(8.8508) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 17.3039(17.5591) | Bit/dim 3.7314(3.7220) | Xent 0.6774(0.7450) | Loss 8.7495(9.0614) | Error 0.2522(0.2647) Steps 0(0.00) | Grad Norm 5.8068(8.6850) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 17.5382(17.5450) | Bit/dim 3.6763(3.7195) | Xent 0.7588(0.7485) | Loss 8.7311(8.9660) | Error 0.2578(0.2654) Steps 0(0.00) | Grad Norm 14.6888(9.1888) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 17.3527(17.5305) | Bit/dim 3.7184(3.7198) | Xent 0.7341(0.7420) | Loss 8.8259(8.9025) | Error 0.2611(0.2634) Steps 0(0.00) | Grad Norm 8.3625(9.0429) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 84.7260, Epoch Time 1068.5838(928.5649), Bit/dim 3.7127(best: 3.7202), Xent 0.7710, Loss 4.0982, Error 0.2713(best: 0.2666)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 18.3324(17.5821) | Bit/dim 3.6962(3.7195) | Xent 0.7291(0.7410) | Loss 8.6372(9.4117) | Error 0.2511(0.2639) Steps 0(0.00) | Grad Norm 6.7706(8.8874) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 17.3461(17.6175) | Bit/dim 3.7235(3.7191) | Xent 0.7760(0.7384) | Loss 8.7362(9.2241) | Error 0.2767(0.2614) Steps 0(0.00) | Grad Norm 13.0665(9.0936) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 18.4211(17.6257) | Bit/dim 3.7045(3.7155) | Xent 0.8007(0.7344) | Loss 8.7385(9.0840) | Error 0.2878(0.2607) Steps 0(0.00) | Grad Norm 11.0335(9.1330) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 18.0936(17.6573) | Bit/dim 3.7357(3.7134) | Xent 0.7371(0.7406) | Loss 8.7591(8.9886) | Error 0.2567(0.2635) Steps 0(0.00) | Grad Norm 8.4360(8.8753) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 18.6533(17.7437) | Bit/dim 3.7446(3.7146) | Xent 0.7602(0.7444) | Loss 8.7373(8.9305) | Error 0.2744(0.2645) Steps 0(0.00) | Grad Norm 5.9166(8.6111) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 17.0625(17.7050) | Bit/dim 3.7814(3.7174) | Xent 0.8031(0.7533) | Loss 8.9528(8.8874) | Error 0.2722(0.2677) Steps 0(0.00) | Grad Norm 12.0366(9.3157) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 84.2777, Epoch Time 1077.2937(933.0268), Bit/dim 3.7215(best: 3.7127), Xent 0.7820, Loss 4.1125, Error 0.2744(best: 0.2666)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 16.9608(17.6813) | Bit/dim 3.6805(3.7142) | Xent 0.7909(0.7504) | Loss 8.6653(9.3152) | Error 0.2767(0.2653) Steps 0(0.00) | Grad Norm 7.3197(8.7397) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 17.3197(17.6690) | Bit/dim 3.6963(3.7129) | Xent 0.7666(0.7434) | Loss 8.6250(9.1476) | Error 0.2656(0.2626) Steps 0(0.00) | Grad Norm 12.0219(8.5568) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 18.4175(17.7480) | Bit/dim 3.7467(3.7137) | Xent 0.7086(0.7353) | Loss 8.7191(9.0340) | Error 0.2467(0.2601) Steps 0(0.00) | Grad Norm 6.0035(8.2896) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 18.7080(17.8076) | Bit/dim 3.7334(3.7140) | Xent 0.7225(0.7311) | Loss 8.8396(8.9553) | Error 0.2578(0.2602) Steps 0(0.00) | Grad Norm 6.5885(7.8222) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 18.0403(17.8587) | Bit/dim 3.7279(3.7142) | Xent 0.6862(0.7289) | Loss 8.6457(8.8853) | Error 0.2433(0.2591) Steps 0(0.00) | Grad Norm 8.2273(7.5369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 84.5778, Epoch Time 1082.8112(937.5203), Bit/dim 3.7162(best: 3.7127), Xent 0.7882, Loss 4.1103, Error 0.2764(best: 0.2666)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 17.4884(17.8340) | Bit/dim 3.7333(3.7132) | Xent 0.7374(0.7262) | Loss 8.8245(9.3901) | Error 0.2733(0.2592) Steps 0(0.00) | Grad Norm 12.9199(7.8246) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.2203(17.8343) | Bit/dim 3.7158(3.7136) | Xent 0.7619(0.7262) | Loss 8.6124(9.2071) | Error 0.2689(0.2590) Steps 0(0.00) | Grad Norm 10.2458(8.5218) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 18.6978(17.8844) | Bit/dim 3.7076(3.7139) | Xent 0.7844(0.7281) | Loss 8.8155(9.0789) | Error 0.2667(0.2592) Steps 0(0.00) | Grad Norm 10.3589(8.5755) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.8855(17.8937) | Bit/dim 3.6909(3.7136) | Xent 0.7193(0.7227) | Loss 8.6832(8.9817) | Error 0.2567(0.2572) Steps 0(0.00) | Grad Norm 11.9879(8.7905) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 18.0172(17.9277) | Bit/dim 3.6837(3.7128) | Xent 0.6882(0.7235) | Loss 8.6319(8.9087) | Error 0.2444(0.2562) Steps 0(0.00) | Grad Norm 9.7948(8.5841) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 18.2170(18.0179) | Bit/dim 3.7373(3.7122) | Xent 0.7002(0.7251) | Loss 8.8274(8.8628) | Error 0.2489(0.2572) Steps 0(0.00) | Grad Norm 6.9043(8.6452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 87.1558, Epoch Time 1095.4067(942.2569), Bit/dim 3.7083(best: 3.7127), Xent 0.7492, Loss 4.0830, Error 0.2610(best: 0.2666)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 18.7402(17.9619) | Bit/dim 3.7232(3.7124) | Xent 0.7528(0.7158) | Loss 8.8146(9.2839) | Error 0.2667(0.2550) Steps 0(0.00) | Grad Norm 5.0075(8.4740) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 18.7244(18.0151) | Bit/dim 3.6846(3.7113) | Xent 0.7163(0.7258) | Loss 8.4835(9.1272) | Error 0.2511(0.2585) Steps 0(0.00) | Grad Norm 7.8146(8.9932) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.4587(17.9830) | Bit/dim 3.7638(3.7133) | Xent 0.6645(0.7293) | Loss 8.6830(9.0169) | Error 0.2289(0.2594) Steps 0(0.00) | Grad Norm 6.8303(8.6780) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.8643(18.0061) | Bit/dim 3.7161(3.7108) | Xent 0.7017(0.7282) | Loss 8.6464(8.9239) | Error 0.2533(0.2590) Steps 0(0.00) | Grad Norm 7.9767(8.7596) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 17.6998(18.0501) | Bit/dim 3.7139(3.7118) | Xent 0.7428(0.7245) | Loss 8.7575(8.8613) | Error 0.2622(0.2572) Steps 0(0.00) | Grad Norm 11.6554(9.0931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 87.5160, Epoch Time 1096.4515(946.8828), Bit/dim 3.7067(best: 3.7083), Xent 0.7529, Loss 4.0832, Error 0.2627(best: 0.2610)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 18.1803(18.1157) | Bit/dim 3.6522(3.7081) | Xent 0.7159(0.7251) | Loss 8.6768(9.4026) | Error 0.2589(0.2575) Steps 0(0.00) | Grad Norm 9.7232(8.9191) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 18.0401(18.1637) | Bit/dim 3.7049(3.7093) | Xent 0.7462(0.7200) | Loss 8.7586(9.2250) | Error 0.2667(0.2558) Steps 0(0.00) | Grad Norm 6.0169(9.1024) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 18.0420(18.1741) | Bit/dim 3.7367(3.7096) | Xent 0.7364(0.7153) | Loss 8.6751(9.0837) | Error 0.2667(0.2540) Steps 0(0.00) | Grad Norm 9.3731(9.0690) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 17.5919(18.1410) | Bit/dim 3.7011(3.7078) | Xent 0.6784(0.7152) | Loss 8.6758(8.9654) | Error 0.2378(0.2549) Steps 0(0.00) | Grad Norm 6.2151(8.8974) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 18.9917(18.1945) | Bit/dim 3.7220(3.7069) | Xent 0.6467(0.7160) | Loss 8.7608(8.9054) | Error 0.2333(0.2543) Steps 0(0.00) | Grad Norm 8.3870(9.5090) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 18.2648(18.1402) | Bit/dim 3.7061(3.7062) | Xent 0.6475(0.7141) | Loss 8.6182(8.8345) | Error 0.2144(0.2543) Steps 0(0.00) | Grad Norm 4.9245(8.4873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 87.2632, Epoch Time 1106.0605(951.6581), Bit/dim 3.7046(best: 3.7067), Xent 0.7413, Loss 4.0753, Error 0.2585(best: 0.2610)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 18.2171(18.1061) | Bit/dim 3.6975(3.7034) | Xent 0.7105(0.7068) | Loss 8.6778(9.2694) | Error 0.2444(0.2526) Steps 0(0.00) | Grad Norm 8.8575(8.6045) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.3099(18.0909) | Bit/dim 3.6770(3.7033) | Xent 0.8157(0.7168) | Loss 8.7150(9.1276) | Error 0.3044(0.2572) Steps 0(0.00) | Grad Norm 14.0513(9.3775) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 18.6702(18.1565) | Bit/dim 3.7361(3.7073) | Xent 0.7733(0.7186) | Loss 8.8306(9.0225) | Error 0.2722(0.2565) Steps 0(0.00) | Grad Norm 6.6703(8.8856) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 18.6639(18.1902) | Bit/dim 3.6740(3.7051) | Xent 0.7166(0.7167) | Loss 8.6029(8.9301) | Error 0.2544(0.2567) Steps 0(0.00) | Grad Norm 5.5145(8.5609) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 19.0015(18.1856) | Bit/dim 3.6751(3.7019) | Xent 0.7132(0.7080) | Loss 8.6289(8.8546) | Error 0.2500(0.2537) Steps 0(0.00) | Grad Norm 5.8170(7.7866) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_30_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 30.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
