{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 252.9112(252.9112) | Bit/dim 8.9616(8.9616) | Xent 2.3026(2.3026) | Loss 20.9561(20.9561) | Error 0.8982(0.8982) Steps 0(0.00) | Grad Norm 28.2532(28.2532) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 210.3109(251.6332) | Bit/dim 8.8761(8.9590) | Xent 2.2924(2.3023) | Loss 20.6529(20.9470) | Error 0.7730(0.8945) Steps 0(0.00) | Grad Norm 25.4093(28.1678) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 224.9040(250.8313) | Bit/dim 8.7789(8.9536) | Xent 2.2788(2.3016) | Loss 20.7082(20.9399) | Error 0.7672(0.8907) Steps 0(0.00) | Grad Norm 21.5312(27.9687) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 198.0153(249.2468) | Bit/dim 8.6996(8.9460) | Xent 2.2610(2.3004) | Loss 20.3061(20.9209) | Error 0.7588(0.8867) Steps 0(0.00) | Grad Norm 16.2683(27.6177) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 200.0091(247.7697) | Bit/dim 8.6216(8.9363) | Xent 2.2400(2.2985) | Loss 20.2979(20.9022) | Error 0.7511(0.8826) Steps 0(0.00) | Grad Norm 11.4752(27.1335) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 211.1707(246.6717) | Bit/dim 8.5333(8.9242) | Xent 2.2217(2.2962) | Loss 19.9386(20.8733) | Error 0.7520(0.8787) Steps 0(0.00) | Grad Norm 8.5750(26.5767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 55.9001, Epoch Time 1369.6967(1369.6967), Bit/dim 8.4936(best: inf), Xent 2.1958, Loss 9.5916, Error 0.7388(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 230.8891(246.1982) | Bit/dim 8.5231(8.9121) | Xent 2.2011(2.2934) | Loss 22.4472(20.9205) | Error 0.7570(0.8751) Steps 0(0.00) | Grad Norm 8.6024(26.0375) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 227.2151(245.6287) | Bit/dim 8.4669(8.8988) | Xent 2.1832(2.2901) | Loss 19.9225(20.8905) | Error 0.7598(0.8716) Steps 0(0.00) | Grad Norm 11.1982(25.5923) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 213.8157(244.6743) | Bit/dim 8.4514(8.8854) | Xent 2.1681(2.2864) | Loss 19.7142(20.8553) | Error 0.7581(0.8682) Steps 0(0.00) | Grad Norm 13.7511(25.2371) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 216.9959(243.8440) | Bit/dim 8.4146(8.8712) | Xent 2.1518(2.2824) | Loss 19.6650(20.8196) | Error 0.7560(0.8648) Steps 0(0.00) | Grad Norm 14.8731(24.9261) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 223.6585(243.2384) | Bit/dim 8.3311(8.8550) | Xent 2.1540(2.2785) | Loss 19.6382(20.7841) | Error 0.7635(0.8618) Steps 0(0.00) | Grad Norm 14.9779(24.6277) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 220.1989(242.5472) | Bit/dim 8.2460(8.8368) | Xent 2.1293(2.2741) | Loss 19.4211(20.7432) | Error 0.7405(0.8582) Steps 0(0.00) | Grad Norm 12.5073(24.2641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 43.1945, Epoch Time 1392.3174(1370.3754), Bit/dim 8.1944(best: 8.4936), Xent 2.1075, Loss 9.2481, Error 0.7265(best: 0.7388)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 211.5783(241.6182) | Bit/dim 8.1803(8.8171) | Xent 2.1118(2.2692) | Loss 21.8390(20.7761) | Error 0.7345(0.8545) Steps 0(0.00) | Grad Norm 9.8964(23.8331) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 215.7090(240.8409) | Bit/dim 8.1294(8.7964) | Xent 2.0904(2.2638) | Loss 18.9320(20.7208) | Error 0.7224(0.8505) Steps 0(0.00) | Grad Norm 7.0907(23.3308) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 189.9493(239.3142) | Bit/dim 8.0852(8.7751) | Xent 2.0816(2.2584) | Loss 19.0535(20.6708) | Error 0.7235(0.8467) Steps 0(0.00) | Grad Norm 7.3451(22.8512) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 197.1589(238.0495) | Bit/dim 8.0128(8.7522) | Xent 2.0884(2.2533) | Loss 18.7147(20.6121) | Error 0.7326(0.8433) Steps 0(0.00) | Grad Norm 9.3106(22.4450) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 198.1292(236.8519) | Bit/dim 7.9705(8.7288) | Xent 2.0711(2.2478) | Loss 18.5258(20.5495) | Error 0.7336(0.8400) Steps 0(0.00) | Grad Norm 10.5102(22.0869) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 213.5660(236.1533) | Bit/dim 7.8682(8.7030) | Xent 2.0660(2.2423) | Loss 18.5052(20.4882) | Error 0.7300(0.8367) Steps 0(0.00) | Grad Norm 10.7675(21.7474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 43.3881, Epoch Time 1285.8156(1367.8386), Bit/dim 7.7676(best: 8.1944), Xent 2.0384, Loss 8.7868, Error 0.7053(best: 0.7265)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 223.1017(235.7618) | Bit/dim 7.7742(8.6751) | Xent 2.0435(2.2364) | Loss 20.8255(20.4983) | Error 0.7125(0.8329) Steps 0(0.00) | Grad Norm 9.1356(21.3690) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 212.3993(235.0609) | Bit/dim 7.6692(8.6449) | Xent 2.0391(2.2305) | Loss 18.2205(20.4299) | Error 0.7015(0.8290) Steps 0(0.00) | Grad Norm 6.4041(20.9201) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 214.5041(234.4442) | Bit/dim 7.5908(8.6133) | Xent 2.0454(2.2249) | Loss 17.9412(20.3553) | Error 0.6911(0.8249) Steps 0(0.00) | Grad Norm 5.2455(20.4498) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 209.3858(233.6924) | Bit/dim 7.5047(8.5800) | Xent 2.0314(2.2191) | Loss 17.6970(20.2755) | Error 0.6949(0.8210) Steps 0(0.00) | Grad Norm 7.0475(20.0478) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 221.9764(233.3410) | Bit/dim 7.4418(8.5459) | Xent 2.0294(2.2134) | Loss 17.5569(20.1940) | Error 0.7070(0.8176) Steps 0(0.00) | Grad Norm 9.4094(19.7286) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 245.4468(233.7041) | Bit/dim 7.3773(8.5108) | Xent 2.0434(2.2083) | Loss 17.4581(20.1119) | Error 0.7109(0.8144) Steps 0(0.00) | Grad Norm 9.2218(19.4134) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 46.1076, Epoch Time 1389.2032(1368.4795), Bit/dim 7.3087(best: 7.7676), Xent 2.0169, Loss 8.3171, Error 0.6776(best: 0.7053)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 246.2565(234.0807) | Bit/dim 7.3084(8.4748) | Xent 2.0255(2.2028) | Loss 20.0473(20.1100) | Error 0.6874(0.8105) Steps 0(0.00) | Grad Norm 5.9811(19.0104) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 219.3608(233.6391) | Bit/dim 7.2494(8.4380) | Xent 2.0237(2.1975) | Loss 17.2406(20.0239) | Error 0.6787(0.8066) Steps 0(0.00) | Grad Norm 4.4135(18.5725) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 214.3593(233.0607) | Bit/dim 7.2064(8.4010) | Xent 2.0255(2.1923) | Loss 17.0301(19.9341) | Error 0.6864(0.8030) Steps 0(0.00) | Grad Norm 7.1997(18.2313) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 244.5060(233.4041) | Bit/dim 7.1630(8.3639) | Xent 2.0375(2.1876) | Loss 17.1247(19.8498) | Error 0.6847(0.7994) Steps 0(0.00) | Grad Norm 7.1359(17.8985) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 226.1412(233.1862) | Bit/dim 7.1294(8.3269) | Xent 2.0310(2.1829) | Loss 16.9566(19.7630) | Error 0.6854(0.7960) Steps 0(0.00) | Grad Norm 4.6204(17.5001) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 211.0123(232.5210) | Bit/dim 7.1071(8.2903) | Xent 2.0350(2.1785) | Loss 16.9458(19.6785) | Error 0.6870(0.7927) Steps 0(0.00) | Grad Norm 4.4153(17.1076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 47.7388, Epoch Time 1424.9670(1370.1741), Bit/dim 7.0833(best: 7.3087), Xent 2.0345, Loss 8.1005, Error 0.6929(best: 0.6776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 218.0409(232.0866) | Bit/dim 7.0695(8.2537) | Xent 2.0389(2.1743) | Loss 19.3165(19.6676) | Error 0.7099(0.7903) Steps 0(0.00) | Grad Norm 5.9976(16.7743) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 225.4123(231.8863) | Bit/dim 7.0601(8.2178) | Xent 2.0461(2.1705) | Loss 16.7806(19.5810) | Error 0.7256(0.7883) Steps 0(0.00) | Grad Norm 3.6070(16.3793) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 256.1742(232.6150) | Bit/dim 7.0677(8.1833) | Xent 2.0475(2.1668) | Loss 16.9202(19.5012) | Error 0.7115(0.7860) Steps 0(0.00) | Grad Norm 4.6035(16.0260) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 250.8381(233.1617) | Bit/dim 7.0551(8.1495) | Xent 2.0558(2.1635) | Loss 16.8369(19.4213) | Error 0.7126(0.7838) Steps 0(0.00) | Grad Norm 4.4941(15.6800) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 256.5297(233.8627) | Bit/dim 7.0278(8.1158) | Xent 2.0386(2.1597) | Loss 16.8357(19.3437) | Error 0.7020(0.7814) Steps 0(0.00) | Grad Norm 4.4142(15.3421) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 258.0886(234.5895) | Bit/dim 7.0200(8.0830) | Xent 2.0469(2.1563) | Loss 16.7818(19.2668) | Error 0.7134(0.7793) Steps 0(0.00) | Grad Norm 4.7100(15.0231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 55.8308, Epoch Time 1536.9253(1375.1767), Bit/dim 7.0219(best: 7.0833), Xent 2.0373, Loss 8.0405, Error 0.6929(best: 0.6776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 289.5845(236.2393) | Bit/dim 7.0264(8.0513) | Xent 2.0432(2.1529) | Loss 19.5089(19.2741) | Error 0.7057(0.7771) Steps 0(0.00) | Grad Norm 5.9292(14.7503) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 263.3969(237.0541) | Bit/dim 7.0164(8.0202) | Xent 2.0501(2.1498) | Loss 16.8675(19.2019) | Error 0.7115(0.7751) Steps 0(0.00) | Grad Norm 3.4658(14.4118) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 283.7139(238.4539) | Bit/dim 6.9965(7.9895) | Xent 2.0371(2.1465) | Loss 16.7757(19.1291) | Error 0.7111(0.7732) Steps 0(0.00) | Grad Norm 3.6310(14.0883) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 281.6031(239.7483) | Bit/dim 6.9993(7.9598) | Xent 2.0485(2.1435) | Loss 16.8283(19.0601) | Error 0.7103(0.7713) Steps 0(0.00) | Grad Norm 6.1445(13.8500) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 268.4936(240.6107) | Bit/dim 6.9979(7.9309) | Xent 2.0323(2.1402) | Loss 16.7524(18.9909) | Error 0.7117(0.7695) Steps 0(0.00) | Grad Norm 10.0238(13.7352) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 303.5731(242.4996) | Bit/dim 6.9889(7.9027) | Xent 2.0356(2.1371) | Loss 16.9058(18.9283) | Error 0.7154(0.7679) Steps 0(0.00) | Grad Norm 15.0534(13.7748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 58.7917, Epoch Time 1765.1122(1386.8747), Bit/dim 6.9928(best: 7.0219), Xent 2.0416, Loss 8.0136, Error 0.7194(best: 0.6776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 287.0353(243.8356) | Bit/dim 6.9797(7.8750) | Xent 2.0621(2.1348) | Loss 19.3746(18.9417) | Error 0.7364(0.7670) Steps 0(0.00) | Grad Norm 21.0050(13.9917) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 294.7592(245.3633) | Bit/dim 6.9798(7.8481) | Xent 2.0470(2.1322) | Loss 16.6789(18.8738) | Error 0.7216(0.7656) Steps 0(0.00) | Grad Norm 25.0227(14.3226) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 294.7065(246.8436) | Bit/dim 6.9689(7.8218) | Xent 2.0180(2.1287) | Loss 16.8133(18.8120) | Error 0.7039(0.7638) Steps 0(0.00) | Grad Norm 11.1138(14.2263) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 310.0558(248.7400) | Bit/dim 6.9684(7.7962) | Xent 2.0046(2.1250) | Loss 16.6676(18.7477) | Error 0.6871(0.7615) Steps 0(0.00) | Grad Norm 3.4514(13.9031) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 316.8283(250.7826) | Bit/dim 6.9651(7.7712) | Xent 2.0260(2.1220) | Loss 16.6211(18.6839) | Error 0.6985(0.7596) Steps 0(0.00) | Grad Norm 12.9640(13.8749) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 302.4360(252.3322) | Bit/dim 6.9629(7.7470) | Xent 2.0350(2.1194) | Loss 16.7673(18.6264) | Error 0.7139(0.7582) Steps 0(0.00) | Grad Norm 26.4874(14.2533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 58.9916, Epoch Time 1880.6335(1401.6875), Bit/dim 6.9479(best: 6.9928), Xent 2.0079, Loss 7.9518, Error 0.6875(best: 0.6776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 297.9106(253.6996) | Bit/dim 6.9494(7.7231) | Xent 2.0163(2.1163) | Loss 19.2262(18.6444) | Error 0.6945(0.7563) Steps 0(0.00) | Grad Norm 20.9950(14.4556) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 294.2919(254.9174) | Bit/dim 6.9353(7.6994) | Xent 2.0230(2.1135) | Loss 16.6786(18.5854) | Error 0.7001(0.7546) Steps 0(0.00) | Grad Norm 20.1405(14.6261) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 291.2425(256.0071) | Bit/dim 6.9305(7.6764) | Xent 2.0423(2.1114) | Loss 16.7214(18.5295) | Error 0.7129(0.7534) Steps 0(0.00) | Grad Norm 26.1871(14.9729) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 292.3822(257.0984) | Bit/dim 6.9056(7.6532) | Xent 1.9985(2.1080) | Loss 16.4278(18.4664) | Error 0.6873(0.7514) Steps 0(0.00) | Grad Norm 15.6251(14.9925) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 304.9907(258.5351) | Bit/dim 6.9024(7.6307) | Xent 1.9928(2.1046) | Loss 16.4848(18.4070) | Error 0.6738(0.7490) Steps 0(0.00) | Grad Norm 11.2225(14.8794) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 299.9224(259.7768) | Bit/dim 6.8856(7.6084) | Xent 1.9793(2.1008) | Loss 16.5158(18.3502) | Error 0.6798(0.7470) Steps 0(0.00) | Grad Norm 10.6427(14.7523) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 56.3579, Epoch Time 1853.0384(1415.2280), Bit/dim 6.8805(best: 6.9479), Xent 1.9705, Loss 7.8657, Error 0.6609(best: 0.6776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 327.5167(261.8090) | Bit/dim 6.8723(7.5863) | Xent 1.9813(2.0972) | Loss 19.1420(18.3740) | Error 0.6626(0.7444) Steps 0(0.00) | Grad Norm 12.2645(14.6777) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 291.5654(262.7017) | Bit/dim 6.8701(7.5648) | Xent 1.9837(2.0938) | Loss 16.4519(18.3163) | Error 0.6849(0.7426) Steps 0(0.00) | Grad Norm 15.4001(14.6993) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 298.9445(263.7889) | Bit/dim 6.8582(7.5436) | Xent 2.0059(2.0912) | Loss 16.4032(18.2589) | Error 0.6987(0.7413) Steps 0(0.00) | Grad Norm 22.4266(14.9312) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 285.7983(264.4492) | Bit/dim 6.8504(7.5228) | Xent 2.0069(2.0886) | Loss 16.4225(18.2038) | Error 0.7085(0.7403) Steps 0(0.00) | Grad Norm 25.6483(15.2527) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 298.1227(265.4594) | Bit/dim 6.8166(7.5016) | Xent 1.9795(2.0854) | Loss 16.2527(18.1453) | Error 0.6845(0.7387) Steps 0(0.00) | Grad Norm 13.9756(15.2144) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 278.9079(265.8629) | Bit/dim 6.8103(7.4809) | Xent 1.9725(2.0820) | Loss 16.3762(18.0922) | Error 0.6836(0.7370) Steps 0(0.00) | Grad Norm 9.4759(15.0422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 52.0642, Epoch Time 1848.7022(1428.2323), Bit/dim 6.7937(best: 6.8805), Xent 1.9491, Loss 7.7682, Error 0.6584(best: 0.6609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 266.2362(265.8741) | Bit/dim 6.7935(7.4602) | Xent 1.9669(2.0785) | Loss 18.8666(18.1155) | Error 0.6675(0.7349) Steps 0(0.00) | Grad Norm 8.7463(14.8533) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 285.6134(266.4663) | Bit/dim 6.7725(7.4396) | Xent 1.9652(2.0751) | Loss 16.2516(18.0595) | Error 0.6747(0.7331) Steps 0(0.00) | Grad Norm 11.7635(14.7606) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 275.9463(266.7507) | Bit/dim 6.7623(7.4193) | Xent 1.9891(2.0726) | Loss 16.2261(18.0045) | Error 0.6916(0.7319) Steps 0(0.00) | Grad Norm 20.6561(14.9375) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 253.7365(266.3602) | Bit/dim 6.7689(7.3998) | Xent 2.0294(2.0713) | Loss 16.1450(17.9488) | Error 0.7378(0.7321) Steps 0(0.00) | Grad Norm 35.6680(15.5594) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 259.7732(266.1626) | Bit/dim 6.7006(7.3788) | Xent 1.9518(2.0677) | Loss 16.0135(17.8907) | Error 0.6540(0.7297) Steps 0(0.00) | Grad Norm 3.5831(15.2001) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 257.8228(265.9124) | Bit/dim 6.7386(7.3596) | Xent 2.0422(2.0669) | Loss 16.2807(17.8424) | Error 0.7185(0.7294) Steps 0(0.00) | Grad Norm 35.8427(15.8194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 55.2772, Epoch Time 1670.1509(1435.4898), Bit/dim 6.6650(best: 6.7937), Xent 1.9360, Loss 7.6330, Error 0.6601(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 262.1782(265.8004) | Bit/dim 6.6677(7.3388) | Xent 1.9410(2.0631) | Loss 18.7339(17.8691) | Error 0.6633(0.7274) Steps 0(0.00) | Grad Norm 12.1388(15.7090) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 264.4825(265.7609) | Bit/dim 6.6300(7.3176) | Xent 1.9500(2.0597) | Loss 16.0626(17.8149) | Error 0.6611(0.7254) Steps 0(0.00) | Grad Norm 9.5254(15.5235) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 266.1485(265.7725) | Bit/dim 6.6494(7.2975) | Xent 2.0062(2.0581) | Loss 15.9831(17.7600) | Error 0.7096(0.7249) Steps 0(0.00) | Grad Norm 40.2270(16.2646) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 262.2964(265.6682) | Bit/dim 6.5641(7.2755) | Xent 1.9941(2.0562) | Loss 15.8120(17.7016) | Error 0.7000(0.7242) Steps 0(0.00) | Grad Norm 31.0114(16.7070) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 253.8850(265.3147) | Bit/dim 6.5632(7.2542) | Xent 2.1486(2.0590) | Loss 15.8370(17.6456) | Error 0.7331(0.7245) Steps 0(0.00) | Grad Norm 37.8084(17.3400) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 255.1463(265.0097) | Bit/dim 6.4944(7.2314) | Xent 1.9481(2.0557) | Loss 15.8207(17.5909) | Error 0.6587(0.7225) Steps 0(0.00) | Grad Norm 9.3151(17.0993) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 52.7013, Epoch Time 1632.4758(1441.3994), Bit/dim 6.4898(best: 6.6650), Xent 1.9813, Loss 7.4805, Error 0.6961(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 258.8573(264.8251) | Bit/dim 6.4907(7.2091) | Xent 1.9874(2.0536) | Loss 18.4387(17.6163) | Error 0.7009(0.7218) Steps 0(0.00) | Grad Norm 28.4039(17.4384) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 260.9137(264.7078) | Bit/dim 6.4439(7.1862) | Xent 2.1113(2.0553) | Loss 15.7339(17.5598) | Error 0.7346(0.7222) Steps 0(0.00) | Grad Norm 36.9537(18.0239) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 274.5629(265.0034) | Bit/dim 6.3891(7.1623) | Xent 1.9471(2.0521) | Loss 15.4927(17.4978) | Error 0.6849(0.7211) Steps 0(0.00) | Grad Norm 27.7934(18.3170) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 243.8633(264.3692) | Bit/dim 6.4499(7.1409) | Xent 2.1565(2.0552) | Loss 15.7386(17.4450) | Error 0.7594(0.7222) Steps 0(0.00) | Grad Norm 90.5736(20.4847) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 282.3645(264.9091) | Bit/dim 6.4541(7.1203) | Xent 2.1526(2.0581) | Loss 15.8045(17.3958) | Error 0.7554(0.7232) Steps 0(0.00) | Grad Norm 54.9226(21.5178) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 262.2390(264.8290) | Bit/dim 6.2524(7.0943) | Xent 1.9726(2.0556) | Loss 15.2200(17.3305) | Error 0.6830(0.7220) Steps 0(0.00) | Grad Norm 40.9560(22.1009) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 51.3444, Epoch Time 1650.3199(1447.6670), Bit/dim 6.3115(best: 6.4898), Xent 2.3300, Loss 7.4765, Error 0.7972(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 284.2547(265.4117) | Bit/dim 6.3126(7.0708) | Xent 2.3329(2.0639) | Loss 18.4044(17.3628) | Error 0.7946(0.7242) Steps 0(0.00) | Grad Norm 79.3125(23.8173) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 275.4123(265.7118) | Bit/dim 6.3861(7.0503) | Xent 2.0516(2.0635) | Loss 15.5495(17.3084) | Error 0.7321(0.7244) Steps 0(0.00) | Grad Norm 40.3625(24.3136) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 274.4335(265.9734) | Bit/dim 6.1464(7.0232) | Xent 2.0316(2.0626) | Loss 14.9591(17.2379) | Error 0.7210(0.7243) Steps 0(0.00) | Grad Norm 59.0243(25.3550) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 299.0642(266.9661) | Bit/dim 6.3942(7.0043) | Xent 2.5305(2.0766) | Loss 16.0656(17.2027) | Error 0.8448(0.7280) Steps 0(0.00) | Grad Norm 175.2466(29.8517) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 267.5871(266.9848) | Bit/dim 6.5465(6.9906) | Xent 2.2264(2.0811) | Loss 16.0481(17.1681) | Error 0.7759(0.7294) Steps 0(0.00) | Grad Norm 148.4353(33.4092) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 280.6807(267.3956) | Bit/dim 6.3658(6.9718) | Xent 3.5728(2.1259) | Loss 17.0157(17.1635) | Error 0.8661(0.7335) Steps 0(0.00) | Grad Norm 252.2596(39.9747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 54.6545, Epoch Time 1752.3442(1456.8073), Bit/dim 6.2522(best: 6.3115), Xent 3.2428, Loss 7.8736, Error 0.8613(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 259.2256(267.1505) | Bit/dim 6.2428(6.9499) | Xent 3.2165(2.1586) | Loss 19.1817(17.2241) | Error 0.8699(0.7376) Steps 0(0.00) | Grad Norm 211.7412(45.1277) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 255.6000(266.8040) | Bit/dim 6.2504(6.9290) | Xent 2.2639(2.1617) | Loss 15.3750(17.1686) | Error 0.8050(0.7396) Steps 0(0.00) | Grad Norm 96.3629(46.6648) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 258.7791(266.5633) | Bit/dim 6.0730(6.9033) | Xent 2.1725(2.1621) | Loss 15.0444(17.1049) | Error 0.7815(0.7409) Steps 0(0.00) | Grad Norm 67.7970(47.2987) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 268.4493(266.6199) | Bit/dim 6.0495(6.8777) | Xent 2.6231(2.1759) | Loss 15.3823(17.0532) | Error 0.8631(0.7445) Steps 0(0.00) | Grad Norm 149.4748(50.3640) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 225.3009(265.3803) | Bit/dim 6.0891(6.8540) | Xent 2.3591(2.1814) | Loss 15.2134(16.9980) | Error 0.8462(0.7476) Steps 0(0.00) | Grad Norm 113.6318(52.2621) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 247.4197(264.8415) | Bit/dim 6.0421(6.8297) | Xent 2.3564(2.1866) | Loss 15.1807(16.9435) | Error 0.8538(0.7508) Steps 0(0.00) | Grad Norm 62.3998(52.5662) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 52.4681, Epoch Time 1583.0101(1460.5934), Bit/dim 6.0270(best: 6.2522), Xent 2.1671, Loss 7.1106, Error 0.7791(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 246.4299(264.2891) | Bit/dim 6.0165(6.8053) | Xent 2.1796(2.1864) | Loss 17.7055(16.9663) | Error 0.7865(0.7518) Steps 0(0.00) | Grad Norm 28.8600(51.8550) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 260.2095(264.1667) | Bit/dim 6.1790(6.7865) | Xent 2.2044(2.1870) | Loss 15.2662(16.9153) | Error 0.8046(0.7534) Steps 0(0.00) | Grad Norm 54.8217(51.9440) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 233.9599(263.2605) | Bit/dim 5.9965(6.7628) | Xent 2.1306(2.1853) | Loss 14.7956(16.8517) | Error 0.7599(0.7536) Steps 0(0.00) | Grad Norm 26.7276(51.1875) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 226.3885(262.1544) | Bit/dim 6.2089(6.7462) | Xent 2.1561(2.1844) | Loss 15.3313(16.8061) | Error 0.7888(0.7547) Steps 0(0.00) | Grad Norm 42.1220(50.9156) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 245.1438(261.6441) | Bit/dim 5.9709(6.7229) | Xent 2.1323(2.1828) | Loss 14.8871(16.7486) | Error 0.7696(0.7551) Steps 0(0.00) | Grad Norm 37.9576(50.5268) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 231.5438(260.7410) | Bit/dim 5.9653(6.7002) | Xent 2.0943(2.1802) | Loss 14.6693(16.6862) | Error 0.7323(0.7544) Steps 0(0.00) | Grad Norm 14.1196(49.4346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 46.1408, Epoch Time 1505.7217(1461.9473), Bit/dim 5.9123(best: 6.0270), Xent 2.1069, Loss 6.9657, Error 0.7629(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 236.6099(260.0171) | Bit/dim 5.8914(6.6759) | Xent 2.1264(2.1786) | Loss 17.4076(16.7078) | Error 0.7664(0.7548) Steps 0(0.00) | Grad Norm 37.1519(49.0661) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 236.2200(259.3032) | Bit/dim 5.8945(6.6525) | Xent 2.0989(2.1762) | Loss 14.6523(16.6462) | Error 0.7441(0.7545) Steps 0(0.00) | Grad Norm 14.3783(48.0255) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 236.9447(258.6324) | Bit/dim 5.8711(6.6290) | Xent 2.1225(2.1746) | Loss 14.5155(16.5822) | Error 0.7558(0.7545) Steps 0(0.00) | Grad Norm 32.7192(47.5663) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 228.2587(257.7212) | Bit/dim 5.7971(6.6041) | Xent 2.0848(2.1719) | Loss 14.2706(16.5129) | Error 0.7280(0.7537) Steps 0(0.00) | Grad Norm 10.9344(46.4673) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 223.5239(256.6953) | Bit/dim 5.8883(6.5826) | Xent 2.0956(2.1696) | Loss 14.5714(16.4546) | Error 0.7375(0.7532) Steps 0(0.00) | Grad Norm 33.8776(46.0896) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 228.3725(255.8456) | Bit/dim 5.7644(6.5580) | Xent 2.0646(2.1664) | Loss 14.3651(16.3919) | Error 0.7124(0.7520) Steps 0(0.00) | Grad Norm 11.6561(45.0566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 48.5246, Epoch Time 1454.5242(1461.7246), Bit/dim 5.7531(best: 5.9123), Xent 2.0670, Loss 6.7866, Error 0.7068(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 241.9515(255.4288) | Bit/dim 5.7566(6.5340) | Xent 2.0970(2.1644) | Loss 16.8985(16.4071) | Error 0.7374(0.7516) Steps 0(0.00) | Grad Norm 21.0440(44.3363) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 239.4402(254.9491) | Bit/dim 5.7442(6.5103) | Xent 2.0627(2.1613) | Loss 14.2645(16.3429) | Error 0.6996(0.7500) Steps 0(0.00) | Grad Norm 9.7290(43.2980) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 235.4195(254.3633) | Bit/dim 5.6726(6.4852) | Xent 2.0684(2.1585) | Loss 14.2442(16.2799) | Error 0.7226(0.7492) Steps 0(0.00) | Grad Norm 11.5220(42.3448) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 226.9528(253.5409) | Bit/dim 5.6840(6.4611) | Xent 2.0592(2.1555) | Loss 14.1562(16.2162) | Error 0.7261(0.7485) Steps 0(0.00) | Grad Norm 8.5054(41.3296) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 230.1120(252.8381) | Bit/dim 5.6419(6.4366) | Xent 2.0652(2.1528) | Loss 14.0974(16.1526) | Error 0.7210(0.7477) Steps 0(0.00) | Grad Norm 5.9570(40.2684) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 244.5347(252.5890) | Bit/dim 5.6284(6.4123) | Xent 2.0675(2.1503) | Loss 14.1212(16.0917) | Error 0.7208(0.7469) Steps 0(0.00) | Grad Norm 5.5619(39.2272) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 47.2232, Epoch Time 1481.4371(1462.3159), Bit/dim 5.6142(best: 5.7531), Xent 2.0265, Loss 6.6275, Error 0.6747(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 244.4213(252.3439) | Bit/dim 5.6046(6.3881) | Xent 2.0502(2.1473) | Loss 16.5285(16.1048) | Error 0.7007(0.7455) Steps 0(0.00) | Grad Norm 5.2116(38.2067) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 239.7557(251.9663) | Bit/dim 5.5960(6.3643) | Xent 2.0475(2.1443) | Loss 13.9001(16.0387) | Error 0.6929(0.7439) Steps 0(0.00) | Grad Norm 5.2980(37.2195) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 243.7213(251.7189) | Bit/dim 5.5510(6.3399) | Xent 2.0350(2.1410) | Loss 13.8484(15.9729) | Error 0.6899(0.7423) Steps 0(0.00) | Grad Norm 4.4009(36.2349) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 230.2370(251.0745) | Bit/dim 5.5581(6.3165) | Xent 2.0304(2.1377) | Loss 13.8384(15.9089) | Error 0.6971(0.7409) Steps 0(0.00) | Grad Norm 4.5443(35.2842) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 223.0880(250.2349) | Bit/dim 5.5252(6.2927) | Xent 2.0159(2.1340) | Loss 13.6000(15.8396) | Error 0.7079(0.7399) Steps 0(0.00) | Grad Norm 7.1056(34.4388) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 231.1968(249.6637) | Bit/dim 5.4916(6.2687) | Xent 2.0171(2.1305) | Loss 13.5817(15.7719) | Error 0.6883(0.7384) Steps 0(0.00) | Grad Norm 3.5722(33.5128) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 46.6367, Epoch Time 1475.2187(1462.7030), Bit/dim 5.4829(best: 5.6142), Xent 1.9979, Loss 6.4818, Error 0.6787(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 223.9986(248.8938) | Bit/dim 5.4864(6.2452) | Xent 2.0125(2.1270) | Loss 16.4202(15.7913) | Error 0.6963(0.7371) Steps 0(0.00) | Grad Norm 8.9909(32.7772) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 216.3533(247.9176) | Bit/dim 5.4605(6.2217) | Xent 2.0024(2.1232) | Loss 13.6217(15.7263) | Error 0.6863(0.7356) Steps 0(0.00) | Grad Norm 4.3624(31.9247) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 239.4702(247.6642) | Bit/dim 5.4503(6.1985) | Xent 2.0001(2.1195) | Loss 13.6248(15.6632) | Error 0.6805(0.7339) Steps 0(0.00) | Grad Norm 7.6841(31.1975) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 230.9983(247.1642) | Bit/dim 5.4214(6.1752) | Xent 1.9745(2.1152) | Loss 13.5182(15.5989) | Error 0.6744(0.7322) Steps 0(0.00) | Grad Norm 5.0307(30.4125) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 237.0391(246.8604) | Bit/dim 5.4218(6.1526) | Xent 2.0006(2.1118) | Loss 13.4535(15.5345) | Error 0.6841(0.7307) Steps 0(0.00) | Grad Norm 7.6986(29.7311) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 224.3115(246.1840) | Bit/dim 5.3951(6.1299) | Xent 1.9680(2.1074) | Loss 13.4986(15.4734) | Error 0.6743(0.7290) Steps 0(0.00) | Grad Norm 7.9159(29.0766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 48.1124, Epoch Time 1436.4701(1461.9160), Bit/dim 5.3781(best: 5.4829), Xent 1.9633, Loss 6.3597, Error 0.6783(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 241.7280(246.0503) | Bit/dim 5.3755(6.1073) | Xent 1.9672(2.1032) | Loss 16.1181(15.4928) | Error 0.6784(0.7275) Steps 0(0.00) | Grad Norm 15.9145(28.6818) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 234.1284(245.6926) | Bit/dim 5.3780(6.0854) | Xent 2.0052(2.1003) | Loss 13.4419(15.4312) | Error 0.7011(0.7267) Steps 0(0.00) | Grad Norm 18.7997(28.3853) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 254.9803(245.9713) | Bit/dim 5.4099(6.0651) | Xent 2.0301(2.0982) | Loss 13.6296(15.3772) | Error 0.7211(0.7265) Steps 0(0.00) | Grad Norm 41.1404(28.7680) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 230.8081(245.5164) | Bit/dim 5.4592(6.0469) | Xent 2.1642(2.1002) | Loss 13.9155(15.3333) | Error 0.7555(0.7274) Steps 0(0.00) | Grad Norm 72.2522(30.0725) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 233.9323(245.1688) | Bit/dim 5.4304(6.0285) | Xent 1.9783(2.0965) | Loss 13.6024(15.2814) | Error 0.6946(0.7264) Steps 0(0.00) | Grad Norm 33.5157(30.1758) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 261.8118(245.6681) | Bit/dim 5.4581(6.0113) | Xent 2.4075(2.1058) | Loss 14.1065(15.2462) | Error 0.8066(0.7288) Steps 0(0.00) | Grad Norm 79.1163(31.6440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 53.6515, Epoch Time 1526.9444(1463.8669), Bit/dim 5.4602(best: 5.3781), Xent 2.2263, Loss 6.5734, Error 0.7673(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 241.1823(245.5335) | Bit/dim 5.4616(5.9948) | Xent 2.2600(2.1105) | Loss 16.5060(15.2840) | Error 0.7697(0.7301) Steps 0(0.00) | Grad Norm 63.1778(32.5900) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 243.9350(245.4856) | Bit/dim 5.2681(5.9730) | Xent 1.9787(2.1065) | Loss 13.1350(15.2195) | Error 0.6775(0.7285) Steps 0(0.00) | Grad Norm 6.0925(31.7951) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 256.1340(245.8050) | Bit/dim 5.3405(5.9541) | Xent 2.0071(2.1035) | Loss 13.3630(15.1638) | Error 0.7107(0.7280) Steps 0(0.00) | Grad Norm 21.2510(31.4788) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 266.0187(246.4115) | Bit/dim 5.2525(5.9330) | Xent 2.0028(2.1005) | Loss 13.3101(15.1082) | Error 0.6905(0.7268) Steps 0(0.00) | Grad Norm 5.7000(30.7054) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 256.7809(246.7225) | Bit/dim 5.2685(5.9131) | Xent 2.0252(2.0983) | Loss 13.2930(15.0537) | Error 0.7154(0.7265) Steps 0(0.00) | Grad Norm 14.5620(30.2211) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 243.8275(246.6357) | Bit/dim 5.2619(5.8936) | Xent 2.0107(2.0956) | Loss 13.2232(14.9988) | Error 0.7059(0.7259) Steps 0(0.00) | Grad Norm 8.5972(29.5724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 55.5972, Epoch Time 1579.2670(1467.3289), Bit/dim 5.2216(best: 5.3781), Xent 1.9766, Loss 6.2099, Error 0.6845(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 245.8965(246.6135) | Bit/dim 5.2360(5.8738) | Xent 1.9877(2.0924) | Loss 15.8449(15.0242) | Error 0.6915(0.7248) Steps 0(0.00) | Grad Norm 6.1110(28.8685) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 255.2396(246.8723) | Bit/dim 5.1955(5.8535) | Xent 2.0189(2.0902) | Loss 13.1031(14.9666) | Error 0.7090(0.7244) Steps 0(0.00) | Grad Norm 8.1951(28.2483) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 251.7803(247.0195) | Bit/dim 5.1714(5.8330) | Xent 1.9922(2.0872) | Loss 13.1091(14.9108) | Error 0.6980(0.7236) Steps 0(0.00) | Grad Norm 3.7591(27.5137) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 236.1061(246.6921) | Bit/dim 5.1737(5.8132) | Xent 1.9986(2.0846) | Loss 13.0786(14.8559) | Error 0.7009(0.7229) Steps 0(0.00) | Grad Norm 6.1394(26.8724) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 262.9913(247.1811) | Bit/dim 5.1337(5.7928) | Xent 1.9973(2.0820) | Loss 12.9893(14.7999) | Error 0.7017(0.7223) Steps 0(0.00) | Grad Norm 7.0906(26.2790) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 253.4399(247.3689) | Bit/dim 5.1314(5.7730) | Xent 1.9949(2.0794) | Loss 12.9664(14.7449) | Error 0.6934(0.7214) Steps 0(0.00) | Grad Norm 8.2264(25.7374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 52.7989, Epoch Time 1574.6919(1470.5498), Bit/dim 5.1502(best: 5.2216), Xent 1.9559, Loss 6.1282, Error 0.6598(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 252.0165(247.5083) | Bit/dim 5.1567(5.7545) | Xent 1.9585(2.0757) | Loss 15.9016(14.7796) | Error 0.6727(0.7199) Steps 0(0.00) | Grad Norm 11.7674(25.3183) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 269.4756(248.1673) | Bit/dim 5.1141(5.7353) | Xent 2.0136(2.0739) | Loss 12.8304(14.7211) | Error 0.7203(0.7199) Steps 0(0.00) | Grad Norm 21.0440(25.1901) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 267.8693(248.7584) | Bit/dim 5.1388(5.7174) | Xent 2.2216(2.0783) | Loss 13.3156(14.6789) | Error 0.7772(0.7217) Steps 0(0.00) | Grad Norm 35.3958(25.4962) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 280.1280(249.6995) | Bit/dim 5.0817(5.6983) | Xent 1.9709(2.0751) | Loss 12.9130(14.6260) | Error 0.6854(0.7206) Steps 0(0.00) | Grad Norm 7.9283(24.9692) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 257.0290(249.9193) | Bit/dim 5.1167(5.6809) | Xent 2.2545(2.0805) | Loss 13.2544(14.5848) | Error 0.7861(0.7225) Steps 0(0.00) | Grad Norm 38.8636(25.3860) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 261.4149(250.2642) | Bit/dim 5.0664(5.6625) | Xent 1.9964(2.0779) | Loss 12.9005(14.5343) | Error 0.6990(0.7218) Steps 0(0.00) | Grad Norm 9.1811(24.8999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 56.2464, Epoch Time 1659.9690(1476.2324), Bit/dim 5.0554(best: 5.1502), Xent 2.0391, Loss 6.0749, Error 0.7076(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 252.5025(250.3314) | Bit/dim 5.0540(5.6442) | Xent 2.0643(2.0775) | Loss 15.3814(14.5597) | Error 0.7309(0.7221) Steps 0(0.00) | Grad Norm 13.8261(24.5677) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 261.5432(250.6677) | Bit/dim 5.0540(5.6265) | Xent 2.0521(2.0768) | Loss 12.8594(14.5087) | Error 0.7280(0.7223) Steps 0(0.00) | Grad Norm 14.2292(24.2575) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 255.2243(250.8044) | Bit/dim 5.1125(5.6111) | Xent 1.9814(2.0739) | Loss 12.9023(14.4605) | Error 0.6890(0.7213) Steps 0(0.00) | Grad Norm 13.7059(23.9410) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 276.1897(251.5660) | Bit/dim 5.0356(5.5938) | Xent 1.9991(2.0717) | Loss 12.7474(14.4091) | Error 0.7063(0.7208) Steps 0(0.00) | Grad Norm 8.7583(23.4855) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 275.0429(252.2703) | Bit/dim 5.0525(5.5776) | Xent 2.0041(2.0696) | Loss 12.8426(14.3621) | Error 0.6995(0.7202) Steps 0(0.00) | Grad Norm 13.5900(23.1886) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 258.8522(252.4677) | Bit/dim 5.1018(5.5633) | Xent 1.9773(2.0669) | Loss 12.8849(14.3178) | Error 0.6987(0.7195) Steps 0(0.00) | Grad Norm 11.2228(22.8297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 53.0960, Epoch Time 1648.3040(1481.3945), Bit/dim 5.0300(best: 5.0554), Xent 1.9667, Loss 6.0134, Error 0.6808(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 243.5267(252.1995) | Bit/dim 5.0358(5.5475) | Xent 1.9844(2.0644) | Loss 15.1935(14.3441) | Error 0.6937(0.7188) Steps 0(0.00) | Grad Norm 6.6927(22.3455) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 263.8646(252.5495) | Bit/dim 5.1166(5.5345) | Xent 2.0046(2.0626) | Loss 12.9459(14.3021) | Error 0.7139(0.7186) Steps 0(0.00) | Grad Norm 26.7883(22.4788) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 258.0533(252.7146) | Bit/dim 5.0369(5.5196) | Xent 1.9791(2.0601) | Loss 12.7369(14.2552) | Error 0.6854(0.7176) Steps 0(0.00) | Grad Norm 11.0883(22.1371) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 277.9026(253.4702) | Bit/dim 5.0286(5.5049) | Xent 1.9765(2.0576) | Loss 12.7229(14.2092) | Error 0.6831(0.7166) Steps 0(0.00) | Grad Norm 10.6046(21.7911) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 284.1301(254.3900) | Bit/dim 5.0010(5.4898) | Xent 2.0091(2.0561) | Loss 12.7154(14.1644) | Error 0.7076(0.7163) Steps 0(0.00) | Grad Norm 12.9479(21.5258) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 258.8356(254.5234) | Bit/dim 4.9782(5.4744) | Xent 1.9738(2.0537) | Loss 12.6432(14.1187) | Error 0.6923(0.7156) Steps 0(0.00) | Grad Norm 6.0837(21.0626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 51.4377, Epoch Time 1653.6742(1486.5629), Bit/dim 4.9774(best: 5.0300), Xent 1.9524, Loss 5.9536, Error 0.6835(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 256.7917(254.5914) | Bit/dim 4.9853(5.4597) | Xent 1.9775(2.0514) | Loss 15.0238(14.1459) | Error 0.6967(0.7150) Steps 0(0.00) | Grad Norm 11.1409(20.7649) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 263.1582(254.8484) | Bit/dim 4.9644(5.4449) | Xent 1.9475(2.0483) | Loss 12.5527(14.0981) | Error 0.6735(0.7138) Steps 0(0.00) | Grad Norm 5.4334(20.3050) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 279.4523(255.5866) | Bit/dim 4.9407(5.4298) | Xent 1.9438(2.0451) | Loss 12.4257(14.0479) | Error 0.6816(0.7128) Steps 0(0.00) | Grad Norm 11.8338(20.0508) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 282.8508(256.4045) | Bit/dim 4.9447(5.4152) | Xent 1.9874(2.0434) | Loss 12.6598(14.0063) | Error 0.7125(0.7128) Steps 0(0.00) | Grad Norm 16.6568(19.9490) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 289.5975(257.4003) | Bit/dim 4.9315(5.4007) | Xent 1.9592(2.0409) | Loss 12.5561(13.9628) | Error 0.6920(0.7122) Steps 0(0.00) | Grad Norm 9.4097(19.6328) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 276.0059(257.9584) | Bit/dim 4.9244(5.3864) | Xent 1.9442(2.0380) | Loss 12.4689(13.9180) | Error 0.6840(0.7113) Steps 0(0.00) | Grad Norm 8.7248(19.3056) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 54.6470, Epoch Time 1718.4683(1493.5201), Bit/dim 4.9106(best: 4.9774), Xent 1.9101, Loss 5.8656, Error 0.6654(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 265.3287(258.1795) | Bit/dim 4.9191(5.3724) | Xent 1.9329(2.0348) | Loss 15.3009(13.9594) | Error 0.6815(0.7104) Steps 0(0.00) | Grad Norm 9.4952(19.0113) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 265.8800(258.4106) | Bit/dim 4.8994(5.3582) | Xent 1.9425(2.0320) | Loss 12.5006(13.9157) | Error 0.6871(0.7097) Steps 0(0.00) | Grad Norm 7.2933(18.6598) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 268.2169(258.7048) | Bit/dim 4.8774(5.3438) | Xent 1.9140(2.0285) | Loss 12.4941(13.8730) | Error 0.6699(0.7086) Steps 0(0.00) | Grad Norm 5.4679(18.2640) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 284.1757(259.4689) | Bit/dim 4.8775(5.3298) | Xent 1.9000(2.0247) | Loss 12.3782(13.8282) | Error 0.6603(0.7071) Steps 0(0.00) | Grad Norm 8.0157(17.9565) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 280.4136(260.0972) | Bit/dim 4.8684(5.3159) | Xent 1.9008(2.0209) | Loss 12.2859(13.7819) | Error 0.6753(0.7061) Steps 0(0.00) | Grad Norm 3.7236(17.5296) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 276.3837(260.5858) | Bit/dim 4.8541(5.3021) | Xent 1.9052(2.0175) | Loss 12.4423(13.7417) | Error 0.6713(0.7051) Steps 0(0.00) | Grad Norm 4.9237(17.1514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 55.0925, Epoch Time 1711.4542(1500.0581), Bit/dim 4.8475(best: 4.9106), Xent 1.8703, Loss 5.7826, Error 0.6491(best: 0.6584)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 284.7519(261.3108) | Bit/dim 4.8525(5.2886) | Xent 1.8956(2.0138) | Loss 14.8690(13.7755) | Error 0.6627(0.7038) Steps 0(0.00) | Grad Norm 3.8925(16.7536) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 281.2609(261.9093) | Bit/dim 4.8383(5.2751) | Xent 1.8941(2.0102) | Loss 12.4063(13.7345) | Error 0.6690(0.7028) Steps 0(0.00) | Grad Norm 3.5212(16.3566) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 273.1427(262.2463) | Bit/dim 4.8271(5.2617) | Xent 1.8792(2.0063) | Loss 12.3132(13.6918) | Error 0.6596(0.7015) Steps 0(0.00) | Grad Norm 5.7501(16.0384) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 275.4255(262.6417) | Bit/dim 4.8237(5.2485) | Xent 1.8834(2.0026) | Loss 12.2697(13.6492) | Error 0.6540(0.7001) Steps 0(0.00) | Grad Norm 7.3369(15.7774) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 266.4527(262.7560) | Bit/dim 4.8326(5.2360) | Xent 1.8938(1.9993) | Loss 12.3010(13.6087) | Error 0.6640(0.6990) Steps 0(0.00) | Grad Norm 11.8146(15.6585) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 266.9556(262.8820) | Bit/dim 4.9059(5.2261) | Xent 1.9206(1.9970) | Loss 12.4523(13.5740) | Error 0.6679(0.6981) Steps 0(0.00) | Grad Norm 20.4075(15.8010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 53.4363, Epoch Time 1717.4967(1506.5812), Bit/dim 4.8984(best: 4.8475), Xent 1.8808, Loss 5.8389, Error 0.6568(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 263.1283(262.8894) | Bit/dim 4.8971(5.2163) | Xent 1.9206(1.9947) | Loss 15.2087(13.6231) | Error 0.6750(0.6974) Steps 0(0.00) | Grad Norm 24.9050(16.0741) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 279.5921(263.3905) | Bit/dim 4.8294(5.2047) | Xent 1.8807(1.9913) | Loss 12.2257(13.5812) | Error 0.6554(0.6961) Steps 0(0.00) | Grad Norm 8.0826(15.8344) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 279.1960(263.8646) | Bit/dim 4.8106(5.1928) | Xent 1.8654(1.9875) | Loss 12.2349(13.5408) | Error 0.6525(0.6948) Steps 0(0.00) | Grad Norm 9.0722(15.6315) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 275.9111(264.2260) | Bit/dim 4.8248(5.1818) | Xent 1.8822(1.9843) | Loss 12.2573(13.5023) | Error 0.6567(0.6937) Steps 0(0.00) | Grad Norm 11.9576(15.5213) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 277.2839(264.6178) | Bit/dim 4.8077(5.1706) | Xent 1.8718(1.9809) | Loss 12.2664(13.4652) | Error 0.6570(0.6926) Steps 0(0.00) | Grad Norm 5.4024(15.2177) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 271.7699(264.8323) | Bit/dim 4.7945(5.1593) | Xent 1.8599(1.9773) | Loss 12.2168(13.4277) | Error 0.6487(0.6912) Steps 0(0.00) | Grad Norm 6.4844(14.9557) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 54.7978, Epoch Time 1718.0460(1512.9252), Bit/dim 4.8072(best: 4.8475), Xent 1.8210, Loss 5.7177, Error 0.6341(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 270.5789(265.0047) | Bit/dim 4.8091(5.1488) | Xent 1.8620(1.9739) | Loss 14.9361(13.4730) | Error 0.6509(0.6900) Steps 0(0.00) | Grad Norm 12.3366(14.8771) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 271.7791(265.2080) | Bit/dim 4.8031(5.1384) | Xent 1.8451(1.9700) | Loss 12.3003(13.4378) | Error 0.6449(0.6887) Steps 0(0.00) | Grad Norm 16.1982(14.9168) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 274.3368(265.4818) | Bit/dim 4.7891(5.1279) | Xent 1.9313(1.9688) | Loss 12.3237(13.4044) | Error 0.6831(0.6885) Steps 0(0.00) | Grad Norm 24.1408(15.1935) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 266.8773(265.5237) | Bit/dim 4.7824(5.1176) | Xent 1.9444(1.9681) | Loss 12.3426(13.3725) | Error 0.6715(0.6880) Steps 0(0.00) | Grad Norm 26.2456(15.5251) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 283.2973(266.0569) | Bit/dim 4.7520(5.1066) | Xent 1.9522(1.9676) | Loss 12.2880(13.3400) | Error 0.7001(0.6884) Steps 0(0.00) | Grad Norm 18.0126(15.5997) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 304.4922(267.2099) | Bit/dim 4.7367(5.0955) | Xent 1.8846(1.9651) | Loss 12.1852(13.3053) | Error 0.6761(0.6880) Steps 0(0.00) | Grad Norm 9.8138(15.4261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 57.0018, Epoch Time 1744.2889(1519.8661), Bit/dim 4.7286(best: 4.8072), Xent 1.8614, Loss 5.6593, Error 0.6694(best: 0.6341)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 279.1285(267.5675) | Bit/dim 4.7226(5.0843) | Xent 1.9149(1.9636) | Loss 14.8943(13.3530) | Error 0.6866(0.6880) Steps 0(0.00) | Grad Norm 11.7155(15.3148) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 278.4347(267.8935) | Bit/dim 4.7358(5.0739) | Xent 1.8974(1.9616) | Loss 12.1240(13.3161) | Error 0.6790(0.6877) Steps 0(0.00) | Grad Norm 13.8730(15.2715) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 271.0125(267.9871) | Bit/dim 4.7256(5.0634) | Xent 1.8730(1.9590) | Loss 12.0297(13.2776) | Error 0.6651(0.6870) Steps 0(0.00) | Grad Norm 9.8540(15.1090) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 269.3286(268.0273) | Bit/dim 4.6961(5.0524) | Xent 1.8298(1.9551) | Loss 11.9999(13.2392) | Error 0.6440(0.6857) Steps 0(0.00) | Grad Norm 5.0911(14.8085) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 294.4023(268.8186) | Bit/dim 4.7129(5.0422) | Xent 1.8259(1.9512) | Loss 12.0491(13.2035) | Error 0.6459(0.6845) Steps 0(0.00) | Grad Norm 9.3269(14.6440) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 274.3952(268.9859) | Bit/dim 4.6937(5.0318) | Xent 1.8545(1.9483) | Loss 12.0663(13.1694) | Error 0.6459(0.6834) Steps 0(0.00) | Grad Norm 11.4420(14.5480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 57.0526, Epoch Time 1739.6562(1526.4598), Bit/dim 4.6799(best: 4.7286), Xent 1.8075, Loss 5.5837, Error 0.6302(best: 0.6341)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 291.1193(269.6499) | Bit/dim 4.6820(5.0213) | Xent 1.8456(1.9452) | Loss 14.7579(13.2171) | Error 0.6490(0.6823) Steps 0(0.00) | Grad Norm 13.3457(14.5119) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 289.8105(270.2547) | Bit/dim 4.7182(5.0122) | Xent 1.8648(1.9428) | Loss 12.0929(13.1833) | Error 0.6673(0.6819) Steps 0(0.00) | Grad Norm 28.4027(14.9286) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 283.7777(270.6604) | Bit/dim 4.7645(5.0047) | Xent 1.9619(1.9434) | Loss 12.3597(13.1586) | Error 0.6891(0.6821) Steps 0(0.00) | Grad Norm 31.4867(15.4254) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 291.8101(271.2949) | Bit/dim 4.7322(4.9966) | Xent 1.8179(1.9396) | Loss 12.0553(13.1255) | Error 0.6320(0.6806) Steps 0(0.00) | Grad Norm 10.8528(15.2882) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 298.8273(272.1209) | Bit/dim 4.6632(4.9866) | Xent 1.8318(1.9364) | Loss 11.8434(13.0871) | Error 0.6474(0.6796) Steps 0(0.00) | Grad Norm 4.7076(14.9708) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 271.8748(272.1135) | Bit/dim 4.7225(4.9786) | Xent 1.8519(1.9339) | Loss 11.9748(13.0537) | Error 0.6664(0.6792) Steps 0(0.00) | Grad Norm 14.8160(14.9661) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 57.7150, Epoch Time 1801.4879(1534.7106), Bit/dim 4.7835(best: 4.6799), Xent 1.7956, Loss 5.6813, Error 0.6218(best: 0.6302)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 273.3379(272.1502) | Bit/dim 4.7805(4.9727) | Xent 1.8201(1.9305) | Loss 14.9203(13.1097) | Error 0.6335(0.6778) Steps 0(0.00) | Grad Norm 13.5962(14.9250) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 283.8901(272.5024) | Bit/dim 4.6857(4.9641) | Xent 1.8309(1.9275) | Loss 11.9824(13.0759) | Error 0.6509(0.6770) Steps 0(0.00) | Grad Norm 12.4416(14.8505) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 296.3616(273.2182) | Bit/dim 4.7152(4.9566) | Xent 1.8340(1.9247) | Loss 12.0548(13.0452) | Error 0.6509(0.6762) Steps 0(0.00) | Grad Norm 15.4366(14.8681) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 274.1150(273.2451) | Bit/dim 4.6605(4.9477) | Xent 1.8434(1.9222) | Loss 12.0120(13.0142) | Error 0.6490(0.6754) Steps 0(0.00) | Grad Norm 14.5652(14.8590) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 279.3919(273.4295) | Bit/dim 4.6908(4.9400) | Xent 1.8776(1.9209) | Loss 12.0617(12.9857) | Error 0.6645(0.6751) Steps 0(0.00) | Grad Norm 20.4091(15.0255) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 282.9091(273.7139) | Bit/dim 4.6514(4.9314) | Xent 1.7984(1.9172) | Loss 11.8273(12.9509) | Error 0.6339(0.6739) Steps 0(0.00) | Grad Norm 8.5414(14.8310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 53.7757, Epoch Time 1759.8968(1541.4662), Bit/dim 4.6435(best: 4.6799), Xent 1.8107, Loss 5.5489, Error 0.6419(best: 0.6218)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 274.0365(273.7236) | Bit/dim 4.6359(4.9225) | Xent 1.8529(1.9153) | Loss 14.6026(13.0005) | Error 0.6611(0.6735) Steps 0(0.00) | Grad Norm 19.9373(14.9842) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 271.5342(273.6579) | Bit/dim 4.6346(4.9139) | Xent 1.8597(1.9136) | Loss 11.8340(12.9655) | Error 0.6710(0.6734) Steps 0(0.00) | Grad Norm 17.2899(15.0534) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 272.1005(273.6112) | Bit/dim 4.7096(4.9077) | Xent 1.8205(1.9108) | Loss 12.0525(12.9381) | Error 0.6354(0.6723) Steps 0(0.00) | Grad Norm 11.3907(14.9435) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 268.5442(273.4591) | Bit/dim 4.7059(4.9017) | Xent 1.8423(1.9088) | Loss 12.0115(12.9103) | Error 0.6549(0.6717) Steps 0(0.00) | Grad Norm 15.5603(14.9620) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 258.1526(272.9999) | Bit/dim 4.6172(4.8931) | Xent 1.8697(1.9076) | Loss 11.8579(12.8787) | Error 0.6611(0.6714) Steps 0(0.00) | Grad Norm 16.4394(15.0063) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 291.8860(273.5665) | Bit/dim 4.6407(4.8856) | Xent 1.9154(1.9078) | Loss 11.9654(12.8513) | Error 0.6706(0.6714) Steps 0(0.00) | Grad Norm 23.4803(15.2605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 54.1844, Epoch Time 1706.2139(1546.4087), Bit/dim 4.6759(best: 4.6435), Xent 1.7827, Loss 5.5672, Error 0.6197(best: 0.6218)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 275.7091(273.6308) | Bit/dim 4.6774(4.8793) | Xent 1.8224(1.9053) | Loss 14.7977(12.9097) | Error 0.6390(0.6704) Steps 0(0.00) | Grad Norm 14.5892(15.2404) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 303.5344(274.5279) | Bit/dim 4.6024(4.8710) | Xent 1.7974(1.9020) | Loss 11.7750(12.8757) | Error 0.6336(0.6693) Steps 0(0.00) | Grad Norm 8.4761(15.0375) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 282.6495(274.7716) | Bit/dim 4.6509(4.8644) | Xent 1.8112(1.8993) | Loss 11.9564(12.8481) | Error 0.6381(0.6684) Steps 0(0.00) | Grad Norm 14.2425(15.0136) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 270.8581(274.6542) | Bit/dim 4.6358(4.8576) | Xent 1.8037(1.8964) | Loss 11.8983(12.8196) | Error 0.6325(0.6673) Steps 0(0.00) | Grad Norm 8.7335(14.8252) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 269.7222(274.5062) | Bit/dim 4.6298(4.8507) | Xent 1.7721(1.8927) | Loss 11.8526(12.7906) | Error 0.6209(0.6659) Steps 0(0.00) | Grad Norm 8.0038(14.6206) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 293.2818(275.0695) | Bit/dim 4.6127(4.8436) | Xent 1.8008(1.8899) | Loss 11.8262(12.7616) | Error 0.6407(0.6652) Steps 0(0.00) | Grad Norm 15.1790(14.6373) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 58.4355, Epoch Time 1769.8972(1553.1133), Bit/dim 4.5987(best: 4.6435), Xent 1.7405, Loss 5.4690, Error 0.6112(best: 0.6197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 276.6852(275.1179) | Bit/dim 4.5988(4.8362) | Xent 1.7810(1.8867) | Loss 14.6993(12.8198) | Error 0.6246(0.6639) Steps 0(0.00) | Grad Norm 16.8593(14.7040) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 284.7577(275.4071) | Bit/dim 4.6152(4.8296) | Xent 1.8998(1.8871) | Loss 12.0817(12.7976) | Error 0.6663(0.6640) Steps 0(0.00) | Grad Norm 25.2465(15.0202) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 317.8062(276.6791) | Bit/dim 4.5886(4.8224) | Xent 1.7777(1.8838) | Loss 11.8127(12.7681) | Error 0.6344(0.6631) Steps 0(0.00) | Grad Norm 11.7301(14.9215) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 292.3261(277.1485) | Bit/dim 4.5531(4.8143) | Xent 1.7472(1.8797) | Loss 11.6400(12.7342) | Error 0.6242(0.6620) Steps 0(0.00) | Grad Norm 8.9434(14.7422) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 277.3084(277.1533) | Bit/dim 4.5878(4.8075) | Xent 1.7948(1.8771) | Loss 11.8299(12.7071) | Error 0.6355(0.6612) Steps 0(0.00) | Grad Norm 22.1685(14.9650) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 283.0748(277.3310) | Bit/dim 4.5608(4.8001) | Xent 1.8245(1.8756) | Loss 11.8197(12.6805) | Error 0.6458(0.6607) Steps 0(0.00) | Grad Norm 20.8723(15.1422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 54.5976, Epoch Time 1802.6819(1560.6004), Bit/dim 4.5439(best: 4.5987), Xent 1.7710, Loss 5.4294, Error 0.6342(best: 0.6112)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_15_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
