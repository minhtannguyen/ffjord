{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_001_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.001, val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 99.7495(99.7495) | Bit/dim 8.8973(8.8973) | Xent 2.3026(2.3026) | Loss 20.9451(20.9451) | Error 0.8982(0.8982) Steps 430(430.00) | Grad Norm 27.4564(27.4564) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 41.3602(97.9978) | Bit/dim 8.8293(8.8952) | Xent 2.2924(2.3023) | Loss 20.7808(20.9402) | Error 0.7829(0.8948) Steps 478(431.44) | Grad Norm 24.9287(27.3805) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 40.1006(96.2609) | Bit/dim 8.7462(8.8907) | Xent 2.2828(2.3017) | Loss 20.6535(20.9316) | Error 0.7724(0.8911) Steps 490(433.20) | Grad Norm 20.9327(27.1871) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 38.4450(94.5264) | Bit/dim 8.6335(8.8830) | Xent 2.2729(2.3008) | Loss 19.9307(20.9016) | Error 0.7665(0.8874) Steps 466(434.18) | Grad Norm 16.2285(26.8583) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 36.8340(92.7956) | Bit/dim 8.5957(8.8744) | Xent 2.2637(2.2997) | Loss 20.0911(20.8772) | Error 0.7696(0.8838) Steps 442(434.42) | Grad Norm 11.0827(26.3851) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 34.8192(91.0563) | Bit/dim 8.4962(8.8631) | Xent 2.2510(2.2983) | Loss 19.8126(20.8453) | Error 0.7471(0.8797) Steps 454(435.00) | Grad Norm 8.2653(25.8415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 36.5897, Epoch Time 343.0306(343.0306), Bit/dim 8.4569(best: inf), Xent 2.2392, Loss 9.5765, Error 0.7432(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 46.8746(89.7309) | Bit/dim 8.4646(8.8511) | Xent 2.2404(2.2965) | Loss 22.3966(20.8918) | Error 0.7518(0.8759) Steps 460(435.75) | Grad Norm 8.2461(25.3136) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 38.6285(88.1978) | Bit/dim 8.4311(8.8385) | Xent 2.2321(2.2946) | Loss 19.9063(20.8623) | Error 0.7551(0.8723) Steps 454(436.30) | Grad Norm 11.1117(24.8876) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 34.9361(86.6000) | Bit/dim 8.4161(8.8258) | Xent 2.2200(2.2924) | Loss 19.3621(20.8173) | Error 0.7638(0.8690) Steps 448(436.65) | Grad Norm 13.5434(24.5472) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 34.6020(85.0400) | Bit/dim 8.3771(8.8124) | Xent 2.2119(2.2899) | Loss 19.6008(20.7808) | Error 0.7770(0.8663) Steps 454(437.17) | Grad Norm 14.8672(24.2568) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 34.2168(83.5153) | Bit/dim 8.3439(8.7983) | Xent 2.2043(2.2874) | Loss 19.5548(20.7440) | Error 0.7744(0.8635) Steps 430(436.96) | Grad Norm 15.0612(23.9810) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 35.3643(82.0708) | Bit/dim 8.2522(8.7819) | Xent 2.1947(2.2846) | Loss 19.4957(20.7065) | Error 0.7731(0.8608) Steps 466(437.83) | Grad Norm 13.6059(23.6697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 18.6611, Epoch Time 259.0027(340.5098), Bit/dim 8.1890(best: 8.4569), Xent 2.1765, Loss 9.2772, Error 0.7500(best: 0.7432)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 39.4368(80.7918) | Bit/dim 8.2281(8.7653) | Xent 2.1762(2.2813) | Loss 21.6925(20.7361) | Error 0.7560(0.8577) Steps 490(439.39) | Grad Norm 11.4140(23.3020) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 35.5713(79.4352) | Bit/dim 8.1163(8.7458) | Xent 2.1698(2.2780) | Loss 18.8774(20.6804) | Error 0.7476(0.8544) Steps 430(439.11) | Grad Norm 8.2510(22.8505) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 36.3101(78.1414) | Bit/dim 8.0772(8.7258) | Xent 2.1571(2.2744) | Loss 18.5245(20.6157) | Error 0.7332(0.8507) Steps 454(439.56) | Grad Norm 6.2629(22.3529) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 39.2677(76.9752) | Bit/dim 7.9710(8.7031) | Xent 2.1417(2.2704) | Loss 18.9025(20.5643) | Error 0.7312(0.8471) Steps 418(438.91) | Grad Norm 6.9714(21.8914) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 38.6490(75.8254) | Bit/dim 7.9225(8.6797) | Xent 2.1338(2.2663) | Loss 18.4775(20.5017) | Error 0.7299(0.8436) Steps 460(439.54) | Grad Norm 9.1535(21.5093) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 38.4662(74.7046) | Bit/dim 7.8678(8.6554) | Xent 2.1243(2.2620) | Loss 18.8385(20.4518) | Error 0.7265(0.8401) Steps 466(440.34) | Grad Norm 10.4105(21.1763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 18.2669, Epoch Time 261.7363(338.1466), Bit/dim 7.7818(best: 8.1890), Xent 2.1067, Loss 8.8351, Error 0.7173(best: 0.7432)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 41.6166(73.7120) | Bit/dim 7.7884(8.6294) | Xent 2.1154(2.2576) | Loss 21.2101(20.4745) | Error 0.7303(0.8368) Steps 490(441.83) | Grad Norm 10.2347(20.8481) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 41.0887(72.7333) | Bit/dim 7.6977(8.6014) | Xent 2.1031(2.2530) | Loss 18.2260(20.4071) | Error 0.7171(0.8332) Steps 442(441.83) | Grad Norm 8.2097(20.4689) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 37.8756(71.6876) | Bit/dim 7.6180(8.5719) | Xent 2.0913(2.2481) | Loss 17.5421(20.3211) | Error 0.6986(0.8292) Steps 436(441.66) | Grad Norm 5.3141(20.0143) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 38.6727(70.6971) | Bit/dim 7.5309(8.5407) | Xent 2.0784(2.2430) | Loss 17.9399(20.2497) | Error 0.6999(0.8253) Steps 454(442.03) | Grad Norm 4.7540(19.5565) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 36.3767(69.6675) | Bit/dim 7.4545(8.5081) | Xent 2.0806(2.2382) | Loss 17.1932(20.1580) | Error 0.7120(0.8219) Steps 424(441.49) | Grad Norm 6.6108(19.1681) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 39.8157(68.7719) | Bit/dim 7.3977(8.4748) | Xent 2.0829(2.2335) | Loss 17.7357(20.0853) | Error 0.7200(0.8188) Steps 442(441.50) | Grad Norm 9.4071(18.8753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 19.8304, Epoch Time 271.3800(336.1436), Bit/dim 7.3369(best: 7.7818), Xent 2.0661, Loss 8.3699, Error 0.7010(best: 0.7173)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 44.0466(68.0302) | Bit/dim 7.3385(8.4407) | Xent 2.0635(2.2284) | Loss 20.4086(20.0950) | Error 0.7032(0.8154) Steps 424(440.98) | Grad Norm 8.8935(18.5758) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 42.0709(67.2514) | Bit/dim 7.2747(8.4057) | Xent 2.0622(2.2234) | Loss 17.4336(20.0152) | Error 0.7040(0.8120) Steps 484(442.27) | Grad Norm 6.6620(18.2184) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 39.6763(66.4242) | Bit/dim 7.2147(8.3700) | Xent 2.0518(2.2183) | Loss 17.4394(19.9379) | Error 0.6843(0.8082) Steps 466(442.98) | Grad Norm 3.7327(17.7838) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 39.2658(65.6094) | Bit/dim 7.1789(8.3342) | Xent 2.0530(2.2133) | Loss 17.2097(19.8561) | Error 0.6887(0.8046) Steps 478(444.03) | Grad Norm 5.0546(17.4020) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 44.0042(64.9613) | Bit/dim 7.1536(8.2988) | Xent 2.0559(2.2086) | Loss 17.4410(19.7836) | Error 0.6955(0.8013) Steps 478(445.05) | Grad Norm 7.6013(17.1079) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 38.3717(64.1636) | Bit/dim 7.1056(8.2630) | Xent 2.0524(2.2039) | Loss 16.9543(19.6987) | Error 0.6967(0.7982) Steps 460(445.50) | Grad Norm 6.0500(16.7762) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 19.6729, Epoch Time 283.3463(334.5596), Bit/dim 7.0838(best: 7.3369), Xent 2.0359, Loss 8.1017, Error 0.6761(best: 0.7010)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 38.3741(63.3899) | Bit/dim 7.0839(8.2277) | Xent 2.0511(2.1993) | Loss 19.5080(19.6930) | Error 0.6961(0.7951) Steps 436(445.21) | Grad Norm 2.5917(16.3507) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 39.5400(62.6744) | Bit/dim 7.0666(8.1928) | Xent 2.0625(2.1952) | Loss 16.9303(19.6101) | Error 0.7156(0.7928) Steps 466(445.84) | Grad Norm 7.4519(16.0837) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 39.7192(61.9857) | Bit/dim 7.0484(8.1585) | Xent 2.0409(2.1906) | Loss 16.7001(19.5228) | Error 0.7184(0.7905) Steps 448(445.90) | Grad Norm 5.9415(15.7794) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 36.2670(61.2142) | Bit/dim 7.0346(8.1248) | Xent 2.0468(2.1863) | Loss 16.7606(19.4400) | Error 0.7145(0.7882) Steps 460(446.32) | Grad Norm 3.0274(15.3969) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 38.4928(60.5325) | Bit/dim 7.0173(8.0915) | Xent 2.0439(2.1820) | Loss 16.9777(19.3661) | Error 0.7123(0.7860) Steps 496(447.81) | Grad Norm 6.3273(15.1248) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 39.3915(59.8983) | Bit/dim 7.0111(8.0591) | Xent 2.0460(2.1779) | Loss 16.7650(19.2881) | Error 0.7123(0.7838) Steps 454(448.00) | Grad Norm 4.0688(14.7931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 20.3666, Epoch Time 267.8081(332.5571), Bit/dim 7.0093(best: 7.0838), Xent 2.0306, Loss 8.0246, Error 0.7014(best: 0.6761)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 40.0537(59.3030) | Bit/dim 7.0064(8.0276) | Xent 2.0368(2.1737) | Loss 18.4510(19.2630) | Error 0.7163(0.7817) Steps 490(449.26) | Grad Norm 3.9238(14.4670) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 41.2658(58.7618) | Bit/dim 6.9956(7.9966) | Xent 2.0353(2.1695) | Loss 16.6127(19.1834) | Error 0.7209(0.7799) Steps 484(450.30) | Grad Norm 4.3652(14.1640) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 39.8510(58.1945) | Bit/dim 7.0001(7.9667) | Xent 2.0271(2.1653) | Loss 16.3103(19.0973) | Error 0.7070(0.7777) Steps 496(451.67) | Grad Norm 5.0882(13.8917) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 40.9330(57.6767) | Bit/dim 6.9931(7.9375) | Xent 2.0300(2.1612) | Loss 16.6814(19.0248) | Error 0.7085(0.7756) Steps 484(452.64) | Grad Norm 5.3179(13.6345) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 44.6787(57.2867) | Bit/dim 6.9854(7.9089) | Xent 2.0314(2.1573) | Loss 16.9766(18.9633) | Error 0.7087(0.7736) Steps 484(453.58) | Grad Norm 9.2557(13.5031) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 44.6228(56.9068) | Bit/dim 6.9770(7.8810) | Xent 2.0236(2.1533) | Loss 16.5317(18.8904) | Error 0.6990(0.7714) Steps 514(455.40) | Grad Norm 13.5409(13.5043) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 20.9423, Epoch Time 288.2476(331.2278), Bit/dim 6.9885(best: 7.0093), Xent 2.0322, Loss 8.0046, Error 0.7203(best: 0.6761)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 42.9464(56.4880) | Bit/dim 6.9787(7.8539) | Xent 2.0433(2.1500) | Loss 19.2999(18.9027) | Error 0.7312(0.7702) Steps 466(455.71) | Grad Norm 23.4077(13.8014) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 45.6372(56.1625) | Bit/dim 6.9706(7.8274) | Xent 2.0368(2.1466) | Loss 16.5982(18.8335) | Error 0.7152(0.7685) Steps 514(457.46) | Grad Norm 25.1614(14.1422) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 46.4671(55.8716) | Bit/dim 6.9636(7.8015) | Xent 2.0041(2.1423) | Loss 16.9688(18.7776) | Error 0.6950(0.7663) Steps 490(458.44) | Grad Norm 11.0839(14.0504) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 48.0020(55.6355) | Bit/dim 6.9575(7.7762) | Xent 1.9868(2.1377) | Loss 16.4608(18.7081) | Error 0.6830(0.7638) Steps 538(460.83) | Grad Norm 6.6024(13.8270) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 46.6315(55.3654) | Bit/dim 6.9572(7.7516) | Xent 2.0073(2.1338) | Loss 16.6413(18.6461) | Error 0.6956(0.7618) Steps 502(462.06) | Grad Norm 22.5479(14.0886) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 42.9500(54.9929) | Bit/dim 6.9436(7.7274) | Xent 2.0526(2.1313) | Loss 16.8485(18.5922) | Error 0.7243(0.7607) Steps 490(462.90) | Grad Norm 32.5577(14.6427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 20.5810, Epoch Time 309.3336(330.5710), Bit/dim 6.9344(best: 6.9885), Xent 1.9577, Loss 7.9132, Error 0.6656(best: 0.6761)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 43.2180(54.6397) | Bit/dim 6.9200(7.7031) | Xent 1.9731(2.1266) | Loss 18.8764(18.6007) | Error 0.6747(0.7581) Steps 472(463.17) | Grad Norm 6.8649(14.4093) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 47.7556(54.4332) | Bit/dim 6.9396(7.6802) | Xent 2.0821(2.1252) | Loss 16.7924(18.5464) | Error 0.7355(0.7574) Steps 490(463.98) | Grad Norm 40.5951(15.1949) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 42.6122(54.0785) | Bit/dim 6.9161(7.6573) | Xent 1.9915(2.1212) | Loss 16.4438(18.4834) | Error 0.6875(0.7553) Steps 472(464.22) | Grad Norm 14.7577(15.1818) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 43.5304(53.7621) | Bit/dim 6.9251(7.6353) | Xent 2.1223(2.1213) | Loss 15.9235(18.4066) | Error 0.7512(0.7552) Steps 502(465.35) | Grad Norm 43.5205(16.0320) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 48.2931(53.5980) | Bit/dim 6.9215(7.6139) | Xent 2.1091(2.1209) | Loss 16.7476(18.3568) | Error 0.7461(0.7549) Steps 544(467.71) | Grad Norm 36.6183(16.6496) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 44.3574(53.3208) | Bit/dim 6.8864(7.5921) | Xent 1.9726(2.1164) | Loss 16.3908(18.2978) | Error 0.6743(0.7525) Steps 496(468.56) | Grad Norm 17.7324(16.6820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 20.0442, Epoch Time 305.6839(329.8244), Bit/dim 6.8936(best: 6.9344), Xent 2.0781, Loss 7.9326, Error 0.7239(best: 0.6656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 44.8265(53.0660) | Bit/dim 6.8824(7.5708) | Xent 2.0825(2.1154) | Loss 19.2443(18.3262) | Error 0.7306(0.7518) Steps 502(469.56) | Grad Norm 23.7833(16.8951) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 47.1539(52.8886) | Bit/dim 6.8735(7.5499) | Xent 1.9572(2.1107) | Loss 16.5084(18.2717) | Error 0.6645(0.7492) Steps 508(470.72) | Grad Norm 9.8877(16.6849) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 44.2907(52.6307) | Bit/dim 6.8585(7.5291) | Xent 2.0337(2.1084) | Loss 16.1555(18.2082) | Error 0.7231(0.7484) Steps 508(471.83) | Grad Norm 18.4961(16.7392) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 44.3367(52.3819) | Bit/dim 6.8588(7.5090) | Xent 2.0406(2.1063) | Loss 16.4157(18.1544) | Error 0.7509(0.7485) Steps 478(472.02) | Grad Norm 31.3527(17.1776) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 43.9131(52.1278) | Bit/dim 6.8293(7.4886) | Xent 1.9677(2.1022) | Loss 16.3788(18.1011) | Error 0.6581(0.7458) Steps 496(472.74) | Grad Norm 5.9265(16.8401) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 40.8225(51.7886) | Bit/dim 6.8368(7.4691) | Xent 2.0475(2.1005) | Loss 16.4387(18.0513) | Error 0.7208(0.7450) Steps 478(472.90) | Grad Norm 28.8625(17.2007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.2670, Epoch Time 302.0158(328.9901), Bit/dim 6.8135(best: 6.8936), Xent 1.9483, Loss 7.7876, Error 0.6491(best: 0.6656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 43.9684(51.5540) | Bit/dim 6.8039(7.4491) | Xent 1.9555(2.0962) | Loss 18.7937(18.0735) | Error 0.6576(0.7424) Steps 478(473.05) | Grad Norm 7.7271(16.9165) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 42.6976(51.2883) | Bit/dim 6.7927(7.4294) | Xent 1.9680(2.0923) | Loss 16.0532(18.0129) | Error 0.6735(0.7404) Steps 478(473.20) | Grad Norm 8.4981(16.6640) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 41.9815(51.0091) | Bit/dim 6.7898(7.4103) | Xent 1.9745(2.0888) | Loss 15.7956(17.9464) | Error 0.6755(0.7384) Steps 478(473.34) | Grad Norm 14.8614(16.6099) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 41.2299(50.7158) | Bit/dim 6.7703(7.3911) | Xent 1.9792(2.0855) | Loss 16.3687(17.8991) | Error 0.6934(0.7371) Steps 460(472.94) | Grad Norm 11.9091(16.4689) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 39.6355(50.3834) | Bit/dim 6.7332(7.3713) | Xent 1.9566(2.0817) | Loss 16.1585(17.8469) | Error 0.6610(0.7348) Steps 478(473.09) | Grad Norm 3.5321(16.0808) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 47.7981(50.3058) | Bit/dim 6.7325(7.3522) | Xent 1.9521(2.0778) | Loss 16.2546(17.7991) | Error 0.6750(0.7330) Steps 490(473.60) | Grad Norm 10.9257(15.9261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 20.2135, Epoch Time 293.2185(327.9170), Bit/dim 6.7091(best: 6.8135), Xent 1.9300, Loss 7.6741, Error 0.6615(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 40.7624(50.0195) | Bit/dim 6.7073(7.3328) | Xent 1.9502(2.0739) | Loss 18.4688(17.8192) | Error 0.6733(0.7312) Steps 472(473.55) | Grad Norm 11.6978(15.7993) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 44.1087(49.8422) | Bit/dim 6.6655(7.3128) | Xent 1.9446(2.0701) | Loss 16.3202(17.7742) | Error 0.6634(0.7292) Steps 520(474.95) | Grad Norm 13.8221(15.7400) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 40.4076(49.5591) | Bit/dim 6.6197(7.2920) | Xent 1.9353(2.0660) | Loss 16.0195(17.7216) | Error 0.6649(0.7272) Steps 466(474.68) | Grad Norm 12.4218(15.6404) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 43.8435(49.3877) | Bit/dim 6.6048(7.2714) | Xent 1.9173(2.0616) | Loss 16.0982(17.6729) | Error 0.6415(0.7247) Steps 490(475.14) | Grad Norm 7.2062(15.3874) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 45.0079(49.2563) | Bit/dim 6.5889(7.2509) | Xent 1.9077(2.0569) | Loss 15.7906(17.6164) | Error 0.6500(0.7224) Steps 490(475.58) | Grad Norm 8.9141(15.1932) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 39.7504(48.9711) | Bit/dim 6.5242(7.2291) | Xent 1.9456(2.0536) | Loss 15.7622(17.5608) | Error 0.6731(0.7209) Steps 478(475.66) | Grad Norm 22.1238(15.4011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 20.3169, Epoch Time 290.0763(326.7817), Bit/dim 6.5007(best: 6.7091), Xent 2.0750, Loss 7.5382, Error 0.7645(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 46.6726(48.9021) | Bit/dim 6.4961(7.2071) | Xent 2.1110(2.0553) | Loss 17.7928(17.5677) | Error 0.7618(0.7222) Steps 514(476.81) | Grad Norm 72.0616(17.1009) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 44.4585(48.7688) | Bit/dim 6.4348(7.1839) | Xent 1.9494(2.0521) | Loss 15.7039(17.5118) | Error 0.6929(0.7213) Steps 478(476.84) | Grad Norm 29.7199(17.4795) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 42.5542(48.5824) | Bit/dim 6.5490(7.1649) | Xent 2.4993(2.0656) | Loss 15.7983(17.4604) | Error 0.7913(0.7234) Steps 460(476.34) | Grad Norm 141.3597(21.1959) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 40.6214(48.3436) | Bit/dim 6.6753(7.1502) | Xent 2.5929(2.0814) | Loss 16.7317(17.4386) | Error 0.8115(0.7260) Steps 484(476.57) | Grad Norm 140.8807(24.7864) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 44.5272(48.2291) | Bit/dim 6.4072(7.1279) | Xent 2.3450(2.0893) | Loss 15.7683(17.3885) | Error 0.7960(0.7281) Steps 514(477.69) | Grad Norm 86.0183(26.6234) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 43.3686(48.0833) | Bit/dim 6.2864(7.1027) | Xent 1.9302(2.0845) | Loss 15.3213(17.3264) | Error 0.6699(0.7264) Steps 478(477.70) | Grad Norm 43.5776(27.1320) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 19.9984, Epoch Time 298.0577(325.9200), Bit/dim 6.3306(best: 6.5007), Xent 2.4522, Loss 7.5566, Error 0.8143(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 42.7977(47.9247) | Bit/dim 6.3311(7.0795) | Xent 2.4644(2.0959) | Loss 18.5625(17.3635) | Error 0.8065(0.7288) Steps 502(478.43) | Grad Norm 105.2078(29.4743) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 46.0723(47.8691) | Bit/dim 6.2934(7.0559) | Xent 2.2351(2.1001) | Loss 15.7990(17.3166) | Error 0.7737(0.7301) Steps 466(478.06) | Grad Norm 79.8068(30.9843) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 41.8464(47.6884) | Bit/dim 6.2101(7.0306) | Xent 2.2581(2.1048) | Loss 15.4786(17.2614) | Error 0.7735(0.7314) Steps 478(478.05) | Grad Norm 136.7158(34.1562) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 44.2427(47.5851) | Bit/dim 6.1689(7.0047) | Xent 2.3581(2.1124) | Loss 15.2898(17.2023) | Error 0.8083(0.7337) Steps 466(477.69) | Grad Norm 132.2327(37.0985) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 40.0427(47.3588) | Bit/dim 6.2868(6.9832) | Xent 2.1734(2.1143) | Loss 15.3720(17.1474) | Error 0.7561(0.7344) Steps 436(476.44) | Grad Norm 118.1150(39.5290) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 40.3018(47.1471) | Bit/dim 6.2271(6.9605) | Xent 2.2392(2.1180) | Loss 15.3859(17.0945) | Error 0.7685(0.7354) Steps 496(477.03) | Grad Norm 129.2684(42.2212) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 20.1816, Epoch Time 291.5924(324.8902), Bit/dim 6.0435(best: 6.3306), Xent 2.0586, Loss 7.0728, Error 0.7511(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 43.0464(47.0241) | Bit/dim 6.0423(6.9330) | Xent 2.0628(2.1163) | Loss 17.1509(17.0962) | Error 0.7421(0.7356) Steps 472(476.88) | Grad Norm 49.2432(42.4319) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 43.9707(46.9325) | Bit/dim 6.0121(6.9053) | Xent 2.0134(2.1133) | Loss 14.6757(17.0236) | Error 0.7134(0.7350) Steps 496(477.45) | Grad Norm 26.3366(41.9490) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 42.3997(46.7965) | Bit/dim 6.0381(6.8793) | Xent 2.3535(2.1205) | Loss 15.3953(16.9748) | Error 0.7849(0.7365) Steps 502(478.19) | Grad Norm 103.1286(43.7844) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 43.7686(46.7056) | Bit/dim 6.0066(6.8531) | Xent 2.2408(2.1241) | Loss 14.9403(16.9137) | Error 0.7638(0.7373) Steps 472(478.00) | Grad Norm 76.0522(44.7524) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 43.2556(46.6021) | Bit/dim 5.9974(6.8275) | Xent 2.0355(2.1214) | Loss 14.5953(16.8442) | Error 0.7090(0.7364) Steps 490(478.36) | Grad Norm 41.1180(44.6434) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 43.4537(46.5077) | Bit/dim 5.9962(6.8025) | Xent 2.0788(2.1201) | Loss 14.8101(16.7832) | Error 0.7356(0.7364) Steps 508(479.25) | Grad Norm 38.9116(44.4714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 20.0033, Epoch Time 295.9264(324.0213), Bit/dim 5.9199(best: 6.0435), Xent 2.0137, Loss 6.9267, Error 0.7001(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 43.3402(46.4127) | Bit/dim 5.9252(6.7762) | Xent 2.0355(2.1176) | Loss 16.8053(16.7838) | Error 0.7132(0.7357) Steps 484(479.39) | Grad Norm 34.5497(44.1738) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 43.4853(46.3248) | Bit/dim 6.0065(6.7531) | Xent 2.0059(2.1142) | Loss 14.4969(16.7152) | Error 0.6946(0.7345) Steps 490(479.71) | Grad Norm 25.8929(43.6254) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 44.8116(46.2794) | Bit/dim 5.9303(6.7284) | Xent 2.0093(2.1111) | Loss 14.5848(16.6513) | Error 0.6965(0.7333) Steps 478(479.66) | Grad Norm 27.0920(43.1294) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 44.0410(46.2123) | Bit/dim 5.8717(6.7027) | Xent 2.0334(2.1088) | Loss 14.4894(16.5864) | Error 0.7163(0.7328) Steps 496(480.15) | Grad Norm 40.6953(43.0563) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 41.9567(46.0846) | Bit/dim 5.8333(6.6766) | Xent 1.9961(2.1054) | Loss 14.4583(16.5226) | Error 0.6790(0.7312) Steps 490(480.45) | Grad Norm 11.6052(42.1128) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 46.3269(46.0919) | Bit/dim 5.8465(6.6517) | Xent 2.0855(2.1048) | Loss 14.4154(16.4594) | Error 0.7356(0.7313) Steps 496(480.91) | Grad Norm 60.8191(42.6740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 20.1373, Epoch Time 300.3414(323.3109), Bit/dim 5.8600(best: 5.9199), Xent 1.9776, Loss 6.8488, Error 0.6654(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 41.2210(45.9458) | Bit/dim 5.8528(6.6278) | Xent 1.9901(2.1014) | Loss 17.1996(16.4816) | Error 0.6829(0.7299) Steps 478(480.83) | Grad Norm 25.3625(42.1546) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 39.0956(45.7403) | Bit/dim 5.9302(6.6068) | Xent 2.1712(2.1034) | Loss 14.6001(16.4251) | Error 0.7833(0.7315) Steps 490(481.10) | Grad Norm 106.0950(44.0729) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 45.0502(45.7196) | Bit/dim 6.0609(6.5905) | Xent 2.1518(2.1049) | Loss 15.0437(16.3837) | Error 0.7762(0.7328) Steps 472(480.83) | Grad Norm 98.3777(45.7020) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 42.9848(45.6375) | Bit/dim 5.8608(6.5686) | Xent 1.9860(2.1013) | Loss 14.4257(16.3250) | Error 0.6823(0.7313) Steps 472(480.56) | Grad Norm 17.9814(44.8704) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 42.5515(45.5449) | Bit/dim 5.7604(6.5443) | Xent 2.0137(2.0987) | Loss 14.4590(16.2690) | Error 0.7049(0.7305) Steps 442(479.41) | Grad Norm 26.8153(44.3287) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 38.3841(45.3301) | Bit/dim 5.8883(6.5246) | Xent 1.9817(2.0952) | Loss 14.1266(16.2047) | Error 0.6846(0.7291) Steps 454(478.64) | Grad Norm 16.7541(43.5015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 19.8092, Epoch Time 285.2970(322.1705), Bit/dim 5.7474(best: 5.8600), Xent 2.0407, Loss 6.7678, Error 0.7331(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 42.3656(45.2412) | Bit/dim 5.7534(6.5015) | Xent 2.0622(2.0942) | Loss 16.6926(16.2194) | Error 0.7370(0.7294) Steps 448(477.72) | Grad Norm 29.3693(43.0775) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 39.3377(45.0641) | Bit/dim 5.6715(6.4766) | Xent 1.9853(2.0909) | Loss 13.8329(16.1478) | Error 0.6747(0.7277) Steps 460(477.19) | Grad Norm 7.5535(42.0118) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 41.2529(44.9497) | Bit/dim 5.7119(6.4537) | Xent 2.0866(2.0908) | Loss 14.1854(16.0889) | Error 0.7304(0.7278) Steps 472(477.04) | Grad Norm 38.4833(41.9059) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 38.6130(44.7596) | Bit/dim 5.7214(6.4317) | Xent 2.0024(2.0881) | Loss 14.0707(16.0283) | Error 0.6944(0.7268) Steps 466(476.71) | Grad Norm 19.3916(41.2305) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 39.7526(44.6094) | Bit/dim 5.6629(6.4086) | Xent 2.1213(2.0891) | Loss 14.2334(15.9745) | Error 0.7569(0.7277) Steps 460(476.20) | Grad Norm 55.9772(41.6729) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 42.6802(44.5515) | Bit/dim 5.6720(6.3865) | Xent 2.0550(2.0881) | Loss 14.2642(15.9232) | Error 0.7274(0.7277) Steps 460(475.72) | Grad Norm 33.5476(41.4292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 20.0204, Epoch Time 279.5657(320.8923), Bit/dim 5.6577(best: 5.7474), Xent 1.9742, Loss 6.6448, Error 0.6769(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 40.0887(44.4177) | Bit/dim 5.6595(6.3647) | Xent 1.9883(2.0851) | Loss 16.9456(15.9539) | Error 0.6910(0.7266) Steps 490(476.15) | Grad Norm 12.6514(40.5658) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 38.4884(44.2398) | Bit/dim 5.6404(6.3430) | Xent 2.0046(2.0827) | Loss 13.5716(15.8824) | Error 0.7015(0.7259) Steps 448(475.30) | Grad Norm 20.3981(39.9608) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 39.0399(44.0838) | Bit/dim 5.6373(6.3218) | Xent 1.9767(2.0795) | Loss 14.0550(15.8276) | Error 0.6923(0.7248) Steps 442(474.30) | Grad Norm 17.9274(39.2998) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 38.8983(43.9282) | Bit/dim 5.5593(6.2989) | Xent 1.9908(2.0769) | Loss 14.0218(15.7734) | Error 0.6796(0.7235) Steps 472(474.23) | Grad Norm 8.7686(38.3839) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 42.2882(43.8790) | Bit/dim 5.5604(6.2768) | Xent 1.9859(2.0741) | Loss 13.7692(15.7133) | Error 0.6834(0.7223) Steps 448(473.45) | Grad Norm 15.4603(37.6961) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 41.5036(43.8078) | Bit/dim 5.6058(6.2567) | Xent 2.0449(2.0733) | Loss 14.0316(15.6628) | Error 0.7154(0.7221) Steps 448(472.68) | Grad Norm 57.2065(38.2815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 19.3078, Epoch Time 275.7443(319.5379), Bit/dim 5.5462(best: 5.6577), Xent 1.9388, Loss 6.5156, Error 0.6689(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 43.6259(43.8023) | Bit/dim 5.5430(6.2352) | Xent 1.9846(2.0706) | Loss 16.5480(15.6894) | Error 0.6946(0.7213) Steps 448(471.94) | Grad Norm 12.6799(37.5134) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 43.8527(43.8038) | Bit/dim 5.4972(6.2131) | Xent 1.9531(2.0671) | Loss 13.4539(15.6223) | Error 0.6701(0.7197) Steps 478(472.12) | Grad Norm 7.4588(36.6118) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 42.0978(43.7526) | Bit/dim 5.4862(6.1913) | Xent 2.0192(2.0656) | Loss 13.5535(15.5602) | Error 0.7050(0.7193) Steps 472(472.12) | Grad Norm 38.6508(36.6729) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 39.8785(43.6364) | Bit/dim 5.5246(6.1713) | Xent 1.9811(2.0631) | Loss 13.6230(15.5021) | Error 0.6992(0.7187) Steps 484(472.48) | Grad Norm 39.7063(36.7639) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 37.8703(43.4634) | Bit/dim 5.5036(6.1513) | Xent 1.9545(2.0599) | Loss 13.7948(15.4509) | Error 0.6693(0.7172) Steps 448(471.74) | Grad Norm 8.2006(35.9070) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 39.1849(43.3351) | Bit/dim 5.5040(6.1318) | Xent 2.0787(2.0604) | Loss 13.9464(15.4058) | Error 0.7215(0.7173) Steps 454(471.21) | Grad Norm 54.4909(36.4646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 19.2628, Epoch Time 282.0980(318.4147), Bit/dim 5.4244(best: 5.5462), Xent 1.9327, Loss 6.3907, Error 0.6580(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 41.2449(43.2724) | Bit/dim 5.4208(6.1105) | Xent 1.9529(2.0572) | Loss 16.1295(15.4275) | Error 0.6715(0.7160) Steps 454(470.69) | Grad Norm 14.7227(35.8123) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 40.0940(43.1770) | Bit/dim 5.4739(6.0914) | Xent 2.1312(2.0594) | Loss 13.9477(15.3831) | Error 0.7550(0.7171) Steps 454(470.19) | Grad Norm 68.1799(36.7833) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 43.9100(43.1990) | Bit/dim 5.4373(6.0718) | Xent 2.0914(2.0604) | Loss 13.8184(15.3362) | Error 0.7274(0.7174) Steps 472(470.25) | Grad Norm 47.7002(37.1108) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 42.3545(43.1737) | Bit/dim 5.3946(6.0515) | Xent 1.9731(2.0578) | Loss 13.4282(15.2789) | Error 0.6833(0.7164) Steps 430(469.04) | Grad Norm 25.7359(36.7696) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 38.3654(43.0294) | Bit/dim 5.4422(6.0332) | Xent 2.0300(2.0569) | Loss 13.1222(15.2142) | Error 0.7151(0.7164) Steps 466(468.95) | Grad Norm 29.8719(36.5627) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 40.0425(42.9398) | Bit/dim 5.3583(6.0130) | Xent 1.9684(2.0543) | Loss 13.4229(15.1605) | Error 0.6753(0.7151) Steps 448(468.32) | Grad Norm 14.6639(35.9057) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 19.2700, Epoch Time 281.4796(317.3066), Bit/dim 5.4729(best: 5.4244), Xent 2.1499, Loss 6.5478, Error 0.7677(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 42.5836(42.9291) | Bit/dim 5.4786(5.9969) | Xent 2.1828(2.0581) | Loss 16.5319(15.2016) | Error 0.7604(0.7165) Steps 448(467.71) | Grad Norm 70.0296(36.9294) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 40.2654(42.8492) | Bit/dim 5.3818(5.9785) | Xent 2.0611(2.0582) | Loss 13.5315(15.1515) | Error 0.7355(0.7171) Steps 460(467.48) | Grad Norm 36.0911(36.9043) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 40.7727(42.7869) | Bit/dim 5.4574(5.9628) | Xent 2.0009(2.0565) | Loss 13.6008(15.1050) | Error 0.7064(0.7167) Steps 478(467.80) | Grad Norm 31.3595(36.7379) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 40.3758(42.7146) | Bit/dim 5.4149(5.9464) | Xent 2.0130(2.0552) | Loss 13.3394(15.0520) | Error 0.7180(0.7168) Steps 448(467.20) | Grad Norm 23.8377(36.3509) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 43.9894(42.7528) | Bit/dim 5.2992(5.9270) | Xent 2.0532(2.0551) | Loss 13.5207(15.0061) | Error 0.7369(0.7174) Steps 478(467.53) | Grad Norm 19.5599(35.8472) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 42.4040(42.7424) | Bit/dim 5.3105(5.9085) | Xent 2.0329(2.0545) | Loss 13.5109(14.9612) | Error 0.7170(0.7174) Steps 466(467.48) | Grad Norm 28.4500(35.6253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 19.2175, Epoch Time 285.6567(316.3571), Bit/dim 5.2505(best: 5.4244), Xent 1.9965, Loss 6.2488, Error 0.6971(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 42.3388(42.7303) | Bit/dim 5.2432(5.8885) | Xent 2.0197(2.0534) | Loss 15.7646(14.9853) | Error 0.7127(0.7172) Steps 454(467.08) | Grad Norm 10.3527(34.8671) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 41.1356(42.6824) | Bit/dim 5.2838(5.8704) | Xent 1.9936(2.0516) | Loss 13.3046(14.9349) | Error 0.6990(0.7167) Steps 460(466.86) | Grad Norm 17.2135(34.3375) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 42.3369(42.6721) | Bit/dim 5.2138(5.8507) | Xent 1.9765(2.0494) | Loss 13.1335(14.8809) | Error 0.6910(0.7159) Steps 454(466.48) | Grad Norm 5.7401(33.4796) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 44.3741(42.7231) | Bit/dim 5.2473(5.8326) | Xent 2.0152(2.0483) | Loss 13.2563(14.8321) | Error 0.7124(0.7158) Steps 472(466.64) | Grad Norm 25.3660(33.2362) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 41.4303(42.6843) | Bit/dim 5.2306(5.8145) | Xent 2.0060(2.0471) | Loss 13.1197(14.7808) | Error 0.6915(0.7151) Steps 466(466.62) | Grad Norm 8.0609(32.4809) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 44.6502(42.7433) | Bit/dim 5.2327(5.7971) | Xent 1.9588(2.0444) | Loss 13.1789(14.7327) | Error 0.6740(0.7138) Steps 454(466.24) | Grad Norm 13.4311(31.9094) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 19.7240, Epoch Time 291.9337(315.6244), Bit/dim 5.1589(best: 5.2505), Xent 1.9491, Loss 6.1334, Error 0.6682(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 44.3495(42.7915) | Bit/dim 5.1539(5.7778) | Xent 1.9630(2.0420) | Loss 15.8354(14.7658) | Error 0.6923(0.7132) Steps 478(466.60) | Grad Norm 3.9623(31.0710) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 43.6574(42.8175) | Bit/dim 5.2168(5.7609) | Xent 1.9781(2.0401) | Loss 13.4371(14.7259) | Error 0.6853(0.7124) Steps 466(466.58) | Grad Norm 23.7338(30.8509) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 44.0715(42.8551) | Bit/dim 5.1743(5.7433) | Xent 1.9636(2.0378) | Loss 12.8744(14.6704) | Error 0.6964(0.7119) Steps 472(466.74) | Grad Norm 17.0226(30.4360) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 42.2039(42.8356) | Bit/dim 5.1710(5.7262) | Xent 1.9504(2.0352) | Loss 12.5952(14.6081) | Error 0.6773(0.7108) Steps 472(466.90) | Grad Norm 10.1148(29.8264) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 44.4740(42.8847) | Bit/dim 5.1124(5.7078) | Xent 2.0139(2.0345) | Loss 13.2167(14.5664) | Error 0.7175(0.7110) Steps 478(467.23) | Grad Norm 16.0706(29.4137) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 43.2383(42.8953) | Bit/dim 5.1018(5.6896) | Xent 2.0037(2.0336) | Loss 12.8809(14.5158) | Error 0.7072(0.7109) Steps 478(467.56) | Grad Norm 10.5658(28.8483) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 20.2635, Epoch Time 298.5051(315.1109), Bit/dim 5.0946(best: 5.1589), Xent 1.9044, Loss 6.0468, Error 0.6510(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 45.2536(42.9661) | Bit/dim 5.1035(5.6720) | Xent 1.9389(2.0307) | Loss 15.2467(14.5377) | Error 0.6749(0.7098) Steps 466(467.51) | Grad Norm 4.6253(28.1216) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 41.2862(42.9157) | Bit/dim 5.1042(5.6550) | Xent 1.9840(2.0293) | Loss 12.6340(14.4806) | Error 0.7077(0.7098) Steps 478(467.82) | Grad Norm 20.5656(27.8949) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 43.9737(42.9474) | Bit/dim 5.1193(5.6389) | Xent 1.9772(2.0278) | Loss 12.8476(14.4316) | Error 0.7046(0.7096) Steps 466(467.77) | Grad Norm 22.1746(27.7233) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 46.1726(43.0442) | Bit/dim 5.0578(5.6215) | Xent 1.9264(2.0247) | Loss 12.9505(14.3872) | Error 0.6654(0.7083) Steps 508(468.98) | Grad Norm 5.6917(27.0624) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 41.9515(43.0114) | Bit/dim 5.0856(5.6054) | Xent 1.9943(2.0238) | Loss 12.9427(14.3439) | Error 0.6937(0.7079) Steps 466(468.89) | Grad Norm 33.2370(27.2476) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 44.3054(43.0502) | Bit/dim 5.0442(5.5886) | Xent 1.9101(2.0204) | Loss 12.6696(14.2936) | Error 0.6614(0.7065) Steps 466(468.80) | Grad Norm 3.4163(26.5327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.6719, Epoch Time 299.5824(314.6450), Bit/dim 5.0725(best: 5.0946), Xent 1.9072, Loss 6.0261, Error 0.6615(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 44.6447(43.0980) | Bit/dim 5.0771(5.5732) | Xent 1.9254(2.0176) | Loss 15.9385(14.3430) | Error 0.6747(0.7055) Steps 502(469.80) | Grad Norm 19.0624(26.3085) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 45.0737(43.1573) | Bit/dim 5.0201(5.5566) | Xent 1.9201(2.0146) | Loss 12.4416(14.2859) | Error 0.6718(0.7045) Steps 472(469.86) | Grad Norm 9.8755(25.8156) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 42.2866(43.1312) | Bit/dim 5.0166(5.5404) | Xent 1.9126(2.0116) | Loss 12.5817(14.2348) | Error 0.6631(0.7033) Steps 472(469.93) | Grad Norm 9.5027(25.3262) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 41.7838(43.0908) | Bit/dim 5.0234(5.5249) | Xent 1.8975(2.0082) | Loss 12.4297(14.1807) | Error 0.6707(0.7023) Steps 478(470.17) | Grad Norm 11.8779(24.9227) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 44.8407(43.1433) | Bit/dim 5.0073(5.5094) | Xent 1.9077(2.0051) | Loss 12.3268(14.1250) | Error 0.6704(0.7013) Steps 484(470.58) | Grad Norm 7.3228(24.3947) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 44.6890(43.1896) | Bit/dim 4.9810(5.4935) | Xent 1.8764(2.0013) | Loss 12.5435(14.0776) | Error 0.6535(0.6999) Steps 496(471.35) | Grad Norm 7.7617(23.8957) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 19.7568, Epoch Time 299.1262(314.1794), Bit/dim 4.9828(best: 5.0725), Xent 1.8686, Loss 5.9171, Error 0.6421(best: 0.6491)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 44.6374(43.2331) | Bit/dim 4.9901(5.4784) | Xent 1.8836(1.9978) | Loss 15.1697(14.1104) | Error 0.6551(0.6986) Steps 478(471.55) | Grad Norm 11.3451(23.5192) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 42.9250(43.2238) | Bit/dim 4.9826(5.4635) | Xent 1.8917(1.9946) | Loss 12.6374(14.0662) | Error 0.6560(0.6973) Steps 466(471.38) | Grad Norm 6.3630(23.0045) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 45.3027(43.2862) | Bit/dim 4.9478(5.4481) | Xent 1.8718(1.9909) | Loss 12.5974(14.0221) | Error 0.6450(0.6957) Steps 460(471.04) | Grad Norm 6.9130(22.5218) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 40.4482(43.2011) | Bit/dim 4.9400(5.4328) | Xent 1.8645(1.9871) | Loss 12.2717(13.9696) | Error 0.6479(0.6943) Steps 454(470.53) | Grad Norm 8.1509(22.0907) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 46.8155(43.3095) | Bit/dim 4.9277(5.4177) | Xent 1.8777(1.9838) | Loss 12.7604(13.9333) | Error 0.6531(0.6930) Steps 448(469.85) | Grad Norm 7.7478(21.6604) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 46.5360(43.4063) | Bit/dim 4.8970(5.4021) | Xent 1.8711(1.9804) | Loss 12.0715(13.8775) | Error 0.6579(0.6920) Steps 514(471.18) | Grad Norm 11.0520(21.3421) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.2471, Epoch Time 302.9913(313.8438), Bit/dim 4.9100(best: 4.9828), Xent 1.8590, Loss 5.8395, Error 0.6489(best: 0.6421)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 43.1027(43.3972) | Bit/dim 4.8960(5.3869) | Xent 1.8848(1.9776) | Loss 15.2500(13.9186) | Error 0.6641(0.6911) Steps 484(471.56) | Grad Norm 19.9906(21.3016) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 46.9281(43.5031) | Bit/dim 4.9233(5.3730) | Xent 1.9515(1.9768) | Loss 12.6113(13.8794) | Error 0.6917(0.6912) Steps 514(472.83) | Grad Norm 27.1753(21.4778) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 45.0932(43.5508) | Bit/dim 4.8991(5.3588) | Xent 1.8479(1.9729) | Loss 12.3272(13.8329) | Error 0.6387(0.6896) Steps 484(473.17) | Grad Norm 9.8712(21.1296) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 41.5909(43.4920) | Bit/dim 4.9716(5.3471) | Xent 2.0313(1.9747) | Loss 12.5864(13.7955) | Error 0.7039(0.6900) Steps 460(472.77) | Grad Norm 54.5905(22.1334) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 45.2961(43.5461) | Bit/dim 4.8967(5.3336) | Xent 1.8475(1.9708) | Loss 12.5243(13.7573) | Error 0.6406(0.6885) Steps 484(473.11) | Grad Norm 14.2810(21.8978) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 41.7067(43.4910) | Bit/dim 5.0423(5.3249) | Xent 2.1278(1.9756) | Loss 12.8226(13.7293) | Error 0.7440(0.6902) Steps 460(472.72) | Grad Norm 54.4819(22.8754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 20.9233, Epoch Time 300.6305(313.4474), Bit/dim 5.0585(best: 4.9100), Xent 2.0401, Loss 6.0786, Error 0.7207(best: 0.6421)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 41.4820(43.4307) | Bit/dim 5.0602(5.3169) | Xent 2.0636(1.9782) | Loss 15.5780(13.7847) | Error 0.7249(0.6912) Steps 472(472.70) | Grad Norm 32.2458(23.1565) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 43.8734(43.4440) | Bit/dim 4.9321(5.3054) | Xent 2.1854(1.9844) | Loss 12.7694(13.7543) | Error 0.7565(0.6932) Steps 484(473.03) | Grad Norm 41.5737(23.7090) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 44.3692(43.4717) | Bit/dim 4.8787(5.2926) | Xent 1.9257(1.9827) | Loss 12.4328(13.7146) | Error 0.6798(0.6928) Steps 484(473.36) | Grad Norm 12.7668(23.3807) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 45.9943(43.5474) | Bit/dim 4.8949(5.2807) | Xent 2.0746(1.9854) | Loss 12.8777(13.6895) | Error 0.7246(0.6938) Steps 490(473.86) | Grad Norm 22.2227(23.3460) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 42.7579(43.5237) | Bit/dim 4.8999(5.2692) | Xent 1.9124(1.9832) | Loss 12.2638(13.6468) | Error 0.6711(0.6931) Steps 484(474.17) | Grad Norm 14.7510(23.0881) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 46.4604(43.6118) | Bit/dim 4.8787(5.2575) | Xent 1.8921(1.9805) | Loss 12.3372(13.6075) | Error 0.6586(0.6920) Steps 490(474.64) | Grad Norm 6.0890(22.5782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 21.1568, Epoch Time 301.8441(313.0993), Bit/dim 4.8728(best: 4.9100), Xent 1.8846, Loss 5.8151, Error 0.6580(best: 0.6421)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 46.0585(43.6852) | Bit/dim 4.8801(5.2462) | Xent 1.9150(1.9785) | Loss 15.1652(13.6542) | Error 0.6773(0.6916) Steps 496(475.28) | Grad Norm 11.7826(22.2543) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 44.2887(43.7033) | Bit/dim 4.8480(5.2343) | Xent 1.9134(1.9766) | Loss 12.4324(13.6176) | Error 0.6789(0.6912) Steps 508(476.26) | Grad Norm 6.7249(21.7884) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 43.7600(43.7050) | Bit/dim 4.8386(5.2224) | Xent 1.9025(1.9743) | Loss 12.4396(13.5822) | Error 0.6709(0.6906) Steps 460(475.78) | Grad Norm 9.1956(21.4106) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 43.3637(43.6948) | Bit/dim 4.8544(5.2114) | Xent 1.8805(1.9715) | Loss 12.1400(13.5389) | Error 0.6587(0.6896) Steps 478(475.84) | Grad Norm 10.4226(21.0810) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 47.4203(43.8066) | Bit/dim 4.8091(5.1993) | Xent 1.8768(1.9687) | Loss 12.0566(13.4945) | Error 0.6580(0.6887) Steps 484(476.09) | Grad Norm 4.1172(20.5721) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 49.4959(43.9772) | Bit/dim 4.8370(5.1884) | Xent 1.8968(1.9665) | Loss 12.1700(13.4547) | Error 0.6639(0.6880) Steps 514(477.22) | Grad Norm 11.4584(20.2987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 21.0392, Epoch Time 311.4579(313.0501), Bit/dim 4.7957(best: 4.8728), Xent 1.8249, Loss 5.7082, Error 0.6353(best: 0.6421)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 43.1433(43.9522) | Bit/dim 4.7963(5.1767) | Xent 1.8391(1.9627) | Loss 15.0486(13.5026) | Error 0.6418(0.6866) Steps 496(477.79) | Grad Norm 6.8346(19.8947) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 44.0777(43.9560) | Bit/dim 4.8010(5.1654) | Xent 1.8371(1.9589) | Loss 12.4257(13.4703) | Error 0.6486(0.6854) Steps 484(477.97) | Grad Norm 11.1662(19.6329) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 44.5763(43.9746) | Bit/dim 4.8012(5.1545) | Xent 1.8330(1.9552) | Loss 12.1174(13.4297) | Error 0.6367(0.6840) Steps 490(478.34) | Grad Norm 5.0819(19.1964) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 44.8976(44.0023) | Bit/dim 4.7690(5.1429) | Xent 1.8283(1.9514) | Loss 12.2013(13.3928) | Error 0.6502(0.6830) Steps 490(478.69) | Grad Norm 9.8410(18.9157) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 48.4027(44.1343) | Bit/dim 4.7702(5.1317) | Xent 1.8370(1.9479) | Loss 12.3015(13.3601) | Error 0.6510(0.6820) Steps 508(479.56) | Grad Norm 10.7782(18.6716) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 47.5584(44.2370) | Bit/dim 4.7662(5.1207) | Xent 1.8203(1.9441) | Loss 12.1997(13.3253) | Error 0.6412(0.6808) Steps 490(479.88) | Grad Norm 9.3904(18.3931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 20.8601, Epoch Time 309.3997(312.9405), Bit/dim 4.7568(best: 4.7957), Xent 1.7505, Loss 5.6321, Error 0.6099(best: 0.6353)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 43.0988(44.2029) | Bit/dim 4.7688(5.1102) | Xent 1.8017(1.9398) | Loss 14.7370(13.3676) | Error 0.6299(0.6793) Steps 490(480.18) | Grad Norm 7.4920(18.0661) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 47.3511(44.2973) | Bit/dim 4.7387(5.0990) | Xent 1.8041(1.9358) | Loss 12.0965(13.3295) | Error 0.6400(0.6781) Steps 526(481.56) | Grad Norm 9.4559(17.8078) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 47.4502(44.3919) | Bit/dim 4.7231(5.0878) | Xent 1.8003(1.9317) | Loss 12.2445(13.2969) | Error 0.6345(0.6768) Steps 490(481.81) | Grad Norm 20.8037(17.8977) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 49.6570(44.5499) | Bit/dim 4.7464(5.0775) | Xent 1.9868(1.9333) | Loss 12.0026(13.2581) | Error 0.6925(0.6772) Steps 532(483.31) | Grad Norm 41.3274(18.6006) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 49.9844(44.7129) | Bit/dim 4.7663(5.0682) | Xent 1.9307(1.9333) | Loss 12.0493(13.2218) | Error 0.6823(0.6774) Steps 484(483.34) | Grad Norm 35.6955(19.1134) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 44.2050(44.6977) | Bit/dim 4.7436(5.0585) | Xent 1.8708(1.9314) | Loss 12.3261(13.1950) | Error 0.6576(0.6768) Steps 514(484.26) | Grad Norm 41.0217(19.7707) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 21.6701, Epoch Time 319.2891(313.1310), Bit/dim 4.7133(best: 4.7568), Xent 1.7309, Loss 5.5788, Error 0.6006(best: 0.6099)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 49.5617(44.8436) | Bit/dim 4.7066(5.0479) | Xent 1.7886(1.9271) | Loss 14.9944(13.2489) | Error 0.6242(0.6752) Steps 514(485.15) | Grad Norm 10.1848(19.4831) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 51.3038(45.0374) | Bit/dim 4.7595(5.0392) | Xent 1.9747(1.9285) | Loss 12.4318(13.2244) | Error 0.6906(0.6757) Steps 532(486.55) | Grad Norm 38.6609(20.0584) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 48.0626(45.1281) | Bit/dim 4.7085(5.0293) | Xent 1.9120(1.9280) | Loss 12.2600(13.1955) | Error 0.6787(0.6758) Steps 526(487.74) | Grad Norm 30.8485(20.3821) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 48.8980(45.2412) | Bit/dim 4.7113(5.0198) | Xent 1.8648(1.9261) | Loss 12.1387(13.1638) | Error 0.6570(0.6752) Steps 484(487.62) | Grad Norm 33.4540(20.7743) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 51.3386(45.4242) | Bit/dim 4.7122(5.0106) | Xent 1.8089(1.9226) | Loss 11.9090(13.1262) | Error 0.6389(0.6741) Steps 520(488.60) | Grad Norm 19.7277(20.7429) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 50.0425(45.5627) | Bit/dim 4.6941(5.0011) | Xent 1.9108(1.9223) | Loss 12.1633(13.0973) | Error 0.6845(0.6744) Steps 526(489.72) | Grad Norm 21.0268(20.7514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 22.3243, Epoch Time 337.6123(313.8654), Bit/dim 4.7660(best: 4.7133), Xent 1.9940, Loss 5.7630, Error 0.7075(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 50.6408(45.7150) | Bit/dim 4.7624(4.9939) | Xent 2.0275(1.9254) | Loss 15.3515(13.1649) | Error 0.7244(0.6759) Steps 550(491.53) | Grad Norm 28.9978(20.9988) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 48.0071(45.7838) | Bit/dim 4.6786(4.9844) | Xent 1.8903(1.9244) | Loss 11.9970(13.1299) | Error 0.6634(0.6756) Steps 556(493.46) | Grad Norm 13.5978(20.7768) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 58.1943(46.1561) | Bit/dim 4.7786(4.9783) | Xent 2.0200(1.9272) | Loss 12.4222(13.1086) | Error 0.7066(0.6765) Steps 508(493.90) | Grad Norm 34.1728(21.1786) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 54.1906(46.3972) | Bit/dim 4.7852(4.9725) | Xent 1.8289(1.9243) | Loss 12.4564(13.0891) | Error 0.6495(0.6757) Steps 526(494.86) | Grad Norm 21.5640(21.1902) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 49.8478(46.5007) | Bit/dim 4.7248(4.9650) | Xent 1.9516(1.9251) | Loss 12.0936(13.0592) | Error 0.6927(0.6762) Steps 574(497.23) | Grad Norm 19.5451(21.1409) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 47.8246(46.5404) | Bit/dim 4.6859(4.9567) | Xent 1.8655(1.9233) | Loss 12.0971(13.0303) | Error 0.6618(0.6758) Steps 538(498.46) | Grad Norm 10.1692(20.8117) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 22.5758, Epoch Time 347.4821(314.8739), Bit/dim 4.7042(best: 4.7133), Xent 1.9096, Loss 5.6590, Error 0.6902(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 48.6987(46.6051) | Bit/dim 4.6966(4.9489) | Xent 1.9353(1.9237) | Loss 14.9632(13.0883) | Error 0.6831(0.6760) Steps 568(500.54) | Grad Norm 22.2052(20.8535) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 53.1512(46.8015) | Bit/dim 4.7123(4.9418) | Xent 1.8624(1.9218) | Loss 12.2630(13.0636) | Error 0.6594(0.6755) Steps 580(502.93) | Grad Norm 9.3215(20.5075) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 50.1733(46.9027) | Bit/dim 4.6402(4.9327) | Xent 1.8671(1.9202) | Loss 12.0551(13.0333) | Error 0.6573(0.6749) Steps 550(504.34) | Grad Norm 8.7759(20.1556) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 51.8561(47.0513) | Bit/dim 4.6884(4.9254) | Xent 1.8838(1.9191) | Loss 12.1190(13.0059) | Error 0.6694(0.6748) Steps 550(505.71) | Grad Norm 11.8372(19.9060) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 49.6092(47.1280) | Bit/dim 4.7627(4.9205) | Xent 1.8362(1.9166) | Loss 11.9910(12.9754) | Error 0.6420(0.6738) Steps 562(507.40) | Grad Norm 13.3388(19.7090) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 47.3221(47.1338) | Bit/dim 4.6665(4.9129) | Xent 1.8390(1.9143) | Loss 11.9366(12.9443) | Error 0.6527(0.6732) Steps 550(508.68) | Grad Norm 9.3542(19.3984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 22.1507, Epoch Time 338.9756(315.5970), Bit/dim 4.6863(best: 4.7042), Xent 1.7784, Loss 5.5755, Error 0.6277(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 52.6373(47.2989) | Bit/dim 4.6800(4.9059) | Xent 1.8405(1.9121) | Loss 15.4127(13.0183) | Error 0.6501(0.6725) Steps 574(510.64) | Grad Norm 15.0083(19.2667) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 49.5414(47.3662) | Bit/dim 4.7252(4.9005) | Xent 1.8483(1.9102) | Loss 12.0536(12.9894) | Error 0.6542(0.6719) Steps 562(512.18) | Grad Norm 14.3199(19.1183) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 49.7237(47.4369) | Bit/dim 4.6745(4.8937) | Xent 1.7881(1.9065) | Loss 11.8281(12.9545) | Error 0.6412(0.6710) Steps 526(512.59) | Grad Norm 6.6047(18.7429) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 51.0887(47.5465) | Bit/dim 4.7079(4.8881) | Xent 1.8335(1.9043) | Loss 12.1125(12.9293) | Error 0.6432(0.6702) Steps 520(512.81) | Grad Norm 18.6495(18.7401) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 48.1240(47.5638) | Bit/dim 4.6820(4.8819) | Xent 1.7840(1.9007) | Loss 11.9806(12.9008) | Error 0.6302(0.6690) Steps 538(513.57) | Grad Norm 6.3071(18.3671) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 49.2897(47.6156) | Bit/dim 4.6673(4.8755) | Xent 1.7922(1.8974) | Loss 11.9524(12.8724) | Error 0.6267(0.6677) Steps 544(514.48) | Grad Norm 9.5097(18.1014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 22.3644, Epoch Time 338.7277(316.2909), Bit/dim 4.6711(best: 4.6863), Xent 1.7368, Loss 5.5395, Error 0.6103(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 47.6444(47.6165) | Bit/dim 4.6762(4.8695) | Xent 1.8172(1.8950) | Loss 14.7115(12.9275) | Error 0.6492(0.6671) Steps 532(515.01) | Grad Norm 11.4262(17.9011) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 53.4024(47.7900) | Bit/dim 4.6062(4.8616) | Xent 1.7720(1.8913) | Loss 11.6700(12.8898) | Error 0.6233(0.6658) Steps 544(515.88) | Grad Norm 9.4028(17.6462) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 49.2025(47.8324) | Bit/dim 4.6454(4.8551) | Xent 1.8031(1.8887) | Loss 12.0853(12.8657) | Error 0.6414(0.6651) Steps 574(517.62) | Grad Norm 21.0138(17.7472) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 49.2716(47.8756) | Bit/dim 4.6185(4.8480) | Xent 1.7517(1.8846) | Loss 11.9645(12.8386) | Error 0.6173(0.6637) Steps 538(518.23) | Grad Norm 15.2221(17.6714) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 48.3908(47.8910) | Bit/dim 4.6104(4.8409) | Xent 1.7356(1.8801) | Loss 11.9639(12.8124) | Error 0.6098(0.6620) Steps 562(519.55) | Grad Norm 7.5377(17.3674) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 49.9432(47.9526) | Bit/dim 4.6265(4.8345) | Xent 1.8061(1.8779) | Loss 11.7027(12.7791) | Error 0.6434(0.6615) Steps 562(520.82) | Grad Norm 17.3831(17.3679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 22.4073, Epoch Time 335.8160(316.8767), Bit/dim 4.5870(best: 4.6711), Xent 1.6992, Loss 5.4366, Error 0.6019(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 49.7410(48.0063) | Bit/dim 4.5962(4.8273) | Xent 1.7588(1.8743) | Loss 14.4531(12.8293) | Error 0.6219(0.6603) Steps 580(522.59) | Grad Norm 9.8371(17.1420) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 51.8373(48.1212) | Bit/dim 4.5924(4.8203) | Xent 1.7738(1.8713) | Loss 11.7479(12.7969) | Error 0.6295(0.6594) Steps 604(525.04) | Grad Norm 28.1485(17.4722) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 49.8224(48.1722) | Bit/dim 4.5688(4.8127) | Xent 1.7495(1.8677) | Loss 11.4742(12.7572) | Error 0.6204(0.6582) Steps 556(525.97) | Grad Norm 6.2910(17.1367) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 50.3319(48.2370) | Bit/dim 4.5630(4.8053) | Xent 1.7689(1.8647) | Loss 11.8212(12.7291) | Error 0.6318(0.6574) Steps 544(526.51) | Grad Norm 19.4505(17.2061) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 52.8515(48.3755) | Bit/dim 4.5843(4.7986) | Xent 1.8340(1.8638) | Loss 11.8791(12.7036) | Error 0.6439(0.6570) Steps 532(526.67) | Grad Norm 29.9386(17.5881) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 49.6939(48.4150) | Bit/dim 4.5559(4.7913) | Xent 1.7582(1.8606) | Loss 11.7443(12.6748) | Error 0.6150(0.6557) Steps 586(528.45) | Grad Norm 14.6438(17.4998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 22.7255, Epoch Time 342.7155(317.6518), Bit/dim 4.5980(best: 4.5870), Xent 1.8671, Loss 5.5316, Error 0.6614(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 47.5065(48.3878) | Bit/dim 4.5995(4.7856) | Xent 1.9178(1.8623) | Loss 14.7600(12.7374) | Error 0.6661(0.6561) Steps 544(528.92) | Grad Norm 43.4921(18.2796) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 52.3113(48.5055) | Bit/dim 4.5597(4.7788) | Xent 1.7881(1.8601) | Loss 11.7595(12.7081) | Error 0.6347(0.6554) Steps 592(530.81) | Grad Norm 21.0536(18.3628) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 54.3543(48.6809) | Bit/dim 4.5947(4.7733) | Xent 2.0179(1.8648) | Loss 12.1475(12.6912) | Error 0.7047(0.6569) Steps 586(532.47) | Grad Norm 37.2048(18.9280) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 51.6425(48.7698) | Bit/dim 4.5831(4.7676) | Xent 1.9987(1.8688) | Loss 12.0413(12.6717) | Error 0.7075(0.6584) Steps 562(533.35) | Grad Norm 22.4620(19.0341) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 50.3669(48.8177) | Bit/dim 4.5232(4.7602) | Xent 1.8179(1.8673) | Loss 11.7810(12.6450) | Error 0.6539(0.6583) Steps 550(533.85) | Grad Norm 5.4340(18.6261) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 53.8153(48.9676) | Bit/dim 4.6085(4.7557) | Xent 1.8240(1.8660) | Loss 11.9086(12.6229) | Error 0.6464(0.6579) Steps 538(533.98) | Grad Norm 25.7992(18.8413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 22.0472, Epoch Time 348.1340(318.5663), Bit/dim 4.6225(best: 4.5870), Xent 1.8814, Loss 5.5633, Error 0.6757(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 50.5427(49.0149) | Bit/dim 4.6210(4.7517) | Xent 1.9319(1.8680) | Loss 15.2156(12.7007) | Error 0.6871(0.6588) Steps 586(535.54) | Grad Norm 32.3293(19.2459) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 50.9945(49.0743) | Bit/dim 4.5662(4.7461) | Xent 1.7669(1.8650) | Loss 11.8901(12.6764) | Error 0.6295(0.6579) Steps 580(536.87) | Grad Norm 9.8971(18.9654) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 51.3609(49.1429) | Bit/dim 4.5938(4.7415) | Xent 1.8328(1.8640) | Loss 12.1099(12.6594) | Error 0.6515(0.6577) Steps 568(537.80) | Grad Norm 19.2916(18.9752) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 52.4204(49.2412) | Bit/dim 4.6138(4.7377) | Xent 1.7933(1.8619) | Loss 12.0774(12.6419) | Error 0.6361(0.6571) Steps 592(539.43) | Grad Norm 14.2903(18.8347) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 48.5745(49.2212) | Bit/dim 4.5472(4.7320) | Xent 1.7695(1.8591) | Loss 11.3041(12.6018) | Error 0.6369(0.6565) Steps 592(541.01) | Grad Norm 8.2586(18.5174) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 48.0765(49.1868) | Bit/dim 4.5497(4.7265) | Xent 1.7945(1.8572) | Loss 11.4207(12.5664) | Error 0.6385(0.6559) Steps 526(540.56) | Grad Norm 12.4817(18.3363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 21.7128, Epoch Time 339.6981(319.2002), Bit/dim 4.5617(best: 4.5870), Xent 1.7080, Loss 5.4157, Error 0.6025(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 48.7512(49.1738) | Bit/dim 4.5678(4.7217) | Xent 1.7437(1.8538) | Loss 14.2879(12.6180) | Error 0.6202(0.6549) Steps 562(541.20) | Grad Norm 8.8667(18.0522) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 53.1956(49.2944) | Bit/dim 4.5440(4.7164) | Xent 1.7485(1.8506) | Loss 11.5291(12.5853) | Error 0.6114(0.6536) Steps 532(540.92) | Grad Norm 7.1695(17.7257) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 52.8312(49.4005) | Bit/dim 4.5249(4.7107) | Xent 1.7538(1.8477) | Loss 11.6184(12.5563) | Error 0.6266(0.6527) Steps 562(541.56) | Grad Norm 13.6985(17.6049) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 46.6302(49.3174) | Bit/dim 4.5426(4.7056) | Xent 1.7315(1.8442) | Loss 11.4025(12.5217) | Error 0.6080(0.6514) Steps 544(541.63) | Grad Norm 8.8599(17.3426) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 49.7313(49.3298) | Bit/dim 4.5199(4.7001) | Xent 1.7328(1.8409) | Loss 11.6580(12.4958) | Error 0.6122(0.6502) Steps 502(540.44) | Grad Norm 9.4584(17.1060) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 51.6637(49.3998) | Bit/dim 4.4974(4.6940) | Xent 1.7590(1.8384) | Loss 11.5382(12.4671) | Error 0.6271(0.6495) Steps 544(540.55) | Grad Norm 19.8029(17.1870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 21.9167, Epoch Time 341.2111(319.8606), Bit/dim 4.4997(best: 4.5617), Xent 1.6923, Loss 5.3458, Error 0.5983(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 51.6111(49.4662) | Bit/dim 4.5079(4.6884) | Xent 1.7414(1.8355) | Loss 14.7243(12.5348) | Error 0.6193(0.6486) Steps 556(541.01) | Grad Norm 20.9146(17.2988) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 49.1171(49.4557) | Bit/dim 4.5014(4.6828) | Xent 1.7462(1.8328) | Loss 11.6019(12.5068) | Error 0.6267(0.6480) Steps 526(540.56) | Grad Norm 10.9969(17.1097) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 49.8464(49.4674) | Bit/dim 4.4843(4.6768) | Xent 1.7303(1.8298) | Loss 11.4336(12.4746) | Error 0.6133(0.6469) Steps 526(540.12) | Grad Norm 9.8231(16.8911) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 55.2671(49.6414) | Bit/dim 4.4667(4.6705) | Xent 1.6833(1.8254) | Loss 11.2926(12.4392) | Error 0.5990(0.6455) Steps 550(540.42) | Grad Norm 4.6902(16.5251) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 48.6885(49.6128) | Bit/dim 4.4674(4.6644) | Xent 1.7426(1.8229) | Loss 11.4110(12.4083) | Error 0.6170(0.6446) Steps 550(540.71) | Grad Norm 14.5483(16.4658) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 49.2299(49.6013) | Bit/dim 4.5203(4.6601) | Xent 1.7896(1.8219) | Loss 11.4512(12.3796) | Error 0.6392(0.6445) Steps 580(541.89) | Grad Norm 21.0073(16.6020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 21.3751, Epoch Time 340.8576(320.4905), Bit/dim 4.4765(best: 4.4997), Xent 1.6857, Loss 5.3194, Error 0.6006(best: 0.5983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 51.1600(49.6481) | Bit/dim 4.4591(4.6541) | Xent 1.7539(1.8198) | Loss 14.1164(12.4317) | Error 0.6200(0.6437) Steps 574(542.85) | Grad Norm 22.7059(16.7852) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 46.3042(49.5478) | Bit/dim 4.4985(4.6494) | Xent 1.8007(1.8193) | Loss 11.4851(12.4033) | Error 0.6259(0.6432) Steps 532(542.52) | Grad Norm 30.3182(17.1911) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 45.8195(49.4359) | Bit/dim 4.4680(4.6440) | Xent 1.7016(1.8157) | Loss 11.2817(12.3697) | Error 0.6008(0.6419) Steps 520(541.85) | Grad Norm 13.1924(17.0712) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 54.5052(49.5880) | Bit/dim 4.4934(4.6394) | Xent 1.9559(1.8199) | Loss 11.5317(12.3445) | Error 0.6844(0.6432) Steps 562(542.45) | Grad Norm 42.0610(17.8209) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 51.8817(49.6568) | Bit/dim 4.4770(4.6346) | Xent 1.8405(1.8206) | Loss 11.3372(12.3143) | Error 0.6551(0.6436) Steps 562(543.04) | Grad Norm 20.9471(17.9147) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 55.4720(49.8313) | Bit/dim 4.4935(4.6303) | Xent 1.7947(1.8198) | Loss 11.7829(12.2984) | Error 0.6385(0.6434) Steps 544(543.07) | Grad Norm 22.1979(18.0432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 22.6936, Epoch Time 343.7184(321.1873), Bit/dim 4.5317(best: 4.4765), Xent 1.7680, Loss 5.4156, Error 0.6396(best: 0.5983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 50.2658(49.8443) | Bit/dim 4.5235(4.6271) | Xent 1.8070(1.8194) | Loss 14.6720(12.3696) | Error 0.6361(0.6432) Steps 526(542.56) | Grad Norm 20.0610(18.1037) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 48.4470(49.8024) | Bit/dim 4.4971(4.6232) | Xent 1.7326(1.8168) | Loss 11.5741(12.3457) | Error 0.6133(0.6423) Steps 556(542.96) | Grad Norm 10.6989(17.8816) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 46.1136(49.6917) | Bit/dim 4.4756(4.6188) | Xent 1.7544(1.8149) | Loss 11.5352(12.3214) | Error 0.6298(0.6419) Steps 532(542.63) | Grad Norm 17.6895(17.8758) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 49.8639(49.6969) | Bit/dim 4.5015(4.6153) | Xent 1.8083(1.8147) | Loss 11.7626(12.3046) | Error 0.6429(0.6419) Steps 544(542.67) | Grad Norm 23.8382(18.0547) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 48.6459(49.6654) | Bit/dim 4.4900(4.6115) | Xent 1.7628(1.8132) | Loss 11.6918(12.2862) | Error 0.6310(0.6416) Steps 550(542.89) | Grad Norm 16.2067(17.9992) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 49.9828(49.6749) | Bit/dim 4.4812(4.6076) | Xent 1.8508(1.8143) | Loss 11.5267(12.2634) | Error 0.6591(0.6421) Steps 568(543.65) | Grad Norm 30.9895(18.3889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 22.0388, Epoch Time 331.0936(321.4845), Bit/dim 4.4714(best: 4.4765), Xent 1.6979, Loss 5.3204, Error 0.6084(best: 0.5983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 49.7593(49.6774) | Bit/dim 4.4787(4.6038) | Xent 1.7405(1.8121) | Loss 14.2144(12.3220) | Error 0.6219(0.6415) Steps 538(543.48) | Grad Norm 11.5515(18.1838) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 53.7372(49.7992) | Bit/dim 4.5148(4.6011) | Xent 1.8313(1.8127) | Loss 12.0282(12.3132) | Error 0.6551(0.6419) Steps 538(543.31) | Grad Norm 29.3147(18.5177) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 50.9071(49.8325) | Bit/dim 4.4550(4.5967) | Xent 1.7587(1.8110) | Loss 11.5736(12.2910) | Error 0.6302(0.6416) Steps 568(544.05) | Grad Norm 16.0493(18.4437) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 47.0401(49.7487) | Bit/dim 4.4516(4.5923) | Xent 1.8011(1.8107) | Loss 11.5653(12.2692) | Error 0.6366(0.6414) Steps 562(544.59) | Grad Norm 21.1894(18.5261) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 46.6412(49.6555) | Bit/dim 4.4501(4.5881) | Xent 1.7900(1.8101) | Loss 11.5960(12.2490) | Error 0.6425(0.6415) Steps 526(544.03) | Grad Norm 16.0558(18.4519) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 49.3797(49.6472) | Bit/dim 4.4204(4.5831) | Xent 1.7628(1.8087) | Loss 11.4953(12.2264) | Error 0.6281(0.6411) Steps 544(544.03) | Grad Norm 12.3659(18.2694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 22.3067, Epoch Time 335.3715(321.9011), Bit/dim 4.4305(best: 4.4714), Xent 1.7054, Loss 5.2832, Error 0.6009(best: 0.5983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 48.7577(49.6205) | Bit/dim 4.4401(4.5788) | Xent 1.7392(1.8066) | Loss 14.4677(12.2936) | Error 0.6238(0.6406) Steps 508(542.95) | Grad Norm 14.5313(18.1572) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 49.7422(49.6242) | Bit/dim 4.4231(4.5741) | Xent 1.7233(1.8041) | Loss 11.5125(12.2702) | Error 0.6070(0.6395) Steps 556(543.34) | Grad Norm 6.0428(17.7938) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 48.0917(49.5782) | Bit/dim 4.4007(4.5689) | Xent 1.7220(1.8017) | Loss 11.1530(12.2367) | Error 0.6189(0.6389) Steps 526(542.82) | Grad Norm 11.8660(17.6160) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 47.3555(49.5115) | Bit/dim 4.3985(4.5638) | Xent 1.7409(1.7998) | Loss 11.3393(12.2098) | Error 0.6196(0.6383) Steps 538(542.68) | Grad Norm 7.5486(17.3139) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 50.1035(49.5293) | Bit/dim 4.4688(4.5609) | Xent 1.6982(1.7968) | Loss 11.5668(12.1905) | Error 0.6031(0.6373) Steps 538(542.54) | Grad Norm 14.0039(17.2146) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 50.2447(49.5507) | Bit/dim 4.4030(4.5562) | Xent 1.7153(1.7943) | Loss 11.4333(12.1678) | Error 0.6134(0.6366) Steps 532(542.22) | Grad Norm 11.0801(17.0306) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 21.8105, Epoch Time 331.9083(322.2013), Bit/dim 4.4143(best: 4.4305), Xent 1.6459, Loss 5.2372, Error 0.5803(best: 0.5983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 47.4233(49.4869) | Bit/dim 4.4028(4.5516) | Xent 1.7031(1.7916) | Loss 14.3069(12.2319) | Error 0.6031(0.6356) Steps 538(542.09) | Grad Norm 14.7874(16.9633) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 49.2132(49.4787) | Bit/dim 4.4216(4.5477) | Xent 1.7637(1.7908) | Loss 11.2899(12.2037) | Error 0.6186(0.6351) Steps 538(541.97) | Grad Norm 21.9131(17.1118) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 51.5763(49.5416) | Bit/dim 4.4177(4.5438) | Xent 1.6805(1.7875) | Loss 11.4053(12.1797) | Error 0.6022(0.6341) Steps 556(542.39) | Grad Norm 8.4670(16.8525) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 51.3742(49.5966) | Bit/dim 4.3716(4.5386) | Xent 1.7492(1.7863) | Loss 11.2430(12.1516) | Error 0.6233(0.6338) Steps 532(542.08) | Grad Norm 13.4563(16.7506) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 48.3664(49.5597) | Bit/dim 4.3953(4.5343) | Xent 1.6666(1.7827) | Loss 11.2714(12.1252) | Error 0.5926(0.6325) Steps 544(542.14) | Grad Norm 9.5077(16.5333) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 51.3213(49.6125) | Bit/dim 4.3879(4.5299) | Xent 1.6720(1.7794) | Loss 11.3610(12.1023) | Error 0.5930(0.6313) Steps 544(542.19) | Grad Norm 9.2893(16.3160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.8673, Epoch Time 338.7235(322.6970), Bit/dim 4.4109(best: 4.4143), Xent 1.6663, Loss 5.2440, Error 0.5993(best: 0.5803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 50.9525(49.6527) | Bit/dim 4.4127(4.5264) | Xent 1.7227(1.7777) | Loss 14.0317(12.1602) | Error 0.6073(0.6306) Steps 550(542.43) | Grad Norm 15.8441(16.3018) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 50.0613(49.6650) | Bit/dim 4.3663(4.5216) | Xent 1.6849(1.7749) | Loss 11.1071(12.1286) | Error 0.6011(0.6297) Steps 514(541.58) | Grad Norm 12.5257(16.1885) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 49.5836(49.6626) | Bit/dim 4.3583(4.5167) | Xent 1.6804(1.7721) | Loss 11.2290(12.1016) | Error 0.5874(0.6285) Steps 544(541.65) | Grad Norm 14.9119(16.1502) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 50.3788(49.6840) | Bit/dim 4.3442(4.5115) | Xent 1.6662(1.7689) | Loss 11.0396(12.0697) | Error 0.6034(0.6277) Steps 574(542.62) | Grad Norm 5.0788(15.8181) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 51.2779(49.7319) | Bit/dim 4.3546(4.5068) | Xent 1.6778(1.7662) | Loss 11.1981(12.0436) | Error 0.6001(0.6269) Steps 562(543.20) | Grad Norm 10.4790(15.6579) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 52.7246(49.8216) | Bit/dim 4.3591(4.5024) | Xent 1.7045(1.7643) | Loss 10.8946(12.0091) | Error 0.6080(0.6263) Steps 532(542.86) | Grad Norm 19.0768(15.7605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.6683, Epoch Time 343.6018(323.3241), Bit/dim 4.3567(best: 4.4109), Xent 1.5943, Loss 5.1539, Error 0.5640(best: 0.5803)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 46.4952(49.7218) | Bit/dim 4.3531(4.4979) | Xent 1.6393(1.7606) | Loss 14.0713(12.0710) | Error 0.5824(0.6250) Steps 544(542.90) | Grad Norm 9.5031(15.5728) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 49.7463(49.7226) | Bit/dim 4.3340(4.4930) | Xent 1.6746(1.7580) | Loss 11.3002(12.0479) | Error 0.6015(0.6243) Steps 556(543.29) | Grad Norm 12.4434(15.4789) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 49.1017(49.7040) | Bit/dim 4.3676(4.4892) | Xent 1.6553(1.7549) | Loss 11.1753(12.0217) | Error 0.5845(0.6231) Steps 550(543.49) | Grad Norm 15.1613(15.4693) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 49.3415(49.6931) | Bit/dim 4.3289(4.4844) | Xent 1.7054(1.7534) | Loss 11.1110(11.9944) | Error 0.6056(0.6226) Steps 556(543.87) | Grad Norm 21.6517(15.6548) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 49.2895(49.6810) | Bit/dim 4.3267(4.4797) | Xent 1.6352(1.7499) | Loss 11.0070(11.9647) | Error 0.5847(0.6214) Steps 532(543.51) | Grad Norm 7.2050(15.4013) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 50.1369(49.6946) | Bit/dim 4.3534(4.4759) | Xent 1.6718(1.7475) | Loss 11.0301(11.9367) | Error 0.5965(0.6207) Steps 544(543.53) | Grad Norm 19.9786(15.5386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 22.6060, Epoch Time 332.7040(323.6055), Bit/dim 4.3245(best: 4.3567), Xent 1.6062, Loss 5.1276, Error 0.5761(best: 0.5640)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 46.0528(49.5854) | Bit/dim 4.3154(4.4711) | Xent 1.6506(1.7446) | Loss 13.8149(11.9930) | Error 0.5860(0.6196) Steps 556(543.90) | Grad Norm 15.8133(15.5469) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 50.4367(49.6109) | Bit/dim 4.3078(4.4662) | Xent 1.6056(1.7405) | Loss 11.1002(11.9663) | Error 0.5740(0.6183) Steps 568(544.62) | Grad Norm 3.7006(15.1915) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 53.2585(49.7204) | Bit/dim 4.3181(4.4617) | Xent 1.6666(1.7382) | Loss 11.2316(11.9442) | Error 0.5980(0.6177) Steps 568(545.32) | Grad Norm 9.1314(15.0097) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 49.6713(49.7189) | Bit/dim 4.3361(4.4580) | Xent 1.6842(1.7366) | Loss 10.9756(11.9152) | Error 0.6010(0.6172) Steps 532(544.93) | Grad Norm 13.2900(14.9581) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 48.8234(49.6920) | Bit/dim 4.3202(4.4538) | Xent 1.6317(1.7335) | Loss 10.9281(11.8856) | Error 0.5816(0.6161) Steps 550(545.08) | Grad Norm 13.2015(14.9054) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 52.2763(49.7696) | Bit/dim 4.3051(4.4494) | Xent 1.6290(1.7303) | Loss 11.1125(11.8624) | Error 0.5782(0.6150) Steps 568(545.77) | Grad Norm 8.2579(14.7060) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 22.0795, Epoch Time 338.6201(324.0560), Bit/dim 4.3216(best: 4.3245), Xent 1.5511, Loss 5.0971, Error 0.5481(best: 0.5640)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 51.4472(49.8199) | Bit/dim 4.3244(4.4456) | Xent 1.6073(1.7266) | Loss 13.2663(11.9045) | Error 0.5769(0.6138) Steps 562(546.25) | Grad Norm 6.5261(14.4606) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 53.2619(49.9231) | Bit/dim 4.3076(4.4415) | Xent 1.7377(1.7270) | Loss 11.1617(11.8822) | Error 0.6091(0.6137) Steps 526(545.64) | Grad Norm 28.5326(14.8827) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 48.6210(49.8841) | Bit/dim 4.2864(4.4368) | Xent 1.6319(1.7241) | Loss 11.0172(11.8562) | Error 0.5825(0.6127) Steps 544(545.60) | Grad Norm 15.4517(14.8998) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 52.0458(49.9489) | Bit/dim 4.3193(4.4333) | Xent 1.7178(1.7239) | Loss 11.1711(11.8357) | Error 0.6094(0.6126) Steps 544(545.55) | Grad Norm 29.2995(15.3318) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 54.1143(50.0739) | Bit/dim 4.3093(4.4296) | Xent 1.6153(1.7207) | Loss 11.1958(11.8165) | Error 0.5745(0.6115) Steps 562(546.04) | Grad Norm 10.5976(15.1898) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 52.9133(50.1591) | Bit/dim 4.3254(4.4265) | Xent 1.6168(1.7176) | Loss 10.9979(11.7919) | Error 0.5715(0.6103) Steps 550(546.16) | Grad Norm 7.3056(14.9532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 21.2930, Epoch Time 349.8527(324.8299), Bit/dim 4.2987(best: 4.3216), Xent 1.6683, Loss 5.1329, Error 0.5950(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 46.1338(50.0383) | Bit/dim 4.3037(4.4228) | Xent 1.7632(1.7189) | Loss 13.6716(11.8483) | Error 0.6241(0.6107) Steps 538(545.91) | Grad Norm 31.8157(15.4591) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 51.3644(50.0781) | Bit/dim 4.2642(4.4180) | Xent 1.6363(1.7164) | Loss 10.9979(11.8228) | Error 0.5841(0.6099) Steps 520(545.14) | Grad Norm 12.8657(15.3813) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 48.9003(50.0428) | Bit/dim 4.2843(4.4140) | Xent 1.6767(1.7153) | Loss 10.9617(11.7970) | Error 0.6014(0.6097) Steps 550(545.28) | Grad Norm 14.8787(15.3662) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 50.9924(50.0713) | Bit/dim 4.3024(4.4107) | Xent 1.5895(1.7115) | Loss 11.0642(11.7750) | Error 0.5606(0.6082) Steps 556(545.60) | Grad Norm 6.9332(15.1133) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 50.7349(50.0912) | Bit/dim 4.2991(4.4073) | Xent 1.6977(1.7111) | Loss 11.1120(11.7551) | Error 0.6008(0.6080) Steps 556(545.92) | Grad Norm 27.2664(15.4778) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 49.6669(50.0784) | Bit/dim 4.2860(4.4037) | Xent 1.6515(1.7093) | Loss 10.9928(11.7322) | Error 0.5916(0.6075) Steps 544(545.86) | Grad Norm 17.7531(15.5461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 21.5023, Epoch Time 334.9863(325.1346), Bit/dim 4.2561(best: 4.2987), Xent 1.5508, Loss 5.0315, Error 0.5559(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 49.5428(50.0624) | Bit/dim 4.2507(4.3991) | Xent 1.6243(1.7067) | Loss 13.6881(11.7909) | Error 0.5806(0.6067) Steps 526(545.26) | Grad Norm 9.4695(15.3638) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 48.9477(50.0289) | Bit/dim 4.3434(4.3974) | Xent 1.6039(1.7036) | Loss 10.8714(11.7633) | Error 0.5627(0.6054) Steps 526(544.69) | Grad Norm 12.1753(15.2682) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 51.5465(50.0745) | Bit/dim 4.2827(4.3940) | Xent 1.6361(1.7016) | Loss 11.1155(11.7439) | Error 0.5885(0.6048) Steps 544(544.66) | Grad Norm 10.1796(15.1155) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 51.5053(50.1174) | Bit/dim 4.2860(4.3907) | Xent 1.6135(1.6990) | Loss 11.1370(11.7257) | Error 0.5780(0.6040) Steps 562(545.18) | Grad Norm 10.0271(14.9628) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 55.3663(50.2748) | Bit/dim 4.2832(4.3875) | Xent 1.6360(1.6971) | Loss 11.2065(11.7101) | Error 0.5866(0.6035) Steps 538(544.97) | Grad Norm 15.6143(14.9824) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 43.7755(50.0799) | Bit/dim 4.2797(4.3843) | Xent 1.6244(1.6949) | Loss 10.7650(11.6818) | Error 0.5899(0.6031) Steps 520(544.22) | Grad Norm 15.8253(15.0077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.9928, Epoch Time 339.7324(325.5725), Bit/dim 4.2738(best: 4.2561), Xent 1.6261, Loss 5.0868, Error 0.5868(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 50.4911(50.0922) | Bit/dim 4.2884(4.3814) | Xent 1.6903(1.6948) | Loss 13.9386(11.7495) | Error 0.6062(0.6032) Steps 574(545.11) | Grad Norm 22.5905(15.2352) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 50.3936(50.1012) | Bit/dim 4.2597(4.3777) | Xent 1.5868(1.6915) | Loss 10.9252(11.7247) | Error 0.5721(0.6023) Steps 580(546.16) | Grad Norm 10.5983(15.0961) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 53.2664(50.1962) | Bit/dim 4.2751(4.3747) | Xent 1.6150(1.6892) | Loss 11.1394(11.7072) | Error 0.5793(0.6016) Steps 544(546.10) | Grad Norm 10.7943(14.9670) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 54.5700(50.3274) | Bit/dim 4.2828(4.3719) | Xent 1.6470(1.6880) | Loss 11.1224(11.6896) | Error 0.5909(0.6013) Steps 562(546.57) | Grad Norm 16.1295(15.0019) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 47.3545(50.2382) | Bit/dim 4.2873(4.3694) | Xent 1.6796(1.6877) | Loss 10.8878(11.6656) | Error 0.5970(0.6011) Steps 538(546.32) | Grad Norm 25.9147(15.3293) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 48.5754(50.1883) | Bit/dim 4.2463(4.3657) | Xent 1.5924(1.6849) | Loss 10.8367(11.6407) | Error 0.5719(0.6003) Steps 556(546.61) | Grad Norm 5.8190(15.0440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 21.9590, Epoch Time 343.0165(326.0958), Bit/dim 4.2804(best: 4.2561), Xent 1.5776, Loss 5.0693, Error 0.5622(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 47.0638(50.0946) | Bit/dim 4.2822(4.3632) | Xent 1.6564(1.6840) | Loss 14.0000(11.7115) | Error 0.5955(0.6001) Steps 544(546.53) | Grad Norm 24.0521(15.3142) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 47.8298(50.0267) | Bit/dim 4.2613(4.3601) | Xent 1.6234(1.6822) | Loss 10.5078(11.6754) | Error 0.5795(0.5995) Steps 544(546.45) | Grad Norm 7.6772(15.0851) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 46.5217(49.9215) | Bit/dim 4.2874(4.3579) | Xent 1.5955(1.6796) | Loss 10.8645(11.6510) | Error 0.5826(0.5990) Steps 538(546.20) | Grad Norm 10.7746(14.9558) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 48.5735(49.8811) | Bit/dim 4.2839(4.3557) | Xent 1.6001(1.6772) | Loss 10.9611(11.6303) | Error 0.5787(0.5984) Steps 532(545.77) | Grad Norm 9.4642(14.7910) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 49.8157(49.8791) | Bit/dim 4.2219(4.3517) | Xent 1.6063(1.6751) | Loss 10.8621(11.6073) | Error 0.5745(0.5977) Steps 538(545.54) | Grad Norm 9.0477(14.6187) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 48.3182(49.8323) | Bit/dim 4.2300(4.3481) | Xent 1.6058(1.6730) | Loss 10.6605(11.5789) | Error 0.5774(0.5971) Steps 550(545.67) | Grad Norm 8.8324(14.4451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 22.4295, Epoch Time 326.7756(326.1162), Bit/dim 4.2325(best: 4.2561), Xent 1.5509, Loss 5.0080, Error 0.5635(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 49.6165(49.8258) | Bit/dim 4.2295(4.3445) | Xent 1.6469(1.6722) | Loss 13.9551(11.6502) | Error 0.5871(0.5968) Steps 556(545.98) | Grad Norm 13.4756(14.4161) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 49.7252(49.8228) | Bit/dim 4.2943(4.3430) | Xent 1.6637(1.6720) | Loss 11.1537(11.6353) | Error 0.5941(0.5967) Steps 520(545.20) | Grad Norm 19.2589(14.5613) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 49.2967(49.8070) | Bit/dim 4.2303(4.3396) | Xent 1.6114(1.6701) | Loss 10.7646(11.6092) | Error 0.5846(0.5963) Steps 550(545.35) | Grad Norm 13.2232(14.5212) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 50.2781(49.8211) | Bit/dim 4.2099(4.3357) | Xent 1.7380(1.6722) | Loss 10.8172(11.5854) | Error 0.6045(0.5966) Steps 562(545.85) | Grad Norm 33.0934(15.0784) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 48.7650(49.7895) | Bit/dim 4.2435(4.3330) | Xent 1.6267(1.6708) | Loss 10.9492(11.5663) | Error 0.5895(0.5964) Steps 538(545.61) | Grad Norm 14.4529(15.0596) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 49.8875(49.7924) | Bit/dim 4.2224(4.3296) | Xent 1.5979(1.6686) | Loss 10.7608(11.5422) | Error 0.5664(0.5955) Steps 568(546.28) | Grad Norm 9.1795(14.8832) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 22.7455, Epoch Time 336.6442(326.4321), Bit/dim 4.2245(best: 4.2325), Xent 1.5707, Loss 5.0099, Error 0.5630(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 49.1245(49.7724) | Bit/dim 4.2054(4.3259) | Xent 1.6202(1.6672) | Loss 12.7503(11.5784) | Error 0.5813(0.5950) Steps 556(546.57) | Grad Norm 16.1998(14.9227) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 50.9971(49.8091) | Bit/dim 4.2249(4.3229) | Xent 1.6199(1.6658) | Loss 10.8540(11.5567) | Error 0.5771(0.5945) Steps 556(546.86) | Grad Norm 11.4322(14.8180) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 52.8451(49.9002) | Bit/dim 4.1954(4.3191) | Xent 1.5905(1.6635) | Loss 10.6533(11.5296) | Error 0.5727(0.5938) Steps 544(546.77) | Grad Norm 10.8788(14.6998) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 49.2606(49.8810) | Bit/dim 4.2062(4.3157) | Xent 1.6203(1.6622) | Loss 11.0502(11.5152) | Error 0.5874(0.5936) Steps 514(545.79) | Grad Norm 10.1685(14.5639) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 48.1274(49.8284) | Bit/dim 4.2427(4.3135) | Xent 1.6424(1.6616) | Loss 11.1219(11.5034) | Error 0.5893(0.5935) Steps 562(546.27) | Grad Norm 17.3767(14.6482) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 48.6180(49.7921) | Bit/dim 4.2081(4.3103) | Xent 1.6144(1.6602) | Loss 10.2288(11.4652) | Error 0.5786(0.5931) Steps 514(545.31) | Grad Norm 12.6721(14.5890) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 22.1367, Epoch Time 337.5621(326.7660), Bit/dim 4.2031(best: 4.2245), Xent 1.5105, Loss 4.9584, Error 0.5404(best: 0.5481)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 48.5237(49.7540) | Bit/dim 4.2129(4.3074) | Xent 1.5631(1.6573) | Loss 13.8206(11.5358) | Error 0.5634(0.5922) Steps 550(545.45) | Grad Norm 8.0606(14.3931) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 54.9078(49.9086) | Bit/dim 4.2461(4.3056) | Xent 1.7432(1.6599) | Loss 11.0453(11.5211) | Error 0.6116(0.5928) Steps 574(546.30) | Grad Norm 34.6327(15.0003) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 50.9300(49.9393) | Bit/dim 4.2114(4.3027) | Xent 1.6251(1.6588) | Loss 10.8361(11.5006) | Error 0.5860(0.5926) Steps 556(546.59) | Grad Norm 13.7538(14.9629) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 46.8975(49.8480) | Bit/dim 4.2059(4.2998) | Xent 1.5937(1.6569) | Loss 10.8743(11.4818) | Error 0.5725(0.5920) Steps 544(546.52) | Grad Norm 15.2120(14.9704) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 52.7353(49.9346) | Bit/dim 4.2224(4.2975) | Xent 1.5948(1.6550) | Loss 10.8245(11.4620) | Error 0.5701(0.5913) Steps 562(546.98) | Grad Norm 8.4371(14.7744) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 50.8072(49.9608) | Bit/dim 4.2420(4.2958) | Xent 1.6198(1.6539) | Loss 11.1056(11.4514) | Error 0.5844(0.5911) Steps 544(546.89) | Grad Norm 13.0362(14.7222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 22.3392, Epoch Time 343.2828(327.2615), Bit/dim 4.2182(best: 4.2031), Xent 1.4978, Loss 4.9671, Error 0.5361(best: 0.5404)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 52.3675(50.0330) | Bit/dim 4.2223(4.2936) | Xent 1.5597(1.6511) | Loss 13.7676(11.5208) | Error 0.5597(0.5902) Steps 538(546.63) | Grad Norm 12.1082(14.6438) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 53.1422(50.1263) | Bit/dim 4.1879(4.2905) | Xent 1.6543(1.6512) | Loss 10.8545(11.5008) | Error 0.5914(0.5902) Steps 532(546.19) | Grad Norm 24.1406(14.9287) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 52.5173(50.1980) | Bit/dim 4.2148(4.2882) | Xent 1.5883(1.6493) | Loss 10.7875(11.4795) | Error 0.5714(0.5896) Steps 556(546.48) | Grad Norm 13.9790(14.9002) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 51.3975(50.2340) | Bit/dim 4.1898(4.2852) | Xent 1.5473(1.6463) | Loss 10.6933(11.4559) | Error 0.5550(0.5886) Steps 568(547.13) | Grad Norm 10.3763(14.7645) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 50.9113(50.2543) | Bit/dim 4.2390(4.2838) | Xent 1.6136(1.6453) | Loss 10.7778(11.4355) | Error 0.5736(0.5881) Steps 574(547.93) | Grad Norm 23.1913(15.0173) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 49.6114(50.2350) | Bit/dim 4.2236(4.2820) | Xent 1.5654(1.6429) | Loss 10.6596(11.4122) | Error 0.5600(0.5873) Steps 526(547.27) | Grad Norm 10.6049(14.8849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 22.4472, Epoch Time 348.5328(327.8996), Bit/dim 4.1922(best: 4.2031), Xent 1.5871, Loss 4.9857, Error 0.5662(best: 0.5361)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 52.5657(50.3050) | Bit/dim 4.2130(4.2800) | Xent 1.6595(1.6434) | Loss 14.2556(11.4975) | Error 0.5827(0.5872) Steps 556(547.54) | Grad Norm 26.2599(15.2262) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 51.3419(50.3361) | Bit/dim 4.1923(4.2773) | Xent 1.5604(1.6409) | Loss 10.9364(11.4807) | Error 0.5591(0.5863) Steps 526(546.89) | Grad Norm 10.5325(15.0854) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 49.7903(50.3197) | Bit/dim 4.1745(4.2743) | Xent 1.5722(1.6388) | Loss 10.6186(11.4548) | Error 0.5715(0.5859) Steps 550(546.98) | Grad Norm 10.5667(14.9498) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 49.6102(50.2984) | Bit/dim 4.1923(4.2718) | Xent 1.5940(1.6375) | Loss 10.6203(11.4298) | Error 0.5671(0.5853) Steps 538(546.71) | Grad Norm 13.5776(14.9087) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 47.1599(50.2043) | Bit/dim 4.2112(4.2700) | Xent 1.5736(1.6356) | Loss 10.5692(11.4040) | Error 0.5635(0.5847) Steps 532(546.27) | Grad Norm 12.5357(14.8375) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 49.9955(50.1980) | Bit/dim 4.1705(4.2670) | Xent 1.5260(1.6323) | Loss 10.5486(11.3783) | Error 0.5520(0.5837) Steps 544(546.20) | Grad Norm 7.3741(14.6136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 21.9757, Epoch Time 338.4228(328.2153), Bit/dim 4.1627(best: 4.1922), Xent 1.4745, Loss 4.9000, Error 0.5298(best: 0.5361)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 46.9685(50.1011) | Bit/dim 4.1592(4.2638) | Xent 1.5375(1.6294) | Loss 13.5493(11.4435) | Error 0.5571(0.5829) Steps 514(545.24) | Grad Norm 13.2021(14.5712) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 47.6604(50.0279) | Bit/dim 4.1922(4.2616) | Xent 1.6445(1.6299) | Loss 10.6914(11.4209) | Error 0.5840(0.5829) Steps 538(545.02) | Grad Norm 31.4377(15.0772) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 47.1002(49.9401) | Bit/dim 4.1802(4.2592) | Xent 1.5381(1.6271) | Loss 10.6254(11.3970) | Error 0.5526(0.5820) Steps 532(544.63) | Grad Norm 12.3893(14.9966) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 50.6947(49.9627) | Bit/dim 4.1812(4.2568) | Xent 1.5510(1.6249) | Loss 10.1895(11.3608) | Error 0.5589(0.5813) Steps 556(544.97) | Grad Norm 13.1192(14.9403) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 44.5905(49.8015) | Bit/dim 4.1700(4.2542) | Xent 1.6024(1.6242) | Loss 10.7859(11.3436) | Error 0.5856(0.5814) Steps 490(543.32) | Grad Norm 17.8095(15.0263) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 46.3626(49.6984) | Bit/dim 4.1604(4.2514) | Xent 1.7277(1.6273) | Loss 10.9705(11.3324) | Error 0.6125(0.5824) Steps 490(541.72) | Grad Norm 25.5969(15.3434) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 21.2551, Epoch Time 320.5005(327.9839), Bit/dim 4.1799(best: 4.1627), Xent 1.5424, Loss 4.9511, Error 0.5489(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 46.3318(49.5974) | Bit/dim 4.1702(4.2490) | Xent 1.5920(1.6262) | Loss 13.3997(11.3944) | Error 0.5722(0.5821) Steps 538(541.61) | Grad Norm 11.3287(15.2230) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 52.6329(49.6884) | Bit/dim 4.1487(4.2460) | Xent 1.5796(1.6248) | Loss 10.6456(11.3719) | Error 0.5716(0.5818) Steps 538(541.50) | Grad Norm 7.7584(14.9991) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 55.3275(49.8576) | Bit/dim 4.1693(4.2437) | Xent 1.5509(1.6226) | Loss 10.7407(11.3530) | Error 0.5616(0.5811) Steps 580(542.66) | Grad Norm 9.3219(14.8288) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 48.8606(49.8277) | Bit/dim 4.1546(4.2410) | Xent 1.5339(1.6200) | Loss 10.7656(11.3354) | Error 0.5457(0.5801) Steps 556(543.06) | Grad Norm 11.4884(14.7285) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 52.2316(49.8998) | Bit/dim 4.1667(4.2388) | Xent 1.6963(1.6222) | Loss 10.8197(11.3199) | Error 0.6011(0.5807) Steps 568(543.81) | Grad Norm 25.7242(15.0584) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 50.0356(49.9039) | Bit/dim 4.1850(4.2372) | Xent 1.5717(1.6207) | Loss 10.7876(11.3039) | Error 0.5674(0.5803) Steps 544(543.81) | Grad Norm 9.9061(14.9038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.0574, Epoch Time 344.2372(328.4715), Bit/dim 4.1276(best: 4.1627), Xent 1.4844, Loss 4.8698, Error 0.5298(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 48.6981(49.8677) | Bit/dim 4.1310(4.2340) | Xent 1.5366(1.6182) | Loss 13.1147(11.3583) | Error 0.5491(0.5794) Steps 538(543.64) | Grad Norm 4.3348(14.5868) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 53.5266(49.9775) | Bit/dim 4.1454(4.2313) | Xent 1.6006(1.6177) | Loss 10.7828(11.3410) | Error 0.5800(0.5794) Steps 550(543.83) | Grad Norm 20.6906(14.7699) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 49.1534(49.9528) | Bit/dim 4.1970(4.2303) | Xent 1.5826(1.6166) | Loss 10.8391(11.3259) | Error 0.5610(0.5788) Steps 544(543.83) | Grad Norm 20.6039(14.9449) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 49.0690(49.9262) | Bit/dim 4.1349(4.2274) | Xent 1.5280(1.6140) | Loss 10.4851(11.3007) | Error 0.5551(0.5781) Steps 532(543.48) | Grad Norm 4.5055(14.6317) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 49.8730(49.9246) | Bit/dim 4.1468(4.2250) | Xent 1.5770(1.6129) | Loss 10.6140(11.2801) | Error 0.5684(0.5778) Steps 550(543.67) | Grad Norm 14.8872(14.6394) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 50.2804(49.9353) | Bit/dim 4.1243(4.2220) | Xent 1.5603(1.6113) | Loss 10.7018(11.2628) | Error 0.5644(0.5774) Steps 556(544.04) | Grad Norm 7.0874(14.4128) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 21.9305, Epoch Time 338.9215(328.7850), Bit/dim 4.1344(best: 4.1276), Xent 1.5041, Loss 4.8865, Error 0.5415(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 48.8922(49.9040) | Bit/dim 4.1333(4.2193) | Xent 1.5898(1.6106) | Loss 13.9021(11.3419) | Error 0.5697(0.5772) Steps 532(543.68) | Grad Norm 16.3395(14.4706) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 47.9580(49.8456) | Bit/dim 4.1882(4.2184) | Xent 1.5913(1.6101) | Loss 10.9001(11.3287) | Error 0.5711(0.5770) Steps 556(544.05) | Grad Norm 21.2627(14.6744) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 54.0753(49.9725) | Bit/dim 4.1213(4.2155) | Xent 1.5183(1.6073) | Loss 10.2619(11.2967) | Error 0.5433(0.5760) Steps 538(543.87) | Grad Norm 6.6722(14.4343) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 50.0204(49.9740) | Bit/dim 4.1527(4.2136) | Xent 1.5650(1.6060) | Loss 10.5766(11.2751) | Error 0.5729(0.5759) Steps 556(544.23) | Grad Norm 16.0012(14.4813) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 49.6769(49.9651) | Bit/dim 4.1988(4.2131) | Xent 1.5095(1.6031) | Loss 10.7114(11.2582) | Error 0.5448(0.5750) Steps 556(544.59) | Grad Norm 8.6473(14.3063) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 46.6177(49.8646) | Bit/dim 4.1359(4.2108) | Xent 1.5052(1.6002) | Loss 10.5421(11.2367) | Error 0.5480(0.5742) Steps 532(544.21) | Grad Norm 8.2131(14.1235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 21.3183, Epoch Time 334.4466(328.9548), Bit/dim 4.1767(best: 4.1276), Xent 1.4834, Loss 4.9184, Error 0.5315(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 54.3078(49.9979) | Bit/dim 4.1852(4.2101) | Xent 1.5717(1.5993) | Loss 13.5680(11.3066) | Error 0.5536(0.5736) Steps 556(544.56) | Grad Norm 18.2752(14.2481) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 46.8110(49.9023) | Bit/dim 4.1263(4.2075) | Xent 1.5232(1.5971) | Loss 10.5992(11.2854) | Error 0.5525(0.5729) Steps 526(544.01) | Grad Norm 14.9615(14.2695) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 49.7124(49.8966) | Bit/dim 4.1527(4.2059) | Xent 1.5159(1.5946) | Loss 10.6513(11.2664) | Error 0.5473(0.5722) Steps 544(544.01) | Grad Norm 12.2973(14.2103) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 48.8812(49.8662) | Bit/dim 4.1387(4.2039) | Xent 1.5153(1.5922) | Loss 10.3457(11.2388) | Error 0.5489(0.5715) Steps 526(543.47) | Grad Norm 11.1056(14.1172) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 49.4649(49.8541) | Bit/dim 4.1048(4.2009) | Xent 1.5201(1.5901) | Loss 10.7077(11.2228) | Error 0.5485(0.5708) Steps 532(543.12) | Grad Norm 7.2624(13.9115) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 49.1883(49.8342) | Bit/dim 4.1352(4.1989) | Xent 1.4812(1.5868) | Loss 10.6761(11.2064) | Error 0.5341(0.5697) Steps 544(543.15) | Grad Norm 7.8751(13.7304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 22.2580, Epoch Time 336.6850(329.1867), Bit/dim 4.1412(best: 4.1276), Xent 1.4708, Loss 4.8766, Error 0.5301(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 48.5635(49.7960) | Bit/dim 4.1333(4.1970) | Xent 1.5381(1.5854) | Loss 13.5997(11.2782) | Error 0.5517(0.5691) Steps 544(543.17) | Grad Norm 21.6202(13.9671) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 48.5024(49.7572) | Bit/dim 4.1286(4.1949) | Xent 1.5603(1.5846) | Loss 10.6065(11.2581) | Error 0.5515(0.5686) Steps 538(543.02) | Grad Norm 19.3154(14.1276) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 49.7342(49.7565) | Bit/dim 4.0972(4.1920) | Xent 1.5435(1.5834) | Loss 10.5167(11.2358) | Error 0.5475(0.5680) Steps 550(543.23) | Grad Norm 14.7066(14.1449) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 47.7042(49.6950) | Bit/dim 4.1188(4.1898) | Xent 1.5567(1.5826) | Loss 10.5755(11.2160) | Error 0.5641(0.5679) Steps 520(542.53) | Grad Norm 18.8715(14.2867) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 45.9259(49.5819) | Bit/dim 4.1463(4.1885) | Xent 1.5098(1.5804) | Loss 10.5603(11.1963) | Error 0.5439(0.5671) Steps 526(542.04) | Grad Norm 16.0132(14.3385) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 55.5836(49.7619) | Bit/dim 4.1056(4.1860) | Xent 1.4714(1.5771) | Loss 10.4831(11.1750) | Error 0.5348(0.5662) Steps 568(542.81) | Grad Norm 7.1350(14.1224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 21.4603, Epoch Time 333.5723(329.3183), Bit/dim 4.0899(best: 4.1276), Xent 1.4383, Loss 4.8090, Error 0.5163(best: 0.5298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 47.8080(49.7033) | Bit/dim 4.0918(4.1832) | Xent 1.4850(1.5744) | Loss 13.2614(11.2375) | Error 0.5366(0.5653) Steps 544(542.85) | Grad Norm 12.9319(14.0867) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 50.1204(49.7158) | Bit/dim 4.1020(4.1807) | Xent 1.5626(1.5740) | Loss 10.4161(11.2129) | Error 0.5499(0.5648) Steps 568(543.60) | Grad Norm 22.4245(14.3368) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 49.1293(49.6982) | Bit/dim 4.1079(4.1786) | Xent 1.4659(1.5708) | Loss 10.5742(11.1937) | Error 0.5251(0.5636) Steps 538(543.44) | Grad Norm 9.7754(14.2000) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 47.7052(49.6385) | Bit/dim 4.1197(4.1768) | Xent 1.4649(1.5676) | Loss 10.3594(11.1687) | Error 0.5250(0.5625) Steps 544(543.45) | Grad Norm 12.4344(14.1470) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 46.6590(49.5491) | Bit/dim 4.0861(4.1741) | Xent 1.5092(1.5658) | Loss 10.3975(11.1456) | Error 0.5384(0.5617) Steps 550(543.65) | Grad Norm 9.9483(14.0211) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 45.6586(49.4324) | Bit/dim 4.0687(4.1709) | Xent 1.4777(1.5632) | Loss 10.3173(11.1207) | Error 0.5353(0.5609) Steps 520(542.94) | Grad Norm 7.1837(13.8160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 22.9209, Epoch Time 326.2506(329.2262), Bit/dim 4.0763(best: 4.0899), Xent 1.4014, Loss 4.7771, Error 0.4992(best: 0.5163)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 48.3129(49.3988) | Bit/dim 4.0877(4.1684) | Xent 1.4772(1.5606) | Loss 13.3492(11.1876) | Error 0.5345(0.5602) Steps 514(542.07) | Grad Norm 8.6773(13.6618) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 51.4592(49.4606) | Bit/dim 4.0734(4.1656) | Xent 1.4625(1.5577) | Loss 10.4210(11.1646) | Error 0.5286(0.5592) Steps 538(541.95) | Grad Norm 12.6886(13.6326) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 51.9258(49.5345) | Bit/dim 4.0773(4.1629) | Xent 1.5664(1.5579) | Loss 10.6494(11.1491) | Error 0.5599(0.5592) Steps 514(541.11) | Grad Norm 21.8311(13.8786) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 52.5955(49.6264) | Bit/dim 4.0848(4.1606) | Xent 1.5619(1.5580) | Loss 10.6248(11.1334) | Error 0.5594(0.5592) Steps 550(541.38) | Grad Norm 23.4006(14.1642) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 48.1172(49.5811) | Bit/dim 4.0778(4.1581) | Xent 1.4794(1.5557) | Loss 10.5625(11.1163) | Error 0.5370(0.5586) Steps 544(541.46) | Grad Norm 8.7455(14.0017) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 50.6836(49.6142) | Bit/dim 4.0629(4.1552) | Xent 1.5902(1.5567) | Loss 10.6657(11.1028) | Error 0.5642(0.5587) Steps 526(540.99) | Grad Norm 14.9806(14.0310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 22.7357, Epoch Time 342.1228(329.6131), Bit/dim 4.0897(best: 4.0763), Xent 1.4536, Loss 4.8165, Error 0.5291(best: 0.4992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 51.0416(49.6570) | Bit/dim 4.0910(4.1533) | Xent 1.4878(1.5547) | Loss 13.2266(11.1665) | Error 0.5389(0.5581) Steps 562(541.62) | Grad Norm 11.9526(13.9687) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 49.0490(49.6388) | Bit/dim 4.0545(4.1503) | Xent 1.4800(1.5524) | Loss 10.1621(11.1363) | Error 0.5345(0.5574) Steps 562(542.23) | Grad Norm 9.0373(13.8207) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 49.1801(49.6250) | Bit/dim 4.0787(4.1482) | Xent 1.4927(1.5506) | Loss 10.4417(11.1155) | Error 0.5470(0.5571) Steps 550(542.47) | Grad Norm 15.0857(13.8587) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 53.6916(49.7470) | Bit/dim 4.1061(4.1469) | Xent 1.4669(1.5481) | Loss 10.5451(11.0984) | Error 0.5276(0.5562) Steps 532(542.15) | Grad Norm 12.4043(13.8150) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 49.1209(49.7282) | Bit/dim 4.0732(4.1447) | Xent 1.4836(1.5462) | Loss 10.5048(11.0806) | Error 0.5403(0.5558) Steps 544(542.21) | Grad Norm 8.1710(13.6457) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 51.3829(49.7778) | Bit/dim 4.0671(4.1424) | Xent 1.4858(1.5444) | Loss 10.4563(11.0619) | Error 0.5341(0.5551) Steps 556(542.62) | Grad Norm 11.7210(13.5880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 22.0628, Epoch Time 341.7399(329.9769), Bit/dim 4.0846(best: 4.0763), Xent 1.4441, Loss 4.8066, Error 0.5222(best: 0.4992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 53.2534(49.8821) | Bit/dim 4.0888(4.1408) | Xent 1.4910(1.5428) | Loss 13.5834(11.1375) | Error 0.5470(0.5549) Steps 580(543.74) | Grad Norm 17.3725(13.7015) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 51.8874(49.9423) | Bit/dim 4.0553(4.1382) | Xent 1.4806(1.5409) | Loss 10.4251(11.1161) | Error 0.5336(0.5542) Steps 532(543.39) | Grad Norm 12.0783(13.6528) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 53.2162(50.0405) | Bit/dim 4.0710(4.1362) | Xent 1.4278(1.5375) | Loss 10.6253(11.1014) | Error 0.5152(0.5531) Steps 568(544.13) | Grad Norm 10.8616(13.5691) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 50.3495(50.0498) | Bit/dim 4.0717(4.1343) | Xent 1.4343(1.5344) | Loss 10.1007(11.0714) | Error 0.5177(0.5520) Steps 574(545.03) | Grad Norm 9.0573(13.4337) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 51.9009(50.1053) | Bit/dim 4.0550(4.1319) | Xent 1.4859(1.5330) | Loss 10.3752(11.0505) | Error 0.5414(0.5517) Steps 544(545.00) | Grad Norm 14.7951(13.4746) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 48.6529(50.0617) | Bit/dim 4.0456(4.1293) | Xent 1.4930(1.5318) | Loss 10.4550(11.0326) | Error 0.5333(0.5511) Steps 544(544.97) | Grad Norm 13.5929(13.4781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.2677, Epoch Time 348.5247(330.5334), Bit/dim 4.0574(best: 4.0763), Xent 1.4153, Loss 4.7650, Error 0.5112(best: 0.4992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 48.6415(50.0191) | Bit/dim 4.0559(4.1271) | Xent 1.4792(1.5302) | Loss 13.3776(11.1030) | Error 0.5384(0.5507) Steps 538(544.76) | Grad Norm 13.0221(13.4644) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 47.3845(49.9401) | Bit/dim 4.0385(4.1244) | Xent 1.4699(1.5284) | Loss 10.4367(11.0830) | Error 0.5276(0.5500) Steps 532(544.37) | Grad Norm 13.7881(13.4742) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 48.5473(49.8983) | Bit/dim 4.0691(4.1228) | Xent 1.4622(1.5264) | Loss 10.5793(11.0679) | Error 0.5240(0.5493) Steps 544(544.36) | Grad Norm 14.6017(13.5080) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 49.6578(49.8911) | Bit/dim 4.0498(4.1206) | Xent 1.4202(1.5232) | Loss 10.3138(11.0453) | Error 0.5186(0.5483) Steps 544(544.35) | Grad Norm 5.0081(13.2530) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 46.7240(49.7961) | Bit/dim 4.0463(4.1184) | Xent 1.4396(1.5207) | Loss 10.4588(11.0277) | Error 0.5280(0.5477) Steps 544(544.34) | Grad Norm 8.2949(13.1042) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 49.6587(49.7919) | Bit/dim 4.0408(4.1160) | Xent 1.4447(1.5184) | Loss 10.4077(11.0091) | Error 0.5246(0.5470) Steps 556(544.69) | Grad Norm 13.7098(13.1224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 21.9338, Epoch Time 328.4181(330.4699), Bit/dim 4.0619(best: 4.0574), Xent 1.3385, Loss 4.7312, Error 0.4847(best: 0.4992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 50.9457(49.8266) | Bit/dim 4.0558(4.1142) | Xent 1.3894(1.5145) | Loss 12.8143(11.0632) | Error 0.5002(0.5456) Steps 562(545.21) | Grad Norm 9.8235(13.0234) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 46.4301(49.7247) | Bit/dim 4.0628(4.1127) | Xent 1.4193(1.5117) | Loss 10.3385(11.0415) | Error 0.5099(0.5446) Steps 544(545.17) | Grad Norm 8.7544(12.8954) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 56.1384(49.9171) | Bit/dim 4.0367(4.1104) | Xent 1.4176(1.5089) | Loss 10.4484(11.0237) | Error 0.5165(0.5437) Steps 568(545.86) | Grad Norm 7.0780(12.7209) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 50.5877(49.9372) | Bit/dim 4.0186(4.1076) | Xent 1.4260(1.5064) | Loss 10.2738(11.0012) | Error 0.5221(0.5431) Steps 520(545.08) | Grad Norm 11.1175(12.6728) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 46.3218(49.8287) | Bit/dim 4.0310(4.1053) | Xent 1.4957(1.5061) | Loss 10.3229(10.9808) | Error 0.5360(0.5429) Steps 538(544.87) | Grad Norm 19.9144(12.8900) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 52.4743(49.9081) | Bit/dim 4.0381(4.1033) | Xent 1.4489(1.5043) | Loss 10.5304(10.9673) | Error 0.5211(0.5422) Steps 550(545.02) | Grad Norm 12.4243(12.8760) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 22.2959, Epoch Time 341.4422(330.7991), Bit/dim 4.0485(best: 4.0574), Xent 1.4007, Loss 4.7488, Error 0.5027(best: 0.4847)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 50.5139(49.9263) | Bit/dim 4.0472(4.1017) | Xent 1.4501(1.5027) | Loss 13.4797(11.0427) | Error 0.5270(0.5418) Steps 550(545.17) | Grad Norm 12.9195(12.8773) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 49.2275(49.9053) | Bit/dim 4.0484(4.1001) | Xent 1.4612(1.5015) | Loss 10.5595(11.0282) | Error 0.5284(0.5414) Steps 544(545.14) | Grad Norm 16.8621(12.9969) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 53.0440(49.9995) | Bit/dim 4.0231(4.0977) | Xent 1.4220(1.4991) | Loss 10.3593(11.0081) | Error 0.5128(0.5405) Steps 556(545.46) | Grad Norm 14.3601(13.0378) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 52.5415(50.0757) | Bit/dim 4.0380(4.0959) | Xent 1.4984(1.4991) | Loss 10.7042(10.9990) | Error 0.5400(0.5405) Steps 544(545.42) | Grad Norm 19.4062(13.2288) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 54.3303(50.2034) | Bit/dim 4.0574(4.0948) | Xent 1.5394(1.5003) | Loss 10.3932(10.9808) | Error 0.5431(0.5406) Steps 580(546.46) | Grad Norm 18.9210(13.3996) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 50.1280(50.2011) | Bit/dim 4.0442(4.0933) | Xent 1.4778(1.4996) | Loss 10.3559(10.9621) | Error 0.5334(0.5403) Steps 556(546.74) | Grad Norm 11.5238(13.3433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 23.2060, Epoch Time 348.7303(331.3370), Bit/dim 4.0554(best: 4.0485), Xent 1.3737, Loss 4.7423, Error 0.4920(best: 0.4847)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 44.9520(50.0436) | Bit/dim 4.0490(4.0919) | Xent 1.4176(1.4971) | Loss 13.0193(11.0238) | Error 0.5112(0.5395) Steps 544(546.66) | Grad Norm 12.0477(13.3045) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 49.1566(50.0170) | Bit/dim 4.0626(4.0911) | Xent 1.4545(1.4959) | Loss 10.4364(11.0062) | Error 0.5236(0.5390) Steps 550(546.76) | Grad Norm 8.0948(13.1482) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 46.9138(49.9239) | Bit/dim 4.0273(4.0892) | Xent 1.4359(1.4941) | Loss 10.3173(10.9855) | Error 0.5212(0.5385) Steps 526(546.14) | Grad Norm 7.9187(12.9913) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 49.9741(49.9254) | Bit/dim 4.0594(4.0883) | Xent 1.4239(1.4920) | Loss 10.3230(10.9657) | Error 0.5181(0.5379) Steps 556(546.43) | Grad Norm 13.4445(13.0049) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 52.6481(50.0071) | Bit/dim 4.0388(4.0868) | Xent 1.4565(1.4909) | Loss 10.3628(10.9476) | Error 0.5266(0.5375) Steps 574(547.26) | Grad Norm 18.6586(13.1745) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 47.5908(49.9346) | Bit/dim 4.0552(4.0858) | Xent 1.4973(1.4911) | Loss 10.3428(10.9294) | Error 0.5331(0.5374) Steps 532(546.80) | Grad Norm 17.6750(13.3095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 22.7535, Epoch Time 329.9200(331.2945), Bit/dim 4.0415(best: 4.0485), Xent 1.3366, Loss 4.7098, Error 0.4805(best: 0.4847)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 51.9948(49.9964) | Bit/dim 4.0519(4.0848) | Xent 1.4128(1.4887) | Loss 13.4277(11.0044) | Error 0.5137(0.5367) Steps 574(547.62) | Grad Norm 5.6792(13.0806) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 52.3288(50.0664) | Bit/dim 4.0389(4.0834) | Xent 1.4956(1.4889) | Loss 10.2354(10.9813) | Error 0.5371(0.5367) Steps 532(547.15) | Grad Norm 16.6110(13.1865) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 49.6876(50.0550) | Bit/dim 4.0235(4.0816) | Xent 1.4578(1.4880) | Loss 10.3203(10.9615) | Error 0.5299(0.5365) Steps 562(547.60) | Grad Norm 11.7246(13.1426) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 49.3757(50.0347) | Bit/dim 4.0159(4.0797) | Xent 1.4183(1.4859) | Loss 10.3437(10.9429) | Error 0.5139(0.5358) Steps 568(548.21) | Grad Norm 5.9206(12.9260) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 50.2713(50.0418) | Bit/dim 4.0388(4.0784) | Xent 1.4161(1.4838) | Loss 10.3798(10.9260) | Error 0.5120(0.5351) Steps 532(547.72) | Grad Norm 8.3935(12.7900) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 53.8286(50.1554) | Bit/dim 4.0306(4.0770) | Xent 1.4345(1.4823) | Loss 10.4337(10.9113) | Error 0.5224(0.5347) Steps 538(547.43) | Grad Norm 9.4964(12.6912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 22.6062, Epoch Time 346.2517(331.7432), Bit/dim 4.0179(best: 4.0415), Xent 1.3489, Loss 4.6924, Error 0.4818(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 46.1276(50.0345) | Bit/dim 4.0254(4.0755) | Xent 1.4272(1.4807) | Loss 13.2994(10.9829) | Error 0.5085(0.5339) Steps 526(546.79) | Grad Norm 8.6469(12.5699) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 52.2657(50.1015) | Bit/dim 4.0345(4.0742) | Xent 1.4766(1.4806) | Loss 10.3343(10.9635) | Error 0.5335(0.5339) Steps 538(546.52) | Grad Norm 19.2882(12.7714) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 42.7075(49.8796) | Bit/dim 4.0436(4.0733) | Xent 1.3898(1.4778) | Loss 10.1771(10.9399) | Error 0.5019(0.5330) Steps 514(545.55) | Grad Norm 8.2577(12.6360) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 50.9714(49.9124) | Bit/dim 3.9998(4.0711) | Xent 1.4033(1.4756) | Loss 10.2549(10.9193) | Error 0.5057(0.5321) Steps 568(546.22) | Grad Norm 5.0431(12.4082) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 46.8477(49.8205) | Bit/dim 4.0297(4.0699) | Xent 1.3742(1.4726) | Loss 10.2669(10.8997) | Error 0.4948(0.5310) Steps 532(545.80) | Grad Norm 6.7340(12.2380) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 50.9347(49.8539) | Bit/dim 4.0111(4.0681) | Xent 1.3936(1.4702) | Loss 10.1832(10.8783) | Error 0.5054(0.5302) Steps 556(546.10) | Grad Norm 9.6785(12.1612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 22.3144, Epoch Time 328.4366(331.6440), Bit/dim 4.0114(best: 4.0179), Xent 1.3605, Loss 4.6917, Error 0.4949(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 52.1844(49.9238) | Bit/dim 4.0164(4.0665) | Xent 1.4215(1.4687) | Loss 12.2342(10.9189) | Error 0.5112(0.5297) Steps 562(546.58) | Grad Norm 11.2764(12.1347) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 50.5842(49.9436) | Bit/dim 4.0288(4.0654) | Xent 1.3789(1.4660) | Loss 10.2162(10.8978) | Error 0.5016(0.5288) Steps 568(547.22) | Grad Norm 11.0643(12.1026) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 51.9269(50.0031) | Bit/dim 3.9926(4.0632) | Xent 1.4121(1.4644) | Loss 10.4193(10.8835) | Error 0.5145(0.5284) Steps 574(548.02) | Grad Norm 9.3662(12.0205) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 50.8446(50.0284) | Bit/dim 4.0395(4.0625) | Xent 1.3903(1.4622) | Loss 10.5613(10.8738) | Error 0.4975(0.5275) Steps 550(548.08) | Grad Norm 15.0458(12.1112) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 48.7185(49.9891) | Bit/dim 4.0340(4.0617) | Xent 1.3856(1.4599) | Loss 10.2414(10.8549) | Error 0.4946(0.5265) Steps 550(548.14) | Grad Norm 12.0662(12.1099) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 54.3420(50.1197) | Bit/dim 3.9883(4.0595) | Xent 1.3952(1.4580) | Loss 10.2762(10.8375) | Error 0.4956(0.5256) Steps 574(548.92) | Grad Norm 11.6046(12.0947) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 22.5962, Epoch Time 347.0074(332.1049), Bit/dim 4.0269(best: 4.0114), Xent 1.3563, Loss 4.7050, Error 0.4878(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 49.9451(50.1144) | Bit/dim 4.0201(4.0583) | Xent 1.4099(1.4565) | Loss 13.1839(10.9079) | Error 0.5089(0.5251) Steps 532(548.41) | Grad Norm 15.8316(12.2068) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 46.6443(50.0103) | Bit/dim 4.0064(4.0567) | Xent 1.3797(1.4542) | Loss 10.3232(10.8903) | Error 0.5032(0.5244) Steps 550(548.46) | Grad Norm 7.8127(12.0750) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 52.8551(50.0957) | Bit/dim 3.9898(4.0547) | Xent 1.3770(1.4519) | Loss 10.3223(10.8733) | Error 0.4990(0.5236) Steps 556(548.68) | Grad Norm 9.3932(11.9945) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 46.6100(49.9911) | Bit/dim 4.0142(4.0535) | Xent 1.3881(1.4500) | Loss 10.4145(10.8595) | Error 0.5030(0.5230) Steps 538(548.36) | Grad Norm 11.4177(11.9772) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 49.2409(49.9686) | Bit/dim 4.0051(4.0520) | Xent 1.4336(1.4495) | Loss 10.1158(10.8372) | Error 0.5141(0.5228) Steps 550(548.41) | Grad Norm 16.8397(12.1231) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 48.8853(49.9361) | Bit/dim 4.0212(4.0511) | Xent 1.4189(1.4486) | Loss 10.3176(10.8216) | Error 0.5040(0.5222) Steps 520(547.56) | Grad Norm 13.7222(12.1711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 22.3793, Epoch Time 332.7150(332.1232), Bit/dim 4.0057(best: 4.0114), Xent 1.3530, Loss 4.6822, Error 0.4866(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 53.2020(50.0341) | Bit/dim 4.0192(4.0502) | Xent 1.3915(1.4469) | Loss 13.2026(10.8931) | Error 0.4942(0.5214) Steps 598(549.07) | Grad Norm 9.4203(12.0886) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 50.7731(50.0562) | Bit/dim 3.9845(4.0482) | Xent 1.3995(1.4454) | Loss 10.2200(10.8729) | Error 0.5026(0.5208) Steps 532(548.56) | Grad Norm 11.2609(12.0637) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 53.6889(50.1652) | Bit/dim 3.9928(4.0465) | Xent 1.3523(1.4426) | Loss 10.0744(10.8489) | Error 0.4931(0.5200) Steps 538(548.24) | Grad Norm 6.8748(11.9081) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 49.8307(50.1552) | Bit/dim 4.0181(4.0457) | Xent 1.3880(1.4410) | Loss 10.3079(10.8327) | Error 0.5021(0.5194) Steps 544(548.12) | Grad Norm 7.8934(11.7876) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 46.5205(50.0461) | Bit/dim 3.9983(4.0443) | Xent 1.3437(1.4381) | Loss 10.1942(10.8135) | Error 0.4870(0.5185) Steps 556(548.35) | Grad Norm 6.7113(11.6353) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 53.6419(50.1540) | Bit/dim 4.0227(4.0436) | Xent 1.3256(1.4347) | Loss 10.2497(10.7966) | Error 0.4761(0.5172) Steps 586(549.48) | Grad Norm 9.4603(11.5701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 22.5062, Epoch Time 346.1312(332.5435), Bit/dim 3.9923(best: 4.0057), Xent 1.3015, Loss 4.6431, Error 0.4744(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 47.0932(50.0622) | Bit/dim 3.9948(4.0421) | Xent 1.3562(1.4324) | Loss 13.0173(10.8632) | Error 0.4928(0.5165) Steps 544(549.32) | Grad Norm 7.9445(11.4613) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 49.3462(50.0407) | Bit/dim 3.9939(4.0407) | Xent 1.3578(1.4301) | Loss 9.9286(10.8352) | Error 0.4878(0.5156) Steps 544(549.16) | Grad Norm 11.6794(11.4679) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 49.6383(50.0286) | Bit/dim 4.0136(4.0399) | Xent 1.3501(1.4277) | Loss 10.2836(10.8187) | Error 0.4836(0.5146) Steps 574(549.90) | Grad Norm 12.6903(11.5045) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 53.5708(50.1349) | Bit/dim 3.9756(4.0380) | Xent 1.4113(1.4272) | Loss 10.2575(10.8018) | Error 0.4950(0.5140) Steps 556(550.09) | Grad Norm 13.2197(11.5560) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 50.1561(50.1355) | Bit/dim 3.9995(4.0368) | Xent 1.3908(1.4261) | Loss 10.3705(10.7889) | Error 0.4946(0.5135) Steps 568(550.62) | Grad Norm 15.7697(11.6824) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 47.8357(50.0665) | Bit/dim 3.9785(4.0351) | Xent 1.4043(1.4255) | Loss 9.8229(10.7599) | Error 0.5131(0.5135) Steps 526(549.89) | Grad Norm 9.6831(11.6224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 22.1909, Epoch Time 335.9355(332.6452), Bit/dim 3.9764(best: 3.9923), Xent 1.3153, Loss 4.6341, Error 0.4771(best: 0.4744)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 53.2271(50.1613) | Bit/dim 3.9672(4.0330) | Xent 1.3810(1.4241) | Loss 13.4005(10.8391) | Error 0.4914(0.5128) Steps 562(550.25) | Grad Norm 9.1358(11.5478) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 51.7231(50.2082) | Bit/dim 3.9767(4.0313) | Xent 1.4841(1.4259) | Loss 10.1917(10.8197) | Error 0.5386(0.5136) Steps 562(550.60) | Grad Norm 21.6776(11.8517) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 47.2314(50.1189) | Bit/dim 3.9928(4.0302) | Xent 1.3754(1.4244) | Loss 10.3351(10.8052) | Error 0.4996(0.5131) Steps 526(549.86) | Grad Norm 6.9217(11.7038) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 49.0935(50.0881) | Bit/dim 4.0037(4.0294) | Xent 1.3605(1.4225) | Loss 10.1736(10.7862) | Error 0.4932(0.5126) Steps 544(549.69) | Grad Norm 9.8288(11.6476) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 49.5443(50.0718) | Bit/dim 3.9992(4.0285) | Xent 1.3499(1.4203) | Loss 10.1125(10.7660) | Error 0.4844(0.5117) Steps 544(549.52) | Grad Norm 9.6136(11.5865) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 51.6813(50.1201) | Bit/dim 4.0113(4.0280) | Xent 1.3328(1.4177) | Loss 10.0919(10.7458) | Error 0.4795(0.5107) Steps 550(549.53) | Grad Norm 7.7171(11.4705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 22.1575, Epoch Time 340.7882(332.8895), Bit/dim 4.0034(best: 3.9764), Xent 1.2790, Loss 4.6429, Error 0.4620(best: 0.4744)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 50.1630(50.1214) | Bit/dim 3.9999(4.0271) | Xent 1.3426(1.4155) | Loss 13.3792(10.8248) | Error 0.4869(0.5100) Steps 550(549.55) | Grad Norm 7.5379(11.3525) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 47.1372(50.0319) | Bit/dim 3.9948(4.0262) | Xent 1.3171(1.4125) | Loss 10.0921(10.8028) | Error 0.4700(0.5088) Steps 538(549.20) | Grad Norm 8.5661(11.2689) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 54.2804(50.1593) | Bit/dim 3.9952(4.0252) | Xent 1.3562(1.4108) | Loss 10.1071(10.7819) | Error 0.4924(0.5083) Steps 556(549.40) | Grad Norm 13.3994(11.3328) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 50.0646(50.1565) | Bit/dim 3.9632(4.0234) | Xent 1.3078(1.4077) | Loss 10.2290(10.7653) | Error 0.4629(0.5070) Steps 556(549.60) | Grad Norm 7.6381(11.2220) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 52.5846(50.2293) | Bit/dim 3.9736(4.0219) | Xent 1.3158(1.4050) | Loss 9.9623(10.7412) | Error 0.4815(0.5062) Steps 556(549.79) | Grad Norm 5.4850(11.0499) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 51.6847(50.2730) | Bit/dim 3.9609(4.0200) | Xent 1.3080(1.4021) | Loss 10.1734(10.7242) | Error 0.4729(0.5052) Steps 550(549.80) | Grad Norm 8.8280(10.9832) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 22.2142, Epoch Time 344.1919(333.2286), Bit/dim 3.9757(best: 3.9764), Xent 1.3413, Loss 4.6463, Error 0.4733(best: 0.4620)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 49.2166(50.2413) | Bit/dim 3.9770(4.0188) | Xent 1.3998(1.4020) | Loss 13.3679(10.8035) | Error 0.4952(0.5049) Steps 544(549.63) | Grad Norm 17.0911(11.1664) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 46.6093(50.1323) | Bit/dim 3.9796(4.0176) | Xent 1.3547(1.4006) | Loss 10.1379(10.7836) | Error 0.4960(0.5046) Steps 532(549.10) | Grad Norm 11.0763(11.1637) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 50.0377(50.1295) | Bit/dim 3.9557(4.0157) | Xent 1.2633(1.3965) | Loss 10.0910(10.7628) | Error 0.4553(0.5032) Steps 526(548.40) | Grad Norm 5.5788(10.9962) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 49.9857(50.1252) | Bit/dim 3.9696(4.0143) | Xent 1.3157(1.3940) | Loss 9.9761(10.7392) | Error 0.4705(0.5022) Steps 532(547.91) | Grad Norm 10.0085(10.9666) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 45.1677(49.9765) | Bit/dim 3.9529(4.0125) | Xent 1.3174(1.3917) | Loss 9.9772(10.7163) | Error 0.4746(0.5013) Steps 532(547.43) | Grad Norm 6.5756(10.8348) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 50.9101(50.0045) | Bit/dim 3.9441(4.0104) | Xent 1.2888(1.3886) | Loss 10.0221(10.6955) | Error 0.4718(0.5005) Steps 556(547.69) | Grad Norm 4.9500(10.6583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 23.1118, Epoch Time 330.6779(333.1521), Bit/dim 3.9635(best: 3.9757), Xent 1.2867, Loss 4.6068, Error 0.4644(best: 0.4620)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 50.2456(50.0117) | Bit/dim 3.9561(4.0088) | Xent 1.3149(1.3864) | Loss 12.7688(10.7577) | Error 0.4752(0.4997) Steps 562(548.12) | Grad Norm 12.7148(10.7200) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 46.6226(49.9100) | Bit/dim 3.9809(4.0080) | Xent 1.3896(1.3865) | Loss 10.1417(10.7392) | Error 0.4881(0.4994) Steps 544(548.00) | Grad Norm 12.7298(10.7803) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 45.8393(49.7879) | Bit/dim 3.9785(4.0071) | Xent 1.3896(1.3866) | Loss 10.3439(10.7274) | Error 0.4871(0.4990) Steps 538(547.70) | Grad Norm 11.5910(10.8046) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 50.1396(49.7985) | Bit/dim 3.9503(4.0054) | Xent 1.3331(1.3850) | Loss 9.9393(10.7037) | Error 0.4809(0.4984) Steps 526(547.05) | Grad Norm 16.1804(10.9659) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 52.0083(49.8648) | Bit/dim 3.9520(4.0038) | Xent 1.5204(1.3891) | Loss 10.0008(10.6826) | Error 0.5339(0.4995) Steps 532(546.59) | Grad Norm 24.5314(11.3728) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 53.7218(49.9805) | Bit/dim 3.9905(4.0034) | Xent 1.3424(1.3877) | Loss 10.3483(10.6726) | Error 0.4841(0.4990) Steps 538(546.34) | Grad Norm 11.2245(11.3684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 22.7299, Epoch Time 337.5386(333.2837), Bit/dim 3.9931(best: 3.9635), Xent 1.2850, Loss 4.6356, Error 0.4580(best: 0.4620)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 51.5857(50.0286) | Bit/dim 3.9952(4.0031) | Xent 1.3236(1.3857) | Loss 13.6195(10.7610) | Error 0.4675(0.4981) Steps 532(545.91) | Grad Norm 9.7743(11.3206) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 48.3822(49.9792) | Bit/dim 3.9628(4.0019) | Xent 1.4338(1.3872) | Loss 9.9935(10.7380) | Error 0.5166(0.4987) Steps 550(546.03) | Grad Norm 13.4822(11.3854) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 49.9474(49.9783) | Bit/dim 3.9512(4.0004) | Xent 1.3118(1.3849) | Loss 10.0454(10.7172) | Error 0.4739(0.4979) Steps 580(547.05) | Grad Norm 7.2620(11.2617) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 48.5712(49.9361) | Bit/dim 3.9676(3.9994) | Xent 1.3625(1.3843) | Loss 10.1925(10.7015) | Error 0.4971(0.4979) Steps 556(547.32) | Grad Norm 9.6853(11.2144) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 48.4768(49.8923) | Bit/dim 3.9532(3.9980) | Xent 1.2882(1.3814) | Loss 10.1501(10.6849) | Error 0.4677(0.4970) Steps 544(547.22) | Grad Norm 8.0609(11.1198) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 52.9049(49.9827) | Bit/dim 3.9647(3.9970) | Xent 1.3949(1.3818) | Loss 10.2395(10.6716) | Error 0.5062(0.4973) Steps 544(547.12) | Grad Norm 14.1877(11.2118) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 22.7282, Epoch Time 338.6195(333.4437), Bit/dim 3.9568(best: 3.9635), Xent 1.2783, Loss 4.5959, Error 0.4539(best: 0.4580)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 54.9986(50.1331) | Bit/dim 3.9449(3.9955) | Xent 1.3367(1.3804) | Loss 13.3933(10.7532) | Error 0.4796(0.4967) Steps 622(549.37) | Grad Norm 14.9076(11.3227) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 53.0868(50.2218) | Bit/dim 3.9600(3.9944) | Xent 1.4002(1.3810) | Loss 10.1114(10.7340) | Error 0.5064(0.4970) Steps 592(550.65) | Grad Norm 12.8419(11.3683) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 53.2937(50.3139) | Bit/dim 3.9713(3.9937) | Xent 1.3328(1.3796) | Loss 10.2028(10.7180) | Error 0.4752(0.4964) Steps 556(550.81) | Grad Norm 5.7394(11.1994) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 53.8390(50.4197) | Bit/dim 3.9451(3.9923) | Xent 1.3055(1.3773) | Loss 10.0304(10.6974) | Error 0.4686(0.4955) Steps 604(552.40) | Grad Norm 7.0648(11.0754) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 51.0317(50.4380) | Bit/dim 3.9552(3.9911) | Xent 1.3251(1.3758) | Loss 10.2166(10.6830) | Error 0.4770(0.4950) Steps 592(553.59) | Grad Norm 6.3105(10.9324) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 54.2701(50.5530) | Bit/dim 3.9384(3.9896) | Xent 1.2788(1.3729) | Loss 10.2019(10.6685) | Error 0.4576(0.4939) Steps 544(553.30) | Grad Norm 4.9530(10.7531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 21.9953, Epoch Time 358.4153(334.1929), Bit/dim 3.9357(best: 3.9568), Xent 1.2460, Loss 4.5587, Error 0.4505(best: 0.4539)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 49.9833(50.5359) | Bit/dim 3.9372(3.9880) | Xent 1.3330(1.3717) | Loss 13.3019(10.7475) | Error 0.4780(0.4934) Steps 556(553.38) | Grad Norm 8.5864(10.6881) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_001_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0 --trust_coefficient 0.001 --clip True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
