{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run2/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 22.8076(24.0757) | Bit/dim 3.5450(3.5443) | Xent 0.0210(0.0430) | Loss 3.5555(3.5658) | Error 0.0056(0.0130) Steps 976(998.41) | Grad Norm 1.7083(2.1810) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 22.9678(23.6859) | Bit/dim 3.5376(3.5390) | Xent 0.0128(0.0371) | Loss 3.5440(3.5575) | Error 0.0056(0.0112) Steps 988(994.91) | Grad Norm 1.0301(1.9189) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 23.3099(23.4911) | Bit/dim 3.5322(3.5352) | Xent 0.0105(0.0315) | Loss 3.5374(3.5510) | Error 0.0033(0.0095) Steps 988(995.15) | Grad Norm 0.6063(1.6657) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 22.4951(23.2942) | Bit/dim 3.5350(3.5349) | Xent 0.0170(0.0276) | Loss 3.5435(3.5487) | Error 0.0044(0.0082) Steps 982(994.36) | Grad Norm 0.8006(1.4409) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 22.4204(23.1705) | Bit/dim 3.4820(3.5294) | Xent 0.0118(0.0245) | Loss 3.4879(3.5416) | Error 0.0022(0.0069) Steps 994(993.16) | Grad Norm 0.6867(1.2519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 115.3219, Epoch Time 1407.4562(1438.5025), Bit/dim 3.5446(best: inf), Xent 2.0331, Loss 4.5612, Error 0.3530(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 23.1707(23.1367) | Bit/dim 3.5243(3.5291) | Xent 0.0046(0.0206) | Loss 3.5266(3.5394) | Error 0.0000(0.0057) Steps 994(992.56) | Grad Norm 0.2588(1.0589) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 22.5272(23.2067) | Bit/dim 3.5137(3.5252) | Xent 0.0173(0.0182) | Loss 3.5224(3.5343) | Error 0.0033(0.0049) Steps 982(995.13) | Grad Norm 0.7133(0.9319) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 23.6903(23.2740) | Bit/dim 3.4732(3.5245) | Xent 0.0159(0.0162) | Loss 3.4812(3.5326) | Error 0.0022(0.0042) Steps 1018(996.42) | Grad Norm 0.3725(0.8205) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 23.7458(23.3088) | Bit/dim 3.5371(3.5217) | Xent 0.0178(0.0150) | Loss 3.5460(3.5292) | Error 0.0033(0.0039) Steps 988(995.85) | Grad Norm 0.8303(0.7735) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 22.7581(23.2880) | Bit/dim 3.5250(3.5220) | Xent 0.0083(0.0143) | Loss 3.5292(3.5291) | Error 0.0022(0.0036) Steps 994(993.95) | Grad Norm 0.5517(0.7367) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 23.4483(23.2377) | Bit/dim 3.5352(3.5210) | Xent 0.0080(0.0130) | Loss 3.5392(3.5275) | Error 0.0022(0.0034) Steps 982(993.95) | Grad Norm 0.4708(0.6934) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 111.0762, Epoch Time 1410.3213(1437.6571), Bit/dim 3.5410(best: 3.5446), Xent 2.1132, Loss 4.5976, Error 0.3540(best: 0.3530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 23.1789(23.2241) | Bit/dim 3.5381(3.5181) | Xent 0.0110(0.0120) | Loss 3.5436(3.5241) | Error 0.0056(0.0031) Steps 988(994.56) | Grad Norm 0.5161(0.6568) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 23.9174(23.2372) | Bit/dim 3.5506(3.5184) | Xent 0.0146(0.0114) | Loss 3.5579(3.5241) | Error 0.0022(0.0028) Steps 976(992.80) | Grad Norm 0.5934(0.6042) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 22.3723(23.2360) | Bit/dim 3.5072(3.5179) | Xent 0.0074(0.0112) | Loss 3.5110(3.5235) | Error 0.0022(0.0028) Steps 982(992.45) | Grad Norm 0.7791(0.6205) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 23.2170(23.2340) | Bit/dim 3.5352(3.5177) | Xent 0.0134(0.0108) | Loss 3.5419(3.5231) | Error 0.0033(0.0027) Steps 1000(992.37) | Grad Norm 0.7979(0.6190) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 23.2534(23.2240) | Bit/dim 3.5486(3.5196) | Xent 0.0045(0.0109) | Loss 3.5508(3.5250) | Error 0.0000(0.0027) Steps 970(991.78) | Grad Norm 0.4240(0.6436) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 111.3431, Epoch Time 1402.8486(1436.6128), Bit/dim 3.5400(best: 3.5410), Xent 2.1322, Loss 4.6061, Error 0.3530(best: 0.3530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 23.3178(23.1591) | Bit/dim 3.5126(3.5178) | Xent 0.0054(0.0109) | Loss 3.5152(3.5232) | Error 0.0022(0.0028) Steps 988(992.71) | Grad Norm 0.4542(0.6368) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 23.2439(23.1786) | Bit/dim 3.5235(3.5197) | Xent 0.0084(0.0106) | Loss 3.5277(3.5250) | Error 0.0022(0.0028) Steps 988(992.00) | Grad Norm 0.4661(0.6598) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 23.7205(23.2763) | Bit/dim 3.5113(3.5195) | Xent 0.0062(0.0097) | Loss 3.5144(3.5243) | Error 0.0011(0.0025) Steps 1018(995.89) | Grad Norm 0.4321(0.6224) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 22.9126(23.2766) | Bit/dim 3.5005(3.5171) | Xent 0.0102(0.0093) | Loss 3.5056(3.5217) | Error 0.0022(0.0023) Steps 982(996.71) | Grad Norm 0.8310(0.6156) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 22.5391(23.2684) | Bit/dim 3.4933(3.5180) | Xent 0.0113(0.0090) | Loss 3.4990(3.5225) | Error 0.0044(0.0022) Steps 982(995.18) | Grad Norm 0.7692(0.5987) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 23.0706(23.3016) | Bit/dim 3.4852(3.5143) | Xent 0.0074(0.0086) | Loss 3.4889(3.5186) | Error 0.0033(0.0020) Steps 988(994.44) | Grad Norm 0.5970(0.6066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 110.7768, Epoch Time 1409.6184(1435.8030), Bit/dim 3.5407(best: 3.5400), Xent 2.1743, Loss 4.6278, Error 0.3525(best: 0.3530)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 23.1398(23.3789) | Bit/dim 3.5164(3.5160) | Xent 0.0037(0.0079) | Loss 3.5182(3.5199) | Error 0.0011(0.0018) Steps 1000(993.31) | Grad Norm 0.2989(0.5620) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 23.7379(23.3708) | Bit/dim 3.5193(3.5154) | Xent 0.0081(0.0081) | Loss 3.5233(3.5195) | Error 0.0022(0.0020) Steps 1000(992.01) | Grad Norm 0.6161(0.5763) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 23.1080(23.3756) | Bit/dim 3.5210(3.5147) | Xent 0.0142(0.0081) | Loss 3.5281(3.5188) | Error 0.0056(0.0021) Steps 1018(995.54) | Grad Norm 0.9390(0.5604) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 24.3956(23.3912) | Bit/dim 3.5261(3.5150) | Xent 0.0046(0.0075) | Loss 3.5284(3.5187) | Error 0.0022(0.0020) Steps 1012(995.75) | Grad Norm 0.5864(0.5352) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 23.4023(23.4734) | Bit/dim 3.5045(3.5152) | Xent 0.0106(0.0078) | Loss 3.5098(3.5191) | Error 0.0033(0.0019) Steps 988(995.07) | Grad Norm 0.8845(0.5648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 110.2257, Epoch Time 1418.3564(1435.2796), Bit/dim 3.5396(best: 3.5400), Xent 2.2169, Loss 4.6480, Error 0.3563(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 23.6718(23.4513) | Bit/dim 3.5350(3.5129) | Xent 0.0060(0.0074) | Loss 3.5380(3.5166) | Error 0.0022(0.0020) Steps 970(993.93) | Grad Norm 0.4601(0.5552) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 23.1225(23.4833) | Bit/dim 3.4935(3.5103) | Xent 0.0054(0.0073) | Loss 3.4962(3.5139) | Error 0.0011(0.0019) Steps 1000(994.49) | Grad Norm 0.6713(0.5671) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 23.2115(23.4410) | Bit/dim 3.5426(3.5123) | Xent 0.0027(0.0070) | Loss 3.5439(3.5158) | Error 0.0000(0.0017) Steps 976(996.44) | Grad Norm 0.2571(0.5380) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 22.8605(23.3631) | Bit/dim 3.5002(3.5105) | Xent 0.0058(0.0073) | Loss 3.5032(3.5142) | Error 0.0022(0.0017) Steps 982(996.52) | Grad Norm 0.5500(0.5437) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 23.3117(23.3614) | Bit/dim 3.5184(3.5130) | Xent 0.0115(0.0077) | Loss 3.5242(3.5168) | Error 0.0033(0.0019) Steps 1006(996.53) | Grad Norm 0.7379(0.5488) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 23.2100(23.3516) | Bit/dim 3.5053(3.5159) | Xent 0.0032(0.0073) | Loss 3.5069(3.5196) | Error 0.0011(0.0017) Steps 994(995.83) | Grad Norm 0.2866(0.5316) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 109.7081, Epoch Time 1409.6796(1434.5116), Bit/dim 3.5398(best: 3.5396), Xent 2.2410, Loss 4.6603, Error 0.3565(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 23.4849(23.3012) | Bit/dim 3.4919(3.5143) | Xent 0.0045(0.0076) | Loss 3.4941(3.5181) | Error 0.0011(0.0019) Steps 976(995.52) | Grad Norm 0.6241(0.5624) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 24.1266(23.3543) | Bit/dim 3.5291(3.5144) | Xent 0.0052(0.0073) | Loss 3.5317(3.5181) | Error 0.0011(0.0019) Steps 1030(997.46) | Grad Norm 0.5674(0.5469) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 23.2631(23.4037) | Bit/dim 3.5003(3.5139) | Xent 0.0035(0.0069) | Loss 3.5020(3.5173) | Error 0.0011(0.0017) Steps 988(997.05) | Grad Norm 0.4442(0.5593) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 24.0251(23.4076) | Bit/dim 3.4962(3.5139) | Xent 0.0053(0.0067) | Loss 3.4988(3.5173) | Error 0.0011(0.0016) Steps 1012(998.09) | Grad Norm 0.5830(0.5561) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 23.3247(23.3785) | Bit/dim 3.5128(3.5136) | Xent 0.0039(0.0064) | Loss 3.5147(3.5168) | Error 0.0011(0.0016) Steps 1012(997.77) | Grad Norm 0.4691(0.5571) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 112.2345, Epoch Time 1413.1157(1433.8697), Bit/dim 3.5393(best: 3.5396), Xent 2.2737, Loss 4.6762, Error 0.3576(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 23.5415(23.3598) | Bit/dim 3.5012(3.5094) | Xent 0.0024(0.0068) | Loss 3.5024(3.5127) | Error 0.0000(0.0016) Steps 1000(998.12) | Grad Norm 0.3281(0.5992) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 23.8110(23.4030) | Bit/dim 3.5110(3.5115) | Xent 0.0045(0.0063) | Loss 3.5132(3.5147) | Error 0.0022(0.0015) Steps 1024(999.47) | Grad Norm 0.7310(0.5640) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 24.2192(23.5464) | Bit/dim 3.4932(3.5110) | Xent 0.0042(0.0058) | Loss 3.4953(3.5139) | Error 0.0011(0.0013) Steps 1018(1001.37) | Grad Norm 0.5309(0.5462) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 23.5696(23.5501) | Bit/dim 3.5376(3.5133) | Xent 0.0055(0.0059) | Loss 3.5404(3.5163) | Error 0.0022(0.0013) Steps 994(999.31) | Grad Norm 0.5450(0.5481) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 22.6686(23.4606) | Bit/dim 3.5168(3.5127) | Xent 0.0057(0.0061) | Loss 3.5197(3.5157) | Error 0.0022(0.0014) Steps 982(999.20) | Grad Norm 0.7147(0.5576) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 24.0590(23.4828) | Bit/dim 3.5446(3.5118) | Xent 0.0022(0.0059) | Loss 3.5457(3.5148) | Error 0.0000(0.0014) Steps 1012(1000.34) | Grad Norm 0.2931(0.5556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 112.0919, Epoch Time 1422.6636(1433.5335), Bit/dim 3.5396(best: 3.5393), Xent 2.3110, Loss 4.6951, Error 0.3573(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 24.1208(23.5004) | Bit/dim 3.5147(3.5120) | Xent 0.0075(0.0057) | Loss 3.5184(3.5148) | Error 0.0033(0.0013) Steps 1018(1001.38) | Grad Norm 0.5600(0.5343) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 23.3316(23.4988) | Bit/dim 3.5067(3.5117) | Xent 0.0074(0.0056) | Loss 3.5104(3.5145) | Error 0.0011(0.0013) Steps 1006(1002.48) | Grad Norm 0.6878(0.5084) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 23.3792(23.4926) | Bit/dim 3.4903(3.5097) | Xent 0.0095(0.0062) | Loss 3.4950(3.5128) | Error 0.0011(0.0015) Steps 1006(1002.72) | Grad Norm 0.6003(0.5533) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 23.1207(23.5309) | Bit/dim 3.4981(3.5086) | Xent 0.0043(0.0065) | Loss 3.5002(3.5119) | Error 0.0011(0.0016) Steps 1000(1003.36) | Grad Norm 0.3866(0.5919) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 23.6121(23.4729) | Bit/dim 3.5217(3.5111) | Xent 0.0019(0.0064) | Loss 3.5226(3.5143) | Error 0.0000(0.0015) Steps 1006(1003.86) | Grad Norm 0.3407(0.6196) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 110.9040, Epoch Time 1418.2993(1433.0765), Bit/dim 3.5389(best: 3.5393), Xent 2.3303, Loss 4.7041, Error 0.3564(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 23.6338(23.4266) | Bit/dim 3.5061(3.5111) | Xent 0.0022(0.0059) | Loss 3.5072(3.5141) | Error 0.0000(0.0013) Steps 1006(1003.65) | Grad Norm 0.2426(0.5756) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 23.6148(23.4590) | Bit/dim 3.5353(3.5114) | Xent 0.0055(0.0058) | Loss 3.5380(3.5143) | Error 0.0011(0.0013) Steps 994(1005.30) | Grad Norm 1.1433(0.6093) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 23.0536(23.4340) | Bit/dim 3.5123(3.5110) | Xent 0.0022(0.0056) | Loss 3.5134(3.5138) | Error 0.0000(0.0012) Steps 1000(1003.33) | Grad Norm 0.2201(0.5817) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 23.0844(23.4045) | Bit/dim 3.4895(3.5106) | Xent 0.0032(0.0056) | Loss 3.4911(3.5134) | Error 0.0000(0.0013) Steps 1006(1001.54) | Grad Norm 0.3557(0.5902) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 23.3756(23.3991) | Bit/dim 3.5005(3.5109) | Xent 0.0058(0.0054) | Loss 3.5034(3.5136) | Error 0.0022(0.0013) Steps 1006(1001.73) | Grad Norm 0.7394(0.6156) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 23.9129(23.4811) | Bit/dim 3.4973(3.5107) | Xent 0.0037(0.0053) | Loss 3.4991(3.5134) | Error 0.0011(0.0012) Steps 994(1002.93) | Grad Norm 0.4035(0.6152) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 110.8942, Epoch Time 1416.9961(1432.5941), Bit/dim 3.5386(best: 3.5389), Xent 2.3548, Loss 4.7159, Error 0.3566(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 23.0580(23.4634) | Bit/dim 3.5085(3.5113) | Xent 0.0032(0.0051) | Loss 3.5101(3.5138) | Error 0.0011(0.0012) Steps 1006(1002.14) | Grad Norm 0.3031(0.5809) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 23.6704(23.5232) | Bit/dim 3.4926(3.5090) | Xent 0.0042(0.0048) | Loss 3.4947(3.5114) | Error 0.0011(0.0011) Steps 982(1000.62) | Grad Norm 0.4576(0.5463) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 23.6215(23.5095) | Bit/dim 3.5075(3.5074) | Xent 0.0054(0.0048) | Loss 3.5102(3.5099) | Error 0.0022(0.0011) Steps 1012(999.27) | Grad Norm 0.6889(0.5133) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 23.1478(23.5073) | Bit/dim 3.5309(3.5082) | Xent 0.0050(0.0049) | Loss 3.5334(3.5107) | Error 0.0022(0.0013) Steps 1000(1000.25) | Grad Norm 0.5393(0.5393) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 24.0805(23.5162) | Bit/dim 3.5086(3.5102) | Xent 0.0041(0.0048) | Loss 3.5106(3.5126) | Error 0.0011(0.0013) Steps 994(1001.21) | Grad Norm 0.4068(0.5227) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 110.9124, Epoch Time 1419.4109(1432.1986), Bit/dim 3.5382(best: 3.5386), Xent 2.3886, Loss 4.7325, Error 0.3594(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 22.6994(23.4460) | Bit/dim 3.4888(3.5106) | Xent 0.0132(0.0054) | Loss 3.4954(3.5133) | Error 0.0044(0.0015) Steps 988(1001.18) | Grad Norm 1.0862(0.5347) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 23.7747(23.4522) | Bit/dim 3.5237(3.5086) | Xent 0.0024(0.0050) | Loss 3.5249(3.5111) | Error 0.0000(0.0013) Steps 1018(1001.46) | Grad Norm 0.3153(0.5149) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 23.5791(23.4252) | Bit/dim 3.5298(3.5084) | Xent 0.0038(0.0050) | Loss 3.5317(3.5109) | Error 0.0022(0.0014) Steps 988(1001.16) | Grad Norm 0.7255(0.5247) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 22.7875(23.4427) | Bit/dim 3.5003(3.5096) | Xent 0.0026(0.0046) | Loss 3.5016(3.5119) | Error 0.0000(0.0011) Steps 988(1000.48) | Grad Norm 0.4800(0.5047) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 23.4148(23.4810) | Bit/dim 3.5193(3.5114) | Xent 0.0044(0.0045) | Loss 3.5215(3.5137) | Error 0.0011(0.0012) Steps 1000(1001.17) | Grad Norm 1.0439(0.5628) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 24.0030(23.4625) | Bit/dim 3.5360(3.5107) | Xent 0.0019(0.0044) | Loss 3.5370(3.5129) | Error 0.0000(0.0012) Steps 1006(1002.02) | Grad Norm 0.2915(0.5638) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 111.4753, Epoch Time 1416.5302(1431.7286), Bit/dim 3.5390(best: 3.5382), Xent 2.4057, Loss 4.7418, Error 0.3585(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 23.0846(23.4185) | Bit/dim 3.4928(3.5099) | Xent 0.0024(0.0046) | Loss 3.4940(3.5122) | Error 0.0011(0.0014) Steps 1006(1001.13) | Grad Norm 0.4627(0.5986) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 24.2998(23.4547) | Bit/dim 3.5141(3.5097) | Xent 0.0037(0.0046) | Loss 3.5160(3.5120) | Error 0.0011(0.0013) Steps 1012(1001.78) | Grad Norm 0.4335(0.6095) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 22.7470(23.4209) | Bit/dim 3.4996(3.5083) | Xent 0.0021(0.0058) | Loss 3.5006(3.5113) | Error 0.0000(0.0016) Steps 1000(1002.39) | Grad Norm 0.3226(0.6807) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 23.0133(23.4360) | Bit/dim 3.4820(3.5086) | Xent 0.0031(0.0054) | Loss 3.4836(3.5113) | Error 0.0000(0.0014) Steps 1006(1003.07) | Grad Norm 0.3590(0.6335) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 23.9857(23.4936) | Bit/dim 3.4779(3.5100) | Xent 0.0032(0.0054) | Loss 3.4795(3.5127) | Error 0.0011(0.0014) Steps 982(1002.72) | Grad Norm 0.3316(0.6246) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 112.3104, Epoch Time 1418.6677(1431.3367), Bit/dim 3.5395(best: 3.5382), Xent 2.4296, Loss 4.7543, Error 0.3590(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 23.4516(23.4870) | Bit/dim 3.5033(3.5089) | Xent 0.0046(0.0055) | Loss 3.5056(3.5117) | Error 0.0011(0.0015) Steps 994(1002.82) | Grad Norm 0.6775(0.6280) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 23.2250(23.4917) | Bit/dim 3.4921(3.5091) | Xent 0.0060(0.0056) | Loss 3.4951(3.5119) | Error 0.0011(0.0013) Steps 1006(1000.80) | Grad Norm 0.5533(0.5897) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 23.5859(23.5067) | Bit/dim 3.4907(3.5089) | Xent 0.0022(0.0051) | Loss 3.4918(3.5114) | Error 0.0000(0.0013) Steps 994(1000.44) | Grad Norm 0.2572(0.5478) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 22.9142(23.4739) | Bit/dim 3.5229(3.5072) | Xent 0.0014(0.0047) | Loss 3.5236(3.5095) | Error 0.0000(0.0012) Steps 1018(1002.96) | Grad Norm 0.3035(0.5483) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 23.6932(23.4770) | Bit/dim 3.5213(3.5078) | Xent 0.0055(0.0046) | Loss 3.5240(3.5101) | Error 0.0022(0.0013) Steps 1006(1003.39) | Grad Norm 0.5109(0.5703) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 22.5734(23.4491) | Bit/dim 3.5043(3.5080) | Xent 0.0057(0.0047) | Loss 3.5072(3.5103) | Error 0.0022(0.0012) Steps 1006(1004.78) | Grad Norm 1.0210(0.5882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 111.5692, Epoch Time 1418.2691(1430.9447), Bit/dim 3.5385(best: 3.5382), Xent 2.4348, Loss 4.7559, Error 0.3604(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 24.2120(23.5064) | Bit/dim 3.4980(3.5067) | Xent 0.0030(0.0047) | Loss 3.4995(3.5090) | Error 0.0011(0.0012) Steps 1006(1005.00) | Grad Norm 0.3143(0.5600) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 23.3836(23.5021) | Bit/dim 3.5133(3.5066) | Xent 0.0048(0.0045) | Loss 3.5157(3.5089) | Error 0.0011(0.0011) Steps 988(1003.57) | Grad Norm 0.7568(0.5412) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 24.2302(23.5896) | Bit/dim 3.5256(3.5074) | Xent 0.0037(0.0042) | Loss 3.5275(3.5095) | Error 0.0022(0.0010) Steps 1000(1002.85) | Grad Norm 0.3831(0.5235) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 23.0034(23.5299) | Bit/dim 3.5447(3.5057) | Xent 0.0022(0.0039) | Loss 3.5458(3.5077) | Error 0.0000(0.0009) Steps 988(1001.51) | Grad Norm 0.4170(0.5406) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 24.1161(23.5437) | Bit/dim 3.4985(3.5069) | Xent 0.0062(0.0041) | Loss 3.5016(3.5090) | Error 0.0022(0.0009) Steps 1024(999.42) | Grad Norm 0.5921(0.5284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 111.2569, Epoch Time 1424.5090(1430.7516), Bit/dim 3.5385(best: 3.5382), Xent 2.4612, Loss 4.7690, Error 0.3596(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 23.2090(23.5415) | Bit/dim 3.5160(3.5079) | Xent 0.0078(0.0041) | Loss 3.5199(3.5099) | Error 0.0011(0.0009) Steps 1006(999.44) | Grad Norm 0.9855(0.5392) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 23.1156(23.4797) | Bit/dim 3.4967(3.5084) | Xent 0.0016(0.0038) | Loss 3.4975(3.5103) | Error 0.0000(0.0009) Steps 1000(1001.05) | Grad Norm 0.3373(0.5120) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 23.6477(23.4935) | Bit/dim 3.5087(3.5087) | Xent 0.0135(0.0039) | Loss 3.5154(3.5107) | Error 0.0022(0.0008) Steps 1012(999.80) | Grad Norm 1.1091(0.5234) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 23.7595(23.5118) | Bit/dim 3.5124(3.5075) | Xent 0.0062(0.0039) | Loss 3.5155(3.5094) | Error 0.0022(0.0008) Steps 1006(1000.68) | Grad Norm 1.3560(0.5285) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 23.1181(23.4714) | Bit/dim 3.4712(3.5059) | Xent 0.0017(0.0039) | Loss 3.4720(3.5079) | Error 0.0000(0.0008) Steps 982(999.77) | Grad Norm 0.2620(0.5340) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 23.0245(23.4239) | Bit/dim 3.5059(3.5068) | Xent 0.0092(0.0048) | Loss 3.5105(3.5092) | Error 0.0011(0.0011) Steps 1012(999.18) | Grad Norm 0.7393(0.5856) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 111.2703, Epoch Time 1415.2103(1430.2854), Bit/dim 3.5399(best: 3.5382), Xent 2.4820, Loss 4.7809, Error 0.3582(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 23.5648(23.4316) | Bit/dim 3.5016(3.5080) | Xent 0.0094(0.0046) | Loss 3.5063(3.5103) | Error 0.0033(0.0012) Steps 994(996.86) | Grad Norm 1.7148(0.6407) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 23.6415(23.4042) | Bit/dim 3.5152(3.5061) | Xent 0.0027(0.0044) | Loss 3.5165(3.5083) | Error 0.0011(0.0011) Steps 1006(996.82) | Grad Norm 0.4175(0.6060) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 23.5580(23.4532) | Bit/dim 3.5061(3.5076) | Xent 0.0025(0.0046) | Loss 3.5073(3.5100) | Error 0.0011(0.0011) Steps 1000(997.71) | Grad Norm 0.6077(0.6065) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 23.5472(23.4328) | Bit/dim 3.5436(3.5058) | Xent 0.0037(0.0047) | Loss 3.5454(3.5081) | Error 0.0011(0.0011) Steps 982(995.97) | Grad Norm 0.7467(0.5816) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 23.1922(23.4479) | Bit/dim 3.5180(3.5077) | Xent 0.0069(0.0044) | Loss 3.5214(3.5099) | Error 0.0022(0.0010) Steps 1012(996.81) | Grad Norm 0.9068(0.5653) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 111.5674, Epoch Time 1416.4841(1429.8714), Bit/dim 3.5380(best: 3.5382), Xent 2.4881, Loss 4.7821, Error 0.3607(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 23.1094(23.4166) | Bit/dim 3.4991(3.5067) | Xent 0.0085(0.0047) | Loss 3.5033(3.5090) | Error 0.0033(0.0012) Steps 1000(997.79) | Grad Norm 1.0915(0.6083) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 22.5103(23.3993) | Bit/dim 3.4960(3.5053) | Xent 0.0024(0.0043) | Loss 3.4972(3.5074) | Error 0.0000(0.0010) Steps 982(997.78) | Grad Norm 0.3437(0.5527) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 23.4502(23.3976) | Bit/dim 3.5433(3.5067) | Xent 0.0098(0.0045) | Loss 3.5482(3.5090) | Error 0.0033(0.0011) Steps 988(996.46) | Grad Norm 0.9671(0.6095) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 23.3280(23.4471) | Bit/dim 3.4815(3.5080) | Xent 0.0024(0.0051) | Loss 3.4826(3.5105) | Error 0.0000(0.0012) Steps 988(995.42) | Grad Norm 0.3483(0.6085) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 23.2504(23.4720) | Bit/dim 3.5144(3.5081) | Xent 0.0067(0.0050) | Loss 3.5177(3.5106) | Error 0.0022(0.0013) Steps 1000(996.85) | Grad Norm 1.0091(0.6245) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 22.6028(23.4375) | Bit/dim 3.5132(3.5062) | Xent 0.0092(0.0049) | Loss 3.5178(3.5086) | Error 0.0022(0.0012) Steps 1000(998.74) | Grad Norm 0.5719(0.6419) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 111.1435, Epoch Time 1417.2377(1429.4923), Bit/dim 3.5379(best: 3.5380), Xent 2.4878, Loss 4.7818, Error 0.3580(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 23.4509(23.4264) | Bit/dim 3.4910(3.5060) | Xent 0.0027(0.0044) | Loss 3.4923(3.5082) | Error 0.0011(0.0011) Steps 994(998.74) | Grad Norm 0.2858(0.5985) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 22.9873(23.4221) | Bit/dim 3.5124(3.5082) | Xent 0.0032(0.0040) | Loss 3.5140(3.5103) | Error 0.0011(0.0010) Steps 994(997.09) | Grad Norm 0.3744(0.5916) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 23.2595(23.4774) | Bit/dim 3.5156(3.5080) | Xent 0.0041(0.0039) | Loss 3.5176(3.5099) | Error 0.0022(0.0010) Steps 1000(998.08) | Grad Norm 0.5403(0.5791) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 23.4392(23.5065) | Bit/dim 3.5200(3.5048) | Xent 0.0089(0.0043) | Loss 3.5245(3.5069) | Error 0.0022(0.0011) Steps 988(996.67) | Grad Norm 1.1024(0.6159) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 23.8641(23.5460) | Bit/dim 3.4808(3.5036) | Xent 0.0019(0.0048) | Loss 3.4817(3.5060) | Error 0.0000(0.0012) Steps 988(997.03) | Grad Norm 0.2806(0.6073) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 111.5619, Epoch Time 1423.0360(1429.2987), Bit/dim 3.5398(best: 3.5379), Xent 2.5357, Loss 4.8077, Error 0.3581(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 23.8876(23.5673) | Bit/dim 3.5045(3.5064) | Xent 0.0016(0.0043) | Loss 3.5053(3.5086) | Error 0.0000(0.0010) Steps 1000(998.00) | Grad Norm 0.2717(0.5631) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 23.7036(23.5397) | Bit/dim 3.5235(3.5052) | Xent 0.0022(0.0042) | Loss 3.5246(3.5074) | Error 0.0000(0.0010) Steps 988(998.65) | Grad Norm 0.5529(0.5700) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 23.6228(23.5305) | Bit/dim 3.5051(3.5055) | Xent 0.0017(0.0041) | Loss 3.5059(3.5075) | Error 0.0000(0.0010) Steps 994(998.31) | Grad Norm 0.3005(0.5779) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 24.0446(23.5346) | Bit/dim 3.5394(3.5044) | Xent 0.0021(0.0044) | Loss 3.5405(3.5067) | Error 0.0000(0.0011) Steps 1006(1000.65) | Grad Norm 0.4986(0.6585) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 23.8313(23.5433) | Bit/dim 3.5035(3.5060) | Xent 0.0110(0.0047) | Loss 3.5090(3.5083) | Error 0.0011(0.0011) Steps 994(998.33) | Grad Norm 0.5712(0.6571) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 23.3921(23.5340) | Bit/dim 3.5215(3.5054) | Xent 0.0055(0.0044) | Loss 3.5242(3.5076) | Error 0.0033(0.0011) Steps 1024(998.48) | Grad Norm 0.5239(0.6138) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 111.2917, Epoch Time 1421.0309(1429.0506), Bit/dim 3.5369(best: 3.5379), Xent 2.5366, Loss 4.8051, Error 0.3641(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 23.7397(23.5503) | Bit/dim 3.4861(3.5048) | Xent 0.0015(0.0044) | Loss 3.4868(3.5070) | Error 0.0000(0.0010) Steps 994(1000.59) | Grad Norm 0.3898(0.5949) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 23.5671(23.5557) | Bit/dim 3.5229(3.5082) | Xent 0.0099(0.0042) | Loss 3.5278(3.5103) | Error 0.0022(0.0010) Steps 1012(1000.05) | Grad Norm 0.9151(0.5971) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 23.0833(23.5393) | Bit/dim 3.4690(3.5061) | Xent 0.0039(0.0040) | Loss 3.4709(3.5081) | Error 0.0011(0.0009) Steps 994(1001.33) | Grad Norm 0.4296(0.5732) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 22.8354(23.5114) | Bit/dim 3.5330(3.5081) | Xent 0.0026(0.0040) | Loss 3.5343(3.5101) | Error 0.0000(0.0010) Steps 1000(999.37) | Grad Norm 0.2820(0.5313) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 23.2399(23.4932) | Bit/dim 3.5010(3.5056) | Xent 0.0019(0.0039) | Loss 3.5019(3.5075) | Error 0.0000(0.0010) Steps 982(998.82) | Grad Norm 0.3660(0.5311) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 111.3875, Epoch Time 1421.7656(1428.8321), Bit/dim 3.5375(best: 3.5369), Xent 2.6216, Loss 4.8483, Error 0.3667(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 23.7648(23.5255) | Bit/dim 3.5166(3.5047) | Xent 0.0017(0.0036) | Loss 3.5174(3.5066) | Error 0.0000(0.0010) Steps 994(999.29) | Grad Norm 0.3078(0.5611) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 23.7167(23.5377) | Bit/dim 3.5228(3.5050) | Xent 0.0126(0.0039) | Loss 3.5290(3.5069) | Error 0.0033(0.0011) Steps 1006(1001.49) | Grad Norm 0.9717(0.5919) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 23.6993(23.5549) | Bit/dim 3.4914(3.5029) | Xent 0.0037(0.0035) | Loss 3.4932(3.5046) | Error 0.0011(0.0010) Steps 1000(1002.50) | Grad Norm 0.7866(0.5812) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 24.1214(23.5313) | Bit/dim 3.5304(3.5040) | Xent 0.0013(0.0039) | Loss 3.5311(3.5060) | Error 0.0000(0.0010) Steps 1006(1001.52) | Grad Norm 0.4481(0.6160) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 23.3239(23.5217) | Bit/dim 3.4830(3.5033) | Xent 0.0034(0.0039) | Loss 3.4847(3.5053) | Error 0.0011(0.0010) Steps 1012(1003.71) | Grad Norm 0.9133(0.6218) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 23.0751(23.4829) | Bit/dim 3.4969(3.5040) | Xent 0.0051(0.0040) | Loss 3.4995(3.5060) | Error 0.0022(0.0010) Steps 1000(1003.82) | Grad Norm 0.7237(0.6279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 111.9596, Epoch Time 1420.6414(1428.5863), Bit/dim 3.5386(best: 3.5369), Xent 2.5913, Loss 4.8342, Error 0.3598(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 23.5954(23.5164) | Bit/dim 3.4882(3.5031) | Xent 0.0029(0.0038) | Loss 3.4896(3.5050) | Error 0.0011(0.0010) Steps 988(1004.06) | Grad Norm 1.0959(0.6239) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 24.0844(23.5623) | Bit/dim 3.5309(3.5047) | Xent 0.0035(0.0042) | Loss 3.5326(3.5068) | Error 0.0011(0.0012) Steps 988(1002.41) | Grad Norm 0.6284(0.6713) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 24.1633(23.5878) | Bit/dim 3.5220(3.5047) | Xent 0.0033(0.0039) | Loss 3.5237(3.5067) | Error 0.0011(0.0011) Steps 982(1002.13) | Grad Norm 0.8667(0.6538) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 23.4823(23.4848) | Bit/dim 3.5116(3.5032) | Xent 0.0057(0.0036) | Loss 3.5145(3.5050) | Error 0.0022(0.0011) Steps 1000(1001.28) | Grad Norm 0.6744(0.6566) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 23.4405(23.5332) | Bit/dim 3.5182(3.5047) | Xent 0.0038(0.0039) | Loss 3.5201(3.5066) | Error 0.0011(0.0012) Steps 1000(1002.16) | Grad Norm 0.7222(0.6948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 111.8301, Epoch Time 1422.7136(1428.4102), Bit/dim 3.5385(best: 3.5369), Xent 2.5642, Loss 4.8206, Error 0.3578(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 23.0835(23.4884) | Bit/dim 3.4952(3.5042) | Xent 0.0010(0.0040) | Loss 3.4957(3.5062) | Error 0.0000(0.0011) Steps 1006(1003.21) | Grad Norm 0.2723(0.6610) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 24.0039(23.5077) | Bit/dim 3.4868(3.5046) | Xent 0.0011(0.0037) | Loss 3.4873(3.5065) | Error 0.0000(0.0010) Steps 1006(1003.73) | Grad Norm 0.2446(0.6420) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 23.6665(23.5337) | Bit/dim 3.5211(3.5041) | Xent 0.0035(0.0036) | Loss 3.5229(3.5059) | Error 0.0011(0.0009) Steps 1000(999.83) | Grad Norm 0.7541(0.6632) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 23.7849(23.5263) | Bit/dim 3.5508(3.5058) | Xent 0.0105(0.0038) | Loss 3.5560(3.5077) | Error 0.0011(0.0009) Steps 1006(1000.01) | Grad Norm 0.6769(0.6121) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 23.3599(23.5726) | Bit/dim 3.5234(3.5040) | Xent 0.0028(0.0034) | Loss 3.5248(3.5057) | Error 0.0000(0.0008) Steps 1000(999.92) | Grad Norm 0.4724(0.5725) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 23.7084(23.6373) | Bit/dim 3.4878(3.5031) | Xent 0.0056(0.0040) | Loss 3.4906(3.5051) | Error 0.0011(0.0010) Steps 1012(1000.23) | Grad Norm 0.3864(0.5744) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 110.4940, Epoch Time 1425.1534(1428.3125), Bit/dim 3.5379(best: 3.5369), Xent 2.6389, Loss 4.8574, Error 0.3624(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 23.3813(23.5614) | Bit/dim 3.4978(3.5005) | Xent 0.0027(0.0041) | Loss 3.4991(3.5026) | Error 0.0011(0.0010) Steps 1000(999.47) | Grad Norm 0.3669(0.6288) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 23.3000(23.6414) | Bit/dim 3.5021(3.5020) | Xent 0.0031(0.0045) | Loss 3.5036(3.5043) | Error 0.0011(0.0011) Steps 982(999.34) | Grad Norm 0.5944(0.6368) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 23.6169(23.5915) | Bit/dim 3.4929(3.5026) | Xent 0.0029(0.0044) | Loss 3.4943(3.5047) | Error 0.0011(0.0011) Steps 982(998.87) | Grad Norm 0.5047(0.6669) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 23.0672(23.5667) | Bit/dim 3.4909(3.5055) | Xent 0.0070(0.0048) | Loss 3.4944(3.5079) | Error 0.0022(0.0012) Steps 1006(997.97) | Grad Norm 0.8376(0.6997) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 24.0554(23.5496) | Bit/dim 3.5023(3.5061) | Xent 0.0048(0.0052) | Loss 3.5047(3.5087) | Error 0.0011(0.0014) Steps 994(997.47) | Grad Norm 0.8131(0.7115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 112.1205, Epoch Time 1422.9602(1428.1519), Bit/dim 3.5379(best: 3.5369), Xent 2.5971, Loss 4.8365, Error 0.3614(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 24.2266(23.5137) | Bit/dim 3.4837(3.5043) | Xent 0.0009(0.0052) | Loss 3.4842(3.5068) | Error 0.0000(0.0013) Steps 1018(998.58) | Grad Norm 0.3575(0.6716) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 23.8321(23.4803) | Bit/dim 3.4944(3.5028) | Xent 0.0069(0.0048) | Loss 3.4978(3.5052) | Error 0.0011(0.0012) Steps 1000(996.73) | Grad Norm 0.6343(0.6426) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 23.2648(23.4677) | Bit/dim 3.5050(3.5010) | Xent 0.0009(0.0051) | Loss 3.5055(3.5036) | Error 0.0000(0.0012) Steps 1000(998.37) | Grad Norm 0.3073(0.6316) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 23.3187(23.5159) | Bit/dim 3.5289(3.5040) | Xent 0.0011(0.0045) | Loss 3.5295(3.5063) | Error 0.0000(0.0011) Steps 988(996.67) | Grad Norm 0.2744(0.6154) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 22.9732(23.4850) | Bit/dim 3.4835(3.5035) | Xent 0.0070(0.0044) | Loss 3.4870(3.5057) | Error 0.0011(0.0011) Steps 1018(999.22) | Grad Norm 0.8955(0.6223) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 23.4283(23.5070) | Bit/dim 3.5028(3.5041) | Xent 0.0028(0.0044) | Loss 3.5041(3.5063) | Error 0.0011(0.0012) Steps 994(999.26) | Grad Norm 0.3250(0.6355) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 110.9799, Epoch Time 1417.7085(1427.8386), Bit/dim 3.5364(best: 3.5369), Xent 2.6081, Loss 4.8404, Error 0.3616(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 23.9765(23.5017) | Bit/dim 3.4787(3.5010) | Xent 0.0041(0.0043) | Loss 3.4807(3.5032) | Error 0.0011(0.0013) Steps 1000(1000.27) | Grad Norm 1.2915(0.7120) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 23.6392(23.4905) | Bit/dim 3.5211(3.5034) | Xent 0.0036(0.0040) | Loss 3.5229(3.5054) | Error 0.0022(0.0012) Steps 994(999.14) | Grad Norm 0.3951(0.6825) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 23.8149(23.5030) | Bit/dim 3.4911(3.5015) | Xent 0.0029(0.0037) | Loss 3.4925(3.5034) | Error 0.0011(0.0011) Steps 988(997.51) | Grad Norm 0.6737(0.6376) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 23.4461(23.5074) | Bit/dim 3.4952(3.5018) | Xent 0.0098(0.0040) | Loss 3.5001(3.5038) | Error 0.0044(0.0011) Steps 970(994.87) | Grad Norm 1.1695(0.6372) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 23.3349(23.5011) | Bit/dim 3.4867(3.5042) | Xent 0.0014(0.0043) | Loss 3.4874(3.5063) | Error 0.0000(0.0011) Steps 1000(997.76) | Grad Norm 0.4079(0.7116) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 111.7618, Epoch Time 1419.6952(1427.5943), Bit/dim 3.5371(best: 3.5364), Xent 2.6077, Loss 4.8410, Error 0.3570(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 23.5001(23.4942) | Bit/dim 3.4799(3.5041) | Xent 0.0021(0.0041) | Loss 3.4810(3.5061) | Error 0.0011(0.0010) Steps 994(997.68) | Grad Norm 0.7541(0.6809) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 23.2969(23.4662) | Bit/dim 3.4637(3.5035) | Xent 0.0056(0.0042) | Loss 3.4665(3.5056) | Error 0.0011(0.0011) Steps 1012(1000.17) | Grad Norm 1.0087(0.7126) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 23.1643(23.4733) | Bit/dim 3.5061(3.5048) | Xent 0.0026(0.0042) | Loss 3.5074(3.5068) | Error 0.0011(0.0011) Steps 1000(1002.38) | Grad Norm 0.8393(0.7163) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 23.0160(23.5023) | Bit/dim 3.5425(3.5079) | Xent 0.0063(0.0040) | Loss 3.5456(3.5099) | Error 0.0022(0.0011) Steps 994(1003.44) | Grad Norm 1.3946(0.6911) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 22.8922(23.4627) | Bit/dim 3.4678(3.5048) | Xent 0.0042(0.0039) | Loss 3.4699(3.5067) | Error 0.0011(0.0010) Steps 994(1002.44) | Grad Norm 0.6848(0.6610) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 23.1037(23.4617) | Bit/dim 3.5330(3.5033) | Xent 0.0018(0.0039) | Loss 3.5339(3.5052) | Error 0.0000(0.0010) Steps 1006(1002.73) | Grad Norm 0.7535(0.6483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 111.1509, Epoch Time 1418.3895(1427.3182), Bit/dim 3.5365(best: 3.5364), Xent 2.6270, Loss 4.8500, Error 0.3611(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 23.6411(23.4011) | Bit/dim 3.4614(3.5012) | Xent 0.0021(0.0039) | Loss 3.4625(3.5032) | Error 0.0000(0.0010) Steps 1018(1002.51) | Grad Norm 0.4438(0.6920) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 23.8477(23.4652) | Bit/dim 3.4551(3.5017) | Xent 0.0089(0.0044) | Loss 3.4596(3.5039) | Error 0.0044(0.0012) Steps 994(1002.02) | Grad Norm 1.0387(0.7146) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 24.2174(23.5037) | Bit/dim 3.5059(3.5016) | Xent 0.0014(0.0040) | Loss 3.5066(3.5036) | Error 0.0000(0.0011) Steps 1024(1002.11) | Grad Norm 0.5023(0.6974) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 23.2386(23.4663) | Bit/dim 3.4774(3.5020) | Xent 0.0048(0.0042) | Loss 3.4798(3.5041) | Error 0.0022(0.0012) Steps 994(1001.38) | Grad Norm 0.8715(0.7253) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 23.8045(23.4065) | Bit/dim 3.5293(3.5031) | Xent 0.0011(0.0042) | Loss 3.5298(3.5052) | Error 0.0000(0.0011) Steps 1000(1000.73) | Grad Norm 0.3000(0.7075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 111.8100, Epoch Time 1417.0077(1427.0088), Bit/dim 3.5361(best: 3.5364), Xent 2.6294, Loss 4.8508, Error 0.3639(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 23.5349(23.4147) | Bit/dim 3.5135(3.5052) | Xent 0.0015(0.0037) | Loss 3.5143(3.5070) | Error 0.0000(0.0010) Steps 1000(999.37) | Grad Norm 0.3583(0.6602) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 23.0336(23.3549) | Bit/dim 3.5312(3.5042) | Xent 0.0120(0.0040) | Loss 3.5372(3.5062) | Error 0.0022(0.0009) Steps 1006(998.26) | Grad Norm 1.6853(0.6562) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 24.0018(23.4083) | Bit/dim 3.5218(3.5031) | Xent 0.0047(0.0040) | Loss 3.5242(3.5051) | Error 0.0022(0.0011) Steps 1006(997.80) | Grad Norm 0.9796(0.7069) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 24.0253(23.4876) | Bit/dim 3.4862(3.5035) | Xent 0.0022(0.0040) | Loss 3.4873(3.5055) | Error 0.0011(0.0012) Steps 988(998.85) | Grad Norm 0.5372(0.7310) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 23.4380(23.5252) | Bit/dim 3.4889(3.5018) | Xent 0.0025(0.0038) | Loss 3.4901(3.5037) | Error 0.0000(0.0011) Steps 994(999.07) | Grad Norm 0.5693(0.7172) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 23.4379(23.5464) | Bit/dim 3.5166(3.5022) | Xent 0.0039(0.0038) | Loss 3.5185(3.5041) | Error 0.0011(0.0011) Steps 1012(1000.84) | Grad Norm 0.6219(0.7252) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 112.2335, Epoch Time 1421.4955(1426.8434), Bit/dim 3.5368(best: 3.5361), Xent 2.6475, Loss 4.8605, Error 0.3618(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 23.4954(23.5436) | Bit/dim 3.5083(3.5028) | Xent 0.0016(0.0039) | Loss 3.5091(3.5047) | Error 0.0000(0.0009) Steps 1000(1000.40) | Grad Norm 0.3499(0.6870) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 23.0375(23.4834) | Bit/dim 3.4999(3.5029) | Xent 0.0012(0.0042) | Loss 3.5005(3.5051) | Error 0.0000(0.0010) Steps 1006(998.86) | Grad Norm 0.2516(0.6561) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 23.3048(23.4579) | Bit/dim 3.5101(3.5019) | Xent 0.0023(0.0043) | Loss 3.5113(3.5040) | Error 0.0011(0.0011) Steps 1006(998.40) | Grad Norm 0.7471(0.6974) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 23.4693(23.4782) | Bit/dim 3.4774(3.4992) | Xent 0.0039(0.0038) | Loss 3.4794(3.5011) | Error 0.0011(0.0010) Steps 1006(998.59) | Grad Norm 0.8498(0.6940) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 23.5530(23.4834) | Bit/dim 3.4975(3.5018) | Xent 0.0087(0.0043) | Loss 3.5019(3.5039) | Error 0.0022(0.0012) Steps 1012(1000.31) | Grad Norm 0.5089(0.6677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 112.4216, Epoch Time 1418.8761(1426.6044), Bit/dim 3.5356(best: 3.5361), Xent 2.6467, Loss 4.8590, Error 0.3651(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 23.4201(23.4881) | Bit/dim 3.5012(3.5011) | Xent 0.0012(0.0038) | Loss 3.5018(3.5030) | Error 0.0000(0.0011) Steps 1000(1001.96) | Grad Norm 0.2016(0.6180) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 23.4666(23.4625) | Bit/dim 3.4932(3.5001) | Xent 0.0034(0.0033) | Loss 3.4949(3.5017) | Error 0.0011(0.0010) Steps 1012(1002.14) | Grad Norm 0.5550(0.5717) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 23.6899(23.5050) | Bit/dim 3.5128(3.5026) | Xent 0.0011(0.0033) | Loss 3.5134(3.5043) | Error 0.0000(0.0010) Steps 970(1000.79) | Grad Norm 0.3517(0.5850) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 23.6699(23.5248) | Bit/dim 3.5056(3.5011) | Xent 0.0027(0.0033) | Loss 3.5069(3.5028) | Error 0.0011(0.0009) Steps 982(1000.46) | Grad Norm 0.7965(0.5780) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 23.4181(23.5348) | Bit/dim 3.4993(3.4992) | Xent 0.0038(0.0036) | Loss 3.5012(3.5010) | Error 0.0011(0.0010) Steps 976(998.95) | Grad Norm 0.5804(0.6246) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 22.6509(23.5046) | Bit/dim 3.4890(3.5013) | Xent 0.0058(0.0040) | Loss 3.4919(3.5033) | Error 0.0022(0.0010) Steps 994(997.80) | Grad Norm 1.2695(0.6483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 111.5702, Epoch Time 1421.2259(1426.4431), Bit/dim 3.5361(best: 3.5356), Xent 2.7102, Loss 4.8912, Error 0.3696(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 23.1565(23.4834) | Bit/dim 3.5030(3.5015) | Xent 0.0022(0.0041) | Loss 3.5041(3.5036) | Error 0.0000(0.0010) Steps 1012(999.09) | Grad Norm 0.4495(0.6685) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 23.5726(23.4815) | Bit/dim 3.5054(3.5013) | Xent 0.0066(0.0042) | Loss 3.5087(3.5034) | Error 0.0011(0.0011) Steps 1012(1000.83) | Grad Norm 0.5014(0.6971) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 23.2904(23.5234) | Bit/dim 3.5015(3.5013) | Xent 0.0030(0.0038) | Loss 3.5030(3.5032) | Error 0.0000(0.0010) Steps 1006(1000.86) | Grad Norm 0.7957(0.7046) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 23.5012(23.4965) | Bit/dim 3.5029(3.5020) | Xent 0.0015(0.0040) | Loss 3.5036(3.5040) | Error 0.0000(0.0011) Steps 1012(1000.37) | Grad Norm 0.3109(0.7422) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 23.2487(23.5291) | Bit/dim 3.5071(3.5031) | Xent 0.0020(0.0036) | Loss 3.5081(3.5049) | Error 0.0011(0.0010) Steps 1006(1000.60) | Grad Norm 0.3846(0.6736) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 111.8779, Epoch Time 1421.4636(1426.2937), Bit/dim 3.5364(best: 3.5356), Xent 2.6985, Loss 4.8857, Error 0.3640(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 24.5494(23.5320) | Bit/dim 3.4683(3.5008) | Xent 0.0044(0.0040) | Loss 3.4705(3.5027) | Error 0.0022(0.0011) Steps 1018(1001.24) | Grad Norm 0.9624(0.7707) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 23.8208(23.4897) | Bit/dim 3.4700(3.5009) | Xent 0.0050(0.0044) | Loss 3.4725(3.5031) | Error 0.0011(0.0013) Steps 1000(1001.42) | Grad Norm 1.1972(0.8558) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 23.9316(23.4864) | Bit/dim 3.5184(3.5007) | Xent 0.0015(0.0045) | Loss 3.5192(3.5030) | Error 0.0000(0.0014) Steps 982(1000.73) | Grad Norm 0.5620(0.8277) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 22.9574(23.4695) | Bit/dim 3.5046(3.5009) | Xent 0.0009(0.0043) | Loss 3.5051(3.5031) | Error 0.0000(0.0013) Steps 1006(1000.12) | Grad Norm 0.3595(0.8129) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 22.8063(23.4813) | Bit/dim 3.4937(3.5023) | Xent 0.0018(0.0038) | Loss 3.4946(3.5042) | Error 0.0000(0.0011) Steps 994(997.59) | Grad Norm 0.4813(0.7131) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 23.4935(23.4574) | Bit/dim 3.4778(3.5034) | Xent 0.0014(0.0040) | Loss 3.4785(3.5054) | Error 0.0000(0.0010) Steps 994(997.80) | Grad Norm 0.2955(0.6637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 110.7736, Epoch Time 1416.4830(1425.9994), Bit/dim 3.5352(best: 3.5356), Xent 2.6621, Loss 4.8662, Error 0.3655(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 23.7265(23.4278) | Bit/dim 3.4899(3.5017) | Xent 0.0030(0.0037) | Loss 3.4914(3.5036) | Error 0.0011(0.0010) Steps 1006(997.84) | Grad Norm 0.8685(0.6570) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 23.0604(23.4317) | Bit/dim 3.5227(3.5046) | Xent 0.0059(0.0035) | Loss 3.5257(3.5064) | Error 0.0011(0.0009) Steps 994(995.18) | Grad Norm 0.6264(0.6546) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 23.7486(23.4143) | Bit/dim 3.4824(3.5030) | Xent 0.0062(0.0042) | Loss 3.4855(3.5051) | Error 0.0022(0.0012) Steps 982(993.52) | Grad Norm 0.7529(0.6942) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 23.5930(23.3950) | Bit/dim 3.5037(3.5016) | Xent 0.0017(0.0041) | Loss 3.5045(3.5036) | Error 0.0000(0.0011) Steps 1018(994.74) | Grad Norm 0.5009(0.7543) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 22.8661(23.3909) | Bit/dim 3.4855(3.5014) | Xent 0.0071(0.0040) | Loss 3.4890(3.5033) | Error 0.0011(0.0010) Steps 1006(997.25) | Grad Norm 0.6743(0.7578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 111.9836, Epoch Time 1413.1904(1425.6151), Bit/dim 3.5360(best: 3.5352), Xent 2.7071, Loss 4.8895, Error 0.3609(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 23.4793(23.3954) | Bit/dim 3.5210(3.5028) | Xent 0.0017(0.0036) | Loss 3.5219(3.5046) | Error 0.0000(0.0009) Steps 1018(999.33) | Grad Norm 0.4302(0.7570) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 23.2298(23.4325) | Bit/dim 3.5112(3.5056) | Xent 0.0019(0.0037) | Loss 3.5122(3.5074) | Error 0.0000(0.0009) Steps 982(998.85) | Grad Norm 0.4654(0.7479) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 23.1518(23.4357) | Bit/dim 3.4592(3.5025) | Xent 0.0136(0.0038) | Loss 3.4661(3.5044) | Error 0.0011(0.0009) Steps 988(998.72) | Grad Norm 1.0329(0.7188) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 22.9602(23.4449) | Bit/dim 3.4993(3.5025) | Xent 0.0008(0.0035) | Loss 3.4997(3.5043) | Error 0.0000(0.0009) Steps 994(996.92) | Grad Norm 0.4687(0.6947) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 24.1826(23.5207) | Bit/dim 3.5261(3.4993) | Xent 0.0016(0.0036) | Loss 3.5269(3.5011) | Error 0.0000(0.0009) Steps 994(997.04) | Grad Norm 0.3066(0.6824) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 22.8800(23.4726) | Bit/dim 3.4853(3.4991) | Xent 0.0010(0.0036) | Loss 3.4858(3.5009) | Error 0.0000(0.0010) Steps 1006(994.97) | Grad Norm 0.6580(0.7222) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 110.8199, Epoch Time 1419.6940(1425.4375), Bit/dim 3.5363(best: 3.5352), Xent 2.7476, Loss 4.9101, Error 0.3664(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 23.2902(23.4349) | Bit/dim 3.5348(3.5008) | Xent 0.0014(0.0035) | Loss 3.5355(3.5025) | Error 0.0000(0.0009) Steps 994(994.44) | Grad Norm 0.5509(0.7284) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 23.8702(23.5056) | Bit/dim 3.5155(3.4993) | Xent 0.0021(0.0032) | Loss 3.5166(3.5009) | Error 0.0011(0.0009) Steps 988(993.99) | Grad Norm 0.4549(0.7114) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 24.2972(23.4647) | Bit/dim 3.4966(3.5013) | Xent 0.0013(0.0032) | Loss 3.4972(3.5029) | Error 0.0000(0.0008) Steps 1000(993.54) | Grad Norm 0.2881(0.7022) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 23.6867(23.5106) | Bit/dim 3.5040(3.5000) | Xent 0.0077(0.0032) | Loss 3.5079(3.5016) | Error 0.0022(0.0008) Steps 976(994.03) | Grad Norm 1.3836(0.6789) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 23.5992(23.5263) | Bit/dim 3.4971(3.5004) | Xent 0.0036(0.0030) | Loss 3.4988(3.5019) | Error 0.0011(0.0008) Steps 1012(994.24) | Grad Norm 1.0871(0.6438) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 112.9311, Epoch Time 1424.0574(1425.3961), Bit/dim 3.5364(best: 3.5352), Xent 2.7402, Loss 4.9065, Error 0.3658(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 23.6007(23.5653) | Bit/dim 3.4905(3.4994) | Xent 0.0021(0.0031) | Loss 3.4915(3.5010) | Error 0.0011(0.0008) Steps 1000(997.18) | Grad Norm 0.8070(0.6843) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 23.5740(23.5673) | Bit/dim 3.5222(3.5003) | Xent 0.0037(0.0035) | Loss 3.5240(3.5020) | Error 0.0022(0.0010) Steps 1012(996.44) | Grad Norm 0.7324(0.7346) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 22.8068(23.5511) | Bit/dim 3.4965(3.5010) | Xent 0.0019(0.0035) | Loss 3.4974(3.5028) | Error 0.0000(0.0009) Steps 1000(995.59) | Grad Norm 0.3958(0.7140) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 23.6194(23.5200) | Bit/dim 3.4960(3.4993) | Xent 0.0015(0.0036) | Loss 3.4968(3.5011) | Error 0.0011(0.0010) Steps 1000(995.73) | Grad Norm 0.6756(0.7292) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 23.1280(23.4446) | Bit/dim 3.5346(3.4997) | Xent 0.0059(0.0045) | Loss 3.5376(3.5020) | Error 0.0011(0.0012) Steps 976(995.64) | Grad Norm 1.0440(0.8220) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 23.0985(23.4740) | Bit/dim 3.5031(3.5012) | Xent 0.0090(0.0045) | Loss 3.5076(3.5034) | Error 0.0022(0.0012) Steps 988(995.80) | Grad Norm 0.9413(0.8570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 111.5215, Epoch Time 1418.7463(1425.1966), Bit/dim 3.5368(best: 3.5352), Xent 2.7335, Loss 4.9036, Error 0.3654(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 23.7937(23.4512) | Bit/dim 3.4809(3.4992) | Xent 0.0044(0.0040) | Loss 3.4832(3.5012) | Error 0.0011(0.0010) Steps 1006(996.34) | Grad Norm 0.4783(0.7871) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 23.2520(23.4830) | Bit/dim 3.4967(3.5001) | Xent 0.0095(0.0041) | Loss 3.5014(3.5021) | Error 0.0022(0.0010) Steps 982(996.14) | Grad Norm 0.6404(0.8453) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 24.0468(23.5447) | Bit/dim 3.5395(3.5022) | Xent 0.0085(0.0047) | Loss 3.5438(3.5045) | Error 0.0022(0.0012) Steps 970(993.45) | Grad Norm 0.9069(0.9359) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 23.4788(23.5182) | Bit/dim 3.5042(3.5014) | Xent 0.0033(0.0047) | Loss 3.5059(3.5037) | Error 0.0000(0.0011) Steps 988(993.32) | Grad Norm 0.8297(0.8757) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 23.1593(23.5384) | Bit/dim 3.5312(3.5003) | Xent 0.0016(0.0048) | Loss 3.5319(3.5027) | Error 0.0000(0.0011) Steps 994(993.58) | Grad Norm 0.4267(0.8702) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 111.4580, Epoch Time 1421.3288(1425.0805), Bit/dim 3.5358(best: 3.5352), Xent 2.7429, Loss 4.9072, Error 0.3677(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 22.9041(23.4540) | Bit/dim 3.5051(3.5003) | Xent 0.0026(0.0043) | Loss 3.5063(3.5025) | Error 0.0022(0.0011) Steps 988(994.56) | Grad Norm 0.7411(0.8040) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 23.3380(23.4346) | Bit/dim 3.5130(3.4985) | Xent 0.0017(0.0036) | Loss 3.5139(3.5003) | Error 0.0000(0.0009) Steps 1000(995.48) | Grad Norm 0.4709(0.7104) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 24.0096(23.4476) | Bit/dim 3.5039(3.4993) | Xent 0.0058(0.0035) | Loss 3.5068(3.5011) | Error 0.0011(0.0009) Steps 1006(993.77) | Grad Norm 1.1828(0.7114) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 22.8793(23.4340) | Bit/dim 3.5185(3.5000) | Xent 0.0103(0.0041) | Loss 3.5236(3.5020) | Error 0.0033(0.0011) Steps 988(993.71) | Grad Norm 2.1755(0.8039) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 23.7922(23.4383) | Bit/dim 3.5287(3.5026) | Xent 0.0022(0.0046) | Loss 3.5298(3.5049) | Error 0.0000(0.0012) Steps 1006(995.54) | Grad Norm 0.3695(0.8438) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 23.9793(23.4206) | Bit/dim 3.4777(3.5021) | Xent 0.0015(0.0042) | Loss 3.4785(3.5042) | Error 0.0000(0.0011) Steps 1024(997.64) | Grad Norm 0.6246(0.8161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 111.0322, Epoch Time 1413.4283(1424.7310), Bit/dim 3.5365(best: 3.5352), Xent 2.7208, Loss 4.8969, Error 0.3655(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 23.0022(23.3938) | Bit/dim 3.4955(3.5002) | Xent 0.0008(0.0037) | Loss 3.4959(3.5021) | Error 0.0000(0.0010) Steps 988(1000.11) | Grad Norm 0.4129(0.7602) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 23.9342(23.4304) | Bit/dim 3.5052(3.4992) | Xent 0.0054(0.0036) | Loss 3.5079(3.5010) | Error 0.0011(0.0009) Steps 976(997.06) | Grad Norm 0.6284(0.7204) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 23.9951(23.4603) | Bit/dim 3.4997(3.5001) | Xent 0.0058(0.0039) | Loss 3.5026(3.5020) | Error 0.0011(0.0010) Steps 1012(996.65) | Grad Norm 1.5111(0.7463) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 23.1021(23.4329) | Bit/dim 3.5044(3.5012) | Xent 0.0056(0.0043) | Loss 3.5072(3.5033) | Error 0.0022(0.0011) Steps 988(996.18) | Grad Norm 0.9159(0.8337) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 23.3860(23.4949) | Bit/dim 3.4980(3.4996) | Xent 0.0019(0.0049) | Loss 3.4990(3.5021) | Error 0.0000(0.0013) Steps 1006(995.68) | Grad Norm 0.7880(0.9131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 112.0899, Epoch Time 1420.1138(1424.5924), Bit/dim 3.5369(best: 3.5352), Xent 2.8221, Loss 4.9480, Error 0.3749(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 22.4304(23.4661) | Bit/dim 3.4654(3.4992) | Xent 0.0099(0.0055) | Loss 3.4704(3.5020) | Error 0.0011(0.0014) Steps 1006(995.71) | Grad Norm 1.0967(1.0658) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 23.8220(23.5074) | Bit/dim 3.4714(3.5009) | Xent 0.0186(0.0055) | Loss 3.4807(3.5037) | Error 0.0056(0.0014) Steps 1012(997.37) | Grad Norm 1.7230(1.0621) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 23.1820(23.4033) | Bit/dim 3.5066(3.5018) | Xent 0.0108(0.0055) | Loss 3.5120(3.5046) | Error 0.0022(0.0014) Steps 1012(998.06) | Grad Norm 0.8524(1.0001) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 23.3895(23.3511) | Bit/dim 3.5401(3.5039) | Xent 0.0037(0.0058) | Loss 3.5420(3.5068) | Error 0.0022(0.0014) Steps 994(998.62) | Grad Norm 1.0936(0.9942) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 22.8948(23.3641) | Bit/dim 3.5110(3.5031) | Xent 0.0010(0.0052) | Loss 3.5115(3.5057) | Error 0.0000(0.0013) Steps 982(998.75) | Grad Norm 0.5897(0.9316) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 23.7278(23.4129) | Bit/dim 3.4797(3.4988) | Xent 0.0018(0.0046) | Loss 3.4807(3.5011) | Error 0.0000(0.0012) Steps 988(999.35) | Grad Norm 0.4998(0.8639) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 113.0754, Epoch Time 1414.7331(1424.2967), Bit/dim 3.5337(best: 3.5352), Xent 2.6907, Loss 4.8791, Error 0.3606(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 23.2695(23.3911) | Bit/dim 3.4790(3.4998) | Xent 0.0035(0.0044) | Loss 3.4807(3.5020) | Error 0.0011(0.0012) Steps 1018(999.69) | Grad Norm 0.8329(0.8144) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 23.2209(23.3652) | Bit/dim 3.4904(3.5009) | Xent 0.0054(0.0040) | Loss 3.4930(3.5029) | Error 0.0011(0.0010) Steps 988(996.26) | Grad Norm 0.6885(0.7418) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 23.1872(23.3347) | Bit/dim 3.4919(3.4995) | Xent 0.0173(0.0047) | Loss 3.5005(3.5018) | Error 0.0044(0.0012) Steps 988(994.52) | Grad Norm 1.1383(0.7586) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 23.4083(23.3061) | Bit/dim 3.4397(3.4959) | Xent 0.0021(0.0045) | Loss 3.4408(3.4981) | Error 0.0011(0.0011) Steps 1000(993.23) | Grad Norm 1.1303(0.7377) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 22.4895(23.3344) | Bit/dim 3.5157(3.4966) | Xent 0.0109(0.0045) | Loss 3.5211(3.4988) | Error 0.0033(0.0012) Steps 982(992.35) | Grad Norm 1.5228(0.7658) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 110.9362, Epoch Time 1409.9398(1423.8660), Bit/dim 3.5356(best: 3.5337), Xent 2.7459, Loss 4.9085, Error 0.3652(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 22.5041(23.2902) | Bit/dim 3.5178(3.5006) | Xent 0.0011(0.0039) | Loss 3.5184(3.5026) | Error 0.0000(0.0011) Steps 964(991.49) | Grad Norm 0.3424(0.7343) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 23.5787(23.3544) | Bit/dim 3.4884(3.4990) | Xent 0.0026(0.0034) | Loss 3.4896(3.5007) | Error 0.0011(0.0009) Steps 958(991.10) | Grad Norm 0.8287(0.7178) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 23.8221(23.4031) | Bit/dim 3.5017(3.5024) | Xent 0.0008(0.0035) | Loss 3.5021(3.5042) | Error 0.0000(0.0009) Steps 1000(990.52) | Grad Norm 0.3228(0.7212) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 23.6292(23.3715) | Bit/dim 3.4852(3.5000) | Xent 0.0038(0.0043) | Loss 3.4871(3.5021) | Error 0.0011(0.0011) Steps 1000(991.77) | Grad Norm 0.8577(0.8461) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 22.8334(23.3353) | Bit/dim 3.4736(3.4993) | Xent 0.0013(0.0044) | Loss 3.4742(3.5015) | Error 0.0000(0.0010) Steps 988(990.43) | Grad Norm 0.3459(0.8136) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 22.7379(23.3174) | Bit/dim 3.4768(3.4973) | Xent 0.0039(0.0042) | Loss 3.4787(3.4994) | Error 0.0011(0.0011) Steps 976(988.36) | Grad Norm 0.7957(0.7951) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 110.8064, Epoch Time 1409.5193(1423.4356), Bit/dim 3.5341(best: 3.5337), Xent 2.7435, Loss 4.9058, Error 0.3648(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 23.0184(23.3886) | Bit/dim 3.5006(3.4952) | Xent 0.0011(0.0038) | Loss 3.5011(3.4971) | Error 0.0000(0.0010) Steps 982(987.33) | Grad Norm 0.2591(0.7437) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 23.2315(23.4315) | Bit/dim 3.4885(3.4954) | Xent 0.0024(0.0040) | Loss 3.4897(3.4974) | Error 0.0011(0.0011) Steps 988(988.09) | Grad Norm 1.0128(0.7945) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 22.5472(23.4177) | Bit/dim 3.5181(3.4963) | Xent 0.0037(0.0038) | Loss 3.5200(3.4982) | Error 0.0011(0.0010) Steps 988(988.89) | Grad Norm 0.8582(0.7357) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 22.8720(23.4185) | Bit/dim 3.5511(3.5000) | Xent 0.0012(0.0038) | Loss 3.5517(3.5019) | Error 0.0000(0.0010) Steps 994(989.70) | Grad Norm 0.2918(0.7286) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 23.6137(23.4630) | Bit/dim 3.4892(3.5008) | Xent 0.0017(0.0043) | Loss 3.4901(3.5030) | Error 0.0011(0.0010) Steps 1000(990.70) | Grad Norm 0.5688(0.7084) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 112.0455, Epoch Time 1420.2371(1423.3396), Bit/dim 3.5332(best: 3.5337), Xent 2.7534, Loss 4.9099, Error 0.3687(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 23.2106(23.4099) | Bit/dim 3.5140(3.4997) | Xent 0.0143(0.0041) | Loss 3.5211(3.5018) | Error 0.0033(0.0010) Steps 958(991.09) | Grad Norm 1.0587(0.6551) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 23.7779(23.4335) | Bit/dim 3.5151(3.4988) | Xent 0.0042(0.0041) | Loss 3.5172(3.5009) | Error 0.0022(0.0010) Steps 1006(989.57) | Grad Norm 0.8892(0.6477) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 23.8684(23.5388) | Bit/dim 3.4709(3.4967) | Xent 0.0069(0.0038) | Loss 3.4743(3.4985) | Error 0.0022(0.0009) Steps 988(990.38) | Grad Norm 1.4800(0.6343) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 24.0101(23.4964) | Bit/dim 3.5288(3.4968) | Xent 0.0051(0.0036) | Loss 3.5314(3.4986) | Error 0.0033(0.0009) Steps 1006(991.37) | Grad Norm 1.1729(0.6761) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 22.6317(23.4355) | Bit/dim 3.5326(3.4996) | Xent 0.0030(0.0034) | Loss 3.5341(3.5014) | Error 0.0011(0.0009) Steps 988(991.89) | Grad Norm 0.7656(0.6966) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 22.8867(23.4193) | Bit/dim 3.4989(3.4990) | Xent 0.0077(0.0038) | Loss 3.5027(3.5009) | Error 0.0011(0.0010) Steps 1006(993.57) | Grad Norm 1.1195(0.7304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 111.2052, Epoch Time 1417.0311(1423.1504), Bit/dim 3.5336(best: 3.5332), Xent 2.7987, Loss 4.9329, Error 0.3699(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 23.2627(23.4147) | Bit/dim 3.4846(3.5003) | Xent 0.0017(0.0039) | Loss 3.4854(3.5023) | Error 0.0000(0.0010) Steps 982(992.28) | Grad Norm 0.4868(0.7266) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 23.0705(23.3771) | Bit/dim 3.4718(3.4983) | Xent 0.0017(0.0036) | Loss 3.4726(3.5001) | Error 0.0011(0.0010) Steps 1006(992.66) | Grad Norm 0.6554(0.7208) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 23.4963(23.4405) | Bit/dim 3.4994(3.4993) | Xent 0.0009(0.0040) | Loss 3.4999(3.5013) | Error 0.0000(0.0010) Steps 982(989.77) | Grad Norm 0.3713(0.7115) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 23.5813(23.4152) | Bit/dim 3.5125(3.4978) | Xent 0.0009(0.0037) | Loss 3.5129(3.4996) | Error 0.0000(0.0009) Steps 1000(992.50) | Grad Norm 0.3253(0.7370) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 23.1716(23.4306) | Bit/dim 3.4988(3.4991) | Xent 0.0037(0.0036) | Loss 3.5007(3.5009) | Error 0.0011(0.0009) Steps 988(991.92) | Grad Norm 0.8152(0.7623) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 112.4591, Epoch Time 1416.8173(1422.9604), Bit/dim 3.5330(best: 3.5332), Xent 2.7372, Loss 4.9016, Error 0.3601(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 23.7056(23.4054) | Bit/dim 3.5005(3.4983) | Xent 0.0066(0.0039) | Loss 3.5038(3.5002) | Error 0.0011(0.0010) Steps 1006(992.58) | Grad Norm 1.5193(0.8747) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 24.1757(23.4199) | Bit/dim 3.4980(3.4974) | Xent 0.0007(0.0043) | Loss 3.4983(3.4996) | Error 0.0000(0.0011) Steps 1000(991.99) | Grad Norm 0.4980(0.8967) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 23.3607(23.4450) | Bit/dim 3.4920(3.4981) | Xent 0.0020(0.0042) | Loss 3.4930(3.5002) | Error 0.0011(0.0011) Steps 994(995.72) | Grad Norm 0.6959(0.9298) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 23.9316(23.4587) | Bit/dim 3.4808(3.4995) | Xent 0.0099(0.0044) | Loss 3.4858(3.5017) | Error 0.0044(0.0012) Steps 988(994.70) | Grad Norm 1.7419(0.9038) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 24.3700(23.5473) | Bit/dim 3.4541(3.4971) | Xent 0.0041(0.0045) | Loss 3.4561(3.4993) | Error 0.0011(0.0011) Steps 1018(998.54) | Grad Norm 0.5535(0.9008) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 23.9533(23.6259) | Bit/dim 3.4844(3.4978) | Xent 0.0025(0.0041) | Loss 3.4857(3.4998) | Error 0.0011(0.0011) Steps 988(997.93) | Grad Norm 0.6327(0.8696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 112.2956, Epoch Time 1425.5458(1423.0379), Bit/dim 3.5334(best: 3.5330), Xent 2.7505, Loss 4.9086, Error 0.3645(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 23.6366(23.6212) | Bit/dim 3.4628(3.4994) | Xent 0.0008(0.0040) | Loss 3.4632(3.5014) | Error 0.0000(0.0010) Steps 988(997.03) | Grad Norm 0.4717(0.8594) | Total Time 14.00(14.00)\n",
      "Iter 16410 | Time 24.3458(23.6279) | Bit/dim 3.5267(3.4964) | Xent 0.0008(0.0033) | Loss 3.5271(3.4981) | Error 0.0000(0.0008) Steps 1012(998.82) | Grad Norm 0.2992(0.7730) | Total Time 14.00(14.00)\n",
      "Iter 16420 | Time 22.9757(23.5312) | Bit/dim 3.5091(3.4997) | Xent 0.0011(0.0034) | Loss 3.5097(3.5014) | Error 0.0000(0.0007) Steps 994(999.59) | Grad Norm 0.3340(0.6854) | Total Time 14.00(14.00)\n",
      "Iter 16430 | Time 23.7472(23.5383) | Bit/dim 3.4815(3.4997) | Xent 0.0027(0.0036) | Loss 3.4828(3.5015) | Error 0.0011(0.0008) Steps 976(997.43) | Grad Norm 0.5061(0.6904) | Total Time 14.00(14.00)\n",
      "Iter 16440 | Time 23.6984(23.5256) | Bit/dim 3.4892(3.4967) | Xent 0.0021(0.0035) | Loss 3.4902(3.4985) | Error 0.0011(0.0009) Steps 982(997.10) | Grad Norm 0.7002(0.7259) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 109.9016, Epoch Time 1419.1647(1422.9217), Bit/dim 3.5322(best: 3.5330), Xent 2.7760, Loss 4.9202, Error 0.3660(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 23.4844(23.4840) | Bit/dim 3.5041(3.4950) | Xent 0.0068(0.0042) | Loss 3.5075(3.4971) | Error 0.0011(0.0009) Steps 1024(997.52) | Grad Norm 0.8750(0.7066) | Total Time 14.00(14.00)\n",
      "Iter 16460 | Time 23.5337(23.4939) | Bit/dim 3.4882(3.4944) | Xent 0.0008(0.0036) | Loss 3.4886(3.4962) | Error 0.0000(0.0008) Steps 994(996.16) | Grad Norm 0.3601(0.7036) | Total Time 14.00(14.00)\n",
      "Iter 16470 | Time 23.6771(23.5084) | Bit/dim 3.4934(3.4949) | Xent 0.0018(0.0035) | Loss 3.4944(3.4967) | Error 0.0011(0.0009) Steps 1018(996.04) | Grad Norm 0.5984(0.7169) | Total Time 14.00(14.00)\n",
      "Iter 16480 | Time 22.8585(23.5189) | Bit/dim 3.4834(3.4953) | Xent 0.0029(0.0034) | Loss 3.4848(3.4970) | Error 0.0011(0.0009) Steps 1006(998.00) | Grad Norm 0.7728(0.7068) | Total Time 14.00(14.00)\n",
      "Iter 16490 | Time 23.4983(23.4915) | Bit/dim 3.4836(3.4976) | Xent 0.0005(0.0028) | Loss 3.4839(3.4990) | Error 0.0000(0.0007) Steps 1006(996.63) | Grad Norm 0.2146(0.5987) | Total Time 14.00(14.00)\n",
      "Iter 16500 | Time 22.6669(23.4055) | Bit/dim 3.5049(3.4975) | Xent 0.0010(0.0027) | Loss 3.5054(3.4988) | Error 0.0000(0.0006) Steps 994(995.06) | Grad Norm 0.2948(0.5729) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 112.0353, Epoch Time 1416.0520(1422.7156), Bit/dim 3.5318(best: 3.5322), Xent 2.7936, Loss 4.9286, Error 0.3701(best: 0.3525)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 23.2407(23.3875) | Bit/dim 3.5110(3.4979) | Xent 0.0064(0.0033) | Loss 3.5142(3.4995) | Error 0.0022(0.0009) Steps 970(994.13) | Grad Norm 1.2377(0.7603) | Total Time 14.00(14.00)\n",
      "Iter 16520 | Time 24.0639(23.4306) | Bit/dim 3.4646(3.4965) | Xent 0.0040(0.0043) | Loss 3.4666(3.4987) | Error 0.0011(0.0011) Steps 1024(995.14) | Grad Norm 1.0634(0.8656) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run2 --resume ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run2/epoch_250_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
