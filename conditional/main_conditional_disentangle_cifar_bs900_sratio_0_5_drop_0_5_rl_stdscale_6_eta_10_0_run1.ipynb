{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=10.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 11.8629(30.5415) | Bit/dim 8.6897(8.9521) | Xent 2.2805(2.3001) | Loss 125.7784(126.2681) | Error 0.8000(0.8604) Steps 0(0.00) | Grad Norm 22.8179(29.5382) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.8641(25.7157) | Bit/dim 8.4873(8.8632) | Xent 2.2262(2.2874) | Loss 128.7265(125.8740) | Error 0.7256(0.8329) Steps 0(0.00) | Grad Norm 9.6182(25.5756) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 11.7995(22.1238) | Bit/dim 8.3923(8.7514) | Xent 2.1748(2.2634) | Loss 117.4176(125.3898) | Error 0.7533(0.8096) Steps 0(0.00) | Grad Norm 9.3339(21.0035) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.3575(19.5285) | Bit/dim 8.1927(8.6233) | Xent 2.1127(2.2346) | Loss 130.1598(125.3294) | Error 0.7211(0.7914) Steps 0(0.00) | Grad Norm 5.9723(17.2395) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.0057(17.6148) | Bit/dim 7.9746(8.4738) | Xent 2.1019(2.2020) | Loss 124.6411(124.7783) | Error 0.7033(0.7758) Steps 0(0.00) | Grad Norm 5.7698(14.3179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 73.0487, Epoch Time 784.7807(784.7807), Bit/dim 7.7700(best: inf), Xent 2.0785, Loss 8.8092, Error 0.7001(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.7496(16.2486) | Bit/dim 7.6553(8.2923) | Xent 2.0891(2.1718) | Loss 124.1213(183.3286) | Error 0.7011(0.7583) Steps 0(0.00) | Grad Norm 5.3232(12.0654) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 12.7140(15.3648) | Bit/dim 7.3659(8.0808) | Xent 2.0761(2.1453) | Loss 120.6950(167.7363) | Error 0.6978(0.7414) Steps 0(0.00) | Grad Norm 4.5480(10.1989) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.4228(14.7655) | Bit/dim 7.1815(7.8642) | Xent 2.0835(2.1249) | Loss 126.1883(156.9455) | Error 0.6789(0.7261) Steps 0(0.00) | Grad Norm 3.3313(8.5216) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 12.9619(14.2762) | Bit/dim 7.0762(7.6684) | Xent 2.0837(2.1141) | Loss 128.5069(148.6739) | Error 0.6956(0.7191) Steps 0(0.00) | Grad Norm 2.5604(6.9989) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.7554(14.0124) | Bit/dim 7.0198(7.5058) | Xent 2.0803(2.1032) | Loss 125.6922(142.7907) | Error 0.7033(0.7151) Steps 0(0.00) | Grad Norm 2.7457(5.7679) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.2987(13.8518) | Bit/dim 6.9997(7.3757) | Xent 2.0671(2.0947) | Loss 128.9774(138.9345) | Error 0.7256(0.7145) Steps 0(0.00) | Grad Norm 6.8200(5.2320) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 72.2366, Epoch Time 809.2365(785.5144), Bit/dim 6.9918(best: 7.7700), Xent 2.0575, Loss 8.0206, Error 0.6948(best: 0.7001)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.1299(13.8235) | Bit/dim 6.9381(7.2706) | Xent 2.0491(2.0853) | Loss 130.1805(188.0810) | Error 0.6867(0.7119) Steps 0(0.00) | Grad Norm 3.9334(5.1355) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.6984(13.8419) | Bit/dim 6.9078(7.1832) | Xent 2.0644(2.0761) | Loss 127.5318(172.2276) | Error 0.7233(0.7090) Steps 0(0.00) | Grad Norm 5.1807(4.7357) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 14.0797(13.8409) | Bit/dim 6.8746(7.1069) | Xent 2.0540(2.0703) | Loss 128.0321(160.0738) | Error 0.6933(0.7056) Steps 0(0.00) | Grad Norm 15.0398(5.4446) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 13.8219(13.8010) | Bit/dim 6.8470(7.0376) | Xent 1.9914(2.0587) | Loss 132.9414(151.4872) | Error 0.6733(0.7001) Steps 0(0.00) | Grad Norm 2.2130(5.6154) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.3998(13.9049) | Bit/dim 6.7210(6.9634) | Xent 1.9775(2.0481) | Loss 130.7402(145.0085) | Error 0.6878(0.6963) Steps 0(0.00) | Grad Norm 15.9098(6.6722) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 74.2978, Epoch Time 855.4583(787.6127), Bit/dim 6.6429(best: 6.9918), Xent 2.0390, Loss 7.6624, Error 0.7056(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 13.7654(13.9084) | Bit/dim 6.5815(6.8814) | Xent 2.0167(2.0447) | Loss 125.1290(202.0964) | Error 0.6944(0.6985) Steps 0(0.00) | Grad Norm 14.1234(12.8929) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.2035(13.8591) | Bit/dim 6.4620(6.7808) | Xent 2.2515(2.0511) | Loss 128.0034(182.4673) | Error 0.8111(0.7056) Steps 0(0.00) | Grad Norm 123.8082(25.4560) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.7792(13.8866) | Bit/dim 6.2748(6.6663) | Xent 2.0740(2.0673) | Loss 128.5590(168.5891) | Error 0.7544(0.7192) Steps 0(0.00) | Grad Norm 43.6222(37.4686) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 12.5352(13.7478) | Bit/dim 6.1371(6.5375) | Xent 2.2015(2.0633) | Loss 122.2735(157.2768) | Error 0.7711(0.7152) Steps 0(0.00) | Grad Norm 102.7725(39.2453) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.8068(13.6874) | Bit/dim 5.9541(6.4092) | Xent 2.0596(2.0717) | Loss 121.9696(148.9570) | Error 0.7322(0.7188) Steps 0(0.00) | Grad Norm 54.2380(48.0898) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.6113(13.6547) | Bit/dim 5.8466(6.2731) | Xent 2.0206(2.0658) | Loss 123.9972(142.8700) | Error 0.6722(0.7164) Steps 0(0.00) | Grad Norm 26.0512(46.6835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 75.1313, Epoch Time 844.8377(789.3295), Bit/dim 5.8431(best: 6.6429), Xent 2.0309, Loss 6.8585, Error 0.7062(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.0756(13.6285) | Bit/dim 5.8060(6.1483) | Xent 1.9636(2.0504) | Loss 120.8875(190.6025) | Error 0.6478(0.7074) Steps 0(0.00) | Grad Norm 18.8486(42.6104) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.2836(13.6320) | Bit/dim 5.7060(6.0444) | Xent 1.9644(2.0416) | Loss 127.3632(174.0613) | Error 0.6200(0.7052) Steps 0(0.00) | Grad Norm 11.8714(46.3972) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.4671(13.5503) | Bit/dim 5.6552(5.9525) | Xent 1.9251(2.0255) | Loss 125.5950(160.5885) | Error 0.6400(0.6969) Steps 0(0.00) | Grad Norm 7.6323(40.3042) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.4464(13.5743) | Bit/dim 5.6934(5.8763) | Xent 2.0754(2.0193) | Loss 116.7183(150.9988) | Error 0.7167(0.6950) Steps 0(0.00) | Grad Norm 81.6514(42.6286) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.3718(13.5920) | Bit/dim 5.6214(5.8145) | Xent 1.9743(2.0098) | Loss 127.0146(144.1620) | Error 0.6633(0.6894) Steps 0(0.00) | Grad Norm 18.1477(38.4505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 74.5437, Epoch Time 836.7767(790.7529), Bit/dim 5.6188(best: 5.8431), Xent 1.9382, Loss 6.5880, Error 0.6546(best: 0.6948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.4310(13.4964) | Bit/dim 5.6924(5.7663) | Xent 2.1793(2.0047) | Loss 126.9841(201.9901) | Error 0.7956(0.6873) Steps 0(0.00) | Grad Norm 182.8263(42.7832) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 12.8404(13.4263) | Bit/dim 5.6154(5.7426) | Xent 1.9423(2.0139) | Loss 123.8956(181.3285) | Error 0.6811(0.6921) Steps 0(0.00) | Grad Norm 21.8442(46.7427) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.6031(13.3759) | Bit/dim 5.5960(5.7019) | Xent 1.9729(2.0083) | Loss 119.1565(166.3496) | Error 0.6844(0.6913) Steps 0(0.00) | Grad Norm 19.8541(40.9116) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.5249(13.3724) | Bit/dim 5.5584(5.6638) | Xent 1.9611(1.9999) | Loss 124.9117(155.4068) | Error 0.6678(0.6898) Steps 0(0.00) | Grad Norm 20.3820(37.0564) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.1506(13.3670) | Bit/dim 5.5176(5.6275) | Xent 1.9559(1.9909) | Loss 120.6069(146.6587) | Error 0.6867(0.6857) Steps 0(0.00) | Grad Norm 13.9229(31.4018) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.3090(13.3760) | Bit/dim 5.5211(5.5936) | Xent 1.9373(1.9887) | Loss 127.3016(140.7353) | Error 0.6478(0.6848) Steps 0(0.00) | Grad Norm 32.4990(31.6777) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 74.4746, Epoch Time 825.2740(791.7885), Bit/dim 5.4763(best: 5.6188), Xent 2.0409, Loss 6.4967, Error 0.7386(best: 0.6546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.3299(13.3447) | Bit/dim 5.4277(5.5582) | Xent 1.9546(1.9921) | Loss 126.5720(189.0548) | Error 0.6744(0.6893) Steps 0(0.00) | Grad Norm 7.3624(31.3344) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.1199(13.3874) | Bit/dim 5.3675(5.5233) | Xent 1.9420(1.9811) | Loss 125.7253(172.3491) | Error 0.6544(0.6861) Steps 0(0.00) | Grad Norm 16.7129(27.2576) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.1650(13.4108) | Bit/dim 5.3410(5.4817) | Xent 1.8955(1.9614) | Loss 129.9817(160.4113) | Error 0.6344(0.6782) Steps 0(0.00) | Grad Norm 10.1112(25.0464) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.7828(13.5511) | Bit/dim 5.3312(5.4481) | Xent 1.9283(1.9473) | Loss 129.5349(152.1656) | Error 0.6733(0.6735) Steps 0(0.00) | Grad Norm 25.6287(26.4746) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 14.2029(13.7089) | Bit/dim 5.3377(5.4101) | Xent 1.8329(1.9318) | Loss 128.4396(146.2351) | Error 0.6178(0.6677) Steps 0(0.00) | Grad Norm 11.9691(25.1290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 77.1688, Epoch Time 849.8913(793.5316), Bit/dim 5.2648(best: 5.4763), Xent 1.8942, Loss 6.2119, Error 0.6523(best: 0.6546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.3819(13.8158) | Bit/dim 5.3252(5.3742) | Xent 1.9341(1.9185) | Loss 128.2931(207.8490) | Error 0.6878(0.6647) Steps 0(0.00) | Grad Norm 50.0414(26.8778) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.1090(13.8313) | Bit/dim 5.5363(5.3726) | Xent 2.2307(1.9614) | Loss 125.0574(186.8654) | Error 0.7800(0.6842) Steps 0(0.00) | Grad Norm 39.5886(35.7146) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.7802(13.9070) | Bit/dim 5.2972(5.3628) | Xent 2.0929(2.0002) | Loss 124.1091(171.0819) | Error 0.7556(0.7021) Steps 0(0.00) | Grad Norm 54.0321(38.1490) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.8293(13.8964) | Bit/dim 5.1810(5.3317) | Xent 2.0246(2.0055) | Loss 125.6054(158.6179) | Error 0.6944(0.7052) Steps 0(0.00) | Grad Norm 7.4990(33.5634) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.2481(13.9408) | Bit/dim 5.1485(5.2905) | Xent 1.9735(2.0024) | Loss 123.6448(149.4260) | Error 0.6956(0.7037) Steps 0(0.00) | Grad Norm 14.1242(27.1830) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.4314(13.9861) | Bit/dim 5.1357(5.2523) | Xent 1.9229(1.9902) | Loss 125.2801(142.4090) | Error 0.6644(0.6980) Steps 0(0.00) | Grad Norm 5.9339(21.6060) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 75.2426, Epoch Time 863.8171(795.6402), Bit/dim 5.1227(best: 5.2648), Xent 1.9132, Loss 6.0793, Error 0.6467(best: 0.6523)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.4693(14.0247) | Bit/dim 5.1161(5.2191) | Xent 1.9465(1.9757) | Loss 126.1115(190.2598) | Error 0.6756(0.6909) Steps 0(0.00) | Grad Norm 54.5387(21.6503) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.5294(14.1454) | Bit/dim 5.0880(5.1884) | Xent 1.9013(1.9654) | Loss 122.0787(173.3824) | Error 0.6567(0.6879) Steps 0(0.00) | Grad Norm 28.1237(23.4889) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.5326(14.2918) | Bit/dim 5.0791(5.1545) | Xent 1.9411(1.9519) | Loss 125.4128(160.3376) | Error 0.6700(0.6835) Steps 0(0.00) | Grad Norm 13.7195(21.6427) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 15.3839(14.3967) | Bit/dim 5.0650(5.1252) | Xent 1.9474(1.9397) | Loss 124.9851(150.7796) | Error 0.6767(0.6785) Steps 0(0.00) | Grad Norm 41.7892(23.5181) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 14.0841(14.4553) | Bit/dim 5.0184(5.0969) | Xent 1.9136(1.9290) | Loss 123.9731(143.8721) | Error 0.6500(0.6728) Steps 0(0.00) | Grad Norm 27.3641(25.4363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 80.0934, Epoch Time 900.1118(798.7743), Bit/dim 4.9743(best: 5.1227), Xent 1.8556, Loss 5.9021, Error 0.6394(best: 0.6467)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.0870(14.5869) | Bit/dim 4.9460(5.0663) | Xent 1.8358(1.9182) | Loss 125.0671(201.7413) | Error 0.6389(0.6690) Steps 0(0.00) | Grad Norm 29.6982(24.2692) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.2481(14.6880) | Bit/dim 4.9775(5.0391) | Xent 1.8523(1.9062) | Loss 122.0554(181.5242) | Error 0.6011(0.6649) Steps 0(0.00) | Grad Norm 44.1193(26.1574) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.5760(14.7301) | Bit/dim 4.9154(5.0163) | Xent 1.8555(1.9035) | Loss 124.1880(166.7519) | Error 0.6656(0.6639) Steps 0(0.00) | Grad Norm 11.4501(28.4327) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.2022(14.7730) | Bit/dim 4.8927(4.9881) | Xent 1.8246(1.8932) | Loss 119.1045(155.9273) | Error 0.6511(0.6612) Steps 0(0.00) | Grad Norm 8.5783(24.5235) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 14.9438(14.8668) | Bit/dim 4.8955(4.9680) | Xent 2.0629(1.8926) | Loss 124.9289(147.9233) | Error 0.7333(0.6615) Steps 0(0.00) | Grad Norm 71.9302(30.0822) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 14.9961(14.8196) | Bit/dim 4.8299(4.9485) | Xent 1.8694(1.8955) | Loss 120.4076(141.7064) | Error 0.6678(0.6639) Steps 0(0.00) | Grad Norm 27.7276(31.6740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 78.9828, Epoch Time 916.3051(802.3003), Bit/dim 4.8491(best: 4.9743), Xent 1.8279, Loss 5.7630, Error 0.6408(best: 0.6394)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 14.1787(14.7776) | Bit/dim 4.8525(4.9220) | Xent 1.7986(1.8774) | Loss 123.8923(191.0852) | Error 0.6489(0.6580) Steps 0(0.00) | Grad Norm 17.4315(27.6960) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.6117(14.8623) | Bit/dim 4.7837(4.8990) | Xent 1.8719(1.8647) | Loss 127.5708(173.9904) | Error 0.6500(0.6555) Steps 0(0.00) | Grad Norm 18.8065(28.9387) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 15.7388(14.9335) | Bit/dim 4.8571(4.8835) | Xent 1.8979(1.8561) | Loss 126.1454(161.0486) | Error 0.6656(0.6531) Steps 0(0.00) | Grad Norm 60.3351(31.5512) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.5983(15.0717) | Bit/dim 4.7391(4.8642) | Xent 1.8465(1.8503) | Loss 124.4330(151.5445) | Error 0.6611(0.6517) Steps 0(0.00) | Grad Norm 27.9505(33.0012) | Total Time 0.00(0.00)\n",
      "Epoch 0011 | Time 78.8135, Epoch Time 928.0799(806.0736), Bit/dim 4.7567(best: 4.8491), Xent 1.7186, Loss 5.6160, Error 0.5950(best: 0.6394)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.5697(15.1200) | Bit/dim 4.8126(4.8228) | Xent 1.7919(1.8213) | Loss 130.4933(201.9854) | Error 0.6278(0.6440) Steps 0(0.00) | Grad Norm 49.2211(33.7484) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.8729(15.2165) | Bit/dim 4.6851(4.8060) | Xent 1.8299(1.8279) | Loss 127.5135(182.0559) | Error 0.6656(0.6470) Steps 0(0.00) | Grad Norm 23.7144(34.6338) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 15.4330(15.3759) | Bit/dim 4.7497(4.7914) | Xent 1.7246(1.8159) | Loss 125.9250(167.5305) | Error 0.6100(0.6429) Steps 0(0.00) | Grad Norm 36.2711(33.4493) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 15.3575(15.3547) | Bit/dim 4.7509(4.7792) | Xent 1.8408(1.8069) | Loss 131.0880(156.5804) | Error 0.6533(0.6405) Steps 0(0.00) | Grad Norm 46.4967(33.3054) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.9892(15.4463) | Bit/dim 4.6872(4.7589) | Xent 1.7364(1.7982) | Loss 124.5841(148.4168) | Error 0.6167(0.6381) Steps 0(0.00) | Grad Norm 18.1517(32.0504) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 16.1573(15.5645) | Bit/dim 4.7892(4.7593) | Xent 2.1385(1.8107) | Loss 131.2596(143.1653) | Error 0.7633(0.6431) Steps 0(0.00) | Grad Norm 78.2395(36.4569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 82.1177, Epoch Time 958.2395(810.6386), Bit/dim 4.7888(best: 4.7567), Xent 1.8305, Loss 5.7040, Error 0.6492(best: 0.5950)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 16.4295(15.6016) | Bit/dim 4.7504(4.7551) | Xent 1.7746(1.8191) | Loss 129.1967(194.8800) | Error 0.6344(0.6460) Steps 0(0.00) | Grad Norm 19.7134(34.7836) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.2034(15.6728) | Bit/dim 4.7050(4.7434) | Xent 1.8026(1.8094) | Loss 120.2422(176.7638) | Error 0.6211(0.6413) Steps 0(0.00) | Grad Norm 29.0151(33.5013) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.5390(15.6270) | Bit/dim 4.6341(4.7225) | Xent 1.7550(1.7993) | Loss 123.0040(163.3793) | Error 0.6378(0.6384) Steps 0(0.00) | Grad Norm 10.1311(29.8557) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.8935(15.7171) | Bit/dim 4.6531(4.7028) | Xent 1.7160(1.7776) | Loss 131.7303(153.6930) | Error 0.6078(0.6306) Steps 0(0.00) | Grad Norm 11.4972(25.3932) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.1791(15.6465) | Bit/dim 4.6051(4.6816) | Xent 1.7021(1.7535) | Loss 130.6621(146.5301) | Error 0.6133(0.6235) Steps 0(0.00) | Grad Norm 24.5931(22.4919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 83.9247, Epoch Time 964.4951(815.2543), Bit/dim 4.7183(best: 4.7567), Xent 2.0464, Loss 5.7415, Error 0.7134(best: 0.5950)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.4693(15.6602) | Bit/dim 4.5973(4.6783) | Xent 1.8716(1.7884) | Loss 134.6734(208.4273) | Error 0.6844(0.6346) Steps 0(0.00) | Grad Norm 40.0390(30.7816) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 15.6758(15.7708) | Bit/dim 4.6464(4.6684) | Xent 1.7238(1.7819) | Loss 129.3042(187.0227) | Error 0.6211(0.6342) Steps 0(0.00) | Grad Norm 16.6339(27.8905) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.5880(15.7360) | Bit/dim 4.6141(4.6548) | Xent 1.7342(1.7751) | Loss 126.6724(171.1259) | Error 0.6200(0.6303) Steps 0(0.00) | Grad Norm 19.6764(27.0236) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 15.9018(15.7537) | Bit/dim 4.5782(4.6410) | Xent 1.7429(1.7575) | Loss 126.1917(159.6513) | Error 0.6356(0.6252) Steps 0(0.00) | Grad Norm 24.3457(25.5634) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 15.7589(15.7173) | Bit/dim 4.5520(4.6209) | Xent 1.6787(1.7395) | Loss 132.2230(151.5788) | Error 0.6044(0.6204) Steps 0(0.00) | Grad Norm 14.4323(22.9312) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.3853(15.7065) | Bit/dim 4.6131(4.6146) | Xent 1.7063(1.7371) | Loss 129.5668(145.7846) | Error 0.6189(0.6197) Steps 0(0.00) | Grad Norm 23.3572(25.5752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 83.9598, Epoch Time 969.2297(819.8736), Bit/dim 4.6906(best: 4.7183), Xent 1.6577, Loss 5.5194, Error 0.5921(best: 0.5950)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 16.2423(15.7286) | Bit/dim 4.5310(4.6105) | Xent 1.6507(1.7290) | Loss 124.5387(198.5648) | Error 0.5922(0.6174) Steps 0(0.00) | Grad Norm 4.7290(26.2567) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.4591(15.7221) | Bit/dim 4.5672(4.5951) | Xent 1.6871(1.7215) | Loss 128.8348(179.9595) | Error 0.5878(0.6143) Steps 0(0.00) | Grad Norm 47.2032(24.5124) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 15.2253(15.7454) | Bit/dim 4.5574(4.5816) | Xent 1.6775(1.7153) | Loss 128.3479(166.6960) | Error 0.6122(0.6115) Steps 0(0.00) | Grad Norm 23.9428(25.5361) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.7898(15.7565) | Bit/dim 4.5032(4.5657) | Xent 1.6235(1.7057) | Loss 128.0475(156.9321) | Error 0.5733(0.6079) Steps 0(0.00) | Grad Norm 7.8786(24.9285) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 15.2719(15.7480) | Bit/dim 4.5366(4.5534) | Xent 1.6337(1.6913) | Loss 130.9821(150.0366) | Error 0.6100(0.6038) Steps 0(0.00) | Grad Norm 19.7385(22.9771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 85.3960, Epoch Time 969.7011(824.3684), Bit/dim 4.4833(best: 4.6906), Xent 1.5745, Loss 5.2706, Error 0.5660(best: 0.5921)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 15.8261(15.7210) | Bit/dim 4.5069(4.5357) | Xent 1.6671(1.6750) | Loss 127.0300(212.9186) | Error 0.5911(0.5998) Steps 0(0.00) | Grad Norm 30.2818(22.0133) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.1838(15.8370) | Bit/dim 4.6528(4.5248) | Xent 1.9681(1.6707) | Loss 136.3096(191.6528) | Error 0.6844(0.5979) Steps 0(0.00) | Grad Norm 74.2035(22.7896) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.9443(15.8889) | Bit/dim 4.4459(4.5153) | Xent 1.6660(1.6951) | Loss 131.6341(175.3478) | Error 0.5844(0.6067) Steps 0(0.00) | Grad Norm 8.5563(24.5942) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 15.8575(15.8127) | Bit/dim 4.4397(4.5025) | Xent 1.5944(1.6861) | Loss 128.8984(163.0055) | Error 0.5611(0.6029) Steps 0(0.00) | Grad Norm 12.8587(22.3187) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.2002(15.7183) | Bit/dim 4.4170(4.4882) | Xent 1.6571(1.6706) | Loss 134.3605(154.4579) | Error 0.5856(0.5983) Steps 0(0.00) | Grad Norm 16.9223(19.0540) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.4382(15.7483) | Bit/dim 4.4816(4.4825) | Xent 1.6548(1.6739) | Loss 128.9248(147.9346) | Error 0.6122(0.6001) Steps 0(0.00) | Grad Norm 28.4073(22.2374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 84.3211, Epoch Time 972.2560(828.8050), Bit/dim 4.4407(best: 4.4833), Xent 1.6435, Loss 5.2625, Error 0.5932(best: 0.5660)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 15.9031(15.7841) | Bit/dim 4.4518(4.4701) | Xent 1.6035(1.6660) | Loss 136.2047(201.1199) | Error 0.5756(0.5979) Steps 0(0.00) | Grad Norm 11.3160(22.1246) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 15.7792(15.7892) | Bit/dim 4.3921(4.4557) | Xent 1.6370(1.6520) | Loss 130.1890(182.4356) | Error 0.5922(0.5935) Steps 0(0.00) | Grad Norm 14.7505(20.7692) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 16.2484(15.8639) | Bit/dim 4.4411(4.4458) | Xent 1.6651(1.6442) | Loss 128.1650(168.9522) | Error 0.5944(0.5911) Steps 0(0.00) | Grad Norm 33.9619(21.4776) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 15.8204(15.9009) | Bit/dim 4.4638(4.4488) | Xent 1.6242(1.6530) | Loss 129.2919(158.5720) | Error 0.6033(0.5943) Steps 0(0.00) | Grad Norm 20.4764(24.1058) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 15.6617(15.8595) | Bit/dim 4.4345(4.4384) | Xent 1.6127(1.6542) | Loss 131.1139(150.6082) | Error 0.5822(0.5957) Steps 0(0.00) | Grad Norm 16.3388(22.5747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 84.3569, Epoch Time 976.1491(833.2253), Bit/dim 4.3780(best: 4.4407), Xent 1.5435, Loss 5.1498, Error 0.5568(best: 0.5660)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 15.8332(15.8212) | Bit/dim 4.3886(4.4205) | Xent 1.5502(1.6407) | Loss 127.6696(210.5825) | Error 0.5544(0.5907) Steps 0(0.00) | Grad Norm 14.8251(20.8441) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.1191(15.8310) | Bit/dim 4.3699(4.4048) | Xent 1.5567(1.6216) | Loss 130.5881(189.2043) | Error 0.5667(0.5850) Steps 0(0.00) | Grad Norm 13.0069(18.1677) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 15.0669(15.8021) | Bit/dim 4.3632(4.3911) | Xent 1.5842(1.6100) | Loss 127.0758(173.6878) | Error 0.5544(0.5805) Steps 0(0.00) | Grad Norm 15.2502(17.9540) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.0336(15.8345) | Bit/dim 4.3057(4.3711) | Xent 1.5982(1.6024) | Loss 122.6019(162.0795) | Error 0.5744(0.5801) Steps 0(0.00) | Grad Norm 9.5579(16.2082) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.0662(15.8958) | Bit/dim 4.3378(4.3627) | Xent 1.5751(1.5929) | Loss 130.5336(153.5214) | Error 0.5489(0.5756) Steps 0(0.00) | Grad Norm 18.3547(17.3220) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 15.5023(15.9231) | Bit/dim 4.3363(4.3543) | Xent 1.6297(1.5994) | Loss 126.7906(146.6735) | Error 0.5822(0.5779) Steps 0(0.00) | Grad Norm 19.8542(19.1128) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 83.9882, Epoch Time 976.8024(837.5327), Bit/dim 4.3977(best: 4.3780), Xent 1.5194, Loss 5.1574, Error 0.5439(best: 0.5568)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 15.7922(15.9920) | Bit/dim 4.3072(4.3484) | Xent 1.6092(1.5991) | Loss 127.6541(196.5270) | Error 0.5856(0.5756) Steps 0(0.00) | Grad Norm 17.0451(20.8449) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.1986(16.0003) | Bit/dim 4.3242(4.3344) | Xent 1.5418(1.5831) | Loss 127.7512(177.8311) | Error 0.5689(0.5720) Steps 0(0.00) | Grad Norm 19.9167(19.0041) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 15.2494(15.9153) | Bit/dim 4.2504(4.3229) | Xent 1.5248(1.5711) | Loss 123.2955(164.5316) | Error 0.5656(0.5686) Steps 0(0.00) | Grad Norm 14.1803(17.5879) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 15.1339(15.9023) | Bit/dim 4.3590(4.3161) | Xent 1.6448(1.5654) | Loss 123.5431(154.4917) | Error 0.5722(0.5654) Steps 0(0.00) | Grad Norm 45.4843(18.0530) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.6792(16.0300) | Bit/dim 4.3031(4.3137) | Xent 1.5991(1.5838) | Loss 127.3464(146.8856) | Error 0.5989(0.5735) Steps 0(0.00) | Grad Norm 11.1656(19.2107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 85.1444, Epoch Time 985.5139(841.9721), Bit/dim 4.2832(best: 4.3780), Xent 1.4663, Loss 5.0163, Error 0.5264(best: 0.5439)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 15.9970(16.0567) | Bit/dim 4.2432(4.3032) | Xent 1.5318(1.5702) | Loss 129.1424(209.5164) | Error 0.5611(0.5703) Steps 0(0.00) | Grad Norm 11.4710(17.6693) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 15.2274(15.9739) | Bit/dim 4.2476(4.2908) | Xent 1.4749(1.5506) | Loss 125.1927(187.4174) | Error 0.5578(0.5627) Steps 0(0.00) | Grad Norm 15.0993(16.4229) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.0918(16.0410) | Bit/dim 4.2008(4.2746) | Xent 1.5364(1.5428) | Loss 126.1860(171.4489) | Error 0.5589(0.5601) Steps 0(0.00) | Grad Norm 15.0070(16.0402) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 15.5547(15.9729) | Bit/dim 4.2591(4.2691) | Xent 1.5165(1.5411) | Loss 124.8733(159.4149) | Error 0.5589(0.5604) Steps 0(0.00) | Grad Norm 10.3871(16.3241) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 16.4367(15.9380) | Bit/dim 4.2249(4.2770) | Xent 1.5842(1.5586) | Loss 128.4572(151.3818) | Error 0.5722(0.5654) Steps 0(0.00) | Grad Norm 17.3001(18.2390) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.0317(16.0697) | Bit/dim 4.1987(4.2669) | Xent 1.4519(1.5471) | Loss 121.7665(145.1173) | Error 0.5278(0.5623) Steps 0(0.00) | Grad Norm 7.1569(16.4622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 83.2968, Epoch Time 982.7896(846.1966), Bit/dim 4.2270(best: 4.2832), Xent 1.4649, Loss 4.9595, Error 0.5317(best: 0.5264)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.4973(16.0309) | Bit/dim 4.1912(4.2511) | Xent 1.4970(1.5312) | Loss 128.0601(195.5126) | Error 0.5311(0.5544) Steps 0(0.00) | Grad Norm 6.0343(14.7411) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.7371(15.9147) | Bit/dim 4.2232(4.2402) | Xent 1.5410(1.5252) | Loss 121.7238(176.7869) | Error 0.5722(0.5519) Steps 0(0.00) | Grad Norm 23.2693(15.7504) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 15.4637(15.8833) | Bit/dim 4.1612(4.2311) | Xent 1.4801(1.5104) | Loss 130.5756(163.5620) | Error 0.5411(0.5470) Steps 0(0.00) | Grad Norm 21.6741(16.8757) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 15.8071(15.8663) | Bit/dim 4.2055(4.2206) | Xent 1.4363(1.5009) | Loss 126.9538(153.6634) | Error 0.5200(0.5457) Steps 0(0.00) | Grad Norm 9.0142(15.8143) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 15.3173(15.7440) | Bit/dim 4.1498(4.2073) | Xent 1.4276(1.4890) | Loss 124.6246(145.7865) | Error 0.5278(0.5410) Steps 0(0.00) | Grad Norm 9.5951(14.2099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 81.4880, Epoch Time 963.5950(849.7186), Bit/dim 4.1807(best: 4.2270), Xent 1.3725, Loss 4.8670, Error 0.5021(best: 0.5264)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 15.0797(15.6723) | Bit/dim 4.1579(4.1966) | Xent 1.4244(1.4773) | Loss 124.6573(204.6747) | Error 0.5233(0.5372) Steps 0(0.00) | Grad Norm 13.8843(14.2287) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.8738(15.6099) | Bit/dim 4.1464(4.1891) | Xent 1.3856(1.4614) | Loss 120.1993(183.4862) | Error 0.5144(0.5310) Steps 0(0.00) | Grad Norm 8.5971(13.4496) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.5863(15.6349) | Bit/dim 4.2725(4.1938) | Xent 1.6647(1.5044) | Loss 125.4961(168.0449) | Error 0.5844(0.5430) Steps 0(0.00) | Grad Norm 32.6032(18.8690) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.0584(15.8015) | Bit/dim 4.2126(4.1997) | Xent 1.5318(1.5194) | Loss 129.5549(157.8756) | Error 0.5511(0.5478) Steps 0(0.00) | Grad Norm 10.9559(17.8998) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 15.3366(15.7186) | Bit/dim 4.1433(4.1887) | Xent 1.4312(1.5053) | Loss 120.3881(148.4445) | Error 0.5133(0.5441) Steps 0(0.00) | Grad Norm 11.2287(15.9080) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 14.8534(15.5659) | Bit/dim 4.1919(4.1801) | Xent 1.4380(1.4979) | Loss 125.5309(141.2573) | Error 0.5267(0.5424) Steps 0(0.00) | Grad Norm 15.0082(15.8053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 80.1425, Epoch Time 956.3310(852.9169), Bit/dim 4.1459(best: 4.1807), Xent 1.3927, Loss 4.8423, Error 0.5027(best: 0.5021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.7665(15.5033) | Bit/dim 4.1577(4.1685) | Xent 1.4010(1.4753) | Loss 123.1154(190.0979) | Error 0.5111(0.5344) Steps 0(0.00) | Grad Norm 10.8876(14.7762) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.8282(15.4179) | Bit/dim 4.1181(4.1550) | Xent 1.3897(1.4617) | Loss 121.9838(172.2381) | Error 0.5244(0.5316) Steps 0(0.00) | Grad Norm 8.8710(14.1809) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 16.1418(15.4057) | Bit/dim 4.1094(4.1423) | Xent 1.3550(1.4439) | Loss 125.3065(159.0924) | Error 0.4800(0.5232) Steps 0(0.00) | Grad Norm 10.1585(13.1272) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.5196(15.4330) | Bit/dim 4.1317(4.1328) | Xent 1.3639(1.4302) | Loss 119.2069(150.1465) | Error 0.4989(0.5199) Steps 0(0.00) | Grad Norm 6.3350(12.0523) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 14.8457(15.4642) | Bit/dim 4.1061(4.1230) | Xent 1.3937(1.4152) | Loss 124.2427(143.6350) | Error 0.5167(0.5156) Steps 0(0.00) | Grad Norm 14.1277(11.3393) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 80.7513, Epoch Time 947.2500(855.7469), Bit/dim 4.0886(best: 4.1459), Xent 1.3188, Loss 4.7480, Error 0.4787(best: 0.5021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 15.4852(15.4475) | Bit/dim 4.0844(4.1162) | Xent 1.4342(1.4139) | Loss 125.6794(202.2604) | Error 0.5278(0.5159) Steps 0(0.00) | Grad Norm 5.9624(12.7038) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 16.6632(15.5458) | Bit/dim 4.0837(4.1081) | Xent 1.3329(1.4037) | Loss 125.1742(181.7244) | Error 0.4978(0.5138) Steps 0(0.00) | Grad Norm 12.6893(12.2897) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.8011(15.6521) | Bit/dim 4.1032(4.1029) | Xent 1.3276(1.3920) | Loss 122.5334(166.8880) | Error 0.4567(0.5072) Steps 0(0.00) | Grad Norm 5.3433(11.1120) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.7496(15.5969) | Bit/dim 4.1277(4.0984) | Xent 1.3854(1.3930) | Loss 123.1525(154.7066) | Error 0.5189(0.5059) Steps 0(0.00) | Grad Norm 26.8597(12.7407) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 16.1285(15.6415) | Bit/dim 4.0891(4.0915) | Xent 1.4736(1.3905) | Loss 127.1187(146.4223) | Error 0.5322(0.5037) Steps 0(0.00) | Grad Norm 14.5400(13.2280) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 15.5265(15.6374) | Bit/dim 4.0857(4.0886) | Xent 1.3569(1.3877) | Loss 124.2069(140.3289) | Error 0.5089(0.5026) Steps 0(0.00) | Grad Norm 14.4198(14.6894) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 80.0015, Epoch Time 959.7649(858.8675), Bit/dim 4.0691(best: 4.0886), Xent 1.3150, Loss 4.7266, Error 0.4753(best: 0.4787)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.8376(15.5755) | Bit/dim 4.0605(4.0811) | Xent 1.4201(1.3748) | Loss 120.5075(190.1535) | Error 0.5133(0.4993) Steps 0(0.00) | Grad Norm 20.8447(14.0775) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.0811(15.5298) | Bit/dim 4.0513(4.0732) | Xent 1.4276(1.3710) | Loss 124.0592(171.7624) | Error 0.5189(0.4986) Steps 0(0.00) | Grad Norm 22.1964(14.2975) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 15.2685(15.5375) | Bit/dim 4.0708(4.0732) | Xent 1.4367(1.3738) | Loss 124.7436(158.8811) | Error 0.5189(0.4985) Steps 0(0.00) | Grad Norm 23.8265(15.8842) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 16.3966(15.5406) | Bit/dim 4.0445(4.0645) | Xent 1.3189(1.3696) | Loss 119.4883(148.8299) | Error 0.4667(0.4954) Steps 0(0.00) | Grad Norm 8.5499(14.2443) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 15.6944(15.4617) | Bit/dim 4.0223(4.0597) | Xent 1.3497(1.3577) | Loss 121.7311(141.0773) | Error 0.4833(0.4926) Steps 0(0.00) | Grad Norm 9.0592(12.6117) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 80.4066, Epoch Time 947.1311(861.5154), Bit/dim 4.0463(best: 4.0691), Xent 1.2545, Loss 4.6735, Error 0.4539(best: 0.4753)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.0862(15.4448) | Bit/dim 4.0088(4.0509) | Xent 1.2527(1.3510) | Loss 118.4413(199.8332) | Error 0.4556(0.4898) Steps 0(0.00) | Grad Norm 11.7184(12.8221) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 14.7235(15.4282) | Bit/dim 4.0228(4.0481) | Xent 1.2848(1.3468) | Loss 117.7013(178.9545) | Error 0.4600(0.4883) Steps 0(0.00) | Grad Norm 11.6554(13.8557) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 15.8369(15.4487) | Bit/dim 4.0343(4.0462) | Xent 1.3817(1.3524) | Loss 121.1478(163.9790) | Error 0.4856(0.4892) Steps 0(0.00) | Grad Norm 10.3727(14.8116) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 15.2249(15.4315) | Bit/dim 4.0531(4.0413) | Xent 1.3157(1.3488) | Loss 117.2479(152.3685) | Error 0.4911(0.4878) Steps 0(0.00) | Grad Norm 6.2421(13.7945) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 14.6668(15.4080) | Bit/dim 3.9821(4.0359) | Xent 1.2925(1.3423) | Loss 118.0730(144.0188) | Error 0.4711(0.4858) Steps 0(0.00) | Grad Norm 4.8853(13.0119) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 14.7286(15.4174) | Bit/dim 4.0210(4.0292) | Xent 1.3478(1.3442) | Loss 115.0311(137.3982) | Error 0.4856(0.4863) Steps 0(0.00) | Grad Norm 16.1992(13.9202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 81.6585, Epoch Time 947.4434(864.0932), Bit/dim 4.0118(best: 4.0463), Xent 1.2476, Loss 4.6356, Error 0.4595(best: 0.4539)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.2259(15.4452) | Bit/dim 4.0066(4.0234) | Xent 1.2765(1.3301) | Loss 127.7290(188.1011) | Error 0.4633(0.4813) Steps 0(0.00) | Grad Norm 9.7088(12.9401) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 15.8182(15.4544) | Bit/dim 4.0278(4.0170) | Xent 1.3253(1.3251) | Loss 120.3317(170.3869) | Error 0.4867(0.4799) Steps 0(0.00) | Grad Norm 9.0221(12.4610) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 14.3914(15.4046) | Bit/dim 3.9680(4.0113) | Xent 1.2727(1.3184) | Loss 115.9512(156.9427) | Error 0.4600(0.4776) Steps 0(0.00) | Grad Norm 8.6995(12.4329) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 15.1610(15.4472) | Bit/dim 3.9844(4.0076) | Xent 1.3386(1.3140) | Loss 122.4015(147.1792) | Error 0.4889(0.4763) Steps 0(0.00) | Grad Norm 7.7325(11.5802) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 15.2072(15.4367) | Bit/dim 4.0224(4.0037) | Xent 1.3962(1.3113) | Loss 121.1019(139.8519) | Error 0.5044(0.4763) Steps 0(0.00) | Grad Norm 12.4784(11.2956) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 81.0858, Epoch Time 949.1483(866.6449), Bit/dim 3.9918(best: 4.0118), Xent 1.2132, Loss 4.5984, Error 0.4415(best: 0.4539)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.5988(15.4175) | Bit/dim 3.9387(4.0007) | Xent 1.2541(1.3012) | Loss 116.6250(197.4035) | Error 0.4500(0.4716) Steps 0(0.00) | Grad Norm 18.4728(11.1867) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.0220(15.4391) | Bit/dim 3.9883(3.9966) | Xent 1.2706(1.3006) | Loss 119.9311(176.4629) | Error 0.4533(0.4715) Steps 0(0.00) | Grad Norm 8.4627(12.3213) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 16.4785(15.5164) | Bit/dim 4.0171(3.9934) | Xent 1.2079(1.2875) | Loss 124.6496(161.4399) | Error 0.4311(0.4666) Steps 0(0.00) | Grad Norm 8.0664(11.0781) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 15.1500(15.4710) | Bit/dim 3.9818(3.9902) | Xent 1.3290(1.2860) | Loss 119.1766(150.1477) | Error 0.4644(0.4654) Steps 0(0.00) | Grad Norm 9.5361(11.3343) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.1890(15.4557) | Bit/dim 3.9667(3.9855) | Xent 1.2932(1.2840) | Loss 120.0679(141.2127) | Error 0.4700(0.4646) Steps 0(0.00) | Grad Norm 10.1286(10.8228) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.4327(15.4401) | Bit/dim 3.9429(3.9788) | Xent 1.2626(1.2826) | Loss 111.1461(134.8140) | Error 0.4589(0.4635) Steps 0(0.00) | Grad Norm 5.3890(10.7581) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 79.9355, Epoch Time 948.4412(869.0988), Bit/dim 3.9627(best: 3.9918), Xent 1.2133, Loss 4.5693, Error 0.4398(best: 0.4415)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 14.5373(15.3947) | Bit/dim 3.9893(3.9777) | Xent 1.2561(1.2791) | Loss 122.0538(183.4762) | Error 0.4622(0.4626) Steps 0(0.00) | Grad Norm 18.8173(12.1378) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.5464(15.4230) | Bit/dim 3.9417(3.9744) | Xent 1.3141(1.2725) | Loss 113.8454(166.6122) | Error 0.4589(0.4587) Steps 0(0.00) | Grad Norm 21.9539(12.2891) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 14.9360(15.3481) | Bit/dim 3.9784(3.9688) | Xent 1.2466(1.2697) | Loss 112.8132(153.7226) | Error 0.4667(0.4579) Steps 0(0.00) | Grad Norm 21.0234(12.5432) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 15.7676(15.3712) | Bit/dim 3.9472(3.9651) | Xent 1.3188(1.2762) | Loss 124.4147(144.2930) | Error 0.4811(0.4616) Steps 0(0.00) | Grad Norm 12.4128(12.7617) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 14.6706(15.2542) | Bit/dim 3.9631(3.9618) | Xent 1.2318(1.2731) | Loss 111.8612(136.9177) | Error 0.4500(0.4607) Steps 0(0.00) | Grad Norm 8.7013(11.4984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 78.6215, Epoch Time 935.6250(871.0946), Bit/dim 3.9484(best: 3.9627), Xent 1.1781, Loss 4.5374, Error 0.4269(best: 0.4398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.6364(15.2326) | Bit/dim 3.9616(3.9604) | Xent 1.2504(1.2660) | Loss 117.9416(191.8047) | Error 0.4689(0.4599) Steps 0(0.00) | Grad Norm 9.0376(12.0983) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.5188(15.1934) | Bit/dim 3.9571(3.9559) | Xent 1.2493(1.2619) | Loss 120.1072(171.9593) | Error 0.4311(0.4559) Steps 0(0.00) | Grad Norm 16.8060(12.3639) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.6965(15.1632) | Bit/dim 3.9629(3.9517) | Xent 1.1802(1.2550) | Loss 114.3186(157.5476) | Error 0.4333(0.4528) Steps 0(0.00) | Grad Norm 18.5708(12.3919) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.1930(15.1150) | Bit/dim 3.9223(3.9470) | Xent 1.3506(1.2589) | Loss 118.2419(146.6948) | Error 0.4978(0.4541) Steps 0(0.00) | Grad Norm 24.8696(13.8836) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.4255(15.0537) | Bit/dim 3.9524(3.9487) | Xent 1.2442(1.2566) | Loss 115.2255(138.1020) | Error 0.4411(0.4527) Steps 0(0.00) | Grad Norm 15.1832(13.6429) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.2339(15.0404) | Bit/dim 3.9426(3.9425) | Xent 1.2712(1.2537) | Loss 117.1255(132.1681) | Error 0.4489(0.4530) Steps 0(0.00) | Grad Norm 13.1190(12.7084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 78.6905, Epoch Time 923.1123(872.6551), Bit/dim 3.9325(best: 3.9484), Xent 1.1680, Loss 4.5165, Error 0.4218(best: 0.4269)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 15.8351(15.0841) | Bit/dim 3.9393(3.9402) | Xent 1.1809(1.2416) | Loss 115.9475(178.1200) | Error 0.4467(0.4502) Steps 0(0.00) | Grad Norm 11.0726(12.1597) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 15.6021(15.0770) | Bit/dim 3.9504(3.9390) | Xent 1.2957(1.2350) | Loss 115.0532(161.4579) | Error 0.4522(0.4462) Steps 0(0.00) | Grad Norm 26.8883(13.6044) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.9368(15.0556) | Bit/dim 3.9185(3.9383) | Xent 1.1812(1.2339) | Loss 114.1155(149.0815) | Error 0.4233(0.4459) Steps 0(0.00) | Grad Norm 8.2580(14.1661) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.2286(15.1124) | Bit/dim 3.9254(3.9354) | Xent 1.2265(1.2384) | Loss 111.0889(140.1772) | Error 0.4456(0.4476) Steps 0(0.00) | Grad Norm 5.7310(13.2713) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 15.2012(15.1520) | Bit/dim 3.9534(3.9303) | Xent 1.2401(1.2344) | Loss 116.2503(133.3458) | Error 0.4589(0.4467) Steps 0(0.00) | Grad Norm 11.6849(12.4862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 79.2161, Epoch Time 931.4748(874.4197), Bit/dim 3.9237(best: 3.9325), Xent 1.1463, Loss 4.4968, Error 0.4082(best: 0.4218)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.1753(15.1686) | Bit/dim 3.9375(3.9268) | Xent 1.2032(1.2286) | Loss 117.3661(190.7660) | Error 0.4267(0.4433) Steps 0(0.00) | Grad Norm 14.7641(11.9722) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 15.6015(15.1965) | Bit/dim 3.9460(3.9236) | Xent 1.1597(1.2184) | Loss 115.3059(170.6739) | Error 0.4267(0.4403) Steps 0(0.00) | Grad Norm 13.9596(11.7496) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 14.9337(15.2162) | Bit/dim 3.9065(3.9190) | Xent 1.1820(1.2106) | Loss 114.1585(156.1187) | Error 0.4167(0.4386) Steps 0(0.00) | Grad Norm 9.6942(11.4630) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.7190(15.2708) | Bit/dim 3.8972(3.9168) | Xent 1.2869(1.2124) | Loss 117.3820(145.4471) | Error 0.4700(0.4381) Steps 0(0.00) | Grad Norm 30.1191(12.7765) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.3240(15.2134) | Bit/dim 3.9120(3.9163) | Xent 1.1744(1.2129) | Loss 111.6545(137.4842) | Error 0.4156(0.4381) Steps 0(0.00) | Grad Norm 11.2496(12.8035) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.4177(15.1520) | Bit/dim 3.9125(3.9150) | Xent 1.2020(1.2065) | Loss 112.9568(131.0643) | Error 0.4289(0.4352) Steps 0(0.00) | Grad Norm 9.9267(12.3855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 77.6811, Epoch Time 931.8476(876.1425), Bit/dim 3.9052(best: 3.9237), Xent 1.1569, Loss 4.4836, Error 0.4116(best: 0.4082)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 15.0210(15.0871) | Bit/dim 3.9331(3.9173) | Xent 1.1771(1.1973) | Loss 109.2711(177.5933) | Error 0.4156(0.4315) Steps 0(0.00) | Grad Norm 10.8718(12.3633) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.7534(15.0954) | Bit/dim 3.8812(3.9102) | Xent 1.2125(1.1868) | Loss 114.8104(160.9167) | Error 0.4344(0.4280) Steps 0(0.00) | Grad Norm 8.5285(11.3280) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 16.1215(15.1874) | Bit/dim 3.8654(3.9049) | Xent 1.2211(1.1816) | Loss 115.5036(148.6280) | Error 0.4522(0.4257) Steps 0(0.00) | Grad Norm 11.3452(11.3937) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.6813(15.2022) | Bit/dim 3.8642(3.9027) | Xent 1.1429(1.1771) | Loss 115.6386(139.5186) | Error 0.4178(0.4259) Steps 0(0.00) | Grad Norm 10.9539(11.6673) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.6496(15.1643) | Bit/dim 3.9060(3.9008) | Xent 1.1976(1.1801) | Loss 113.4564(132.8341) | Error 0.4367(0.4260) Steps 0(0.00) | Grad Norm 15.9081(11.6538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 77.4977, Epoch Time 929.1190(877.7318), Bit/dim 3.9024(best: 3.9052), Xent 1.1721, Loss 4.4885, Error 0.4208(best: 0.4082)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 14.8546(15.1056) | Bit/dim 3.9187(3.9020) | Xent 1.1448(1.1829) | Loss 109.7411(186.1669) | Error 0.4189(0.4266) Steps 0(0.00) | Grad Norm 21.9717(12.2385) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.7637(15.1730) | Bit/dim 3.8739(3.9011) | Xent 1.1818(1.1787) | Loss 120.0930(167.6634) | Error 0.4156(0.4231) Steps 0(0.00) | Grad Norm 18.0359(12.6092) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.6189(15.1288) | Bit/dim 3.8808(3.8978) | Xent 1.1808(1.1811) | Loss 115.0611(153.6372) | Error 0.4033(0.4230) Steps 0(0.00) | Grad Norm 21.9293(12.8518) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 14.2912(15.0828) | Bit/dim 3.9029(3.8959) | Xent 1.2138(1.1803) | Loss 112.2033(143.3043) | Error 0.4244(0.4219) Steps 0(0.00) | Grad Norm 8.7437(13.3665) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.3033(15.1383) | Bit/dim 3.8679(3.8920) | Xent 1.1706(1.1774) | Loss 118.2093(135.9274) | Error 0.4256(0.4211) Steps 0(0.00) | Grad Norm 10.2460(12.5105) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.8434(15.1431) | Bit/dim 3.8770(3.8894) | Xent 1.2159(1.1710) | Loss 115.7720(130.1924) | Error 0.4278(0.4198) Steps 0(0.00) | Grad Norm 24.1842(12.5712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 77.1083, Epoch Time 926.7331(879.2019), Bit/dim 3.8921(best: 3.9024), Xent 1.2165, Loss 4.5004, Error 0.4371(best: 0.4082)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.2668(15.1824) | Bit/dim 3.8977(3.8901) | Xent 1.2232(1.1866) | Loss 118.3165(176.2406) | Error 0.4511(0.4252) Steps 0(0.00) | Grad Norm 16.9859(14.1882) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 14.8162(15.1724) | Bit/dim 3.8494(3.8850) | Xent 1.0660(1.1721) | Loss 112.3884(159.7055) | Error 0.3711(0.4191) Steps 0(0.00) | Grad Norm 9.2934(13.1734) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.6832(15.1645) | Bit/dim 3.8801(3.8850) | Xent 1.1326(1.1606) | Loss 111.0393(147.7007) | Error 0.4122(0.4160) Steps 0(0.00) | Grad Norm 10.9954(12.3029) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.3403(15.1656) | Bit/dim 3.8805(3.8819) | Xent 1.1311(1.1537) | Loss 111.5047(138.8221) | Error 0.4022(0.4141) Steps 0(0.00) | Grad Norm 9.2190(13.2328) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 15.6625(15.1405) | Bit/dim 3.8908(3.8846) | Xent 1.2046(1.1556) | Loss 110.4570(132.4898) | Error 0.4267(0.4146) Steps 0(0.00) | Grad Norm 12.3790(13.4477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 77.6022, Epoch Time 929.6895(880.7165), Bit/dim 3.8765(best: 3.8921), Xent 1.0982, Loss 4.4256, Error 0.3941(best: 0.4082)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 15.1597(15.1172) | Bit/dim 3.8546(3.8842) | Xent 1.1102(1.1535) | Loss 111.1686(187.0296) | Error 0.3800(0.4142) Steps 0(0.00) | Grad Norm 10.8387(13.3210) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 14.7559(15.0487) | Bit/dim 3.8690(3.8833) | Xent 1.1593(1.1441) | Loss 112.2145(167.7590) | Error 0.4211(0.4101) Steps 0(0.00) | Grad Norm 10.9908(12.4735) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.0572(15.0783) | Bit/dim 3.8709(3.8802) | Xent 1.0822(1.1356) | Loss 117.2746(153.6745) | Error 0.3933(0.4086) Steps 0(0.00) | Grad Norm 7.7965(12.1970) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.0062(15.0703) | Bit/dim 3.8587(3.8768) | Xent 1.1535(1.1324) | Loss 113.5320(143.5125) | Error 0.4056(0.4068) Steps 0(0.00) | Grad Norm 7.6844(11.5987) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.6468(15.0665) | Bit/dim 3.8302(3.8725) | Xent 1.0693(1.1143) | Loss 113.3875(136.1436) | Error 0.3744(0.3998) Steps 0(0.00) | Grad Norm 8.8508(11.6004) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.7565(15.0814) | Bit/dim 3.8262(3.8699) | Xent 1.1272(1.1238) | Loss 117.3798(130.6172) | Error 0.4100(0.4015) Steps 0(0.00) | Grad Norm 6.7524(12.3321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 77.8090, Epoch Time 923.1882(881.9906), Bit/dim 3.8644(best: 3.8765), Xent 1.1151, Loss 4.4220, Error 0.4069(best: 0.3941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.6896(15.1078) | Bit/dim 3.8644(3.8672) | Xent 1.0878(1.1193) | Loss 114.4608(178.5633) | Error 0.4044(0.4008) Steps 0(0.00) | Grad Norm 14.0401(12.4427) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 15.5183(15.1017) | Bit/dim 3.8821(3.8637) | Xent 1.0563(1.1080) | Loss 119.3525(161.7777) | Error 0.3811(0.3988) Steps 0(0.00) | Grad Norm 13.9103(12.2271) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.2504(15.1091) | Bit/dim 3.9084(3.8651) | Xent 1.0832(1.0984) | Loss 113.8258(149.2069) | Error 0.3989(0.3934) Steps 0(0.00) | Grad Norm 8.0034(11.5720) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.9769(15.0540) | Bit/dim 3.8477(3.8631) | Xent 1.1170(1.0896) | Loss 113.0337(140.0197) | Error 0.3867(0.3896) Steps 0(0.00) | Grad Norm 16.3484(11.6595) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.5687(15.0746) | Bit/dim 3.8900(3.8637) | Xent 1.0787(1.0910) | Loss 118.8166(133.3724) | Error 0.4000(0.3918) Steps 0(0.00) | Grad Norm 18.9921(11.3345) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 79.4900, Epoch Time 927.0340(883.3419), Bit/dim 3.8605(best: 3.8644), Xent 1.1058, Loss 4.4135, Error 0.4002(best: 0.3941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 14.9185(15.0447) | Bit/dim 3.8670(3.8641) | Xent 1.1885(1.0992) | Loss 116.1895(189.5801) | Error 0.4111(0.3932) Steps 0(0.00) | Grad Norm 19.4840(12.6958) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.2031(15.0416) | Bit/dim 3.8616(3.8605) | Xent 1.1079(1.0966) | Loss 121.7925(170.3832) | Error 0.3856(0.3925) Steps 0(0.00) | Grad Norm 11.2855(12.3442) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 14.9979(15.1011) | Bit/dim 3.8682(3.8597) | Xent 1.1058(1.0881) | Loss 119.5883(155.7465) | Error 0.3744(0.3904) Steps 0(0.00) | Grad Norm 13.6007(11.9462) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.5211(15.1190) | Bit/dim 3.8401(3.8564) | Xent 1.0123(1.0819) | Loss 115.1193(145.3346) | Error 0.3589(0.3862) Steps 0(0.00) | Grad Norm 9.5516(11.5103) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.6181(15.0993) | Bit/dim 3.8647(3.8542) | Xent 1.0363(1.0812) | Loss 111.9156(136.8057) | Error 0.3733(0.3864) Steps 0(0.00) | Grad Norm 15.2472(11.8225) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.4437(15.1555) | Bit/dim 3.8536(3.8501) | Xent 1.1333(1.0844) | Loss 114.8887(131.1717) | Error 0.4067(0.3870) Steps 0(0.00) | Grad Norm 16.0551(11.9116) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 77.9933, Epoch Time 928.8695(884.7078), Bit/dim 3.8513(best: 3.8605), Xent 1.0459, Loss 4.3742, Error 0.3745(best: 0.3941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.0494(15.1396) | Bit/dim 3.8736(3.8530) | Xent 1.0635(1.0790) | Loss 111.7089(178.5742) | Error 0.3911(0.3861) Steps 0(0.00) | Grad Norm 14.6543(12.5136) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 15.8180(15.1255) | Bit/dim 3.8658(3.8514) | Xent 1.1175(1.0798) | Loss 115.9678(161.8833) | Error 0.3933(0.3846) Steps 0(0.00) | Grad Norm 9.2958(12.6740) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 14.1271(15.1264) | Bit/dim 3.8361(3.8483) | Xent 1.0235(1.0699) | Loss 110.8856(149.5244) | Error 0.3533(0.3798) Steps 0(0.00) | Grad Norm 12.5410(11.5952) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 14.7369(15.0914) | Bit/dim 3.8282(3.8440) | Xent 1.0104(1.0687) | Loss 113.2973(140.6712) | Error 0.3622(0.3802) Steps 0(0.00) | Grad Norm 12.2388(11.8672) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 14.5015(15.0477) | Bit/dim 3.8285(3.8407) | Xent 1.0673(1.0650) | Loss 114.8280(133.5738) | Error 0.3756(0.3790) Steps 0(0.00) | Grad Norm 8.7110(11.4200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 77.8319, Epoch Time 922.8927(885.8533), Bit/dim 3.8370(best: 3.8513), Xent 1.0067, Loss 4.3404, Error 0.3621(best: 0.3745)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 14.5980(14.9844) | Bit/dim 3.8298(3.8397) | Xent 1.0864(1.0602) | Loss 113.0996(186.3517) | Error 0.3944(0.3785) Steps 0(0.00) | Grad Norm 9.8759(11.2371) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 14.8806(14.9513) | Bit/dim 3.8626(3.8424) | Xent 1.0376(1.0530) | Loss 111.3118(167.4107) | Error 0.3644(0.3770) Steps 0(0.00) | Grad Norm 9.3207(10.7737) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 15.1980(14.9535) | Bit/dim 3.8489(3.8404) | Xent 1.0286(1.0493) | Loss 116.5160(153.7276) | Error 0.3800(0.3766) Steps 0(0.00) | Grad Norm 11.1921(10.7052) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 14.8896(14.9874) | Bit/dim 3.8467(3.8396) | Xent 1.0326(1.0474) | Loss 116.8475(144.0334) | Error 0.3522(0.3759) Steps 0(0.00) | Grad Norm 15.0488(11.0542) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.7555(15.0208) | Bit/dim 3.8332(3.8392) | Xent 1.0201(1.0520) | Loss 120.4901(136.6496) | Error 0.3711(0.3764) Steps 0(0.00) | Grad Norm 11.4814(12.2957) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.1039(14.9816) | Bit/dim 3.7969(3.8343) | Xent 1.0801(1.0564) | Loss 118.2786(131.2207) | Error 0.3856(0.3772) Steps 0(0.00) | Grad Norm 14.6988(12.1422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 79.8031, Epoch Time 920.5203(886.8933), Bit/dim 3.8370(best: 3.8370), Xent 1.0005, Loss 4.3373, Error 0.3542(best: 0.3621)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.3660(15.0986) | Bit/dim 3.8137(3.8354) | Xent 1.0315(1.0559) | Loss 118.9022(179.9092) | Error 0.3656(0.3769) Steps 0(0.00) | Grad Norm 7.0513(12.4207) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.4142(15.0274) | Bit/dim 3.8221(3.8338) | Xent 1.0428(1.0492) | Loss 120.1573(162.8710) | Error 0.3800(0.3747) Steps 0(0.00) | Grad Norm 6.4880(11.6871) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 14.4940(15.0571) | Bit/dim 3.8485(3.8329) | Xent 1.0189(1.0501) | Loss 113.9706(150.6438) | Error 0.3656(0.3766) Steps 0(0.00) | Grad Norm 11.4801(11.6923) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.2042(15.0137) | Bit/dim 3.7822(3.8308) | Xent 1.1257(1.0575) | Loss 115.8943(141.2886) | Error 0.4067(0.3781) Steps 0(0.00) | Grad Norm 18.3982(12.9073) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.7282(15.0987) | Bit/dim 3.8115(3.8269) | Xent 1.0534(1.0617) | Loss 116.6876(134.9762) | Error 0.3711(0.3793) Steps 0(0.00) | Grad Norm 11.5535(13.0441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 78.7129, Epoch Time 928.7439(888.1488), Bit/dim 3.8206(best: 3.8370), Xent 1.0119, Loss 4.3266, Error 0.3592(best: 0.3542)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.4817(15.0930) | Bit/dim 3.8506(3.8279) | Xent 0.9680(1.0514) | Loss 115.3450(188.8916) | Error 0.3400(0.3750) Steps 0(0.00) | Grad Norm 12.3185(12.8506) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 14.8811(15.0359) | Bit/dim 3.8401(3.8262) | Xent 1.0168(1.0388) | Loss 117.6372(169.3469) | Error 0.3500(0.3714) Steps 0(0.00) | Grad Norm 10.4168(12.1756) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.5908(15.0067) | Bit/dim 3.8385(3.8209) | Xent 1.0036(1.0291) | Loss 110.9774(154.8948) | Error 0.3600(0.3678) Steps 0(0.00) | Grad Norm 5.7683(10.8046) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.9497(15.0072) | Bit/dim 3.8208(3.8191) | Xent 1.0317(1.0221) | Loss 119.6072(144.1538) | Error 0.3778(0.3645) Steps 0(0.00) | Grad Norm 9.3829(10.7531) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.1412(15.0598) | Bit/dim 3.8006(3.8188) | Xent 1.0376(1.0320) | Loss 112.7579(136.5711) | Error 0.3678(0.3666) Steps 0(0.00) | Grad Norm 8.7728(11.1164) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 14.7925(15.0005) | Bit/dim 3.8405(3.8217) | Xent 1.0778(1.0301) | Loss 117.2446(131.1450) | Error 0.3933(0.3672) Steps 0(0.00) | Grad Norm 11.0968(11.1242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 79.6369, Epoch Time 921.9203(889.1620), Bit/dim 3.8157(best: 3.8206), Xent 0.9863, Loss 4.3089, Error 0.3503(best: 0.3542)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 14.7555(15.0550) | Bit/dim 3.8072(3.8175) | Xent 1.0375(1.0245) | Loss 115.9556(178.3685) | Error 0.3667(0.3664) Steps 0(0.00) | Grad Norm 11.7810(10.5608) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 14.8639(15.0774) | Bit/dim 3.8080(3.8179) | Xent 1.0298(1.0161) | Loss 111.8931(161.7422) | Error 0.3589(0.3609) Steps 0(0.00) | Grad Norm 10.5111(10.5923) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.7927(15.1268) | Bit/dim 3.7917(3.8164) | Xent 1.0086(1.0094) | Loss 120.8705(150.0367) | Error 0.3544(0.3593) Steps 0(0.00) | Grad Norm 16.0100(10.1467) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 14.9330(15.1236) | Bit/dim 3.8199(3.8170) | Xent 1.0319(1.0152) | Loss 110.6776(140.9374) | Error 0.3467(0.3610) Steps 0(0.00) | Grad Norm 10.9401(10.6973) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 14.8793(15.1778) | Bit/dim 3.8396(3.8139) | Xent 1.0613(1.0174) | Loss 111.4373(134.1323) | Error 0.3811(0.3618) Steps 0(0.00) | Grad Norm 22.8785(11.7226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 78.9015, Epoch Time 933.1531(890.4817), Bit/dim 3.8248(best: 3.8157), Xent 1.0517, Loss 4.3506, Error 0.3797(best: 0.3503)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 14.9359(15.1207) | Bit/dim 3.7921(3.8162) | Xent 0.9805(1.0181) | Loss 115.1011(190.7159) | Error 0.3489(0.3624) Steps 0(0.00) | Grad Norm 7.5713(12.0596) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 14.8196(15.0792) | Bit/dim 3.8155(3.8125) | Xent 1.0353(1.0143) | Loss 114.4707(171.2199) | Error 0.3611(0.3599) Steps 0(0.00) | Grad Norm 15.5483(11.7160) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 15.3234(15.0718) | Bit/dim 3.8207(3.8117) | Xent 0.9490(1.0085) | Loss 119.7360(156.7114) | Error 0.3289(0.3585) Steps 0(0.00) | Grad Norm 7.7754(11.4427) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.5493(15.0865) | Bit/dim 3.8218(3.8135) | Xent 1.0477(1.0018) | Loss 116.4542(146.3636) | Error 0.3956(0.3574) Steps 0(0.00) | Grad Norm 11.2769(11.3675) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.1427(15.0543) | Bit/dim 3.7968(3.8076) | Xent 1.0125(0.9960) | Loss 117.8778(138.4040) | Error 0.3444(0.3551) Steps 0(0.00) | Grad Norm 11.3547(11.0884) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.0641(15.0496) | Bit/dim 3.8163(3.8076) | Xent 1.0556(1.0015) | Loss 115.3439(133.2354) | Error 0.4000(0.3578) Steps 0(0.00) | Grad Norm 6.8671(11.3317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 79.2045, Epoch Time 923.5147(891.4727), Bit/dim 3.8075(best: 3.8157), Xent 0.9649, Loss 4.2899, Error 0.3431(best: 0.3503)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.2667(15.0717) | Bit/dim 3.8048(3.8084) | Xent 0.9702(0.9918) | Loss 115.7650(180.1748) | Error 0.3478(0.3550) Steps 0(0.00) | Grad Norm 10.7027(10.6915) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.1876(15.0719) | Bit/dim 3.8355(3.8080) | Xent 1.0450(0.9920) | Loss 118.2725(163.4459) | Error 0.3489(0.3535) Steps 0(0.00) | Grad Norm 15.9351(10.6317) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.7520(15.1191) | Bit/dim 3.8172(3.8061) | Xent 0.9936(0.9922) | Loss 118.2470(151.3774) | Error 0.3544(0.3541) Steps 0(0.00) | Grad Norm 11.1780(10.8910) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 14.9224(15.0912) | Bit/dim 3.8298(3.8023) | Xent 1.0326(0.9925) | Loss 118.1805(142.1196) | Error 0.3811(0.3545) Steps 0(0.00) | Grad Norm 14.5703(10.9252) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 14.4598(15.0347) | Bit/dim 3.7946(3.8041) | Xent 1.0872(1.0044) | Loss 113.8010(135.1364) | Error 0.3889(0.3588) Steps 0(0.00) | Grad Norm 14.7139(11.8420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 79.0663, Epoch Time 926.5984(892.5265), Bit/dim 3.7963(best: 3.8075), Xent 0.9664, Loss 4.2795, Error 0.3461(best: 0.3431)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.6680(15.0759) | Bit/dim 3.7783(3.8022) | Xent 0.9300(0.9992) | Loss 117.8626(193.9543) | Error 0.3356(0.3563) Steps 0(0.00) | Grad Norm 5.4514(11.0560) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.1117(15.0646) | Bit/dim 3.8155(3.8003) | Xent 0.9518(0.9847) | Loss 118.0054(173.6862) | Error 0.3422(0.3504) Steps 0(0.00) | Grad Norm 5.3960(10.2131) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.0401(15.0863) | Bit/dim 3.7902(3.8002) | Xent 0.9721(0.9753) | Loss 110.1012(158.6796) | Error 0.3411(0.3487) Steps 0(0.00) | Grad Norm 7.0170(9.8839) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 14.9009(15.1095) | Bit/dim 3.7679(3.7968) | Xent 0.9876(0.9712) | Loss 112.8523(147.0247) | Error 0.3544(0.3455) Steps 0(0.00) | Grad Norm 10.5542(9.6782) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 14.8258(15.1087) | Bit/dim 3.7240(3.7951) | Xent 1.0581(0.9765) | Loss 114.3265(138.7575) | Error 0.3767(0.3463) Steps 0(0.00) | Grad Norm 15.0487(10.3757) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 14.9106(15.1609) | Bit/dim 3.7960(3.7958) | Xent 0.9628(0.9752) | Loss 113.1110(132.9785) | Error 0.3433(0.3470) Steps 0(0.00) | Grad Norm 11.1326(10.2933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 78.6629, Epoch Time 931.1129(893.6841), Bit/dim 3.7884(best: 3.7963), Xent 0.9590, Loss 4.2679, Error 0.3425(best: 0.3431)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.4816(15.1088) | Bit/dim 3.8060(3.7942) | Xent 0.9384(0.9754) | Loss 115.0424(180.2656) | Error 0.3422(0.3473) Steps 0(0.00) | Grad Norm 12.5599(10.6586) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.9872(15.0664) | Bit/dim 3.7648(3.7941) | Xent 0.9007(0.9736) | Loss 114.8135(163.5054) | Error 0.3222(0.3473) Steps 0(0.00) | Grad Norm 8.7534(11.0478) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 14.7593(15.0619) | Bit/dim 3.7941(3.7933) | Xent 0.9190(0.9675) | Loss 118.6377(151.9099) | Error 0.3344(0.3459) Steps 0(0.00) | Grad Norm 9.1837(10.7918) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 15.2116(15.1002) | Bit/dim 3.7966(3.7929) | Xent 0.9089(0.9738) | Loss 124.1286(143.2316) | Error 0.3200(0.3469) Steps 0(0.00) | Grad Norm 15.4709(11.1821) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.1344(15.1060) | Bit/dim 3.7791(3.7924) | Xent 0.8411(0.9676) | Loss 121.3657(136.2064) | Error 0.3156(0.3453) Steps 0(0.00) | Grad Norm 11.0528(11.4893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 78.5758, Epoch Time 924.6058(894.6117), Bit/dim 3.7871(best: 3.7884), Xent 0.9718, Loss 4.2730, Error 0.3509(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.1221(15.0795) | Bit/dim 3.7629(3.7905) | Xent 0.9575(0.9618) | Loss 113.6601(193.3668) | Error 0.3522(0.3435) Steps 0(0.00) | Grad Norm 8.2103(11.0053) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.6244(15.0906) | Bit/dim 3.8194(3.7901) | Xent 0.9677(0.9641) | Loss 118.4705(173.5156) | Error 0.3333(0.3441) Steps 0(0.00) | Grad Norm 9.8795(11.3717) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.6849(15.0604) | Bit/dim 3.7662(3.7874) | Xent 0.9281(0.9620) | Loss 113.8767(158.6959) | Error 0.3311(0.3439) Steps 0(0.00) | Grad Norm 8.7318(11.1578) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 15.2512(15.0597) | Bit/dim 3.8110(3.7853) | Xent 0.9611(0.9584) | Loss 119.2238(148.0065) | Error 0.3411(0.3423) Steps 0(0.00) | Grad Norm 6.1939(10.5620) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.4006(15.0972) | Bit/dim 3.7849(3.7865) | Xent 1.0019(0.9629) | Loss 120.3686(140.2747) | Error 0.3544(0.3424) Steps 0(0.00) | Grad Norm 8.3976(10.8928) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.2928(15.0973) | Bit/dim 3.7605(3.7861) | Xent 0.9016(0.9586) | Loss 116.9686(134.5044) | Error 0.3322(0.3417) Steps 0(0.00) | Grad Norm 9.6894(10.9169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 79.5565, Epoch Time 927.4674(895.5974), Bit/dim 3.7836(best: 3.7871), Xent 0.9174, Loss 4.2423, Error 0.3256(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.6741(15.0813) | Bit/dim 3.7584(3.7847) | Xent 0.8728(0.9503) | Loss 120.6021(185.0966) | Error 0.3156(0.3391) Steps 0(0.00) | Grad Norm 13.5356(11.4325) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.9761(15.1164) | Bit/dim 3.7815(3.7853) | Xent 0.9589(0.9499) | Loss 118.7587(168.3329) | Error 0.3322(0.3366) Steps 0(0.00) | Grad Norm 10.2389(11.6732) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.4949(15.1748) | Bit/dim 3.8079(3.7830) | Xent 0.8862(0.9496) | Loss 121.9869(155.7883) | Error 0.3211(0.3380) Steps 0(0.00) | Grad Norm 16.2426(12.0053) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 15.4626(15.1253) | Bit/dim 3.7626(3.7843) | Xent 0.9258(0.9458) | Loss 124.0881(146.8714) | Error 0.3233(0.3371) Steps 0(0.00) | Grad Norm 10.1876(11.3651) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.1835(15.1295) | Bit/dim 3.8159(3.7836) | Xent 1.0058(0.9502) | Loss 119.8386(139.7958) | Error 0.3589(0.3397) Steps 0(0.00) | Grad Norm 10.5205(10.9569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 80.2111, Epoch Time 930.3961(896.6414), Bit/dim 3.7743(best: 3.7836), Xent 0.9123, Loss 4.2305, Error 0.3213(best: 0.3256)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.1755(15.0828) | Bit/dim 3.8010(3.7790) | Xent 0.8940(0.9410) | Loss 114.6748(195.8889) | Error 0.3222(0.3367) Steps 0(0.00) | Grad Norm 9.5051(10.3079) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.6339(15.1081) | Bit/dim 3.8009(3.7799) | Xent 1.0110(0.9478) | Loss 124.7700(175.9062) | Error 0.3522(0.3377) Steps 0(0.00) | Grad Norm 21.5034(11.5976) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.2026(15.1678) | Bit/dim 3.7824(3.7807) | Xent 1.0015(0.9519) | Loss 123.7077(161.2489) | Error 0.3556(0.3399) Steps 0(0.00) | Grad Norm 17.7834(12.0643) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 14.5757(15.1509) | Bit/dim 3.7889(3.7827) | Xent 0.9868(0.9565) | Loss 120.7601(150.3603) | Error 0.3533(0.3416) Steps 0(0.00) | Grad Norm 8.9922(11.5040) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.2492(15.1295) | Bit/dim 3.7433(3.7803) | Xent 0.9847(0.9595) | Loss 122.5272(142.5481) | Error 0.3611(0.3421) Steps 0(0.00) | Grad Norm 8.1428(11.9120) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 15.0556(15.1102) | Bit/dim 3.7544(3.7785) | Xent 0.9426(0.9589) | Loss 125.0148(136.9774) | Error 0.3444(0.3430) Steps 0(0.00) | Grad Norm 13.6151(11.4456) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 80.0720, Epoch Time 930.4705(897.6562), Bit/dim 3.7772(best: 3.7743), Xent 0.9111, Loss 4.2328, Error 0.3259(best: 0.3213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.0941(15.1216) | Bit/dim 3.7638(3.7764) | Xent 0.8592(0.9394) | Loss 119.2957(188.1397) | Error 0.3044(0.3351) Steps 0(0.00) | Grad Norm 7.5404(10.7236) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.3747(15.1295) | Bit/dim 3.7723(3.7723) | Xent 0.9175(0.9370) | Loss 115.4682(170.2440) | Error 0.3356(0.3355) Steps 0(0.00) | Grad Norm 6.1172(10.4285) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.7068(15.1902) | Bit/dim 3.7882(3.7721) | Xent 0.8625(0.9363) | Loss 117.8233(157.0899) | Error 0.3067(0.3345) Steps 0(0.00) | Grad Norm 12.1360(10.5103) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.4239(15.1781) | Bit/dim 3.7659(3.7756) | Xent 0.9453(0.9436) | Loss 123.6628(147.7477) | Error 0.3444(0.3366) Steps 0(0.00) | Grad Norm 8.4761(11.8178) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.5231(15.1308) | Bit/dim 3.7523(3.7758) | Xent 0.8932(0.9388) | Loss 129.0328(140.8899) | Error 0.3378(0.3339) Steps 0(0.00) | Grad Norm 7.3142(11.3398) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 82.7750, Epoch Time 934.9282(898.7744), Bit/dim 3.7727(best: 3.7743), Xent 0.9005, Loss 4.2229, Error 0.3216(best: 0.3213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.6891(15.1629) | Bit/dim 3.7532(3.7755) | Xent 0.9147(0.9295) | Loss 119.9005(199.3041) | Error 0.3078(0.3297) Steps 0(0.00) | Grad Norm 9.0780(10.7123) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.6384(15.1838) | Bit/dim 3.7609(3.7727) | Xent 0.8496(0.9248) | Loss 117.0830(178.5115) | Error 0.3167(0.3293) Steps 0(0.00) | Grad Norm 9.0520(10.2671) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.8692(15.1367) | Bit/dim 3.7468(3.7701) | Xent 0.9194(0.9241) | Loss 120.9242(162.9660) | Error 0.3189(0.3295) Steps 0(0.00) | Grad Norm 8.2930(10.6430) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.8601(15.1083) | Bit/dim 3.7726(3.7699) | Xent 0.8757(0.9152) | Loss 123.6982(152.0139) | Error 0.3133(0.3258) Steps 0(0.00) | Grad Norm 5.4472(9.5750) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.5636(15.0999) | Bit/dim 3.7541(3.7682) | Xent 0.9666(0.9230) | Loss 119.3366(143.6443) | Error 0.3478(0.3288) Steps 0(0.00) | Grad Norm 12.5496(10.3994) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 14.7020(15.0932) | Bit/dim 3.7485(3.7636) | Xent 0.8815(0.9228) | Loss 112.9877(137.3824) | Error 0.3211(0.3294) Steps 0(0.00) | Grad Norm 11.2425(10.6241) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 80.7039, Epoch Time 930.5663(899.7281), Bit/dim 3.7636(best: 3.7727), Xent 0.9028, Loss 4.2150, Error 0.3240(best: 0.3213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 14.8798(15.1071) | Bit/dim 3.7517(3.7622) | Xent 0.9130(0.9160) | Loss 119.4776(186.6052) | Error 0.3333(0.3267) Steps 0(0.00) | Grad Norm 6.8535(9.7499) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.9809(15.1119) | Bit/dim 3.7694(3.7628) | Xent 0.9540(0.9162) | Loss 114.2723(168.9168) | Error 0.3378(0.3260) Steps 0(0.00) | Grad Norm 11.6480(9.8725) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.2026(15.1237) | Bit/dim 3.7401(3.7610) | Xent 0.9720(0.9173) | Loss 118.8046(155.6934) | Error 0.3389(0.3268) Steps 0(0.00) | Grad Norm 8.2865(9.8859) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.2029(15.1406) | Bit/dim 3.7529(3.7607) | Xent 0.7992(0.9067) | Loss 123.1714(147.0459) | Error 0.2889(0.3239) Steps 0(0.00) | Grad Norm 13.3093(10.3208) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.9072(15.1736) | Bit/dim 3.7730(3.7627) | Xent 0.9127(0.9127) | Loss 122.4105(140.5587) | Error 0.3478(0.3274) Steps 0(0.00) | Grad Norm 10.4359(10.6013) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 81.7821, Epoch Time 936.1175(900.8198), Bit/dim 3.7623(best: 3.7636), Xent 0.9120, Loss 4.2183, Error 0.3282(best: 0.3213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.0157(15.2127) | Bit/dim 3.7267(3.7611) | Xent 0.8590(0.8997) | Loss 116.8489(198.5258) | Error 0.3000(0.3227) Steps 0(0.00) | Grad Norm 8.9649(10.3750) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 14.5861(15.2264) | Bit/dim 3.7107(3.7588) | Xent 0.8446(0.8953) | Loss 123.6893(177.9155) | Error 0.3000(0.3204) Steps 0(0.00) | Grad Norm 14.3426(10.5945) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.9715(15.2008) | Bit/dim 3.7186(3.7590) | Xent 0.8087(0.8897) | Loss 128.1459(162.7137) | Error 0.2878(0.3186) Steps 0(0.00) | Grad Norm 8.9258(10.6150) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.0618(15.1719) | Bit/dim 3.7580(3.7598) | Xent 0.9526(0.9041) | Loss 123.1492(151.7319) | Error 0.3411(0.3243) Steps 0(0.00) | Grad Norm 11.0653(11.3317) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 14.8282(15.1534) | Bit/dim 3.7730(3.7605) | Xent 0.9330(0.9097) | Loss 120.6121(144.4778) | Error 0.3344(0.3248) Steps 0(0.00) | Grad Norm 7.7330(11.5630) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.4258(15.1605) | Bit/dim 3.7416(3.7610) | Xent 0.9836(0.9150) | Loss 119.3137(138.1819) | Error 0.3489(0.3274) Steps 0(0.00) | Grad Norm 20.2494(11.7178) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 81.1156, Epoch Time 933.1424(901.7895), Bit/dim 3.7632(best: 3.7623), Xent 0.8923, Loss 4.2093, Error 0.3149(best: 0.3213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.4238(15.1962) | Bit/dim 3.7871(3.7609) | Xent 0.8858(0.9224) | Loss 128.3714(188.1131) | Error 0.2989(0.3307) Steps 0(0.00) | Grad Norm 6.3128(11.7476) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.4698(15.1968) | Bit/dim 3.7687(3.7613) | Xent 0.9007(0.9162) | Loss 118.5532(170.4698) | Error 0.3167(0.3292) Steps 0(0.00) | Grad Norm 11.7909(11.2518) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.8780(15.1269) | Bit/dim 3.7900(3.7595) | Xent 0.8670(0.9098) | Loss 117.7480(157.2055) | Error 0.3200(0.3277) Steps 0(0.00) | Grad Norm 7.4946(10.6646) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 14.9435(15.1378) | Bit/dim 3.7662(3.7590) | Xent 0.8755(0.9082) | Loss 116.9192(147.4597) | Error 0.3067(0.3268) Steps 0(0.00) | Grad Norm 7.5202(10.6489) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.8159(15.1378) | Bit/dim 3.7594(3.7545) | Xent 0.9975(0.9034) | Loss 125.5527(140.8254) | Error 0.3489(0.3249) Steps 0(0.00) | Grad Norm 11.0553(10.2974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 82.8424, Epoch Time 934.0227(902.7565), Bit/dim 3.7624(best: 3.7623), Xent 0.8953, Loss 4.2100, Error 0.3179(best: 0.3149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 14.7450(15.1252) | Bit/dim 3.7484(3.7574) | Xent 0.8704(0.8966) | Loss 123.1362(200.9375) | Error 0.3122(0.3224) Steps 0(0.00) | Grad Norm 8.6062(10.0972) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 14.9389(15.1464) | Bit/dim 3.7578(3.7571) | Xent 0.8932(0.8846) | Loss 121.8499(180.4493) | Error 0.3278(0.3177) Steps 0(0.00) | Grad Norm 5.7629(9.2494) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 14.6504(15.0732) | Bit/dim 3.7355(3.7524) | Xent 0.9438(0.8844) | Loss 120.1340(165.0030) | Error 0.3389(0.3184) Steps 0(0.00) | Grad Norm 10.7982(9.4437) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 14.5025(15.0914) | Bit/dim 3.7529(3.7534) | Xent 0.9154(0.8910) | Loss 120.5861(153.4311) | Error 0.3044(0.3191) Steps 0(0.00) | Grad Norm 7.4369(10.1111) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.9177(15.0802) | Bit/dim 3.7580(3.7516) | Xent 0.8888(0.8921) | Loss 116.2153(144.8539) | Error 0.3156(0.3179) Steps 0(0.00) | Grad Norm 8.4396(9.6714) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 14.9548(15.0747) | Bit/dim 3.7440(3.7487) | Xent 0.8279(0.8791) | Loss 120.9317(138.3585) | Error 0.2944(0.3140) Steps 0(0.00) | Grad Norm 7.5147(8.9436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 83.6725, Epoch Time 930.7445(903.5961), Bit/dim 3.7479(best: 3.7623), Xent 0.8633, Loss 4.1795, Error 0.3049(best: 0.3149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 14.9218(15.1721) | Bit/dim 3.7430(3.7506) | Xent 0.8070(0.8670) | Loss 122.3773(188.5682) | Error 0.2833(0.3099) Steps 0(0.00) | Grad Norm 15.1064(8.9357) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.5260(15.2489) | Bit/dim 3.7238(3.7506) | Xent 0.8763(0.8705) | Loss 122.5567(171.5727) | Error 0.3111(0.3102) Steps 0(0.00) | Grad Norm 13.0894(9.5335) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.2186(15.2028) | Bit/dim 3.7617(3.7486) | Xent 0.8952(0.8757) | Loss 116.9866(157.7432) | Error 0.3167(0.3116) Steps 0(0.00) | Grad Norm 10.9636(9.6634) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.0810(15.2093) | Bit/dim 3.7069(3.7444) | Xent 0.8681(0.8656) | Loss 118.5068(147.9720) | Error 0.3156(0.3102) Steps 0(0.00) | Grad Norm 6.2496(9.2302) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.0342(15.1815) | Bit/dim 3.7675(3.7451) | Xent 1.0253(0.8801) | Loss 126.8877(141.5866) | Error 0.3644(0.3143) Steps 0(0.00) | Grad Norm 23.6201(10.2746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 82.1916, Epoch Time 941.8203(904.7429), Bit/dim 3.7536(best: 3.7479), Xent 0.8890, Loss 4.1981, Error 0.3124(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 14.9828(15.2102) | Bit/dim 3.7631(3.7469) | Xent 0.8859(0.8800) | Loss 121.6615(201.5711) | Error 0.3100(0.3147) Steps 0(0.00) | Grad Norm 13.4448(10.9427) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.6379(15.2703) | Bit/dim 3.7574(3.7478) | Xent 0.7696(0.8710) | Loss 125.8304(180.9389) | Error 0.2978(0.3126) Steps 0(0.00) | Grad Norm 10.2150(10.4013) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.3087(15.3317) | Bit/dim 3.7542(3.7481) | Xent 0.8995(0.8779) | Loss 121.1892(166.1635) | Error 0.3089(0.3140) Steps 0(0.00) | Grad Norm 6.0589(10.5599) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.7048(15.4143) | Bit/dim 3.7424(3.7467) | Xent 0.8538(0.8705) | Loss 118.5213(154.8106) | Error 0.3022(0.3116) Steps 0(0.00) | Grad Norm 8.7385(10.1569) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.0264(15.3699) | Bit/dim 3.7655(3.7472) | Xent 0.8656(0.8678) | Loss 119.6221(146.0434) | Error 0.3122(0.3099) Steps 0(0.00) | Grad Norm 8.3069(9.6132) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.5368(15.3347) | Bit/dim 3.7278(3.7425) | Xent 0.8914(0.8692) | Loss 120.9762(139.3987) | Error 0.3022(0.3086) Steps 0(0.00) | Grad Norm 6.9084(9.8216) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 82.0036, Epoch Time 946.2472(905.9880), Bit/dim 3.7406(best: 3.7479), Xent 0.8676, Loss 4.1744, Error 0.3062(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 14.8852(15.3007) | Bit/dim 3.7435(3.7428) | Xent 0.7823(0.8592) | Loss 119.5648(190.0126) | Error 0.2867(0.3066) Steps 0(0.00) | Grad Norm 9.2629(9.9204) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.9790(15.3419) | Bit/dim 3.7468(3.7410) | Xent 0.8152(0.8576) | Loss 125.3874(172.2991) | Error 0.3000(0.3069) Steps 0(0.00) | Grad Norm 10.2759(9.7188) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.6936(15.2971) | Bit/dim 3.7271(3.7392) | Xent 0.8496(0.8560) | Loss 134.5780(159.2369) | Error 0.3033(0.3064) Steps 0(0.00) | Grad Norm 6.7977(9.7805) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.3207(15.3449) | Bit/dim 3.7427(3.7389) | Xent 0.8947(0.8597) | Loss 127.4296(149.8351) | Error 0.3289(0.3083) Steps 0(0.00) | Grad Norm 15.9112(10.3625) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.5173(15.2689) | Bit/dim 3.7553(3.7417) | Xent 0.8445(0.8633) | Loss 119.5409(142.0924) | Error 0.3056(0.3099) Steps 0(0.00) | Grad Norm 6.5452(10.2640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 80.4986, Epoch Time 936.3652(906.8993), Bit/dim 3.7351(best: 3.7406), Xent 0.9078, Loss 4.1890, Error 0.3208(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 14.9854(15.1835) | Bit/dim 3.7285(3.7407) | Xent 0.9287(0.8699) | Loss 122.1378(199.9496) | Error 0.3244(0.3109) Steps 0(0.00) | Grad Norm 14.2758(10.6349) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.6527(15.2003) | Bit/dim 3.7348(3.7459) | Xent 0.8161(0.8683) | Loss 120.3033(179.9735) | Error 0.2900(0.3096) Steps 0(0.00) | Grad Norm 7.8486(10.5775) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.5488(15.1980) | Bit/dim 3.7210(3.7458) | Xent 0.8714(0.8682) | Loss 123.9788(164.7859) | Error 0.3089(0.3102) Steps 0(0.00) | Grad Norm 9.1975(10.9183) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.2631(15.2309) | Bit/dim 3.7656(3.7447) | Xent 0.8998(0.8671) | Loss 121.5712(153.3794) | Error 0.3200(0.3097) Steps 0(0.00) | Grad Norm 12.1133(10.9244) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.2410(15.2255) | Bit/dim 3.6928(3.7401) | Xent 0.9016(0.8601) | Loss 127.0005(145.3078) | Error 0.3178(0.3073) Steps 0(0.00) | Grad Norm 9.9258(10.4702) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.6099(15.2748) | Bit/dim 3.7214(3.7372) | Xent 0.8883(0.8623) | Loss 122.1116(139.5734) | Error 0.3200(0.3088) Steps 0(0.00) | Grad Norm 11.0695(10.7288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 82.5123, Epoch Time 940.1864(907.8979), Bit/dim 3.7372(best: 3.7351), Xent 0.8639, Loss 4.1691, Error 0.3050(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.0290(15.3221) | Bit/dim 3.7269(3.7354) | Xent 0.9033(0.8577) | Loss 122.4451(191.1242) | Error 0.3322(0.3066) Steps 0(0.00) | Grad Norm 8.2145(10.5699) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 14.8265(15.3395) | Bit/dim 3.7284(3.7321) | Xent 0.8170(0.8522) | Loss 122.7887(173.2996) | Error 0.2822(0.3048) Steps 0(0.00) | Grad Norm 8.2105(10.0014) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.2665(15.3381) | Bit/dim 3.7792(3.7377) | Xent 0.8245(0.8479) | Loss 120.6557(159.9921) | Error 0.2989(0.3033) Steps 0(0.00) | Grad Norm 9.7209(10.0868) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.4662(15.4111) | Bit/dim 3.7355(3.7364) | Xent 0.8918(0.8477) | Loss 121.4123(150.7626) | Error 0.3167(0.3011) Steps 0(0.00) | Grad Norm 11.3786(9.9725) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.5467(15.4463) | Bit/dim 3.7137(3.7343) | Xent 0.8002(0.8402) | Loss 127.3303(143.7652) | Error 0.2833(0.2984) Steps 0(0.00) | Grad Norm 10.4884(9.7984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 83.3274, Epoch Time 951.5075(909.2062), Bit/dim 3.7303(best: 3.7351), Xent 0.8794, Loss 4.1700, Error 0.3065(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.7511(15.3848) | Bit/dim 3.7209(3.7313) | Xent 0.7885(0.8360) | Loss 117.9696(201.3676) | Error 0.2678(0.2965) Steps 0(0.00) | Grad Norm 10.8995(9.9279) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.1735(15.4646) | Bit/dim 3.7064(3.7305) | Xent 0.8081(0.8278) | Loss 123.9417(181.1666) | Error 0.2867(0.2945) Steps 0(0.00) | Grad Norm 6.0359(9.3448) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 16.0638(15.5321) | Bit/dim 3.7020(3.7287) | Xent 0.8809(0.8256) | Loss 125.2374(166.2796) | Error 0.3144(0.2945) Steps 0(0.00) | Grad Norm 6.6279(9.0762) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.1463(15.5555) | Bit/dim 3.7199(3.7260) | Xent 0.8511(0.8315) | Loss 127.2694(155.3929) | Error 0.3122(0.2971) Steps 0(0.00) | Grad Norm 6.6201(9.5446) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.4011(15.5510) | Bit/dim 3.7048(3.7259) | Xent 0.9040(0.8403) | Loss 122.9481(147.0714) | Error 0.3144(0.2998) Steps 0(0.00) | Grad Norm 9.7396(9.8609) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.0915(15.5624) | Bit/dim 3.7742(3.7280) | Xent 0.8197(0.8418) | Loss 121.1388(140.9860) | Error 0.3044(0.3006) Steps 0(0.00) | Grad Norm 9.4290(10.0220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 83.9444, Epoch Time 960.2778(910.7384), Bit/dim 3.7319(best: 3.7303), Xent 0.8673, Loss 4.1656, Error 0.3115(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 14.8451(15.5223) | Bit/dim 3.7031(3.7270) | Xent 0.8224(0.8425) | Loss 116.5193(191.6158) | Error 0.2856(0.3019) Steps 0(0.00) | Grad Norm 11.7543(10.1742) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.9387(15.5032) | Bit/dim 3.7342(3.7273) | Xent 0.7987(0.8316) | Loss 126.7232(174.0915) | Error 0.2833(0.2981) Steps 0(0.00) | Grad Norm 5.2822(10.0940) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.6384(15.5128) | Bit/dim 3.7392(3.7257) | Xent 0.8490(0.8357) | Loss 119.5404(160.6494) | Error 0.2878(0.2990) Steps 0(0.00) | Grad Norm 6.7217(10.2259) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.1953(15.5697) | Bit/dim 3.6839(3.7249) | Xent 0.8920(0.8379) | Loss 130.5206(151.3379) | Error 0.3133(0.3008) Steps 0(0.00) | Grad Norm 10.2101(10.2006) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.2067(15.5551) | Bit/dim 3.7430(3.7240) | Xent 0.9425(0.8408) | Loss 117.5579(144.8433) | Error 0.3356(0.3009) Steps 0(0.00) | Grad Norm 12.3314(10.3943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 82.1608, Epoch Time 954.2016(912.0423), Bit/dim 3.7234(best: 3.7303), Xent 0.8735, Loss 4.1601, Error 0.3142(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.1787(15.5468) | Bit/dim 3.7404(3.7275) | Xent 0.8283(0.8408) | Loss 123.5410(202.4825) | Error 0.2767(0.3007) Steps 0(0.00) | Grad Norm 7.8334(10.4512) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.1648(15.5132) | Bit/dim 3.7058(3.7271) | Xent 0.7900(0.8305) | Loss 122.8030(181.3014) | Error 0.2822(0.2966) Steps 0(0.00) | Grad Norm 5.9800(9.6563) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.3711(15.5347) | Bit/dim 3.6979(3.7238) | Xent 0.7878(0.8202) | Loss 113.7447(166.0571) | Error 0.2778(0.2928) Steps 0(0.00) | Grad Norm 4.0371(9.2052) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.0966(15.4665) | Bit/dim 3.7469(3.7213) | Xent 0.8735(0.8219) | Loss 126.3030(155.2366) | Error 0.3078(0.2941) Steps 0(0.00) | Grad Norm 8.6003(9.0228) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.7290(15.4884) | Bit/dim 3.7268(3.7241) | Xent 0.8240(0.8223) | Loss 124.6042(147.0725) | Error 0.2867(0.2932) Steps 0(0.00) | Grad Norm 6.7219(8.9023) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.4673(15.5170) | Bit/dim 3.7266(3.7230) | Xent 0.8437(0.8208) | Loss 128.3957(141.3810) | Error 0.2967(0.2923) Steps 0(0.00) | Grad Norm 14.3362(9.5175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 85.5669, Epoch Time 956.4981(913.3759), Bit/dim 3.7185(best: 3.7234), Xent 0.8233, Loss 4.1302, Error 0.2897(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.2561(15.5202) | Bit/dim 3.7395(3.7217) | Xent 0.8412(0.8124) | Loss 118.0162(193.3904) | Error 0.3078(0.2899) Steps 0(0.00) | Grad Norm 9.8183(9.4916) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.6633(15.5615) | Bit/dim 3.7488(3.7197) | Xent 0.7861(0.8119) | Loss 121.6786(175.6452) | Error 0.2789(0.2906) Steps 0(0.00) | Grad Norm 13.2952(9.7742) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 17.1793(15.6524) | Bit/dim 3.7479(3.7202) | Xent 0.8536(0.8160) | Loss 121.2525(162.4267) | Error 0.3044(0.2918) Steps 0(0.00) | Grad Norm 9.0436(10.2107) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.5780(15.6536) | Bit/dim 3.7643(3.7225) | Xent 0.8547(0.8212) | Loss 125.5390(152.3904) | Error 0.3044(0.2944) Steps 0(0.00) | Grad Norm 10.6408(10.4770) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.6255(15.6150) | Bit/dim 3.7160(3.7205) | Xent 0.7864(0.8234) | Loss 129.5361(145.2302) | Error 0.2811(0.2959) Steps 0(0.00) | Grad Norm 6.9620(10.4876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 84.9098, Epoch Time 963.9950(914.8945), Bit/dim 3.7182(best: 3.7185), Xent 0.8553, Loss 4.1458, Error 0.3026(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.2994(15.5405) | Bit/dim 3.6749(3.7217) | Xent 0.8519(0.8261) | Loss 126.3985(205.6069) | Error 0.2978(0.2966) Steps 0(0.00) | Grad Norm 11.0920(10.6369) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.6275(15.5806) | Bit/dim 3.7130(3.7218) | Xent 0.8761(0.8261) | Loss 123.1542(184.5762) | Error 0.3111(0.2969) Steps 0(0.00) | Grad Norm 9.3792(10.2914) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.5604(15.6053) | Bit/dim 3.7281(3.7195) | Xent 0.8243(0.8191) | Loss 128.3666(168.5839) | Error 0.2978(0.2948) Steps 0(0.00) | Grad Norm 9.1493(9.7625) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.3799(15.5772) | Bit/dim 3.7056(3.7183) | Xent 0.8328(0.8217) | Loss 119.6625(156.4458) | Error 0.3144(0.2961) Steps 0(0.00) | Grad Norm 10.3202(9.8715) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.0904(15.5270) | Bit/dim 3.7418(3.7207) | Xent 0.7788(0.8200) | Loss 117.7184(147.4381) | Error 0.2900(0.2941) Steps 0(0.00) | Grad Norm 8.0104(9.7804) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.3633(15.5583) | Bit/dim 3.7316(3.7191) | Xent 0.7725(0.8232) | Loss 123.5781(141.2462) | Error 0.2789(0.2954) Steps 0(0.00) | Grad Norm 12.7854(10.0462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 83.3947, Epoch Time 955.9249(916.1254), Bit/dim 3.7220(best: 3.7182), Xent 0.8404, Loss 4.1422, Error 0.2960(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.6618(15.5260) | Bit/dim 3.7177(3.7156) | Xent 0.7461(0.8104) | Loss 124.1008(190.0378) | Error 0.2722(0.2918) Steps 0(0.00) | Grad Norm 5.0259(9.2734) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.1069(15.5530) | Bit/dim 3.7146(3.7155) | Xent 0.7554(0.8025) | Loss 124.7240(172.9574) | Error 0.2689(0.2874) Steps 0(0.00) | Grad Norm 4.3865(8.9218) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.1684(15.5868) | Bit/dim 3.6954(3.7127) | Xent 0.8968(0.8069) | Loss 123.6546(159.7363) | Error 0.3267(0.2899) Steps 0(0.00) | Grad Norm 16.0615(9.1178) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.0568(15.5635) | Bit/dim 3.6959(3.7139) | Xent 0.7748(0.7990) | Loss 125.9830(150.9551) | Error 0.2856(0.2870) Steps 0(0.00) | Grad Norm 7.0909(9.4561) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.9901(15.5912) | Bit/dim 3.7217(3.7151) | Xent 0.7860(0.7953) | Loss 127.6773(144.7433) | Error 0.2756(0.2840) Steps 0(0.00) | Grad Norm 7.6631(8.6166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 84.8210, Epoch Time 961.5099(917.4869), Bit/dim 3.7120(best: 3.7182), Xent 0.8589, Loss 4.1415, Error 0.3033(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.6618(15.6392) | Bit/dim 3.6853(3.7161) | Xent 0.7781(0.7931) | Loss 124.2706(205.5594) | Error 0.2778(0.2832) Steps 0(0.00) | Grad Norm 12.3726(8.9084) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.8678(15.6826) | Bit/dim 3.7207(3.7161) | Xent 0.7829(0.7845) | Loss 123.8069(184.0993) | Error 0.2756(0.2795) Steps 0(0.00) | Grad Norm 7.1626(8.8582) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.2173(15.6530) | Bit/dim 3.7345(3.7141) | Xent 0.7632(0.7915) | Loss 124.0743(168.0938) | Error 0.2822(0.2828) Steps 0(0.00) | Grad Norm 5.8245(9.1995) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.8021(15.6437) | Bit/dim 3.7118(3.7133) | Xent 0.7701(0.7878) | Loss 127.7797(156.3491) | Error 0.2711(0.2830) Steps 0(0.00) | Grad Norm 5.9998(8.7177) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.5111(15.6854) | Bit/dim 3.7055(3.7128) | Xent 0.8209(0.7851) | Loss 123.8473(147.8069) | Error 0.3033(0.2821) Steps 0(0.00) | Grad Norm 6.4338(8.3478) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.0303(15.7063) | Bit/dim 3.7250(3.7122) | Xent 0.8770(0.7894) | Loss 123.6018(141.3463) | Error 0.3100(0.2834) Steps 0(0.00) | Grad Norm 10.1167(8.5740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 84.5343, Epoch Time 967.1334(918.9763), Bit/dim 3.7087(best: 3.7120), Xent 0.8157, Loss 4.1166, Error 0.2921(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 14.3751(15.7073) | Bit/dim 3.7227(3.7121) | Xent 0.7356(0.7823) | Loss 122.3257(191.9706) | Error 0.2622(0.2801) Steps 0(0.00) | Grad Norm 5.8322(8.5952) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 15.6509(15.6536) | Bit/dim 3.7221(3.7108) | Xent 0.8370(0.7821) | Loss 122.2283(173.9948) | Error 0.3022(0.2809) Steps 0(0.00) | Grad Norm 12.7616(9.2326) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.0041(15.6145) | Bit/dim 3.7204(3.7122) | Xent 0.7751(0.7823) | Loss 120.9603(160.7373) | Error 0.2744(0.2796) Steps 0(0.00) | Grad Norm 12.6047(9.3079) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 15.7749(15.5688) | Bit/dim 3.6830(3.7100) | Xent 0.8243(0.7826) | Loss 130.2463(151.0918) | Error 0.2944(0.2789) Steps 0(0.00) | Grad Norm 16.9076(9.4079) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 15.7809(15.5915) | Bit/dim 3.7105(3.7087) | Xent 0.7781(0.7948) | Loss 126.4084(143.9302) | Error 0.2878(0.2842) Steps 0(0.00) | Grad Norm 8.6461(9.6531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 85.2967, Epoch Time 960.7308(920.2290), Bit/dim 3.7176(best: 3.7087), Xent 0.8312, Loss 4.1332, Error 0.2959(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.2850(15.7125) | Bit/dim 3.7360(3.7118) | Xent 0.7827(0.7956) | Loss 124.3532(206.9083) | Error 0.2944(0.2853) Steps 0(0.00) | Grad Norm 9.0119(9.5457) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.2942(15.6985) | Bit/dim 3.6984(3.7106) | Xent 0.7338(0.7931) | Loss 122.7871(185.8956) | Error 0.2489(0.2844) Steps 0(0.00) | Grad Norm 4.9404(9.5258) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.4938(15.6262) | Bit/dim 3.7200(3.7084) | Xent 0.7443(0.7854) | Loss 124.9617(169.8485) | Error 0.2867(0.2816) Steps 0(0.00) | Grad Norm 8.1869(9.2584) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.3930(15.6021) | Bit/dim 3.7153(3.7068) | Xent 0.7953(0.7818) | Loss 130.8854(158.0336) | Error 0.2878(0.2808) Steps 0(0.00) | Grad Norm 7.8957(8.6069) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 14.8081(15.6164) | Bit/dim 3.6971(3.7056) | Xent 0.7880(0.7832) | Loss 125.8492(149.7969) | Error 0.2911(0.2811) Steps 0(0.00) | Grad Norm 8.7052(8.8279) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.7806(15.6048) | Bit/dim 3.7355(3.7071) | Xent 0.8055(0.7885) | Loss 127.1602(143.6795) | Error 0.2833(0.2838) Steps 0(0.00) | Grad Norm 12.9838(10.2341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 84.0636, Epoch Time 961.9743(921.4813), Bit/dim 3.7144(best: 3.7087), Xent 0.8347, Loss 4.1318, Error 0.2909(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.3022(15.6722) | Bit/dim 3.6570(3.7087) | Xent 0.7223(0.7857) | Loss 127.1457(193.7869) | Error 0.2500(0.2821) Steps 0(0.00) | Grad Norm 6.1520(9.9052) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.8999(15.6724) | Bit/dim 3.6667(3.7087) | Xent 0.7178(0.7747) | Loss 129.5393(176.1305) | Error 0.2489(0.2786) Steps 0(0.00) | Grad Norm 7.4545(9.6670) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.7745(15.6530) | Bit/dim 3.7272(3.7083) | Xent 0.8466(0.7719) | Loss 127.5239(163.4001) | Error 0.2956(0.2771) Steps 0(0.00) | Grad Norm 11.3457(9.4533) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.9585(15.6838) | Bit/dim 3.6863(3.7076) | Xent 0.8471(0.7796) | Loss 126.7796(153.4668) | Error 0.2989(0.2796) Steps 0(0.00) | Grad Norm 11.9537(9.7011) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.3979(15.6950) | Bit/dim 3.7114(3.7046) | Xent 0.7327(0.7763) | Loss 127.8617(146.5744) | Error 0.2633(0.2783) Steps 0(0.00) | Grad Norm 8.0756(9.7975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 85.7591, Epoch Time 971.4365(922.9800), Bit/dim 3.7105(best: 3.7087), Xent 0.8180, Loss 4.1195, Error 0.2924(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 14.9077(15.7198) | Bit/dim 3.7292(3.7052) | Xent 0.8166(0.7830) | Loss 125.7177(206.7842) | Error 0.2956(0.2812) Steps 0(0.00) | Grad Norm 9.3024(10.1541) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.6373(15.6731) | Bit/dim 3.7011(3.7034) | Xent 0.7587(0.7787) | Loss 127.9435(185.6336) | Error 0.2811(0.2794) Steps 0(0.00) | Grad Norm 9.1527(10.0604) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 16.8254(15.6756) | Bit/dim 3.6999(3.7045) | Xent 0.7213(0.7735) | Loss 130.9527(170.2753) | Error 0.2778(0.2787) Steps 0(0.00) | Grad Norm 8.0825(10.1366) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.7511(15.6424) | Bit/dim 3.7099(3.7069) | Xent 0.7841(0.7712) | Loss 126.4532(158.5385) | Error 0.2833(0.2779) Steps 0(0.00) | Grad Norm 10.4745(9.5727) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.9269(15.6404) | Bit/dim 3.7033(3.7051) | Xent 0.7629(0.7738) | Loss 128.1794(150.0142) | Error 0.2900(0.2784) Steps 0(0.00) | Grad Norm 5.1073(9.7711) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.0505(15.5797) | Bit/dim 3.7051(3.7009) | Xent 0.7891(0.7754) | Loss 122.5969(143.6278) | Error 0.2856(0.2793) Steps 0(0.00) | Grad Norm 12.0600(10.0675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 86.4622, Epoch Time 959.9864(924.0902), Bit/dim 3.7014(best: 3.7087), Xent 0.8346, Loss 4.1187, Error 0.2957(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.2661(15.5968) | Bit/dim 3.6960(3.7044) | Xent 0.7841(0.7711) | Loss 125.1385(197.7823) | Error 0.2856(0.2776) Steps 0(0.00) | Grad Norm 15.1825(10.2831) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 14.4705(15.5327) | Bit/dim 3.6910(3.7023) | Xent 0.7009(0.7635) | Loss 122.6017(179.1342) | Error 0.2600(0.2748) Steps 0(0.00) | Grad Norm 4.8515(9.8066) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.0103(15.5223) | Bit/dim 3.7227(3.7017) | Xent 0.7155(0.7585) | Loss 133.7459(165.6738) | Error 0.2467(0.2734) Steps 0(0.00) | Grad Norm 8.9692(9.2051) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 16.1813(15.5448) | Bit/dim 3.6800(3.6980) | Xent 0.8514(0.7663) | Loss 128.6917(155.6852) | Error 0.3056(0.2731) Steps 0(0.00) | Grad Norm 17.5254(9.5407) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 16.7577(15.6048) | Bit/dim 3.7235(3.6994) | Xent 0.7552(0.7790) | Loss 124.3228(147.8409) | Error 0.2644(0.2760) Steps 0(0.00) | Grad Norm 5.8400(10.2662) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 85.4409, Epoch Time 962.0868(925.2301), Bit/dim 3.6952(best: 3.7014), Xent 0.8025, Loss 4.0965, Error 0.2815(best: 0.2897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 15.6894(15.6810) | Bit/dim 3.7040(3.7025) | Xent 0.6592(0.7677) | Loss 129.5584(207.2057) | Error 0.2356(0.2727) Steps 0(0.00) | Grad Norm 7.8482(9.5753) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.3104(15.7204) | Bit/dim 3.7260(3.7011) | Xent 0.7382(0.7689) | Loss 129.0691(186.4655) | Error 0.2600(0.2743) Steps 0(0.00) | Grad Norm 6.3678(9.4521) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 15.5666(15.7475) | Bit/dim 3.6755(3.6980) | Xent 0.7193(0.7654) | Loss 128.6692(170.9465) | Error 0.2533(0.2734) Steps 0(0.00) | Grad Norm 11.1034(9.2140) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.2756(15.7474) | Bit/dim 3.7079(3.6983) | Xent 0.7439(0.7563) | Loss 129.4222(159.6106) | Error 0.2700(0.2705) Steps 0(0.00) | Grad Norm 7.3810(8.9849) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.4513(15.7984) | Bit/dim 3.6795(3.6975) | Xent 0.7010(0.7571) | Loss 125.8538(150.5213) | Error 0.2633(0.2722) Steps 0(0.00) | Grad Norm 8.4853(8.6559) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 14.9175(15.7144) | Bit/dim 3.6911(3.6968) | Xent 0.7594(0.7563) | Loss 121.5633(143.5776) | Error 0.2822(0.2713) Steps 0(0.00) | Grad Norm 7.1907(8.4689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 84.6854, Epoch Time 970.8128(926.5976), Bit/dim 3.6982(best: 3.6952), Xent 0.7737, Loss 4.0850, Error 0.2761(best: 0.2815)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.7863(15.7135) | Bit/dim 3.7149(3.6953) | Xent 0.7630(0.7477) | Loss 128.9200(194.3859) | Error 0.2722(0.2685) Steps 0(0.00) | Grad Norm 12.6711(8.5279) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.0413(15.7415) | Bit/dim 3.7111(3.6974) | Xent 0.7476(0.7476) | Loss 124.3812(176.6379) | Error 0.2800(0.2688) Steps 0(0.00) | Grad Norm 8.2917(8.9322) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.4005(15.7572) | Bit/dim 3.7069(3.6977) | Xent 0.7135(0.7422) | Loss 129.5687(163.4658) | Error 0.2578(0.2668) Steps 0(0.00) | Grad Norm 8.3302(8.7668) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.2267(15.7254) | Bit/dim 3.6432(3.6949) | Xent 0.7365(0.7418) | Loss 128.9426(153.3611) | Error 0.2744(0.2655) Steps 0(0.00) | Grad Norm 5.3301(8.5625) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 14.7885(15.7312) | Bit/dim 3.6923(3.6947) | Xent 0.7215(0.7402) | Loss 126.1871(146.1772) | Error 0.2633(0.2647) Steps 0(0.00) | Grad Norm 10.3512(8.8297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 85.4109, Epoch Time 969.3542(927.8803), Bit/dim 3.6932(best: 3.6952), Xent 0.7962, Loss 4.0913, Error 0.2822(best: 0.2761)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 15.5581(15.7192) | Bit/dim 3.6812(3.6961) | Xent 0.7089(0.7418) | Loss 122.8391(208.0622) | Error 0.2622(0.2654) Steps 0(0.00) | Grad Norm 9.2262(8.8480) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.2592(15.7543) | Bit/dim 3.7103(3.6972) | Xent 0.6990(0.7424) | Loss 123.0807(186.4106) | Error 0.2478(0.2661) Steps 0(0.00) | Grad Norm 6.1473(9.1945) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.3759(15.7458) | Bit/dim 3.6955(3.6946) | Xent 0.7886(0.7385) | Loss 122.9436(170.7623) | Error 0.2722(0.2648) Steps 0(0.00) | Grad Norm 8.0011(9.3892) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.4954(15.7629) | Bit/dim 3.7162(3.6932) | Xent 0.6896(0.7384) | Loss 125.2463(159.2920) | Error 0.2556(0.2659) Steps 0(0.00) | Grad Norm 6.7104(9.1296) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.4464(15.6967) | Bit/dim 3.7188(3.6932) | Xent 0.7680(0.7363) | Loss 127.5579(150.9051) | Error 0.2756(0.2643) Steps 0(0.00) | Grad Norm 8.3251(8.5162) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 14.9564(15.6493) | Bit/dim 3.7365(3.6948) | Xent 0.7997(0.7420) | Loss 127.9196(144.5684) | Error 0.2778(0.2670) Steps 0(0.00) | Grad Norm 11.7761(9.3357) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 86.3710, Epoch Time 967.7370(929.0760), Bit/dim 3.7016(best: 3.6932), Xent 0.7912, Loss 4.0972, Error 0.2797(best: 0.2761)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.5190(15.6869) | Bit/dim 3.6567(3.6924) | Xent 0.7343(0.7436) | Loss 128.1960(199.2540) | Error 0.2556(0.2674) Steps 0(0.00) | Grad Norm 5.8832(9.1315) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.7037(15.7725) | Bit/dim 3.6751(3.6914) | Xent 0.7130(0.7394) | Loss 121.3381(180.4609) | Error 0.2656(0.2648) Steps 0(0.00) | Grad Norm 5.5809(8.8734) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.6852(15.8288) | Bit/dim 3.7214(3.6924) | Xent 0.7049(0.7342) | Loss 129.5875(167.2921) | Error 0.2656(0.2619) Steps 0(0.00) | Grad Norm 7.8301(9.1610) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 15.7866(15.8675) | Bit/dim 3.7074(3.6925) | Xent 0.7583(0.7380) | Loss 129.5512(156.9519) | Error 0.2622(0.2632) Steps 0(0.00) | Grad Norm 10.0099(9.0279) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.5385(15.8153) | Bit/dim 3.7104(3.6932) | Xent 0.6819(0.7370) | Loss 124.7769(148.9535) | Error 0.2267(0.2615) Steps 0(0.00) | Grad Norm 7.4760(8.7159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 84.6777, Epoch Time 977.2100(930.5200), Bit/dim 3.6913(best: 3.6932), Xent 0.8136, Loss 4.0981, Error 0.2834(best: 0.2761)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 15.1593(15.8271) | Bit/dim 3.7137(3.6921) | Xent 0.7332(0.7335) | Loss 127.4477(208.2466) | Error 0.2533(0.2600) Steps 0(0.00) | Grad Norm 10.0433(8.5440) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 15.3898(15.8517) | Bit/dim 3.6951(3.6915) | Xent 0.7728(0.7325) | Loss 122.4060(186.4358) | Error 0.2822(0.2610) Steps 0(0.00) | Grad Norm 11.1057(9.5850) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.2279(15.8159) | Bit/dim 3.6882(3.6920) | Xent 0.7953(0.7438) | Loss 125.8393(170.7228) | Error 0.2811(0.2639) Steps 0(0.00) | Grad Norm 8.9594(10.2891) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 15.1912(15.7567) | Bit/dim 3.6764(3.6927) | Xent 0.7657(0.7424) | Loss 127.5839(159.2311) | Error 0.2767(0.2639) Steps 0(0.00) | Grad Norm 9.6796(9.7560) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 16.8010(15.7981) | Bit/dim 3.6724(3.6936) | Xent 0.7105(0.7411) | Loss 128.8584(150.9083) | Error 0.2444(0.2637) Steps 0(0.00) | Grad Norm 9.0013(9.1664) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 15.7123(15.7939) | Bit/dim 3.7198(3.6935) | Xent 0.7297(0.7392) | Loss 129.3229(144.5909) | Error 0.2600(0.2637) Steps 0(0.00) | Grad Norm 11.1342(9.1227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 85.5065, Epoch Time 972.5986(931.7823), Bit/dim 3.6896(best: 3.6913), Xent 0.7821, Loss 4.0806, Error 0.2738(best: 0.2761)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 15.9188(15.8345) | Bit/dim 3.7058(3.6941) | Xent 0.8108(0.7351) | Loss 126.2271(196.8577) | Error 0.2856(0.2623) Steps 0(0.00) | Grad Norm 9.3062(9.6458) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 15.2122(15.7952) | Bit/dim 3.6746(3.6938) | Xent 0.6698(0.7356) | Loss 122.8727(177.9928) | Error 0.2478(0.2630) Steps 0(0.00) | Grad Norm 6.2418(9.6962) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 15.6398(15.7654) | Bit/dim 3.7375(3.6942) | Xent 0.7367(0.7380) | Loss 128.1859(164.2382) | Error 0.2622(0.2641) Steps 0(0.00) | Grad Norm 9.8239(9.7301) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.0539(15.7255) | Bit/dim 3.6895(3.6906) | Xent 0.6832(0.7403) | Loss 126.7467(154.0269) | Error 0.2333(0.2649) Steps 0(0.00) | Grad Norm 6.4239(9.3825) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 15.1673(15.7745) | Bit/dim 3.6857(3.6901) | Xent 0.6801(0.7299) | Loss 130.2283(146.7968) | Error 0.2456(0.2608) Steps 0(0.00) | Grad Norm 4.7729(8.5576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 86.7580, Epoch Time 973.3150(933.0283), Bit/dim 3.6862(best: 3.6896), Xent 0.7608, Loss 4.0666, Error 0.2670(best: 0.2738)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 15.8219(15.7945) | Bit/dim 3.6416(3.6867) | Xent 0.7319(0.7274) | Loss 128.4244(209.1455) | Error 0.2589(0.2601) Steps 0(0.00) | Grad Norm 8.3273(8.5182) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.2563(15.8442) | Bit/dim 3.6882(3.6890) | Xent 0.7918(0.7202) | Loss 134.5771(188.4695) | Error 0.2756(0.2568) Steps 0(0.00) | Grad Norm 10.2482(8.6537) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.0782(15.7726) | Bit/dim 3.7207(3.6899) | Xent 0.7197(0.7130) | Loss 125.1671(172.6306) | Error 0.2522(0.2547) Steps 0(0.00) | Grad Norm 4.4198(8.1604) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 14.6430(15.7713) | Bit/dim 3.6785(3.6875) | Xent 0.7229(0.7169) | Loss 125.1404(160.6044) | Error 0.2378(0.2560) Steps 0(0.00) | Grad Norm 8.7502(8.0614) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 15.6942(15.7641) | Bit/dim 3.7071(3.6878) | Xent 0.7258(0.7178) | Loss 129.4541(152.4929) | Error 0.2567(0.2569) Steps 0(0.00) | Grad Norm 10.0662(8.6313) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 15.6527(15.7669) | Bit/dim 3.6835(3.6871) | Xent 0.6988(0.7190) | Loss 130.3336(145.7412) | Error 0.2556(0.2573) Steps 0(0.00) | Grad Norm 7.3389(8.7578) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 87.6096, Epoch Time 973.8031(934.2516), Bit/dim 3.6814(best: 3.6862), Xent 0.7690, Loss 4.0659, Error 0.2680(best: 0.2670)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 15.7626(15.7755) | Bit/dim 3.6688(3.6839) | Xent 0.6293(0.7049) | Loss 128.5960(200.5073) | Error 0.2344(0.2530) Steps 0(0.00) | Grad Norm 5.6903(8.5626) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 16.4654(15.8162) | Bit/dim 3.6628(3.6832) | Xent 0.8199(0.7080) | Loss 128.4142(181.6479) | Error 0.3033(0.2546) Steps 0(0.00) | Grad Norm 17.4798(8.9238) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 10.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
