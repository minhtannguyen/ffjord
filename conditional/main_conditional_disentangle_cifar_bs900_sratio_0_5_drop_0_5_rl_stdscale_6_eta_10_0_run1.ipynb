{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=10.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4350 | Time 17.5763(18.2211) | Bit/dim 3.6632(3.6957) | Xent 0.6905(0.7259) | Loss 121.5723(205.5460) | Error 0.2467(0.2592) Steps 0(0.00) | Grad Norm 6.3038(9.0314) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.9551(18.1662) | Bit/dim 3.6814(3.6942) | Xent 0.6826(0.7209) | Loss 124.8669(184.5176) | Error 0.2367(0.2570) Steps 0(0.00) | Grad Norm 6.8300(8.3940) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 19.0910(18.1074) | Bit/dim 3.6589(3.6940) | Xent 0.7261(0.7192) | Loss 123.9249(168.6230) | Error 0.2544(0.2570) Steps 0(0.00) | Grad Norm 5.5485(8.8512) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.4806(17.8621) | Bit/dim 3.6894(3.6922) | Xent 0.7432(0.7123) | Loss 115.6604(156.7784) | Error 0.2611(0.2546) Steps 0(0.00) | Grad Norm 8.3745(8.7092) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 17.4413(17.7446) | Bit/dim 3.6453(3.6892) | Xent 0.6417(0.7008) | Loss 131.0320(148.2466) | Error 0.2389(0.2504) Steps 0(0.00) | Grad Norm 6.9426(8.3471) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 17.9439(17.5210) | Bit/dim 3.6871(3.6903) | Xent 0.7595(0.7036) | Loss 119.7051(140.9122) | Error 0.2667(0.2512) Steps 0(0.00) | Grad Norm 10.5468(8.5367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 94.0172, Epoch Time 1098.3944(1041.0522), Bit/dim 3.6935(best: inf), Xent 0.8156, Loss 4.1014, Error 0.2811(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 17.6789(17.4181) | Bit/dim 3.6654(3.6879) | Xent 0.7356(0.7092) | Loss 120.6163(192.8862) | Error 0.2567(0.2525) Steps 0(0.00) | Grad Norm 11.1652(9.1135) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 17.8084(17.4241) | Bit/dim 3.6980(3.6907) | Xent 0.7618(0.7123) | Loss 121.6186(175.2976) | Error 0.2633(0.2532) Steps 0(0.00) | Grad Norm 11.3580(9.2284) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 16.9937(17.3917) | Bit/dim 3.6932(3.6914) | Xent 0.6790(0.7069) | Loss 124.3787(162.2917) | Error 0.2389(0.2512) Steps 0(0.00) | Grad Norm 5.9552(8.8009) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.5511(17.2856) | Bit/dim 3.6805(3.6913) | Xent 0.6919(0.7031) | Loss 120.5988(151.5254) | Error 0.2389(0.2515) Steps 0(0.00) | Grad Norm 10.7751(8.9862) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.3531(17.2618) | Bit/dim 3.6966(3.6912) | Xent 0.7346(0.7006) | Loss 127.4670(143.8992) | Error 0.2678(0.2508) Steps 0(0.00) | Grad Norm 6.5873(8.5201) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 89.5951, Epoch Time 1055.8662(1041.4966), Bit/dim 3.6862(best: 3.6935), Xent 0.7619, Loss 4.0671, Error 0.2671(best: 0.2811)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.2757(17.1903) | Bit/dim 3.6704(3.6899) | Xent 0.6462(0.6965) | Loss 124.8152(205.7920) | Error 0.2389(0.2500) Steps 0(0.00) | Grad Norm 5.8873(8.3844) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 16.4250(17.2381) | Bit/dim 3.7151(3.6912) | Xent 0.6603(0.6934) | Loss 112.7406(184.8843) | Error 0.2389(0.2488) Steps 0(0.00) | Grad Norm 9.5135(8.3094) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 17.4189(17.2809) | Bit/dim 3.6972(3.6907) | Xent 0.7353(0.6920) | Loss 126.0194(168.9191) | Error 0.2544(0.2470) Steps 0(0.00) | Grad Norm 9.4448(8.3380) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 18.4746(17.3922) | Bit/dim 3.6980(3.6876) | Xent 0.8121(0.7003) | Loss 126.3575(156.9918) | Error 0.2889(0.2501) Steps 0(0.00) | Grad Norm 13.0295(8.6127) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.6281(17.4207) | Bit/dim 3.7155(3.6887) | Xent 0.6963(0.7008) | Loss 124.8948(148.8136) | Error 0.2511(0.2495) Steps 0(0.00) | Grad Norm 8.8301(8.5314) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 18.2927(17.4758) | Bit/dim 3.6748(3.6853) | Xent 0.7181(0.7000) | Loss 125.5255(142.6644) | Error 0.2500(0.2495) Steps 0(0.00) | Grad Norm 9.2236(8.3481) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 90.0480, Epoch Time 1065.7509(1042.2243), Bit/dim 3.6890(best: 3.6862), Xent 0.7805, Loss 4.0792, Error 0.2725(best: 0.2671)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 17.5311(17.5321) | Bit/dim 3.6978(3.6890) | Xent 0.6671(0.6989) | Loss 125.6315(195.3007) | Error 0.2256(0.2482) Steps 0(0.00) | Grad Norm 6.6087(8.7220) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 18.2781(17.5833) | Bit/dim 3.6509(3.6882) | Xent 0.6736(0.6964) | Loss 126.8510(176.9870) | Error 0.2256(0.2479) Steps 0(0.00) | Grad Norm 7.8106(8.4553) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 20.0452(17.5743) | Bit/dim 3.7079(3.6898) | Xent 0.7147(0.6932) | Loss 127.3697(163.2819) | Error 0.2644(0.2460) Steps 0(0.00) | Grad Norm 7.1750(8.3661) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 17.2443(17.4435) | Bit/dim 3.6612(3.6893) | Xent 0.6896(0.6940) | Loss 112.2981(152.1651) | Error 0.2600(0.2464) Steps 0(0.00) | Grad Norm 9.8791(8.5811) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 17.6622(17.6600) | Bit/dim 3.6921(3.6858) | Xent 0.6787(0.6931) | Loss 125.7348(145.6178) | Error 0.2344(0.2464) Steps 0(0.00) | Grad Norm 10.3143(8.6317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 87.4863, Epoch Time 1078.5635(1043.3144), Bit/dim 3.6824(best: 3.6862), Xent 0.7502, Loss 4.0575, Error 0.2604(best: 0.2671)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 19.1045(17.7974) | Bit/dim 3.6587(3.6785) | Xent 0.7004(0.6917) | Loss 130.4747(202.6332) | Error 0.2489(0.2454) Steps 0(0.00) | Grad Norm 10.7400(9.1686) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 20.0268(17.8487) | Bit/dim 3.7225(3.6836) | Xent 0.7123(0.6865) | Loss 126.3211(181.8463) | Error 0.2478(0.2429) Steps 0(0.00) | Grad Norm 7.5913(8.9282) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 19.0345(17.7828) | Bit/dim 3.7081(3.6849) | Xent 0.6739(0.6925) | Loss 126.6953(166.9041) | Error 0.2456(0.2457) Steps 0(0.00) | Grad Norm 5.9559(9.0878) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 16.9218(17.7141) | Bit/dim 3.7021(3.6852) | Xent 0.7648(0.7013) | Loss 117.6639(155.7388) | Error 0.2800(0.2497) Steps 0(0.00) | Grad Norm 9.6427(9.6062) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 21.8272(17.7270) | Bit/dim 3.7087(3.6852) | Xent 0.7344(0.7051) | Loss 137.6038(148.0308) | Error 0.2556(0.2511) Steps 0(0.00) | Grad Norm 5.9292(9.3396) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.5975(17.6431) | Bit/dim 3.6808(3.6826) | Xent 0.7043(0.7013) | Loss 126.5982(142.2358) | Error 0.2556(0.2504) Steps 0(0.00) | Grad Norm 5.4232(8.6555) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 90.4567, Epoch Time 1083.5500(1044.5215), Bit/dim 3.6827(best: 3.6824), Xent 0.7408, Loss 4.0531, Error 0.2563(best: 0.2604)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 17.2468(17.7067) | Bit/dim 3.6681(3.6836) | Xent 0.6594(0.6928) | Loss 117.4041(190.8353) | Error 0.2322(0.2466) Steps 0(0.00) | Grad Norm 6.1983(9.0572) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 16.3403(17.6281) | Bit/dim 3.6741(3.6832) | Xent 0.6803(0.6860) | Loss 121.0517(174.0821) | Error 0.2489(0.2452) Steps 0(0.00) | Grad Norm 5.2905(8.4424) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.5398(17.5440) | Bit/dim 3.6764(3.6820) | Xent 0.6208(0.6790) | Loss 129.9664(161.6564) | Error 0.2111(0.2419) Steps 0(0.00) | Grad Norm 6.9179(8.0833) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 18.4036(17.6822) | Bit/dim 3.6982(3.6812) | Xent 0.7186(0.6802) | Loss 131.5336(151.7608) | Error 0.2544(0.2427) Steps 0(0.00) | Grad Norm 9.1393(8.2382) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 17.2942(17.6564) | Bit/dim 3.6455(3.6797) | Xent 0.6990(0.6910) | Loss 124.2687(144.5995) | Error 0.2467(0.2457) Steps 0(0.00) | Grad Norm 6.8540(8.9451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 90.1674, Epoch Time 1079.4730(1045.5701), Bit/dim 3.6854(best: 3.6824), Xent 0.7756, Loss 4.0732, Error 0.2681(best: 0.2563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 15.7676(17.6015) | Bit/dim 3.6911(3.6851) | Xent 0.5882(0.6861) | Loss 106.4961(206.2501) | Error 0.2222(0.2449) Steps 0(0.00) | Grad Norm 4.0422(9.1704) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 18.8579(17.5417) | Bit/dim 3.6662(3.6859) | Xent 0.6508(0.6767) | Loss 133.1550(185.7520) | Error 0.2478(0.2419) Steps 0(0.00) | Grad Norm 6.7305(8.7077) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 20.3763(17.6497) | Bit/dim 3.6568(3.6814) | Xent 0.6699(0.6701) | Loss 131.1094(169.9339) | Error 0.2400(0.2397) Steps 0(0.00) | Grad Norm 6.4242(8.0205) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 16.1115(17.4983) | Bit/dim 3.7135(3.6818) | Xent 0.5698(0.6672) | Loss 113.8544(157.2413) | Error 0.2133(0.2390) Steps 0(0.00) | Grad Norm 7.0085(7.3976) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 17.8608(17.6879) | Bit/dim 3.6614(3.6801) | Xent 0.6694(0.6751) | Loss 123.1725(148.9277) | Error 0.2422(0.2407) Steps 0(0.00) | Grad Norm 10.5686(7.9340) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.0115(17.5385) | Bit/dim 3.7039(3.6788) | Xent 0.7223(0.6785) | Loss 117.8471(141.3459) | Error 0.2589(0.2434) Steps 0(0.00) | Grad Norm 9.2044(8.1545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 89.1418, Epoch Time 1070.9916(1046.3327), Bit/dim 3.6751(best: 3.6824), Xent 0.7544, Loss 4.0523, Error 0.2614(best: 0.2563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 16.9199(17.6670) | Bit/dim 3.6949(3.6815) | Xent 0.6373(0.6736) | Loss 123.9167(190.6659) | Error 0.2400(0.2413) Steps 0(0.00) | Grad Norm 8.8552(8.9155) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 17.1191(17.5366) | Bit/dim 3.6856(3.6807) | Xent 0.6799(0.6675) | Loss 120.2912(172.6975) | Error 0.2433(0.2395) Steps 0(0.00) | Grad Norm 10.1191(8.7048) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 19.1232(17.5143) | Bit/dim 3.6607(3.6759) | Xent 0.6516(0.6747) | Loss 122.2325(159.9306) | Error 0.2256(0.2416) Steps 0(0.00) | Grad Norm 5.8916(8.7717) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 17.3729(17.4862) | Bit/dim 3.6691(3.6763) | Xent 0.7290(0.6802) | Loss 118.6944(150.0194) | Error 0.2600(0.2438) Steps 0(0.00) | Grad Norm 12.9340(9.0341) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 19.1333(17.6540) | Bit/dim 3.6537(3.6766) | Xent 0.7152(0.6804) | Loss 128.4400(143.4349) | Error 0.2644(0.2437) Steps 0(0.00) | Grad Norm 8.7323(8.7690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 89.3571, Epoch Time 1075.8671(1047.2187), Bit/dim 3.6717(best: 3.6751), Xent 0.7239, Loss 4.0337, Error 0.2553(best: 0.2563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 16.5534(17.5666) | Bit/dim 3.6577(3.6772) | Xent 0.7505(0.6762) | Loss 118.5196(203.7035) | Error 0.2656(0.2431) Steps 0(0.00) | Grad Norm 7.7277(8.9466) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.4479(17.6048) | Bit/dim 3.6817(3.6790) | Xent 0.6977(0.6753) | Loss 119.2686(182.3346) | Error 0.2600(0.2421) Steps 0(0.00) | Grad Norm 9.6818(9.0174) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 18.0906(17.6794) | Bit/dim 3.6569(3.6736) | Xent 0.6658(0.6654) | Loss 135.4293(168.1704) | Error 0.2411(0.2403) Steps 0(0.00) | Grad Norm 5.6437(8.1707) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 17.1175(17.7193) | Bit/dim 3.6818(3.6731) | Xent 0.6649(0.6661) | Loss 117.4025(156.1594) | Error 0.2267(0.2395) Steps 0(0.00) | Grad Norm 6.7877(7.9625) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 16.9908(17.6166) | Bit/dim 3.6507(3.6708) | Xent 0.6329(0.6552) | Loss 124.3490(147.2839) | Error 0.2200(0.2359) Steps 0(0.00) | Grad Norm 10.1843(7.5559) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 19.6281(17.6161) | Bit/dim 3.7046(3.6751) | Xent 0.6652(0.6567) | Loss 128.9775(141.2433) | Error 0.2367(0.2361) Steps 0(0.00) | Grad Norm 8.6295(7.6582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 89.2150, Epoch Time 1077.2224(1048.1188), Bit/dim 3.6747(best: 3.6717), Xent 0.7206, Loss 4.0350, Error 0.2538(best: 0.2553)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 18.0895(17.5949) | Bit/dim 3.7066(3.6739) | Xent 0.5305(0.6462) | Loss 126.8328(188.7989) | Error 0.2000(0.2326) Steps 0(0.00) | Grad Norm 5.4971(8.5173) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 18.8554(17.5940) | Bit/dim 3.6706(3.6757) | Xent 0.5948(0.6507) | Loss 126.3243(172.1361) | Error 0.2144(0.2330) Steps 0(0.00) | Grad Norm 4.0666(8.8948) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 17.4063(17.5863) | Bit/dim 3.6522(3.6737) | Xent 0.7063(0.6639) | Loss 126.6559(160.0017) | Error 0.2544(0.2378) Steps 0(0.00) | Grad Norm 9.6050(8.8956) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 18.9380(17.6895) | Bit/dim 3.6985(3.6755) | Xent 0.6829(0.6692) | Loss 135.5467(150.7480) | Error 0.2522(0.2386) Steps 0(0.00) | Grad Norm 10.1470(8.8256) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 20.8431(17.6845) | Bit/dim 3.7108(3.6798) | Xent 0.6476(0.6757) | Loss 136.8299(144.1614) | Error 0.2467(0.2407) Steps 0(0.00) | Grad Norm 6.0540(8.8783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 89.6862, Epoch Time 1077.0988(1048.9882), Bit/dim 3.6768(best: 3.6717), Xent 0.7365, Loss 4.0450, Error 0.2587(best: 0.2538)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.7013(17.6096) | Bit/dim 3.6771(3.6783) | Xent 0.6238(0.6634) | Loss 128.0245(205.8454) | Error 0.2356(0.2363) Steps 0(0.00) | Grad Norm 6.8832(8.7325) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 18.9436(17.6273) | Bit/dim 3.6453(3.6757) | Xent 0.6531(0.6589) | Loss 124.2279(184.5050) | Error 0.2422(0.2352) Steps 0(0.00) | Grad Norm 8.8360(8.2995) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 19.4014(17.6934) | Bit/dim 3.6641(3.6737) | Xent 0.6359(0.6507) | Loss 128.6594(167.9067) | Error 0.2300(0.2318) Steps 0(0.00) | Grad Norm 4.5588(7.8448) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 17.7404(17.5942) | Bit/dim 3.6657(3.6725) | Xent 0.5998(0.6447) | Loss 124.8295(156.7587) | Error 0.2311(0.2301) Steps 0(0.00) | Grad Norm 7.2069(7.5755) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 18.2507(17.6216) | Bit/dim 3.6575(3.6719) | Xent 0.6735(0.6400) | Loss 122.4881(147.6892) | Error 0.2489(0.2280) Steps 0(0.00) | Grad Norm 9.1427(7.4826) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 16.2916(17.5532) | Bit/dim 3.6369(3.6712) | Xent 0.7049(0.6499) | Loss 128.6673(141.1760) | Error 0.2444(0.2314) Steps 0(0.00) | Grad Norm 5.4227(7.8230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 90.4816, Epoch Time 1074.4729(1049.7528), Bit/dim 3.6762(best: 3.6717), Xent 0.7893, Loss 4.0709, Error 0.2742(best: 0.2538)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.0370(17.5992) | Bit/dim 3.6778(3.6738) | Xent 0.5838(0.6516) | Loss 125.6927(195.9098) | Error 0.2300(0.2335) Steps 0(0.00) | Grad Norm 6.0364(8.4121) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 16.9771(17.5504) | Bit/dim 3.6550(3.6726) | Xent 0.5958(0.6475) | Loss 114.7582(176.4902) | Error 0.2156(0.2315) Steps 0(0.00) | Grad Norm 5.5883(8.5761) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.1836(17.5103) | Bit/dim 3.6598(3.6745) | Xent 0.7129(0.6459) | Loss 117.8173(162.9637) | Error 0.2600(0.2316) Steps 0(0.00) | Grad Norm 10.8266(8.4395) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 15.4933(17.5227) | Bit/dim 3.6699(3.6705) | Xent 0.6082(0.6512) | Loss 124.4454(153.4096) | Error 0.2211(0.2330) Steps 0(0.00) | Grad Norm 7.9333(8.2568) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.6524(17.4977) | Bit/dim 3.6484(3.6731) | Xent 0.6433(0.6486) | Loss 126.2112(146.3297) | Error 0.2211(0.2313) Steps 0(0.00) | Grad Norm 6.2646(8.1615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 91.2609, Epoch Time 1069.7357(1050.3523), Bit/dim 3.6692(best: 3.6717), Xent 0.7214, Loss 4.0299, Error 0.2522(best: 0.2538)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 16.6831(17.4190) | Bit/dim 3.7058(3.6726) | Xent 0.6660(0.6511) | Loss 121.7873(203.3492) | Error 0.2433(0.2319) Steps 0(0.00) | Grad Norm 7.5121(8.7599) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 18.3527(17.5695) | Bit/dim 3.6244(3.6717) | Xent 0.6038(0.6538) | Loss 124.4603(182.1678) | Error 0.2200(0.2327) Steps 0(0.00) | Grad Norm 9.6730(8.8315) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 18.3239(17.6933) | Bit/dim 3.6765(3.6716) | Xent 0.6241(0.6504) | Loss 124.9475(166.9158) | Error 0.2278(0.2322) Steps 0(0.00) | Grad Norm 9.1124(8.5073) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 19.2694(17.5517) | Bit/dim 3.6531(3.6700) | Xent 0.6577(0.6454) | Loss 130.7853(156.0698) | Error 0.2289(0.2296) Steps 0(0.00) | Grad Norm 13.6725(8.3796) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 17.1701(17.4756) | Bit/dim 3.6329(3.6674) | Xent 0.6249(0.6447) | Loss 118.0654(148.0259) | Error 0.2178(0.2287) Steps 0(0.00) | Grad Norm 4.3766(7.8139) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.2280(17.4843) | Bit/dim 3.6598(3.6690) | Xent 0.6056(0.6470) | Loss 121.0716(142.3881) | Error 0.2233(0.2309) Steps 0(0.00) | Grad Norm 9.9195(8.0496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 91.3223, Epoch Time 1076.0248(1051.1224), Bit/dim 3.6742(best: 3.6692), Xent 0.7354, Loss 4.0420, Error 0.2596(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 16.7626(17.4670) | Bit/dim 3.6791(3.6727) | Xent 0.6371(0.6477) | Loss 122.3558(194.5897) | Error 0.2367(0.2324) Steps 0(0.00) | Grad Norm 9.1982(8.8442) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 18.2719(17.4461) | Bit/dim 3.6746(3.6725) | Xent 0.5912(0.6415) | Loss 131.9969(176.4347) | Error 0.2033(0.2301) Steps 0(0.00) | Grad Norm 4.7221(8.3552) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 15.6446(17.5582) | Bit/dim 3.6674(3.6710) | Xent 0.7300(0.6417) | Loss 112.4646(162.6480) | Error 0.2411(0.2288) Steps 0(0.00) | Grad Norm 10.1950(8.5052) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 18.1215(17.6729) | Bit/dim 3.6765(3.6710) | Xent 0.6091(0.6395) | Loss 132.7370(152.4376) | Error 0.2111(0.2277) Steps 0(0.00) | Grad Norm 6.7072(8.2335) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 16.3096(17.7205) | Bit/dim 3.6592(3.6694) | Xent 0.6139(0.6398) | Loss 116.4675(145.2935) | Error 0.2222(0.2279) Steps 0(0.00) | Grad Norm 11.1573(7.8948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 90.7104, Epoch Time 1084.7513(1052.1313), Bit/dim 3.6672(best: 3.6692), Xent 0.7120, Loss 4.0231, Error 0.2477(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 16.4552(17.7005) | Bit/dim 3.6897(3.6713) | Xent 0.6281(0.6377) | Loss 121.5908(205.5121) | Error 0.2244(0.2279) Steps 0(0.00) | Grad Norm 6.3637(8.2429) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.1888(17.8207) | Bit/dim 3.6783(3.6736) | Xent 0.6521(0.6333) | Loss 126.3140(185.1209) | Error 0.2311(0.2265) Steps 0(0.00) | Grad Norm 17.3254(8.5953) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 18.1021(17.6763) | Bit/dim 3.6804(3.6739) | Xent 0.6394(0.6404) | Loss 124.8542(169.6982) | Error 0.2433(0.2304) Steps 0(0.00) | Grad Norm 9.6604(8.8754) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 18.4102(17.7310) | Bit/dim 3.6604(3.6711) | Xent 0.6915(0.6433) | Loss 123.5954(157.6058) | Error 0.2411(0.2307) Steps 0(0.00) | Grad Norm 13.8578(9.3691) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 19.9045(17.6150) | Bit/dim 3.6879(3.6695) | Xent 0.6355(0.6506) | Loss 124.1137(149.1829) | Error 0.2278(0.2327) Steps 0(0.00) | Grad Norm 7.3809(9.4209) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 17.3824(17.5739) | Bit/dim 3.6689(3.6715) | Xent 0.6115(0.6486) | Loss 123.9472(142.4721) | Error 0.2200(0.2311) Steps 0(0.00) | Grad Norm 5.9161(8.7227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 88.4910, Epoch Time 1073.8985(1052.7843), Bit/dim 3.6779(best: 3.6672), Xent 0.7458, Loss 4.0508, Error 0.2546(best: 0.2477)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.2081(17.5332) | Bit/dim 3.6796(3.6707) | Xent 0.5929(0.6420) | Loss 131.8071(193.7880) | Error 0.2078(0.2297) Steps 0(0.00) | Grad Norm 7.2702(8.3676) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 17.5645(17.5405) | Bit/dim 3.6725(3.6687) | Xent 0.6066(0.6346) | Loss 123.0703(175.5484) | Error 0.2200(0.2272) Steps 0(0.00) | Grad Norm 6.8526(8.0572) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 17.7907(17.5050) | Bit/dim 3.6909(3.6687) | Xent 0.5667(0.6272) | Loss 126.7327(162.8078) | Error 0.1911(0.2240) Steps 0(0.00) | Grad Norm 7.2536(7.8718) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 18.1475(17.4827) | Bit/dim 3.6506(3.6676) | Xent 0.6702(0.6329) | Loss 131.8401(152.6956) | Error 0.2422(0.2273) Steps 0(0.00) | Grad Norm 10.2231(8.6071) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 19.1872(17.5789) | Bit/dim 3.6838(3.6692) | Xent 0.6003(0.6313) | Loss 132.9406(145.9256) | Error 0.2311(0.2269) Steps 0(0.00) | Grad Norm 4.7169(7.9975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 89.7394, Epoch Time 1075.5268(1053.4666), Bit/dim 3.6638(best: 3.6672), Xent 0.7149, Loss 4.0213, Error 0.2441(best: 0.2477)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.0679(17.7463) | Bit/dim 3.6735(3.6674) | Xent 0.6132(0.6304) | Loss 118.2998(207.5437) | Error 0.2256(0.2273) Steps 0(0.00) | Grad Norm 7.0059(8.1602) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 19.6855(17.6972) | Bit/dim 3.6492(3.6673) | Xent 0.6277(0.6242) | Loss 129.5352(186.7651) | Error 0.2300(0.2245) Steps 0(0.00) | Grad Norm 5.4163(7.5245) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 17.4178(17.7458) | Bit/dim 3.6559(3.6639) | Xent 0.6284(0.6184) | Loss 122.9497(170.8518) | Error 0.2200(0.2223) Steps 0(0.00) | Grad Norm 8.2576(7.2156) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.4600(17.9030) | Bit/dim 3.6500(3.6632) | Xent 0.5770(0.6246) | Loss 127.2128(159.3005) | Error 0.2178(0.2243) Steps 0(0.00) | Grad Norm 7.0704(7.5447) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 17.4903(17.6887) | Bit/dim 3.6240(3.6619) | Xent 0.6529(0.6281) | Loss 117.7128(149.3239) | Error 0.2433(0.2254) Steps 0(0.00) | Grad Norm 5.1465(7.8247) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 17.0357(17.6175) | Bit/dim 3.6554(3.6617) | Xent 0.5663(0.6276) | Loss 119.6675(143.3986) | Error 0.2044(0.2249) Steps 0(0.00) | Grad Norm 7.7415(7.7998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 88.8049, Epoch Time 1079.1325(1054.2366), Bit/dim 3.6643(best: 3.6638), Xent 0.7071, Loss 4.0178, Error 0.2445(best: 0.2441)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.9924(17.7066) | Bit/dim 3.6739(3.6678) | Xent 0.5824(0.6221) | Loss 131.9204(196.0954) | Error 0.1967(0.2213) Steps 0(0.00) | Grad Norm 6.5574(8.3791) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 17.9789(17.7448) | Bit/dim 3.6300(3.6668) | Xent 0.5628(0.6191) | Loss 122.3835(179.3240) | Error 0.2033(0.2204) Steps 0(0.00) | Grad Norm 5.6753(8.4195) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 17.1647(17.7596) | Bit/dim 3.6892(3.6684) | Xent 0.6209(0.6243) | Loss 126.7016(164.9346) | Error 0.2200(0.2222) Steps 0(0.00) | Grad Norm 6.5742(8.4231) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 17.0565(17.8802) | Bit/dim 3.6640(3.6696) | Xent 0.6006(0.6234) | Loss 124.9872(156.0427) | Error 0.2056(0.2212) Steps 0(0.00) | Grad Norm 6.6199(8.1650) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 17.3121(17.7749) | Bit/dim 3.6653(3.6694) | Xent 0.6006(0.6210) | Loss 132.1205(147.3621) | Error 0.2200(0.2201) Steps 0(0.00) | Grad Norm 6.6468(8.0164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 90.9117, Epoch Time 1090.3416(1055.3197), Bit/dim 3.6621(best: 3.6638), Xent 0.7105, Loss 4.0173, Error 0.2500(best: 0.2441)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 19.3394(17.7477) | Bit/dim 3.6463(3.6682) | Xent 0.5647(0.6182) | Loss 138.7975(207.5746) | Error 0.2078(0.2206) Steps 0(0.00) | Grad Norm 7.6407(8.4137) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 16.9475(17.8106) | Bit/dim 3.6781(3.6678) | Xent 0.6321(0.6094) | Loss 123.6543(187.1734) | Error 0.2322(0.2175) Steps 0(0.00) | Grad Norm 5.4034(8.0471) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.4519(17.8169) | Bit/dim 3.6200(3.6691) | Xent 0.5642(0.6080) | Loss 123.6663(171.1278) | Error 0.1989(0.2160) Steps 0(0.00) | Grad Norm 8.0929(7.8647) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 16.9120(17.7047) | Bit/dim 3.6378(3.6668) | Xent 0.5445(0.6026) | Loss 122.9268(158.7689) | Error 0.1878(0.2148) Steps 0(0.00) | Grad Norm 5.7756(7.6604) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 16.4845(17.6993) | Bit/dim 3.6909(3.6645) | Xent 0.6948(0.6044) | Loss 123.6350(150.4606) | Error 0.2544(0.2158) Steps 0(0.00) | Grad Norm 7.7042(7.4237) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.2196(17.7213) | Bit/dim 3.6650(3.6639) | Xent 0.6251(0.6084) | Loss 120.7422(143.3353) | Error 0.2367(0.2173) Steps 0(0.00) | Grad Norm 7.2167(7.4463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 92.1970, Epoch Time 1085.9847(1056.2397), Bit/dim 3.6604(best: 3.6621), Xent 0.7259, Loss 4.0233, Error 0.2501(best: 0.2441)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 17.6968(17.9544) | Bit/dim 3.7031(3.6660) | Xent 0.5686(0.6028) | Loss 125.4715(196.7854) | Error 0.1900(0.2148) Steps 0(0.00) | Grad Norm 8.7491(7.8778) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 19.0361(18.1038) | Bit/dim 3.6510(3.6628) | Xent 0.5861(0.5967) | Loss 129.8641(178.9873) | Error 0.2033(0.2125) Steps 0(0.00) | Grad Norm 5.8279(7.6506) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 17.3264(17.9995) | Bit/dim 3.6667(3.6638) | Xent 0.5865(0.5956) | Loss 122.9291(164.9551) | Error 0.2167(0.2124) Steps 0(0.00) | Grad Norm 9.9446(7.9092) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 19.7035(18.2206) | Bit/dim 3.6929(3.6638) | Xent 0.6013(0.5999) | Loss 131.0550(155.2497) | Error 0.2067(0.2140) Steps 0(0.00) | Grad Norm 8.3859(8.0019) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 17.5481(18.1931) | Bit/dim 3.6489(3.6640) | Xent 0.5972(0.6009) | Loss 120.1407(147.2531) | Error 0.2222(0.2150) Steps 0(0.00) | Grad Norm 5.5956(7.5804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 89.9099, Epoch Time 1116.9816(1058.0619), Bit/dim 3.6636(best: 3.6604), Xent 0.7346, Loss 4.0309, Error 0.2564(best: 0.2441)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 17.5133(18.2322) | Bit/dim 3.6742(3.6624) | Xent 0.6554(0.6046) | Loss 126.7168(205.8990) | Error 0.2411(0.2159) Steps 0(0.00) | Grad Norm 17.2717(8.7573) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 17.1414(18.1359) | Bit/dim 3.6242(3.6643) | Xent 0.6442(0.6073) | Loss 125.2242(185.4385) | Error 0.2333(0.2176) Steps 0(0.00) | Grad Norm 9.1113(8.8380) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 18.0481(18.1997) | Bit/dim 3.6381(3.6636) | Xent 0.5821(0.5978) | Loss 124.7557(170.1579) | Error 0.2089(0.2142) Steps 0(0.00) | Grad Norm 5.4692(8.5861) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 17.0335(18.1433) | Bit/dim 3.6650(3.6638) | Xent 0.6182(0.5947) | Loss 117.0894(157.8509) | Error 0.2322(0.2132) Steps 0(0.00) | Grad Norm 6.1470(8.2189) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 18.4886(18.1782) | Bit/dim 3.6346(3.6613) | Xent 0.5538(0.5953) | Loss 128.8715(149.6849) | Error 0.2067(0.2136) Steps 0(0.00) | Grad Norm 6.5015(8.0714) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 17.7613(18.2630) | Bit/dim 3.6444(3.6587) | Xent 0.6131(0.5988) | Loss 116.4363(142.7672) | Error 0.2133(0.2130) Steps 0(0.00) | Grad Norm 5.7632(7.5783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 92.0188, Epoch Time 1112.8783(1059.7064), Bit/dim 3.6590(best: 3.6604), Xent 0.7002, Loss 4.0090, Error 0.2418(best: 0.2441)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 16.7041(18.3990) | Bit/dim 3.6745(3.6640) | Xent 0.5942(0.6024) | Loss 123.3002(196.7208) | Error 0.2144(0.2144) Steps 0(0.00) | Grad Norm 5.9638(8.0622) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.9362(18.4445) | Bit/dim 3.7113(3.6640) | Xent 0.5772(0.5985) | Loss 126.6036(178.0095) | Error 0.2056(0.2140) Steps 0(0.00) | Grad Norm 8.5773(8.2069) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 17.8083(18.4404) | Bit/dim 3.6488(3.6620) | Xent 0.6102(0.5978) | Loss 130.2406(164.4730) | Error 0.2100(0.2132) Steps 0(0.00) | Grad Norm 4.7533(7.9571) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 19.0444(18.3864) | Bit/dim 3.6324(3.6583) | Xent 0.5711(0.6010) | Loss 133.2540(154.3480) | Error 0.2078(0.2134) Steps 0(0.00) | Grad Norm 5.7223(7.6784) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 18.2401(18.1958) | Bit/dim 3.6458(3.6583) | Xent 0.6106(0.6060) | Loss 122.0009(146.6216) | Error 0.2256(0.2156) Steps 0(0.00) | Grad Norm 11.0152(8.0035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 92.2142, Epoch Time 1120.4048(1061.5274), Bit/dim 3.6637(best: 3.6590), Xent 0.7124, Loss 4.0199, Error 0.2441(best: 0.2418)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 24.0609(18.4641) | Bit/dim 3.6668(3.6581) | Xent 0.6091(0.6027) | Loss 127.1185(208.4313) | Error 0.2267(0.2149) Steps 0(0.00) | Grad Norm 9.0381(8.7226) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 18.3068(18.3472) | Bit/dim 3.6327(3.6573) | Xent 0.5668(0.5993) | Loss 122.9150(186.0607) | Error 0.2144(0.2139) Steps 0(0.00) | Grad Norm 7.3399(8.4910) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 15.8828(18.1354) | Bit/dim 3.6501(3.6560) | Xent 0.5815(0.5933) | Loss 109.1046(169.0346) | Error 0.2078(0.2112) Steps 0(0.00) | Grad Norm 8.9052(8.1741) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.5892(18.0799) | Bit/dim 3.6417(3.6588) | Xent 0.6185(0.5916) | Loss 121.5606(157.6012) | Error 0.2211(0.2106) Steps 0(0.00) | Grad Norm 9.2841(8.5325) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 18.4592(18.2624) | Bit/dim 3.6726(3.6608) | Xent 0.6500(0.5959) | Loss 135.4867(150.1893) | Error 0.2389(0.2107) Steps 0(0.00) | Grad Norm 6.5511(8.4890) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 20.4999(18.1978) | Bit/dim 3.6782(3.6606) | Xent 0.6753(0.5972) | Loss 126.1722(143.6665) | Error 0.2422(0.2125) Steps 0(0.00) | Grad Norm 8.1485(8.5430) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 90.9868, Epoch Time 1109.8446(1062.9769), Bit/dim 3.6597(best: 3.6590), Xent 0.6965, Loss 4.0080, Error 0.2378(best: 0.2418)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 18.0118(18.1844) | Bit/dim 3.6073(3.6584) | Xent 0.5634(0.5926) | Loss 125.8491(197.2292) | Error 0.2011(0.2112) Steps 0(0.00) | Grad Norm 5.7101(8.4539) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 17.3305(17.9892) | Bit/dim 3.6337(3.6590) | Xent 0.5076(0.5852) | Loss 123.1501(178.5388) | Error 0.1789(0.2089) Steps 0(0.00) | Grad Norm 6.8187(8.1178) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 17.7726(17.9779) | Bit/dim 3.6391(3.6565) | Xent 0.5841(0.5857) | Loss 129.5583(164.4585) | Error 0.2133(0.2091) Steps 0(0.00) | Grad Norm 7.4270(8.2031) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 17.5311(18.0386) | Bit/dim 3.6879(3.6575) | Xent 0.6102(0.5854) | Loss 123.5630(153.9006) | Error 0.2189(0.2093) Steps 0(0.00) | Grad Norm 11.0849(8.1268) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.4775(18.1340) | Bit/dim 3.6414(3.6592) | Xent 0.5698(0.5965) | Loss 122.3310(146.1456) | Error 0.2078(0.2118) Steps 0(0.00) | Grad Norm 3.8939(8.8217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 91.4598, Epoch Time 1100.0564(1064.0893), Bit/dim 3.6575(best: 3.6590), Xent 0.7046, Loss 4.0098, Error 0.2376(best: 0.2378)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 17.8939(18.0328) | Bit/dim 3.6795(3.6585) | Xent 0.5462(0.5903) | Loss 129.5269(207.8934) | Error 0.1800(0.2090) Steps 0(0.00) | Grad Norm 6.4904(8.7778) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 17.3523(18.0698) | Bit/dim 3.6370(3.6587) | Xent 0.5666(0.5872) | Loss 125.6777(187.1413) | Error 0.2100(0.2081) Steps 0(0.00) | Grad Norm 7.3800(8.3143) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 17.2605(17.9562) | Bit/dim 3.6738(3.6566) | Xent 0.5646(0.5859) | Loss 128.0410(170.6412) | Error 0.1867(0.2078) Steps 0(0.00) | Grad Norm 5.1182(7.9833) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 18.0685(18.0132) | Bit/dim 3.6528(3.6562) | Xent 0.5150(0.5802) | Loss 124.1513(158.5862) | Error 0.1778(0.2058) Steps 0(0.00) | Grad Norm 3.7781(7.3693) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 17.8777(18.0374) | Bit/dim 3.6360(3.6561) | Xent 0.5649(0.5727) | Loss 126.5648(150.4601) | Error 0.2022(0.2035) Steps 0(0.00) | Grad Norm 6.3302(6.6751) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 17.0911(18.2579) | Bit/dim 3.6326(3.6522) | Xent 0.6292(0.5775) | Loss 122.0521(144.6340) | Error 0.2233(0.2051) Steps 0(0.00) | Grad Norm 11.1414(6.9082) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 91.8048, Epoch Time 1108.9289(1065.4345), Bit/dim 3.6576(best: 3.6575), Xent 0.7018, Loss 4.0085, Error 0.2410(best: 0.2376)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 22.7431(18.3732) | Bit/dim 3.6523(3.6519) | Xent 0.6038(0.5787) | Loss 143.7315(194.7330) | Error 0.2233(0.2054) Steps 0(0.00) | Grad Norm 10.1016(8.0269) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 19.2816(18.2740) | Bit/dim 3.6620(3.6522) | Xent 0.6064(0.5886) | Loss 135.4372(177.4641) | Error 0.2211(0.2092) Steps 0(0.00) | Grad Norm 8.5591(8.3395) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.9276(18.1949) | Bit/dim 3.6642(3.6566) | Xent 0.5797(0.5889) | Loss 130.9897(164.2042) | Error 0.2211(0.2095) Steps 0(0.00) | Grad Norm 8.4747(8.2904) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 17.9963(18.2664) | Bit/dim 3.6653(3.6591) | Xent 0.5935(0.5857) | Loss 132.4490(154.7685) | Error 0.2144(0.2094) Steps 0(0.00) | Grad Norm 12.0467(8.7136) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 18.3513(18.3328) | Bit/dim 3.6515(3.6603) | Xent 0.6096(0.5874) | Loss 132.2644(148.0941) | Error 0.2078(0.2100) Steps 0(0.00) | Grad Norm 8.1734(8.7803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 92.0546, Epoch Time 1116.4838(1066.9659), Bit/dim 3.6557(best: 3.6575), Xent 0.7014, Loss 4.0064, Error 0.2414(best: 0.2376)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 17.7165(18.2609) | Bit/dim 3.6237(3.6614) | Xent 0.5494(0.5877) | Loss 122.7738(207.3787) | Error 0.2111(0.2103) Steps 0(0.00) | Grad Norm 5.9561(9.1237) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 18.2854(18.3227) | Bit/dim 3.6553(3.6629) | Xent 0.5155(0.5823) | Loss 122.1100(186.4341) | Error 0.1978(0.2096) Steps 0(0.00) | Grad Norm 8.9994(8.8184) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 16.1707(18.3142) | Bit/dim 3.6623(3.6614) | Xent 0.5824(0.5823) | Loss 122.1685(171.1280) | Error 0.2100(0.2083) Steps 0(0.00) | Grad Norm 9.4547(8.6345) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.5454(18.3449) | Bit/dim 3.6264(3.6588) | Xent 0.5370(0.5787) | Loss 137.4706(159.8491) | Error 0.1900(0.2068) Steps 0(0.00) | Grad Norm 6.4178(8.1584) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 16.9364(18.2190) | Bit/dim 3.6415(3.6583) | Xent 0.6064(0.5798) | Loss 122.0474(151.2770) | Error 0.2111(0.2069) Steps 0(0.00) | Grad Norm 9.9513(7.7942) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 17.6482(18.1862) | Bit/dim 3.6491(3.6578) | Xent 0.6127(0.5826) | Loss 119.2608(144.1676) | Error 0.1989(0.2070) Steps 0(0.00) | Grad Norm 10.0569(8.2491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 92.5665, Epoch Time 1111.2246(1068.2937), Bit/dim 3.6579(best: 3.6557), Xent 0.6872, Loss 4.0016, Error 0.2368(best: 0.2376)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 17.2733(18.1842) | Bit/dim 3.6534(3.6562) | Xent 0.5378(0.5772) | Loss 130.1180(198.4473) | Error 0.1989(0.2047) Steps 0(0.00) | Grad Norm 7.1343(8.2446) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 20.9078(18.2911) | Bit/dim 3.6411(3.6549) | Xent 0.5833(0.5671) | Loss 133.3299(179.3616) | Error 0.2122(0.2016) Steps 0(0.00) | Grad Norm 8.1650(7.8600) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 17.2874(18.3826) | Bit/dim 3.6610(3.6506) | Xent 0.5882(0.5707) | Loss 126.1713(165.5434) | Error 0.2011(0.2038) Steps 0(0.00) | Grad Norm 9.5434(8.0098) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 16.1709(18.4674) | Bit/dim 3.6512(3.6527) | Xent 0.5687(0.5738) | Loss 114.1347(155.3465) | Error 0.1967(0.2042) Steps 0(0.00) | Grad Norm 10.0752(8.6451) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 18.5630(18.4252) | Bit/dim 3.6643(3.6529) | Xent 0.6076(0.5723) | Loss 119.0114(147.3196) | Error 0.2156(0.2046) Steps 0(0.00) | Grad Norm 10.7801(8.2620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 91.8885, Epoch Time 1128.3397(1070.0951), Bit/dim 3.6565(best: 3.6557), Xent 0.6879, Loss 4.0004, Error 0.2380(best: 0.2368)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 19.5577(18.4524) | Bit/dim 3.6451(3.6541) | Xent 0.5742(0.5689) | Loss 134.1070(209.6941) | Error 0.2111(0.2037) Steps 0(0.00) | Grad Norm 4.8911(8.1525) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 18.1706(18.4095) | Bit/dim 3.6705(3.6556) | Xent 0.5508(0.5615) | Loss 124.6005(187.9287) | Error 0.2033(0.2022) Steps 0(0.00) | Grad Norm 6.4761(7.8921) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 16.4492(18.3824) | Bit/dim 3.6169(3.6533) | Xent 0.5561(0.5621) | Loss 128.2049(171.4641) | Error 0.1967(0.2012) Steps 0(0.00) | Grad Norm 6.7536(7.7944) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 18.2564(18.4645) | Bit/dim 3.6707(3.6534) | Xent 0.6548(0.5768) | Loss 127.8778(160.4234) | Error 0.2356(0.2058) Steps 0(0.00) | Grad Norm 11.7936(8.3869) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 19.8997(18.3464) | Bit/dim 3.6218(3.6525) | Xent 0.5340(0.5777) | Loss 129.5777(151.5761) | Error 0.1933(0.2067) Steps 0(0.00) | Grad Norm 8.6083(8.5178) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.5456(18.3379) | Bit/dim 3.6437(3.6530) | Xent 0.5310(0.5751) | Loss 133.5535(144.9372) | Error 0.1967(0.2060) Steps 0(0.00) | Grad Norm 6.3077(8.3280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 92.4834, Epoch Time 1120.0129(1071.5926), Bit/dim 3.6469(best: 3.6557), Xent 0.6794, Loss 3.9865, Error 0.2351(best: 0.2368)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 19.0770(18.2971) | Bit/dim 3.6478(3.6514) | Xent 0.5322(0.5673) | Loss 133.6282(198.8924) | Error 0.1911(0.2028) Steps 0(0.00) | Grad Norm 7.5997(8.3011) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 15.9380(18.1274) | Bit/dim 3.6387(3.6509) | Xent 0.5482(0.5651) | Loss 118.9087(178.6008) | Error 0.1956(0.2024) Steps 0(0.00) | Grad Norm 5.7357(8.6039) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 17.4917(18.3072) | Bit/dim 3.6731(3.6537) | Xent 0.5400(0.5623) | Loss 120.0613(165.4414) | Error 0.1778(0.2019) Steps 0(0.00) | Grad Norm 6.5867(8.6900) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 18.2522(18.2018) | Bit/dim 3.6516(3.6503) | Xent 0.5846(0.5634) | Loss 129.2955(154.6066) | Error 0.2033(0.2022) Steps 0(0.00) | Grad Norm 8.2407(8.4551) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 18.1329(18.2296) | Bit/dim 3.6457(3.6484) | Xent 0.5526(0.5621) | Loss 124.7478(146.9776) | Error 0.1900(0.2015) Steps 0(0.00) | Grad Norm 7.7820(7.7010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 92.0993, Epoch Time 1107.3091(1072.6641), Bit/dim 3.6580(best: 3.6469), Xent 0.7015, Loss 4.0087, Error 0.2424(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 18.2373(18.1937) | Bit/dim 3.6700(3.6499) | Xent 0.5884(0.5582) | Loss 116.0128(208.7739) | Error 0.2078(0.2001) Steps 0(0.00) | Grad Norm 9.0707(8.0894) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 17.7803(18.1295) | Bit/dim 3.6601(3.6488) | Xent 0.5173(0.5603) | Loss 127.6058(186.7962) | Error 0.1889(0.2011) Steps 0(0.00) | Grad Norm 7.8247(8.3068) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 16.7389(17.9801) | Bit/dim 3.6488(3.6495) | Xent 0.5428(0.5568) | Loss 121.2994(170.0264) | Error 0.1978(0.2000) Steps 0(0.00) | Grad Norm 3.9812(7.8398) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 17.7931(18.0517) | Bit/dim 3.6467(3.6477) | Xent 0.5529(0.5570) | Loss 129.9676(159.0579) | Error 0.2078(0.2007) Steps 0(0.00) | Grad Norm 6.7790(7.7562) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 18.7485(18.1740) | Bit/dim 3.6262(3.6498) | Xent 0.5642(0.5525) | Loss 124.4678(150.6151) | Error 0.2022(0.1985) Steps 0(0.00) | Grad Norm 4.4900(7.2020) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 17.5388(18.1266) | Bit/dim 3.6122(3.6439) | Xent 0.5357(0.5540) | Loss 127.1504(144.1336) | Error 0.1833(0.1977) Steps 0(0.00) | Grad Norm 8.6026(7.2090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 91.8212, Epoch Time 1105.3203(1073.6438), Bit/dim 3.6493(best: 3.6469), Xent 0.6950, Loss 3.9968, Error 0.2367(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 19.2205(18.0673) | Bit/dim 3.6349(3.6469) | Xent 0.5296(0.5515) | Loss 128.8138(195.7916) | Error 0.1833(0.1962) Steps 0(0.00) | Grad Norm 5.5121(8.0217) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 17.7895(18.0843) | Bit/dim 3.6690(3.6497) | Xent 0.5389(0.5496) | Loss 121.5913(177.7982) | Error 0.1978(0.1967) Steps 0(0.00) | Grad Norm 6.5035(7.7477) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.5383(17.9914) | Bit/dim 3.6459(3.6478) | Xent 0.6143(0.5532) | Loss 128.3549(164.0225) | Error 0.2244(0.1972) Steps 0(0.00) | Grad Norm 13.7232(8.1667) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 18.8748(18.1303) | Bit/dim 3.6659(3.6480) | Xent 0.5428(0.5606) | Loss 130.8585(154.5400) | Error 0.2056(0.1999) Steps 0(0.00) | Grad Norm 6.1398(8.0735) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 18.6300(18.3858) | Bit/dim 3.6345(3.6464) | Xent 0.5652(0.5603) | Loss 132.7516(148.2401) | Error 0.2056(0.1996) Steps 0(0.00) | Grad Norm 10.9688(8.0132) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 92.0270, Epoch Time 1114.7185(1074.8760), Bit/dim 3.6479(best: 3.6469), Xent 0.6869, Loss 3.9914, Error 0.2373(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 18.1489(18.3649) | Bit/dim 3.6766(3.6463) | Xent 0.4734(0.5584) | Loss 131.1552(210.5119) | Error 0.1722(0.1991) Steps 0(0.00) | Grad Norm 5.1948(8.4097) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 17.2041(18.1886) | Bit/dim 3.6461(3.6501) | Xent 0.5717(0.5569) | Loss 125.9487(188.6118) | Error 0.1956(0.1985) Steps 0(0.00) | Grad Norm 6.5620(8.3143) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 18.9458(18.1033) | Bit/dim 3.6618(3.6492) | Xent 0.5734(0.5615) | Loss 128.9417(171.0668) | Error 0.2056(0.2002) Steps 0(0.00) | Grad Norm 9.7318(8.5530) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 17.2796(18.2050) | Bit/dim 3.6400(3.6505) | Xent 0.5465(0.5596) | Loss 126.0808(159.5674) | Error 0.2000(0.1984) Steps 0(0.00) | Grad Norm 7.6120(8.3424) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 17.8350(18.2451) | Bit/dim 3.6602(3.6475) | Xent 0.5335(0.5518) | Loss 124.2495(151.0474) | Error 0.1833(0.1957) Steps 0(0.00) | Grad Norm 4.3854(8.0185) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 17.8302(18.2965) | Bit/dim 3.6877(3.6486) | Xent 0.5676(0.5519) | Loss 123.1029(144.3514) | Error 0.2000(0.1961) Steps 0(0.00) | Grad Norm 8.5275(7.8859) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 91.5703, Epoch Time 1111.1373(1075.9639), Bit/dim 3.6482(best: 3.6469), Xent 0.7771, Loss 4.0368, Error 0.2575(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 17.7424(18.2696) | Bit/dim 3.6611(3.6503) | Xent 0.6325(0.5601) | Loss 131.0442(197.6790) | Error 0.2244(0.1985) Steps 0(0.00) | Grad Norm 11.0870(8.9881) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 19.2494(18.4430) | Bit/dim 3.6472(3.6493) | Xent 0.5933(0.5633) | Loss 123.5643(179.4678) | Error 0.2033(0.1993) Steps 0(0.00) | Grad Norm 11.6524(8.9733) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 17.8479(18.4631) | Bit/dim 3.6440(3.6512) | Xent 0.5694(0.5661) | Loss 127.8040(165.7469) | Error 0.1967(0.1997) Steps 0(0.00) | Grad Norm 13.9561(9.3206) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 17.0979(18.2557) | Bit/dim 3.6292(3.6500) | Xent 0.5274(0.5608) | Loss 129.5365(155.0176) | Error 0.1933(0.1989) Steps 0(0.00) | Grad Norm 6.0984(8.7829) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 17.5350(18.1485) | Bit/dim 3.6174(3.6471) | Xent 0.5648(0.5524) | Loss 121.6054(147.4716) | Error 0.2044(0.1971) Steps 0(0.00) | Grad Norm 8.3738(8.1442) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 91.6806, Epoch Time 1110.9619(1077.0138), Bit/dim 3.6416(best: 3.6469), Xent 0.7099, Loss 3.9965, Error 0.2376(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 17.6350(18.1444) | Bit/dim 3.6702(3.6492) | Xent 0.5398(0.5490) | Loss 127.2910(210.9318) | Error 0.1878(0.1947) Steps 0(0.00) | Grad Norm 5.6725(8.0711) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 18.5888(18.1730) | Bit/dim 3.6643(3.6521) | Xent 0.5775(0.5449) | Loss 131.2589(188.4234) | Error 0.2133(0.1924) Steps 0(0.00) | Grad Norm 7.4313(7.9394) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 18.1696(18.2363) | Bit/dim 3.6374(3.6492) | Xent 0.5218(0.5386) | Loss 128.7803(172.3647) | Error 0.1878(0.1918) Steps 0(0.00) | Grad Norm 6.9850(7.7601) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 16.8375(18.1556) | Bit/dim 3.6326(3.6459) | Xent 0.5149(0.5414) | Loss 123.4153(160.8500) | Error 0.1867(0.1939) Steps 0(0.00) | Grad Norm 4.9094(7.4602) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.2051(18.2129) | Bit/dim 3.6094(3.6419) | Xent 0.5343(0.5364) | Loss 126.4429(152.0287) | Error 0.1856(0.1914) Steps 0(0.00) | Grad Norm 7.5506(7.1602) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 19.1151(18.2890) | Bit/dim 3.6114(3.6431) | Xent 0.5265(0.5491) | Loss 129.3808(145.3351) | Error 0.1878(0.1964) Steps 0(0.00) | Grad Norm 6.2055(7.9236) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 92.2519, Epoch Time 1124.1867(1078.4290), Bit/dim 3.6484(best: 3.6416), Xent 0.6846, Loss 3.9906, Error 0.2349(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 17.4417(18.3119) | Bit/dim 3.6225(3.6449) | Xent 0.6118(0.5505) | Loss 119.6231(199.6742) | Error 0.2222(0.1970) Steps 0(0.00) | Grad Norm 12.9180(8.3354) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 16.9978(18.2751) | Bit/dim 3.6368(3.6464) | Xent 0.5100(0.5444) | Loss 126.6574(180.1546) | Error 0.1878(0.1940) Steps 0(0.00) | Grad Norm 7.2593(8.3757) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 17.1584(18.5391) | Bit/dim 3.6497(3.6500) | Xent 0.5696(0.5426) | Loss 113.9461(166.4614) | Error 0.1967(0.1929) Steps 0(0.00) | Grad Norm 6.1851(8.0510) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 17.3441(18.4554) | Bit/dim 3.6386(3.6462) | Xent 0.5551(0.5497) | Loss 121.2383(155.9035) | Error 0.1900(0.1948) Steps 0(0.00) | Grad Norm 8.2342(8.4191) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 17.2622(18.3919) | Bit/dim 3.6324(3.6458) | Xent 0.5620(0.5482) | Loss 111.3737(148.0808) | Error 0.2011(0.1938) Steps 0(0.00) | Grad Norm 9.8274(8.3587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 92.2231, Epoch Time 1128.4350(1079.9292), Bit/dim 3.6392(best: 3.6416), Xent 0.6985, Loss 3.9885, Error 0.2362(best: 0.2349)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 19.0425(18.4359) | Bit/dim 3.6520(3.6468) | Xent 0.5297(0.5407) | Loss 130.4609(208.0552) | Error 0.1900(0.1918) Steps 0(0.00) | Grad Norm 7.6611(8.3354) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 19.8495(18.5046) | Bit/dim 3.6681(3.6497) | Xent 0.5603(0.5442) | Loss 134.7512(188.9096) | Error 0.1978(0.1927) Steps 0(0.00) | Grad Norm 11.9749(8.2577) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 18.3680(18.5653) | Bit/dim 3.6719(3.6502) | Xent 0.5459(0.5410) | Loss 134.3743(173.9539) | Error 0.1900(0.1918) Steps 0(0.00) | Grad Norm 7.4885(8.0430) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 16.7288(18.5187) | Bit/dim 3.6064(3.6475) | Xent 0.5125(0.5327) | Loss 122.6809(161.6337) | Error 0.1844(0.1895) Steps 0(0.00) | Grad Norm 7.0527(7.5959) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 19.1201(18.6909) | Bit/dim 3.6120(3.6440) | Xent 0.5221(0.5312) | Loss 130.5266(153.3865) | Error 0.1911(0.1883) Steps 0(0.00) | Grad Norm 4.1129(7.3031) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 20.2631(18.8354) | Bit/dim 3.6513(3.6443) | Xent 0.5842(0.5306) | Loss 139.5904(147.0119) | Error 0.2011(0.1882) Steps 0(0.00) | Grad Norm 9.7552(7.2387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 93.5422, Epoch Time 1145.9837(1081.9108), Bit/dim 3.6412(best: 3.6392), Xent 0.6826, Loss 3.9825, Error 0.2296(best: 0.2349)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 19.5885(18.6951) | Bit/dim 3.6157(3.6443) | Xent 0.5271(0.5260) | Loss 141.4070(201.0855) | Error 0.1900(0.1867) Steps 0(0.00) | Grad Norm 7.4497(7.3304) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.1443(18.6295) | Bit/dim 3.6623(3.6432) | Xent 0.5182(0.5256) | Loss 135.6973(182.1008) | Error 0.1867(0.1860) Steps 0(0.00) | Grad Norm 7.5116(7.1983) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 18.5584(18.7084) | Bit/dim 3.6257(3.6428) | Xent 0.5190(0.5248) | Loss 118.3237(168.2548) | Error 0.1911(0.1858) Steps 0(0.00) | Grad Norm 4.6770(7.4011) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 19.8894(18.6096) | Bit/dim 3.6655(3.6464) | Xent 0.4918(0.5241) | Loss 135.7206(157.5233) | Error 0.1867(0.1855) Steps 0(0.00) | Grad Norm 6.9717(7.4474) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 17.5357(18.6094) | Bit/dim 3.6384(3.6467) | Xent 0.5493(0.5262) | Loss 124.5456(149.9987) | Error 0.1989(0.1871) Steps 0(0.00) | Grad Norm 8.2559(7.4594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 94.1592, Epoch Time 1130.8060(1083.3777), Bit/dim 3.6482(best: 3.6392), Xent 0.6881, Loss 3.9922, Error 0.2320(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 17.9271(18.5604) | Bit/dim 3.6826(3.6432) | Xent 0.5702(0.5262) | Loss 126.1655(212.5439) | Error 0.1978(0.1866) Steps 0(0.00) | Grad Norm 15.7635(8.3091) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 18.1317(18.7261) | Bit/dim 3.6379(3.6438) | Xent 0.5426(0.5288) | Loss 119.4082(189.9749) | Error 0.2011(0.1875) Steps 0(0.00) | Grad Norm 8.6021(8.3420) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 17.6664(18.6590) | Bit/dim 3.6235(3.6440) | Xent 0.6346(0.5360) | Loss 125.1625(173.4667) | Error 0.2344(0.1913) Steps 0(0.00) | Grad Norm 10.4948(8.4896) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 17.3821(18.5620) | Bit/dim 3.6742(3.6421) | Xent 0.5176(0.5368) | Loss 133.0669(161.9103) | Error 0.1778(0.1920) Steps 0(0.00) | Grad Norm 5.8208(8.0499) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 18.2941(18.3506) | Bit/dim 3.6721(3.6416) | Xent 0.5393(0.5359) | Loss 133.8103(152.8523) | Error 0.1778(0.1902) Steps 0(0.00) | Grad Norm 8.5663(7.7374) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 18.7887(18.2563) | Bit/dim 3.6335(3.6420) | Xent 0.5580(0.5383) | Loss 123.3271(145.3659) | Error 0.1967(0.1908) Steps 0(0.00) | Grad Norm 7.6079(7.8902) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 91.9850, Epoch Time 1119.5961(1084.4642), Bit/dim 3.6460(best: 3.6392), Xent 0.6960, Loss 3.9940, Error 0.2352(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.2502(18.3534) | Bit/dim 3.6609(3.6418) | Xent 0.4898(0.5381) | Loss 139.5388(199.9375) | Error 0.1789(0.1914) Steps 0(0.00) | Grad Norm 7.5804(8.0402) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 18.2440(18.4361) | Bit/dim 3.6367(3.6453) | Xent 0.4641(0.5378) | Loss 128.2993(181.3635) | Error 0.1700(0.1915) Steps 0(0.00) | Grad Norm 5.8993(8.1637) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 19.8615(18.4906) | Bit/dim 3.6457(3.6461) | Xent 0.4844(0.5325) | Loss 132.5275(167.2655) | Error 0.1744(0.1905) Steps 0(0.00) | Grad Norm 5.6348(7.7969) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 19.9414(18.5559) | Bit/dim 3.6519(3.6438) | Xent 0.5111(0.5277) | Loss 133.4335(156.8546) | Error 0.1789(0.1884) Steps 0(0.00) | Grad Norm 5.2601(7.8480) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 18.4096(18.6240) | Bit/dim 3.6288(3.6428) | Xent 0.5129(0.5213) | Loss 125.9093(149.5529) | Error 0.1833(0.1851) Steps 0(0.00) | Grad Norm 7.9932(7.7498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 93.6095, Epoch Time 1138.9260(1086.0981), Bit/dim 3.6446(best: 3.6392), Xent 0.6959, Loss 3.9926, Error 0.2296(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.4731(18.6383) | Bit/dim 3.6408(3.6398) | Xent 0.4952(0.5111) | Loss 126.3641(208.3133) | Error 0.1711(0.1810) Steps 0(0.00) | Grad Norm 6.1987(7.9480) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 20.7716(18.6795) | Bit/dim 3.6030(3.6363) | Xent 0.5579(0.5081) | Loss 139.2645(187.2976) | Error 0.1989(0.1804) Steps 0(0.00) | Grad Norm 6.8911(7.6092) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 18.5887(18.7699) | Bit/dim 3.6385(3.6369) | Xent 0.4657(0.5085) | Loss 129.9459(172.7821) | Error 0.1678(0.1810) Steps 0(0.00) | Grad Norm 4.3708(7.5864) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 16.6998(18.5275) | Bit/dim 3.6549(3.6377) | Xent 0.5164(0.5155) | Loss 120.4652(160.4966) | Error 0.1944(0.1845) Steps 0(0.00) | Grad Norm 5.5874(7.6086) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 17.9653(18.3521) | Bit/dim 3.6593(3.6390) | Xent 0.5358(0.5098) | Loss 131.7525(151.1423) | Error 0.1833(0.1828) Steps 0(0.00) | Grad Norm 5.5040(7.2678) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 19.8011(18.3049) | Bit/dim 3.6482(3.6405) | Xent 0.5208(0.5089) | Loss 136.4582(145.6390) | Error 0.1844(0.1824) Steps 0(0.00) | Grad Norm 6.9361(7.1541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 92.7528, Epoch Time 1121.2165(1087.1516), Bit/dim 3.6434(best: 3.6392), Xent 0.7204, Loss 4.0036, Error 0.2379(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 19.5667(18.3723) | Bit/dim 3.6521(3.6394) | Xent 0.5535(0.5116) | Loss 125.4630(198.3249) | Error 0.2011(0.1831) Steps 0(0.00) | Grad Norm 8.8835(7.8430) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 18.8411(18.4896) | Bit/dim 3.6224(3.6398) | Xent 0.4957(0.5085) | Loss 126.6998(179.5243) | Error 0.1789(0.1827) Steps 0(0.00) | Grad Norm 8.7300(7.5010) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 19.5592(18.5695) | Bit/dim 3.6864(3.6426) | Xent 0.5203(0.5062) | Loss 132.1564(165.8365) | Error 0.1844(0.1809) Steps 0(0.00) | Grad Norm 8.5079(7.4970) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 18.2429(18.5630) | Bit/dim 3.6390(3.6373) | Xent 0.5538(0.5134) | Loss 128.8039(155.9733) | Error 0.2000(0.1847) Steps 0(0.00) | Grad Norm 6.1635(7.5216) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 17.8744(18.7059) | Bit/dim 3.6259(3.6360) | Xent 0.5320(0.5137) | Loss 132.5390(149.2135) | Error 0.1889(0.1849) Steps 0(0.00) | Grad Norm 6.0461(7.1404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 92.6927, Epoch Time 1143.8146(1088.8515), Bit/dim 3.6385(best: 3.6392), Xent 0.7072, Loss 3.9921, Error 0.2374(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 19.2132(18.7453) | Bit/dim 3.6311(3.6375) | Xent 0.5008(0.5079) | Loss 132.5924(213.2247) | Error 0.1844(0.1826) Steps 0(0.00) | Grad Norm 7.9106(7.4278) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 18.6250(18.7211) | Bit/dim 3.6633(3.6385) | Xent 0.4883(0.5070) | Loss 128.9002(192.1387) | Error 0.1811(0.1824) Steps 0(0.00) | Grad Norm 9.7632(7.5176) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 19.6489(18.7578) | Bit/dim 3.6464(3.6385) | Xent 0.4934(0.5100) | Loss 129.8076(176.0995) | Error 0.1711(0.1833) Steps 0(0.00) | Grad Norm 5.4799(7.5110) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 18.7250(18.7841) | Bit/dim 3.6179(3.6388) | Xent 0.5263(0.5108) | Loss 131.9786(164.0478) | Error 0.1911(0.1840) Steps 0(0.00) | Grad Norm 6.8451(7.4005) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 17.4546(18.8299) | Bit/dim 3.6393(3.6392) | Xent 0.5240(0.5102) | Loss 129.1398(155.0556) | Error 0.1867(0.1831) Steps 0(0.00) | Grad Norm 6.8720(7.1786) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.0844(18.7059) | Bit/dim 3.6267(3.6365) | Xent 0.5194(0.5155) | Loss 127.4980(147.8911) | Error 0.1878(0.1851) Steps 0(0.00) | Grad Norm 10.3286(7.6047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 92.3977, Epoch Time 1140.8373(1090.4111), Bit/dim 3.6441(best: 3.6385), Xent 0.7201, Loss 4.0042, Error 0.2383(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 17.3070(18.6197) | Bit/dim 3.6594(3.6397) | Xent 0.4858(0.5099) | Loss 130.7241(200.5645) | Error 0.1867(0.1832) Steps 0(0.00) | Grad Norm 5.4286(7.9270) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 19.0066(18.6155) | Bit/dim 3.6565(3.6425) | Xent 0.5041(0.5111) | Loss 140.8923(182.8163) | Error 0.1811(0.1837) Steps 0(0.00) | Grad Norm 5.2750(7.5336) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 17.0396(18.5229) | Bit/dim 3.5932(3.6398) | Xent 0.4772(0.5136) | Loss 119.8702(168.5758) | Error 0.1733(0.1846) Steps 0(0.00) | Grad Norm 5.8996(7.4098) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 16.2940(18.4920) | Bit/dim 3.6186(3.6382) | Xent 0.5270(0.5151) | Loss 122.0065(158.3836) | Error 0.1756(0.1848) Steps 0(0.00) | Grad Norm 5.8258(7.7969) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 17.9980(18.5435) | Bit/dim 3.6278(3.6360) | Xent 0.5353(0.5124) | Loss 128.8393(150.8368) | Error 0.2000(0.1836) Steps 0(0.00) | Grad Norm 7.1043(7.6034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 93.3558, Epoch Time 1128.6634(1091.5587), Bit/dim 3.6357(best: 3.6385), Xent 0.6858, Loss 3.9787, Error 0.2349(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 17.8347(18.6085) | Bit/dim 3.6306(3.6357) | Xent 0.5021(0.5070) | Loss 126.5876(213.9871) | Error 0.1722(0.1819) Steps 0(0.00) | Grad Norm 10.8121(7.7040) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 17.1355(18.5622) | Bit/dim 3.6646(3.6366) | Xent 0.5231(0.5079) | Loss 120.9819(191.6033) | Error 0.1767(0.1814) Steps 0(0.00) | Grad Norm 12.4153(7.7653) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 17.8929(18.6328) | Bit/dim 3.6230(3.6371) | Xent 0.4878(0.5074) | Loss 123.4168(175.4574) | Error 0.1656(0.1806) Steps 0(0.00) | Grad Norm 6.3327(7.8661) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 17.3479(18.5826) | Bit/dim 3.6466(3.6342) | Xent 0.4667(0.5040) | Loss 132.5757(163.3985) | Error 0.1544(0.1795) Steps 0(0.00) | Grad Norm 7.2546(7.7467) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 19.7261(18.5857) | Bit/dim 3.6572(3.6376) | Xent 0.5682(0.5046) | Loss 132.7441(154.4763) | Error 0.2156(0.1792) Steps 0(0.00) | Grad Norm 9.1347(7.5809) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 18.5337(18.5994) | Bit/dim 3.6339(3.6369) | Xent 0.4729(0.5057) | Loss 128.2471(148.1412) | Error 0.1800(0.1802) Steps 0(0.00) | Grad Norm 6.3264(7.4038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 94.1662, Epoch Time 1137.1824(1092.9274), Bit/dim 3.6347(best: 3.6357), Xent 0.6679, Loss 3.9686, Error 0.2272(best: 0.2296)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 19.8934(18.5998) | Bit/dim 3.6263(3.6361) | Xent 0.4961(0.4992) | Loss 146.9867(204.1729) | Error 0.1767(0.1793) Steps 0(0.00) | Grad Norm 6.3906(7.9295) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 19.7326(18.6859) | Bit/dim 3.6333(3.6367) | Xent 0.5331(0.5074) | Loss 149.5092(185.3470) | Error 0.1833(0.1823) Steps 0(0.00) | Grad Norm 8.8176(7.9351) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 18.4152(18.7245) | Bit/dim 3.6649(3.6402) | Xent 0.5090(0.5026) | Loss 134.2591(171.0818) | Error 0.1844(0.1803) Steps 0(0.00) | Grad Norm 7.9404(7.8151) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 18.9740(18.7992) | Bit/dim 3.6172(3.6405) | Xent 0.4986(0.5088) | Loss 133.2621(160.4321) | Error 0.1789(0.1820) Steps 0(0.00) | Grad Norm 7.2878(8.2648) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 17.7697(18.6461) | Bit/dim 3.6121(3.6380) | Xent 0.4965(0.5062) | Loss 131.3906(151.9231) | Error 0.1789(0.1811) Steps 0(0.00) | Grad Norm 5.2693(7.6993) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 92.6426, Epoch Time 1139.8384(1094.3347), Bit/dim 3.6374(best: 3.6347), Xent 0.7472, Loss 4.0110, Error 0.2473(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 18.5287(18.5380) | Bit/dim 3.6903(3.6391) | Xent 0.5494(0.5158) | Loss 133.1157(215.9898) | Error 0.2033(0.1849) Steps 0(0.00) | Grad Norm 10.5756(8.3469) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 19.8090(18.7737) | Bit/dim 3.6556(3.6445) | Xent 0.5664(0.5152) | Loss 131.7136(194.7421) | Error 0.1944(0.1842) Steps 0(0.00) | Grad Norm 11.1220(8.4771) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 25.6525(19.0083) | Bit/dim 3.6400(3.6447) | Xent 0.5010(0.5046) | Loss 149.6707(178.5732) | Error 0.1756(0.1788) Steps 0(0.00) | Grad Norm 7.1212(8.0243) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 19.5494(19.1417) | Bit/dim 3.6363(3.6425) | Xent 0.4764(0.5011) | Loss 131.4091(166.9249) | Error 0.1756(0.1791) Steps 0(0.00) | Grad Norm 5.8880(7.8516) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 18.2389(19.1867) | Bit/dim 3.6238(3.6405) | Xent 0.4843(0.4972) | Loss 129.8754(157.7695) | Error 0.1767(0.1779) Steps 0(0.00) | Grad Norm 7.8151(7.6605) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 20.5816(19.2315) | Bit/dim 3.6141(3.6354) | Xent 0.5538(0.4971) | Loss 140.3506(151.3300) | Error 0.1956(0.1788) Steps 0(0.00) | Grad Norm 7.7764(7.7425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 93.6609, Epoch Time 1172.9604(1096.6935), Bit/dim 3.6389(best: 3.6347), Xent 0.7001, Loss 3.9890, Error 0.2352(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 19.0740(19.0946) | Bit/dim 3.6659(3.6338) | Xent 0.4646(0.4934) | Loss 131.7626(206.9469) | Error 0.1733(0.1763) Steps 0(0.00) | Grad Norm 6.8309(7.8165) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 19.9824(19.1611) | Bit/dim 3.6438(3.6349) | Xent 0.4326(0.4880) | Loss 132.9930(187.2709) | Error 0.1533(0.1741) Steps 0(0.00) | Grad Norm 7.4517(7.8960) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 18.4731(19.2599) | Bit/dim 3.5958(3.6351) | Xent 0.4945(0.4840) | Loss 130.2580(172.2883) | Error 0.1733(0.1727) Steps 0(0.00) | Grad Norm 11.9376(7.7098) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 17.2256(19.0235) | Bit/dim 3.6767(3.6357) | Xent 0.5328(0.4939) | Loss 125.4537(160.8986) | Error 0.1900(0.1759) Steps 0(0.00) | Grad Norm 12.3433(8.4569) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 19.1841(19.1350) | Bit/dim 3.6309(3.6359) | Xent 0.4807(0.4929) | Loss 130.5606(152.4431) | Error 0.1800(0.1752) Steps 0(0.00) | Grad Norm 6.2661(8.1597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 94.3357, Epoch Time 1160.1025(1098.5958), Bit/dim 3.6352(best: 3.6347), Xent 0.7078, Loss 3.9891, Error 0.2417(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 17.5870(18.9863) | Bit/dim 3.6100(3.6346) | Xent 0.4880(0.5016) | Loss 124.7629(214.0066) | Error 0.1911(0.1792) Steps 0(0.00) | Grad Norm 4.7823(8.5321) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 18.4313(18.8501) | Bit/dim 3.6718(3.6382) | Xent 0.4978(0.4975) | Loss 137.0992(191.5908) | Error 0.1711(0.1784) Steps 0(0.00) | Grad Norm 8.0050(8.1127) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 19.3594(18.9160) | Bit/dim 3.6186(3.6362) | Xent 0.4537(0.4917) | Loss 135.0279(175.6905) | Error 0.1656(0.1764) Steps 0(0.00) | Grad Norm 5.5110(7.6642) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 19.6420(18.9446) | Bit/dim 3.6404(3.6377) | Xent 0.4490(0.4882) | Loss 126.3720(163.1228) | Error 0.1667(0.1751) Steps 0(0.00) | Grad Norm 6.0690(7.4751) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 20.8474(19.0215) | Bit/dim 3.6407(3.6352) | Xent 0.4912(0.4835) | Loss 142.7601(154.8590) | Error 0.1700(0.1724) Steps 0(0.00) | Grad Norm 7.8034(7.1267) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 18.0687(18.8686) | Bit/dim 3.6561(3.6321) | Xent 0.4519(0.4913) | Loss 126.0673(148.3502) | Error 0.1600(0.1758) Steps 0(0.00) | Grad Norm 5.0541(7.3544) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 93.0928, Epoch Time 1144.3877(1099.9695), Bit/dim 3.6338(best: 3.6347), Xent 0.6877, Loss 3.9776, Error 0.2307(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 20.2914(18.8286) | Bit/dim 3.6288(3.6358) | Xent 0.4531(0.4856) | Loss 130.9042(202.8235) | Error 0.1611(0.1743) Steps 0(0.00) | Grad Norm 6.3678(7.3418) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 19.2720(18.8378) | Bit/dim 3.6281(3.6358) | Xent 0.4335(0.4750) | Loss 134.9889(183.2209) | Error 0.1611(0.1700) Steps 0(0.00) | Grad Norm 7.3603(6.8888) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 18.4168(18.8747) | Bit/dim 3.6229(3.6339) | Xent 0.4465(0.4738) | Loss 127.0727(168.3712) | Error 0.1533(0.1688) Steps 0(0.00) | Grad Norm 5.8226(6.9030) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 17.5777(18.8163) | Bit/dim 3.6145(3.6312) | Xent 0.5034(0.4737) | Loss 129.4498(158.1998) | Error 0.1656(0.1681) Steps 0(0.00) | Grad Norm 11.6591(7.2677) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 18.8871(18.7512) | Bit/dim 3.6446(3.6296) | Xent 0.4702(0.4779) | Loss 135.9535(150.5609) | Error 0.1578(0.1694) Steps 0(0.00) | Grad Norm 8.0271(7.2654) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 92.8943, Epoch Time 1143.8247(1101.2852), Bit/dim 3.6328(best: 3.6338), Xent 0.6969, Loss 3.9812, Error 0.2349(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 19.8099(18.8938) | Bit/dim 3.6400(3.6314) | Xent 0.4649(0.4795) | Loss 137.4074(213.2816) | Error 0.1744(0.1708) Steps 0(0.00) | Grad Norm 5.6482(7.5647) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 20.4032(18.9996) | Bit/dim 3.6411(3.6342) | Xent 0.5504(0.4878) | Loss 137.0381(193.5683) | Error 0.1944(0.1743) Steps 0(0.00) | Grad Norm 7.3015(7.6875) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 17.7039(18.9884) | Bit/dim 3.6584(3.6376) | Xent 0.4721(0.4848) | Loss 125.8882(177.5570) | Error 0.1589(0.1734) Steps 0(0.00) | Grad Norm 7.4680(7.6575) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 18.8150(18.8575) | Bit/dim 3.6254(3.6341) | Xent 0.4900(0.4895) | Loss 133.9505(165.2909) | Error 0.1611(0.1744) Steps 0(0.00) | Grad Norm 7.5617(7.8438) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 19.4586(18.9107) | Bit/dim 3.6094(3.6342) | Xent 0.5040(0.4899) | Loss 137.9581(156.9624) | Error 0.1611(0.1742) Steps 0(0.00) | Grad Norm 7.9726(7.8639) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 17.0711(18.7940) | Bit/dim 3.6439(3.6368) | Xent 0.4931(0.4892) | Loss 118.3806(150.2968) | Error 0.1700(0.1739) Steps 0(0.00) | Grad Norm 7.7174(7.8774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 93.5306, Epoch Time 1157.2772(1102.9649), Bit/dim 3.6430(best: 3.6328), Xent 0.6765, Loss 3.9813, Error 0.2268(best: 0.2272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.1322(18.9094) | Bit/dim 3.6624(3.6448) | Xent 0.4764(0.4959) | Loss 133.9009(204.9399) | Error 0.1822(0.1771) Steps 0(0.00) | Grad Norm 8.9884(8.6343) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 19.8328(18.8142) | Bit/dim 3.6542(3.6487) | Xent 0.4677(0.5031) | Loss 131.8196(185.1023) | Error 0.1633(0.1803) Steps 0(0.00) | Grad Norm 9.3781(9.0565) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 18.3883(18.8967) | Bit/dim 3.6479(3.6489) | Xent 0.5319(0.4960) | Loss 133.2946(170.5597) | Error 0.1900(0.1779) Steps 0(0.00) | Grad Norm 10.2505(8.5891) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 19.1609(18.8611) | Bit/dim 3.6243(3.6459) | Xent 0.4758(0.4926) | Loss 132.1017(160.0393) | Error 0.1589(0.1774) Steps 0(0.00) | Grad Norm 6.9490(8.4225) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 21.1108(18.8669) | Bit/dim 3.6222(3.6424) | Xent 0.4922(0.4937) | Loss 136.8218(152.0122) | Error 0.1711(0.1772) Steps 0(0.00) | Grad Norm 5.5639(8.1599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 93.5324, Epoch Time 1152.1864(1104.4416), Bit/dim 3.6387(best: 3.6328), Xent 0.6725, Loss 3.9750, Error 0.2233(best: 0.2268)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 20.2445(19.0054) | Bit/dim 3.6389(3.6391) | Xent 0.4519(0.4873) | Loss 138.7882(217.0407) | Error 0.1656(0.1758) Steps 0(0.00) | Grad Norm 4.0127(7.9382) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 18.2971(19.1691) | Bit/dim 3.6078(3.6366) | Xent 0.4341(0.4830) | Loss 128.9340(194.9881) | Error 0.1567(0.1750) Steps 0(0.00) | Grad Norm 3.5553(8.0924) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.9306(19.0623) | Bit/dim 3.6316(3.6377) | Xent 0.5184(0.4817) | Loss 135.9954(177.7687) | Error 0.1833(0.1733) Steps 0(0.00) | Grad Norm 8.6312(7.7787) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 19.1433(18.9664) | Bit/dim 3.6324(3.6354) | Xent 0.3977(0.4866) | Loss 130.7661(165.6028) | Error 0.1444(0.1747) Steps 0(0.00) | Grad Norm 5.8922(7.6925) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 21.4292(19.0243) | Bit/dim 3.6202(3.6340) | Xent 0.4493(0.4762) | Loss 123.0802(155.5070) | Error 0.1611(0.1705) Steps 0(0.00) | Grad Norm 7.2229(7.3310) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 17.5226(18.8974) | Bit/dim 3.6468(3.6329) | Xent 0.5185(0.4755) | Loss 126.7256(148.1239) | Error 0.1767(0.1699) Steps 0(0.00) | Grad Norm 8.4149(7.0660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 92.8189, Epoch Time 1156.0449(1105.9897), Bit/dim 3.6334(best: 3.6328), Xent 0.6772, Loss 3.9720, Error 0.2232(best: 0.2233)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 18.5757(18.8972) | Bit/dim 3.6654(3.6354) | Xent 0.4253(0.4675) | Loss 128.6858(203.0234) | Error 0.1556(0.1676) Steps 0(0.00) | Grad Norm 6.5550(7.2274) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 18.7044(18.8129) | Bit/dim 3.6461(3.6346) | Xent 0.4973(0.4624) | Loss 129.1430(183.8548) | Error 0.1700(0.1657) Steps 0(0.00) | Grad Norm 6.9582(7.0544) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 18.6275(18.8068) | Bit/dim 3.6142(3.6346) | Xent 0.4459(0.4622) | Loss 129.8374(170.2008) | Error 0.1667(0.1656) Steps 0(0.00) | Grad Norm 5.9616(7.2604) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 19.2704(19.0243) | Bit/dim 3.6235(3.6306) | Xent 0.4549(0.4634) | Loss 129.6299(159.9860) | Error 0.1556(0.1664) Steps 0(0.00) | Grad Norm 6.7810(7.1111) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 18.8849(19.0225) | Bit/dim 3.6250(3.6295) | Xent 0.4638(0.4623) | Loss 130.4668(152.0216) | Error 0.1800(0.1660) Steps 0(0.00) | Grad Norm 5.0877(6.6297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 95.2863, Epoch Time 1161.2792(1107.6484), Bit/dim 3.6330(best: 3.6328), Xent 0.6937, Loss 3.9798, Error 0.2301(best: 0.2232)\n",
      "Iter 7330 | Time 18.5511(19.1767) | Bit/dim 3.6315(3.6323) | Xent 0.4190(0.4679) | Loss 123.8875(194.1373) | Error 0.1478(0.1668) Steps 0(0.00) | Grad Norm 10.1016(7.7514) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 19.2535(19.1732) | Bit/dim 3.6293(3.6356) | Xent 0.5117(0.4626) | Loss 130.0843(176.5727) | Error 0.1822(0.1652) Steps 0(0.00) | Grad Norm 8.5154(7.7042) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 17.9602(19.0393) | Bit/dim 3.6110(3.6358) | Xent 0.5034(0.4663) | Loss 130.0338(164.1903) | Error 0.1689(0.1664) Steps 0(0.00) | Grad Norm 5.7051(7.6258) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 19.3004(19.0542) | Bit/dim 3.6045(3.6319) | Xent 0.5099(0.4742) | Loss 134.4592(155.7390) | Error 0.1811(0.1684) Steps 0(0.00) | Grad Norm 9.7612(7.7807) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 18.1727(19.0615) | Bit/dim 3.6028(3.6297) | Xent 0.4255(0.4798) | Loss 126.8940(148.8896) | Error 0.1556(0.1711) Steps 0(0.00) | Grad Norm 5.8811(7.6732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 96.0406, Epoch Time 1163.5097(1109.3242), Bit/dim 3.6334(best: 3.6328), Xent 0.7027, Loss 3.9848, Error 0.2295(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 19.0371(19.0193) | Bit/dim 3.6256(3.6312) | Xent 0.4378(0.4794) | Loss 132.4140(202.1303) | Error 0.1667(0.1705) Steps 0(0.00) | Grad Norm 8.2596(7.9575) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 19.0316(18.9988) | Bit/dim 3.6214(3.6303) | Xent 0.4608(0.4781) | Loss 128.3232(182.5006) | Error 0.1622(0.1698) Steps 0(0.00) | Grad Norm 7.9529(8.0683) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 18.4894(19.0072) | Bit/dim 3.6207(3.6314) | Xent 0.5153(0.4768) | Loss 125.5496(168.3758) | Error 0.1911(0.1708) Steps 0(0.00) | Grad Norm 11.2106(8.2756) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 18.0032(19.0290) | Bit/dim 3.6486(3.6316) | Xent 0.5795(0.4820) | Loss 123.0800(158.2123) | Error 0.1956(0.1726) Steps 0(0.00) | Grad Norm 12.0335(8.5866) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 19.5976(19.3178) | Bit/dim 3.6294(3.6301) | Xent 0.5629(0.4885) | Loss 127.2553(151.1643) | Error 0.2000(0.1737) Steps 0(0.00) | Grad Norm 6.7575(8.5161) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 95.4137, Epoch Time 1173.4873(1111.2491), Bit/dim 3.6316(best: 3.6328), Xent 0.6644, Loss 3.9638, Error 0.2238(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 18.7412(19.3237) | Bit/dim 3.6459(3.6333) | Xent 0.4794(0.4809) | Loss 129.3542(214.6484) | Error 0.1744(0.1711) Steps 0(0.00) | Grad Norm 6.7580(8.7200) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 19.5401(19.1597) | Bit/dim 3.6273(3.6342) | Xent 0.5049(0.4784) | Loss 132.8888(193.0192) | Error 0.1867(0.1705) Steps 0(0.00) | Grad Norm 5.8134(8.3158) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 18.4924(19.1570) | Bit/dim 3.6065(3.6306) | Xent 0.4842(0.4711) | Loss 129.7319(175.9862) | Error 0.1756(0.1678) Steps 0(0.00) | Grad Norm 6.9944(7.8820) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 19.4721(19.2412) | Bit/dim 3.6373(3.6294) | Xent 0.4323(0.4601) | Loss 140.2068(164.0970) | Error 0.1589(0.1643) Steps 0(0.00) | Grad Norm 4.8020(7.1204) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 18.0612(18.9319) | Bit/dim 3.6183(3.6265) | Xent 0.4783(0.4582) | Loss 127.7838(154.1578) | Error 0.1800(0.1647) Steps 0(0.00) | Grad Norm 11.6875(6.9711) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 19.0860(18.9020) | Bit/dim 3.6342(3.6266) | Xent 0.4745(0.4590) | Loss 131.0546(147.2120) | Error 0.1789(0.1657) Steps 0(0.00) | Grad Norm 8.9522(7.3619) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 92.4172, Epoch Time 1147.8049(1112.3458), Bit/dim 3.6271(best: 3.6316), Xent 0.6848, Loss 3.9695, Error 0.2307(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 19.2257(18.8354) | Bit/dim 3.6111(3.6241) | Xent 0.5031(0.4605) | Loss 131.0863(197.4963) | Error 0.1889(0.1664) Steps 0(0.00) | Grad Norm 9.2510(7.8438) | Total Time 0.00(0.00)\n",
      "Iter 7500 | Time 33.2493(19.3014) | Bit/dim 3.6550(3.6264) | Xent 0.4350(0.4483) | Loss 136.5360(179.9894) | Error 0.1522(0.1619) Steps 0(0.00) | Grad Norm 4.9557(7.3599) | Total Time 0.00(0.00)\n",
      "Iter 7510 | Time 20.1097(19.2127) | Bit/dim 3.6333(3.6274) | Xent 0.4479(0.4466) | Loss 133.8397(166.8791) | Error 0.1656(0.1611) Steps 0(0.00) | Grad Norm 5.9823(7.1196) | Total Time 0.00(0.00)\n",
      "Iter 7520 | Time 19.9786(19.2995) | Bit/dim 3.6558(3.6248) | Xent 0.4545(0.4498) | Loss 135.6167(157.3478) | Error 0.1622(0.1609) Steps 0(0.00) | Grad Norm 8.3505(7.1199) | Total Time 0.00(0.00)\n",
      "Iter 7530 | Time 19.5580(19.2353) | Bit/dim 3.6087(3.6276) | Xent 0.4334(0.4578) | Loss 131.5961(150.0448) | Error 0.1533(0.1643) Steps 0(0.00) | Grad Norm 7.7085(7.2538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 92.6675, Epoch Time 1167.9748(1114.0146), Bit/dim 3.6341(best: 3.6271), Xent 0.6750, Loss 3.9717, Error 0.2241(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 19.9643(19.0805) | Bit/dim 3.6341(3.6309) | Xent 0.3689(0.4554) | Loss 125.2521(214.1838) | Error 0.1167(0.1626) Steps 0(0.00) | Grad Norm 4.3700(7.4840) | Total Time 0.00(0.00)\n",
      "Iter 7550 | Time 19.6868(19.1242) | Bit/dim 3.6256(3.6316) | Xent 0.4907(0.4544) | Loss 139.5864(192.9348) | Error 0.1778(0.1618) Steps 0(0.00) | Grad Norm 9.9173(7.9811) | Total Time 0.00(0.00)\n",
      "Iter 7560 | Time 19.8625(19.3606) | Bit/dim 3.6105(3.6283) | Xent 0.4273(0.4502) | Loss 137.4934(177.1559) | Error 0.1522(0.1601) Steps 0(0.00) | Grad Norm 4.3819(7.4772) | Total Time 0.00(0.00)\n",
      "Iter 7570 | Time 20.2018(19.3983) | Bit/dim 3.6393(3.6265) | Xent 0.4557(0.4457) | Loss 132.1155(164.7652) | Error 0.1733(0.1587) Steps 0(0.00) | Grad Norm 7.6672(7.0246) | Total Time 0.00(0.00)\n",
      "Iter 7580 | Time 19.5677(19.3633) | Bit/dim 3.6422(3.6283) | Xent 0.4389(0.4459) | Loss 134.9221(155.7714) | Error 0.1589(0.1588) Steps 0(0.00) | Grad Norm 7.1803(7.0361) | Total Time 0.00(0.00)\n",
      "Iter 7590 | Time 18.6681(19.1981) | Bit/dim 3.6298(3.6275) | Xent 0.4539(0.4476) | Loss 127.0758(148.6310) | Error 0.1656(0.1599) Steps 0(0.00) | Grad Norm 7.6909(7.1387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 93.9387, Epoch Time 1173.3219(1115.7939), Bit/dim 3.6314(best: 3.6271), Xent 0.7151, Loss 3.9889, Error 0.2313(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 19.2149(19.3520) | Bit/dim 3.6376(3.6301) | Xent 0.5330(0.4498) | Loss 130.8368(202.7272) | Error 0.1944(0.1616) Steps 0(0.00) | Grad Norm 10.7469(7.8301) | Total Time 0.00(0.00)\n",
      "Iter 7610 | Time 22.6246(19.4352) | Bit/dim 3.6242(3.6281) | Xent 0.4795(0.4519) | Loss 159.2781(185.9512) | Error 0.1689(0.1608) Steps 0(0.00) | Grad Norm 6.4577(8.0216) | Total Time 0.00(0.00)\n",
      "Iter 7620 | Time 18.2868(19.4777) | Bit/dim 3.6562(3.6334) | Xent 0.4527(0.4510) | Loss 128.7752(173.9124) | Error 0.1700(0.1613) Steps 0(0.00) | Grad Norm 8.7257(8.2486) | Total Time 0.00(0.00)\n",
      "Iter 7630 | Time 18.9978(19.4416) | Bit/dim 3.6102(3.6311) | Xent 0.3985(0.4481) | Loss 122.9253(163.6249) | Error 0.1322(0.1589) Steps 0(0.00) | Grad Norm 4.9387(7.9236) | Total Time 0.00(0.00)\n",
      "Iter 7640 | Time 19.5354(19.3025) | Bit/dim 3.6214(3.6318) | Xent 0.5037(0.4551) | Loss 126.3491(155.0372) | Error 0.1711(0.1619) Steps 0(0.00) | Grad Norm 6.9746(7.6937) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 94.5481, Epoch Time 1181.2125(1117.7564), Bit/dim 3.6349(best: 3.6271), Xent 0.7071, Loss 3.9884, Error 0.2330(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 19.9650(19.2570) | Bit/dim 3.6225(3.6321) | Xent 0.4386(0.4583) | Loss 135.1770(217.8622) | Error 0.1522(0.1630) Steps 0(0.00) | Grad Norm 3.8293(8.4201) | Total Time 0.00(0.00)\n",
      "Iter 7660 | Time 19.9380(19.5817) | Bit/dim 3.6385(3.6341) | Xent 0.4504(0.4544) | Loss 141.9896(196.9058) | Error 0.1500(0.1628) Steps 0(0.00) | Grad Norm 8.0126(8.2173) | Total Time 0.00(0.00)\n",
      "Iter 7670 | Time 19.7372(19.4950) | Bit/dim 3.6206(3.6305) | Xent 0.4314(0.4479) | Loss 130.3028(180.2882) | Error 0.1389(0.1598) Steps 0(0.00) | Grad Norm 4.3158(7.4651) | Total Time 0.00(0.00)\n",
      "Iter 7680 | Time 22.4545(19.6335) | Bit/dim 3.6031(3.6294) | Xent 0.4092(0.4468) | Loss 129.0050(167.8234) | Error 0.1589(0.1608) Steps 0(0.00) | Grad Norm 6.4773(7.5934) | Total Time 0.00(0.00)\n",
      "Iter 7690 | Time 21.4427(19.5022) | Bit/dim 3.6437(3.6311) | Xent 0.4994(0.4540) | Loss 131.3247(157.8165) | Error 0.1722(0.1637) Steps 0(0.00) | Grad Norm 7.5999(7.6111) | Total Time 0.00(0.00)\n",
      "Iter 7700 | Time 19.2563(19.5595) | Bit/dim 3.5877(3.6295) | Xent 0.4596(0.4522) | Loss 128.1013(150.4419) | Error 0.1589(0.1622) Steps 0(0.00) | Grad Norm 6.3319(7.2466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 93.5887, Epoch Time 1192.1924(1119.9895), Bit/dim 3.6280(best: 3.6271), Xent 0.7031, Loss 3.9796, Error 0.2290(best: 0.2232)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 17.8294(19.5424) | Bit/dim 3.6179(3.6272) | Xent 0.3905(0.4466) | Loss 124.9832(204.7846) | Error 0.1311(0.1600) Steps 0(0.00) | Grad Norm 5.4456(7.4444) | Total Time 0.00(0.00)\n",
      "Iter 7720 | Time 21.2496(19.5465) | Bit/dim 3.6086(3.6238) | Xent 0.4043(0.4412) | Loss 130.7107(185.8871) | Error 0.1578(0.1585) Steps 0(0.00) | Grad Norm 5.8133(7.1881) | Total Time 0.00(0.00)\n",
      "Iter 7730 | Time 18.0264(19.5433) | Bit/dim 3.6374(3.6205) | Xent 0.4284(0.4383) | Loss 125.6335(171.5377) | Error 0.1556(0.1571) Steps 0(0.00) | Grad Norm 7.9060(7.1214) | Total Time 0.00(0.00)\n",
      "Iter 7740 | Time 20.3492(19.4966) | Bit/dim 3.6429(3.6243) | Xent 0.4572(0.4396) | Loss 129.4400(160.1469) | Error 0.1633(0.1576) Steps 0(0.00) | Grad Norm 10.2592(7.2045) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_10_0_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 10.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
