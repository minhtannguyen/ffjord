{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=5.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_5_0_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_5_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4300 | Time 19.7817(18.4440) | Bit/dim 3.7010(3.6943) | Xent 0.7511(0.7097) | Loss 69.0235(100.9221) | Error 0.2411(0.2520) Steps 0(0.00) | Grad Norm 12.1996(8.4446) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 18.9507(18.3340) | Bit/dim 3.7028(3.6947) | Xent 0.7096(0.7066) | Loss 67.8790(91.9034) | Error 0.2500(0.2530) Steps 0(0.00) | Grad Norm 6.0971(8.4443) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.6685(18.2449) | Bit/dim 3.6754(3.6931) | Xent 0.7214(0.7070) | Loss 71.4695(85.3031) | Error 0.2589(0.2524) Steps 0(0.00) | Grad Norm 5.8772(8.5210) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.5426(18.1758) | Bit/dim 3.6790(3.6926) | Xent 0.6456(0.7028) | Loss 66.6163(80.5922) | Error 0.2256(0.2507) Steps 0(0.00) | Grad Norm 9.1082(8.5216) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 17.4668(17.9858) | Bit/dim 3.7071(3.6922) | Xent 0.7267(0.7003) | Loss 65.7669(76.6861) | Error 0.2567(0.2494) Steps 0(0.00) | Grad Norm 12.4725(9.0433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 93.4676, Epoch Time 1123.3068(1059.7097), Bit/dim 3.6894(best: inf), Xent 0.7707, Loss 4.0747, Error 0.2706(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 16.1272(17.7857) | Bit/dim 3.6723(3.6903) | Xent 0.6991(0.7102) | Loss 64.5689(106.7929) | Error 0.2356(0.2528) Steps 0(0.00) | Grad Norm 7.1401(9.0395) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.2022(17.7162) | Bit/dim 3.7088(3.6895) | Xent 0.7251(0.7083) | Loss 67.4403(96.4715) | Error 0.2544(0.2520) Steps 0(0.00) | Grad Norm 14.7619(9.0185) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 18.0984(17.5488) | Bit/dim 3.6854(3.6911) | Xent 0.6575(0.7014) | Loss 69.5844(88.3578) | Error 0.2289(0.2490) Steps 0(0.00) | Grad Norm 6.4159(8.7947) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 17.9245(17.5782) | Bit/dim 3.7012(3.6912) | Xent 0.6550(0.6904) | Loss 70.4336(82.6614) | Error 0.2356(0.2460) Steps 0(0.00) | Grad Norm 6.8198(8.1648) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 19.1301(17.5229) | Bit/dim 3.6609(3.6905) | Xent 0.6642(0.6982) | Loss 70.9658(78.4977) | Error 0.2433(0.2491) Steps 0(0.00) | Grad Norm 6.7538(8.6375) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 17.5791(17.4881) | Bit/dim 3.6964(3.6919) | Xent 0.7806(0.7105) | Loss 68.2941(75.2648) | Error 0.2833(0.2525) Steps 0(0.00) | Grad Norm 6.8225(9.1652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 89.4409, Epoch Time 1062.2462(1059.7858), Bit/dim 3.6907(best: 3.6894), Xent 0.7931, Loss 4.0873, Error 0.2784(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 19.3411(17.4484) | Bit/dim 3.6525(3.6915) | Xent 0.7470(0.7141) | Loss 68.6752(102.2766) | Error 0.2700(0.2542) Steps 0(0.00) | Grad Norm 7.8018(9.3456) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.2020(17.3947) | Bit/dim 3.6850(3.6936) | Xent 0.6830(0.7034) | Loss 68.9933(93.0532) | Error 0.2467(0.2500) Steps 0(0.00) | Grad Norm 6.7157(8.6294) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 18.1051(17.3805) | Bit/dim 3.6534(3.6907) | Xent 0.6998(0.6975) | Loss 64.1493(85.8058) | Error 0.2533(0.2476) Steps 0(0.00) | Grad Norm 7.1884(8.2628) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.5849(17.6071) | Bit/dim 3.6602(3.6870) | Xent 0.7333(0.7017) | Loss 64.4828(80.7104) | Error 0.2600(0.2500) Steps 0(0.00) | Grad Norm 7.7606(8.3830) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 18.2171(17.7026) | Bit/dim 3.7002(3.6889) | Xent 0.7320(0.7030) | Loss 70.1134(77.1247) | Error 0.2500(0.2507) Steps 0(0.00) | Grad Norm 9.9795(8.1793) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 90.6443, Epoch Time 1077.8980(1060.3292), Bit/dim 3.6848(best: 3.6894), Xent 0.7466, Loss 4.0581, Error 0.2616(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.3525(17.6922) | Bit/dim 3.6954(3.6871) | Xent 0.7174(0.6999) | Loss 64.9626(107.9571) | Error 0.2567(0.2503) Steps 0(0.00) | Grad Norm 9.9275(8.0676) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.6228(17.7095) | Bit/dim 3.6816(3.6903) | Xent 0.7079(0.7040) | Loss 67.4918(97.3900) | Error 0.2544(0.2529) Steps 0(0.00) | Grad Norm 10.2381(8.9933) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 15.9156(17.5811) | Bit/dim 3.7095(3.6893) | Xent 0.6829(0.7024) | Loss 65.6589(89.6087) | Error 0.2500(0.2519) Steps 0(0.00) | Grad Norm 12.1711(8.8273) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.3538(17.6275) | Bit/dim 3.6858(3.6904) | Xent 0.6556(0.7008) | Loss 68.7775(83.7502) | Error 0.2178(0.2500) Steps 0(0.00) | Grad Norm 6.7066(8.4379) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 18.0291(17.7485) | Bit/dim 3.6788(3.6864) | Xent 0.7526(0.7003) | Loss 68.5228(79.4472) | Error 0.2722(0.2496) Steps 0(0.00) | Grad Norm 11.8179(8.4101) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 18.5025(17.7898) | Bit/dim 3.6727(3.6835) | Xent 0.6814(0.6974) | Loss 70.9803(76.2643) | Error 0.2456(0.2486) Steps 0(0.00) | Grad Norm 5.2791(8.1689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 89.2759, Epoch Time 1083.9143(1061.0367), Bit/dim 3.6844(best: 3.6848), Xent 0.7235, Loss 4.0461, Error 0.2522(best: 0.2616)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 17.2629(17.8491) | Bit/dim 3.6979(3.6825) | Xent 0.6913(0.6914) | Loss 65.6000(101.2289) | Error 0.2511(0.2478) Steps 0(0.00) | Grad Norm 10.1194(9.1550) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.8001(17.7149) | Bit/dim 3.6833(3.6848) | Xent 0.6424(0.6911) | Loss 69.8845(91.9250) | Error 0.2244(0.2477) Steps 0(0.00) | Grad Norm 5.9353(9.0165) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 16.5907(17.5733) | Bit/dim 3.6741(3.6845) | Xent 0.6571(0.6882) | Loss 66.4989(85.1921) | Error 0.2311(0.2468) Steps 0(0.00) | Grad Norm 10.4055(8.9335) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 17.9881(17.5384) | Bit/dim 3.6790(3.6853) | Xent 0.7002(0.6903) | Loss 63.9552(80.3268) | Error 0.2444(0.2467) Steps 0(0.00) | Grad Norm 11.1012(9.3208) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.4803(17.6543) | Bit/dim 3.6692(3.6868) | Xent 0.6773(0.6943) | Loss 58.9100(76.6569) | Error 0.2356(0.2481) Steps 0(0.00) | Grad Norm 6.4346(9.1185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 91.0315, Epoch Time 1074.9444(1061.4539), Bit/dim 3.6846(best: 3.6844), Xent 0.7592, Loss 4.0643, Error 0.2643(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.5510(17.6374) | Bit/dim 3.6814(3.6852) | Xent 0.7190(0.6970) | Loss 72.7509(106.0591) | Error 0.2489(0.2491) Steps 0(0.00) | Grad Norm 11.0678(9.3904) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 18.1689(17.6454) | Bit/dim 3.6988(3.6878) | Xent 0.6526(0.6913) | Loss 68.7517(95.7856) | Error 0.2167(0.2464) Steps 0(0.00) | Grad Norm 6.9092(8.9994) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 16.1322(17.5829) | Bit/dim 3.6522(3.6851) | Xent 0.6843(0.6862) | Loss 63.6603(88.2202) | Error 0.2411(0.2449) Steps 0(0.00) | Grad Norm 5.4462(8.3920) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.8716(17.7930) | Bit/dim 3.6812(3.6845) | Xent 0.6985(0.6856) | Loss 57.8681(82.5744) | Error 0.2322(0.2450) Steps 0(0.00) | Grad Norm 9.0716(8.2516) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 16.9930(17.7445) | Bit/dim 3.6382(3.6825) | Xent 0.7683(0.6910) | Loss 66.7244(78.2277) | Error 0.2722(0.2466) Steps 0(0.00) | Grad Norm 11.6251(8.8744) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.6491(17.8837) | Bit/dim 3.6968(3.6861) | Xent 0.6573(0.6955) | Loss 64.2091(74.8946) | Error 0.2444(0.2470) Steps 0(0.00) | Grad Norm 12.4075(9.2854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 91.6926, Epoch Time 1092.9865(1062.3999), Bit/dim 3.7006(best: 3.6844), Xent 0.7949, Loss 4.0980, Error 0.2819(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 17.9599(17.7826) | Bit/dim 3.6963(3.6892) | Xent 0.7554(0.6967) | Loss 68.5669(101.8727) | Error 0.2678(0.2475) Steps 0(0.00) | Grad Norm 7.2532(9.3650) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.2313(17.7998) | Bit/dim 3.6779(3.6854) | Xent 0.6678(0.6869) | Loss 70.9522(93.0348) | Error 0.2589(0.2451) Steps 0(0.00) | Grad Norm 6.6871(8.6424) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.6767(17.6766) | Bit/dim 3.6960(3.6833) | Xent 0.6374(0.6850) | Loss 65.9308(86.0833) | Error 0.2300(0.2435) Steps 0(0.00) | Grad Norm 7.1524(8.4296) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 16.1875(17.5945) | Bit/dim 3.6532(3.6859) | Xent 0.6736(0.6828) | Loss 61.0233(80.8356) | Error 0.2200(0.2429) Steps 0(0.00) | Grad Norm 6.5253(8.4334) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 16.2012(17.6548) | Bit/dim 3.7088(3.6852) | Xent 0.6650(0.6891) | Loss 61.5512(76.8529) | Error 0.2500(0.2461) Steps 0(0.00) | Grad Norm 8.7971(8.6493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 91.6677, Epoch Time 1074.3133(1062.7573), Bit/dim 3.6804(best: 3.6844), Xent 0.7556, Loss 4.0582, Error 0.2694(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 19.3834(17.6422) | Bit/dim 3.6782(3.6855) | Xent 0.7142(0.6936) | Loss 68.8506(105.7261) | Error 0.2600(0.2471) Steps 0(0.00) | Grad Norm 13.4980(9.3116) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 19.0177(17.7096) | Bit/dim 3.6819(3.6861) | Xent 0.6556(0.6829) | Loss 68.5751(95.2919) | Error 0.2400(0.2426) Steps 0(0.00) | Grad Norm 4.8207(8.5659) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 17.1550(17.6572) | Bit/dim 3.6756(3.6817) | Xent 0.6471(0.6768) | Loss 66.5650(87.7929) | Error 0.2522(0.2409) Steps 0(0.00) | Grad Norm 6.5412(8.1878) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.2113(17.7811) | Bit/dim 3.7288(3.6819) | Xent 0.6561(0.6763) | Loss 67.3928(82.2834) | Error 0.2233(0.2403) Steps 0(0.00) | Grad Norm 5.5929(7.8462) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 19.1278(17.7455) | Bit/dim 3.6986(3.6792) | Xent 0.6213(0.6734) | Loss 67.8366(77.8301) | Error 0.2300(0.2396) Steps 0(0.00) | Grad Norm 7.2832(7.8154) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.6031(17.9389) | Bit/dim 3.7363(3.6775) | Xent 0.6576(0.6643) | Loss 64.3617(74.8982) | Error 0.2556(0.2375) Steps 0(0.00) | Grad Norm 6.1849(6.9874) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 91.1904, Epoch Time 1095.5647(1063.7415), Bit/dim 3.6756(best: 3.6804), Xent 0.7135, Loss 4.0323, Error 0.2512(best: 0.2522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 17.4704(17.7794) | Bit/dim 3.6883(3.6765) | Xent 0.7129(0.6706) | Loss 63.5628(101.4060) | Error 0.2633(0.2399) Steps 0(0.00) | Grad Norm 11.4665(7.8918) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 19.1357(17.8582) | Bit/dim 3.6763(3.6744) | Xent 0.5981(0.6693) | Loss 68.1018(92.5953) | Error 0.2111(0.2385) Steps 0(0.00) | Grad Norm 8.5086(8.0557) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 18.5771(17.8230) | Bit/dim 3.6637(3.6750) | Xent 0.6461(0.6649) | Loss 66.4958(85.8390) | Error 0.2278(0.2380) Steps 0(0.00) | Grad Norm 7.3761(7.8380) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.9187(17.8144) | Bit/dim 3.6536(3.6738) | Xent 0.6113(0.6581) | Loss 67.7967(80.5895) | Error 0.2289(0.2365) Steps 0(0.00) | Grad Norm 5.9551(7.4656) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 17.7401(17.7516) | Bit/dim 3.7074(3.6736) | Xent 0.7345(0.6585) | Loss 67.9079(76.8160) | Error 0.2489(0.2365) Steps 0(0.00) | Grad Norm 10.3637(7.8138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 91.2503, Epoch Time 1084.1134(1064.3527), Bit/dim 3.6778(best: 3.6756), Xent 0.7129, Loss 4.0342, Error 0.2561(best: 0.2512)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 16.6568(17.7090) | Bit/dim 3.6719(3.6744) | Xent 0.6799(0.6555) | Loss 64.5198(103.9852) | Error 0.2422(0.2347) Steps 0(0.00) | Grad Norm 8.5430(8.0364) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 18.3514(17.7389) | Bit/dim 3.6979(3.6739) | Xent 0.6011(0.6469) | Loss 67.3023(94.0357) | Error 0.2189(0.2310) Steps 0(0.00) | Grad Norm 8.3991(7.6786) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.1070(17.8255) | Bit/dim 3.6564(3.6735) | Xent 0.6269(0.6462) | Loss 69.3469(87.1233) | Error 0.2200(0.2316) Steps 0(0.00) | Grad Norm 6.2522(7.8384) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 16.5179(17.7667) | Bit/dim 3.6720(3.6721) | Xent 0.6863(0.6608) | Loss 61.2356(81.5446) | Error 0.2556(0.2367) Steps 0(0.00) | Grad Norm 8.7730(8.2249) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 18.4981(17.9087) | Bit/dim 3.6907(3.6769) | Xent 0.6535(0.6616) | Loss 69.2953(77.6970) | Error 0.2333(0.2381) Steps 0(0.00) | Grad Norm 11.0583(8.6981) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 18.6585(17.9788) | Bit/dim 3.6559(3.6783) | Xent 0.6086(0.6654) | Loss 68.8114(75.0933) | Error 0.2056(0.2383) Steps 0(0.00) | Grad Norm 5.5680(8.6858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 89.8389, Epoch Time 1093.8464(1065.2375), Bit/dim 3.6731(best: 3.6756), Xent 0.7071, Loss 4.0267, Error 0.2507(best: 0.2512)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.7736(17.8080) | Bit/dim 3.6764(3.6763) | Xent 0.6651(0.6597) | Loss 66.3158(102.4881) | Error 0.2478(0.2364) Steps 0(0.00) | Grad Norm 7.0210(8.7047) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 15.9704(17.8375) | Bit/dim 3.6705(3.6762) | Xent 0.6420(0.6544) | Loss 64.5770(92.7756) | Error 0.2378(0.2352) Steps 0(0.00) | Grad Norm 7.5205(8.3223) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 19.0479(17.8544) | Bit/dim 3.7188(3.6766) | Xent 0.7009(0.6529) | Loss 64.7743(86.1151) | Error 0.2400(0.2345) Steps 0(0.00) | Grad Norm 7.6472(8.1818) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 17.3131(17.7956) | Bit/dim 3.6922(3.6763) | Xent 0.6496(0.6502) | Loss 60.1673(80.7699) | Error 0.2311(0.2329) Steps 0(0.00) | Grad Norm 7.3496(7.6750) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 18.1202(17.7060) | Bit/dim 3.6916(3.6750) | Xent 0.7201(0.6554) | Loss 64.7015(76.8170) | Error 0.2600(0.2341) Steps 0(0.00) | Grad Norm 9.0848(8.0556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 92.5860, Epoch Time 1082.1812(1065.7458), Bit/dim 3.6805(best: 3.6731), Xent 0.7545, Loss 4.0577, Error 0.2658(best: 0.2507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 16.8122(17.7156) | Bit/dim 3.7003(3.6753) | Xent 0.6916(0.6666) | Loss 60.6173(108.2454) | Error 0.2422(0.2382) Steps 0(0.00) | Grad Norm 8.6374(8.6084) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.7518(17.7166) | Bit/dim 3.6406(3.6734) | Xent 0.6659(0.6561) | Loss 68.4987(97.1192) | Error 0.2500(0.2359) Steps 0(0.00) | Grad Norm 9.4308(8.0851) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 16.8381(17.7463) | Bit/dim 3.6850(3.6750) | Xent 0.5579(0.6431) | Loss 62.1008(88.9768) | Error 0.1956(0.2306) Steps 0(0.00) | Grad Norm 5.0923(7.7385) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 19.5465(17.7790) | Bit/dim 3.6522(3.6722) | Xent 0.7053(0.6501) | Loss 69.1305(83.3868) | Error 0.2733(0.2331) Steps 0(0.00) | Grad Norm 8.2094(7.7758) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 17.1877(17.6768) | Bit/dim 3.6768(3.6726) | Xent 0.6808(0.6458) | Loss 65.9614(78.8849) | Error 0.2389(0.2308) Steps 0(0.00) | Grad Norm 11.2938(7.8971) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.3811(17.5727) | Bit/dim 3.6663(3.6721) | Xent 0.6695(0.6495) | Loss 66.1574(75.2469) | Error 0.2389(0.2327) Steps 0(0.00) | Grad Norm 8.1857(8.1745) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 92.3640, Epoch Time 1079.9172(1066.1710), Bit/dim 3.6777(best: 3.6731), Xent 0.7016, Loss 4.0285, Error 0.2468(best: 0.2507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.2084(17.5482) | Bit/dim 3.6812(3.6748) | Xent 0.6377(0.6459) | Loss 63.9926(100.4101) | Error 0.2322(0.2298) Steps 0(0.00) | Grad Norm 11.6207(8.4267) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.1452(17.6943) | Bit/dim 3.6328(3.6715) | Xent 0.6278(0.6434) | Loss 66.7580(91.4289) | Error 0.2289(0.2294) Steps 0(0.00) | Grad Norm 7.3412(8.3997) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 18.1994(17.5753) | Bit/dim 3.6894(3.6738) | Xent 0.6260(0.6446) | Loss 62.6221(84.6803) | Error 0.2144(0.2293) Steps 0(0.00) | Grad Norm 9.1391(8.5955) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 18.9852(17.5606) | Bit/dim 3.7044(3.6723) | Xent 0.6715(0.6505) | Loss 72.3104(80.1313) | Error 0.2333(0.2307) Steps 0(0.00) | Grad Norm 10.1104(8.6781) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.6125(17.6606) | Bit/dim 3.6652(3.6711) | Xent 0.6377(0.6512) | Loss 66.5854(76.8269) | Error 0.2256(0.2323) Steps 0(0.00) | Grad Norm 7.9307(8.5265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 91.2844, Epoch Time 1079.7360(1066.5779), Bit/dim 3.6761(best: 3.6731), Xent 0.7203, Loss 4.0363, Error 0.2517(best: 0.2468)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 18.4185(17.6478) | Bit/dim 3.6497(3.6721) | Xent 0.6536(0.6540) | Loss 72.4044(107.6431) | Error 0.2467(0.2342) Steps 0(0.00) | Grad Norm 7.8488(8.3846) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.3641(17.5655) | Bit/dim 3.6391(3.6729) | Xent 0.5885(0.6453) | Loss 63.7536(96.8444) | Error 0.2233(0.2310) Steps 0(0.00) | Grad Norm 7.2890(8.0813) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 16.4701(17.6485) | Bit/dim 3.6457(3.6725) | Xent 0.6499(0.6397) | Loss 66.1604(88.9529) | Error 0.2311(0.2290) Steps 0(0.00) | Grad Norm 9.5647(7.9940) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.3075(17.8287) | Bit/dim 3.6825(3.6721) | Xent 0.6223(0.6436) | Loss 65.0258(82.8634) | Error 0.2156(0.2294) Steps 0(0.00) | Grad Norm 7.8430(8.5786) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 19.0546(17.8184) | Bit/dim 3.6731(3.6729) | Xent 0.5685(0.6394) | Loss 70.0495(78.7910) | Error 0.2122(0.2286) Steps 0(0.00) | Grad Norm 5.3095(8.1530) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.3747(17.8994) | Bit/dim 3.6585(3.6707) | Xent 0.6756(0.6422) | Loss 70.8619(75.7693) | Error 0.2433(0.2282) Steps 0(0.00) | Grad Norm 6.3211(7.9729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 93.4155, Epoch Time 1096.1372(1067.4647), Bit/dim 3.6674(best: 3.6731), Xent 0.7086, Loss 4.0217, Error 0.2480(best: 0.2468)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.6950(18.0461) | Bit/dim 3.6804(3.6744) | Xent 0.6199(0.6374) | Loss 72.3405(103.8872) | Error 0.2267(0.2277) Steps 0(0.00) | Grad Norm 13.1789(8.2283) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.1067(18.0372) | Bit/dim 3.6814(3.6763) | Xent 0.6338(0.6339) | Loss 71.8703(94.9503) | Error 0.2356(0.2261) Steps 0(0.00) | Grad Norm 16.6096(9.0095) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 21.3199(17.9835) | Bit/dim 3.6655(3.6749) | Xent 0.6596(0.6412) | Loss 67.6403(87.8851) | Error 0.2367(0.2282) Steps 0(0.00) | Grad Norm 9.1838(9.3043) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 16.7948(17.9066) | Bit/dim 3.6405(3.6718) | Xent 0.7151(0.6460) | Loss 67.1794(82.4058) | Error 0.2544(0.2296) Steps 0(0.00) | Grad Norm 7.3527(9.3393) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 19.5145(17.8205) | Bit/dim 3.6499(3.6694) | Xent 0.5949(0.6455) | Loss 63.8603(77.8215) | Error 0.2067(0.2305) Steps 0(0.00) | Grad Norm 4.4034(8.7933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 91.5701, Epoch Time 1094.6418(1068.2800), Bit/dim 3.6767(best: 3.6674), Xent 0.7353, Loss 4.0444, Error 0.2566(best: 0.2468)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.5990(17.8535) | Bit/dim 3.6718(3.6704) | Xent 0.6657(0.6428) | Loss 64.7107(108.0330) | Error 0.2456(0.2301) Steps 0(0.00) | Grad Norm 9.9942(8.8475) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.0072(17.7951) | Bit/dim 3.6620(3.6674) | Xent 0.5765(0.6376) | Loss 66.2993(96.9891) | Error 0.2000(0.2283) Steps 0(0.00) | Grad Norm 5.9560(8.2001) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 16.7896(17.6285) | Bit/dim 3.6598(3.6670) | Xent 0.6171(0.6292) | Loss 66.6683(88.9733) | Error 0.2167(0.2261) Steps 0(0.00) | Grad Norm 6.5382(7.6986) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 16.9589(17.4822) | Bit/dim 3.6798(3.6693) | Xent 0.6474(0.6333) | Loss 60.0740(82.7505) | Error 0.2289(0.2281) Steps 0(0.00) | Grad Norm 9.2884(8.2422) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 17.3601(17.6108) | Bit/dim 3.7045(3.6703) | Xent 0.5957(0.6306) | Loss 66.7461(79.0726) | Error 0.2044(0.2281) Steps 0(0.00) | Grad Norm 9.7885(8.1965) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 19.6345(17.8503) | Bit/dim 3.6452(3.6684) | Xent 0.6333(0.6297) | Loss 65.6291(75.9218) | Error 0.2256(0.2278) Steps 0(0.00) | Grad Norm 7.4317(7.8377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 92.2119, Epoch Time 1083.0709(1068.7237), Bit/dim 3.6686(best: 3.6674), Xent 0.6894, Loss 4.0133, Error 0.2406(best: 0.2468)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.0225(17.8211) | Bit/dim 3.6757(3.6708) | Xent 0.6198(0.6335) | Loss 65.7356(102.2848) | Error 0.2311(0.2282) Steps 0(0.00) | Grad Norm 8.0968(8.3404) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 17.5980(17.9995) | Bit/dim 3.6505(3.6687) | Xent 0.6030(0.6298) | Loss 62.5752(92.7553) | Error 0.2233(0.2260) Steps 0(0.00) | Grad Norm 5.4019(8.4177) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 19.8426(17.9292) | Bit/dim 3.6729(3.6673) | Xent 0.6140(0.6310) | Loss 72.4987(86.2048) | Error 0.2067(0.2254) Steps 0(0.00) | Grad Norm 6.9447(7.9908) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 17.1116(17.8998) | Bit/dim 3.6546(3.6652) | Xent 0.6537(0.6252) | Loss 61.8414(80.7271) | Error 0.2456(0.2231) Steps 0(0.00) | Grad Norm 6.8520(7.4045) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 17.0021(17.7608) | Bit/dim 3.6486(3.6643) | Xent 0.6654(0.6246) | Loss 67.4627(77.1005) | Error 0.2344(0.2221) Steps 0(0.00) | Grad Norm 7.4911(7.3680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 90.5277, Epoch Time 1087.2665(1069.2800), Bit/dim 3.6641(best: 3.6674), Xent 0.7061, Loss 4.0171, Error 0.2510(best: 0.2406)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 18.4966(17.7813) | Bit/dim 3.6990(3.6632) | Xent 0.6054(0.6236) | Loss 69.6632(106.9687) | Error 0.2167(0.2228) Steps 0(0.00) | Grad Norm 6.5760(8.0092) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 18.9956(17.6726) | Bit/dim 3.6537(3.6630) | Xent 0.6196(0.6231) | Loss 71.1956(96.2368) | Error 0.2267(0.2221) Steps 0(0.00) | Grad Norm 9.8154(8.2469) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 17.6272(17.8235) | Bit/dim 3.6940(3.6634) | Xent 0.7894(0.6285) | Loss 64.9442(88.4195) | Error 0.2678(0.2242) Steps 0(0.00) | Grad Norm 12.4852(8.4961) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.4628(17.8173) | Bit/dim 3.6616(3.6641) | Xent 0.6540(0.6330) | Loss 66.6309(82.9081) | Error 0.2278(0.2248) Steps 0(0.00) | Grad Norm 7.1246(8.3184) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 17.4481(17.7234) | Bit/dim 3.6752(3.6644) | Xent 0.6704(0.6356) | Loss 67.1451(78.7314) | Error 0.2333(0.2266) Steps 0(0.00) | Grad Norm 13.0427(8.2572) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 18.8379(17.6798) | Bit/dim 3.6479(3.6655) | Xent 0.6428(0.6358) | Loss 68.6507(75.6168) | Error 0.2289(0.2271) Steps 0(0.00) | Grad Norm 9.1401(8.6407) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 92.3131, Epoch Time 1085.3808(1069.7630), Bit/dim 3.6656(best: 3.6641), Xent 0.6961, Loss 4.0137, Error 0.2428(best: 0.2406)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.9083(17.6780) | Bit/dim 3.6651(3.6646) | Xent 0.6130(0.6307) | Loss 72.2334(101.8676) | Error 0.2067(0.2256) Steps 0(0.00) | Grad Norm 5.3464(8.7208) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 17.8531(17.8065) | Bit/dim 3.6933(3.6653) | Xent 0.5866(0.6242) | Loss 65.7298(92.8702) | Error 0.2156(0.2236) Steps 0(0.00) | Grad Norm 4.7534(8.4532) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 18.6786(17.7572) | Bit/dim 3.6558(3.6653) | Xent 0.6231(0.6161) | Loss 65.5823(86.2388) | Error 0.2233(0.2192) Steps 0(0.00) | Grad Norm 9.4776(7.9193) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 17.7631(17.6851) | Bit/dim 3.6420(3.6636) | Xent 0.6127(0.6112) | Loss 68.1374(81.2756) | Error 0.2300(0.2173) Steps 0(0.00) | Grad Norm 6.5564(7.7848) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 17.9645(17.6820) | Bit/dim 3.6758(3.6623) | Xent 0.5922(0.6065) | Loss 63.7419(77.2695) | Error 0.2111(0.2155) Steps 0(0.00) | Grad Norm 4.1359(7.3279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 90.5584, Epoch Time 1083.6095(1070.1784), Bit/dim 3.6631(best: 3.6641), Xent 0.7296, Loss 4.0279, Error 0.2546(best: 0.2406)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 21.0594(17.8976) | Bit/dim 3.6318(3.6617) | Xent 0.6952(0.6093) | Loss 68.1811(107.8198) | Error 0.2467(0.2170) Steps 0(0.00) | Grad Norm 11.8307(7.6434) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 19.7386(17.9363) | Bit/dim 3.6429(3.6622) | Xent 0.5728(0.6067) | Loss 73.5293(97.6554) | Error 0.2067(0.2153) Steps 0(0.00) | Grad Norm 8.1330(7.8324) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.6181(17.8931) | Bit/dim 3.6417(3.6594) | Xent 0.6409(0.6044) | Loss 65.1617(89.5763) | Error 0.2233(0.2152) Steps 0(0.00) | Grad Norm 8.6766(8.0806) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 20.3678(17.8654) | Bit/dim 3.6483(3.6596) | Xent 0.6422(0.6103) | Loss 73.6180(83.9397) | Error 0.2344(0.2173) Steps 0(0.00) | Grad Norm 6.4594(8.1546) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 18.3776(17.8712) | Bit/dim 3.6551(3.6623) | Xent 0.6036(0.6141) | Loss 68.3961(79.3957) | Error 0.2144(0.2196) Steps 0(0.00) | Grad Norm 6.3743(7.9753) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.2141(18.0053) | Bit/dim 3.6683(3.6634) | Xent 0.6871(0.6193) | Loss 68.0092(76.3100) | Error 0.2500(0.2208) Steps 0(0.00) | Grad Norm 8.4602(7.9696) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 92.0801, Epoch Time 1104.0023(1071.1931), Bit/dim 3.6663(best: 3.6631), Xent 0.6984, Loss 4.0155, Error 0.2413(best: 0.2406)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 19.9640(18.2051) | Bit/dim 3.6850(3.6630) | Xent 0.6481(0.6245) | Loss 72.1836(102.8297) | Error 0.2411(0.2229) Steps 0(0.00) | Grad Norm 6.9656(8.4797) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 19.7125(18.0865) | Bit/dim 3.6724(3.6586) | Xent 0.5582(0.6144) | Loss 68.3916(93.5068) | Error 0.1989(0.2185) Steps 0(0.00) | Grad Norm 5.7602(7.6658) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 21.1620(17.9687) | Bit/dim 3.6575(3.6608) | Xent 0.6158(0.6060) | Loss 67.8054(86.3400) | Error 0.2111(0.2150) Steps 0(0.00) | Grad Norm 7.0153(7.5688) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 18.0459(18.0063) | Bit/dim 3.6637(3.6624) | Xent 0.6294(0.6061) | Loss 65.2236(81.3093) | Error 0.2189(0.2145) Steps 0(0.00) | Grad Norm 6.7131(7.5727) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 17.2682(17.9094) | Bit/dim 3.6347(3.6592) | Xent 0.6596(0.6132) | Loss 65.3561(77.2574) | Error 0.2333(0.2170) Steps 0(0.00) | Grad Norm 8.6314(8.0706) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 93.2694, Epoch Time 1103.0928(1072.1501), Bit/dim 3.6611(best: 3.6631), Xent 0.6789, Loss 4.0005, Error 0.2370(best: 0.2406)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 19.8282(18.2152) | Bit/dim 3.6405(3.6598) | Xent 0.6234(0.6129) | Loss 67.2207(108.9896) | Error 0.2189(0.2170) Steps 0(0.00) | Grad Norm 7.5240(7.9934) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 19.1344(18.1975) | Bit/dim 3.6333(3.6612) | Xent 0.5260(0.6006) | Loss 66.6078(97.9774) | Error 0.1889(0.2131) Steps 0(0.00) | Grad Norm 7.9305(7.8613) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 17.5441(18.2164) | Bit/dim 3.6553(3.6626) | Xent 0.6252(0.5993) | Loss 63.0562(89.7516) | Error 0.2222(0.2129) Steps 0(0.00) | Grad Norm 11.6929(7.8499) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 17.6220(18.1585) | Bit/dim 3.6772(3.6593) | Xent 0.6755(0.6018) | Loss 65.9768(83.7648) | Error 0.2400(0.2148) Steps 0(0.00) | Grad Norm 12.2334(8.5780) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 16.8981(18.0475) | Bit/dim 3.6884(3.6571) | Xent 0.6343(0.6101) | Loss 65.8460(79.3902) | Error 0.2089(0.2170) Steps 0(0.00) | Grad Norm 7.5205(8.6358) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 18.4602(18.0140) | Bit/dim 3.6441(3.6559) | Xent 0.5697(0.6123) | Loss 67.0818(76.0184) | Error 0.2078(0.2165) Steps 0(0.00) | Grad Norm 9.6023(8.7377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 91.7786, Epoch Time 1107.7965(1073.2195), Bit/dim 3.6636(best: 3.6611), Xent 0.6977, Loss 4.0124, Error 0.2445(best: 0.2370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.7743(18.1323) | Bit/dim 3.6391(3.6539) | Xent 0.6136(0.6029) | Loss 67.2065(102.8078) | Error 0.2022(0.2134) Steps 0(0.00) | Grad Norm 6.4839(8.5891) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.4173(17.9923) | Bit/dim 3.6514(3.6558) | Xent 0.6016(0.6014) | Loss 61.1035(92.7520) | Error 0.2311(0.2141) Steps 0(0.00) | Grad Norm 11.3511(8.3505) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 17.9783(17.8822) | Bit/dim 3.6781(3.6554) | Xent 0.5873(0.6014) | Loss 66.1440(85.5905) | Error 0.2056(0.2151) Steps 0(0.00) | Grad Norm 8.4700(8.4224) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 19.5776(17.9484) | Bit/dim 3.6619(3.6595) | Xent 0.5480(0.6014) | Loss 66.8934(80.5803) | Error 0.1978(0.2151) Steps 0(0.00) | Grad Norm 4.6767(8.2478) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 17.5586(18.0138) | Bit/dim 3.6607(3.6578) | Xent 0.5671(0.5990) | Loss 67.2830(77.1123) | Error 0.1978(0.2141) Steps 0(0.00) | Grad Norm 6.6627(8.0062) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 91.9897, Epoch Time 1104.0286(1074.1438), Bit/dim 3.6584(best: 3.6611), Xent 0.6778, Loss 3.9973, Error 0.2365(best: 0.2370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 19.2039(18.1413) | Bit/dim 3.6456(3.6586) | Xent 0.6111(0.5911) | Loss 70.1236(108.3910) | Error 0.2189(0.2116) Steps 0(0.00) | Grad Norm 5.1566(7.6356) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 16.7051(17.9640) | Bit/dim 3.6712(3.6574) | Xent 0.6217(0.5936) | Loss 62.6983(97.1504) | Error 0.2211(0.2117) Steps 0(0.00) | Grad Norm 10.3982(7.9637) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.9665(17.9402) | Bit/dim 3.6663(3.6589) | Xent 0.5660(0.5857) | Loss 66.0497(89.2555) | Error 0.2011(0.2090) Steps 0(0.00) | Grad Norm 7.9690(7.9499) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 15.9235(17.7568) | Bit/dim 3.6610(3.6567) | Xent 0.6029(0.5862) | Loss 63.9390(83.1569) | Error 0.2156(0.2088) Steps 0(0.00) | Grad Norm 5.7657(7.7832) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 18.0747(17.7567) | Bit/dim 3.6869(3.6581) | Xent 0.6459(0.5911) | Loss 69.9664(79.0903) | Error 0.2267(0.2100) Steps 0(0.00) | Grad Norm 10.5069(8.1509) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 17.4090(17.7584) | Bit/dim 3.6471(3.6565) | Xent 0.5898(0.5938) | Loss 67.6658(75.6852) | Error 0.1989(0.2106) Steps 0(0.00) | Grad Norm 8.6954(7.9548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 93.1874, Epoch Time 1085.3826(1074.4810), Bit/dim 3.6628(best: 3.6584), Xent 0.6974, Loss 4.0115, Error 0.2406(best: 0.2365)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 18.3193(17.8696) | Bit/dim 3.6624(3.6574) | Xent 0.5346(0.5888) | Loss 70.1393(102.6319) | Error 0.1878(0.2096) Steps 0(0.00) | Grad Norm 4.9797(7.4794) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 16.4459(17.9713) | Bit/dim 3.6453(3.6553) | Xent 0.5537(0.5860) | Loss 61.4867(93.3597) | Error 0.2033(0.2085) Steps 0(0.00) | Grad Norm 8.0055(7.4980) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 18.4152(17.9233) | Bit/dim 3.6726(3.6557) | Xent 0.6353(0.5829) | Loss 64.3398(86.3973) | Error 0.2233(0.2072) Steps 0(0.00) | Grad Norm 8.9348(7.1091) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 17.8063(18.0797) | Bit/dim 3.6859(3.6568) | Xent 0.6754(0.5830) | Loss 67.9564(81.5140) | Error 0.2311(0.2077) Steps 0(0.00) | Grad Norm 7.6494(7.3048) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.0653(18.0456) | Bit/dim 3.6347(3.6524) | Xent 0.5757(0.5812) | Loss 67.0504(77.8325) | Error 0.2156(0.2073) Steps 0(0.00) | Grad Norm 8.1080(7.1659) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 92.3080, Epoch Time 1105.1678(1075.4016), Bit/dim 3.6617(best: 3.6584), Xent 0.6836, Loss 4.0035, Error 0.2389(best: 0.2365)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 18.8707(17.9407) | Bit/dim 3.6377(3.6515) | Xent 0.5375(0.5776) | Loss 73.3491(107.1616) | Error 0.1956(0.2062) Steps 0(0.00) | Grad Norm 6.4448(7.5838) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 17.6656(17.9222) | Bit/dim 3.6616(3.6492) | Xent 0.5653(0.5776) | Loss 68.9287(96.4745) | Error 0.2089(0.2065) Steps 0(0.00) | Grad Norm 6.6226(7.5174) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 16.1677(17.8598) | Bit/dim 3.6721(3.6501) | Xent 0.5790(0.5800) | Loss 65.4305(88.5056) | Error 0.2000(0.2075) Steps 0(0.00) | Grad Norm 11.5061(7.6565) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 19.1233(17.8523) | Bit/dim 3.6706(3.6530) | Xent 0.5689(0.5829) | Loss 68.5639(83.0393) | Error 0.2044(0.2090) Steps 0(0.00) | Grad Norm 7.1824(7.7667) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 17.6681(17.9431) | Bit/dim 3.6486(3.6547) | Xent 0.6035(0.5807) | Loss 67.3445(79.1023) | Error 0.2178(0.2079) Steps 0(0.00) | Grad Norm 9.2764(7.7006) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 20.2482(17.9424) | Bit/dim 3.6399(3.6587) | Xent 0.5787(0.5852) | Loss 70.8169(76.1325) | Error 0.1944(0.2100) Steps 0(0.00) | Grad Norm 8.9330(8.2206) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 91.4980, Epoch Time 1093.6056(1075.9477), Bit/dim 3.6632(best: 3.6584), Xent 0.6679, Loss 3.9972, Error 0.2333(best: 0.2365)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 17.4081(17.9180) | Bit/dim 3.6575(3.6585) | Xent 0.5503(0.5899) | Loss 64.4175(101.5037) | Error 0.2000(0.2099) Steps 0(0.00) | Grad Norm 7.8011(8.5139) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 16.9665(17.9070) | Bit/dim 3.6469(3.6581) | Xent 0.5739(0.5779) | Loss 63.9500(92.3597) | Error 0.1944(0.2072) Steps 0(0.00) | Grad Norm 5.2508(7.7178) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 17.9259(17.9940) | Bit/dim 3.5947(3.6536) | Xent 0.5905(0.5758) | Loss 62.4106(85.8668) | Error 0.2144(0.2049) Steps 0(0.00) | Grad Norm 9.2306(7.6097) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 17.5172(18.0684) | Bit/dim 3.6476(3.6549) | Xent 0.6076(0.5796) | Loss 70.8031(81.1938) | Error 0.2089(0.2065) Steps 0(0.00) | Grad Norm 10.7401(8.2575) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 16.8417(17.9424) | Bit/dim 3.6540(3.6584) | Xent 0.6092(0.5999) | Loss 60.0874(77.2351) | Error 0.2144(0.2135) Steps 0(0.00) | Grad Norm 7.1156(9.4594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 92.8132, Epoch Time 1102.9835(1076.7588), Bit/dim 3.6657(best: 3.6584), Xent 0.7084, Loss 4.0199, Error 0.2465(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 17.9141(18.0043) | Bit/dim 3.6350(3.6584) | Xent 0.5700(0.5942) | Loss 66.0907(108.3055) | Error 0.2067(0.2117) Steps 0(0.00) | Grad Norm 9.9480(9.1956) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 19.3228(18.0645) | Bit/dim 3.6441(3.6582) | Xent 0.5605(0.5898) | Loss 65.9906(97.2801) | Error 0.2000(0.2110) Steps 0(0.00) | Grad Norm 8.2091(8.7628) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 18.4163(18.2164) | Bit/dim 3.6339(3.6538) | Xent 0.5975(0.5783) | Loss 68.8839(89.6054) | Error 0.2044(0.2058) Steps 0(0.00) | Grad Norm 6.5379(7.9710) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.0671(18.4305) | Bit/dim 3.6729(3.6527) | Xent 0.6066(0.5790) | Loss 66.2560(83.9045) | Error 0.2056(0.2061) Steps 0(0.00) | Grad Norm 10.3086(8.3376) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 19.9885(18.3637) | Bit/dim 3.6762(3.6520) | Xent 0.5815(0.5787) | Loss 71.7783(79.9443) | Error 0.1844(0.2054) Steps 0(0.00) | Grad Norm 10.3309(8.4207) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 17.2127(18.3009) | Bit/dim 3.6836(3.6554) | Xent 0.5718(0.5813) | Loss 63.8621(76.1146) | Error 0.2022(0.2058) Steps 0(0.00) | Grad Norm 6.3254(8.4714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 92.5246, Epoch Time 1121.5798(1078.1034), Bit/dim 3.6616(best: 3.6584), Xent 0.6973, Loss 4.0102, Error 0.2458(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 21.1343(18.3948) | Bit/dim 3.6592(3.6560) | Xent 0.5593(0.5696) | Loss 73.1511(103.2685) | Error 0.1844(0.2019) Steps 0(0.00) | Grad Norm 6.6342(8.1767) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 18.0519(18.3464) | Bit/dim 3.6093(3.6542) | Xent 0.5337(0.5733) | Loss 65.6989(93.6417) | Error 0.1822(0.2029) Steps 0(0.00) | Grad Norm 6.4691(8.6236) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 20.3495(18.3815) | Bit/dim 3.6374(3.6532) | Xent 0.5330(0.5717) | Loss 65.3950(86.9431) | Error 0.1767(0.2015) Steps 0(0.00) | Grad Norm 6.6574(8.2016) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 17.6033(18.2949) | Bit/dim 3.6260(3.6495) | Xent 0.5684(0.5706) | Loss 66.4009(81.8735) | Error 0.1922(0.2015) Steps 0(0.00) | Grad Norm 10.8947(7.8814) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 19.9669(18.4591) | Bit/dim 3.6802(3.6489) | Xent 0.5097(0.5704) | Loss 66.3985(77.9591) | Error 0.1767(0.2010) Steps 0(0.00) | Grad Norm 10.3552(7.8499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 91.8782, Epoch Time 1124.1591(1079.4851), Bit/dim 3.6473(best: 3.6584), Xent 0.6852, Loss 3.9899, Error 0.2422(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 16.4085(18.3367) | Bit/dim 3.6609(3.6496) | Xent 0.5796(0.5691) | Loss 61.2337(109.1763) | Error 0.2267(0.2014) Steps 0(0.00) | Grad Norm 10.3150(7.7557) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 17.3100(18.2108) | Bit/dim 3.6434(3.6482) | Xent 0.5619(0.5662) | Loss 62.6947(97.7890) | Error 0.1989(0.2014) Steps 0(0.00) | Grad Norm 10.3118(8.1308) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 18.7156(18.3457) | Bit/dim 3.6596(3.6512) | Xent 0.6163(0.5753) | Loss 68.3021(89.9583) | Error 0.2067(0.2031) Steps 0(0.00) | Grad Norm 11.2978(8.7967) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 16.6309(18.2564) | Bit/dim 3.6365(3.6518) | Xent 0.5854(0.5811) | Loss 65.8609(84.1262) | Error 0.2000(0.2054) Steps 0(0.00) | Grad Norm 6.8326(8.7542) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 15.9890(18.1584) | Bit/dim 3.6707(3.6532) | Xent 0.5994(0.5899) | Loss 61.7541(79.2897) | Error 0.2156(0.2089) Steps 0(0.00) | Grad Norm 6.3351(8.5192) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.2911(18.2530) | Bit/dim 3.6398(3.6512) | Xent 0.5361(0.5860) | Loss 67.2214(76.2680) | Error 0.1900(0.2074) Steps 0(0.00) | Grad Norm 4.6875(7.9435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 92.7751, Epoch Time 1110.5513(1080.4171), Bit/dim 3.6567(best: 3.6473), Xent 0.6882, Loss 4.0008, Error 0.2422(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 17.8978(18.1695) | Bit/dim 3.6332(3.6509) | Xent 0.5559(0.5806) | Loss 66.4437(102.5377) | Error 0.1978(0.2061) Steps 0(0.00) | Grad Norm 13.1392(8.0583) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 17.6700(18.0186) | Bit/dim 3.6265(3.6513) | Xent 0.5948(0.5757) | Loss 67.9310(92.8518) | Error 0.2167(0.2038) Steps 0(0.00) | Grad Norm 9.2991(7.9476) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 18.1839(18.0561) | Bit/dim 3.6435(3.6507) | Xent 0.5223(0.5639) | Loss 66.4183(86.2284) | Error 0.1944(0.1998) Steps 0(0.00) | Grad Norm 4.7909(7.3266) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 17.0676(18.0677) | Bit/dim 3.6821(3.6523) | Xent 0.5170(0.5576) | Loss 65.8426(81.2306) | Error 0.1878(0.1987) Steps 0(0.00) | Grad Norm 7.5136(7.1471) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 18.2237(17.9890) | Bit/dim 3.6373(3.6493) | Xent 0.5471(0.5556) | Loss 66.4140(77.2428) | Error 0.1933(0.1975) Steps 0(0.00) | Grad Norm 6.5532(6.8653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 92.9327, Epoch Time 1096.0071(1080.8848), Bit/dim 3.6488(best: 3.6473), Xent 0.6779, Loss 3.9877, Error 0.2359(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 20.0808(17.9490) | Bit/dim 3.6572(3.6462) | Xent 0.5104(0.5527) | Loss 67.1595(107.4460) | Error 0.1811(0.1953) Steps 0(0.00) | Grad Norm 10.2909(7.2861) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 18.1005(18.0759) | Bit/dim 3.6655(3.6475) | Xent 0.5755(0.5514) | Loss 69.7685(96.8827) | Error 0.2111(0.1952) Steps 0(0.00) | Grad Norm 10.5564(7.4686) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 19.6887(18.1601) | Bit/dim 3.6577(3.6484) | Xent 0.5266(0.5504) | Loss 72.4665(89.0305) | Error 0.1867(0.1945) Steps 0(0.00) | Grad Norm 8.9112(7.6990) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 18.1533(18.2180) | Bit/dim 3.6816(3.6479) | Xent 0.5696(0.5544) | Loss 65.8208(83.2539) | Error 0.2011(0.1964) Steps 0(0.00) | Grad Norm 11.8096(8.0959) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 17.8473(18.4378) | Bit/dim 3.6863(3.6474) | Xent 0.5524(0.5578) | Loss 66.6167(79.3913) | Error 0.2067(0.1983) Steps 0(0.00) | Grad Norm 7.3404(8.1780) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 17.3482(18.4580) | Bit/dim 3.6260(3.6450) | Xent 0.5673(0.5561) | Loss 64.1684(76.2937) | Error 0.2044(0.1982) Steps 0(0.00) | Grad Norm 7.6400(7.9224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 92.3909, Epoch Time 1127.1614(1082.2731), Bit/dim 3.6418(best: 3.6473), Xent 0.6827, Loss 3.9832, Error 0.2371(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 18.4174(18.4151) | Bit/dim 3.6450(3.6466) | Xent 0.4624(0.5478) | Loss 68.9024(103.1652) | Error 0.1711(0.1947) Steps 0(0.00) | Grad Norm 6.7941(7.8226) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 18.1409(18.3641) | Bit/dim 3.6682(3.6478) | Xent 0.5152(0.5472) | Loss 61.9724(93.8158) | Error 0.1867(0.1946) Steps 0(0.00) | Grad Norm 7.9146(7.8326) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.2172(18.3385) | Bit/dim 3.6527(3.6453) | Xent 0.5473(0.5455) | Loss 64.1676(86.8973) | Error 0.1956(0.1939) Steps 0(0.00) | Grad Norm 6.3404(7.9027) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 18.2164(18.3239) | Bit/dim 3.6539(3.6461) | Xent 0.5095(0.5423) | Loss 65.9787(81.6850) | Error 0.1844(0.1933) Steps 0(0.00) | Grad Norm 5.9603(7.4754) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 18.1493(18.1735) | Bit/dim 3.6528(3.6456) | Xent 0.5794(0.5465) | Loss 68.0599(77.6127) | Error 0.2233(0.1940) Steps 0(0.00) | Grad Norm 7.5663(7.7889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 91.4133, Epoch Time 1112.9148(1083.1923), Bit/dim 3.6502(best: 3.6418), Xent 0.6715, Loss 3.9859, Error 0.2331(best: 0.2333)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 18.7221(18.3195) | Bit/dim 3.6708(3.6481) | Xent 0.5330(0.5485) | Loss 72.7501(107.8436) | Error 0.1833(0.1956) Steps 0(0.00) | Grad Norm 7.5401(7.9219) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 17.5694(18.3578) | Bit/dim 3.6033(3.6469) | Xent 0.5208(0.5559) | Loss 67.2763(97.4242) | Error 0.1922(0.1987) Steps 0(0.00) | Grad Norm 6.7847(8.0846) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 17.7893(18.5036) | Bit/dim 3.6148(3.6449) | Xent 0.5812(0.5528) | Loss 61.1019(89.6225) | Error 0.2044(0.1982) Steps 0(0.00) | Grad Norm 10.6600(8.1617) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 18.2829(18.3575) | Bit/dim 3.6285(3.6479) | Xent 0.5712(0.5528) | Loss 71.3122(83.7684) | Error 0.1989(0.1968) Steps 0(0.00) | Grad Norm 10.6045(8.1375) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 16.8043(18.3281) | Bit/dim 3.6260(3.6453) | Xent 0.5317(0.5522) | Loss 61.4719(79.1419) | Error 0.1833(0.1959) Steps 0(0.00) | Grad Norm 6.3778(8.0099) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 18.1555(18.3511) | Bit/dim 3.6519(3.6466) | Xent 0.5962(0.5514) | Loss 64.8290(76.3058) | Error 0.2156(0.1956) Steps 0(0.00) | Grad Norm 8.7179(7.8071) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 91.1769, Epoch Time 1122.2156(1084.3630), Bit/dim 3.6457(best: 3.6418), Xent 0.7273, Loss 4.0094, Error 0.2459(best: 0.2331)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 17.4581(18.2854) | Bit/dim 3.6587(3.6472) | Xent 0.4899(0.5444) | Loss 68.8886(103.4230) | Error 0.1833(0.1930) Steps 0(0.00) | Grad Norm 7.0031(7.7505) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 16.5879(18.3358) | Bit/dim 3.6522(3.6478) | Xent 0.4494(0.5370) | Loss 64.5988(93.8141) | Error 0.1656(0.1911) Steps 0(0.00) | Grad Norm 4.5071(7.2407) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 18.2689(18.2629) | Bit/dim 3.6219(3.6463) | Xent 0.5076(0.5348) | Loss 68.8707(87.3972) | Error 0.1756(0.1911) Steps 0(0.00) | Grad Norm 6.0050(7.3078) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 17.2737(18.1998) | Bit/dim 3.6384(3.6450) | Xent 0.6025(0.5416) | Loss 67.0401(82.2742) | Error 0.2200(0.1946) Steps 0(0.00) | Grad Norm 11.9390(7.6419) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 19.9289(18.2752) | Bit/dim 3.6473(3.6405) | Xent 0.6269(0.5495) | Loss 69.2239(78.4908) | Error 0.2167(0.1971) Steps 0(0.00) | Grad Norm 7.0737(8.2193) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 91.4400, Epoch Time 1112.1733(1085.1973), Bit/dim 3.6492(best: 3.6418), Xent 0.6944, Loss 3.9964, Error 0.2408(best: 0.2331)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 18.4735(18.2739) | Bit/dim 3.6381(3.6448) | Xent 0.5270(0.5508) | Loss 67.4576(109.7248) | Error 0.1778(0.1969) Steps 0(0.00) | Grad Norm 5.1105(8.3515) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 18.4883(18.1938) | Bit/dim 3.6390(3.6426) | Xent 0.6265(0.5543) | Loss 62.3472(98.4066) | Error 0.2244(0.1985) Steps 0(0.00) | Grad Norm 10.1257(8.7182) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 19.1143(18.2245) | Bit/dim 3.6794(3.6465) | Xent 0.5582(0.5533) | Loss 70.3075(90.3005) | Error 0.1933(0.1978) Steps 0(0.00) | Grad Norm 7.7283(8.4761) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 18.8547(18.3624) | Bit/dim 3.6324(3.6452) | Xent 0.5592(0.5507) | Loss 67.3296(84.3233) | Error 0.1922(0.1960) Steps 0(0.00) | Grad Norm 5.0674(8.0766) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.6448(18.3594) | Bit/dim 3.6462(3.6413) | Xent 0.5088(0.5448) | Loss 67.7920(79.8191) | Error 0.1844(0.1947) Steps 0(0.00) | Grad Norm 6.2662(7.6840) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 18.1164(18.2995) | Bit/dim 3.6453(3.6448) | Xent 0.5180(0.5429) | Loss 68.0513(76.1744) | Error 0.1833(0.1926) Steps 0(0.00) | Grad Norm 4.6644(7.5254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 92.7522, Epoch Time 1116.3335(1086.1314), Bit/dim 3.6416(best: 3.6418), Xent 0.6643, Loss 3.9738, Error 0.2315(best: 0.2331)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.7492(18.1881) | Bit/dim 3.6568(3.6446) | Xent 0.5110(0.5366) | Loss 71.1580(102.7131) | Error 0.1944(0.1912) Steps 0(0.00) | Grad Norm 7.6096(7.5025) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 19.6677(18.3039) | Bit/dim 3.6366(3.6454) | Xent 0.5516(0.5323) | Loss 71.1108(93.7103) | Error 0.1933(0.1892) Steps 0(0.00) | Grad Norm 6.1878(7.4420) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 17.5040(18.1565) | Bit/dim 3.6594(3.6459) | Xent 0.5318(0.5305) | Loss 68.6925(86.4613) | Error 0.1889(0.1895) Steps 0(0.00) | Grad Norm 10.7950(7.4494) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 18.9238(18.2665) | Bit/dim 3.6438(3.6423) | Xent 0.5579(0.5283) | Loss 67.2924(81.4436) | Error 0.2011(0.1895) Steps 0(0.00) | Grad Norm 8.6230(7.4834) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 17.8112(18.4827) | Bit/dim 3.7027(3.6413) | Xent 0.5811(0.5318) | Loss 63.8994(77.7581) | Error 0.1967(0.1901) Steps 0(0.00) | Grad Norm 7.3452(7.3353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 93.2435, Epoch Time 1125.4949(1087.3123), Bit/dim 3.6506(best: 3.6416), Xent 0.6592, Loss 3.9801, Error 0.2271(best: 0.2315)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 17.0338(18.5071) | Bit/dim 3.6578(3.6425) | Xent 0.5393(0.5341) | Loss 66.2765(109.2887) | Error 0.1867(0.1905) Steps 0(0.00) | Grad Norm 8.0965(7.8585) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 17.5902(18.4712) | Bit/dim 3.6635(3.6435) | Xent 0.4775(0.5318) | Loss 66.9982(98.0395) | Error 0.1667(0.1896) Steps 0(0.00) | Grad Norm 6.9591(7.6036) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 17.1418(18.4688) | Bit/dim 3.6414(3.6442) | Xent 0.5909(0.5346) | Loss 64.8296(90.0736) | Error 0.2189(0.1903) Steps 0(0.00) | Grad Norm 6.9344(7.7207) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 18.8261(18.4823) | Bit/dim 3.6208(3.6438) | Xent 0.5618(0.5277) | Loss 65.2741(84.0839) | Error 0.2044(0.1885) Steps 0(0.00) | Grad Norm 6.8912(7.5934) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 18.7141(18.6205) | Bit/dim 3.6558(3.6463) | Xent 0.5400(0.5287) | Loss 65.3355(80.1045) | Error 0.1833(0.1891) Steps 0(0.00) | Grad Norm 6.7360(7.4195) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 20.3067(18.5757) | Bit/dim 3.5893(3.6407) | Xent 0.5752(0.5252) | Loss 69.1786(76.6757) | Error 0.1967(0.1870) Steps 0(0.00) | Grad Norm 8.8883(7.0834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 93.4775, Epoch Time 1130.4369(1088.6060), Bit/dim 3.6452(best: 3.6416), Xent 0.6732, Loss 3.9817, Error 0.2317(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 20.2729(18.5818) | Bit/dim 3.6513(3.6394) | Xent 0.5030(0.5206) | Loss 68.5991(103.5534) | Error 0.1733(0.1844) Steps 0(0.00) | Grad Norm 5.8476(7.6931) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.8774(18.6101) | Bit/dim 3.6338(3.6414) | Xent 0.6559(0.5243) | Loss 66.2637(93.6939) | Error 0.2356(0.1860) Steps 0(0.00) | Grad Norm 8.9456(7.8245) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 21.2923(18.6540) | Bit/dim 3.6208(3.6385) | Xent 0.5051(0.5186) | Loss 68.5163(86.9645) | Error 0.1889(0.1849) Steps 0(0.00) | Grad Norm 6.3725(7.2712) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 17.1761(18.5286) | Bit/dim 3.6199(3.6380) | Xent 0.5247(0.5222) | Loss 67.6864(82.0370) | Error 0.1900(0.1859) Steps 0(0.00) | Grad Norm 6.8103(7.2332) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 18.6127(18.4466) | Bit/dim 3.6741(3.6393) | Xent 0.4911(0.5256) | Loss 64.7455(78.0899) | Error 0.1922(0.1869) Steps 0(0.00) | Grad Norm 6.8642(7.5295) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 91.8551, Epoch Time 1125.2727(1089.7060), Bit/dim 3.6474(best: 3.6416), Xent 0.6705, Loss 3.9826, Error 0.2266(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 18.2768(18.4102) | Bit/dim 3.6352(3.6387) | Xent 0.5533(0.5287) | Loss 68.3175(109.5205) | Error 0.2011(0.1874) Steps 0(0.00) | Grad Norm 6.8723(7.3103) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 18.5105(18.3523) | Bit/dim 3.6515(3.6406) | Xent 0.4768(0.5218) | Loss 68.9799(98.7259) | Error 0.1822(0.1855) Steps 0(0.00) | Grad Norm 6.7042(7.0378) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 18.0937(18.4667) | Bit/dim 3.6322(3.6439) | Xent 0.5436(0.5192) | Loss 65.1649(90.5034) | Error 0.2000(0.1845) Steps 0(0.00) | Grad Norm 5.5570(7.2717) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 19.5182(18.3452) | Bit/dim 3.6109(3.6429) | Xent 0.5307(0.5218) | Loss 69.6717(84.3941) | Error 0.1778(0.1855) Steps 0(0.00) | Grad Norm 6.7146(7.0914) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 20.3481(18.4623) | Bit/dim 3.6048(3.6395) | Xent 0.4855(0.5107) | Loss 73.1906(80.3766) | Error 0.1678(0.1814) Steps 0(0.00) | Grad Norm 4.3593(6.6613) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 18.1154(18.5666) | Bit/dim 3.6200(3.6375) | Xent 0.5540(0.5165) | Loss 70.2131(77.0717) | Error 0.1989(0.1823) Steps 0(0.00) | Grad Norm 9.3225(7.0550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 95.0937, Epoch Time 1130.8895(1090.9415), Bit/dim 3.6410(best: 3.6416), Xent 0.6958, Loss 3.9889, Error 0.2394(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 18.1102(18.5400) | Bit/dim 3.6458(3.6358) | Xent 0.4855(0.5084) | Loss 63.8985(102.1199) | Error 0.1678(0.1797) Steps 0(0.00) | Grad Norm 8.2197(7.1837) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 17.3759(18.8723) | Bit/dim 3.6316(3.6341) | Xent 0.5069(0.5087) | Loss 66.5153(93.7676) | Error 0.1833(0.1804) Steps 0(0.00) | Grad Norm 8.2244(7.3534) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 19.1028(18.7806) | Bit/dim 3.6396(3.6349) | Xent 0.5600(0.5098) | Loss 70.1227(87.2367) | Error 0.2033(0.1801) Steps 0(0.00) | Grad Norm 7.1613(7.0072) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 16.6440(18.5418) | Bit/dim 3.6313(3.6359) | Xent 0.5276(0.5186) | Loss 61.2736(81.5714) | Error 0.1989(0.1839) Steps 0(0.00) | Grad Norm 8.4198(7.6547) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.4402(18.4713) | Bit/dim 3.6382(3.6389) | Xent 0.5590(0.5234) | Loss 65.3382(78.0664) | Error 0.1933(0.1848) Steps 0(0.00) | Grad Norm 6.3076(7.4877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 93.5039, Epoch Time 1132.2578(1092.1810), Bit/dim 3.6448(best: 3.6410), Xent 0.6934, Loss 3.9915, Error 0.2348(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.5198(18.3614) | Bit/dim 3.6414(3.6397) | Xent 0.4773(0.5164) | Loss 64.6693(108.6537) | Error 0.1856(0.1834) Steps 0(0.00) | Grad Norm 9.6590(7.6546) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 18.1791(18.5027) | Bit/dim 3.6454(3.6395) | Xent 0.5611(0.5162) | Loss 64.8948(97.7756) | Error 0.2033(0.1835) Steps 0(0.00) | Grad Norm 7.9681(7.6646) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 19.2604(18.6311) | Bit/dim 3.6428(3.6415) | Xent 0.4754(0.5086) | Loss 70.0915(89.8351) | Error 0.1678(0.1802) Steps 0(0.00) | Grad Norm 8.6258(7.4822) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 18.0449(18.7052) | Bit/dim 3.6119(3.6384) | Xent 0.5354(0.5123) | Loss 66.5923(84.4249) | Error 0.1822(0.1821) Steps 0(0.00) | Grad Norm 6.7681(7.4608) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 18.0881(18.6083) | Bit/dim 3.6454(3.6369) | Xent 0.5423(0.5183) | Loss 66.3943(80.1596) | Error 0.1856(0.1846) Steps 0(0.00) | Grad Norm 8.5378(7.8060) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 20.5452(18.8387) | Bit/dim 3.6325(3.6360) | Xent 0.5355(0.5168) | Loss 69.6392(77.2494) | Error 0.1889(0.1844) Steps 0(0.00) | Grad Norm 12.9537(7.7448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 92.9889, Epoch Time 1147.6759(1093.8459), Bit/dim 3.6392(best: 3.6410), Xent 0.6903, Loss 3.9844, Error 0.2367(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 18.5039(18.8816) | Bit/dim 3.6550(3.6373) | Xent 0.4463(0.5183) | Loss 69.6159(104.4728) | Error 0.1567(0.1845) Steps 0(0.00) | Grad Norm 5.1773(7.8574) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 19.9637(18.8932) | Bit/dim 3.6230(3.6385) | Xent 0.5557(0.5171) | Loss 71.4789(95.0079) | Error 0.1922(0.1839) Steps 0(0.00) | Grad Norm 8.3206(8.0481) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 19.6373(19.0329) | Bit/dim 3.6283(3.6385) | Xent 0.4943(0.5195) | Loss 72.3276(88.6274) | Error 0.1678(0.1847) Steps 0(0.00) | Grad Norm 5.4426(7.9000) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 20.5403(19.1863) | Bit/dim 3.6204(3.6364) | Xent 0.5322(0.5157) | Loss 74.1662(83.5392) | Error 0.1944(0.1825) Steps 0(0.00) | Grad Norm 5.6787(7.5194) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 17.9061(19.0746) | Bit/dim 3.5990(3.6346) | Xent 0.4876(0.5105) | Loss 66.8978(79.5513) | Error 0.1700(0.1802) Steps 0(0.00) | Grad Norm 5.7848(7.3293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 92.9803, Epoch Time 1161.6237(1095.8792), Bit/dim 3.6324(best: 3.6392), Xent 0.6759, Loss 3.9703, Error 0.2320(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 18.7691(18.9992) | Bit/dim 3.6379(3.6336) | Xent 0.4380(0.5034) | Loss 69.9342(110.5370) | Error 0.1478(0.1782) Steps 0(0.00) | Grad Norm 8.2731(7.2485) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 18.8407(18.8020) | Bit/dim 3.6681(3.6368) | Xent 0.4788(0.4993) | Loss 69.8649(99.2195) | Error 0.1778(0.1771) Steps 0(0.00) | Grad Norm 7.4568(7.2757) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 17.4677(18.6333) | Bit/dim 3.6127(3.6359) | Xent 0.4959(0.5041) | Loss 67.9103(91.2601) | Error 0.1822(0.1785) Steps 0(0.00) | Grad Norm 6.1658(7.4074) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 18.3214(18.7333) | Bit/dim 3.6418(3.6359) | Xent 0.5301(0.5102) | Loss 70.6215(85.2068) | Error 0.1878(0.1804) Steps 0(0.00) | Grad Norm 8.5687(7.9141) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 21.0763(18.8330) | Bit/dim 3.6168(3.6330) | Xent 0.4854(0.5069) | Loss 73.8050(80.6148) | Error 0.1722(0.1804) Steps 0(0.00) | Grad Norm 4.8562(7.5710) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.8212(18.9492) | Bit/dim 3.6140(3.6310) | Xent 0.4716(0.5032) | Loss 69.4528(77.8493) | Error 0.1622(0.1790) Steps 0(0.00) | Grad Norm 7.2990(7.3015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 93.6966, Epoch Time 1143.9369(1097.3209), Bit/dim 3.6342(best: 3.6324), Xent 0.6504, Loss 3.9594, Error 0.2230(best: 0.2266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 19.6521(18.9611) | Bit/dim 3.6343(3.6325) | Xent 0.4120(0.4989) | Loss 70.9155(105.6293) | Error 0.1556(0.1781) Steps 0(0.00) | Grad Norm 7.2535(7.7295) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 21.6840(18.9592) | Bit/dim 3.6342(3.6323) | Xent 0.5294(0.4939) | Loss 72.5683(96.1248) | Error 0.1800(0.1751) Steps 0(0.00) | Grad Norm 6.4476(7.5571) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 18.7065(18.9366) | Bit/dim 3.6212(3.6308) | Xent 0.4913(0.4922) | Loss 68.5967(89.1035) | Error 0.1811(0.1749) Steps 0(0.00) | Grad Norm 6.1910(7.3798) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 18.4598(18.8755) | Bit/dim 3.6450(3.6330) | Xent 0.5013(0.5014) | Loss 67.6364(83.6900) | Error 0.1689(0.1777) Steps 0(0.00) | Grad Norm 8.2475(7.7521) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 21.4304(18.8974) | Bit/dim 3.6387(3.6337) | Xent 0.5138(0.5069) | Loss 75.1529(79.6021) | Error 0.1822(0.1797) Steps 0(0.00) | Grad Norm 8.0505(7.8654) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 94.3238, Epoch Time 1147.1097(1098.8146), Bit/dim 3.6429(best: 3.6324), Xent 0.6843, Loss 3.9851, Error 0.2324(best: 0.2230)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 16.4682(18.8121) | Bit/dim 3.6079(3.6345) | Xent 0.5065(0.5118) | Loss 64.0393(111.3010) | Error 0.1989(0.1826) Steps 0(0.00) | Grad Norm 8.2053(8.1455) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.0370(18.7270) | Bit/dim 3.6453(3.6353) | Xent 0.5763(0.5074) | Loss 70.7164(100.0358) | Error 0.1922(0.1802) Steps 0(0.00) | Grad Norm 10.7563(7.8471) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 18.4318(18.7286) | Bit/dim 3.6273(3.6366) | Xent 0.5107(0.5050) | Loss 68.9616(91.7980) | Error 0.1689(0.1788) Steps 0(0.00) | Grad Norm 8.3635(7.8620) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.8717(18.8086) | Bit/dim 3.6312(3.6394) | Xent 0.5563(0.5039) | Loss 73.8487(86.1270) | Error 0.1878(0.1786) Steps 0(0.00) | Grad Norm 8.5883(7.8449) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 19.3872(18.8667) | Bit/dim 3.6401(3.6382) | Xent 0.5796(0.5062) | Loss 64.7271(81.3984) | Error 0.2056(0.1798) Steps 0(0.00) | Grad Norm 9.5405(7.8642) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 20.1994(18.8914) | Bit/dim 3.6067(3.6368) | Xent 0.4518(0.5061) | Loss 69.3966(77.9044) | Error 0.1689(0.1808) Steps 0(0.00) | Grad Norm 8.2510(7.9883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 93.4091, Epoch Time 1149.5725(1100.3373), Bit/dim 3.6358(best: 3.6324), Xent 0.6590, Loss 3.9653, Error 0.2228(best: 0.2230)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.3150(18.8411) | Bit/dim 3.6300(3.6367) | Xent 0.4509(0.5001) | Loss 66.6499(105.2457) | Error 0.1578(0.1776) Steps 0(0.00) | Grad Norm 4.0439(7.9188) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 20.5510(18.9568) | Bit/dim 3.6318(3.6379) | Xent 0.5385(0.4954) | Loss 74.5689(96.0045) | Error 0.1889(0.1746) Steps 0(0.00) | Grad Norm 11.1779(7.8164) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 20.9369(19.1170) | Bit/dim 3.6493(3.6334) | Xent 0.6072(0.4907) | Loss 68.9327(83.5742) | Error 0.2056(0.1741) Steps 0(0.00) | Grad Norm 12.8885(7.4241) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.9860(19.1211) | Bit/dim 3.6307(3.6319) | Xent 0.4628(0.4883) | Loss 68.5558(79.8775) | Error 0.1700(0.1739) Steps 0(0.00) | Grad Norm 10.3899(7.5283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 93.2921, Epoch Time 1169.8807(1102.4236), Bit/dim 3.6335(best: 3.6324), Xent 0.6765, Loss 3.9718, Error 0.2351(best: 0.2228)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 19.1549(19.1178) | Bit/dim 3.5919(3.6272) | Xent 0.4031(0.4898) | Loss 68.5908(113.1001) | Error 0.1278(0.1731) Steps 0(0.00) | Grad Norm 5.1176(7.7155) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 19.8719(19.1247) | Bit/dim 3.6190(3.6299) | Xent 0.4776(0.4841) | Loss 68.4574(101.3893) | Error 0.1656(0.1712) Steps 0(0.00) | Grad Norm 8.6027(7.9624) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 18.3976(19.1681) | Bit/dim 3.6444(3.6310) | Xent 0.4494(0.4785) | Loss 68.7612(92.6903) | Error 0.1522(0.1689) Steps 0(0.00) | Grad Norm 3.8135(7.5816) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 18.0456(19.0915) | Bit/dim 3.6193(3.6297) | Xent 0.4985(0.4845) | Loss 63.6255(86.5462) | Error 0.1789(0.1709) Steps 0(0.00) | Grad Norm 6.6328(7.7043) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 18.1250(18.9695) | Bit/dim 3.6177(3.6323) | Xent 0.5127(0.4873) | Loss 68.4290(81.4092) | Error 0.1767(0.1731) Steps 0(0.00) | Grad Norm 7.3839(7.8542) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 18.5429(19.1128) | Bit/dim 3.6593(3.6342) | Xent 0.4988(0.4865) | Loss 64.1069(77.7039) | Error 0.1844(0.1733) Steps 0(0.00) | Grad Norm 6.9845(7.6062) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 94.2991, Epoch Time 1158.6203(1104.1095), Bit/dim 3.6376(best: 3.6324), Xent 0.6665, Loss 3.9709, Error 0.2213(best: 0.2228)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 19.3125(18.9767) | Bit/dim 3.5989(3.6319) | Xent 0.4584(0.4864) | Loss 71.1270(104.4439) | Error 0.1489(0.1723) Steps 0(0.00) | Grad Norm 5.9073(7.7332) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 20.0977(19.0073) | Bit/dim 3.6284(3.6322) | Xent 0.4662(0.4835) | Loss 72.7590(95.3525) | Error 0.1556(0.1712) Steps 0(0.00) | Grad Norm 9.7356(7.4311) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 18.5054(18.9125) | Bit/dim 3.6455(3.6329) | Xent 0.5370(0.4790) | Loss 69.7961(88.2231) | Error 0.1767(0.1693) Steps 0(0.00) | Grad Norm 8.2599(7.2785) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 18.0028(18.9218) | Bit/dim 3.6378(3.6343) | Xent 0.4624(0.4772) | Loss 68.1881(82.9808) | Error 0.1611(0.1695) Steps 0(0.00) | Grad Norm 4.5275(7.2494) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 18.3315(18.8121) | Bit/dim 3.5999(3.6314) | Xent 0.5314(0.4869) | Loss 70.9907(79.2224) | Error 0.1800(0.1728) Steps 0(0.00) | Grad Norm 9.7940(7.5313) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 94.9744, Epoch Time 1148.8146(1105.4507), Bit/dim 3.6399(best: 3.6324), Xent 0.6627, Loss 3.9713, Error 0.2243(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 18.6379(18.9166) | Bit/dim 3.6364(3.6327) | Xent 0.4444(0.4848) | Loss 68.4060(111.5300) | Error 0.1622(0.1721) Steps 0(0.00) | Grad Norm 5.9652(7.6500) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.2595(18.9538) | Bit/dim 3.6210(3.6334) | Xent 0.4996(0.4860) | Loss 67.8279(100.0228) | Error 0.1644(0.1720) Steps 0(0.00) | Grad Norm 6.8138(7.6958) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 18.4100(18.9790) | Bit/dim 3.6191(3.6339) | Xent 0.4749(0.4862) | Loss 67.7987(91.6603) | Error 0.1678(0.1729) Steps 0(0.00) | Grad Norm 9.3223(7.7338) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 17.9587(18.9115) | Bit/dim 3.6185(3.6303) | Xent 0.4775(0.4847) | Loss 68.7029(85.6572) | Error 0.1678(0.1721) Steps 0(0.00) | Grad Norm 7.3740(7.5016) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 18.7689(18.9561) | Bit/dim 3.6483(3.6282) | Xent 0.5437(0.4876) | Loss 66.6522(81.4221) | Error 0.1889(0.1730) Steps 0(0.00) | Grad Norm 9.9021(7.4423) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 17.3516(18.8580) | Bit/dim 3.6669(3.6285) | Xent 0.5110(0.4869) | Loss 64.3837(77.9150) | Error 0.1922(0.1725) Steps 0(0.00) | Grad Norm 8.6892(7.1420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 94.6604, Epoch Time 1153.3924(1106.8889), Bit/dim 3.6327(best: 3.6324), Xent 0.6766, Loss 3.9710, Error 0.2222(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 18.8951(18.9963) | Bit/dim 3.6330(3.6287) | Xent 0.4535(0.4842) | Loss 71.1172(105.9966) | Error 0.1644(0.1726) Steps 0(0.00) | Grad Norm 7.2952(7.5739) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 18.5151(18.9429) | Bit/dim 3.6386(3.6307) | Xent 0.4854(0.4901) | Loss 74.0594(96.5779) | Error 0.1756(0.1748) Steps 0(0.00) | Grad Norm 6.7698(7.7622) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 18.5964(18.9666) | Bit/dim 3.6225(3.6302) | Xent 0.4742(0.4814) | Loss 67.7507(89.7784) | Error 0.1689(0.1713) Steps 0(0.00) | Grad Norm 9.1604(7.5491) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 20.2583(18.9986) | Bit/dim 3.6242(3.6315) | Xent 0.5258(0.4934) | Loss 72.8327(84.8262) | Error 0.1878(0.1742) Steps 0(0.00) | Grad Norm 9.9103(8.0826) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 18.1860(19.0656) | Bit/dim 3.6279(3.6338) | Xent 0.4420(0.4950) | Loss 67.2796(81.1221) | Error 0.1411(0.1747) Steps 0(0.00) | Grad Norm 5.9702(8.0808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 93.8711, Epoch Time 1161.4259(1108.5251), Bit/dim 3.6391(best: 3.6324), Xent 0.6497, Loss 3.9640, Error 0.2244(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 19.0940(19.0636) | Bit/dim 3.6435(3.6364) | Xent 0.4852(0.4944) | Loss 72.1134(113.4437) | Error 0.1644(0.1757) Steps 0(0.00) | Grad Norm 6.2191(7.9188) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 17.4211(18.9465) | Bit/dim 3.6246(3.6359) | Xent 0.5285(0.4871) | Loss 68.4020(101.4681) | Error 0.1978(0.1735) Steps 0(0.00) | Grad Norm 13.0222(7.9356) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 18.4796(18.6858) | Bit/dim 3.6203(3.6357) | Xent 0.4487(0.4881) | Loss 67.8856(92.8495) | Error 0.1467(0.1733) Steps 0(0.00) | Grad Norm 5.8253(8.0579) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 20.5728(18.9170) | Bit/dim 3.6111(3.6341) | Xent 0.4757(0.4789) | Loss 73.4350(86.6507) | Error 0.1644(0.1695) Steps 0(0.00) | Grad Norm 4.7954(7.6034) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 16.8146(18.8099) | Bit/dim 3.6144(3.6339) | Xent 0.5870(0.4853) | Loss 66.8626(81.9940) | Error 0.2100(0.1725) Steps 0(0.00) | Grad Norm 7.7938(7.7493) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 21.2871(18.8964) | Bit/dim 3.6441(3.6302) | Xent 0.5007(0.4860) | Loss 72.1862(78.7467) | Error 0.1811(0.1727) Steps 0(0.00) | Grad Norm 9.7224(7.4855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 94.0164, Epoch Time 1147.9536(1109.7079), Bit/dim 3.6259(best: 3.6324), Xent 0.6716, Loss 3.9617, Error 0.2278(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.7700(18.8465) | Bit/dim 3.5925(3.6286) | Xent 0.4769(0.4852) | Loss 70.8675(106.2559) | Error 0.1856(0.1741) Steps 0(0.00) | Grad Norm 8.8490(7.7497) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 21.0759(19.0841) | Bit/dim 3.6107(3.6294) | Xent 0.4425(0.4757) | Loss 72.8696(96.6841) | Error 0.1622(0.1713) Steps 0(0.00) | Grad Norm 7.5088(7.4580) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 17.9517(18.9200) | Bit/dim 3.6083(3.6297) | Xent 0.5170(0.4770) | Loss 68.8807(89.4853) | Error 0.1711(0.1713) Steps 0(0.00) | Grad Norm 10.0768(7.5483) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 17.2160(18.7820) | Bit/dim 3.6326(3.6295) | Xent 0.5229(0.4795) | Loss 66.7330(83.9291) | Error 0.1789(0.1724) Steps 0(0.00) | Grad Norm 7.5647(7.7528) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 18.3585(18.8912) | Bit/dim 3.6235(3.6302) | Xent 0.4633(0.4750) | Loss 68.1352(79.7815) | Error 0.1689(0.1696) Steps 0(0.00) | Grad Norm 5.6235(7.5679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 94.5873, Epoch Time 1150.0355(1110.9177), Bit/dim 3.6259(best: 3.6259), Xent 0.6479, Loss 3.9498, Error 0.2191(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 19.9742(18.8650) | Bit/dim 3.6355(3.6314) | Xent 0.4397(0.4716) | Loss 68.6720(111.8086) | Error 0.1444(0.1670) Steps 0(0.00) | Grad Norm 5.8871(7.4278) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 18.2125(18.7810) | Bit/dim 3.6112(3.6304) | Xent 0.5238(0.4720) | Loss 67.2728(100.7211) | Error 0.2000(0.1666) Steps 0(0.00) | Grad Norm 11.6577(7.5848) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.1019(18.8960) | Bit/dim 3.6340(3.6292) | Xent 0.4315(0.4698) | Loss 68.9559(92.6791) | Error 0.1467(0.1644) Steps 0(0.00) | Grad Norm 8.9202(7.7757) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 18.7636(18.9210) | Bit/dim 3.6196(3.6295) | Xent 0.4102(0.4674) | Loss 67.2254(86.4920) | Error 0.1411(0.1645) Steps 0(0.00) | Grad Norm 7.2253(7.7408) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 19.9263(19.1154) | Bit/dim 3.6052(3.6259) | Xent 0.4623(0.4682) | Loss 73.7641(82.2196) | Error 0.1733(0.1662) Steps 0(0.00) | Grad Norm 8.1215(7.4395) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 19.7237(19.2826) | Bit/dim 3.6131(3.6262) | Xent 0.5226(0.4652) | Loss 70.6019(78.8765) | Error 0.1800(0.1649) Steps 0(0.00) | Grad Norm 10.4165(7.1820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 95.6221, Epoch Time 1171.7160(1112.7417), Bit/dim 3.6322(best: 3.6259), Xent 0.6402, Loss 3.9523, Error 0.2149(best: 0.2191)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 17.9839(19.2173) | Bit/dim 3.6134(3.6275) | Xent 0.5753(0.4669) | Loss 68.7300(107.1499) | Error 0.2022(0.1658) Steps 0(0.00) | Grad Norm 8.0492(7.5828) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 19.0498(19.1263) | Bit/dim 3.6295(3.6285) | Xent 0.4190(0.4626) | Loss 66.4817(96.8424) | Error 0.1500(0.1643) Steps 0(0.00) | Grad Norm 5.7658(7.4518) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 16.2784(19.0684) | Bit/dim 3.6855(3.6331) | Xent 0.4293(0.4622) | Loss 66.5565(89.3269) | Error 0.1644(0.1639) Steps 0(0.00) | Grad Norm 11.5091(7.8221) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 18.1197(18.9956) | Bit/dim 3.6258(3.6293) | Xent 0.4416(0.4661) | Loss 72.6405(84.4065) | Error 0.1511(0.1651) Steps 0(0.00) | Grad Norm 4.1793(7.6463) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 20.4232(19.0031) | Bit/dim 3.6211(3.6282) | Xent 0.5186(0.4660) | Loss 70.4626(80.2706) | Error 0.1789(0.1647) Steps 0(0.00) | Grad Norm 10.8127(7.6945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 94.1623, Epoch Time 1154.5636(1113.9963), Bit/dim 3.6314(best: 3.6259), Xent 0.6471, Loss 3.9550, Error 0.2190(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 19.4575(19.0044) | Bit/dim 3.6481(3.6277) | Xent 0.4935(0.4727) | Loss 70.7125(110.9984) | Error 0.1733(0.1670) Steps 0(0.00) | Grad Norm 8.1564(8.1221) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 20.2232(18.9732) | Bit/dim 3.6426(3.6295) | Xent 0.4581(0.4652) | Loss 70.7317(100.1147) | Error 0.1667(0.1646) Steps 0(0.00) | Grad Norm 8.4685(7.9644) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 18.5010(18.9469) | Bit/dim 3.6349(3.6290) | Xent 0.4331(0.4617) | Loss 65.9675(92.1485) | Error 0.1522(0.1637) Steps 0(0.00) | Grad Norm 8.4152(7.7251) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 19.2901(19.0271) | Bit/dim 3.6383(3.6283) | Xent 0.5383(0.4737) | Loss 72.8092(86.3218) | Error 0.1867(0.1674) Steps 0(0.00) | Grad Norm 9.4289(8.4661) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 19.2396(19.1223) | Bit/dim 3.6572(3.6274) | Xent 0.4782(0.4829) | Loss 68.8694(81.7005) | Error 0.1700(0.1707) Steps 0(0.00) | Grad Norm 6.6146(8.5540) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 21.6364(19.3707) | Bit/dim 3.6447(3.6286) | Xent 0.4424(0.4780) | Loss 72.2692(78.5230) | Error 0.1567(0.1697) Steps 0(0.00) | Grad Norm 7.4044(8.2255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 94.9404, Epoch Time 1172.2010(1115.7425), Bit/dim 3.6253(best: 3.6259), Xent 0.6423, Loss 3.9464, Error 0.2157(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 19.4783(19.2132) | Bit/dim 3.6296(3.6300) | Xent 0.4582(0.4744) | Loss 70.3533(106.1103) | Error 0.1667(0.1686) Steps 0(0.00) | Grad Norm 10.8009(8.3165) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 20.4017(19.1295) | Bit/dim 3.5962(3.6276) | Xent 0.4042(0.4677) | Loss 67.3418(96.1839) | Error 0.1467(0.1665) Steps 0(0.00) | Grad Norm 6.8130(8.0977) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 18.0375(19.0439) | Bit/dim 3.6622(3.6254) | Xent 0.4798(0.4595) | Loss 68.3286(88.9613) | Error 0.1722(0.1627) Steps 0(0.00) | Grad Norm 6.7521(7.6339) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 17.7900(19.0344) | Bit/dim 3.5952(3.6239) | Xent 0.4699(0.4632) | Loss 67.1881(83.6579) | Error 0.1744(0.1648) Steps 0(0.00) | Grad Norm 5.0079(7.5030) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 17.9665(18.9374) | Bit/dim 3.6281(3.6236) | Xent 0.4520(0.4631) | Loss 67.7603(79.4328) | Error 0.1644(0.1656) Steps 0(0.00) | Grad Norm 5.8767(7.5413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 92.6793, Epoch Time 1148.8868(1116.7368), Bit/dim 3.6294(best: 3.6253), Xent 0.6550, Loss 3.9568, Error 0.2188(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.5049(19.0380) | Bit/dim 3.6160(3.6218) | Xent 0.4601(0.4588) | Loss 73.9890(109.4592) | Error 0.1633(0.1631) Steps 0(0.00) | Grad Norm 6.3710(7.7645) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 18.0982(19.0229) | Bit/dim 3.6287(3.6205) | Xent 0.4732(0.4536) | Loss 66.6908(98.8231) | Error 0.1656(0.1612) Steps 0(0.00) | Grad Norm 10.1821(7.7502) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 16.4869(19.0488) | Bit/dim 3.6496(3.6229) | Xent 0.4722(0.4499) | Loss 69.8461(91.0597) | Error 0.1767(0.1597) Steps 0(0.00) | Grad Norm 4.8587(7.5915) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 20.2208(19.0388) | Bit/dim 3.6153(3.6216) | Xent 0.5048(0.4565) | Loss 72.1160(85.3243) | Error 0.1856(0.1629) Steps 0(0.00) | Grad Norm 7.1964(7.5924) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 18.1982(19.0601) | Bit/dim 3.6369(3.6232) | Xent 0.5360(0.4641) | Loss 70.7371(81.1203) | Error 0.1889(0.1653) Steps 0(0.00) | Grad Norm 7.4753(7.6416) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 19.7895(18.9336) | Bit/dim 3.6391(3.6246) | Xent 0.4555(0.4579) | Loss 70.1350(77.6844) | Error 0.1567(0.1630) Steps 0(0.00) | Grad Norm 7.2080(7.2253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 93.0111, Epoch Time 1156.8122(1117.9391), Bit/dim 3.6234(best: 3.6253), Xent 0.6631, Loss 3.9549, Error 0.2254(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 18.7281(18.9210) | Bit/dim 3.6199(3.6267) | Xent 0.4690(0.4534) | Loss 70.8164(105.8522) | Error 0.1689(0.1616) Steps 0(0.00) | Grad Norm 6.3248(7.2686) | Total Time 0.00(0.00)\n",
      "Iter 7500 | Time 21.1549(19.1069) | Bit/dim 3.6132(3.6232) | Xent 0.4645(0.4597) | Loss 72.7483(96.7187) | Error 0.1667(0.1643) Steps 0(0.00) | Grad Norm 6.4529(7.7536) | Total Time 0.00(0.00)\n",
      "Iter 7510 | Time 19.1996(19.4254) | Bit/dim 3.6245(3.6230) | Xent 0.4018(0.4506) | Loss 66.9722(89.8545) | Error 0.1578(0.1615) Steps 0(0.00) | Grad Norm 4.8879(7.2876) | Total Time 0.00(0.00)\n",
      "Iter 7520 | Time 19.6291(19.4961) | Bit/dim 3.6178(3.6230) | Xent 0.4779(0.4524) | Loss 69.1220(84.2239) | Error 0.1689(0.1613) Steps 0(0.00) | Grad Norm 10.9791(7.5072) | Total Time 0.00(0.00)\n",
      "Iter 7530 | Time 19.2657(19.3241) | Bit/dim 3.6237(3.6238) | Xent 0.4542(0.4513) | Loss 72.9478(80.3807) | Error 0.1556(0.1605) Steps 0(0.00) | Grad Norm 8.7543(7.0585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 95.2176, Epoch Time 1181.6894(1119.8516), Bit/dim 3.6296(best: 3.6234), Xent 0.7011, Loss 3.9801, Error 0.2265(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 17.7464(19.2323) | Bit/dim 3.6624(3.6245) | Xent 0.4430(0.4555) | Loss 70.3547(112.2852) | Error 0.1456(0.1619) Steps 0(0.00) | Grad Norm 8.2816(7.4291) | Total Time 0.00(0.00)\n",
      "Iter 7550 | Time 18.0455(19.2523) | Bit/dim 3.6029(3.6235) | Xent 0.4508(0.4548) | Loss 65.5238(100.9187) | Error 0.1544(0.1612) Steps 0(0.00) | Grad Norm 6.2968(7.2630) | Total Time 0.00(0.00)\n",
      "Iter 7560 | Time 17.8205(19.2895) | Bit/dim 3.6154(3.6216) | Xent 0.3971(0.4482) | Loss 69.2722(92.9837) | Error 0.1567(0.1590) Steps 0(0.00) | Grad Norm 6.4280(6.8811) | Total Time 0.00(0.00)\n",
      "Iter 7570 | Time 19.2297(19.2168) | Bit/dim 3.6420(3.6247) | Xent 0.4269(0.4433) | Loss 69.5917(86.8830) | Error 0.1578(0.1567) Steps 0(0.00) | Grad Norm 10.6799(6.9767) | Total Time 0.00(0.00)\n",
      "Iter 7580 | Time 18.1279(19.3368) | Bit/dim 3.6032(3.6191) | Xent 0.4596(0.4461) | Loss 65.5276(82.2509) | Error 0.1678(0.1578) Steps 0(0.00) | Grad Norm 6.7725(7.1411) | Total Time 0.00(0.00)\n",
      "Iter 7590 | Time 19.5171(19.2601) | Bit/dim 3.6521(3.6227) | Xent 0.5850(0.4532) | Loss 68.7111(78.9118) | Error 0.2011(0.1603) Steps 0(0.00) | Grad Norm 15.6026(7.3844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 94.2425, Epoch Time 1171.8327(1121.4110), Bit/dim 3.6200(best: 3.6234), Xent 0.6756, Loss 3.9578, Error 0.2308(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 17.0713(19.2432) | Bit/dim 3.6085(3.6223) | Xent 0.4618(0.4583) | Loss 66.3783(105.8811) | Error 0.1611(0.1634) Steps 0(0.00) | Grad Norm 9.0509(7.7088) | Total Time 0.00(0.00)\n",
      "Iter 7610 | Time 18.1255(19.2097) | Bit/dim 3.6238(3.6221) | Xent 0.4897(0.4513) | Loss 66.5066(96.1438) | Error 0.1756(0.1615) Steps 0(0.00) | Grad Norm 6.4796(7.6861) | Total Time 0.00(0.00)\n",
      "Iter 7620 | Time 18.2853(19.0428) | Bit/dim 3.5972(3.6182) | Xent 0.4220(0.4492) | Loss 70.6484(89.0664) | Error 0.1589(0.1599) Steps 0(0.00) | Grad Norm 8.0200(7.1667) | Total Time 0.00(0.00)\n",
      "Iter 7630 | Time 18.6017(19.2027) | Bit/dim 3.6379(3.6197) | Xent 0.4036(0.4471) | Loss 69.5149(84.3072) | Error 0.1467(0.1594) Steps 0(0.00) | Grad Norm 5.5211(6.9926) | Total Time 0.00(0.00)\n",
      "Iter 7640 | Time 19.9417(19.3397) | Bit/dim 3.5888(3.6219) | Xent 0.4589(0.4448) | Loss 70.9715(80.5703) | Error 0.1678(0.1580) Steps 0(0.00) | Grad Norm 4.0739(6.5380) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 95.1464, Epoch Time 1171.7706(1122.9218), Bit/dim 3.6182(best: 3.6200), Xent 0.6495, Loss 3.9429, Error 0.2123(best: 0.2149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 18.7645(19.3643) | Bit/dim 3.6397(3.6204) | Xent 0.4030(0.4370) | Loss 68.7977(113.0315) | Error 0.1533(0.1559) Steps 0(0.00) | Grad Norm 5.7098(6.3773) | Total Time 0.00(0.00)\n",
      "Iter 7660 | Time 17.9721(19.3500) | Bit/dim 3.6258(3.6188) | Xent 0.3901(0.4258) | Loss 68.8314(101.4750) | Error 0.1389(0.1520) Steps 0(0.00) | Grad Norm 5.5842(6.1845) | Total Time 0.00(0.00)\n",
      "Iter 7670 | Time 18.4270(19.3514) | Bit/dim 3.5918(3.6141) | Xent 0.4081(0.4280) | Loss 66.3829(93.1571) | Error 0.1422(0.1527) Steps 0(0.00) | Grad Norm 5.0045(6.3850) | Total Time 0.00(0.00)\n",
      "Iter 7680 | Time 18.6244(19.2816) | Bit/dim 3.6311(3.6153) | Xent 0.4495(0.4314) | Loss 70.0309(86.8303) | Error 0.1633(0.1531) Steps 0(0.00) | Grad Norm 9.2711(6.5877) | Total Time 0.00(0.00)\n",
      "Iter 7690 | Time 18.9864(19.2906) | Bit/dim 3.6289(3.6199) | Xent 0.4637(0.4371) | Loss 66.7533(82.1821) | Error 0.1544(0.1545) Steps 0(0.00) | Grad Norm 12.9757(7.1967) | Total Time 0.00(0.00)\n",
      "Iter 7700 | Time 19.4002(19.2518) | Bit/dim 3.6142(3.6221) | Xent 0.4759(0.4344) | Loss 71.9322(78.9489) | Error 0.1744(0.1540) Steps 0(0.00) | Grad Norm 7.6163(6.8448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 94.7612, Epoch Time 1174.8268(1124.4790), Bit/dim 3.6222(best: 3.6182), Xent 0.6946, Loss 3.9695, Error 0.2289(best: 0.2123)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 19.4908(19.2912) | Bit/dim 3.6300(3.6216) | Xent 0.4447(0.4359) | Loss 72.5369(107.1836) | Error 0.1700(0.1552) Steps 0(0.00) | Grad Norm 6.9692(6.9862) | Total Time 0.00(0.00)\n",
      "Iter 7720 | Time 16.9316(19.0854) | Bit/dim 3.5858(3.6225) | Xent 0.4270(0.4298) | Loss 66.9630(96.9711) | Error 0.1500(0.1526) Steps 0(0.00) | Grad Norm 5.7307(6.7056) | Total Time 0.00(0.00)\n",
      "Iter 7730 | Time 19.1827(19.0033) | Bit/dim 3.6334(3.6216) | Xent 0.4184(0.4332) | Loss 66.8939(89.2500) | Error 0.1389(0.1541) Steps 0(0.00) | Grad Norm 5.6195(7.1884) | Total Time 0.00(0.00)\n",
      "Iter 7740 | Time 18.9747(19.1540) | Bit/dim 3.6160(3.6190) | Xent 0.4608(0.4321) | Loss 68.3372(84.2415) | Error 0.1611(0.1532) Steps 0(0.00) | Grad Norm 8.4525(7.0917) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_5_0_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_5_0_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 5.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
