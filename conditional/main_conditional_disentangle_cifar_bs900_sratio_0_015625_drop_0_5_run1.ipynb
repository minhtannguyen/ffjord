{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.015625, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_015625_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=96, bias=True)\n",
      "  (project_class): LinearZeros(in_features=48, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1360630\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 12.7373(29.3302) | Bit/dim 8.9949(9.2173) | Xent 2.3026(2.3026) | Loss 8.9949(9.2173) | Error 0.9067(0.8927) Steps 574(574.00) | Grad Norm 12.8982(17.0772) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 12.9283(24.9694) | Bit/dim 8.6545(9.0882) | Xent 2.3026(2.3026) | Loss 8.6545(9.0882) | Error 0.8833(0.8945) Steps 574(574.00) | Grad Norm 4.8625(14.6550) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 12.6695(21.7783) | Bit/dim 8.3203(8.9225) | Xent 2.3026(2.3026) | Loss 8.3203(8.9225) | Error 0.8856(0.8956) Steps 574(574.00) | Grad Norm 2.9134(11.7557) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 12.7121(19.3958) | Bit/dim 8.2450(8.7617) | Xent 2.3026(2.3026) | Loss 8.2450(8.7617) | Error 0.9000(0.8976) Steps 574(574.00) | Grad Norm 2.9029(9.4699) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 12.6624(17.6616) | Bit/dim 7.9508(8.5826) | Xent 2.3026(2.3026) | Loss 7.9508(8.5826) | Error 0.8967(0.8979) Steps 574(574.00) | Grad Norm 2.6203(7.7188) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 76.7655, Epoch Time 817.3292(817.3292), Bit/dim 7.7815(best: inf), Xent 2.3026, Loss 7.7815, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.7846(16.4417) | Bit/dim 7.6610(8.3721) | Xent 2.3026(2.3026) | Loss 7.6610(8.3721) | Error 0.9144(0.8979) Steps 574(574.00) | Grad Norm 2.5594(6.3612) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 12.6113(15.4575) | Bit/dim 7.3760(8.1399) | Xent 2.3026(2.3026) | Loss 7.3760(8.1399) | Error 0.9011(0.8983) Steps 574(574.00) | Grad Norm 2.3763(5.2947) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 12.9538(14.7156) | Bit/dim 7.1518(7.9033) | Xent 2.3026(2.3026) | Loss 7.1518(7.9033) | Error 0.9033(0.8991) Steps 574(574.00) | Grad Norm 1.2790(4.3485) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 13.7812(14.3276) | Bit/dim 7.0643(7.6912) | Xent 2.3026(2.3026) | Loss 7.0643(7.6912) | Error 0.9022(0.9005) Steps 598(578.20) | Grad Norm 0.9641(3.5086) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 14.7360(14.4069) | Bit/dim 7.0067(7.5178) | Xent 2.3026(2.3026) | Loss 7.0067(7.5178) | Error 0.8844(0.9002) Steps 610(586.26) | Grad Norm 0.4800(2.7915) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.0184(14.5206) | Bit/dim 6.9786(7.3819) | Xent 2.3026(2.3026) | Loss 6.9786(7.3819) | Error 0.9011(0.8994) Steps 622(594.63) | Grad Norm 0.4799(2.2051) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 76.3560, Epoch Time 838.6361(817.9684), Bit/dim 6.9751(best: 7.7815), Xent 2.3026, Loss 6.9751, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 15.2947(14.7193) | Bit/dim 6.9337(7.2710) | Xent 2.3026(2.3026) | Loss 6.9337(7.2710) | Error 0.8833(0.8988) Steps 628(603.11) | Grad Norm 0.4840(1.7681) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 15.0842(14.8904) | Bit/dim 6.8956(7.1772) | Xent 2.3026(2.3026) | Loss 6.8956(7.1772) | Error 0.8956(0.9004) Steps 628(609.64) | Grad Norm 0.5166(1.4300) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 15.9008(15.0925) | Bit/dim 6.8444(7.0968) | Xent 2.3026(2.3026) | Loss 6.8444(7.0968) | Error 0.9144(0.9001) Steps 634(615.75) | Grad Norm 0.5109(1.2051) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.3042(15.2673) | Bit/dim 6.7200(7.0133) | Xent 2.3026(2.3026) | Loss 6.7200(7.0133) | Error 0.9000(0.9003) Steps 640(621.07) | Grad Norm 0.6165(1.0661) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 15.9337(15.4403) | Bit/dim 6.5960(6.9192) | Xent 2.3026(2.3026) | Loss 6.5960(6.9192) | Error 0.8967(0.9003) Steps 640(626.04) | Grad Norm 3.0575(1.1283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 78.7678, Epoch Time 954.0422(822.0506), Bit/dim 6.4629(best: 6.9751), Xent 2.3026, Loss 6.4629, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 15.8876(15.5521) | Bit/dim 6.3726(6.8031) | Xent 2.3026(2.3026) | Loss 6.3726(6.8031) | Error 0.9111(0.9010) Steps 640(629.71) | Grad Norm 8.3405(3.8778) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 16.4072(15.6990) | Bit/dim 6.1942(6.6638) | Xent 2.3026(2.3026) | Loss 6.1942(6.6638) | Error 0.9056(0.9016) Steps 640(632.58) | Grad Norm 33.8917(8.8956) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.0489(15.8934) | Bit/dim 5.9974(6.5064) | Xent 2.3026(2.3026) | Loss 5.9974(6.5064) | Error 0.9089(0.9006) Steps 670(639.22) | Grad Norm 20.7555(11.6579) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 16.4957(16.1818) | Bit/dim 5.8124(6.3452) | Xent 2.3026(2.3026) | Loss 5.8124(6.3452) | Error 0.9133(0.9006) Steps 664(646.61) | Grad Norm 11.7321(12.7936) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 16.1606(16.2878) | Bit/dim 5.7522(6.1934) | Xent 2.3026(2.3026) | Loss 5.7522(6.1934) | Error 0.8889(0.9002) Steps 664(649.91) | Grad Norm 11.7552(12.0959) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 16.5821(16.3991) | Bit/dim 5.6436(6.0564) | Xent 2.3026(2.3026) | Loss 5.6436(6.0564) | Error 0.9011(0.8992) Steps 670(654.63) | Grad Norm 10.6912(10.8852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 82.6146, Epoch Time 1006.4222(827.5818), Bit/dim 5.6547(best: 6.4629), Xent 2.3026, Loss 5.6547, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.2610(16.4910) | Bit/dim 5.6537(5.9468) | Xent 2.3026(2.3026) | Loss 5.6537(5.9468) | Error 0.8922(0.8986) Steps 676(659.96) | Grad Norm 6.5542(9.7726) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.4763(16.6145) | Bit/dim 5.5924(5.8561) | Xent 2.3026(2.3026) | Loss 5.5924(5.8561) | Error 0.9044(0.8982) Steps 670(663.09) | Grad Norm 12.2083(8.2897) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 16.5876(16.6084) | Bit/dim 5.6065(5.7879) | Xent 2.3026(2.3026) | Loss 5.6065(5.7879) | Error 0.9067(0.8990) Steps 664(665.15) | Grad Norm 2.9459(10.7638) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 15.7832(16.4687) | Bit/dim 5.5309(5.7313) | Xent 2.3026(2.3026) | Loss 5.5309(5.7313) | Error 0.9222(0.9004) Steps 658(663.10) | Grad Norm 4.4172(9.2132) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.8105(16.4189) | Bit/dim 5.5408(5.6783) | Xent 2.3026(2.3026) | Loss 5.5408(5.6783) | Error 0.8911(0.9001) Steps 676(662.77) | Grad Norm 4.1601(7.6743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 82.1005, Epoch Time 1007.2349(832.9714), Bit/dim 5.5212(best: 5.6547), Xent 2.3026, Loss 5.5212, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.4394(16.4229) | Bit/dim 5.5110(5.6362) | Xent 2.3026(2.3026) | Loss 5.5110(5.6362) | Error 0.9011(0.9011) Steps 676(665.92) | Grad Norm 3.2407(7.3685) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.0325(16.5679) | Bit/dim 5.4515(5.5977) | Xent 2.3026(2.3026) | Loss 5.4515(5.5977) | Error 0.8933(0.9002) Steps 682(668.63) | Grad Norm 1.4172(6.6588) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 16.7467(16.6800) | Bit/dim 5.4276(5.5610) | Xent 2.3026(2.3026) | Loss 5.4276(5.5610) | Error 0.8811(0.9007) Steps 676(670.41) | Grad Norm 1.3844(6.2126) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 16.5111(16.6769) | Bit/dim 5.4220(5.5297) | Xent 2.3026(2.3026) | Loss 5.4220(5.5297) | Error 0.8989(0.8994) Steps 676(671.43) | Grad Norm 10.1917(6.4951) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 16.6573(16.6737) | Bit/dim 5.3592(5.4942) | Xent 2.3026(2.3026) | Loss 5.3592(5.4942) | Error 0.9078(0.8999) Steps 676(672.48) | Grad Norm 11.8326(7.0167) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 16.5087(16.6414) | Bit/dim 5.2676(5.4531) | Xent 2.3026(2.3026) | Loss 5.2676(5.4531) | Error 0.8922(0.9000) Steps 676(674.33) | Grad Norm 5.5239(8.2007) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 83.3111, Epoch Time 1019.4854(838.5668), Bit/dim 5.2958(best: 5.5212), Xent 2.3026, Loss 5.2958, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 16.4745(16.5954) | Bit/dim 5.2528(5.4065) | Xent 2.3026(2.3026) | Loss 5.2528(5.4065) | Error 0.9111(0.9005) Steps 676(675.50) | Grad Norm 5.6357(8.5235) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 16.5917(16.5891) | Bit/dim 5.2329(5.3609) | Xent 2.3026(2.3026) | Loss 5.2329(5.3609) | Error 0.9100(0.9009) Steps 682(677.37) | Grad Norm 17.2582(9.3637) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 16.8042(16.6115) | Bit/dim 5.2178(5.3174) | Xent 2.3026(2.3026) | Loss 5.2178(5.3174) | Error 0.8922(0.9012) Steps 682(678.59) | Grad Norm 9.4259(9.1617) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 16.3158(16.6292) | Bit/dim 5.1547(5.2786) | Xent 2.3026(2.3026) | Loss 5.1547(5.2786) | Error 0.9133(0.9009) Steps 682(679.91) | Grad Norm 7.9371(9.1917) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 15.7137(16.4377) | Bit/dim 5.1097(5.2363) | Xent 2.3026(2.3026) | Loss 5.1097(5.2363) | Error 0.8967(0.9006) Steps 670(678.98) | Grad Norm 3.0141(9.4278) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 83.3615, Epoch Time 1001.0344(843.4408), Bit/dim 5.0842(best: 5.2958), Xent 2.3026, Loss 5.0842, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 15.4295(16.2638) | Bit/dim 5.0208(5.1919) | Xent 2.3026(2.3026) | Loss 5.0208(5.1919) | Error 0.9144(0.9006) Steps 670(676.07) | Grad Norm 7.2824(9.3601) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 15.3859(16.0740) | Bit/dim 5.0475(5.1507) | Xent 2.3026(2.3026) | Loss 5.0475(5.1507) | Error 0.8889(0.8990) Steps 658(671.98) | Grad Norm 4.2500(8.0415) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 15.6456(16.0052) | Bit/dim 4.9942(5.1072) | Xent 2.3026(2.3026) | Loss 4.9942(5.1072) | Error 0.9011(0.8988) Steps 670(668.70) | Grad Norm 4.1373(7.1773) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 17.0309(16.0888) | Bit/dim 4.9552(5.0647) | Xent 2.3026(2.3026) | Loss 4.9552(5.0647) | Error 0.9011(0.8990) Steps 670(667.87) | Grad Norm 4.0878(7.0778) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 16.0807(16.0735) | Bit/dim 4.9032(5.0248) | Xent 2.3026(2.3026) | Loss 4.9032(5.0248) | Error 0.8878(0.8982) Steps 670(668.43) | Grad Norm 3.8562(6.8610) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 16.0637(16.0402) | Bit/dim 5.1882(5.0491) | Xent 2.3026(2.3026) | Loss 5.1882(5.0491) | Error 0.8956(0.9003) Steps 670(667.84) | Grad Norm 30.7013(11.3729) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 83.8828, Epoch Time 975.4979(847.4025), Bit/dim 4.9812(best: 5.0842), Xent 2.3026, Loss 4.9812, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 16.1226(16.0276) | Bit/dim 4.9963(5.0398) | Xent 2.3026(2.3026) | Loss 4.9963(5.0398) | Error 0.8922(0.8989) Steps 670(668.77) | Grad Norm 5.2207(10.9110) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 16.1593(16.2007) | Bit/dim 4.8618(5.0080) | Xent 2.3026(2.3026) | Loss 4.8618(5.0080) | Error 0.9011(0.9000) Steps 682(671.95) | Grad Norm 9.3990(9.9610) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 15.9336(16.3399) | Bit/dim 4.8183(4.9604) | Xent 2.3026(2.3026) | Loss 4.8183(4.9604) | Error 0.8811(0.9000) Steps 676(675.89) | Grad Norm 4.6151(8.5642) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 17.4997(16.6006) | Bit/dim 4.8008(4.9172) | Xent 2.3026(2.3026) | Loss 4.8008(4.9172) | Error 0.8811(0.8991) Steps 688(679.09) | Grad Norm 5.4933(7.3834) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 17.3600(16.8149) | Bit/dim 4.7775(4.8802) | Xent 2.3026(2.3026) | Loss 4.7775(4.8802) | Error 0.9056(0.8997) Steps 694(682.77) | Grad Norm 3.3408(6.7399) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 86.0259, Epoch Time 1030.8866(852.9070), Bit/dim 4.7349(best: 4.9812), Xent 2.3026, Loss 4.7349, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 17.8265(17.0267) | Bit/dim 4.7188(4.8454) | Xent 2.3026(2.3026) | Loss 4.7188(4.8454) | Error 0.9011(0.9011) Steps 700(687.77) | Grad Norm 3.4781(6.3891) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 17.8331(17.1948) | Bit/dim 4.6947(4.8099) | Xent 2.3026(2.3026) | Loss 4.6947(4.8099) | Error 0.8978(0.9013) Steps 706(692.45) | Grad Norm 1.1929(6.2415) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 17.9137(17.3514) | Bit/dim 4.7463(4.7820) | Xent 2.3026(2.3026) | Loss 4.7463(4.7820) | Error 0.9011(0.9010) Steps 724(697.67) | Grad Norm 23.4144(7.7476) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 17.5238(17.4786) | Bit/dim 4.8859(4.7889) | Xent 2.3026(2.3026) | Loss 4.8859(4.7889) | Error 0.9100(0.9003) Steps 700(700.56) | Grad Norm 19.0101(11.0270) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 17.4906(17.4798) | Bit/dim 4.7668(4.7876) | Xent 2.3026(2.3026) | Loss 4.7668(4.7876) | Error 0.8933(0.9000) Steps 730(703.52) | Grad Norm 6.8441(11.1981) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.2093(17.5679) | Bit/dim 4.6458(4.7636) | Xent 2.3026(2.3026) | Loss 4.6458(4.7636) | Error 0.8833(0.8994) Steps 712(706.45) | Grad Norm 4.5029(10.3890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 90.4095, Epoch Time 1080.1673(859.7248), Bit/dim 4.6945(best: 4.7349), Xent 2.3026, Loss 4.6945, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.0710(17.6596) | Bit/dim 4.6425(4.7362) | Xent 2.3026(2.3026) | Loss 4.6425(4.7362) | Error 0.8967(0.9005) Steps 700(707.91) | Grad Norm 3.4373(8.9960) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 18.3398(17.7776) | Bit/dim 4.6102(4.7046) | Xent 2.3026(2.3026) | Loss 4.6102(4.7046) | Error 0.9000(0.9000) Steps 712(710.10) | Grad Norm 3.8673(7.5020) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 17.9534(17.8374) | Bit/dim 4.5801(4.6778) | Xent 2.3026(2.3026) | Loss 4.5801(4.6778) | Error 0.8989(0.9017) Steps 718(711.06) | Grad Norm 4.9840(6.6091) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 17.6165(17.8439) | Bit/dim 4.5884(4.6565) | Xent 2.3026(2.3026) | Loss 4.5884(4.6565) | Error 0.9022(0.9012) Steps 718(712.74) | Grad Norm 5.4564(6.5783) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 17.7157(17.8224) | Bit/dim 4.5937(4.6415) | Xent 2.3026(2.3026) | Loss 4.5937(4.6415) | Error 0.8889(0.8988) Steps 712(713.90) | Grad Norm 10.0242(7.7595) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 88.6409, Epoch Time 1089.0756(866.6054), Bit/dim 4.8434(best: 4.6945), Xent 2.3026, Loss 4.8434, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 17.8250(17.7303) | Bit/dim 4.6552(4.6829) | Xent 2.3026(2.3026) | Loss 4.6552(4.6829) | Error 0.8756(0.8980) Steps 718(712.55) | Grad Norm 3.0373(10.7654) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 17.5224(17.7441) | Bit/dim 4.6151(4.6743) | Xent 2.3026(2.3026) | Loss 4.6151(4.6743) | Error 0.9156(0.8995) Steps 712(711.75) | Grad Norm 3.5959(9.1210) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 16.9136(17.6440) | Bit/dim 4.5643(4.6502) | Xent 2.3026(2.3026) | Loss 4.5643(4.6502) | Error 0.9089(0.8993) Steps 694(707.70) | Grad Norm 2.0936(7.3517) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 16.9601(17.4690) | Bit/dim 4.5143(4.6219) | Xent 2.3026(2.3026) | Loss 4.5143(4.6219) | Error 0.9122(0.9008) Steps 682(702.45) | Grad Norm 0.9612(5.9844) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 17.3293(17.4465) | Bit/dim 4.5005(4.5927) | Xent 2.3026(2.3026) | Loss 4.5005(4.5927) | Error 0.9000(0.9001) Steps 706(699.83) | Grad Norm 0.6544(4.8218) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 17.9458(17.4482) | Bit/dim 4.5044(4.5679) | Xent 2.3026(2.3026) | Loss 4.5044(4.5679) | Error 0.8956(0.8996) Steps 700(700.04) | Grad Norm 3.7914(4.3743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 86.8623, Epoch Time 1059.1047(872.3803), Bit/dim 4.5040(best: 4.6945), Xent 2.3026, Loss 4.5040, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 17.7957(17.5206) | Bit/dim 4.4845(4.5479) | Xent 2.3026(2.3026) | Loss 4.4845(4.5479) | Error 0.9000(0.9002) Steps 718(703.42) | Grad Norm 3.9698(4.1616) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 17.9309(17.5307) | Bit/dim 4.5035(4.5262) | Xent 2.3026(2.3026) | Loss 4.5035(4.5262) | Error 0.9233(0.9001) Steps 706(704.40) | Grad Norm 4.1774(4.1357) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 18.0660(17.5758) | Bit/dim 4.9914(4.5470) | Xent 2.3026(2.3026) | Loss 4.9914(4.5470) | Error 0.9056(0.9004) Steps 730(707.17) | Grad Norm 18.2037(6.7452) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 17.2946(17.5667) | Bit/dim 4.5800(4.5812) | Xent 2.3026(2.3026) | Loss 4.5800(4.5812) | Error 0.9000(0.9008) Steps 706(707.81) | Grad Norm 3.3868(7.1985) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 17.3028(17.5496) | Bit/dim 4.5226(4.5739) | Xent 2.3026(2.3026) | Loss 4.5226(4.5739) | Error 0.9089(0.9001) Steps 694(706.60) | Grad Norm 1.4682(6.1892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 85.8495, Epoch Time 1066.5493(878.2054), Bit/dim 4.4791(best: 4.5040), Xent 2.3026, Loss 4.4791, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.7838(17.4058) | Bit/dim 4.4435(4.5496) | Xent 2.3026(2.3026) | Loss 4.4435(4.5496) | Error 0.8978(0.8996) Steps 670(699.64) | Grad Norm 2.1553(5.1124) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 17.3717(17.3001) | Bit/dim 4.4439(4.5221) | Xent 2.3026(2.3026) | Loss 4.4439(4.5221) | Error 0.9000(0.9008) Steps 688(695.07) | Grad Norm 1.2051(4.1156) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 17.3883(17.2873) | Bit/dim 4.3970(4.4941) | Xent 2.3026(2.3026) | Loss 4.3970(4.4941) | Error 0.9044(0.9004) Steps 670(691.76) | Grad Norm 1.8721(3.3153) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 17.5021(17.3357) | Bit/dim 4.3795(4.4673) | Xent 2.3026(2.3026) | Loss 4.3795(4.4673) | Error 0.8967(0.9005) Steps 670(688.59) | Grad Norm 1.5471(2.8484) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 17.4231(17.3342) | Bit/dim 4.3603(4.4466) | Xent 2.3026(2.3026) | Loss 4.3603(4.4466) | Error 0.8889(0.8985) Steps 670(684.05) | Grad Norm 1.8215(2.4787) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 17.4675(17.3661) | Bit/dim 4.3667(4.4253) | Xent 2.3026(2.3026) | Loss 4.3667(4.4253) | Error 0.9056(0.9002) Steps 676(680.98) | Grad Norm 2.7960(2.4221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 81.8453, Epoch Time 1048.2110(883.3056), Bit/dim 4.3576(best: 4.4791), Xent 2.3026, Loss 4.3576, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 17.1236(17.2737) | Bit/dim 4.3440(4.4065) | Xent 2.3026(2.3026) | Loss 4.3440(4.4065) | Error 0.9167(0.9020) Steps 670(679.24) | Grad Norm 2.2691(2.4470) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 17.4552(17.2934) | Bit/dim 4.3230(4.3863) | Xent 2.3026(2.3026) | Loss 4.3230(4.3863) | Error 0.8811(0.8997) Steps 676(680.74) | Grad Norm 0.6738(2.3218) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 17.9093(17.2861) | Bit/dim 4.3096(4.3762) | Xent 2.3026(2.3026) | Loss 4.3096(4.3762) | Error 0.9100(0.9001) Steps 694(680.91) | Grad Norm 6.3174(3.5999) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 17.3110(17.3000) | Bit/dim 4.4610(4.3727) | Xent 2.3026(2.3026) | Loss 4.4610(4.3727) | Error 0.9078(0.9007) Steps 700(685.01) | Grad Norm 15.2532(4.6818) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 16.2690(17.2578) | Bit/dim 4.3554(4.3707) | Xent 2.3026(2.3026) | Loss 4.3554(4.3707) | Error 0.9067(0.9007) Steps 676(685.73) | Grad Norm 6.1628(5.5860) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 83.0899, Epoch Time 1046.6284(888.2053), Bit/dim 4.3048(best: 4.3576), Xent 2.3026, Loss 4.3048, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 17.3912(17.3053) | Bit/dim 4.3349(4.3514) | Xent 2.3026(2.3026) | Loss 4.3349(4.3514) | Error 0.9033(0.9001) Steps 688(685.83) | Grad Norm 2.4663(4.9773) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 17.2709(17.3072) | Bit/dim 4.2731(4.3325) | Xent 2.3026(2.3026) | Loss 4.2731(4.3325) | Error 0.8967(0.9004) Steps 682(685.17) | Grad Norm 2.7062(4.3220) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 16.8293(17.2973) | Bit/dim 4.2450(4.3134) | Xent 2.3026(2.3026) | Loss 4.2450(4.3134) | Error 0.8878(0.8992) Steps 694(685.44) | Grad Norm 3.8621(3.8509) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 17.1487(17.3032) | Bit/dim 4.2641(4.3069) | Xent 2.3026(2.3026) | Loss 4.2641(4.3069) | Error 0.9189(0.8998) Steps 700(686.56) | Grad Norm 5.9539(4.9829) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 16.9067(17.3151) | Bit/dim 4.2374(4.2914) | Xent 2.3026(2.3026) | Loss 4.2374(4.2914) | Error 0.9011(0.8984) Steps 688(689.07) | Grad Norm 5.5731(5.0621) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 17.4478(17.3568) | Bit/dim 4.2212(4.2752) | Xent 2.3026(2.3026) | Loss 4.2212(4.2752) | Error 0.9067(0.9001) Steps 682(688.02) | Grad Norm 2.8989(4.7018) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 83.1250, Epoch Time 1054.0056(893.1793), Bit/dim 4.2098(best: 4.3048), Xent 2.3026, Loss 4.2098, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 18.8367(17.4635) | Bit/dim 4.3551(4.2629) | Xent 2.3026(2.3026) | Loss 4.3551(4.2629) | Error 0.9011(0.9002) Steps 712(689.83) | Grad Norm 17.5266(4.8391) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 18.2357(17.5811) | Bit/dim 4.3890(4.3115) | Xent 2.3026(2.3026) | Loss 4.3890(4.3115) | Error 0.8844(0.8985) Steps 760(698.65) | Grad Norm 5.6744(6.6679) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 17.2704(17.6088) | Bit/dim 4.2932(4.3081) | Xent 2.3026(2.3026) | Loss 4.2932(4.3081) | Error 0.9022(0.8992) Steps 706(704.81) | Grad Norm 3.1314(6.0064) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 17.1653(17.5833) | Bit/dim 4.2069(4.2859) | Xent 2.3026(2.3026) | Loss 4.2069(4.2859) | Error 0.8811(0.9001) Steps 694(704.16) | Grad Norm 1.9795(5.1132) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 17.8514(17.6497) | Bit/dim 4.1452(4.2584) | Xent 2.3026(2.3026) | Loss 4.1452(4.2584) | Error 0.8944(0.9002) Steps 706(704.00) | Grad Norm 2.5589(4.2783) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 85.5572, Epoch Time 1077.5433(898.7102), Bit/dim 4.1580(best: 4.2098), Xent 2.3026, Loss 4.1580, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 17.5377(17.7262) | Bit/dim 4.1565(4.2346) | Xent 2.3026(2.3026) | Loss 4.1565(4.2346) | Error 0.8856(0.8998) Steps 706(704.10) | Grad Norm 2.5305(3.8096) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 17.8118(17.8075) | Bit/dim 4.1448(4.2105) | Xent 2.3026(2.3026) | Loss 4.1448(4.2105) | Error 0.9200(0.9010) Steps 712(706.46) | Grad Norm 1.9807(3.4280) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 18.1760(17.8646) | Bit/dim 4.1508(4.1952) | Xent 2.3026(2.3026) | Loss 4.1508(4.1952) | Error 0.9033(0.9008) Steps 724(709.13) | Grad Norm 5.5658(3.9575) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 18.0566(18.0135) | Bit/dim 4.1045(4.1841) | Xent 2.3026(2.3026) | Loss 4.1045(4.1841) | Error 0.8967(0.9002) Steps 736(715.65) | Grad Norm 3.6503(4.5434) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 18.0550(18.0837) | Bit/dim 4.0871(4.1674) | Xent 2.3026(2.3026) | Loss 4.0871(4.1674) | Error 0.8978(0.9001) Steps 748(720.22) | Grad Norm 1.5919(4.5257) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 17.9604(18.1615) | Bit/dim 4.0637(4.1479) | Xent 2.3026(2.3026) | Loss 4.0637(4.1479) | Error 0.9000(0.9002) Steps 718(724.32) | Grad Norm 2.8273(3.9898) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 90.9700, Epoch Time 1108.2755(904.9972), Bit/dim 4.0898(best: 4.1580), Xent 2.3026, Loss 4.0898, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 18.4627(18.2114) | Bit/dim 4.0607(4.1299) | Xent 2.3026(2.3026) | Loss 4.0607(4.1299) | Error 0.8900(0.8991) Steps 766(730.86) | Grad Norm 2.5054(3.4842) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 18.9274(18.3732) | Bit/dim 4.0451(4.1203) | Xent 2.3026(2.3026) | Loss 4.0451(4.1203) | Error 0.9144(0.8989) Steps 766(738.34) | Grad Norm 3.1700(4.1649) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 18.2121(18.4781) | Bit/dim 4.0621(4.1060) | Xent 2.3026(2.3026) | Loss 4.0621(4.1060) | Error 0.9100(0.8997) Steps 754(745.88) | Grad Norm 5.8635(4.2721) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 18.6851(18.5280) | Bit/dim 4.0612(4.0961) | Xent 2.3026(2.3026) | Loss 4.0612(4.0961) | Error 0.8944(0.9009) Steps 760(749.94) | Grad Norm 4.5023(4.3964) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 18.7571(18.6525) | Bit/dim 4.0068(4.0845) | Xent 2.3026(2.3026) | Loss 4.0068(4.0845) | Error 0.9067(0.9006) Steps 778(754.84) | Grad Norm 4.8016(4.4255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 92.7979, Epoch Time 1138.6311(912.0062), Bit/dim 4.0417(best: 4.0898), Xent 2.3026, Loss 4.0417, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 18.9382(18.6895) | Bit/dim 4.0629(4.0720) | Xent 2.3026(2.3026) | Loss 4.0629(4.0720) | Error 0.8933(0.9005) Steps 766(759.43) | Grad Norm 2.8331(4.4905) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 18.5940(18.6603) | Bit/dim 4.0147(4.0606) | Xent 2.3026(2.3026) | Loss 4.0147(4.0606) | Error 0.8844(0.8983) Steps 784(762.90) | Grad Norm 2.0153(4.6208) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 18.4796(18.7148) | Bit/dim 3.9964(4.0481) | Xent 2.3026(2.3026) | Loss 3.9964(4.0481) | Error 0.8911(0.8981) Steps 778(765.66) | Grad Norm 6.2013(4.6733) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 18.9046(18.7378) | Bit/dim 4.0032(4.0358) | Xent 2.3026(2.3026) | Loss 4.0032(4.0358) | Error 0.8944(0.8987) Steps 778(767.53) | Grad Norm 1.6966(4.1369) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 18.6442(18.7488) | Bit/dim 4.0271(4.0343) | Xent 2.3026(2.3026) | Loss 4.0271(4.0343) | Error 0.9122(0.9006) Steps 766(769.40) | Grad Norm 8.8380(4.7521) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 18.2918(18.6997) | Bit/dim 4.0031(4.0310) | Xent 2.3026(2.3026) | Loss 4.0031(4.0310) | Error 0.9056(0.9016) Steps 748(768.00) | Grad Norm 1.5850(4.5853) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 90.6768, Epoch Time 1136.9556(918.7547), Bit/dim 3.9934(best: 4.0417), Xent 2.3026, Loss 3.9934, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 18.3436(18.6543) | Bit/dim 3.9930(4.0204) | Xent 2.3026(2.3026) | Loss 3.9930(4.0204) | Error 0.9144(0.9030) Steps 766(766.37) | Grad Norm 1.5612(3.8380) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 18.9033(18.7049) | Bit/dim 4.0127(4.0096) | Xent 2.3026(2.3026) | Loss 4.0127(4.0096) | Error 0.8944(0.9029) Steps 778(766.26) | Grad Norm 8.1590(3.8067) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 18.6225(18.7091) | Bit/dim 3.9857(4.0037) | Xent 2.3026(2.3026) | Loss 3.9857(4.0037) | Error 0.9144(0.9011) Steps 766(765.63) | Grad Norm 6.2222(4.2901) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 19.1028(18.7903) | Bit/dim 3.9373(3.9954) | Xent 2.3026(2.3026) | Loss 3.9373(3.9954) | Error 0.8933(0.9006) Steps 778(767.18) | Grad Norm 1.7284(4.2855) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 19.4732(18.8727) | Bit/dim 3.9395(3.9847) | Xent 2.3026(2.3026) | Loss 3.9395(3.9847) | Error 0.8878(0.8992) Steps 766(769.67) | Grad Norm 3.8402(4.1895) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 93.6408, Epoch Time 1147.4474(925.6154), Bit/dim 3.9473(best: 3.9934), Xent 2.3026, Loss 3.9473, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 18.8453(18.9727) | Bit/dim 3.9312(3.9737) | Xent 2.3026(2.3026) | Loss 3.9312(3.9737) | Error 0.9167(0.8992) Steps 778(771.33) | Grad Norm 2.5559(3.8348) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 19.5297(19.0360) | Bit/dim 3.9421(3.9632) | Xent 2.3026(2.3026) | Loss 3.9421(3.9632) | Error 0.9000(0.8993) Steps 784(773.92) | Grad Norm 8.6195(3.8394) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 19.4487(19.0568) | Bit/dim 3.8953(3.9551) | Xent 2.3026(2.3026) | Loss 3.8953(3.9551) | Error 0.9011(0.9002) Steps 796(776.16) | Grad Norm 4.8890(4.1014) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 18.2839(18.9697) | Bit/dim 3.9379(3.9514) | Xent 2.3026(2.3026) | Loss 3.9379(3.9514) | Error 0.8844(0.9001) Steps 766(775.38) | Grad Norm 4.2496(4.1384) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 18.6902(18.9078) | Bit/dim 3.9296(3.9409) | Xent 2.3026(2.3026) | Loss 3.9296(3.9409) | Error 0.8889(0.8999) Steps 754(773.66) | Grad Norm 3.4269(3.7584) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 18.7633(18.7906) | Bit/dim 3.9232(3.9353) | Xent 2.3026(2.3026) | Loss 3.9232(3.9353) | Error 0.9144(0.8996) Steps 766(771.48) | Grad Norm 1.8853(3.1774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 90.7102, Epoch Time 1146.1346(932.2310), Bit/dim 3.9090(best: 3.9473), Xent 2.3026, Loss 3.9090, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 18.1796(18.6539) | Bit/dim 3.9149(3.9353) | Xent 2.3026(2.3026) | Loss 3.9149(3.9353) | Error 0.9033(0.9001) Steps 766(767.55) | Grad Norm 6.5509(4.0973) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 18.5465(18.6448) | Bit/dim 3.9297(3.9266) | Xent 2.3026(2.3026) | Loss 3.9297(3.9266) | Error 0.9033(0.9010) Steps 748(764.87) | Grad Norm 4.9446(4.3733) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 18.0825(18.5796) | Bit/dim 3.8873(3.9203) | Xent 2.3026(2.3026) | Loss 3.8873(3.9203) | Error 0.8878(0.8999) Steps 766(763.30) | Grad Norm 3.0990(4.0901) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 18.1619(18.4919) | Bit/dim 3.8825(3.9132) | Xent 2.3026(2.3026) | Loss 3.8825(3.9132) | Error 0.8900(0.8992) Steps 760(761.93) | Grad Norm 5.3979(4.2543) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 19.0119(18.4720) | Bit/dim 3.8951(3.9054) | Xent 2.3026(2.3026) | Loss 3.8951(3.9054) | Error 0.8933(0.9002) Steps 760(760.99) | Grad Norm 1.6921(3.7925) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 90.5091, Epoch Time 1116.9796(937.7735), Bit/dim 3.8843(best: 3.9090), Xent 2.3026, Loss 3.8843, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 18.5985(18.3850) | Bit/dim 3.8855(3.8996) | Xent 2.3026(2.3026) | Loss 3.8855(3.8996) | Error 0.9000(0.9012) Steps 742(758.56) | Grad Norm 2.7599(3.8629) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 18.3856(18.3946) | Bit/dim 3.8515(3.8938) | Xent 2.3026(2.3026) | Loss 3.8515(3.8938) | Error 0.9033(0.9013) Steps 760(757.38) | Grad Norm 2.7955(3.9792) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 18.1011(18.4070) | Bit/dim 3.8878(3.8882) | Xent 2.3026(2.3026) | Loss 3.8878(3.8882) | Error 0.9044(0.9000) Steps 754(757.30) | Grad Norm 7.2878(3.8133) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 18.3969(18.3839) | Bit/dim 3.8369(3.8835) | Xent 2.3026(2.3026) | Loss 3.8369(3.8835) | Error 0.8978(0.8997) Steps 760(756.54) | Grad Norm 3.9038(4.0421) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 19.0942(18.3830) | Bit/dim 3.8561(3.8785) | Xent 2.3026(2.3026) | Loss 3.8561(3.8785) | Error 0.8989(0.8994) Steps 760(755.60) | Grad Norm 5.0876(4.3361) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 18.2919(18.4071) | Bit/dim 3.8580(3.8714) | Xent 2.3026(2.3026) | Loss 3.8580(3.8714) | Error 0.9144(0.8998) Steps 754(754.98) | Grad Norm 1.3516(3.7152) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 87.8245, Epoch Time 1114.9678(943.0893), Bit/dim 3.8512(best: 3.8843), Xent 2.3026, Loss 3.8512, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 18.1781(18.3896) | Bit/dim 3.8456(3.8665) | Xent 2.3026(2.3026) | Loss 3.8456(3.8665) | Error 0.9022(0.9007) Steps 754(754.88) | Grad Norm 6.0582(4.0395) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 17.9425(18.3912) | Bit/dim 3.8718(3.8629) | Xent 2.3026(2.3026) | Loss 3.8718(3.8629) | Error 0.9111(0.9000) Steps 742(755.39) | Grad Norm 4.4212(4.0658) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 17.9451(18.4177) | Bit/dim 3.8679(3.8590) | Xent 2.3026(2.3026) | Loss 3.8679(3.8590) | Error 0.9022(0.8992) Steps 736(754.02) | Grad Norm 3.8872(4.2940) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 18.4199(18.4055) | Bit/dim 3.8970(3.8587) | Xent 2.3026(2.3026) | Loss 3.8970(3.8587) | Error 0.9100(0.8993) Steps 754(753.53) | Grad Norm 6.4810(4.7064) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 18.7763(18.4178) | Bit/dim 3.8258(3.8558) | Xent 2.3026(2.3026) | Loss 3.8258(3.8558) | Error 0.8956(0.8997) Steps 754(752.83) | Grad Norm 3.0707(4.5295) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 87.5025, Epoch Time 1116.9209(948.3043), Bit/dim 3.8341(best: 3.8512), Xent 2.3026, Loss 3.8341, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 18.4941(18.4245) | Bit/dim 3.8276(3.8493) | Xent 2.3026(2.3026) | Loss 3.8276(3.8493) | Error 0.9100(0.9007) Steps 760(752.94) | Grad Norm 1.4762(4.1870) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 18.6121(18.4820) | Bit/dim 3.8604(3.8472) | Xent 2.3026(2.3026) | Loss 3.8604(3.8472) | Error 0.8956(0.9006) Steps 748(753.21) | Grad Norm 3.4204(4.0885) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 18.7464(18.4770) | Bit/dim 3.8264(3.8396) | Xent 2.3026(2.3026) | Loss 3.8264(3.8396) | Error 0.8978(0.8993) Steps 748(753.12) | Grad Norm 3.6717(4.0267) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 18.8104(18.5451) | Bit/dim 3.8110(3.8363) | Xent 2.3026(2.3026) | Loss 3.8110(3.8363) | Error 0.9044(0.8998) Steps 766(752.97) | Grad Norm 7.4079(4.5338) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 18.4426(18.5409) | Bit/dim 3.8223(3.8318) | Xent 2.3026(2.3026) | Loss 3.8223(3.8318) | Error 0.9056(0.8997) Steps 754(753.72) | Grad Norm 1.8487(4.1256) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 17.8854(18.5109) | Bit/dim 3.7992(3.8260) | Xent 2.3026(2.3026) | Loss 3.7992(3.8260) | Error 0.8889(0.8999) Steps 742(753.31) | Grad Norm 3.2089(3.6364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 89.0546, Epoch Time 1124.6013(953.5932), Bit/dim 3.8153(best: 3.8341), Xent 2.3026, Loss 3.8153, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 17.9229(18.5459) | Bit/dim 3.7975(3.8239) | Xent 2.3026(2.3026) | Loss 3.7975(3.8239) | Error 0.8967(0.9014) Steps 754(753.63) | Grad Norm 3.5751(4.2064) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 18.7290(18.5151) | Bit/dim 3.8018(3.8219) | Xent 2.3026(2.3026) | Loss 3.8018(3.8219) | Error 0.9011(0.9011) Steps 760(753.31) | Grad Norm 4.0967(4.2233) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 18.9772(18.5744) | Bit/dim 3.8028(3.8173) | Xent 2.3026(2.3026) | Loss 3.8028(3.8173) | Error 0.8933(0.9000) Steps 760(753.35) | Grad Norm 5.1584(4.2680) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 18.6889(18.5404) | Bit/dim 3.7850(3.8134) | Xent 2.3026(2.3026) | Loss 3.7850(3.8134) | Error 0.9022(0.8987) Steps 754(752.91) | Grad Norm 5.0719(4.3506) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 18.2769(18.5584) | Bit/dim 3.8329(3.8102) | Xent 2.3026(2.3026) | Loss 3.8329(3.8102) | Error 0.9044(0.9003) Steps 748(754.09) | Grad Norm 4.1965(4.4420) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 88.8184, Epoch Time 1125.0902(958.7381), Bit/dim 3.7999(best: 3.8153), Xent 2.3026, Loss 3.7999, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 18.1240(18.5279) | Bit/dim 3.8072(3.8085) | Xent 2.3026(2.3026) | Loss 3.8072(3.8085) | Error 0.9233(0.8995) Steps 754(754.01) | Grad Norm 6.6369(4.6599) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 17.8892(18.5086) | Bit/dim 3.7961(3.8085) | Xent 2.3026(2.3026) | Loss 3.7961(3.8085) | Error 0.9044(0.8997) Steps 760(753.17) | Grad Norm 4.3384(4.5053) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 18.9493(18.5169) | Bit/dim 3.8153(3.8034) | Xent 2.3026(2.3026) | Loss 3.8153(3.8034) | Error 0.8989(0.9007) Steps 754(753.84) | Grad Norm 1.7025(4.4432) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 18.3583(18.5772) | Bit/dim 3.7776(3.7971) | Xent 2.3026(2.3026) | Loss 3.7776(3.7971) | Error 0.8911(0.9003) Steps 760(755.21) | Grad Norm 5.2697(4.3735) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 18.6314(18.6562) | Bit/dim 3.8007(3.7948) | Xent 2.3026(2.3026) | Loss 3.8007(3.7948) | Error 0.9122(0.9001) Steps 754(757.39) | Grad Norm 2.8557(4.2221) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 18.9466(18.7279) | Bit/dim 3.7876(3.7932) | Xent 2.3026(2.3026) | Loss 3.7876(3.7932) | Error 0.9000(0.8998) Steps 748(757.57) | Grad Norm 4.6583(4.4413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 87.8461, Epoch Time 1132.1611(963.9408), Bit/dim 3.7819(best: 3.7999), Xent 2.3026, Loss 3.7819, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 18.5834(18.6653) | Bit/dim 3.8071(3.7886) | Xent 2.3026(2.3026) | Loss 3.8071(3.7886) | Error 0.8978(0.8996) Steps 760(757.74) | Grad Norm 2.7697(4.4300) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 19.6649(18.8164) | Bit/dim 3.7954(3.7881) | Xent 2.3026(2.3026) | Loss 3.7954(3.7881) | Error 0.9000(0.9002) Steps 760(757.88) | Grad Norm 5.0307(4.5292) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 18.0615(18.7624) | Bit/dim 3.7815(3.7858) | Xent 2.3026(2.3026) | Loss 3.7815(3.7858) | Error 0.9133(0.8999) Steps 748(757.25) | Grad Norm 5.0659(4.6291) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 18.8562(18.8322) | Bit/dim 3.7650(3.7847) | Xent 2.3026(2.3026) | Loss 3.7650(3.7847) | Error 0.8956(0.9004) Steps 754(757.46) | Grad Norm 1.3989(4.2862) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 18.8286(18.8304) | Bit/dim 3.7807(3.7814) | Xent 2.3026(2.3026) | Loss 3.7807(3.7814) | Error 0.8911(0.8999) Steps 772(757.77) | Grad Norm 5.7766(4.4126) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 89.9941, Epoch Time 1144.7256(969.3643), Bit/dim 3.7743(best: 3.7819), Xent 2.3026, Loss 3.7743, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 19.3688(18.9185) | Bit/dim 3.7620(3.7755) | Xent 2.3026(2.3026) | Loss 3.7620(3.7755) | Error 0.8967(0.8996) Steps 748(758.18) | Grad Norm 2.7688(4.2458) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 19.2685(18.8764) | Bit/dim 3.7637(3.7751) | Xent 2.3026(2.3026) | Loss 3.7637(3.7751) | Error 0.9067(0.9005) Steps 748(757.96) | Grad Norm 4.1436(4.0426) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 18.5663(18.8807) | Bit/dim 3.7776(3.7740) | Xent 2.3026(2.3026) | Loss 3.7776(3.7740) | Error 0.9067(0.9010) Steps 748(758.95) | Grad Norm 6.0587(4.2015) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 18.2755(18.8488) | Bit/dim 3.7520(3.7701) | Xent 2.3026(2.3026) | Loss 3.7520(3.7701) | Error 0.8956(0.8996) Steps 748(758.68) | Grad Norm 4.9177(4.4463) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 19.2341(18.8652) | Bit/dim 3.7666(3.7653) | Xent 2.3026(2.3026) | Loss 3.7666(3.7653) | Error 0.9089(0.8994) Steps 760(759.21) | Grad Norm 8.6137(4.6629) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 18.6551(18.8443) | Bit/dim 3.7900(3.7647) | Xent 2.3026(2.3026) | Loss 3.7900(3.7647) | Error 0.9122(0.8997) Steps 742(759.21) | Grad Norm 5.1059(4.5656) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 87.9230, Epoch Time 1140.6533(974.5030), Bit/dim 3.7583(best: 3.7743), Xent 2.3026, Loss 3.7583, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 18.7507(18.8166) | Bit/dim 3.7463(3.7606) | Xent 2.3026(2.3026) | Loss 3.7463(3.7606) | Error 0.8944(0.8994) Steps 754(758.52) | Grad Norm 2.5927(4.2346) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 18.6868(18.7546) | Bit/dim 3.7851(3.7589) | Xent 2.3026(2.3026) | Loss 3.7851(3.7589) | Error 0.8933(0.8996) Steps 754(756.94) | Grad Norm 2.3278(3.8344) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 19.0773(18.8100) | Bit/dim 3.7446(3.7573) | Xent 2.3026(2.3026) | Loss 3.7446(3.7573) | Error 0.8989(0.8997) Steps 760(759.42) | Grad Norm 4.0187(4.3266) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 18.9005(18.7955) | Bit/dim 3.7525(3.7564) | Xent 2.3026(2.3026) | Loss 3.7525(3.7564) | Error 0.9022(0.9015) Steps 760(759.40) | Grad Norm 3.6726(4.0239) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 18.5865(18.8893) | Bit/dim 3.7520(3.7544) | Xent 2.3026(2.3026) | Loss 3.7520(3.7544) | Error 0.9067(0.9009) Steps 760(758.07) | Grad Norm 2.5754(4.2630) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 85.3930, Epoch Time 1140.0716(979.4700), Bit/dim 3.7425(best: 3.7583), Xent 2.3026, Loss 3.7425, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 18.5975(18.9433) | Bit/dim 3.7166(3.7499) | Xent 2.3026(2.3026) | Loss 3.7166(3.7499) | Error 0.8822(0.8983) Steps 748(760.24) | Grad Norm 1.7289(3.8468) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 18.9756(19.0010) | Bit/dim 3.7408(3.7467) | Xent 2.3026(2.3026) | Loss 3.7408(3.7467) | Error 0.9078(0.8996) Steps 742(759.85) | Grad Norm 5.3553(3.7853) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 19.6543(19.0572) | Bit/dim 3.7502(3.7442) | Xent 2.3026(2.3026) | Loss 3.7502(3.7442) | Error 0.9033(0.9000) Steps 772(761.17) | Grad Norm 3.9038(3.9705) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 19.7609(19.0837) | Bit/dim 3.7323(3.7430) | Xent 2.3026(2.3026) | Loss 3.7323(3.7430) | Error 0.8867(0.9003) Steps 784(762.46) | Grad Norm 2.2767(4.1795) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 19.5875(19.1503) | Bit/dim 3.7314(3.7420) | Xent 2.3026(2.3026) | Loss 3.7314(3.7420) | Error 0.9200(0.9000) Steps 754(762.86) | Grad Norm 2.8227(4.1386) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 18.8257(19.1281) | Bit/dim 3.7529(3.7398) | Xent 2.3026(2.3026) | Loss 3.7529(3.7398) | Error 0.9056(0.9000) Steps 766(763.72) | Grad Norm 2.2838(3.9492) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 86.6801, Epoch Time 1156.3493(984.7764), Bit/dim 3.7260(best: 3.7425), Xent 2.3026, Loss 3.7260, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 18.7106(19.0739) | Bit/dim 3.7359(3.7383) | Xent 2.3026(2.3026) | Loss 3.7359(3.7383) | Error 0.8978(0.9006) Steps 772(764.23) | Grad Norm 5.7788(3.8332) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 18.9795(19.0412) | Bit/dim 3.7170(3.7376) | Xent 2.3026(2.3026) | Loss 3.7170(3.7376) | Error 0.9000(0.9007) Steps 754(763.29) | Grad Norm 6.1458(4.4019) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 18.9523(19.1095) | Bit/dim 3.7346(3.7361) | Xent 2.3026(2.3026) | Loss 3.7346(3.7361) | Error 0.8989(0.9014) Steps 772(765.46) | Grad Norm 4.9869(4.1950) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 19.1591(19.0828) | Bit/dim 3.7363(3.7332) | Xent 2.3026(2.3026) | Loss 3.7363(3.7332) | Error 0.9000(0.9002) Steps 778(766.31) | Grad Norm 1.9854(3.9491) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 19.9416(19.0439) | Bit/dim 3.7092(3.7283) | Xent 2.3026(2.3026) | Loss 3.7092(3.7283) | Error 0.8978(0.8995) Steps 784(767.88) | Grad Norm 4.8205(4.0387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 87.6922, Epoch Time 1150.3626(989.7440), Bit/dim 3.7167(best: 3.7260), Xent 2.3026, Loss 3.7167, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 19.9977(19.0982) | Bit/dim 3.7204(3.7253) | Xent 2.3026(2.3026) | Loss 3.7204(3.7253) | Error 0.9078(0.8993) Steps 760(768.65) | Grad Norm 3.6004(3.8174) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 19.8098(19.0974) | Bit/dim 3.7227(3.7265) | Xent 2.3026(2.3026) | Loss 3.7227(3.7265) | Error 0.8967(0.9002) Steps 784(769.29) | Grad Norm 5.2248(3.9009) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 18.8570(19.1236) | Bit/dim 3.7468(3.7257) | Xent 2.3026(2.3026) | Loss 3.7468(3.7257) | Error 0.8989(0.9011) Steps 778(770.66) | Grad Norm 3.2113(4.0380) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 19.1087(19.1890) | Bit/dim 3.7175(3.7228) | Xent 2.3026(2.3026) | Loss 3.7175(3.7228) | Error 0.8944(0.8996) Steps 784(771.51) | Grad Norm 2.5764(4.1481) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 19.2563(19.1630) | Bit/dim 3.7530(3.7238) | Xent 2.3026(2.3026) | Loss 3.7530(3.7238) | Error 0.8833(0.9002) Steps 772(771.79) | Grad Norm 3.5600(4.0291) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 19.5365(19.1647) | Bit/dim 3.7067(3.7167) | Xent 2.3026(2.3026) | Loss 3.7067(3.7167) | Error 0.9044(0.8996) Steps 784(773.37) | Grad Norm 4.4931(3.7990) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 88.1211, Epoch Time 1160.2977(994.8606), Bit/dim 3.7119(best: 3.7167), Xent 2.3026, Loss 3.7119, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 20.0676(19.2542) | Bit/dim 3.6949(3.7158) | Xent 2.3026(2.3026) | Loss 3.6949(3.7158) | Error 0.9056(0.8995) Steps 796(775.91) | Grad Norm 1.7928(3.5855) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 20.0294(19.3190) | Bit/dim 3.6935(3.7135) | Xent 2.3026(2.3026) | Loss 3.6935(3.7135) | Error 0.8989(0.9010) Steps 784(776.34) | Grad Norm 4.5915(3.3200) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 19.0909(19.2740) | Bit/dim 3.6679(3.7128) | Xent 2.3026(2.3026) | Loss 3.6679(3.7128) | Error 0.8778(0.9000) Steps 772(776.32) | Grad Norm 3.7028(3.9411) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 19.2964(19.3314) | Bit/dim 3.6968(3.7123) | Xent 2.3026(2.3026) | Loss 3.6968(3.7123) | Error 0.8933(0.9006) Steps 778(776.17) | Grad Norm 4.9649(3.9321) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 20.1821(19.3708) | Bit/dim 3.6916(3.7079) | Xent 2.3026(2.3026) | Loss 3.6916(3.7079) | Error 0.8956(0.8986) Steps 796(778.01) | Grad Norm 4.3512(3.8250) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 89.2199, Epoch Time 1174.8808(1000.2612), Bit/dim 3.7074(best: 3.7119), Xent 2.3026, Loss 3.7074, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 18.9941(19.4303) | Bit/dim 3.6966(3.7069) | Xent 2.3026(2.3026) | Loss 3.6966(3.7069) | Error 0.8911(0.8991) Steps 778(778.95) | Grad Norm 3.7834(3.9328) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 20.2690(19.4800) | Bit/dim 3.7014(3.7035) | Xent 2.3026(2.3026) | Loss 3.7014(3.7035) | Error 0.9067(0.8988) Steps 802(781.27) | Grad Norm 3.5541(3.7429) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 19.2185(19.4997) | Bit/dim 3.6870(3.7016) | Xent 2.3026(2.3026) | Loss 3.6870(3.7016) | Error 0.9089(0.8988) Steps 790(782.29) | Grad Norm 4.1709(3.3934) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 19.9538(19.4720) | Bit/dim 3.7024(3.7029) | Xent 2.3026(2.3026) | Loss 3.7024(3.7029) | Error 0.9033(0.9001) Steps 784(781.19) | Grad Norm 3.1854(3.7121) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 19.8308(19.4535) | Bit/dim 3.7159(3.6995) | Xent 2.3026(2.3026) | Loss 3.7159(3.6995) | Error 0.9000(0.9002) Steps 778(782.54) | Grad Norm 2.0563(3.9587) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 19.0795(19.3674) | Bit/dim 3.7120(3.7015) | Xent 2.3026(2.3026) | Loss 3.7120(3.7015) | Error 0.8933(0.9005) Steps 784(782.75) | Grad Norm 4.7332(3.9701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 91.5574, Epoch Time 1175.6497(1005.5229), Bit/dim 3.7007(best: 3.7074), Xent 2.3026, Loss 3.7007, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 19.2161(19.4099) | Bit/dim 3.7017(3.7002) | Xent 2.3026(2.3026) | Loss 3.7017(3.7002) | Error 0.9089(0.9004) Steps 790(785.25) | Grad Norm 3.5604(3.7475) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 19.2049(19.4218) | Bit/dim 3.6935(3.6992) | Xent 2.3026(2.3026) | Loss 3.6935(3.6992) | Error 0.8889(0.9011) Steps 778(785.32) | Grad Norm 2.9981(3.8596) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 19.6606(19.4088) | Bit/dim 3.6730(3.6958) | Xent 2.3026(2.3026) | Loss 3.6730(3.6958) | Error 0.8967(0.9012) Steps 790(784.79) | Grad Norm 2.9983(3.7599) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 19.2313(19.4200) | Bit/dim 3.6820(3.6929) | Xent 2.3026(2.3026) | Loss 3.6820(3.6929) | Error 0.8989(0.8999) Steps 778(785.24) | Grad Norm 3.2641(3.9386) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 19.1181(19.3215) | Bit/dim 3.7029(3.6937) | Xent 2.3026(2.3026) | Loss 3.7029(3.6937) | Error 0.9056(0.8994) Steps 778(783.57) | Grad Norm 4.1476(4.2085) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 88.5809, Epoch Time 1169.8982(1010.4541), Bit/dim 3.6920(best: 3.7007), Xent 2.3026, Loss 3.6920, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 18.8750(19.3179) | Bit/dim 3.6847(3.6944) | Xent 2.3026(2.3026) | Loss 3.6847(3.6944) | Error 0.8911(0.8998) Steps 802(784.87) | Grad Norm 3.5705(4.3937) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 19.0249(19.2979) | Bit/dim 3.6948(3.6959) | Xent 2.3026(2.3026) | Loss 3.6948(3.6959) | Error 0.8922(0.8993) Steps 790(784.50) | Grad Norm 3.3188(4.0275) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 19.3248(19.3046) | Bit/dim 3.6989(3.6917) | Xent 2.3026(2.3026) | Loss 3.6989(3.6917) | Error 0.8944(0.8990) Steps 778(784.07) | Grad Norm 1.6631(3.5372) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 19.0215(19.2863) | Bit/dim 3.6766(3.6903) | Xent 2.3026(2.3026) | Loss 3.6766(3.6903) | Error 0.9100(0.9004) Steps 784(785.29) | Grad Norm 5.7028(3.7095) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 19.5254(19.3325) | Bit/dim 3.6614(3.6859) | Xent 2.3026(2.3026) | Loss 3.6614(3.6859) | Error 0.8933(0.8998) Steps 784(786.98) | Grad Norm 3.6900(3.5753) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 19.1541(19.2278) | Bit/dim 3.7111(3.6842) | Xent 2.3026(2.3026) | Loss 3.7111(3.6842) | Error 0.9044(0.9002) Steps 802(787.27) | Grad Norm 3.2672(3.8210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 89.2838, Epoch Time 1163.5865(1015.0481), Bit/dim 3.6797(best: 3.6920), Xent 2.3026, Loss 3.6797, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 18.9984(19.2900) | Bit/dim 3.6925(3.6833) | Xent 2.3026(2.3026) | Loss 3.6925(3.6833) | Error 0.8922(0.9006) Steps 808(787.11) | Grad Norm 5.5107(3.6597) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 19.1761(19.2441) | Bit/dim 3.6262(3.6801) | Xent 2.3026(2.3026) | Loss 3.6262(3.6801) | Error 0.9044(0.8993) Steps 772(787.91) | Grad Norm 4.0819(3.6541) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 19.3974(19.1793) | Bit/dim 3.6734(3.6777) | Xent 2.3026(2.3026) | Loss 3.6734(3.6777) | Error 0.8989(0.8988) Steps 778(787.07) | Grad Norm 2.5420(3.4505) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 19.5410(19.1327) | Bit/dim 3.6517(3.6771) | Xent 2.3026(2.3026) | Loss 3.6517(3.6771) | Error 0.8933(0.8995) Steps 784(787.08) | Grad Norm 6.0582(3.8508) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 19.0685(19.1262) | Bit/dim 3.7081(3.6777) | Xent 2.3026(2.3026) | Loss 3.7081(3.6777) | Error 0.9011(0.8995) Steps 796(786.18) | Grad Norm 2.1974(3.6643) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 89.0427, Epoch Time 1157.8906(1019.3334), Bit/dim 3.6731(best: 3.6797), Xent 2.3026, Loss 3.6731, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 18.8100(19.1462) | Bit/dim 3.6573(3.6752) | Xent 2.3026(2.3026) | Loss 3.6573(3.6752) | Error 0.8944(0.8999) Steps 778(784.85) | Grad Norm 4.5112(3.7977) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 19.0185(19.1647) | Bit/dim 3.6716(3.6743) | Xent 2.3026(2.3026) | Loss 3.6716(3.6743) | Error 0.9067(0.9003) Steps 778(784.02) | Grad Norm 3.9272(3.4931) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 19.3749(19.0737) | Bit/dim 3.6734(3.6708) | Xent 2.3026(2.3026) | Loss 3.6734(3.6708) | Error 0.8878(0.8997) Steps 778(783.20) | Grad Norm 3.1047(3.6957) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 19.5066(19.1025) | Bit/dim 3.6735(3.6715) | Xent 2.3026(2.3026) | Loss 3.6735(3.6715) | Error 0.8967(0.9003) Steps 784(783.85) | Grad Norm 2.8126(3.8305) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 18.9464(19.0851) | Bit/dim 3.6987(3.6725) | Xent 2.3026(2.3026) | Loss 3.6987(3.6725) | Error 0.9167(0.9014) Steps 784(783.38) | Grad Norm 2.6075(3.4682) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 18.7073(19.0494) | Bit/dim 3.6629(3.6703) | Xent 2.3026(2.3026) | Loss 3.6629(3.6703) | Error 0.8800(0.9007) Steps 778(783.25) | Grad Norm 6.7354(3.4825) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 91.0478, Epoch Time 1155.2578(1023.4111), Bit/dim 3.6826(best: 3.6731), Xent 2.3026, Loss 3.6826, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 17.9679(18.9445) | Bit/dim 3.6259(3.6699) | Xent 2.3026(2.3026) | Loss 3.6259(3.6699) | Error 0.8967(0.9006) Steps 784(783.75) | Grad Norm 4.1034(3.6400) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 20.1767(18.9793) | Bit/dim 3.6650(3.6701) | Xent 2.3026(2.3026) | Loss 3.6650(3.6701) | Error 0.9022(0.9001) Steps 796(783.59) | Grad Norm 5.2709(3.7851) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 18.6484(18.9850) | Bit/dim 3.6716(3.6703) | Xent 2.3026(2.3026) | Loss 3.6716(3.6703) | Error 0.8800(0.8995) Steps 778(783.84) | Grad Norm 5.5470(3.7743) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 17.9075(18.9834) | Bit/dim 3.6537(3.6670) | Xent 2.3026(2.3026) | Loss 3.6537(3.6670) | Error 0.9000(0.8996) Steps 790(784.54) | Grad Norm 3.0988(3.9069) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 19.2967(18.9867) | Bit/dim 3.6520(3.6625) | Xent 2.3026(2.3026) | Loss 3.6520(3.6625) | Error 0.8978(0.8996) Steps 790(783.87) | Grad Norm 2.2727(3.6976) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 90.2415, Epoch Time 1147.8675(1027.1448), Bit/dim 3.6573(best: 3.6731), Xent 2.3026, Loss 3.6573, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 18.5345(18.9279) | Bit/dim 3.6395(3.6629) | Xent 2.3026(2.3026) | Loss 3.6395(3.6629) | Error 0.9133(0.9007) Steps 772(783.77) | Grad Norm 3.1620(3.4385) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 18.3574(18.9134) | Bit/dim 3.6426(3.6611) | Xent 2.3026(2.3026) | Loss 3.6426(3.6611) | Error 0.9033(0.8996) Steps 790(785.00) | Grad Norm 4.3576(3.6594) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 18.8887(18.9123) | Bit/dim 3.6161(3.6608) | Xent 2.3026(2.3026) | Loss 3.6161(3.6608) | Error 0.8856(0.9001) Steps 790(785.97) | Grad Norm 4.6470(3.6100) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 19.2405(18.9309) | Bit/dim 3.6625(3.6602) | Xent 2.3026(2.3026) | Loss 3.6625(3.6602) | Error 0.8978(0.9007) Steps 790(786.84) | Grad Norm 2.4407(3.4160) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 19.2497(18.9472) | Bit/dim 3.6917(3.6589) | Xent 2.3026(2.3026) | Loss 3.6917(3.6589) | Error 0.9167(0.8998) Steps 790(785.91) | Grad Norm 6.2689(3.7752) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 18.9498(18.9662) | Bit/dim 3.6641(3.6565) | Xent 2.3026(2.3026) | Loss 3.6641(3.6565) | Error 0.9078(0.9001) Steps 784(785.54) | Grad Norm 2.9124(3.6029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 90.2680, Epoch Time 1146.9002(1030.7375), Bit/dim 3.6496(best: 3.6573), Xent 2.3026, Loss 3.6496, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 18.7555(18.9298) | Bit/dim 3.6137(3.6534) | Xent 2.3026(2.3026) | Loss 3.6137(3.6534) | Error 0.8922(0.9004) Steps 778(785.79) | Grad Norm 2.5670(3.5471) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 18.7807(18.9282) | Bit/dim 3.6601(3.6532) | Xent 2.3026(2.3026) | Loss 3.6601(3.6532) | Error 0.8978(0.9001) Steps 790(785.45) | Grad Norm 3.9614(3.5205) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 18.7979(18.9085) | Bit/dim 3.6693(3.6503) | Xent 2.3026(2.3026) | Loss 3.6693(3.6503) | Error 0.9022(0.8997) Steps 784(784.90) | Grad Norm 5.9980(3.5002) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 18.2465(18.8866) | Bit/dim 3.6493(3.6511) | Xent 2.3026(2.3026) | Loss 3.6493(3.6511) | Error 0.8911(0.8999) Steps 778(784.78) | Grad Norm 3.1558(3.6224) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 18.8694(18.8251) | Bit/dim 3.6554(3.6518) | Xent 2.3026(2.3026) | Loss 3.6554(3.6518) | Error 0.8856(0.8999) Steps 802(783.78) | Grad Norm 2.3150(3.5047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 90.0104, Epoch Time 1142.6313(1034.0943), Bit/dim 3.6467(best: 3.6496), Xent 2.3026, Loss 3.6467, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 18.9345(18.7831) | Bit/dim 3.6132(3.6495) | Xent 2.3026(2.3026) | Loss 3.6132(3.6495) | Error 0.9089(0.9004) Steps 778(784.42) | Grad Norm 3.3400(3.5214) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 18.8591(18.7418) | Bit/dim 3.6323(3.6468) | Xent 2.3026(2.3026) | Loss 3.6323(3.6468) | Error 0.9022(0.9004) Steps 790(784.20) | Grad Norm 1.9573(3.1847) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 18.7346(18.7286) | Bit/dim 3.6426(3.6463) | Xent 2.3026(2.3026) | Loss 3.6426(3.6463) | Error 0.8922(0.8984) Steps 790(785.05) | Grad Norm 3.9333(3.5602) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 18.4521(18.6849) | Bit/dim 3.6663(3.6475) | Xent 2.3026(2.3026) | Loss 3.6663(3.6475) | Error 0.8911(0.9004) Steps 778(784.31) | Grad Norm 1.4066(3.4325) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 19.4574(18.6969) | Bit/dim 3.6092(3.6463) | Xent 2.3026(2.3026) | Loss 3.6092(3.6463) | Error 0.9044(0.9005) Steps 802(785.03) | Grad Norm 4.8855(3.2901) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 19.3215(18.6985) | Bit/dim 3.6086(3.6455) | Xent 2.3026(2.3026) | Loss 3.6086(3.6455) | Error 0.8967(0.8998) Steps 790(783.98) | Grad Norm 3.0059(3.3633) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 90.4418, Epoch Time 1130.9962(1037.0013), Bit/dim 3.6411(best: 3.6467), Xent 2.3026, Loss 3.6411, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 18.1191(18.6623) | Bit/dim 3.6288(3.6450) | Xent 2.3026(2.3026) | Loss 3.6288(3.6450) | Error 0.8844(0.8994) Steps 790(784.65) | Grad Norm 3.8956(3.4896) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 18.1562(18.6879) | Bit/dim 3.6250(3.6434) | Xent 2.3026(2.3026) | Loss 3.6250(3.6434) | Error 0.9011(0.8995) Steps 778(783.68) | Grad Norm 1.5494(3.3520) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 18.5639(18.6564) | Bit/dim 3.6381(3.6430) | Xent 2.3026(2.3026) | Loss 3.6381(3.6430) | Error 0.9089(0.9008) Steps 790(784.26) | Grad Norm 2.6516(3.4704) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 18.1741(18.6111) | Bit/dim 3.6273(3.6394) | Xent 2.3026(2.3026) | Loss 3.6273(3.6394) | Error 0.9078(0.9008) Steps 778(784.66) | Grad Norm 3.2401(3.6414) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 18.0583(18.6367) | Bit/dim 3.6359(3.6392) | Xent 2.3026(2.3026) | Loss 3.6359(3.6392) | Error 0.9033(0.9002) Steps 778(784.05) | Grad Norm 3.2043(3.4675) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 90.7775, Epoch Time 1129.9452(1039.7897), Bit/dim 3.6387(best: 3.6411), Xent 2.3026, Loss 3.6387, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 18.5614(18.6273) | Bit/dim 3.6737(3.6397) | Xent 2.3026(2.3026) | Loss 3.6737(3.6397) | Error 0.9133(0.8999) Steps 778(784.46) | Grad Norm 3.3796(3.6348) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 18.4055(18.6566) | Bit/dim 3.6157(3.6394) | Xent 2.3026(2.3026) | Loss 3.6157(3.6394) | Error 0.9022(0.9004) Steps 784(784.43) | Grad Norm 2.4807(3.3400) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 18.1585(18.6463) | Bit/dim 3.6148(3.6372) | Xent 2.3026(2.3026) | Loss 3.6148(3.6372) | Error 0.8933(0.8994) Steps 778(783.37) | Grad Norm 1.7091(3.4079) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 19.2946(18.6812) | Bit/dim 3.6635(3.6371) | Xent 2.3026(2.3026) | Loss 3.6635(3.6371) | Error 0.9067(0.8990) Steps 778(782.13) | Grad Norm 5.2267(3.4772) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 18.9302(18.6718) | Bit/dim 3.6234(3.6359) | Xent 2.3026(2.3026) | Loss 3.6234(3.6359) | Error 0.9000(0.8999) Steps 778(782.69) | Grad Norm 2.5402(3.1911) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 18.8295(18.7581) | Bit/dim 3.6065(3.6339) | Xent 2.3026(2.3026) | Loss 3.6065(3.6339) | Error 0.9056(0.9002) Steps 784(783.86) | Grad Norm 5.7688(3.5493) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 91.3351, Epoch Time 1139.6178(1042.7845), Bit/dim 3.6341(best: 3.6387), Xent 2.3026, Loss 3.6341, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 17.9838(18.6698) | Bit/dim 3.6367(3.6326) | Xent 2.3026(2.3026) | Loss 3.6367(3.6326) | Error 0.8900(0.9002) Steps 784(783.57) | Grad Norm 2.9698(3.5572) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 18.2707(18.6673) | Bit/dim 3.6372(3.6304) | Xent 2.3026(2.3026) | Loss 3.6372(3.6304) | Error 0.9211(0.9000) Steps 790(783.36) | Grad Norm 3.3702(3.2374) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 18.1857(18.7170) | Bit/dim 3.6339(3.6347) | Xent 2.3026(2.3026) | Loss 3.6339(3.6347) | Error 0.9089(0.9007) Steps 784(784.61) | Grad Norm 3.9890(3.3825) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 19.0822(18.6722) | Bit/dim 3.6459(3.6330) | Xent 2.3026(2.3026) | Loss 3.6459(3.6330) | Error 0.8878(0.8996) Steps 784(784.84) | Grad Norm 4.0524(3.3900) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 18.7382(18.6113) | Bit/dim 3.6192(3.6300) | Xent 2.3026(2.3026) | Loss 3.6192(3.6300) | Error 0.9056(0.9006) Steps 778(783.80) | Grad Norm 2.5482(3.2718) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 92.6388, Epoch Time 1130.1954(1045.4068), Bit/dim 3.6279(best: 3.6341), Xent 2.3026, Loss 3.6279, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 18.3101(18.5966) | Bit/dim 3.6137(3.6266) | Xent 2.3026(2.3026) | Loss 3.6137(3.6266) | Error 0.8900(0.8993) Steps 796(783.65) | Grad Norm 2.6002(3.3378) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 18.7313(18.6275) | Bit/dim 3.6352(3.6246) | Xent 2.3026(2.3026) | Loss 3.6352(3.6246) | Error 0.9033(0.8984) Steps 790(783.85) | Grad Norm 3.0845(3.3597) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 19.0092(18.7032) | Bit/dim 3.6183(3.6243) | Xent 2.3026(2.3026) | Loss 3.6183(3.6243) | Error 0.8978(0.8991) Steps 796(784.24) | Grad Norm 4.4986(3.5284) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 18.3076(18.6984) | Bit/dim 3.5909(3.6275) | Xent 2.3026(2.3026) | Loss 3.5909(3.6275) | Error 0.8856(0.9006) Steps 790(783.65) | Grad Norm 2.2162(3.3110) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 18.2634(18.6713) | Bit/dim 3.6231(3.6248) | Xent 2.3026(2.3026) | Loss 3.6231(3.6248) | Error 0.9078(0.9009) Steps 802(784.47) | Grad Norm 2.5610(3.4019) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 19.2442(18.6589) | Bit/dim 3.6236(3.6257) | Xent 2.3026(2.3026) | Loss 3.6236(3.6257) | Error 0.8933(0.9003) Steps 772(782.93) | Grad Norm 2.9525(3.4424) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 91.5164, Epoch Time 1136.7662(1048.1476), Bit/dim 3.6249(best: 3.6279), Xent 2.3026, Loss 3.6249, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 18.8897(18.5928) | Bit/dim 3.6358(3.6265) | Xent 2.3026(2.3026) | Loss 3.6358(3.6265) | Error 0.9122(0.9020) Steps 778(781.49) | Grad Norm 2.5609(3.1383) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 19.1763(18.6198) | Bit/dim 3.6237(3.6243) | Xent 2.3026(2.3026) | Loss 3.6237(3.6243) | Error 0.9100(0.9003) Steps 784(781.97) | Grad Norm 1.8638(3.2758) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 19.5454(18.6726) | Bit/dim 3.5918(3.6203) | Xent 2.3026(2.3026) | Loss 3.5918(3.6203) | Error 0.9089(0.9002) Steps 802(781.67) | Grad Norm 3.1454(3.3009) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 18.4403(18.7084) | Bit/dim 3.5567(3.6185) | Xent 2.3026(2.3026) | Loss 3.5567(3.6185) | Error 0.8767(0.8996) Steps 790(782.78) | Grad Norm 2.2412(3.3987) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 18.5781(18.6883) | Bit/dim 3.6174(3.6187) | Xent 2.3026(2.3026) | Loss 3.6174(3.6187) | Error 0.9000(0.8996) Steps 772(781.65) | Grad Norm 2.0839(3.2393) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 90.6478, Epoch Time 1132.4291(1050.6761), Bit/dim 3.6147(best: 3.6249), Xent 2.3026, Loss 3.6147, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 18.0330(18.6293) | Bit/dim 3.6153(3.6180) | Xent 2.3026(2.3026) | Loss 3.6153(3.6180) | Error 0.8989(0.9003) Steps 766(781.66) | Grad Norm 3.2929(2.9874) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 19.3003(18.6834) | Bit/dim 3.6536(3.6185) | Xent 2.3026(2.3026) | Loss 3.6536(3.6185) | Error 0.9044(0.9001) Steps 784(782.83) | Grad Norm 2.3961(3.2297) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 18.4584(18.6605) | Bit/dim 3.6111(3.6159) | Xent 2.3026(2.3026) | Loss 3.6111(3.6159) | Error 0.9111(0.8998) Steps 784(782.25) | Grad Norm 4.0402(3.2421) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 19.0325(18.6672) | Bit/dim 3.5821(3.6143) | Xent 2.3026(2.3026) | Loss 3.5821(3.6143) | Error 0.8867(0.9006) Steps 778(781.87) | Grad Norm 4.2281(3.3954) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 18.6288(18.5791) | Bit/dim 3.6430(3.6148) | Xent 2.3026(2.3026) | Loss 3.6430(3.6148) | Error 0.9089(0.9011) Steps 784(781.04) | Grad Norm 4.0989(3.4579) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 19.6860(18.6709) | Bit/dim 3.6510(3.6153) | Xent 2.3026(2.3026) | Loss 3.6510(3.6153) | Error 0.8911(0.8995) Steps 778(781.10) | Grad Norm 3.4154(3.2782) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 90.2240, Epoch Time 1132.9609(1053.1446), Bit/dim 3.6115(best: 3.6147), Xent 2.3026, Loss 3.6115, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 18.5112(18.6749) | Bit/dim 3.6117(3.6129) | Xent 2.3026(2.3026) | Loss 3.6117(3.6129) | Error 0.8911(0.8987) Steps 784(782.07) | Grad Norm 5.0251(3.2450) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 18.7826(18.7230) | Bit/dim 3.6124(3.6126) | Xent 2.3026(2.3026) | Loss 3.6124(3.6126) | Error 0.8967(0.8999) Steps 778(783.56) | Grad Norm 2.2783(3.3418) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 18.8920(18.7253) | Bit/dim 3.6220(3.6140) | Xent 2.3026(2.3026) | Loss 3.6220(3.6140) | Error 0.8744(0.8995) Steps 784(783.22) | Grad Norm 2.7150(3.2546) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 19.2116(18.7326) | Bit/dim 3.5964(3.6139) | Xent 2.3026(2.3026) | Loss 3.5964(3.6139) | Error 0.8911(0.8998) Steps 784(783.84) | Grad Norm 3.8773(3.5287) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 19.2970(18.7319) | Bit/dim 3.6008(3.6110) | Xent 2.3026(2.3026) | Loss 3.6008(3.6110) | Error 0.9089(0.9000) Steps 784(782.98) | Grad Norm 2.1867(3.3456) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 91.2224, Epoch Time 1139.8079(1055.7445), Bit/dim 3.6091(best: 3.6115), Xent 2.3026, Loss 3.6091, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 18.7254(18.7630) | Bit/dim 3.6052(3.6109) | Xent 2.3026(2.3026) | Loss 3.6052(3.6109) | Error 0.9144(0.9005) Steps 772(782.67) | Grad Norm 1.9145(2.9800) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 18.9110(18.8324) | Bit/dim 3.6195(3.6096) | Xent 2.3026(2.3026) | Loss 3.6195(3.6096) | Error 0.9144(0.9000) Steps 790(786.36) | Grad Norm 2.9172(3.2190) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 19.2451(18.8166) | Bit/dim 3.6076(3.6121) | Xent 2.3026(2.3026) | Loss 3.6076(3.6121) | Error 0.9167(0.9011) Steps 796(785.47) | Grad Norm 3.7012(3.2980) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 19.4226(18.8115) | Bit/dim 3.6250(3.6090) | Xent 2.3026(2.3026) | Loss 3.6250(3.6090) | Error 0.9011(0.9008) Steps 778(786.20) | Grad Norm 3.4176(3.1547) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 19.3227(18.8258) | Bit/dim 3.5705(3.6055) | Xent 2.3026(2.3026) | Loss 3.5705(3.6055) | Error 0.8978(0.9001) Steps 814(787.99) | Grad Norm 4.6321(3.2077) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 18.1978(18.8478) | Bit/dim 3.6010(3.6046) | Xent 2.3026(2.3026) | Loss 3.6010(3.6046) | Error 0.9011(0.8999) Steps 772(787.67) | Grad Norm 3.5919(3.3326) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 90.1971, Epoch Time 1143.7205(1058.3838), Bit/dim 3.6090(best: 3.6091), Xent 2.3026, Loss 3.6090, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 18.6343(18.8862) | Bit/dim 3.5978(3.6051) | Xent 2.3026(2.3026) | Loss 3.5978(3.6051) | Error 0.9011(0.9001) Steps 790(786.04) | Grad Norm 2.7417(3.2807) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 18.2232(18.8567) | Bit/dim 3.5914(3.6044) | Xent 2.3026(2.3026) | Loss 3.5914(3.6044) | Error 0.8967(0.9005) Steps 784(787.16) | Grad Norm 5.1270(3.3901) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 18.8546(18.8313) | Bit/dim 3.5890(3.6031) | Xent 2.3026(2.3026) | Loss 3.5890(3.6031) | Error 0.8844(0.8996) Steps 796(787.99) | Grad Norm 2.9728(3.3556) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 18.2758(18.8502) | Bit/dim 3.5978(3.6003) | Xent 2.3026(2.3026) | Loss 3.5978(3.6003) | Error 0.8978(0.8992) Steps 808(789.82) | Grad Norm 4.0412(3.3238) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 18.9207(18.9038) | Bit/dim 3.6052(3.6012) | Xent 2.3026(2.3026) | Loss 3.6052(3.6012) | Error 0.9144(0.8990) Steps 778(789.84) | Grad Norm 2.1506(3.3210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 91.0194, Epoch Time 1146.1407(1061.0165), Bit/dim 3.5982(best: 3.6090), Xent 2.3026, Loss 3.5982, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 17.4326(18.8133) | Bit/dim 3.5900(3.6005) | Xent 2.3026(2.3026) | Loss 3.5900(3.6005) | Error 0.9044(0.9009) Steps 784(788.48) | Grad Norm 2.0538(3.0655) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 20.3038(18.8597) | Bit/dim 3.5761(3.6009) | Xent 2.3026(2.3026) | Loss 3.5761(3.6009) | Error 0.8911(0.9012) Steps 784(787.30) | Grad Norm 2.2368(3.2698) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 18.6050(18.8246) | Bit/dim 3.5782(3.6009) | Xent 2.3026(2.3026) | Loss 3.5782(3.6009) | Error 0.9056(0.9011) Steps 790(787.16) | Grad Norm 2.4895(3.3740) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 18.0781(18.8014) | Bit/dim 3.5724(3.5968) | Xent 2.3026(2.3026) | Loss 3.5724(3.5968) | Error 0.8911(0.9006) Steps 796(787.89) | Grad Norm 5.6555(3.3357) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 19.5021(18.7858) | Bit/dim 3.6138(3.5981) | Xent 2.3026(2.3026) | Loss 3.6138(3.5981) | Error 0.9100(0.9005) Steps 790(787.71) | Grad Norm 3.5382(3.3186) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 18.5580(18.7709) | Bit/dim 3.6109(3.5989) | Xent 2.3026(2.3026) | Loss 3.6109(3.5989) | Error 0.8978(0.8990) Steps 796(788.26) | Grad Norm 4.7969(3.2851) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 91.6424, Epoch Time 1139.4949(1063.3708), Bit/dim 3.6023(best: 3.5982), Xent 2.3026, Loss 3.6023, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 18.3704(18.7745) | Bit/dim 3.5943(3.5978) | Xent 2.3026(2.3026) | Loss 3.5943(3.5978) | Error 0.8956(0.8992) Steps 778(787.50) | Grad Norm 2.5643(3.3134) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 19.0598(18.8366) | Bit/dim 3.6083(3.5976) | Xent 2.3026(2.3026) | Loss 3.6083(3.5976) | Error 0.8956(0.8983) Steps 778(788.55) | Grad Norm 3.3164(3.0891) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 18.9732(18.7972) | Bit/dim 3.5777(3.5934) | Xent 2.3026(2.3026) | Loss 3.5777(3.5934) | Error 0.9133(0.8990) Steps 790(789.42) | Grad Norm 3.9981(3.2313) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 18.5114(18.7619) | Bit/dim 3.5855(3.5935) | Xent 2.3026(2.3026) | Loss 3.5855(3.5935) | Error 0.9033(0.9006) Steps 778(789.39) | Grad Norm 2.8920(3.2232) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 19.4298(18.7524) | Bit/dim 3.5775(3.5944) | Xent 2.3026(2.3026) | Loss 3.5775(3.5944) | Error 0.8811(0.8997) Steps 814(789.47) | Grad Norm 3.5865(3.1821) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 93.3580, Epoch Time 1142.7069(1065.7509), Bit/dim 3.5919(best: 3.5982), Xent 2.3026, Loss 3.5919, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.7218(18.7828) | Bit/dim 3.5949(3.5918) | Xent 2.3026(2.3026) | Loss 3.5949(3.5918) | Error 0.8878(0.8987) Steps 784(789.44) | Grad Norm 5.2341(3.2797) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 19.1378(18.8235) | Bit/dim 3.6168(3.5915) | Xent 2.3026(2.3026) | Loss 3.6168(3.5915) | Error 0.8956(0.8982) Steps 802(790.59) | Grad Norm 3.1761(3.3817) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 18.2243(18.7644) | Bit/dim 3.5422(3.5879) | Xent 2.3026(2.3026) | Loss 3.5422(3.5879) | Error 0.8844(0.8988) Steps 790(791.36) | Grad Norm 2.1250(3.0838) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 17.9860(18.7458) | Bit/dim 3.6120(3.5905) | Xent 2.3026(2.3026) | Loss 3.6120(3.5905) | Error 0.8889(0.8995) Steps 790(791.11) | Grad Norm 3.7226(3.1334) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 18.2848(18.7974) | Bit/dim 3.5864(3.5899) | Xent 2.3026(2.3026) | Loss 3.5864(3.5899) | Error 0.9056(0.9001) Steps 766(790.41) | Grad Norm 2.3543(3.0648) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 18.9516(18.8259) | Bit/dim 3.6013(3.5913) | Xent 2.3026(2.3026) | Loss 3.6013(3.5913) | Error 0.9011(0.9008) Steps 796(792.09) | Grad Norm 3.4038(3.1642) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 95.2069, Epoch Time 1146.2965(1068.1673), Bit/dim 3.5875(best: 3.5919), Xent 2.3026, Loss 3.5875, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 18.9602(18.8234) | Bit/dim 3.6044(3.5913) | Xent 2.3026(2.3026) | Loss 3.6044(3.5913) | Error 0.9089(0.8999) Steps 790(792.47) | Grad Norm 1.9985(2.9987) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 19.1454(18.8647) | Bit/dim 3.5909(3.5903) | Xent 2.3026(2.3026) | Loss 3.5909(3.5903) | Error 0.8922(0.9001) Steps 796(792.75) | Grad Norm 3.4477(3.0293) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 19.6037(18.9420) | Bit/dim 3.5859(3.5909) | Xent 2.3026(2.3026) | Loss 3.5859(3.5909) | Error 0.8967(0.8999) Steps 790(792.06) | Grad Norm 2.1376(3.0641) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 18.2128(18.9042) | Bit/dim 3.5967(3.5877) | Xent 2.3026(2.3026) | Loss 3.5967(3.5877) | Error 0.9000(0.8999) Steps 772(791.71) | Grad Norm 3.6961(2.9672) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 18.5246(18.8068) | Bit/dim 3.5955(3.5878) | Xent 2.3026(2.3026) | Loss 3.5955(3.5878) | Error 0.8867(0.9009) Steps 790(790.29) | Grad Norm 4.0679(3.0037) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 96.0179, Epoch Time 1148.3900(1070.5740), Bit/dim 3.5833(best: 3.5875), Xent 2.3026, Loss 3.5833, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 19.2489(18.8530) | Bit/dim 3.5860(3.5854) | Xent 2.3026(2.3026) | Loss 3.5860(3.5854) | Error 0.8967(0.9007) Steps 784(790.02) | Grad Norm 3.0101(3.0604) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 18.7577(18.8047) | Bit/dim 3.5683(3.5843) | Xent 2.3026(2.3026) | Loss 3.5683(3.5843) | Error 0.9078(0.9003) Steps 790(790.87) | Grad Norm 3.8337(3.1055) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 18.9404(18.7518) | Bit/dim 3.5951(3.5831) | Xent 2.3026(2.3026) | Loss 3.5951(3.5831) | Error 0.9133(0.9005) Steps 808(790.16) | Grad Norm 2.1363(3.1026) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 19.0969(18.7788) | Bit/dim 3.5499(3.5826) | Xent 2.3026(2.3026) | Loss 3.5499(3.5826) | Error 0.9000(0.9004) Steps 808(793.13) | Grad Norm 2.7386(3.1983) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 18.4508(18.7151) | Bit/dim 3.5777(3.5841) | Xent 2.3026(2.3026) | Loss 3.5777(3.5841) | Error 0.9100(0.9007) Steps 796(792.19) | Grad Norm 3.3634(3.1675) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 19.0316(18.7078) | Bit/dim 3.5718(3.5841) | Xent 2.3026(2.3026) | Loss 3.5718(3.5841) | Error 0.8889(0.8997) Steps 802(794.56) | Grad Norm 2.9080(3.1434) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 94.5395, Epoch Time 1140.3715(1072.6679), Bit/dim 3.5819(best: 3.5833), Xent 2.3026, Loss 3.5819, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 19.1325(18.7824) | Bit/dim 3.5171(3.5810) | Xent 2.3026(2.3026) | Loss 3.5171(3.5810) | Error 0.8789(0.8975) Steps 796(796.20) | Grad Norm 2.2445(3.0704) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 19.3939(18.7992) | Bit/dim 3.5833(3.5786) | Xent 2.3026(2.3026) | Loss 3.5833(3.5786) | Error 0.9056(0.8986) Steps 802(794.28) | Grad Norm 4.2743(3.0346) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 18.1328(18.7990) | Bit/dim 3.5703(3.5800) | Xent 2.3026(2.3026) | Loss 3.5703(3.5800) | Error 0.9044(0.9012) Steps 796(795.69) | Grad Norm 1.7915(3.1158) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 18.8117(18.8073) | Bit/dim 3.6075(3.5814) | Xent 2.3026(2.3026) | Loss 3.6075(3.5814) | Error 0.9133(0.9010) Steps 784(794.86) | Grad Norm 3.0270(3.0411) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 19.5150(18.8179) | Bit/dim 3.5876(3.5812) | Xent 2.3026(2.3026) | Loss 3.5876(3.5812) | Error 0.9178(0.9012) Steps 778(795.41) | Grad Norm 2.7962(3.0529) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 93.3222, Epoch Time 1146.8067(1074.8921), Bit/dim 3.5777(best: 3.5819), Xent 2.3026, Loss 3.5777, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 17.8504(18.7937) | Bit/dim 3.5741(3.5817) | Xent 2.3026(2.3026) | Loss 3.5741(3.5817) | Error 0.8978(0.9008) Steps 790(795.45) | Grad Norm 4.2898(3.0872) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 19.2884(18.8220) | Bit/dim 3.5912(3.5813) | Xent 2.3026(2.3026) | Loss 3.5912(3.5813) | Error 0.9100(0.9008) Steps 790(795.24) | Grad Norm 2.1083(2.9741) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 19.3846(18.8176) | Bit/dim 3.5992(3.5803) | Xent 2.3026(2.3026) | Loss 3.5992(3.5803) | Error 0.9011(0.9008) Steps 820(795.83) | Grad Norm 5.1666(3.1282) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 18.0579(18.8074) | Bit/dim 3.5910(3.5789) | Xent 2.3026(2.3026) | Loss 3.5910(3.5789) | Error 0.9067(0.9017) Steps 814(796.14) | Grad Norm 1.5242(3.0946) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 18.4941(18.8489) | Bit/dim 3.5616(3.5744) | Xent 2.3026(2.3026) | Loss 3.5616(3.5744) | Error 0.8967(0.9005) Steps 790(794.12) | Grad Norm 2.9446(3.0701) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 19.0464(18.8464) | Bit/dim 3.5810(3.5750) | Xent 2.3026(2.3026) | Loss 3.5810(3.5750) | Error 0.9056(0.8997) Steps 814(795.85) | Grad Norm 2.9989(3.2032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 95.6084, Epoch Time 1147.8239(1077.0800), Bit/dim 3.5777(best: 3.5777), Xent 2.3026, Loss 3.5777, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 17.9996(18.8411) | Bit/dim 3.5649(3.5756) | Xent 2.3026(2.3026) | Loss 3.5649(3.5756) | Error 0.9122(0.8999) Steps 796(794.56) | Grad Norm 2.2772(3.1602) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 19.6311(18.8879) | Bit/dim 3.5385(3.5735) | Xent 2.3026(2.3026) | Loss 3.5385(3.5735) | Error 0.8844(0.8994) Steps 778(794.38) | Grad Norm 3.6238(2.9330) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 19.1108(18.9080) | Bit/dim 3.5208(3.5724) | Xent 2.3026(2.3026) | Loss 3.5208(3.5724) | Error 0.8978(0.9005) Steps 814(794.69) | Grad Norm 1.8994(2.8702) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 19.2593(18.9763) | Bit/dim 3.5781(3.5726) | Xent 2.3026(2.3026) | Loss 3.5781(3.5726) | Error 0.8922(0.9003) Steps 808(796.18) | Grad Norm 3.7618(3.0014) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 19.7738(19.0686) | Bit/dim 3.6063(3.5726) | Xent 2.3026(2.3026) | Loss 3.6063(3.5726) | Error 0.9089(0.9000) Steps 808(798.51) | Grad Norm 2.7975(3.1112) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 94.7262, Epoch Time 1159.0569(1079.5393), Bit/dim 3.5755(best: 3.5777), Xent 2.3026, Loss 3.5755, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 18.2914(19.0712) | Bit/dim 3.5732(3.5743) | Xent 2.3026(2.3026) | Loss 3.5732(3.5743) | Error 0.9067(0.8997) Steps 784(796.10) | Grad Norm 2.3436(3.0434) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 19.1551(19.0000) | Bit/dim 3.5639(3.5744) | Xent 2.3026(2.3026) | Loss 3.5639(3.5744) | Error 0.8933(0.8997) Steps 796(796.58) | Grad Norm 2.8649(3.0434) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 19.7723(18.9687) | Bit/dim 3.5416(3.5737) | Xent 2.3026(2.3026) | Loss 3.5416(3.5737) | Error 0.8944(0.9000) Steps 796(796.64) | Grad Norm 4.4184(3.1146) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 18.9755(19.0806) | Bit/dim 3.5501(3.5720) | Xent 2.3026(2.3026) | Loss 3.5501(3.5720) | Error 0.9111(0.8991) Steps 790(798.19) | Grad Norm 2.2904(3.1347) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 19.3788(19.1184) | Bit/dim 3.5742(3.5713) | Xent 2.3026(2.3026) | Loss 3.5742(3.5713) | Error 0.8933(0.8996) Steps 790(800.33) | Grad Norm 3.0510(3.1616) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 18.3948(19.1224) | Bit/dim 3.5531(3.5671) | Xent 2.3026(2.3026) | Loss 3.5531(3.5671) | Error 0.9089(0.8997) Steps 790(800.09) | Grad Norm 3.9965(3.0285) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 94.3006, Epoch Time 1160.2542(1081.9608), Bit/dim 3.5728(best: 3.5755), Xent 2.3026, Loss 3.5728, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 19.5925(19.2195) | Bit/dim 3.5678(3.5663) | Xent 2.3026(2.3026) | Loss 3.5678(3.5663) | Error 0.8911(0.9005) Steps 826(803.54) | Grad Norm 3.0086(3.2590) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 19.5430(19.2513) | Bit/dim 3.5686(3.5672) | Xent 2.3026(2.3026) | Loss 3.5686(3.5672) | Error 0.9056(0.9009) Steps 808(802.23) | Grad Norm 2.4419(3.1956) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 19.5420(19.2854) | Bit/dim 3.5528(3.5660) | Xent 2.3026(2.3026) | Loss 3.5528(3.5660) | Error 0.8867(0.8999) Steps 772(799.67) | Grad Norm 3.1464(3.0373) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 20.3623(19.3718) | Bit/dim 3.5767(3.5667) | Xent 2.3026(2.3026) | Loss 3.5767(3.5667) | Error 0.9033(0.8985) Steps 832(802.44) | Grad Norm 3.3163(3.1081) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 19.3130(19.4160) | Bit/dim 3.5691(3.5662) | Xent 2.3026(2.3026) | Loss 3.5691(3.5662) | Error 0.9144(0.8990) Steps 778(803.97) | Grad Norm 2.3872(3.0249) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 94.3240, Epoch Time 1180.7972(1084.9259), Bit/dim 3.5661(best: 3.5728), Xent 2.3026, Loss 3.5661, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 19.0133(19.4322) | Bit/dim 3.5758(3.5664) | Xent 2.3026(2.3026) | Loss 3.5758(3.5664) | Error 0.9067(0.8999) Steps 802(805.01) | Grad Norm 3.2819(3.0003) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 19.5445(19.4542) | Bit/dim 3.5837(3.5656) | Xent 2.3026(2.3026) | Loss 3.5837(3.5656) | Error 0.9089(0.8991) Steps 808(804.36) | Grad Norm 2.4954(2.9157) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 18.6930(19.4012) | Bit/dim 3.5827(3.5661) | Xent 2.3026(2.3026) | Loss 3.5827(3.5661) | Error 0.9022(0.8998) Steps 808(805.45) | Grad Norm 1.2133(2.7550) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 19.4606(19.4097) | Bit/dim 3.5220(3.5648) | Xent 2.3026(2.3026) | Loss 3.5220(3.5648) | Error 0.8756(0.8998) Steps 820(807.89) | Grad Norm 3.2484(2.6774) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 19.2469(19.4759) | Bit/dim 3.5637(3.5637) | Xent 2.3026(2.3026) | Loss 3.5637(3.5637) | Error 0.9011(0.8994) Steps 820(810.61) | Grad Norm 4.0862(2.9891) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 19.6682(19.5185) | Bit/dim 3.5973(3.5650) | Xent 2.3026(2.3026) | Loss 3.5973(3.5650) | Error 0.9122(0.9003) Steps 790(809.04) | Grad Norm 2.2677(2.9430) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 94.2571, Epoch Time 1183.7131(1087.8895), Bit/dim 3.5612(best: 3.5661), Xent 2.3026, Loss 3.5612, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 19.7450(19.5917) | Bit/dim 3.5570(3.5638) | Xent 2.3026(2.3026) | Loss 3.5570(3.5638) | Error 0.9067(0.9007) Steps 814(809.01) | Grad Norm 4.1930(3.0044) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 20.0366(19.6426) | Bit/dim 3.5760(3.5648) | Xent 2.3026(2.3026) | Loss 3.5760(3.5648) | Error 0.8978(0.9017) Steps 826(809.94) | Grad Norm 1.8334(2.8491) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 19.0781(19.6547) | Bit/dim 3.5397(3.5625) | Xent 2.3026(2.3026) | Loss 3.5397(3.5625) | Error 0.8900(0.9002) Steps 820(810.48) | Grad Norm 2.4513(2.9489) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 19.4669(19.6122) | Bit/dim 3.5835(3.5609) | Xent 2.3026(2.3026) | Loss 3.5835(3.5609) | Error 0.9033(0.9004) Steps 802(812.04) | Grad Norm 4.9202(2.9063) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 20.0606(19.6783) | Bit/dim 3.5722(3.5607) | Xent 2.3026(2.3026) | Loss 3.5722(3.5607) | Error 0.8956(0.8996) Steps 820(813.06) | Grad Norm 1.8825(2.8396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Iter 3580 | Time 19.3802(19.7108) | Bit/dim 3.5595(3.5595) | Xent 2.3026(2.3026) | Loss 3.5595(3.5595) | Error 0.9089(0.8994) Steps 820(814.27) | Grad Norm 3.5403(2.9091) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 20.4335(19.7384) | Bit/dim 3.5586(3.5597) | Xent 2.3026(2.3026) | Loss 3.5586(3.5597) | Error 0.8900(0.8986) Steps 808(814.58) | Grad Norm 1.9797(2.8120) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 20.1859(19.7937) | Bit/dim 3.5567(3.5610) | Xent 2.3026(2.3026) | Loss 3.5567(3.5610) | Error 0.9089(0.8997) Steps 820(816.09) | Grad Norm 3.1803(2.9556) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 20.1610(19.7477) | Bit/dim 3.5204(3.5580) | Xent 2.3026(2.3026) | Loss 3.5204(3.5580) | Error 0.8944(0.9000) Steps 814(815.67) | Grad Norm 1.9356(2.8819) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 19.7067(19.7286) | Bit/dim 3.5656(3.5583) | Xent 2.3026(2.3026) | Loss 3.5656(3.5583) | Error 0.8833(0.8988) Steps 826(815.72) | Grad Norm 3.9968(3.1215) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 20.0769(19.7485) | Bit/dim 3.5649(3.5599) | Xent 2.3026(2.3026) | Loss 3.5649(3.5599) | Error 0.8944(0.9007) Steps 802(813.91) | Grad Norm 2.1514(2.9888) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 93.0442, Epoch Time 1196.4791(1094.2983), Bit/dim 3.5596(best: 3.5612), Xent 2.3026, Loss 3.5596, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 20.0052(19.7943) | Bit/dim 3.5784(3.5599) | Xent 2.3026(2.3026) | Loss 3.5784(3.5599) | Error 0.9100(0.9014) Steps 826(816.00) | Grad Norm 3.4140(2.9369) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 19.7325(19.8460) | Bit/dim 3.5249(3.5565) | Xent 2.3026(2.3026) | Loss 3.5249(3.5565) | Error 0.9033(0.9013) Steps 802(816.62) | Grad Norm 1.7272(2.7379) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 19.9678(19.9022) | Bit/dim 3.5740(3.5565) | Xent 2.3026(2.3026) | Loss 3.5740(3.5565) | Error 0.8944(0.9010) Steps 838(817.99) | Grad Norm 1.7129(2.8390) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 20.2610(19.9055) | Bit/dim 3.5804(3.5548) | Xent 2.3026(2.3026) | Loss 3.5804(3.5548) | Error 0.8911(0.9008) Steps 838(817.44) | Grad Norm 3.2388(2.8502) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 19.7080(19.8779) | Bit/dim 3.5799(3.5562) | Xent 2.3026(2.3026) | Loss 3.5799(3.5562) | Error 0.9056(0.8990) Steps 808(818.68) | Grad Norm 3.0554(2.8706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 95.0950, Epoch Time 1207.8358(1097.7044), Bit/dim 3.5541(best: 3.5596), Xent 2.3026, Loss 3.5541, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 19.5317(19.8618) | Bit/dim 3.5429(3.5572) | Xent 2.3026(2.3026) | Loss 3.5429(3.5572) | Error 0.9144(0.9002) Steps 808(820.44) | Grad Norm 2.4529(3.0173) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 20.2105(19.8957) | Bit/dim 3.5463(3.5552) | Xent 2.3026(2.3026) | Loss 3.5463(3.5552) | Error 0.9011(0.9005) Steps 826(820.65) | Grad Norm 3.3141(3.0796) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 20.2567(19.9127) | Bit/dim 3.5595(3.5548) | Xent 2.3026(2.3026) | Loss 3.5595(3.5548) | Error 0.8789(0.8998) Steps 820(820.02) | Grad Norm 1.6988(2.9851) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 20.0325(19.9526) | Bit/dim 3.5548(3.5535) | Xent 2.3026(2.3026) | Loss 3.5548(3.5535) | Error 0.8856(0.8999) Steps 820(820.06) | Grad Norm 1.9693(2.7697) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 20.1012(19.9927) | Bit/dim 3.6016(3.5536) | Xent 2.3026(2.3026) | Loss 3.6016(3.5536) | Error 0.9056(0.8993) Steps 826(819.89) | Grad Norm 3.5974(3.0127) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 19.6475(19.9913) | Bit/dim 3.5318(3.5543) | Xent 2.3026(2.3026) | Loss 3.5318(3.5543) | Error 0.8944(0.9001) Steps 832(819.95) | Grad Norm 2.8038(3.0096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 96.1806, Epoch Time 1211.6719(1101.1235), Bit/dim 3.5545(best: 3.5541), Xent 2.3026, Loss 3.5545, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 19.9090(19.9109) | Bit/dim 3.5331(3.5516) | Xent 2.3026(2.3026) | Loss 3.5331(3.5516) | Error 0.8756(0.8990) Steps 832(819.85) | Grad Norm 2.2564(2.7890) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 20.0558(19.8360) | Bit/dim 3.5464(3.5526) | Xent 2.3026(2.3026) | Loss 3.5464(3.5526) | Error 0.8967(0.8986) Steps 838(823.33) | Grad Norm 2.6500(2.7702) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 20.3525(19.8545) | Bit/dim 3.5589(3.5526) | Xent 2.3026(2.3026) | Loss 3.5589(3.5526) | Error 0.9022(0.8997) Steps 820(823.93) | Grad Norm 3.4208(2.6664) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 19.6873(19.9176) | Bit/dim 3.5789(3.5499) | Xent 2.3026(2.3026) | Loss 3.5789(3.5499) | Error 0.9156(0.9007) Steps 820(825.80) | Grad Norm 3.3954(2.8707) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 19.8823(19.8931) | Bit/dim 3.5277(3.5477) | Xent 2.3026(2.3026) | Loss 3.5277(3.5477) | Error 0.8778(0.8994) Steps 808(824.51) | Grad Norm 2.0484(2.7654) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 96.6773, Epoch Time 1205.0680(1104.2418), Bit/dim 3.5507(best: 3.5541), Xent 2.3026, Loss 3.5507, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 19.4447(19.9227) | Bit/dim 3.5549(3.5486) | Xent 2.3026(2.3026) | Loss 3.5549(3.5486) | Error 0.9089(0.9004) Steps 820(825.41) | Grad Norm 4.5043(2.8040) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 19.5964(19.8928) | Bit/dim 3.5422(3.5500) | Xent 2.3026(2.3026) | Loss 3.5422(3.5500) | Error 0.8878(0.9008) Steps 808(824.68) | Grad Norm 4.7842(3.1980) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 20.1486(19.8963) | Bit/dim 3.5820(3.5519) | Xent 2.3026(2.3026) | Loss 3.5820(3.5519) | Error 0.8944(0.8995) Steps 820(821.06) | Grad Norm 1.9426(3.0338) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 20.1939(19.8832) | Bit/dim 3.5223(3.5498) | Xent 2.3026(2.3026) | Loss 3.5223(3.5498) | Error 0.8833(0.8991) Steps 844(823.31) | Grad Norm 3.6050(2.7915) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 19.4578(19.8818) | Bit/dim 3.5293(3.5487) | Xent 2.3026(2.3026) | Loss 3.5293(3.5487) | Error 0.8978(0.8998) Steps 832(824.30) | Grad Norm 1.9754(2.9127) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 20.2255(19.8606) | Bit/dim 3.5336(3.5479) | Xent 2.3026(2.3026) | Loss 3.5336(3.5479) | Error 0.8989(0.9000) Steps 826(824.95) | Grad Norm 2.0737(2.8310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 96.0352, Epoch Time 1204.0260(1107.2353), Bit/dim 3.5457(best: 3.5507), Xent 2.3026, Loss 3.5457, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 19.6938(19.8207) | Bit/dim 3.5566(3.5460) | Xent 2.3026(2.3026) | Loss 3.5566(3.5460) | Error 0.9122(0.9008) Steps 820(825.80) | Grad Norm 3.5361(2.6937) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 20.2845(19.8317) | Bit/dim 3.5499(3.5438) | Xent 2.3026(2.3026) | Loss 3.5499(3.5438) | Error 0.8867(0.8997) Steps 832(825.96) | Grad Norm 1.4506(2.7520) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 19.3171(19.8558) | Bit/dim 3.5336(3.5420) | Xent 2.3026(2.3026) | Loss 3.5336(3.5420) | Error 0.8922(0.8996) Steps 814(826.44) | Grad Norm 2.9153(2.6521) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 20.1167(19.8837) | Bit/dim 3.5487(3.5443) | Xent 2.3026(2.3026) | Loss 3.5487(3.5443) | Error 0.8989(0.9004) Steps 826(825.83) | Grad Norm 4.5786(2.8995) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 19.7624(19.8807) | Bit/dim 3.5494(3.5448) | Xent 2.3026(2.3026) | Loss 3.5494(3.5448) | Error 0.8978(0.8999) Steps 826(823.83) | Grad Norm 2.3806(2.7727) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 96.7104, Epoch Time 1207.7059(1110.2494), Bit/dim 3.5428(best: 3.5457), Xent 2.3026, Loss 3.5428, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 19.9218(19.9044) | Bit/dim 3.5089(3.5428) | Xent 2.3026(2.3026) | Loss 3.5089(3.5428) | Error 0.8922(0.8983) Steps 820(824.56) | Grad Norm 3.8019(2.8160) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 19.6751(19.9257) | Bit/dim 3.5570(3.5435) | Xent 2.3026(2.3026) | Loss 3.5570(3.5435) | Error 0.9200(0.8996) Steps 826(824.01) | Grad Norm 2.8517(2.8119) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 20.3699(19.9954) | Bit/dim 3.5473(3.5430) | Xent 2.3026(2.3026) | Loss 3.5473(3.5430) | Error 0.9022(0.8997) Steps 838(824.92) | Grad Norm 1.7093(2.8337) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 19.5196(19.9224) | Bit/dim 3.5538(3.5417) | Xent 2.3026(2.3026) | Loss 3.5538(3.5417) | Error 0.8978(0.9007) Steps 826(825.83) | Grad Norm 4.0077(2.7773) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 19.5054(19.8594) | Bit/dim 3.5440(3.5429) | Xent 2.3026(2.3026) | Loss 3.5440(3.5429) | Error 0.9178(0.9008) Steps 826(826.68) | Grad Norm 2.9688(2.9192) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 19.4591(19.8708) | Bit/dim 3.5393(3.5428) | Xent 2.3026(2.3026) | Loss 3.5393(3.5428) | Error 0.9100(0.8998) Steps 832(827.86) | Grad Norm 3.8341(2.8652) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 95.6925, Epoch Time 1205.1531(1113.0966), Bit/dim 3.5416(best: 3.5428), Xent 2.3026, Loss 3.5416, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 19.9689(19.9243) | Bit/dim 3.5682(3.5442) | Xent 2.3026(2.3026) | Loss 3.5682(3.5442) | Error 0.8944(0.9002) Steps 832(827.21) | Grad Norm 2.3196(2.7281) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 19.7343(19.9405) | Bit/dim 3.5378(3.5424) | Xent 2.3026(2.3026) | Loss 3.5378(3.5424) | Error 0.9033(0.8997) Steps 820(827.32) | Grad Norm 2.9543(2.8601) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 19.4997(19.8631) | Bit/dim 3.5258(3.5397) | Xent 2.3026(2.3026) | Loss 3.5258(3.5397) | Error 0.9089(0.8998) Steps 820(827.14) | Grad Norm 2.9067(2.8114) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 19.6634(19.8401) | Bit/dim 3.5361(3.5394) | Xent 2.3026(2.3026) | Loss 3.5361(3.5394) | Error 0.8967(0.8994) Steps 832(828.80) | Grad Norm 4.3115(2.8987) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 19.5749(19.8033) | Bit/dim 3.5329(3.5407) | Xent 2.3026(2.3026) | Loss 3.5329(3.5407) | Error 0.8967(0.9000) Steps 832(829.99) | Grad Norm 2.0378(2.8849) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 96.8206, Epoch Time 1204.8001(1115.8477), Bit/dim 3.5401(best: 3.5416), Xent 2.3026, Loss 3.5401, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 20.1413(19.8388) | Bit/dim 3.5555(3.5380) | Xent 2.3026(2.3026) | Loss 3.5555(3.5380) | Error 0.8889(0.8999) Steps 838(830.48) | Grad Norm 1.5677(2.7968) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 19.6148(19.8368) | Bit/dim 3.5147(3.5371) | Xent 2.3026(2.3026) | Loss 3.5147(3.5371) | Error 0.8978(0.9001) Steps 826(829.82) | Grad Norm 3.2931(2.8429) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 19.0974(19.8543) | Bit/dim 3.5340(3.5378) | Xent 2.3026(2.3026) | Loss 3.5340(3.5378) | Error 0.8833(0.9000) Steps 838(829.22) | Grad Norm 2.4633(2.7859) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 20.3255(19.9034) | Bit/dim 3.5189(3.5363) | Xent 2.3026(2.3026) | Loss 3.5189(3.5363) | Error 0.8811(0.8996) Steps 844(830.00) | Grad Norm 2.7089(2.8400) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 19.7760(19.9612) | Bit/dim 3.5689(3.5368) | Xent 2.3026(2.3026) | Loss 3.5689(3.5368) | Error 0.8956(0.8987) Steps 826(830.36) | Grad Norm 3.2937(2.7248) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 20.4220(19.9512) | Bit/dim 3.5511(3.5375) | Xent 2.3026(2.3026) | Loss 3.5511(3.5375) | Error 0.9100(0.9004) Steps 844(831.21) | Grad Norm 3.6499(2.7231) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 96.5640, Epoch Time 1210.2828(1118.6807), Bit/dim 3.5393(best: 3.5401), Xent 2.3026, Loss 3.5393, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 19.8133(19.9510) | Bit/dim 3.5307(3.5370) | Xent 2.3026(2.3026) | Loss 3.5307(3.5370) | Error 0.8800(0.9006) Steps 838(831.74) | Grad Norm 2.0456(2.8491) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 20.0343(19.9930) | Bit/dim 3.5591(3.5389) | Xent 2.3026(2.3026) | Loss 3.5591(3.5389) | Error 0.8989(0.9000) Steps 844(831.14) | Grad Norm 4.1931(2.7797) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 19.8432(19.9932) | Bit/dim 3.5224(3.5383) | Xent 2.3026(2.3026) | Loss 3.5224(3.5383) | Error 0.8856(0.9008) Steps 838(831.62) | Grad Norm 3.4932(2.7543) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 20.2940(20.0141) | Bit/dim 3.5278(3.5365) | Xent 2.3026(2.3026) | Loss 3.5278(3.5365) | Error 0.8978(0.9012) Steps 838(832.78) | Grad Norm 1.8902(2.8438) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 19.6734(20.0132) | Bit/dim 3.5309(3.5334) | Xent 2.3026(2.3026) | Loss 3.5309(3.5334) | Error 0.9067(0.8997) Steps 832(833.38) | Grad Norm 1.9168(2.5364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 96.9758, Epoch Time 1212.7915(1121.5040), Bit/dim 3.5319(best: 3.5393), Xent 2.3026, Loss 3.5319, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 20.5238(20.0010) | Bit/dim 3.5409(3.5332) | Xent 2.3026(2.3026) | Loss 3.5409(3.5332) | Error 0.9000(0.9004) Steps 832(834.03) | Grad Norm 3.7222(2.6953) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 20.4366(20.0314) | Bit/dim 3.5239(3.5326) | Xent 2.3026(2.3026) | Loss 3.5239(3.5326) | Error 0.8933(0.9003) Steps 850(836.59) | Grad Norm 3.8294(2.8039) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 20.3820(20.0633) | Bit/dim 3.5292(3.5339) | Xent 2.3026(2.3026) | Loss 3.5292(3.5339) | Error 0.9011(0.8999) Steps 838(837.23) | Grad Norm 2.0857(2.7024) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 19.6000(20.0693) | Bit/dim 3.5058(3.5334) | Xent 2.3026(2.3026) | Loss 3.5058(3.5334) | Error 0.8900(0.8995) Steps 838(837.82) | Grad Norm 1.8689(2.7964) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 20.0236(20.0648) | Bit/dim 3.5501(3.5328) | Xent 2.3026(2.3026) | Loss 3.5501(3.5328) | Error 0.9144(0.9001) Steps 832(836.82) | Grad Norm 1.8123(2.6923) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 19.5278(20.0541) | Bit/dim 3.5320(3.5324) | Xent 2.3026(2.3026) | Loss 3.5320(3.5324) | Error 0.8956(0.8998) Steps 838(838.27) | Grad Norm 2.9984(2.8043) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 99.2712, Epoch Time 1221.3687(1124.5000), Bit/dim 3.5352(best: 3.5319), Xent 2.3026, Loss 3.5352, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 20.0041(20.0246) | Bit/dim 3.5298(3.5305) | Xent 2.3026(2.3026) | Loss 3.5298(3.5305) | Error 0.9022(0.9006) Steps 832(837.75) | Grad Norm 2.0584(2.7090) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 20.0316(20.0395) | Bit/dim 3.5534(3.5315) | Xent 2.3026(2.3026) | Loss 3.5534(3.5315) | Error 0.9056(0.9012) Steps 844(839.49) | Grad Norm 2.5134(2.6662) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 21.2755(20.1346) | Bit/dim 3.5173(3.5313) | Xent 2.3026(2.3026) | Loss 3.5173(3.5313) | Error 0.9022(0.9002) Steps 850(840.70) | Grad Norm 1.7143(2.7153) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 20.2097(20.1834) | Bit/dim 3.5239(3.5316) | Xent 2.3026(2.3026) | Loss 3.5239(3.5316) | Error 0.8878(0.8998) Steps 838(841.03) | Grad Norm 4.1709(2.7096) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 20.3602(20.2164) | Bit/dim 3.5283(3.5303) | Xent 2.3026(2.3026) | Loss 3.5283(3.5303) | Error 0.9067(0.9002) Steps 850(841.95) | Grad Norm 2.0006(2.6914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 98.6894, Epoch Time 1225.5815(1127.5324), Bit/dim 3.5337(best: 3.5319), Xent 2.3026, Loss 3.5337, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 20.4670(20.2105) | Bit/dim 3.5620(3.5342) | Xent 2.3026(2.3026) | Loss 3.5620(3.5342) | Error 0.9200(0.9000) Steps 856(842.69) | Grad Norm 1.8185(2.5898) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 20.3586(20.1489) | Bit/dim 3.5349(3.5321) | Xent 2.3026(2.3026) | Loss 3.5349(3.5321) | Error 0.9111(0.9005) Steps 856(842.65) | Grad Norm 2.9184(2.8124) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 20.2967(20.1663) | Bit/dim 3.5380(3.5342) | Xent 2.3026(2.3026) | Loss 3.5380(3.5342) | Error 0.8956(0.9005) Steps 856(843.34) | Grad Norm 1.7190(2.8521) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 19.8288(20.1463) | Bit/dim 3.5450(3.5308) | Xent 2.3026(2.3026) | Loss 3.5450(3.5308) | Error 0.8978(0.9002) Steps 850(844.39) | Grad Norm 2.8851(2.7057) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 20.2811(20.1966) | Bit/dim 3.5294(3.5276) | Xent 2.3026(2.3026) | Loss 3.5294(3.5276) | Error 0.8900(0.8988) Steps 850(844.70) | Grad Norm 3.8808(2.6786) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 20.1460(20.1428) | Bit/dim 3.4990(3.5256) | Xent 2.3026(2.3026) | Loss 3.4990(3.5256) | Error 0.8878(0.8994) Steps 856(845.10) | Grad Norm 2.2219(2.6341) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 97.9736, Epoch Time 1221.7029(1130.3575), Bit/dim 3.5263(best: 3.5319), Xent 2.3026, Loss 3.5263, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 20.0309(20.1547) | Bit/dim 3.5279(3.5246) | Xent 2.3026(2.3026) | Loss 3.5279(3.5246) | Error 0.9067(0.8999) Steps 850(846.04) | Grad Norm 3.3495(2.8754) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 20.4678(20.2171) | Bit/dim 3.5131(3.5230) | Xent 2.3026(2.3026) | Loss 3.5131(3.5230) | Error 0.8944(0.9008) Steps 844(845.18) | Grad Norm 2.5472(2.7314) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 19.9606(20.2751) | Bit/dim 3.5393(3.5261) | Xent 2.3026(2.3026) | Loss 3.5393(3.5261) | Error 0.8900(0.9001) Steps 844(845.31) | Grad Norm 4.1605(2.7527) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 20.2932(20.2544) | Bit/dim 3.5314(3.5270) | Xent 2.3026(2.3026) | Loss 3.5314(3.5270) | Error 0.9022(0.8997) Steps 856(844.78) | Grad Norm 2.6401(2.6472) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 20.1241(20.2791) | Bit/dim 3.5168(3.5254) | Xent 2.3026(2.3026) | Loss 3.5168(3.5254) | Error 0.8944(0.8990) Steps 844(844.43) | Grad Norm 2.1537(2.6763) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 98.3599, Epoch Time 1229.9218(1133.3445), Bit/dim 3.5270(best: 3.5263), Xent 2.3026, Loss 3.5270, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 20.3952(20.2404) | Bit/dim 3.5274(3.5242) | Xent 2.3026(2.3026) | Loss 3.5274(3.5242) | Error 0.8822(0.8994) Steps 844(844.32) | Grad Norm 3.0838(2.7525) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 19.8622(20.1551) | Bit/dim 3.5120(3.5243) | Xent 2.3026(2.3026) | Loss 3.5120(3.5243) | Error 0.8967(0.8992) Steps 844(843.98) | Grad Norm 1.8857(2.8118) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 19.5277(20.1255) | Bit/dim 3.5605(3.5264) | Xent 2.3026(2.3026) | Loss 3.5605(3.5264) | Error 0.9133(0.8998) Steps 838(844.30) | Grad Norm 2.4387(2.6169) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 20.0808(20.1348) | Bit/dim 3.5324(3.5265) | Xent 2.3026(2.3026) | Loss 3.5324(3.5265) | Error 0.9033(0.9006) Steps 850(845.07) | Grad Norm 3.8438(2.8510) | Total Time 14.00(14.00)\n",
      "Iter 4390 | Time 20.5017(20.1474) | Bit/dim 3.5423(3.5265) | Xent 2.3026(2.3026) | Loss 3.5423(3.5265) | Error 0.8933(0.9002) Steps 838(845.28) | Grad Norm 2.4216(2.6961) | Total Time 14.00(14.00)\n",
      "Iter 4400 | Time 20.3733(20.1680) | Bit/dim 3.4663(3.5233) | Xent 2.3026(2.3026) | Loss 3.4663(3.5233) | Error 0.8811(0.8995) Steps 850(845.15) | Grad Norm 1.6995(2.7395) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 99.0403, Epoch Time 1222.8831(1136.0306), Bit/dim 3.5224(best: 3.5263), Xent 2.3026, Loss 3.5224, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 20.4549(20.1847) | Bit/dim 3.5159(3.5237) | Xent 2.3026(2.3026) | Loss 3.5159(3.5237) | Error 0.8978(0.9001) Steps 856(846.29) | Grad Norm 3.2322(2.7334) | Total Time 14.00(14.00)\n",
      "Iter 4420 | Time 20.1269(20.1923) | Bit/dim 3.5324(3.5214) | Xent 2.3026(2.3026) | Loss 3.5324(3.5214) | Error 0.9100(0.8996) Steps 844(846.22) | Grad Norm 1.6361(2.7115) | Total Time 14.00(14.00)\n",
      "Iter 4430 | Time 19.8974(20.2089) | Bit/dim 3.5535(3.5199) | Xent 2.3026(2.3026) | Loss 3.5535(3.5199) | Error 0.9033(0.9007) Steps 850(846.62) | Grad Norm 3.1120(2.5515) | Total Time 14.00(14.00)\n",
      "Iter 4440 | Time 19.9100(20.2123) | Bit/dim 3.5214(3.5212) | Xent 2.3026(2.3026) | Loss 3.5214(3.5212) | Error 0.9011(0.9006) Steps 850(845.91) | Grad Norm 2.4473(2.5978) | Total Time 14.00(14.00)\n",
      "Iter 4450 | Time 19.8196(20.2661) | Bit/dim 3.5187(3.5214) | Xent 2.3026(2.3026) | Loss 3.5187(3.5214) | Error 0.9011(0.8994) Steps 850(845.76) | Grad Norm 4.0787(2.6421) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 99.4983, Epoch Time 1231.8118(1138.9041), Bit/dim 3.5228(best: 3.5224), Xent 2.3026, Loss 3.5228, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 20.1999(20.2703) | Bit/dim 3.5143(3.5198) | Xent 2.3026(2.3026) | Loss 3.5143(3.5198) | Error 0.8911(0.8989) Steps 844(847.01) | Grad Norm 3.1696(2.6875) | Total Time 14.00(14.00)\n",
      "Iter 4470 | Time 20.6117(20.2066) | Bit/dim 3.5322(3.5203) | Xent 2.3026(2.3026) | Loss 3.5322(3.5203) | Error 0.9222(0.8995) Steps 856(846.84) | Grad Norm 2.3195(2.6345) | Total Time 14.00(14.00)\n",
      "Iter 4480 | Time 20.0863(20.2307) | Bit/dim 3.4941(3.5206) | Xent 2.3026(2.3026) | Loss 3.4941(3.5206) | Error 0.9111(0.8993) Steps 832(845.90) | Grad Norm 1.9408(2.7059) | Total Time 14.00(14.00)\n",
      "Iter 4490 | Time 20.5240(20.2697) | Bit/dim 3.5245(3.5214) | Xent 2.3026(2.3026) | Loss 3.5245(3.5214) | Error 0.9033(0.9000) Steps 844(845.40) | Grad Norm 3.2407(2.7435) | Total Time 14.00(14.00)\n",
      "Iter 4500 | Time 19.6722(20.2879) | Bit/dim 3.4982(3.5213) | Xent 2.3026(2.3026) | Loss 3.4982(3.5213) | Error 0.8889(0.9002) Steps 838(844.64) | Grad Norm 4.2074(2.8430) | Total Time 14.00(14.00)\n",
      "Iter 4510 | Time 20.2042(20.3216) | Bit/dim 3.5521(3.5195) | Xent 2.3026(2.3026) | Loss 3.5521(3.5195) | Error 0.8978(0.9005) Steps 838(844.86) | Grad Norm 2.3926(2.7410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 98.5101, Epoch Time 1231.0718(1141.6691), Bit/dim 3.5182(best: 3.5224), Xent 2.3026, Loss 3.5182, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 19.2440(20.2483) | Bit/dim 3.5205(3.5186) | Xent 2.3026(2.3026) | Loss 3.5205(3.5186) | Error 0.8844(0.9000) Steps 844(846.30) | Grad Norm 3.2245(2.8282) | Total Time 14.00(14.00)\n",
      "Iter 4530 | Time 21.0610(20.2389) | Bit/dim 3.4839(3.5166) | Xent 2.3026(2.3026) | Loss 3.4839(3.5166) | Error 0.9011(0.8998) Steps 862(846.36) | Grad Norm 2.3271(2.7889) | Total Time 14.00(14.00)\n",
      "Iter 4540 | Time 19.7689(20.2350) | Bit/dim 3.5063(3.5184) | Xent 2.3026(2.3026) | Loss 3.5063(3.5184) | Error 0.8933(0.8996) Steps 838(847.13) | Grad Norm 2.0689(2.5793) | Total Time 14.00(14.00)\n",
      "Iter 4550 | Time 19.7631(20.2713) | Bit/dim 3.5141(3.5170) | Xent 2.3026(2.3026) | Loss 3.5141(3.5170) | Error 0.9089(0.8996) Steps 844(848.22) | Grad Norm 1.7238(2.5107) | Total Time 14.00(14.00)\n",
      "Iter 4560 | Time 21.2025(20.2928) | Bit/dim 3.5137(3.5186) | Xent 2.3026(2.3026) | Loss 3.5137(3.5186) | Error 0.8922(0.9008) Steps 856(848.45) | Grad Norm 1.4702(2.5894) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 99.2883, Epoch Time 1229.4853(1144.3036), Bit/dim 3.5181(best: 3.5182), Xent 2.3026, Loss 3.5181, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 20.9088(20.3517) | Bit/dim 3.4976(3.5168) | Xent 2.3026(2.3026) | Loss 3.4976(3.5168) | Error 0.9000(0.9014) Steps 850(846.97) | Grad Norm 2.4302(2.7713) | Total Time 14.00(14.00)\n",
      "Iter 4580 | Time 19.7619(20.3058) | Bit/dim 3.4900(3.5173) | Xent 2.3026(2.3026) | Loss 3.4900(3.5173) | Error 0.9067(0.9006) Steps 844(847.14) | Grad Norm 2.5736(2.6335) | Total Time 14.00(14.00)\n",
      "Iter 4590 | Time 19.7969(20.3327) | Bit/dim 3.5099(3.5183) | Xent 2.3026(2.3026) | Loss 3.5099(3.5183) | Error 0.9000(0.8994) Steps 850(846.13) | Grad Norm 1.8415(2.7697) | Total Time 14.00(14.00)\n",
      "Iter 4600 | Time 20.5117(20.3898) | Bit/dim 3.5035(3.5189) | Xent 2.3026(2.3026) | Loss 3.5035(3.5189) | Error 0.9111(0.9003) Steps 886(848.73) | Grad Norm 1.3643(2.5343) | Total Time 14.00(14.00)\n",
      "Iter 4610 | Time 20.9927(20.3530) | Bit/dim 3.5127(3.5164) | Xent 2.3026(2.3026) | Loss 3.5127(3.5164) | Error 0.9111(0.9008) Steps 868(848.19) | Grad Norm 2.1359(2.3764) | Total Time 14.00(14.00)\n",
      "Iter 4620 | Time 20.7751(20.4304) | Bit/dim 3.5080(3.5166) | Xent 2.3026(2.3026) | Loss 3.5080(3.5166) | Error 0.9067(0.9007) Steps 862(850.85) | Grad Norm 2.9015(2.6558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 98.3867, Epoch Time 1237.7375(1147.1066), Bit/dim 3.5176(best: 3.5181), Xent 2.3026, Loss 3.5176, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 21.2390(20.4265) | Bit/dim 3.5274(3.5163) | Xent 2.3026(2.3026) | Loss 3.5274(3.5163) | Error 0.8978(0.9001) Steps 838(851.42) | Grad Norm 3.5203(2.6191) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 20.3289(20.4695) | Bit/dim 3.4781(3.5148) | Xent 2.3026(2.3026) | Loss 3.4781(3.5148) | Error 0.8878(0.9004) Steps 862(853.64) | Grad Norm 4.6504(2.8288) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 19.7558(20.3720) | Bit/dim 3.5031(3.5125) | Xent 2.3026(2.3026) | Loss 3.5031(3.5125) | Error 0.8911(0.9000) Steps 850(852.74) | Grad Norm 1.9808(2.6766) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 19.6152(20.2858) | Bit/dim 3.5212(3.5143) | Xent 2.3026(2.3026) | Loss 3.5212(3.5143) | Error 0.8978(0.8999) Steps 844(851.96) | Grad Norm 2.6121(2.6198) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 20.7684(20.3770) | Bit/dim 3.5002(3.5146) | Xent 2.3026(2.3026) | Loss 3.5002(3.5146) | Error 0.8922(0.9000) Steps 874(854.84) | Grad Norm 2.6541(2.6415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 98.8092, Epoch Time 1237.0066(1149.8036), Bit/dim 3.5214(best: 3.5176), Xent 2.3026, Loss 3.5214, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 20.3694(20.4242) | Bit/dim 3.5290(3.5143) | Xent 2.3026(2.3026) | Loss 3.5290(3.5143) | Error 0.9067(0.9003) Steps 874(856.14) | Grad Norm 1.3663(2.7636) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 19.7161(20.4217) | Bit/dim 3.5182(3.5134) | Xent 2.3026(2.3026) | Loss 3.5182(3.5134) | Error 0.9067(0.8998) Steps 862(856.56) | Grad Norm 3.0802(2.7648) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 19.8789(20.4218) | Bit/dim 3.5623(3.5138) | Xent 2.3026(2.3026) | Loss 3.5623(3.5138) | Error 0.9078(0.9018) Steps 862(857.53) | Grad Norm 2.8851(2.6357) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 20.2446(20.4837) | Bit/dim 3.5323(3.5134) | Xent 2.3026(2.3026) | Loss 3.5323(3.5134) | Error 0.8844(0.9003) Steps 868(860.96) | Grad Norm 2.8353(2.7549) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 20.3295(20.5010) | Bit/dim 3.4552(3.5110) | Xent 2.3026(2.3026) | Loss 3.4552(3.5110) | Error 0.8900(0.9004) Steps 874(861.36) | Grad Norm 3.9771(2.7916) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 21.0928(20.5669) | Bit/dim 3.5210(3.5105) | Xent 2.3026(2.3026) | Loss 3.5210(3.5105) | Error 0.8878(0.8993) Steps 874(861.44) | Grad Norm 2.1421(2.7392) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 99.5324, Epoch Time 1245.3753(1152.6708), Bit/dim 3.5115(best: 3.5176), Xent 2.3026, Loss 3.5115, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.5631(20.6208) | Bit/dim 3.5472(3.5110) | Xent 2.3026(2.3026) | Loss 3.5472(3.5110) | Error 0.9200(0.8994) Steps 862(863.45) | Grad Norm 2.6673(2.7802) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 20.6693(20.6829) | Bit/dim 3.4926(3.5101) | Xent 2.3026(2.3026) | Loss 3.4926(3.5101) | Error 0.8889(0.8995) Steps 862(865.73) | Grad Norm 2.0257(2.6926) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 20.8549(20.7360) | Bit/dim 3.4706(3.5092) | Xent 2.3026(2.3026) | Loss 3.4706(3.5092) | Error 0.9067(0.9009) Steps 856(863.46) | Grad Norm 2.0438(2.6639) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 19.9469(20.7126) | Bit/dim 3.5137(3.5091) | Xent 2.3026(2.3026) | Loss 3.5137(3.5091) | Error 0.9000(0.9002) Steps 856(863.88) | Grad Norm 3.0409(2.7715) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 20.2514(20.6972) | Bit/dim 3.5239(3.5101) | Xent 2.3026(2.3026) | Loss 3.5239(3.5101) | Error 0.8922(0.9002) Steps 880(862.57) | Grad Norm 2.4869(2.7113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 99.0765, Epoch Time 1258.7031(1155.8517), Bit/dim 3.5151(best: 3.5115), Xent 2.3026, Loss 3.5151, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 20.1284(20.7081) | Bit/dim 3.5467(3.5131) | Xent 2.3026(2.3026) | Loss 3.5467(3.5131) | Error 0.9044(0.8990) Steps 838(862.63) | Grad Norm 1.8689(2.7617) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 20.4662(20.6588) | Bit/dim 3.4920(3.5118) | Xent 2.3026(2.3026) | Loss 3.4920(3.5118) | Error 0.8989(0.8999) Steps 868(862.74) | Grad Norm 2.4526(2.6468) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 20.5226(20.7292) | Bit/dim 3.4847(3.5094) | Xent 2.3026(2.3026) | Loss 3.4847(3.5094) | Error 0.8767(0.8997) Steps 868(864.60) | Grad Norm 3.2842(2.7528) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 20.7282(20.7147) | Bit/dim 3.5169(3.5085) | Xent 2.3026(2.3026) | Loss 3.5169(3.5085) | Error 0.8911(0.8997) Steps 856(864.27) | Grad Norm 2.3512(2.6926) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 20.1209(20.7122) | Bit/dim 3.5074(3.5081) | Xent 2.3026(2.3026) | Loss 3.5074(3.5081) | Error 0.9122(0.8982) Steps 850(865.47) | Grad Norm 3.4711(2.5292) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 20.2036(20.5910) | Bit/dim 3.5018(3.5056) | Xent 2.3026(2.3026) | Loss 3.5018(3.5056) | Error 0.9022(0.9003) Steps 868(863.52) | Grad Norm 1.6760(2.6562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 98.9407, Epoch Time 1249.0025(1158.6462), Bit/dim 3.5068(best: 3.5115), Xent 2.3026, Loss 3.5068, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 20.8456(20.5674) | Bit/dim 3.5476(3.5059) | Xent 2.3026(2.3026) | Loss 3.5476(3.5059) | Error 0.9178(0.9002) Steps 856(862.75) | Grad Norm 1.7205(2.4056) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 21.1686(20.7211) | Bit/dim 3.4605(3.5041) | Xent 2.3026(2.3026) | Loss 3.4605(3.5041) | Error 0.8822(0.8978) Steps 880(864.97) | Grad Norm 4.0237(2.6773) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 20.6416(20.7198) | Bit/dim 3.4835(3.5026) | Xent 2.3026(2.3026) | Loss 3.4835(3.5026) | Error 0.9089(0.8980) Steps 850(864.65) | Grad Norm 2.3104(2.5603) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 20.8230(20.7255) | Bit/dim 3.5230(3.5041) | Xent 2.3026(2.3026) | Loss 3.5230(3.5041) | Error 0.9144(0.8997) Steps 862(863.95) | Grad Norm 1.7162(2.6751) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 20.8355(20.6666) | Bit/dim 3.5080(3.5041) | Xent 2.3026(2.3026) | Loss 3.5080(3.5041) | Error 0.8922(0.9007) Steps 844(861.97) | Grad Norm 2.8635(2.5709) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 98.0780, Epoch Time 1252.8964(1161.4737), Bit/dim 3.5054(best: 3.5068), Xent 2.3026, Loss 3.5054, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 20.8015(20.6444) | Bit/dim 3.5129(3.5062) | Xent 2.3026(2.3026) | Loss 3.5129(3.5062) | Error 0.8922(0.9010) Steps 868(863.70) | Grad Norm 3.1567(2.6244) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 21.2836(20.6685) | Bit/dim 3.5116(3.5042) | Xent 2.3026(2.3026) | Loss 3.5116(3.5042) | Error 0.8789(0.8985) Steps 874(863.52) | Grad Norm 2.2971(2.6428) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 20.7484(20.6973) | Bit/dim 3.4746(3.5051) | Xent 2.3026(2.3026) | Loss 3.4746(3.5051) | Error 0.9056(0.9012) Steps 874(863.98) | Grad Norm 3.1879(2.7936) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 19.9504(20.6299) | Bit/dim 3.4981(3.5029) | Xent 2.3026(2.3026) | Loss 3.4981(3.5029) | Error 0.9078(0.9017) Steps 850(862.94) | Grad Norm 2.1135(2.6760) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 20.5378(20.6431) | Bit/dim 3.5111(3.5029) | Xent 2.3026(2.3026) | Loss 3.5111(3.5029) | Error 0.8844(0.8995) Steps 856(865.33) | Grad Norm 2.8459(2.7476) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 20.2536(20.6215) | Bit/dim 3.4770(3.5040) | Xent 2.3026(2.3026) | Loss 3.4770(3.5040) | Error 0.9011(0.9006) Steps 868(864.07) | Grad Norm 2.6615(2.6756) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 97.8853, Epoch Time 1250.0498(1164.1310), Bit/dim 3.5093(best: 3.5054), Xent 2.3026, Loss 3.5093, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 20.4098(20.5462) | Bit/dim 3.5312(3.5032) | Xent 2.3026(2.3026) | Loss 3.5312(3.5032) | Error 0.9078(0.9004) Steps 862(863.79) | Grad Norm 3.6668(2.6777) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 20.8050(20.5681) | Bit/dim 3.4941(3.5024) | Xent 2.3026(2.3026) | Loss 3.4941(3.5024) | Error 0.9000(0.8988) Steps 874(866.18) | Grad Norm 2.1120(2.6200) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 20.8597(20.6082) | Bit/dim 3.4842(3.5016) | Xent 2.3026(2.3026) | Loss 3.4842(3.5016) | Error 0.8922(0.9003) Steps 862(866.81) | Grad Norm 1.9558(2.7068) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 21.0784(20.6658) | Bit/dim 3.5032(3.5002) | Xent 2.3026(2.3026) | Loss 3.5032(3.5002) | Error 0.8844(0.8993) Steps 856(866.74) | Grad Norm 1.7117(2.5937) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 21.0565(20.7095) | Bit/dim 3.5457(3.5024) | Xent 2.3026(2.3026) | Loss 3.5457(3.5024) | Error 0.9056(0.9006) Steps 868(868.24) | Grad Norm 3.2113(2.6673) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 99.9735, Epoch Time 1252.3331(1166.7771), Bit/dim 3.5022(best: 3.5054), Xent 2.3026, Loss 3.5022, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 21.3845(20.7135) | Bit/dim 3.5113(3.5000) | Xent 2.3026(2.3026) | Loss 3.5113(3.5000) | Error 0.9122(0.9013) Steps 874(869.59) | Grad Norm 2.9520(2.7182) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 21.0545(20.7625) | Bit/dim 3.4881(3.4992) | Xent 2.3026(2.3026) | Loss 3.4881(3.4992) | Error 0.9133(0.9011) Steps 874(870.70) | Grad Norm 2.2202(2.6538) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 20.5097(20.7947) | Bit/dim 3.4911(3.4990) | Xent 2.3026(2.3026) | Loss 3.4911(3.4990) | Error 0.8967(0.9003) Steps 874(870.51) | Grad Norm 2.2050(2.6107) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 20.9652(20.7416) | Bit/dim 3.4836(3.5005) | Xent 2.3026(2.3026) | Loss 3.4836(3.5005) | Error 0.9067(0.9006) Steps 868(870.48) | Grad Norm 2.9485(2.6171) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 20.3382(20.7474) | Bit/dim 3.5011(3.4997) | Xent 2.3026(2.3026) | Loss 3.5011(3.4997) | Error 0.9044(0.9005) Steps 856(868.86) | Grad Norm 2.4230(2.6567) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 20.1989(20.7333) | Bit/dim 3.5118(3.4989) | Xent 2.3026(2.3026) | Loss 3.5118(3.4989) | Error 0.8789(0.8994) Steps 868(867.80) | Grad Norm 2.8368(2.5880) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 100.0766, Epoch Time 1259.0774(1169.5461), Bit/dim 3.5047(best: 3.5022), Xent 2.3026, Loss 3.5047, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 21.2458(20.7489) | Bit/dim 3.4777(3.4990) | Xent 2.3026(2.3026) | Loss 3.4777(3.4990) | Error 0.9022(0.8998) Steps 886(868.74) | Grad Norm 2.0324(2.6664) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 21.2526(20.7906) | Bit/dim 3.5102(3.4978) | Xent 2.3026(2.3026) | Loss 3.5102(3.4978) | Error 0.8967(0.9000) Steps 874(870.33) | Grad Norm 1.9818(2.4964) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 21.0177(20.8020) | Bit/dim 3.4636(3.4981) | Xent 2.3026(2.3026) | Loss 3.4636(3.4981) | Error 0.8844(0.8997) Steps 868(869.38) | Grad Norm 3.5035(2.5114) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 20.6713(20.8639) | Bit/dim 3.5283(3.5000) | Xent 2.3026(2.3026) | Loss 3.5283(3.5000) | Error 0.9000(0.9002) Steps 862(869.40) | Grad Norm 3.0338(2.6306) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 20.4931(20.7820) | Bit/dim 3.5023(3.4999) | Xent 2.3026(2.3026) | Loss 3.5023(3.4999) | Error 0.8867(0.8990) Steps 874(867.66) | Grad Norm 2.5565(2.5419) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 98.6423, Epoch Time 1258.0276(1172.2005), Bit/dim 3.4983(best: 3.5022), Xent 2.3026, Loss 3.4983, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 20.3010(20.6870) | Bit/dim 3.4735(3.4961) | Xent 2.3026(2.3026) | Loss 3.4735(3.4961) | Error 0.8922(0.8995) Steps 844(864.57) | Grad Norm 4.3473(2.6365) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 20.7789(20.6897) | Bit/dim 3.4851(3.4964) | Xent 2.3026(2.3026) | Loss 3.4851(3.4964) | Error 0.9189(0.8999) Steps 874(864.31) | Grad Norm 2.5383(2.7163) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 20.4704(20.5990) | Bit/dim 3.4525(3.4953) | Xent 2.3026(2.3026) | Loss 3.4525(3.4953) | Error 0.8856(0.8992) Steps 856(863.44) | Grad Norm 2.1056(2.4946) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 20.8034(20.5863) | Bit/dim 3.5007(3.4966) | Xent 2.3026(2.3026) | Loss 3.5007(3.4966) | Error 0.9156(0.8998) Steps 856(863.83) | Grad Norm 1.8032(2.6322) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 20.8079(20.5761) | Bit/dim 3.5147(3.4960) | Xent 2.3026(2.3026) | Loss 3.5147(3.4960) | Error 0.8967(0.8992) Steps 862(862.73) | Grad Norm 2.1244(2.5082) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 20.9564(20.6082) | Bit/dim 3.4531(3.4979) | Xent 2.3026(2.3026) | Loss 3.4531(3.4979) | Error 0.8967(0.8998) Steps 868(862.77) | Grad Norm 1.8948(2.6016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 98.2221, Epoch Time 1244.4090(1174.3668), Bit/dim 3.4972(best: 3.4983), Xent 2.3026, Loss 3.4972, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 21.1154(20.6755) | Bit/dim 3.5135(3.4986) | Xent 2.3026(2.3026) | Loss 3.5135(3.4986) | Error 0.8889(0.8994) Steps 868(864.51) | Grad Norm 2.0709(2.5133) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 20.9066(20.6808) | Bit/dim 3.4925(3.4985) | Xent 2.3026(2.3026) | Loss 3.4925(3.4985) | Error 0.8911(0.9012) Steps 880(865.34) | Grad Norm 3.0757(2.5663) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 21.6391(20.6842) | Bit/dim 3.5067(3.4974) | Xent 2.3026(2.3026) | Loss 3.5067(3.4974) | Error 0.8889(0.9013) Steps 886(866.46) | Grad Norm 2.3409(2.5720) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 20.3236(20.6664) | Bit/dim 3.5088(3.4955) | Xent 2.3026(2.3026) | Loss 3.5088(3.4955) | Error 0.8900(0.9003) Steps 868(865.87) | Grad Norm 3.8727(2.6829) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 20.1641(20.6102) | Bit/dim 3.5014(3.4958) | Xent 2.3026(2.3026) | Loss 3.5014(3.4958) | Error 0.8978(0.9004) Steps 868(865.44) | Grad Norm 2.3221(2.7067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 98.8940, Epoch Time 1249.6853(1176.6264), Bit/dim 3.4957(best: 3.4972), Xent 2.3026, Loss 3.4957, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.8266(20.5896) | Bit/dim 3.4703(3.4943) | Xent 2.3026(2.3026) | Loss 3.4703(3.4943) | Error 0.9033(0.8992) Steps 880(864.70) | Grad Norm 2.1824(2.5509) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 20.9889(20.5731) | Bit/dim 3.4973(3.4961) | Xent 2.3026(2.3026) | Loss 3.4973(3.4961) | Error 0.8922(0.8993) Steps 868(864.07) | Grad Norm 2.0316(2.4277) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 19.9799(20.5765) | Bit/dim 3.5209(3.4953) | Xent 2.3026(2.3026) | Loss 3.5209(3.4953) | Error 0.8944(0.8999) Steps 862(864.82) | Grad Norm 2.4050(2.5297) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 20.5103(20.5649) | Bit/dim 3.4877(3.4941) | Xent 2.3026(2.3026) | Loss 3.4877(3.4941) | Error 0.9244(0.9008) Steps 850(863.41) | Grad Norm 2.4102(2.4785) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 20.9582(20.6121) | Bit/dim 3.4576(3.4929) | Xent 2.3026(2.3026) | Loss 3.4576(3.4929) | Error 0.8956(0.8995) Steps 856(864.42) | Grad Norm 2.0580(2.6696) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 20.1443(20.5825) | Bit/dim 3.4749(3.4913) | Xent 2.3026(2.3026) | Loss 3.4749(3.4913) | Error 0.8967(0.8998) Steps 868(865.92) | Grad Norm 1.8819(2.5303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 98.6850, Epoch Time 1247.3658(1178.7485), Bit/dim 3.4943(best: 3.4957), Xent 2.3026, Loss 3.4943, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 20.5503(20.5289) | Bit/dim 3.5031(3.4937) | Xent 2.3026(2.3026) | Loss 3.5031(3.4937) | Error 0.9156(0.8997) Steps 862(864.66) | Grad Norm 2.0980(2.5899) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 20.3827(20.5815) | Bit/dim 3.4918(3.4918) | Xent 2.3026(2.3026) | Loss 3.4918(3.4918) | Error 0.9089(0.9000) Steps 862(864.32) | Grad Norm 2.3327(2.7130) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 20.9348(20.6270) | Bit/dim 3.4634(3.4919) | Xent 2.3026(2.3026) | Loss 3.4634(3.4919) | Error 0.8933(0.9002) Steps 874(864.72) | Grad Norm 1.9458(2.7171) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 20.3819(20.5443) | Bit/dim 3.4638(3.4912) | Xent 2.3026(2.3026) | Loss 3.4638(3.4912) | Error 0.8911(0.8995) Steps 856(863.62) | Grad Norm 2.3175(2.5625) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 20.8933(20.5786) | Bit/dim 3.5042(3.4908) | Xent 2.3026(2.3026) | Loss 3.5042(3.4908) | Error 0.9033(0.8999) Steps 856(862.84) | Grad Norm 2.1888(2.5775) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 99.3484, Epoch Time 1247.5921(1180.8138), Bit/dim 3.4889(best: 3.4943), Xent 2.3026, Loss 3.4889, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 21.0483(20.5758) | Bit/dim 3.4676(3.4890) | Xent 2.3026(2.3026) | Loss 3.4676(3.4890) | Error 0.9056(0.9002) Steps 862(864.67) | Grad Norm 3.4745(2.6862) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 20.1412(20.5804) | Bit/dim 3.5084(3.4886) | Xent 2.3026(2.3026) | Loss 3.5084(3.4886) | Error 0.9100(0.8995) Steps 892(866.94) | Grad Norm 2.8807(2.6454) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 20.9165(20.5917) | Bit/dim 3.5165(3.4902) | Xent 2.3026(2.3026) | Loss 3.5165(3.4902) | Error 0.9144(0.9005) Steps 850(865.42) | Grad Norm 2.9619(2.6009) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 19.9082(20.5242) | Bit/dim 3.5146(3.4918) | Xent 2.3026(2.3026) | Loss 3.5146(3.4918) | Error 0.9056(0.9007) Steps 862(863.62) | Grad Norm 3.2166(2.6371) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 20.6333(20.4884) | Bit/dim 3.5071(3.4907) | Xent 2.3026(2.3026) | Loss 3.5071(3.4907) | Error 0.9089(0.9003) Steps 874(863.69) | Grad Norm 2.9782(2.6858) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 20.5805(20.4666) | Bit/dim 3.4685(3.4897) | Xent 2.3026(2.3026) | Loss 3.4685(3.4897) | Error 0.9089(0.9003) Steps 850(862.55) | Grad Norm 1.9552(2.5347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 98.5892, Epoch Time 1240.5202(1182.6050), Bit/dim 3.4887(best: 3.4889), Xent 2.3026, Loss 3.4887, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 20.2652(20.4928) | Bit/dim 3.4828(3.4894) | Xent 2.3026(2.3026) | Loss 3.4828(3.4894) | Error 0.9000(0.9003) Steps 868(864.83) | Grad Norm 1.8548(2.6206) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 19.9516(20.5055) | Bit/dim 3.4824(3.4890) | Xent 2.3026(2.3026) | Loss 3.4824(3.4890) | Error 0.8933(0.9001) Steps 856(864.41) | Grad Norm 2.7979(2.5933) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 20.0132(20.5057) | Bit/dim 3.4757(3.4866) | Xent 2.3026(2.3026) | Loss 3.4757(3.4866) | Error 0.9022(0.8988) Steps 868(866.77) | Grad Norm 2.2792(2.6066) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 20.2515(20.5088) | Bit/dim 3.4668(3.4876) | Xent 2.3026(2.3026) | Loss 3.4668(3.4876) | Error 0.9033(0.9007) Steps 856(866.04) | Grad Norm 3.0167(2.6571) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 20.8138(20.4809) | Bit/dim 3.4442(3.4861) | Xent 2.3026(2.3026) | Loss 3.4442(3.4861) | Error 0.9044(0.9003) Steps 862(863.98) | Grad Norm 1.8657(2.4942) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 99.1620, Epoch Time 1242.0879(1184.3895), Bit/dim 3.4910(best: 3.4887), Xent 2.3026, Loss 3.4910, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 20.5813(20.5055) | Bit/dim 3.5117(3.4904) | Xent 2.3026(2.3026) | Loss 3.5117(3.4904) | Error 0.9156(0.9006) Steps 886(863.34) | Grad Norm 2.5182(2.4917) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 21.1689(20.5237) | Bit/dim 3.5044(3.4886) | Xent 2.3026(2.3026) | Loss 3.5044(3.4886) | Error 0.9189(0.8995) Steps 874(862.14) | Grad Norm 2.1018(2.4896) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 19.9728(20.5371) | Bit/dim 3.4958(3.4895) | Xent 2.3026(2.3026) | Loss 3.4958(3.4895) | Error 0.9011(0.8997) Steps 856(861.36) | Grad Norm 2.7576(2.4892) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 20.6164(20.5777) | Bit/dim 3.4584(3.4882) | Xent 2.3026(2.3026) | Loss 3.4584(3.4882) | Error 0.9056(0.9000) Steps 880(862.16) | Grad Norm 1.9231(2.4738) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 21.0009(20.6139) | Bit/dim 3.4915(3.4835) | Xent 2.3026(2.3026) | Loss 3.4915(3.4835) | Error 0.8789(0.8994) Steps 874(862.44) | Grad Norm 2.2117(2.4557) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 20.9302(20.6713) | Bit/dim 3.4657(3.4844) | Xent 2.3026(2.3026) | Loss 3.4657(3.4844) | Error 0.9056(0.9006) Steps 874(863.85) | Grad Norm 2.4015(2.3690) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 100.3027, Epoch Time 1254.7051(1186.4990), Bit/dim 3.4887(best: 3.4887), Xent 2.3026, Loss 3.4887, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 20.6511(20.6103) | Bit/dim 3.4827(3.4857) | Xent 2.3026(2.3026) | Loss 3.4827(3.4857) | Error 0.8900(0.9004) Steps 868(863.54) | Grad Norm 3.1394(2.5730) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 20.5357(20.5871) | Bit/dim 3.4709(3.4856) | Xent 2.3026(2.3026) | Loss 3.4709(3.4856) | Error 0.9067(0.9015) Steps 844(863.68) | Grad Norm 1.7055(2.4182) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 21.1321(20.5860) | Bit/dim 3.5031(3.4860) | Xent 2.3026(2.3026) | Loss 3.5031(3.4860) | Error 0.9044(0.9019) Steps 886(865.62) | Grad Norm 2.7288(2.6599) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 20.4652(20.6000) | Bit/dim 3.4630(3.4845) | Xent 2.3026(2.3026) | Loss 3.4630(3.4845) | Error 0.9033(0.9005) Steps 862(866.46) | Grad Norm 2.8764(2.6627) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 20.1994(20.5482) | Bit/dim 3.5197(3.4846) | Xent 2.3026(2.3026) | Loss 3.5197(3.4846) | Error 0.8811(0.8996) Steps 862(864.05) | Grad Norm 2.5183(2.5865) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 99.5568, Epoch Time 1243.7472(1188.2164), Bit/dim 3.4844(best: 3.4887), Xent 2.3026, Loss 3.4844, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 20.9692(20.5174) | Bit/dim 3.4951(3.4853) | Xent 2.3026(2.3026) | Loss 3.4951(3.4853) | Error 0.8944(0.8991) Steps 862(862.70) | Grad Norm 2.0252(2.4947) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 21.3814(20.5925) | Bit/dim 3.4523(3.4833) | Xent 2.3026(2.3026) | Loss 3.4523(3.4833) | Error 0.8756(0.8985) Steps 862(865.52) | Grad Norm 2.4269(2.5163) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 20.3479(20.4706) | Bit/dim 3.4806(3.4839) | Xent 2.3026(2.3026) | Loss 3.4806(3.4839) | Error 0.9233(0.9001) Steps 856(864.96) | Grad Norm 2.2545(2.5233) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 21.0451(20.4015) | Bit/dim 3.4936(3.4859) | Xent 2.3026(2.3026) | Loss 3.4936(3.4859) | Error 0.8889(0.9002) Steps 880(863.46) | Grad Norm 2.5034(2.5412) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 21.5805(20.3512) | Bit/dim 3.4507(3.4843) | Xent 2.3026(2.3026) | Loss 3.4507(3.4843) | Error 0.9000(0.8994) Steps 886(864.06) | Grad Norm 3.4294(2.6143) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 20.2740(20.3733) | Bit/dim 3.5048(3.4820) | Xent 2.3026(2.3026) | Loss 3.5048(3.4820) | Error 0.9056(0.8998) Steps 850(863.99) | Grad Norm 1.5253(2.4560) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 97.8770, Epoch Time 1233.9355(1189.5880), Bit/dim 3.4834(best: 3.4844), Xent 2.3026, Loss 3.4834, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 19.5757(20.2469) | Bit/dim 3.4511(3.4827) | Xent 2.3026(2.3026) | Loss 3.4511(3.4827) | Error 0.8733(0.9002) Steps 856(860.75) | Grad Norm 1.6422(2.3217) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 20.6539(20.3377) | Bit/dim 3.4445(3.4819) | Xent 2.3026(2.3026) | Loss 3.4445(3.4819) | Error 0.8911(0.8993) Steps 874(861.71) | Grad Norm 1.9010(2.4214) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 20.4083(20.3634) | Bit/dim 3.4976(3.4824) | Xent 2.3026(2.3026) | Loss 3.4976(3.4824) | Error 0.9111(0.8997) Steps 886(862.86) | Grad Norm 3.4970(2.5184) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 20.1321(20.3845) | Bit/dim 3.4712(3.4811) | Xent 2.3026(2.3026) | Loss 3.4712(3.4811) | Error 0.9000(0.8984) Steps 862(863.18) | Grad Norm 1.5777(2.3400) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 20.6262(20.3883) | Bit/dim 3.4477(3.4811) | Xent 2.3026(2.3026) | Loss 3.4477(3.4811) | Error 0.9000(0.8990) Steps 862(863.30) | Grad Norm 2.9512(2.4009) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 100.8043, Epoch Time 1235.9350(1190.9784), Bit/dim 3.4863(best: 3.4834), Xent 2.3026, Loss 3.4863, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 21.3354(20.3824) | Bit/dim 3.4423(3.4811) | Xent 2.3026(2.3026) | Loss 3.4423(3.4811) | Error 0.8856(0.8991) Steps 856(863.92) | Grad Norm 3.1996(2.3924) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 19.8740(20.2740) | Bit/dim 3.4943(3.4840) | Xent 2.3026(2.3026) | Loss 3.4943(3.4840) | Error 0.8978(0.9001) Steps 838(860.64) | Grad Norm 3.4635(2.5453) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 20.3913(20.4016) | Bit/dim 3.4955(3.4830) | Xent 2.3026(2.3026) | Loss 3.4955(3.4830) | Error 0.8978(0.9006) Steps 856(862.05) | Grad Norm 2.1625(2.5406) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 21.2245(20.4248) | Bit/dim 3.4783(3.4800) | Xent 2.3026(2.3026) | Loss 3.4783(3.4800) | Error 0.8967(0.8995) Steps 862(861.61) | Grad Norm 2.2001(2.6069) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 20.3159(20.4316) | Bit/dim 3.4863(3.4799) | Xent 2.3026(2.3026) | Loss 3.4863(3.4799) | Error 0.9078(0.9002) Steps 868(862.81) | Grad Norm 2.6642(2.5316) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 21.3091(20.4781) | Bit/dim 3.4941(3.4803) | Xent 2.3026(2.3026) | Loss 3.4941(3.4803) | Error 0.8944(0.9003) Steps 838(860.56) | Grad Norm 2.6520(2.5892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 99.8027, Epoch Time 1240.4073(1192.4613), Bit/dim 3.4809(best: 3.4834), Xent 2.3026, Loss 3.4809, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 19.9667(20.3875) | Bit/dim 3.5007(3.4787) | Xent 2.3026(2.3026) | Loss 3.5007(3.4787) | Error 0.9022(0.9005) Steps 850(859.09) | Grad Norm 3.1088(2.5743) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 19.5682(20.3823) | Bit/dim 3.4900(3.4770) | Xent 2.3026(2.3026) | Loss 3.4900(3.4770) | Error 0.9144(0.9001) Steps 856(860.04) | Grad Norm 2.5297(2.5657) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 20.6260(20.4287) | Bit/dim 3.5025(3.4798) | Xent 2.3026(2.3026) | Loss 3.5025(3.4798) | Error 0.9011(0.8988) Steps 868(862.89) | Grad Norm 1.8353(2.6111) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 20.2052(20.3455) | Bit/dim 3.5121(3.4800) | Xent 2.3026(2.3026) | Loss 3.5121(3.4800) | Error 0.9056(0.8996) Steps 880(863.22) | Grad Norm 2.3887(2.6278) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 20.2632(20.3151) | Bit/dim 3.4799(3.4808) | Xent 2.3026(2.3026) | Loss 3.4799(3.4808) | Error 0.9156(0.9010) Steps 850(861.10) | Grad Norm 2.8514(2.5850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 98.7085, Epoch Time 1230.6122(1193.6058), Bit/dim 3.4784(best: 3.4809), Xent 2.3026, Loss 3.4784, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 19.7458(20.3362) | Bit/dim 3.4403(3.4767) | Xent 2.3026(2.3026) | Loss 3.4403(3.4767) | Error 0.9011(0.9004) Steps 862(860.89) | Grad Norm 2.4135(2.4916) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 20.2080(20.3115) | Bit/dim 3.4708(3.4774) | Xent 2.3026(2.3026) | Loss 3.4708(3.4774) | Error 0.8944(0.8989) Steps 856(859.71) | Grad Norm 2.3221(2.4993) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 19.6941(20.2942) | Bit/dim 3.4577(3.4782) | Xent 2.3026(2.3026) | Loss 3.4577(3.4782) | Error 0.9044(0.9000) Steps 844(858.05) | Grad Norm 3.4574(2.4894) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 19.8033(20.2787) | Bit/dim 3.4875(3.4786) | Xent 2.3026(2.3026) | Loss 3.4875(3.4786) | Error 0.8978(0.9004) Steps 868(857.15) | Grad Norm 2.6160(2.5837) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 20.4392(20.2960) | Bit/dim 3.4804(3.4796) | Xent 2.3026(2.3026) | Loss 3.4804(3.4796) | Error 0.9111(0.9005) Steps 850(858.32) | Grad Norm 1.6277(2.5216) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 19.7845(20.2943) | Bit/dim 3.4777(3.4786) | Xent 2.3026(2.3026) | Loss 3.4777(3.4786) | Error 0.9111(0.9008) Steps 856(858.10) | Grad Norm 2.2507(2.4044) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 98.6944, Epoch Time 1230.8173(1194.7222), Bit/dim 3.4789(best: 3.4784), Xent 2.3026, Loss 3.4789, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 19.3532(20.2461) | Bit/dim 3.5110(3.4806) | Xent 2.3026(2.3026) | Loss 3.5110(3.4806) | Error 0.9156(0.9013) Steps 856(859.32) | Grad Norm 1.5188(2.3442) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 20.4500(20.2132) | Bit/dim 3.4773(3.4792) | Xent 2.3026(2.3026) | Loss 3.4773(3.4792) | Error 0.8956(0.9009) Steps 874(859.80) | Grad Norm 3.3649(2.3315) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 20.2782(20.2460) | Bit/dim 3.4760(3.4762) | Xent 2.3026(2.3026) | Loss 3.4760(3.4762) | Error 0.9089(0.9007) Steps 874(861.03) | Grad Norm 2.3765(2.3986) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 20.0349(20.2311) | Bit/dim 3.5011(3.4765) | Xent 2.3026(2.3026) | Loss 3.5011(3.4765) | Error 0.9033(0.9006) Steps 868(860.67) | Grad Norm 1.1260(2.4749) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 19.5832(20.2124) | Bit/dim 3.4795(3.4757) | Xent 2.3026(2.3026) | Loss 3.4795(3.4757) | Error 0.9033(0.9007) Steps 862(861.60) | Grad Norm 2.0095(2.4468) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 99.5760, Epoch Time 1226.3299(1195.6704), Bit/dim 3.4769(best: 3.4784), Xent 2.3026, Loss 3.4769, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 20.4930(20.2752) | Bit/dim 3.4884(3.4779) | Xent 2.3026(2.3026) | Loss 3.4884(3.4779) | Error 0.8978(0.8995) Steps 874(861.95) | Grad Norm 1.9434(2.2828) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 19.9330(20.2526) | Bit/dim 3.4634(3.4777) | Xent 2.3026(2.3026) | Loss 3.4634(3.4777) | Error 0.9156(0.9002) Steps 844(861.64) | Grad Norm 1.4160(2.3268) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 20.0977(20.1397) | Bit/dim 3.5010(3.4770) | Xent 2.3026(2.3026) | Loss 3.5010(3.4770) | Error 0.9011(0.9004) Steps 874(861.19) | Grad Norm 2.9430(2.4729) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 20.6226(20.1319) | Bit/dim 3.4981(3.4762) | Xent 2.3026(2.3026) | Loss 3.4981(3.4762) | Error 0.8989(0.9003) Steps 850(861.25) | Grad Norm 2.0391(2.5346) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 20.2066(20.1884) | Bit/dim 3.4422(3.4739) | Xent 2.3026(2.3026) | Loss 3.4422(3.4739) | Error 0.8922(0.9006) Steps 856(860.94) | Grad Norm 3.4195(2.4990) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 19.5923(20.2066) | Bit/dim 3.4805(3.4745) | Xent 2.3026(2.3026) | Loss 3.4805(3.4745) | Error 0.9000(0.9000) Steps 874(859.93) | Grad Norm 2.7073(2.5903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 100.8981, Epoch Time 1227.3196(1196.6199), Bit/dim 3.4798(best: 3.4769), Xent 2.3026, Loss 3.4798, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 19.8486(20.2267) | Bit/dim 3.4644(3.4733) | Xent 2.3026(2.3026) | Loss 3.4644(3.4733) | Error 0.8989(0.8996) Steps 856(860.51) | Grad Norm 2.6157(2.5460) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 19.8874(20.1796) | Bit/dim 3.4938(3.4739) | Xent 2.3026(2.3026) | Loss 3.4938(3.4739) | Error 0.9067(0.9008) Steps 868(862.19) | Grad Norm 2.1517(2.4614) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 19.8236(20.1848) | Bit/dim 3.4601(3.4741) | Xent 2.3026(2.3026) | Loss 3.4601(3.4741) | Error 0.8978(0.9001) Steps 844(862.49) | Grad Norm 1.4814(2.4641) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 20.9486(20.2954) | Bit/dim 3.4636(3.4748) | Xent 2.3026(2.3026) | Loss 3.4636(3.4748) | Error 0.8956(0.9002) Steps 862(863.84) | Grad Norm 1.9559(2.5449) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 20.8991(20.3436) | Bit/dim 3.4724(3.4740) | Xent 2.3026(2.3026) | Loss 3.4724(3.4740) | Error 0.8889(0.8994) Steps 874(865.64) | Grad Norm 2.1999(2.5167) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 102.6856, Epoch Time 1238.1903(1197.8670), Bit/dim 3.4744(best: 3.4769), Xent 2.3026, Loss 3.4744, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 20.4314(20.4023) | Bit/dim 3.4661(3.4735) | Xent 2.3026(2.3026) | Loss 3.4661(3.4735) | Error 0.9078(0.9005) Steps 856(865.81) | Grad Norm 3.8283(2.4334) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 19.9370(20.4133) | Bit/dim 3.4714(3.4755) | Xent 2.3026(2.3026) | Loss 3.4714(3.4755) | Error 0.9033(0.9007) Steps 856(866.47) | Grad Norm 2.3101(2.5041) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 20.0399(20.3611) | Bit/dim 3.4947(3.4723) | Xent 2.3026(2.3026) | Loss 3.4947(3.4723) | Error 0.9067(0.8990) Steps 874(867.72) | Grad Norm 2.1452(2.4096) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 19.8048(20.4077) | Bit/dim 3.4844(3.4730) | Xent 2.3026(2.3026) | Loss 3.4844(3.4730) | Error 0.9044(0.8994) Steps 844(868.36) | Grad Norm 2.2859(2.3467) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 19.5063(20.3723) | Bit/dim 3.4697(3.4737) | Xent 2.3026(2.3026) | Loss 3.4697(3.4737) | Error 0.9122(0.9005) Steps 862(867.91) | Grad Norm 2.1513(2.4879) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 21.3911(20.4806) | Bit/dim 3.4779(3.4722) | Xent 2.3026(2.3026) | Loss 3.4779(3.4722) | Error 0.9044(0.9008) Steps 886(871.36) | Grad Norm 1.3437(2.3890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 102.5726, Epoch Time 1244.2878(1199.2596), Bit/dim 3.4741(best: 3.4744), Xent 2.3026, Loss 3.4741, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 20.2284(20.5165) | Bit/dim 3.4801(3.4736) | Xent 2.3026(2.3026) | Loss 3.4801(3.4736) | Error 0.9044(0.9013) Steps 886(872.76) | Grad Norm 2.9904(2.4566) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 20.1724(20.4489) | Bit/dim 3.4914(3.4749) | Xent 2.3026(2.3026) | Loss 3.4914(3.4749) | Error 0.9111(0.9013) Steps 868(871.80) | Grad Norm 2.4854(2.4891) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 20.7778(20.4054) | Bit/dim 3.4715(3.4751) | Xent 2.3026(2.3026) | Loss 3.4715(3.4751) | Error 0.9011(0.9006) Steps 880(870.44) | Grad Norm 3.1301(2.5661) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 20.4411(20.5015) | Bit/dim 3.4476(3.4715) | Xent 2.3026(2.3026) | Loss 3.4476(3.4715) | Error 0.9189(0.9013) Steps 868(872.11) | Grad Norm 2.1025(2.5550) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 19.9270(20.4368) | Bit/dim 3.4266(3.4699) | Xent 2.3026(2.3026) | Loss 3.4266(3.4699) | Error 0.8944(0.8997) Steps 868(870.88) | Grad Norm 2.4775(2.4984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 100.9075, Epoch Time 1241.2396(1200.5190), Bit/dim 3.4736(best: 3.4741), Xent 2.3026, Loss 3.4736, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 20.4809(20.3881) | Bit/dim 3.4729(3.4714) | Xent 2.3026(2.3026) | Loss 3.4729(3.4714) | Error 0.8967(0.8989) Steps 868(869.69) | Grad Norm 2.4920(2.4434) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 20.6280(20.4841) | Bit/dim 3.4899(3.4700) | Xent 2.3026(2.3026) | Loss 3.4899(3.4700) | Error 0.9011(0.8990) Steps 880(869.35) | Grad Norm 2.6580(2.3287) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 19.9978(20.4424) | Bit/dim 3.4581(3.4690) | Xent 2.3026(2.3026) | Loss 3.4581(3.4690) | Error 0.9056(0.8990) Steps 892(871.19) | Grad Norm 3.9101(2.4707) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 20.5846(20.5265) | Bit/dim 3.4803(3.4704) | Xent 2.3026(2.3026) | Loss 3.4803(3.4704) | Error 0.9267(0.8992) Steps 886(873.16) | Grad Norm 2.2505(2.4120) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 20.8088(20.5724) | Bit/dim 3.4834(3.4704) | Xent 2.3026(2.3026) | Loss 3.4834(3.4704) | Error 0.8978(0.9002) Steps 874(872.56) | Grad Norm 2.6422(2.4460) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 20.3016(20.5135) | Bit/dim 3.4978(3.4702) | Xent 2.3026(2.3026) | Loss 3.4978(3.4702) | Error 0.9056(0.9007) Steps 862(871.76) | Grad Norm 1.8721(2.4347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 101.4199, Epoch Time 1247.0368(1201.9145), Bit/dim 3.4698(best: 3.4736), Xent 2.3026, Loss 3.4698, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 20.2917(20.4658) | Bit/dim 3.4492(3.4698) | Xent 2.3026(2.3026) | Loss 3.4492(3.4698) | Error 0.8867(0.9010) Steps 862(872.13) | Grad Norm 2.3766(2.3286) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 20.5571(20.4366) | Bit/dim 3.4544(3.4719) | Xent 2.3026(2.3026) | Loss 3.4544(3.4719) | Error 0.9044(0.9008) Steps 874(871.86) | Grad Norm 2.1034(2.4248) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 20.0007(20.4221) | Bit/dim 3.4902(3.4702) | Xent 2.3026(2.3026) | Loss 3.4902(3.4702) | Error 0.8967(0.9010) Steps 862(872.26) | Grad Norm 2.5837(2.4544) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 20.2413(20.4362) | Bit/dim 3.4774(3.4711) | Xent 2.3026(2.3026) | Loss 3.4774(3.4711) | Error 0.9011(0.9001) Steps 868(871.85) | Grad Norm 1.7440(2.4652) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 20.2838(20.4479) | Bit/dim 3.4548(3.4676) | Xent 2.3026(2.3026) | Loss 3.4548(3.4676) | Error 0.8944(0.8991) Steps 862(871.08) | Grad Norm 1.8249(2.3908) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 101.4173, Epoch Time 1239.9509(1203.0556), Bit/dim 3.4694(best: 3.4698), Xent 2.3026, Loss 3.4694, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 20.5393(20.4494) | Bit/dim 3.4822(3.4679) | Xent 2.3026(2.3026) | Loss 3.4822(3.4679) | Error 0.9011(0.9000) Steps 874(870.33) | Grad Norm 2.7197(2.5960) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 20.3023(20.4090) | Bit/dim 3.5039(3.4670) | Xent 2.3026(2.3026) | Loss 3.5039(3.4670) | Error 0.9211(0.9000) Steps 868(870.43) | Grad Norm 1.7058(2.4089) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 20.1389(20.4557) | Bit/dim 3.4909(3.4695) | Xent 2.3026(2.3026) | Loss 3.4909(3.4695) | Error 0.9000(0.9014) Steps 856(871.67) | Grad Norm 1.9434(2.4645) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 20.2610(20.4355) | Bit/dim 3.4748(3.4686) | Xent 2.3026(2.3026) | Loss 3.4748(3.4686) | Error 0.8922(0.9000) Steps 856(871.48) | Grad Norm 2.6786(2.3464) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 20.7535(20.4319) | Bit/dim 3.4854(3.4685) | Xent 2.3026(2.3026) | Loss 3.4854(3.4685) | Error 0.8833(0.8994) Steps 868(871.36) | Grad Norm 2.3355(2.4006) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 20.6113(20.3265) | Bit/dim 3.4327(3.4678) | Xent 2.3026(2.3026) | Loss 3.4327(3.4678) | Error 0.8722(0.8993) Steps 874(868.96) | Grad Norm 2.3022(2.4549) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 101.6321, Epoch Time 1236.8574(1204.0697), Bit/dim 3.4726(best: 3.4694), Xent 2.3026, Loss 3.4726, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 19.9064(20.2266) | Bit/dim 3.4424(3.4656) | Xent 2.3026(2.3026) | Loss 3.4424(3.4656) | Error 0.9089(0.8989) Steps 856(868.27) | Grad Norm 2.5950(2.4419) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 20.4165(20.2707) | Bit/dim 3.4376(3.4667) | Xent 2.3026(2.3026) | Loss 3.4376(3.4667) | Error 0.9078(0.8992) Steps 868(867.37) | Grad Norm 2.7166(2.5828) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 20.3784(20.3719) | Bit/dim 3.5008(3.4691) | Xent 2.3026(2.3026) | Loss 3.5008(3.4691) | Error 0.9011(0.8991) Steps 880(868.28) | Grad Norm 2.0503(2.4496) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 20.3222(20.4391) | Bit/dim 3.4355(3.4647) | Xent 2.3026(2.3026) | Loss 3.4355(3.4647) | Error 0.9044(0.8978) Steps 874(872.52) | Grad Norm 3.2188(2.4976) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 19.6384(20.4363) | Bit/dim 3.4598(3.4664) | Xent 2.3026(2.3026) | Loss 3.4598(3.4664) | Error 0.8978(0.8997) Steps 862(871.79) | Grad Norm 1.8931(2.3823) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 104.2109, Epoch Time 1244.4845(1205.2821), Bit/dim 3.4644(best: 3.4694), Xent 2.3026, Loss 3.4644, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 20.9839(20.4751) | Bit/dim 3.4742(3.4673) | Xent 2.3026(2.3026) | Loss 3.4742(3.4673) | Error 0.9056(0.9021) Steps 874(871.88) | Grad Norm 2.6215(2.3900) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 20.5195(20.5309) | Bit/dim 3.4574(3.4651) | Xent 2.3026(2.3026) | Loss 3.4574(3.4651) | Error 0.9033(0.9016) Steps 880(872.83) | Grad Norm 2.8759(2.2987) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 20.7846(20.5687) | Bit/dim 3.5063(3.4653) | Xent 2.3026(2.3026) | Loss 3.5063(3.4653) | Error 0.9111(0.9002) Steps 880(871.44) | Grad Norm 3.5759(2.5013) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 20.0835(20.5262) | Bit/dim 3.4629(3.4663) | Xent 2.3026(2.3026) | Loss 3.4629(3.4663) | Error 0.9089(0.9015) Steps 868(871.32) | Grad Norm 1.4104(2.3963) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 19.4584(20.4440) | Bit/dim 3.4333(3.4652) | Xent 2.3026(2.3026) | Loss 3.4333(3.4652) | Error 0.8956(0.9003) Steps 856(868.05) | Grad Norm 3.1267(2.3759) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 20.8738(20.5291) | Bit/dim 3.4758(3.4653) | Xent 2.3026(2.3026) | Loss 3.4758(3.4653) | Error 0.9089(0.8995) Steps 898(869.30) | Grad Norm 3.4051(2.5019) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 101.1557, Epoch Time 1247.3478(1206.5441), Bit/dim 3.4676(best: 3.4644), Xent 2.3026, Loss 3.4676, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 20.1536(20.5195) | Bit/dim 3.4544(3.4629) | Xent 2.3026(2.3026) | Loss 3.4544(3.4629) | Error 0.8989(0.8986) Steps 874(869.22) | Grad Norm 2.5130(2.5056) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 20.1682(20.5113) | Bit/dim 3.4623(3.4643) | Xent 2.3026(2.3026) | Loss 3.4623(3.4643) | Error 0.8800(0.8999) Steps 874(867.37) | Grad Norm 3.0670(2.4262) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 20.2986(20.5155) | Bit/dim 3.4280(3.4668) | Xent 2.3026(2.3026) | Loss 3.4280(3.4668) | Error 0.8944(0.8994) Steps 862(870.07) | Grad Norm 1.8001(2.4602) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 21.3578(20.5732) | Bit/dim 3.4216(3.4661) | Xent 2.3026(2.3026) | Loss 3.4216(3.4661) | Error 0.8889(0.8999) Steps 862(868.87) | Grad Norm 2.4048(2.3657) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 19.8764(20.5396) | Bit/dim 3.4511(3.4648) | Xent 2.3026(2.3026) | Loss 3.4511(3.4648) | Error 0.9133(0.8998) Steps 856(868.55) | Grad Norm 2.9981(2.4830) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 99.5093, Epoch Time 1244.3361(1207.6779), Bit/dim 3.4692(best: 3.4644), Xent 2.3026, Loss 3.4692, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 20.6127(20.4961) | Bit/dim 3.4747(3.4649) | Xent 2.3026(2.3026) | Loss 3.4747(3.4649) | Error 0.9178(0.9000) Steps 862(866.27) | Grad Norm 1.7817(2.4387) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 20.1723(20.5106) | Bit/dim 3.4799(3.4630) | Xent 2.3026(2.3026) | Loss 3.4799(3.4630) | Error 0.9011(0.9002) Steps 856(866.40) | Grad Norm 2.5778(2.4341) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 20.5852(20.4308) | Bit/dim 3.4862(3.4630) | Xent 2.3026(2.3026) | Loss 3.4862(3.4630) | Error 0.8922(0.8987) Steps 868(866.69) | Grad Norm 2.5044(2.4022) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 20.2562(20.4048) | Bit/dim 3.4930(3.4629) | Xent 2.3026(2.3026) | Loss 3.4930(3.4629) | Error 0.9089(0.8996) Steps 856(865.67) | Grad Norm 2.7708(2.4970) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 20.6082(20.3847) | Bit/dim 3.4663(3.4643) | Xent 2.3026(2.3026) | Loss 3.4663(3.4643) | Error 0.9089(0.8992) Steps 880(866.64) | Grad Norm 2.2077(2.4621) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 20.2494(20.4056) | Bit/dim 3.4469(3.4623) | Xent 2.3026(2.3026) | Loss 3.4469(3.4623) | Error 0.9067(0.9005) Steps 856(867.69) | Grad Norm 2.6619(2.3701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 101.8952, Epoch Time 1238.7999(1208.6115), Bit/dim 3.4649(best: 3.4644), Xent 2.3026, Loss 3.4649, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 20.6475(20.4734) | Bit/dim 3.4581(3.4620) | Xent 2.3026(2.3026) | Loss 3.4581(3.4620) | Error 0.9178(0.8993) Steps 856(868.89) | Grad Norm 2.1899(2.3449) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 20.3734(20.4220) | Bit/dim 3.4295(3.4630) | Xent 2.3026(2.3026) | Loss 3.4295(3.4630) | Error 0.8789(0.9006) Steps 892(869.69) | Grad Norm 1.6748(2.3223) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 20.0304(20.4267) | Bit/dim 3.4937(3.4664) | Xent 2.3026(2.3026) | Loss 3.4937(3.4664) | Error 0.9133(0.9010) Steps 862(871.85) | Grad Norm 1.9970(2.4396) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 20.9088(20.4658) | Bit/dim 3.4349(3.4619) | Xent 2.3026(2.3026) | Loss 3.4349(3.4619) | Error 0.9100(0.9002) Steps 874(872.08) | Grad Norm 1.6749(2.3106) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 20.3321(20.4432) | Bit/dim 3.4795(3.4588) | Xent 2.3026(2.3026) | Loss 3.4795(3.4588) | Error 0.8911(0.8997) Steps 880(872.26) | Grad Norm 3.7460(2.3769) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 101.7127, Epoch Time 1242.9532(1209.6418), Bit/dim 3.4601(best: 3.4644), Xent 2.3026, Loss 3.4601, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 20.7432(20.4278) | Bit/dim 3.4808(3.4619) | Xent 2.3026(2.3026) | Loss 3.4808(3.4619) | Error 0.9111(0.9010) Steps 892(873.40) | Grad Norm 3.0755(2.3572) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 20.7971(20.5059) | Bit/dim 3.4820(3.4635) | Xent 2.3026(2.3026) | Loss 3.4820(3.4635) | Error 0.8989(0.9000) Steps 874(875.06) | Grad Norm 3.7589(2.4592) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 20.0700(20.4611) | Bit/dim 3.4658(3.4638) | Xent 2.3026(2.3026) | Loss 3.4658(3.4638) | Error 0.8822(0.9000) Steps 874(875.02) | Grad Norm 2.3850(2.4681) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 20.1666(20.4364) | Bit/dim 3.4394(3.4622) | Xent 2.3026(2.3026) | Loss 3.4394(3.4622) | Error 0.9033(0.8999) Steps 892(874.27) | Grad Norm 2.3887(2.5346) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 20.4252(20.5293) | Bit/dim 3.4653(3.4611) | Xent 2.3026(2.3026) | Loss 3.4653(3.4611) | Error 0.9044(0.9003) Steps 880(876.27) | Grad Norm 2.1130(2.3990) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 20.7094(20.5970) | Bit/dim 3.4513(3.4596) | Xent 2.3026(2.3026) | Loss 3.4513(3.4596) | Error 0.9044(0.8997) Steps 880(877.94) | Grad Norm 2.9516(2.4796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 102.8785, Epoch Time 1252.0979(1210.9154), Bit/dim 3.4629(best: 3.4601), Xent 2.3026, Loss 3.4629, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 21.1728(20.6494) | Bit/dim 3.4676(3.4570) | Xent 2.3026(2.3026) | Loss 3.4676(3.4570) | Error 0.8933(0.8999) Steps 868(879.31) | Grad Norm 2.0729(2.3784) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 20.0876(20.6468) | Bit/dim 3.4342(3.4571) | Xent 2.3026(2.3026) | Loss 3.4342(3.4571) | Error 0.8789(0.8985) Steps 880(878.51) | Grad Norm 3.3915(2.3508) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 20.4805(20.5865) | Bit/dim 3.4867(3.4596) | Xent 2.3026(2.3026) | Loss 3.4867(3.4596) | Error 0.9078(0.8991) Steps 862(878.50) | Grad Norm 1.8797(2.3779) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 20.7102(20.5712) | Bit/dim 3.4584(3.4587) | Xent 2.3026(2.3026) | Loss 3.4584(3.4587) | Error 0.8844(0.8991) Steps 898(878.61) | Grad Norm 2.8446(2.3532) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 20.5799(20.5611) | Bit/dim 3.4540(3.4592) | Xent 2.3026(2.3026) | Loss 3.4540(3.4592) | Error 0.8944(0.9006) Steps 874(878.06) | Grad Norm 2.4179(2.3884) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 103.9059, Epoch Time 1250.4789(1212.1024), Bit/dim 3.4594(best: 3.4601), Xent 2.3026, Loss 3.4594, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 20.7587(20.5243) | Bit/dim 3.4739(3.4608) | Xent 2.3026(2.3026) | Loss 3.4739(3.4608) | Error 0.9089(0.9005) Steps 880(877.52) | Grad Norm 2.9987(2.2990) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 20.8884(20.6124) | Bit/dim 3.4624(3.4622) | Xent 2.3026(2.3026) | Loss 3.4624(3.4622) | Error 0.8944(0.9012) Steps 874(878.45) | Grad Norm 1.3410(2.3424) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 20.0988(20.5336) | Bit/dim 3.4939(3.4621) | Xent 2.3026(2.3026) | Loss 3.4939(3.4621) | Error 0.8978(0.8997) Steps 892(877.39) | Grad Norm 2.9900(2.3641) | Total Time 14.00(14.00)\n",
      "Iter 6690 | Time 20.4737(20.5385) | Bit/dim 3.4484(3.4625) | Xent 2.3026(2.3026) | Loss 3.4484(3.4625) | Error 0.8933(0.9001) Steps 868(877.46) | Grad Norm 1.6109(2.2065) | Total Time 14.00(14.00)\n",
      "Iter 6700 | Time 20.4425(20.5471) | Bit/dim 3.4757(3.4587) | Xent 2.3026(2.3026) | Loss 3.4757(3.4587) | Error 0.8978(0.8991) Steps 886(877.43) | Grad Norm 3.5658(2.4211) | Total Time 14.00(14.00)\n",
      "Iter 6710 | Time 20.7267(20.5985) | Bit/dim 3.3774(3.4557) | Xent 2.3026(2.3026) | Loss 3.3774(3.4557) | Error 0.9000(0.8997) Steps 892(878.94) | Grad Norm 2.6370(2.4588) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 101.7600, Epoch Time 1250.6854(1213.2598), Bit/dim 3.4599(best: 3.4594), Xent 2.3026, Loss 3.4599, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 19.6901(20.5102) | Bit/dim 3.4740(3.4562) | Xent 2.3026(2.3026) | Loss 3.4740(3.4562) | Error 0.9089(0.9012) Steps 868(877.14) | Grad Norm 1.5117(2.4208) | Total Time 14.00(14.00)\n",
      "Iter 6730 | Time 20.5932(20.5433) | Bit/dim 3.4561(3.4552) | Xent 2.3026(2.3026) | Loss 3.4561(3.4552) | Error 0.9033(0.9008) Steps 886(877.45) | Grad Norm 1.9967(2.2379) | Total Time 14.00(14.00)\n",
      "Iter 6740 | Time 20.2390(20.5332) | Bit/dim 3.4668(3.4578) | Xent 2.3026(2.3026) | Loss 3.4668(3.4578) | Error 0.8956(0.9001) Steps 862(875.53) | Grad Norm 1.5663(2.3714) | Total Time 14.00(14.00)\n",
      "Iter 6750 | Time 20.4200(20.5995) | Bit/dim 3.4453(3.4581) | Xent 2.3026(2.3026) | Loss 3.4453(3.4581) | Error 0.9089(0.9011) Steps 886(876.98) | Grad Norm 4.1421(2.4783) | Total Time 14.00(14.00)\n",
      "Iter 6760 | Time 20.2962(20.5642) | Bit/dim 3.4661(3.4563) | Xent 2.3026(2.3026) | Loss 3.4661(3.4563) | Error 0.9089(0.8996) Steps 868(878.39) | Grad Norm 2.3544(2.4971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 101.8560, Epoch Time 1248.7315(1214.3240), Bit/dim 3.4575(best: 3.4594), Xent 2.3026, Loss 3.4575, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 20.5539(20.5794) | Bit/dim 3.4343(3.4561) | Xent 2.3026(2.3026) | Loss 3.4343(3.4561) | Error 0.9044(0.8998) Steps 862(877.09) | Grad Norm 1.9658(2.3606) | Total Time 14.00(14.00)\n",
      "Iter 6780 | Time 20.9533(20.5587) | Bit/dim 3.4765(3.4577) | Xent 2.3026(2.3026) | Loss 3.4765(3.4577) | Error 0.8844(0.9000) Steps 880(877.50) | Grad Norm 1.7554(2.3128) | Total Time 14.00(14.00)\n",
      "Iter 6790 | Time 20.3888(20.6495) | Bit/dim 3.3946(3.4555) | Xent 2.3026(2.3026) | Loss 3.3946(3.4555) | Error 0.8800(0.8997) Steps 880(879.44) | Grad Norm 1.8536(2.2506) | Total Time 14.00(14.00)\n",
      "Iter 6800 | Time 21.1841(20.6867) | Bit/dim 3.4570(3.4568) | Xent 2.3026(2.3026) | Loss 3.4570(3.4568) | Error 0.9033(0.8996) Steps 904(880.67) | Grad Norm 2.6262(2.3627) | Total Time 14.00(14.00)\n",
      "Iter 6810 | Time 21.0380(20.6874) | Bit/dim 3.4735(3.4564) | Xent 2.3026(2.3026) | Loss 3.4735(3.4564) | Error 0.9122(0.8997) Steps 880(881.61) | Grad Norm 2.3526(2.2636) | Total Time 14.00(14.00)\n",
      "Iter 6820 | Time 20.1849(20.6237) | Bit/dim 3.4654(3.4571) | Xent 2.3026(2.3026) | Loss 3.4654(3.4571) | Error 0.8933(0.9001) Steps 880(882.01) | Grad Norm 1.9982(2.1959) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 103.5465, Epoch Time 1254.9969(1215.5442), Bit/dim 3.4574(best: 3.4575), Xent 2.3026, Loss 3.4574, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 20.6570(20.6465) | Bit/dim 3.4657(3.4579) | Xent 2.3026(2.3026) | Loss 3.4657(3.4579) | Error 0.9000(0.9014) Steps 874(885.66) | Grad Norm 2.0751(2.3796) | Total Time 14.00(14.00)\n",
      "Iter 6840 | Time 20.4662(20.6748) | Bit/dim 3.4423(3.4569) | Xent 2.3026(2.3026) | Loss 3.4423(3.4569) | Error 0.9100(0.9006) Steps 880(884.90) | Grad Norm 2.9128(2.3155) | Total Time 14.00(14.00)\n",
      "Iter 6850 | Time 20.8083(20.6608) | Bit/dim 3.4772(3.4544) | Xent 2.3026(2.3026) | Loss 3.4772(3.4544) | Error 0.9122(0.9003) Steps 892(884.53) | Grad Norm 1.9244(2.2592) | Total Time 14.00(14.00)\n",
      "Iter 6860 | Time 21.1654(20.6789) | Bit/dim 3.4696(3.4554) | Xent 2.3026(2.3026) | Loss 3.4696(3.4554) | Error 0.8967(0.8995) Steps 874(884.91) | Grad Norm 1.6836(2.1325) | Total Time 14.00(14.00)\n",
      "Iter 6870 | Time 19.5567(20.7509) | Bit/dim 3.4717(3.4552) | Xent 2.3026(2.3026) | Loss 3.4717(3.4552) | Error 0.9078(0.8996) Steps 868(886.58) | Grad Norm 1.3987(2.2954) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 105.4291, Epoch Time 1263.6553(1216.9875), Bit/dim 3.4605(best: 3.4574), Xent 2.3026, Loss 3.4605, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 21.9062(20.7996) | Bit/dim 3.4654(3.4565) | Xent 2.3026(2.3026) | Loss 3.4654(3.4565) | Error 0.8833(0.8984) Steps 904(885.81) | Grad Norm 2.5490(2.3351) | Total Time 14.00(14.00)\n",
      "Iter 6890 | Time 20.7246(20.8412) | Bit/dim 3.4239(3.4538) | Xent 2.3026(2.3026) | Loss 3.4239(3.4538) | Error 0.8844(0.9001) Steps 880(885.72) | Grad Norm 4.4218(2.3525) | Total Time 14.00(14.00)\n",
      "Iter 6900 | Time 20.5257(20.8432) | Bit/dim 3.4696(3.4555) | Xent 2.3026(2.3026) | Loss 3.4696(3.4555) | Error 0.9067(0.9002) Steps 904(885.83) | Grad Norm 1.7688(2.4187) | Total Time 14.00(14.00)\n",
      "Iter 6910 | Time 20.8843(20.8967) | Bit/dim 3.4362(3.4556) | Xent 2.3026(2.3026) | Loss 3.4362(3.4556) | Error 0.9156(0.8992) Steps 886(885.17) | Grad Norm 1.8814(2.3113) | Total Time 14.00(14.00)\n",
      "Iter 6920 | Time 20.9802(20.9060) | Bit/dim 3.4555(3.4567) | Xent 2.3026(2.3026) | Loss 3.4555(3.4567) | Error 0.8944(0.9009) Steps 898(884.30) | Grad Norm 2.3826(2.2439) | Total Time 14.00(14.00)\n",
      "Iter 6930 | Time 21.0654(21.0234) | Bit/dim 3.4294(3.4531) | Xent 2.3026(2.3026) | Loss 3.4294(3.4531) | Error 0.8967(0.9002) Steps 874(885.92) | Grad Norm 2.2259(2.2690) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 103.6338, Epoch Time 1277.0085(1218.7881), Bit/dim 3.4557(best: 3.4574), Xent 2.3026, Loss 3.4557, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 21.0373(20.9689) | Bit/dim 3.4399(3.4535) | Xent 2.3026(2.3026) | Loss 3.4399(3.4535) | Error 0.9022(0.9011) Steps 904(886.15) | Grad Norm 2.9517(2.2679) | Total Time 14.00(14.00)\n",
      "Iter 6950 | Time 20.3985(20.9784) | Bit/dim 3.4560(3.4533) | Xent 2.3026(2.3026) | Loss 3.4560(3.4533) | Error 0.8844(0.9002) Steps 886(886.32) | Grad Norm 2.5597(2.3489) | Total Time 14.00(14.00)\n",
      "Iter 6960 | Time 20.0666(20.9221) | Bit/dim 3.4448(3.4520) | Xent 2.3026(2.3026) | Loss 3.4448(3.4520) | Error 0.8978(0.9002) Steps 868(885.66) | Grad Norm 2.6067(2.2530) | Total Time 14.00(14.00)\n",
      "Iter 6970 | Time 21.9560(20.9776) | Bit/dim 3.4363(3.4521) | Xent 2.3026(2.3026) | Loss 3.4363(3.4521) | Error 0.9078(0.9004) Steps 892(886.94) | Grad Norm 1.7818(2.3187) | Total Time 14.00(14.00)\n",
      "Iter 6980 | Time 21.2347(21.0664) | Bit/dim 3.4520(3.4531) | Xent 2.3026(2.3026) | Loss 3.4520(3.4531) | Error 0.8989(0.8999) Steps 886(887.65) | Grad Norm 2.7797(2.2812) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 103.8748, Epoch Time 1275.7395(1220.4967), Bit/dim 3.4568(best: 3.4557), Xent 2.3026, Loss 3.4568, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 20.9152(21.0386) | Bit/dim 3.4550(3.4521) | Xent 2.3026(2.3026) | Loss 3.4550(3.4521) | Error 0.9078(0.8996) Steps 880(888.11) | Grad Norm 1.7858(2.3282) | Total Time 14.00(14.00)\n",
      "Iter 7000 | Time 20.7247(20.9726) | Bit/dim 3.4678(3.4539) | Xent 2.3026(2.3026) | Loss 3.4678(3.4539) | Error 0.9033(0.9008) Steps 886(887.61) | Grad Norm 2.7461(2.2937) | Total Time 14.00(14.00)\n",
      "Iter 7010 | Time 21.0038(20.9650) | Bit/dim 3.4426(3.4548) | Xent 2.3026(2.3026) | Loss 3.4426(3.4548) | Error 0.9133(0.9018) Steps 910(890.33) | Grad Norm 3.3110(2.4329) | Total Time 14.00(14.00)\n",
      "Iter 7020 | Time 20.2576(20.9575) | Bit/dim 3.4119(3.4517) | Xent 2.3026(2.3026) | Loss 3.4119(3.4517) | Error 0.9033(0.9010) Steps 886(890.26) | Grad Norm 2.5376(2.3996) | Total Time 14.00(14.00)\n",
      "Iter 7030 | Time 20.5817(20.9111) | Bit/dim 3.4391(3.4527) | Xent 2.3026(2.3026) | Loss 3.4391(3.4527) | Error 0.8933(0.9005) Steps 910(890.99) | Grad Norm 2.5379(2.4189) | Total Time 14.00(14.00)\n",
      "Iter 7040 | Time 19.7217(20.8691) | Bit/dim 3.4612(3.4524) | Xent 2.3026(2.3026) | Loss 3.4612(3.4524) | Error 0.9044(0.8991) Steps 892(888.81) | Grad Norm 2.3768(2.3850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 103.2482, Epoch Time 1265.6047(1221.8499), Bit/dim 3.4582(best: 3.4557), Xent 2.3026, Loss 3.4582, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 20.5198(20.8692) | Bit/dim 3.4236(3.4496) | Xent 2.3026(2.3026) | Loss 3.4236(3.4496) | Error 0.8867(0.8987) Steps 904(890.03) | Grad Norm 1.7324(2.3645) | Total Time 14.00(14.00)\n",
      "Iter 7060 | Time 21.1687(20.8545) | Bit/dim 3.4623(3.4516) | Xent 2.3026(2.3026) | Loss 3.4623(3.4516) | Error 0.8989(0.8988) Steps 898(891.13) | Grad Norm 1.5173(2.1591) | Total Time 14.00(14.00)\n",
      "Iter 7070 | Time 20.3918(20.8242) | Bit/dim 3.4447(3.4533) | Xent 2.3026(2.3026) | Loss 3.4447(3.4533) | Error 0.8978(0.8992) Steps 892(890.79) | Grad Norm 1.8350(2.3023) | Total Time 14.00(14.00)\n",
      "Iter 7080 | Time 21.0861(20.8793) | Bit/dim 3.4541(3.4517) | Xent 2.3026(2.3026) | Loss 3.4541(3.4517) | Error 0.9078(0.8987) Steps 886(890.98) | Grad Norm 2.1080(2.3415) | Total Time 14.00(14.00)\n",
      "Iter 7090 | Time 21.7369(20.9835) | Bit/dim 3.4637(3.4519) | Xent 2.3026(2.3026) | Loss 3.4637(3.4519) | Error 0.9100(0.9006) Steps 880(892.06) | Grad Norm 2.0423(2.2363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 103.1889, Epoch Time 1272.4897(1223.3691), Bit/dim 3.4555(best: 3.4557), Xent 2.3026, Loss 3.4555, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 20.6736(20.9374) | Bit/dim 3.4357(3.4513) | Xent 2.3026(2.3026) | Loss 3.4357(3.4513) | Error 0.8867(0.9003) Steps 904(892.39) | Grad Norm 2.1656(2.2602) | Total Time 14.00(14.00)\n",
      "Iter 7110 | Time 20.1012(20.8925) | Bit/dim 3.4267(3.4522) | Xent 2.3026(2.3026) | Loss 3.4267(3.4522) | Error 0.8967(0.9021) Steps 886(891.13) | Grad Norm 1.6280(2.2175) | Total Time 14.00(14.00)\n",
      "Iter 7120 | Time 21.6657(20.9407) | Bit/dim 3.4257(3.4527) | Xent 2.3026(2.3026) | Loss 3.4257(3.4527) | Error 0.8856(0.8999) Steps 892(891.00) | Grad Norm 1.4478(2.3187) | Total Time 14.00(14.00)\n",
      "Iter 7130 | Time 21.3748(20.9649) | Bit/dim 3.4528(3.4513) | Xent 2.3026(2.3026) | Loss 3.4528(3.4513) | Error 0.8856(0.8998) Steps 916(892.40) | Grad Norm 2.7640(2.3822) | Total Time 14.00(14.00)\n",
      "Iter 7140 | Time 20.8613(20.9527) | Bit/dim 3.4335(3.4507) | Xent 2.3026(2.3026) | Loss 3.4335(3.4507) | Error 0.8978(0.9000) Steps 904(892.19) | Grad Norm 2.1466(2.2404) | Total Time 14.00(14.00)\n",
      "Iter 7150 | Time 20.8740(20.9462) | Bit/dim 3.4489(3.4504) | Xent 2.3026(2.3026) | Loss 3.4489(3.4504) | Error 0.8744(0.8992) Steps 898(893.46) | Grad Norm 2.1904(2.3312) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 104.1254, Epoch Time 1270.3490(1224.7785), Bit/dim 3.4515(best: 3.4555), Xent 2.3026, Loss 3.4515, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 21.2521(20.9569) | Bit/dim 3.4246(3.4483) | Xent 2.3026(2.3026) | Loss 3.4246(3.4483) | Error 0.9067(0.8989) Steps 910(893.20) | Grad Norm 2.5293(2.4098) | Total Time 14.00(14.00)\n",
      "Iter 7170 | Time 20.7053(20.9373) | Bit/dim 3.4417(3.4496) | Xent 2.3026(2.3026) | Loss 3.4417(3.4496) | Error 0.8956(0.8983) Steps 892(894.33) | Grad Norm 3.2456(2.3886) | Total Time 14.00(14.00)\n",
      "Iter 7180 | Time 21.4091(20.8556) | Bit/dim 3.4700(3.4515) | Xent 2.3026(2.3026) | Loss 3.4700(3.4515) | Error 0.8978(0.9005) Steps 886(891.72) | Grad Norm 2.5048(2.3239) | Total Time 14.00(14.00)\n",
      "Iter 7190 | Time 20.8121(20.8550) | Bit/dim 3.4474(3.4513) | Xent 2.3026(2.3026) | Loss 3.4474(3.4513) | Error 0.8967(0.9013) Steps 922(892.27) | Grad Norm 1.6024(2.3318) | Total Time 14.00(14.00)\n",
      "Iter 7200 | Time 20.9558(20.8581) | Bit/dim 3.4346(3.4494) | Xent 2.3026(2.3026) | Loss 3.4346(3.4494) | Error 0.9067(0.8996) Steps 886(893.83) | Grad Norm 3.0743(2.2950) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 105.1340, Epoch Time 1269.3319(1226.1151), Bit/dim 3.4513(best: 3.4515), Xent 2.3026, Loss 3.4513, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 20.6229(20.8562) | Bit/dim 3.4406(3.4483) | Xent 2.3026(2.3026) | Loss 3.4406(3.4483) | Error 0.9022(0.9001) Steps 898(893.96) | Grad Norm 1.2809(2.3077) | Total Time 14.00(14.00)\n",
      "Iter 7220 | Time 20.7977(20.8277) | Bit/dim 3.4253(3.4472) | Xent 2.3026(2.3026) | Loss 3.4253(3.4472) | Error 0.8900(0.9007) Steps 910(893.44) | Grad Norm 1.8752(2.1424) | Total Time 14.00(14.00)\n",
      "Iter 7230 | Time 21.5668(20.8580) | Bit/dim 3.4538(3.4475) | Xent 2.3026(2.3026) | Loss 3.4538(3.4475) | Error 0.8867(0.8997) Steps 916(894.62) | Grad Norm 2.9174(2.2909) | Total Time 14.00(14.00)\n",
      "Iter 7240 | Time 20.7746(20.9553) | Bit/dim 3.4302(3.4479) | Xent 2.3026(2.3026) | Loss 3.4302(3.4479) | Error 0.9078(0.8999) Steps 892(895.52) | Grad Norm 1.7875(2.2107) | Total Time 14.00(14.00)\n",
      "Iter 7250 | Time 21.0639(20.8799) | Bit/dim 3.4819(3.4508) | Xent 2.3026(2.3026) | Loss 3.4819(3.4508) | Error 0.9089(0.9009) Steps 904(897.33) | Grad Norm 2.2052(2.3057) | Total Time 14.00(14.00)\n",
      "Iter 7260 | Time 21.4348(20.9441) | Bit/dim 3.4439(3.4497) | Xent 2.3026(2.3026) | Loss 3.4439(3.4497) | Error 0.8944(0.8998) Steps 916(898.13) | Grad Norm 2.9132(2.3543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 104.0544, Epoch Time 1270.0602(1227.4335), Bit/dim 3.4527(best: 3.4513), Xent 2.3026, Loss 3.4527, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 20.3678(20.9189) | Bit/dim 3.4281(3.4504) | Xent 2.3026(2.3026) | Loss 3.4281(3.4504) | Error 0.8856(0.9002) Steps 898(897.33) | Grad Norm 2.5052(2.3171) | Total Time 14.00(14.00)\n",
      "Iter 7280 | Time 20.5368(20.8471) | Bit/dim 3.4542(3.4494) | Xent 2.3026(2.3026) | Loss 3.4542(3.4494) | Error 0.8978(0.9000) Steps 886(896.85) | Grad Norm 1.4950(2.2533) | Total Time 14.00(14.00)\n",
      "Iter 7290 | Time 21.4865(20.9300) | Bit/dim 3.3896(3.4444) | Xent 2.3026(2.3026) | Loss 3.3896(3.4444) | Error 0.8878(0.8981) Steps 922(900.46) | Grad Norm 2.2947(2.3200) | Total Time 14.00(14.00)\n",
      "Iter 7300 | Time 19.7498(20.8774) | Bit/dim 3.4562(3.4456) | Xent 2.3026(2.3026) | Loss 3.4562(3.4456) | Error 0.8956(0.8987) Steps 886(898.25) | Grad Norm 1.7902(2.3444) | Total Time 14.00(14.00)\n",
      "Iter 7310 | Time 20.4583(20.8218) | Bit/dim 3.4570(3.4471) | Xent 2.3026(2.3026) | Loss 3.4570(3.4471) | Error 0.9056(0.9001) Steps 886(898.00) | Grad Norm 1.9803(2.2513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 103.2778, Epoch Time 1266.1491(1228.5949), Bit/dim 3.4528(best: 3.4513), Xent 2.3026, Loss 3.4528, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 21.4424(20.8833) | Bit/dim 3.4333(3.4467) | Xent 2.3026(2.3026) | Loss 3.4333(3.4467) | Error 0.9000(0.9011) Steps 910(898.16) | Grad Norm 1.7137(2.3051) | Total Time 14.00(14.00)\n",
      "Iter 7330 | Time 21.3468(20.8538) | Bit/dim 3.4538(3.4455) | Xent 2.3026(2.3026) | Loss 3.4538(3.4455) | Error 0.9089(0.9016) Steps 910(899.48) | Grad Norm 1.5804(2.3040) | Total Time 14.00(14.00)\n",
      "Iter 7340 | Time 20.5364(20.9085) | Bit/dim 3.4435(3.4459) | Xent 2.3026(2.3026) | Loss 3.4435(3.4459) | Error 0.8778(0.9015) Steps 892(898.97) | Grad Norm 2.5847(2.4468) | Total Time 14.00(14.00)\n",
      "Iter 7350 | Time 21.0090(20.9325) | Bit/dim 3.4511(3.4472) | Xent 2.3026(2.3026) | Loss 3.4511(3.4472) | Error 0.8856(0.9002) Steps 898(899.87) | Grad Norm 1.7830(2.3160) | Total Time 14.00(14.00)\n",
      "Iter 7360 | Time 21.1460(20.9613) | Bit/dim 3.4540(3.4466) | Xent 2.3026(2.3026) | Loss 3.4540(3.4466) | Error 0.8856(0.9001) Steps 898(901.69) | Grad Norm 1.5050(2.3631) | Total Time 14.00(14.00)\n",
      "Iter 7370 | Time 21.0347(20.9208) | Bit/dim 3.4634(3.4461) | Xent 2.3026(2.3026) | Loss 3.4634(3.4461) | Error 0.8989(0.8998) Steps 922(900.97) | Grad Norm 2.7525(2.2133) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 104.5238, Epoch Time 1272.7716(1229.9202), Bit/dim 3.4497(best: 3.4513), Xent 2.3026, Loss 3.4497, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 20.6580(20.9412) | Bit/dim 3.4372(3.4455) | Xent 2.3026(2.3026) | Loss 3.4372(3.4455) | Error 0.9033(0.8997) Steps 886(900.33) | Grad Norm 1.8917(2.2981) | Total Time 14.00(14.00)\n",
      "Iter 7390 | Time 21.2583(20.9807) | Bit/dim 3.3906(3.4423) | Xent 2.3026(2.3026) | Loss 3.3906(3.4423) | Error 0.9078(0.9005) Steps 904(900.19) | Grad Norm 2.1746(2.1891) | Total Time 14.00(14.00)\n",
      "Iter 7400 | Time 20.2643(20.9720) | Bit/dim 3.4390(3.4426) | Xent 2.3026(2.3026) | Loss 3.4390(3.4426) | Error 0.9156(0.9006) Steps 898(898.93) | Grad Norm 2.2216(2.2933) | Total Time 14.00(14.00)\n",
      "Iter 7410 | Time 20.3268(20.8843) | Bit/dim 3.4639(3.4468) | Xent 2.3026(2.3026) | Loss 3.4639(3.4468) | Error 0.8989(0.8995) Steps 898(898.15) | Grad Norm 1.9776(2.3046) | Total Time 14.00(14.00)\n",
      "Iter 7420 | Time 20.9754(20.8721) | Bit/dim 3.4613(3.4483) | Xent 2.3026(2.3026) | Loss 3.4613(3.4483) | Error 0.9033(0.9009) Steps 892(897.02) | Grad Norm 3.1522(2.2542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 104.1609, Epoch Time 1271.4135(1231.1650), Bit/dim 3.4494(best: 3.4497), Xent 2.3026, Loss 3.4494, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.5697(20.8851) | Bit/dim 3.4356(3.4474) | Xent 2.3026(2.3026) | Loss 3.4356(3.4474) | Error 0.9022(0.9004) Steps 892(898.94) | Grad Norm 2.3989(2.2692) | Total Time 14.00(14.00)\n",
      "Iter 7440 | Time 20.1223(20.7903) | Bit/dim 3.4398(3.4480) | Xent 2.3026(2.3026) | Loss 3.4398(3.4480) | Error 0.9000(0.9000) Steps 898(899.92) | Grad Norm 2.9073(2.4083) | Total Time 14.00(14.00)\n",
      "Iter 7450 | Time 20.4074(20.8211) | Bit/dim 3.4293(3.4454) | Xent 2.3026(2.3026) | Loss 3.4293(3.4454) | Error 0.8889(0.8999) Steps 898(899.56) | Grad Norm 2.1294(2.2179) | Total Time 14.00(14.00)\n",
      "Iter 7460 | Time 21.3375(20.9051) | Bit/dim 3.4570(3.4433) | Xent 2.3026(2.3026) | Loss 3.4570(3.4433) | Error 0.9089(0.9005) Steps 904(901.24) | Grad Norm 3.0492(2.2660) | Total Time 14.00(14.00)\n",
      "Iter 7470 | Time 21.2785(20.9222) | Bit/dim 3.4572(3.4420) | Xent 2.3026(2.3026) | Loss 3.4572(3.4420) | Error 0.9089(0.8997) Steps 898(903.07) | Grad Norm 1.9261(2.3431) | Total Time 14.00(14.00)\n",
      "Iter 7480 | Time 20.4446(20.9147) | Bit/dim 3.4568(3.4437) | Xent 2.3026(2.3026) | Loss 3.4568(3.4437) | Error 0.9011(0.8995) Steps 898(903.28) | Grad Norm 3.0127(2.3413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 103.4825, Epoch Time 1268.2634(1232.2780), Bit/dim 3.4517(best: 3.4494), Xent 2.3026, Loss 3.4517, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 20.2291(20.9383) | Bit/dim 3.4348(3.4452) | Xent 2.3026(2.3026) | Loss 3.4348(3.4452) | Error 0.8822(0.8991) Steps 910(906.03) | Grad Norm 2.4016(2.4359) | Total Time 14.00(14.00)\n",
      "Iter 7500 | Time 21.3480(20.9843) | Bit/dim 3.4242(3.4461) | Xent 2.3026(2.3026) | Loss 3.4242(3.4461) | Error 0.8811(0.8991) Steps 886(906.95) | Grad Norm 1.9897(2.3969) | Total Time 14.00(14.00)\n",
      "Iter 7510 | Time 21.2291(21.0440) | Bit/dim 3.4635(3.4454) | Xent 2.3026(2.3026) | Loss 3.4635(3.4454) | Error 0.8922(0.8991) Steps 916(907.14) | Grad Norm 2.0415(2.3838) | Total Time 14.00(14.00)\n",
      "Iter 7520 | Time 20.6775(21.0330) | Bit/dim 3.4064(3.4455) | Xent 2.3026(2.3026) | Loss 3.4064(3.4455) | Error 0.8933(0.8992) Steps 886(905.81) | Grad Norm 2.1658(2.3171) | Total Time 14.00(14.00)\n",
      "Iter 7530 | Time 21.3640(21.0012) | Bit/dim 3.4034(3.4419) | Xent 2.3026(2.3026) | Loss 3.4034(3.4419) | Error 0.8989(0.8998) Steps 892(903.36) | Grad Norm 0.9987(2.1847) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 104.1972, Epoch Time 1277.9441(1233.6480), Bit/dim 3.4470(best: 3.4494), Xent 2.3026, Loss 3.4470, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 20.8907(20.9732) | Bit/dim 3.4437(3.4412) | Xent 2.3026(2.3026) | Loss 3.4437(3.4412) | Error 0.8833(0.8986) Steps 910(902.70) | Grad Norm 3.5764(2.2883) | Total Time 14.00(14.00)\n",
      "Iter 7550 | Time 20.3322(20.9371) | Bit/dim 3.4312(3.4407) | Xent 2.3026(2.3026) | Loss 3.4312(3.4407) | Error 0.8889(0.8982) Steps 886(902.30) | Grad Norm 2.3579(2.2655) | Total Time 14.00(14.00)\n",
      "Iter 7560 | Time 21.1067(20.8973) | Bit/dim 3.4453(3.4423) | Xent 2.3026(2.3026) | Loss 3.4453(3.4423) | Error 0.8989(0.8983) Steps 904(901.38) | Grad Norm 2.5773(2.1591) | Total Time 14.00(14.00)\n",
      "Iter 7570 | Time 20.8786(20.8877) | Bit/dim 3.4296(3.4445) | Xent 2.3026(2.3026) | Loss 3.4296(3.4445) | Error 0.9267(0.9001) Steps 916(902.19) | Grad Norm 2.7132(2.2549) | Total Time 14.00(14.00)\n",
      "Iter 7580 | Time 21.4982(20.8632) | Bit/dim 3.4548(3.4422) | Xent 2.3026(2.3026) | Loss 3.4548(3.4422) | Error 0.8989(0.9011) Steps 910(902.42) | Grad Norm 1.6629(2.1808) | Total Time 14.00(14.00)\n",
      "Iter 7590 | Time 21.4060(20.8463) | Bit/dim 3.4362(3.4424) | Xent 2.3026(2.3026) | Loss 3.4362(3.4424) | Error 0.9111(0.9014) Steps 910(901.55) | Grad Norm 1.5599(2.2619) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 104.6744, Epoch Time 1265.4927(1234.6033), Bit/dim 3.4466(best: 3.4470), Xent 2.3026, Loss 3.4466, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 21.1893(20.8945) | Bit/dim 3.4522(3.4422) | Xent 2.3026(2.3026) | Loss 3.4522(3.4422) | Error 0.9044(0.9008) Steps 898(902.66) | Grad Norm 2.3999(2.2098) | Total Time 14.00(14.00)\n",
      "Iter 7610 | Time 20.7378(20.8848) | Bit/dim 3.4387(3.4421) | Xent 2.3026(2.3026) | Loss 3.4387(3.4421) | Error 0.9078(0.8998) Steps 916(902.22) | Grad Norm 1.7988(2.2189) | Total Time 14.00(14.00)\n",
      "Iter 7620 | Time 20.9984(20.8969) | Bit/dim 3.4204(3.4404) | Xent 2.3026(2.3026) | Loss 3.4204(3.4404) | Error 0.9000(0.8998) Steps 904(901.69) | Grad Norm 2.4449(2.1897) | Total Time 14.00(14.00)\n",
      "Iter 7630 | Time 20.1733(20.8405) | Bit/dim 3.4259(3.4432) | Xent 2.3026(2.3026) | Loss 3.4259(3.4432) | Error 0.8933(0.8999) Steps 904(901.77) | Grad Norm 1.9038(2.1978) | Total Time 14.00(14.00)\n",
      "Iter 7640 | Time 20.5168(20.7860) | Bit/dim 3.4069(3.4398) | Xent 2.3026(2.3026) | Loss 3.4069(3.4398) | Error 0.8867(0.9007) Steps 904(903.31) | Grad Norm 2.0060(2.3384) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 104.5477, Epoch Time 1267.5590(1235.5920), Bit/dim 3.4434(best: 3.4466), Xent 2.3026, Loss 3.4434, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 21.0897(20.8620) | Bit/dim 3.4142(3.4404) | Xent 2.3026(2.3026) | Loss 3.4142(3.4404) | Error 0.8944(0.9006) Steps 916(903.99) | Grad Norm 2.2428(2.2454) | Total Time 14.00(14.00)\n",
      "Iter 7660 | Time 21.1011(20.9062) | Bit/dim 3.4729(3.4443) | Xent 2.3026(2.3026) | Loss 3.4729(3.4443) | Error 0.9233(0.8994) Steps 898(904.13) | Grad Norm 1.5786(2.2373) | Total Time 14.00(14.00)\n",
      "Iter 7670 | Time 20.1142(20.8785) | Bit/dim 3.4218(3.4427) | Xent 2.3026(2.3026) | Loss 3.4218(3.4427) | Error 0.8989(0.9000) Steps 898(903.44) | Grad Norm 2.5006(2.3091) | Total Time 14.00(14.00)\n",
      "Iter 7680 | Time 20.7314(20.8630) | Bit/dim 3.4116(3.4429) | Xent 2.3026(2.3026) | Loss 3.4116(3.4429) | Error 0.9022(0.9006) Steps 922(903.89) | Grad Norm 2.2731(2.2737) | Total Time 14.00(14.00)\n",
      "Iter 7690 | Time 20.8129(20.8769) | Bit/dim 3.4167(3.4400) | Xent 2.3026(2.3026) | Loss 3.4167(3.4400) | Error 0.8989(0.9003) Steps 904(904.51) | Grad Norm 2.0051(2.1532) | Total Time 14.00(14.00)\n",
      "Iter 7700 | Time 20.8056(20.9431) | Bit/dim 3.3964(3.4393) | Xent 2.3026(2.3026) | Loss 3.3964(3.4393) | Error 0.8889(0.9004) Steps 922(905.65) | Grad Norm 2.7347(2.3568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 104.8339, Epoch Time 1273.2429(1236.7215), Bit/dim 3.4467(best: 3.4434), Xent 2.3026, Loss 3.4467, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 21.3626(20.8933) | Bit/dim 3.4145(3.4393) | Xent 2.3026(2.3026) | Loss 3.4145(3.4393) | Error 0.8867(0.9000) Steps 904(903.99) | Grad Norm 3.0127(2.2851) | Total Time 14.00(14.00)\n",
      "Iter 7720 | Time 20.6428(20.9879) | Bit/dim 3.4513(3.4401) | Xent 2.3026(2.3026) | Loss 3.4513(3.4401) | Error 0.9044(0.8999) Steps 904(906.09) | Grad Norm 2.1040(2.4062) | Total Time 14.00(14.00)\n",
      "Iter 7730 | Time 21.4925(21.0035) | Bit/dim 3.4161(3.4388) | Xent 2.3026(2.3026) | Loss 3.4161(3.4388) | Error 0.8967(0.8991) Steps 934(905.76) | Grad Norm 1.1999(2.3044) | Total Time 14.00(14.00)\n",
      "Iter 7740 | Time 20.7338(20.9690) | Bit/dim 3.4528(3.4403) | Xent 2.3026(2.3026) | Loss 3.4528(3.4403) | Error 0.8900(0.8988) Steps 904(907.60) | Grad Norm 2.2087(2.3094) | Total Time 14.00(14.00)\n",
      "Iter 7750 | Time 21.9224(21.0545) | Bit/dim 3.4622(3.4396) | Xent 2.3026(2.3026) | Loss 3.4622(3.4396) | Error 0.9056(0.9005) Steps 904(907.22) | Grad Norm 2.6977(2.2075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 107.1119, Epoch Time 1280.5869(1238.0375), Bit/dim 3.4434(best: 3.4434), Xent 2.3026, Loss 3.4434, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 20.9017(21.0372) | Bit/dim 3.4325(3.4394) | Xent 2.3026(2.3026) | Loss 3.4325(3.4394) | Error 0.8789(0.8998) Steps 886(906.74) | Grad Norm 2.3674(2.2338) | Total Time 14.00(14.00)\n",
      "Iter 7770 | Time 20.7182(21.0205) | Bit/dim 3.4453(3.4394) | Xent 2.3026(2.3026) | Loss 3.4453(3.4394) | Error 0.9189(0.9010) Steps 886(904.36) | Grad Norm 2.1191(2.2799) | Total Time 14.00(14.00)\n",
      "Iter 7780 | Time 21.1874(21.0090) | Bit/dim 3.4188(3.4388) | Xent 2.3026(2.3026) | Loss 3.4188(3.4388) | Error 0.8856(0.8999) Steps 916(905.57) | Grad Norm 2.6519(2.3267) | Total Time 14.00(14.00)\n",
      "Iter 7790 | Time 20.5751(20.9755) | Bit/dim 3.4384(3.4405) | Xent 2.3026(2.3026) | Loss 3.4384(3.4405) | Error 0.9000(0.8999) Steps 904(904.52) | Grad Norm 2.9007(2.3544) | Total Time 14.00(14.00)\n",
      "Iter 7800 | Time 21.0075(20.9898) | Bit/dim 3.4362(3.4391) | Xent 2.3026(2.3026) | Loss 3.4362(3.4391) | Error 0.9011(0.9004) Steps 910(905.09) | Grad Norm 1.3357(2.2461) | Total Time 14.00(14.00)\n",
      "Iter 7810 | Time 21.0464(21.0303) | Bit/dim 3.4155(3.4397) | Xent 2.3026(2.3026) | Loss 3.4155(3.4397) | Error 0.8989(0.9001) Steps 898(903.70) | Grad Norm 2.6497(2.2683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 106.1967, Epoch Time 1277.3729(1239.2175), Bit/dim 3.4408(best: 3.4434), Xent 2.3026, Loss 3.4408, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 21.1818(21.0288) | Bit/dim 3.4518(3.4356) | Xent 2.3026(2.3026) | Loss 3.4518(3.4356) | Error 0.9100(0.9007) Steps 886(904.01) | Grad Norm 2.6161(2.3240) | Total Time 14.00(14.00)\n",
      "Iter 7830 | Time 20.3326(20.9972) | Bit/dim 3.4577(3.4351) | Xent 2.3026(2.3026) | Loss 3.4577(3.4351) | Error 0.9156(0.9009) Steps 892(904.94) | Grad Norm 1.8360(2.1938) | Total Time 14.00(14.00)\n",
      "Iter 7840 | Time 20.7650(20.9988) | Bit/dim 3.4378(3.4381) | Xent 2.3026(2.3026) | Loss 3.4378(3.4381) | Error 0.9000(0.9005) Steps 928(906.09) | Grad Norm 2.6527(2.1616) | Total Time 14.00(14.00)\n",
      "Iter 7850 | Time 21.5176(21.0665) | Bit/dim 3.4369(3.4385) | Xent 2.3026(2.3026) | Loss 3.4369(3.4385) | Error 0.9100(0.9003) Steps 922(907.67) | Grad Norm 2.3423(2.2802) | Total Time 14.00(14.00)\n",
      "Iter 7860 | Time 20.9096(21.0602) | Bit/dim 3.4485(3.4413) | Xent 2.3026(2.3026) | Loss 3.4485(3.4413) | Error 0.9022(0.9005) Steps 904(906.45) | Grad Norm 1.7669(2.1478) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 105.9788, Epoch Time 1279.0648(1240.4130), Bit/dim 3.4364(best: 3.4408), Xent 2.3026, Loss 3.4364, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 21.1905(21.0720) | Bit/dim 3.4598(3.4397) | Xent 2.3026(2.3026) | Loss 3.4598(3.4397) | Error 0.9144(0.8992) Steps 904(906.40) | Grad Norm 2.0909(2.0741) | Total Time 14.00(14.00)\n",
      "Iter 7880 | Time 21.0421(21.1037) | Bit/dim 3.4415(3.4392) | Xent 2.3026(2.3026) | Loss 3.4415(3.4392) | Error 0.9133(0.9001) Steps 910(906.25) | Grad Norm 2.5883(2.0492) | Total Time 14.00(14.00)\n",
      "Iter 7890 | Time 21.1090(21.1314) | Bit/dim 3.4180(3.4387) | Xent 2.3026(2.3026) | Loss 3.4180(3.4387) | Error 0.8978(0.9003) Steps 916(908.21) | Grad Norm 3.2095(2.1793) | Total Time 14.00(14.00)\n",
      "Iter 7900 | Time 20.2600(21.1315) | Bit/dim 3.4251(3.4387) | Xent 2.3026(2.3026) | Loss 3.4251(3.4387) | Error 0.9022(0.9009) Steps 922(910.67) | Grad Norm 2.9490(2.3444) | Total Time 14.00(14.00)\n",
      "Iter 7910 | Time 20.8749(21.0799) | Bit/dim 3.3937(3.4375) | Xent 2.3026(2.3026) | Loss 3.3937(3.4375) | Error 0.8822(0.9000) Steps 904(910.46) | Grad Norm 2.1238(2.2676) | Total Time 14.00(14.00)\n",
      "Iter 7920 | Time 21.0407(21.0115) | Bit/dim 3.4511(3.4374) | Xent 2.3026(2.3026) | Loss 3.4511(3.4374) | Error 0.9056(0.8994) Steps 916(910.75) | Grad Norm 2.7803(2.3531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 107.3949, Epoch Time 1283.6052(1241.7087), Bit/dim 3.4398(best: 3.4364), Xent 2.3026, Loss 3.4398, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 21.3122(20.9305) | Bit/dim 3.4134(3.4367) | Xent 2.3026(2.3026) | Loss 3.4134(3.4367) | Error 0.9022(0.9004) Steps 904(909.71) | Grad Norm 1.8303(2.1786) | Total Time 14.00(14.00)\n",
      "Iter 7940 | Time 21.0333(20.9355) | Bit/dim 3.4223(3.4369) | Xent 2.3026(2.3026) | Loss 3.4223(3.4369) | Error 0.9056(0.9000) Steps 910(909.39) | Grad Norm 1.5226(2.2863) | Total Time 14.00(14.00)\n",
      "Iter 7950 | Time 20.4170(20.9741) | Bit/dim 3.4461(3.4359) | Xent 2.3026(2.3026) | Loss 3.4461(3.4359) | Error 0.9089(0.8997) Steps 928(911.69) | Grad Norm 2.4332(2.1245) | Total Time 14.00(14.00)\n",
      "Iter 7960 | Time 20.4098(21.0154) | Bit/dim 3.4216(3.4371) | Xent 2.3026(2.3026) | Loss 3.4216(3.4371) | Error 0.9056(0.9001) Steps 916(910.94) | Grad Norm 1.9271(2.2810) | Total Time 14.00(14.00)\n",
      "Iter 7970 | Time 21.3812(21.0576) | Bit/dim 3.4456(3.4378) | Xent 2.3026(2.3026) | Loss 3.4456(3.4378) | Error 0.8900(0.9004) Steps 898(911.11) | Grad Norm 1.5031(2.1261) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 107.2703, Epoch Time 1280.5559(1242.8741), Bit/dim 3.4404(best: 3.4364), Xent 2.3026, Loss 3.4404, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 21.2402(21.1397) | Bit/dim 3.4419(3.4381) | Xent 2.3026(2.3026) | Loss 3.4419(3.4381) | Error 0.8989(0.8997) Steps 934(911.75) | Grad Norm 2.7365(2.1672) | Total Time 14.00(14.00)\n",
      "Iter 7990 | Time 21.0590(21.2186) | Bit/dim 3.4536(3.4369) | Xent 2.3026(2.3026) | Loss 3.4536(3.4369) | Error 0.8978(0.8988) Steps 910(912.93) | Grad Norm 2.9040(2.2339) | Total Time 14.00(14.00)\n",
      "Iter 8000 | Time 21.4254(21.2238) | Bit/dim 3.4061(3.4353) | Xent 2.3026(2.3026) | Loss 3.4061(3.4353) | Error 0.8867(0.8982) Steps 922(911.85) | Grad Norm 3.6667(2.2932) | Total Time 14.00(14.00)\n",
      "Iter 8010 | Time 21.0023(21.2098) | Bit/dim 3.4664(3.4388) | Xent 2.3026(2.3026) | Loss 3.4664(3.4388) | Error 0.8933(0.8996) Steps 898(910.67) | Grad Norm 1.9199(2.3246) | Total Time 14.00(14.00)\n",
      "Iter 8020 | Time 20.8984(21.1375) | Bit/dim 3.4069(3.4346) | Xent 2.3026(2.3026) | Loss 3.4069(3.4346) | Error 0.8844(0.8993) Steps 892(908.18) | Grad Norm 1.6623(2.1558) | Total Time 14.00(14.00)\n",
      "Iter 8030 | Time 20.8380(21.1255) | Bit/dim 3.4045(3.4347) | Xent 2.3026(2.3026) | Loss 3.4045(3.4347) | Error 0.9067(0.9000) Steps 898(907.56) | Grad Norm 1.3375(2.2041) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 105.6052, Epoch Time 1288.3551(1244.2386), Bit/dim 3.4375(best: 3.4364), Xent 2.3026, Loss 3.4375, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 21.1033(21.0693) | Bit/dim 3.4320(3.4371) | Xent 2.3026(2.3026) | Loss 3.4320(3.4371) | Error 0.8922(0.9001) Steps 916(907.79) | Grad Norm 2.8323(2.1921) | Total Time 14.00(14.00)\n",
      "Iter 8050 | Time 20.7482(21.1043) | Bit/dim 3.4442(3.4375) | Xent 2.3026(2.3026) | Loss 3.4442(3.4375) | Error 0.8967(0.9009) Steps 916(908.40) | Grad Norm 2.5638(2.4449) | Total Time 14.00(14.00)\n",
      "Iter 8060 | Time 20.8662(21.0421) | Bit/dim 3.4148(3.4350) | Xent 2.3026(2.3026) | Loss 3.4148(3.4350) | Error 0.9000(0.9011) Steps 910(908.76) | Grad Norm 2.8508(2.4270) | Total Time 14.00(14.00)\n",
      "Iter 8070 | Time 20.7595(21.0247) | Bit/dim 3.4513(3.4347) | Xent 2.3026(2.3026) | Loss 3.4513(3.4347) | Error 0.9078(0.8995) Steps 916(908.49) | Grad Norm 1.7009(2.2495) | Total Time 14.00(14.00)\n",
      "Iter 8080 | Time 21.2265(21.0651) | Bit/dim 3.4331(3.4335) | Xent 2.3026(2.3026) | Loss 3.4331(3.4335) | Error 0.8789(0.8989) Steps 928(909.51) | Grad Norm 2.0843(2.2789) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 107.4895, Epoch Time 1282.1073(1245.3746), Bit/dim 3.4356(best: 3.4364), Xent 2.3026, Loss 3.4356, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 20.8780(21.0643) | Bit/dim 3.4075(3.4335) | Xent 2.3026(2.3026) | Loss 3.4075(3.4335) | Error 0.8911(0.8983) Steps 904(909.77) | Grad Norm 2.2855(2.1630) | Total Time 14.00(14.00)\n",
      "Iter 8100 | Time 20.6974(21.0761) | Bit/dim 3.4596(3.4340) | Xent 2.3026(2.3026) | Loss 3.4596(3.4340) | Error 0.9200(0.9000) Steps 910(909.72) | Grad Norm 1.9610(2.1316) | Total Time 14.00(14.00)\n",
      "Iter 8110 | Time 21.4330(21.0902) | Bit/dim 3.4191(3.4317) | Xent 2.3026(2.3026) | Loss 3.4191(3.4317) | Error 0.8856(0.8994) Steps 928(910.83) | Grad Norm 2.2065(2.2067) | Total Time 14.00(14.00)\n",
      "Iter 8120 | Time 20.6078(21.0641) | Bit/dim 3.4372(3.4342) | Xent 2.3026(2.3026) | Loss 3.4372(3.4342) | Error 0.9056(0.8993) Steps 916(912.71) | Grad Norm 2.5173(2.1716) | Total Time 14.00(14.00)\n",
      "Iter 8130 | Time 21.1377(21.0664) | Bit/dim 3.4568(3.4337) | Xent 2.3026(2.3026) | Loss 3.4568(3.4337) | Error 0.9156(0.9003) Steps 910(911.99) | Grad Norm 1.8332(2.2402) | Total Time 14.00(14.00)\n",
      "Iter 8140 | Time 21.2261(21.0370) | Bit/dim 3.4513(3.4366) | Xent 2.3026(2.3026) | Loss 3.4513(3.4366) | Error 0.8944(0.8997) Steps 904(910.40) | Grad Norm 2.4275(2.3255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 106.6296, Epoch Time 1279.0448(1246.3847), Bit/dim 3.4371(best: 3.4356), Xent 2.3026, Loss 3.4371, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 21.0096(21.0263) | Bit/dim 3.4456(3.4381) | Xent 2.3026(2.3026) | Loss 3.4456(3.4381) | Error 0.8933(0.8987) Steps 934(910.47) | Grad Norm 2.4958(2.3166) | Total Time 14.00(14.00)\n",
      "Iter 8160 | Time 21.4544(20.9963) | Bit/dim 3.4140(3.4372) | Xent 2.3026(2.3026) | Loss 3.4140(3.4372) | Error 0.8922(0.8991) Steps 904(909.82) | Grad Norm 1.5897(2.1563) | Total Time 14.00(14.00)\n",
      "Iter 8170 | Time 21.0694(21.0013) | Bit/dim 3.4434(3.4363) | Xent 2.3026(2.3026) | Loss 3.4434(3.4363) | Error 0.9056(0.8991) Steps 916(907.43) | Grad Norm 2.2454(2.1721) | Total Time 14.00(14.00)\n",
      "Iter 8180 | Time 21.1130(20.9354) | Bit/dim 3.4139(3.4342) | Xent 2.3026(2.3026) | Loss 3.4139(3.4342) | Error 0.9111(0.8994) Steps 910(909.80) | Grad Norm 2.1830(2.1502) | Total Time 14.00(14.00)\n",
      "Iter 8190 | Time 20.5711(20.8531) | Bit/dim 3.4702(3.4318) | Xent 2.3026(2.3026) | Loss 3.4702(3.4318) | Error 0.9033(0.9009) Steps 916(909.52) | Grad Norm 2.6472(2.1224) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 106.1567, Epoch Time 1269.8214(1247.0878), Bit/dim 3.4336(best: 3.4356), Xent 2.3026, Loss 3.4336, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 21.2484(20.8756) | Bit/dim 3.4566(3.4325) | Xent 2.3026(2.3026) | Loss 3.4566(3.4325) | Error 0.8978(0.9013) Steps 928(910.83) | Grad Norm 2.1163(2.1619) | Total Time 14.00(14.00)\n",
      "Iter 8210 | Time 21.3140(20.9492) | Bit/dim 3.4323(3.4334) | Xent 2.3026(2.3026) | Loss 3.4323(3.4334) | Error 0.8922(0.9011) Steps 934(912.09) | Grad Norm 2.4602(2.2030) | Total Time 14.00(14.00)\n",
      "Iter 8220 | Time 21.8057(20.9708) | Bit/dim 3.4613(3.4321) | Xent 2.3026(2.3026) | Loss 3.4613(3.4321) | Error 0.8889(0.9000) Steps 910(912.28) | Grad Norm 1.5112(2.0516) | Total Time 14.00(14.00)\n",
      "Iter 8230 | Time 20.9523(20.9360) | Bit/dim 3.4359(3.4289) | Xent 2.3026(2.3026) | Loss 3.4359(3.4289) | Error 0.9056(0.8998) Steps 898(911.61) | Grad Norm 2.6259(2.1489) | Total Time 14.00(14.00)\n",
      "Iter 8240 | Time 21.5038(21.0092) | Bit/dim 3.4304(3.4300) | Xent 2.3026(2.3026) | Loss 3.4304(3.4300) | Error 0.8711(0.8995) Steps 916(913.06) | Grad Norm 1.9545(2.1285) | Total Time 14.00(14.00)\n",
      "Iter 8250 | Time 21.2992(21.0296) | Bit/dim 3.4297(3.4312) | Xent 2.3026(2.3026) | Loss 3.4297(3.4312) | Error 0.8967(0.9001) Steps 904(912.02) | Grad Norm 2.9332(2.1688) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 108.1766, Epoch Time 1282.0877(1248.1378), Bit/dim 3.4389(best: 3.4336), Xent 2.3026, Loss 3.4389, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 20.7018(21.0711) | Bit/dim 3.4458(3.4311) | Xent 2.3026(2.3026) | Loss 3.4458(3.4311) | Error 0.8956(0.8993) Steps 916(912.14) | Grad Norm 2.4560(2.1497) | Total Time 14.00(14.00)\n",
      "Iter 8270 | Time 21.3838(21.1205) | Bit/dim 3.4132(3.4320) | Xent 2.3026(2.3026) | Loss 3.4132(3.4320) | Error 0.9156(0.8993) Steps 910(912.09) | Grad Norm 1.3737(2.1840) | Total Time 14.00(14.00)\n",
      "Iter 8280 | Time 21.2458(21.1115) | Bit/dim 3.4285(3.4315) | Xent 2.3026(2.3026) | Loss 3.4285(3.4315) | Error 0.8933(0.9001) Steps 910(910.10) | Grad Norm 2.9223(2.3013) | Total Time 14.00(14.00)\n",
      "Iter 8290 | Time 20.5211(21.0383) | Bit/dim 3.4123(3.4303) | Xent 2.3026(2.3026) | Loss 3.4123(3.4303) | Error 0.9033(0.9007) Steps 916(910.87) | Grad Norm 1.6849(2.2708) | Total Time 14.00(14.00)\n",
      "Iter 8300 | Time 20.4578(21.0247) | Bit/dim 3.4239(3.4308) | Xent 2.3026(2.3026) | Loss 3.4239(3.4308) | Error 0.9022(0.9009) Steps 904(911.78) | Grad Norm 2.1097(2.2220) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 107.8910, Epoch Time 1284.0412(1249.2149), Bit/dim 3.4334(best: 3.4336), Xent 2.3026, Loss 3.4334, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 21.1562(21.0543) | Bit/dim 3.4337(3.4310) | Xent 2.3026(2.3026) | Loss 3.4337(3.4310) | Error 0.8822(0.8989) Steps 928(913.08) | Grad Norm 2.2572(2.1629) | Total Time 14.00(14.00)\n",
      "Iter 8320 | Time 20.9700(21.0703) | Bit/dim 3.4671(3.4310) | Xent 2.3026(2.3026) | Loss 3.4671(3.4310) | Error 0.9056(0.8986) Steps 928(914.73) | Grad Norm 2.6633(2.2170) | Total Time 14.00(14.00)\n",
      "Iter 8330 | Time 21.5001(21.1265) | Bit/dim 3.4006(3.4279) | Xent 2.3026(2.3026) | Loss 3.4006(3.4279) | Error 0.8967(0.8990) Steps 904(915.72) | Grad Norm 2.7315(2.2222) | Total Time 14.00(14.00)\n",
      "Iter 8340 | Time 21.5436(21.1313) | Bit/dim 3.4730(3.4309) | Xent 2.3026(2.3026) | Loss 3.4730(3.4309) | Error 0.9156(0.9000) Steps 916(915.69) | Grad Norm 1.7654(2.1876) | Total Time 14.00(14.00)\n",
      "Iter 8350 | Time 20.5704(21.1085) | Bit/dim 3.4463(3.4338) | Xent 2.3026(2.3026) | Loss 3.4463(3.4338) | Error 0.8956(0.9015) Steps 916(914.39) | Grad Norm 1.9469(2.2123) | Total Time 14.00(14.00)\n",
      "Iter 8360 | Time 21.6776(21.1265) | Bit/dim 3.4206(3.4331) | Xent 2.3026(2.3026) | Loss 3.4206(3.4331) | Error 0.9200(0.9007) Steps 916(914.85) | Grad Norm 2.4968(2.1878) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 108.5797, Epoch Time 1287.3875(1250.3601), Bit/dim 3.4376(best: 3.4334), Xent 2.3026, Loss 3.4376, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 21.3294(21.1194) | Bit/dim 3.4233(3.4306) | Xent 2.3026(2.3026) | Loss 3.4233(3.4306) | Error 0.8900(0.9000) Steps 916(917.49) | Grad Norm 2.4156(2.1951) | Total Time 14.00(14.00)\n",
      "Iter 8380 | Time 21.2498(21.0716) | Bit/dim 3.4419(3.4316) | Xent 2.3026(2.3026) | Loss 3.4419(3.4316) | Error 0.9156(0.9006) Steps 910(918.11) | Grad Norm 1.7743(2.2387) | Total Time 14.00(14.00)\n",
      "Iter 8390 | Time 21.1204(21.0430) | Bit/dim 3.4819(3.4319) | Xent 2.3026(2.3026) | Loss 3.4819(3.4319) | Error 0.9144(0.9010) Steps 922(915.89) | Grad Norm 2.1792(2.1707) | Total Time 14.00(14.00)\n",
      "Iter 8400 | Time 20.7134(21.0370) | Bit/dim 3.4101(3.4295) | Xent 2.3026(2.3026) | Loss 3.4101(3.4295) | Error 0.9000(0.9004) Steps 898(914.30) | Grad Norm 2.7754(2.1457) | Total Time 14.00(14.00)\n",
      "Iter 8410 | Time 21.0500(21.0365) | Bit/dim 3.4026(3.4295) | Xent 2.3026(2.3026) | Loss 3.4026(3.4295) | Error 0.9011(0.8997) Steps 898(913.32) | Grad Norm 1.9462(2.2739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 106.5328, Epoch Time 1278.0388(1251.1905), Bit/dim 3.4318(best: 3.4334), Xent 2.3026, Loss 3.4318, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 20.2635(20.9822) | Bit/dim 3.4325(3.4309) | Xent 2.3026(2.3026) | Loss 3.4325(3.4309) | Error 0.8922(0.9000) Steps 910(914.52) | Grad Norm 2.4027(2.2703) | Total Time 14.00(14.00)\n",
      "Iter 8430 | Time 20.6776(20.9469) | Bit/dim 3.3994(3.4282) | Xent 2.3026(2.3026) | Loss 3.3994(3.4282) | Error 0.8833(0.9002) Steps 940(915.73) | Grad Norm 2.0230(2.1541) | Total Time 14.00(14.00)\n",
      "Iter 8440 | Time 21.2172(20.9239) | Bit/dim 3.4411(3.4283) | Xent 2.3026(2.3026) | Loss 3.4411(3.4283) | Error 0.8844(0.9002) Steps 934(918.65) | Grad Norm 2.0149(2.2621) | Total Time 14.00(14.00)\n",
      "Iter 8450 | Time 21.5149(20.9741) | Bit/dim 3.4549(3.4298) | Xent 2.3026(2.3026) | Loss 3.4549(3.4298) | Error 0.9078(0.9012) Steps 922(918.52) | Grad Norm 1.9872(2.2501) | Total Time 14.00(14.00)\n",
      "Iter 8460 | Time 20.6625(20.9512) | Bit/dim 3.4345(3.4291) | Xent 2.3026(2.3026) | Loss 3.4345(3.4291) | Error 0.8956(0.9001) Steps 916(918.43) | Grad Norm 2.1058(2.1525) | Total Time 14.00(14.00)\n",
      "Iter 8470 | Time 20.8698(20.9512) | Bit/dim 3.4451(3.4306) | Xent 2.3026(2.3026) | Loss 3.4451(3.4306) | Error 0.9000(0.8997) Steps 928(916.31) | Grad Norm 1.3807(2.2183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 108.0861, Epoch Time 1274.6292(1251.8936), Bit/dim 3.4310(best: 3.4318), Xent 2.3026, Loss 3.4310, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 20.2053(20.9609) | Bit/dim 3.3845(3.4290) | Xent 2.3026(2.3026) | Loss 3.3845(3.4290) | Error 0.9022(0.9007) Steps 898(916.33) | Grad Norm 1.6180(2.2468) | Total Time 14.00(14.00)\n",
      "Iter 8490 | Time 20.4287(20.9690) | Bit/dim 3.4322(3.4305) | Xent 2.3026(2.3026) | Loss 3.4322(3.4305) | Error 0.8867(0.9001) Steps 916(916.66) | Grad Norm 2.0677(2.1115) | Total Time 14.00(14.00)\n",
      "Iter 8500 | Time 20.6024(20.9468) | Bit/dim 3.4235(3.4293) | Xent 2.3026(2.3026) | Loss 3.4235(3.4293) | Error 0.8956(0.8991) Steps 934(917.53) | Grad Norm 2.2499(2.0675) | Total Time 14.00(14.00)\n",
      "Iter 8510 | Time 21.5729(20.9999) | Bit/dim 3.4805(3.4299) | Xent 2.3026(2.3026) | Loss 3.4805(3.4299) | Error 0.9122(0.8991) Steps 898(918.07) | Grad Norm 2.3728(2.2398) | Total Time 14.00(14.00)\n",
      "Iter 8520 | Time 21.2421(21.0158) | Bit/dim 3.3941(3.4295) | Xent 2.3026(2.3026) | Loss 3.3941(3.4295) | Error 0.9044(0.9006) Steps 910(915.41) | Grad Norm 1.5304(2.0803) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 109.6375, Epoch Time 1282.7503(1252.8193), Bit/dim 3.4371(best: 3.4310), Xent 2.3026, Loss 3.4371, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 20.9139(21.0328) | Bit/dim 3.4419(3.4290) | Xent 2.3026(2.3026) | Loss 3.4419(3.4290) | Error 0.8878(0.8993) Steps 904(915.78) | Grad Norm 1.8802(2.1801) | Total Time 14.00(14.00)\n",
      "Iter 8540 | Time 21.6087(21.0141) | Bit/dim 3.4567(3.4271) | Xent 2.3026(2.3026) | Loss 3.4567(3.4271) | Error 0.9022(0.9000) Steps 904(916.12) | Grad Norm 1.7295(2.1585) | Total Time 14.00(14.00)\n",
      "Iter 8550 | Time 20.4902(21.0456) | Bit/dim 3.4237(3.4292) | Xent 2.3026(2.3026) | Loss 3.4237(3.4292) | Error 0.8822(0.9004) Steps 922(916.42) | Grad Norm 2.3324(2.2063) | Total Time 14.00(14.00)\n",
      "Iter 8560 | Time 21.2145(21.0469) | Bit/dim 3.4196(3.4316) | Xent 2.3026(2.3026) | Loss 3.4196(3.4316) | Error 0.9056(0.9008) Steps 892(915.48) | Grad Norm 1.7869(2.1659) | Total Time 14.00(14.00)\n",
      "Iter 8570 | Time 21.0503(21.0280) | Bit/dim 3.4396(3.4305) | Xent 2.3026(2.3026) | Loss 3.4396(3.4305) | Error 0.9100(0.8999) Steps 904(915.58) | Grad Norm 1.5007(2.1410) | Total Time 14.00(14.00)\n",
      "Iter 8580 | Time 22.2246(21.1230) | Bit/dim 3.4327(3.4265) | Xent 2.3026(2.3026) | Loss 3.4327(3.4265) | Error 0.8778(0.8996) Steps 946(916.68) | Grad Norm 1.4226(2.1842) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 107.7553, Epoch Time 1283.0300(1253.7257), Bit/dim 3.4277(best: 3.4310), Xent 2.3026, Loss 3.4277, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 20.2571(21.0840) | Bit/dim 3.4093(3.4253) | Xent 2.3026(2.3026) | Loss 3.4093(3.4253) | Error 0.8900(0.8986) Steps 922(916.77) | Grad Norm 1.5739(2.1205) | Total Time 14.00(14.00)\n",
      "Iter 8600 | Time 21.4765(21.0500) | Bit/dim 3.4694(3.4283) | Xent 2.3026(2.3026) | Loss 3.4694(3.4283) | Error 0.9056(0.8997) Steps 898(914.98) | Grad Norm 1.7444(1.9856) | Total Time 14.00(14.00)\n",
      "Iter 8610 | Time 21.1890(21.0366) | Bit/dim 3.4364(3.4257) | Xent 2.3026(2.3026) | Loss 3.4364(3.4257) | Error 0.9011(0.8997) Steps 922(913.39) | Grad Norm 2.6853(2.1070) | Total Time 14.00(14.00)\n",
      "Iter 8620 | Time 21.2228(21.0591) | Bit/dim 3.4200(3.4241) | Xent 2.3026(2.3026) | Loss 3.4200(3.4241) | Error 0.8911(0.8988) Steps 916(915.52) | Grad Norm 1.3895(2.1672) | Total Time 14.00(14.00)\n",
      "Iter 8630 | Time 20.9905(21.0377) | Bit/dim 3.4624(3.4297) | Xent 2.3026(2.3026) | Loss 3.4624(3.4297) | Error 0.9167(0.9006) Steps 916(917.16) | Grad Norm 2.2273(2.1062) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 107.7385, Epoch Time 1279.3124(1254.4933), Bit/dim 3.4301(best: 3.4277), Xent 2.3026, Loss 3.4301, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 20.8972(21.0145) | Bit/dim 3.4473(3.4296) | Xent 2.3026(2.3026) | Loss 3.4473(3.4296) | Error 0.9111(0.9008) Steps 916(917.46) | Grad Norm 1.6086(2.1466) | Total Time 14.00(14.00)\n",
      "Iter 8650 | Time 20.5368(20.9567) | Bit/dim 3.4166(3.4290) | Xent 2.3026(2.3026) | Loss 3.4166(3.4290) | Error 0.9044(0.9004) Steps 910(917.48) | Grad Norm 1.6902(2.0327) | Total Time 14.00(14.00)\n",
      "Iter 8660 | Time 21.2342(20.9963) | Bit/dim 3.4547(3.4301) | Xent 2.3026(2.3026) | Loss 3.4547(3.4301) | Error 0.8967(0.9018) Steps 922(919.65) | Grad Norm 2.6742(2.2592) | Total Time 14.00(14.00)\n",
      "Iter 8670 | Time 21.4934(21.0412) | Bit/dim 3.4066(3.4312) | Xent 2.3026(2.3026) | Loss 3.4066(3.4312) | Error 0.9033(0.9009) Steps 922(920.10) | Grad Norm 2.0965(2.2009) | Total Time 14.00(14.00)\n",
      "Iter 8680 | Time 20.9434(20.9972) | Bit/dim 3.4033(3.4276) | Xent 2.3026(2.3026) | Loss 3.4033(3.4276) | Error 0.8900(0.8992) Steps 916(920.04) | Grad Norm 2.6290(2.1217) | Total Time 14.00(14.00)\n",
      "Iter 8690 | Time 20.3453(20.9701) | Bit/dim 3.4051(3.4246) | Xent 2.3026(2.3026) | Loss 3.4051(3.4246) | Error 0.9000(0.8992) Steps 928(920.87) | Grad Norm 1.2781(2.1495) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 109.7365, Epoch Time 1278.3473(1255.2089), Bit/dim 3.4284(best: 3.4277), Xent 2.3026, Loss 3.4284, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 20.6209(21.0050) | Bit/dim 3.4023(3.4228) | Xent 2.3026(2.3026) | Loss 3.4023(3.4228) | Error 0.9056(0.8990) Steps 910(919.65) | Grad Norm 1.7669(2.0797) | Total Time 14.00(14.00)\n",
      "Iter 8710 | Time 20.5780(21.0213) | Bit/dim 3.3836(3.4254) | Xent 2.3026(2.3026) | Loss 3.3836(3.4254) | Error 0.8822(0.8994) Steps 922(917.75) | Grad Norm 1.9875(2.1015) | Total Time 14.00(14.00)\n",
      "Iter 8720 | Time 20.4941(20.9928) | Bit/dim 3.4395(3.4267) | Xent 2.3026(2.3026) | Loss 3.4395(3.4267) | Error 0.9144(0.8996) Steps 916(917.78) | Grad Norm 2.2519(2.0808) | Total Time 14.00(14.00)\n",
      "Iter 8730 | Time 21.2311(21.0253) | Bit/dim 3.4491(3.4291) | Xent 2.3026(2.3026) | Loss 3.4491(3.4291) | Error 0.8978(0.9008) Steps 922(919.27) | Grad Norm 2.0662(2.2277) | Total Time 14.00(14.00)\n",
      "Iter 8740 | Time 21.2850(21.0248) | Bit/dim 3.4373(3.4265) | Xent 2.3026(2.3026) | Loss 3.4373(3.4265) | Error 0.8922(0.9005) Steps 910(918.87) | Grad Norm 1.9624(2.1044) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 108.6396, Epoch Time 1281.8327(1256.0076), Bit/dim 3.4305(best: 3.4277), Xent 2.3026, Loss 3.4305, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 21.2821(20.9880) | Bit/dim 3.4133(3.4248) | Xent 2.3026(2.3026) | Loss 3.4133(3.4248) | Error 0.9011(0.9003) Steps 916(919.96) | Grad Norm 1.2278(2.1412) | Total Time 14.00(14.00)\n",
      "Iter 8760 | Time 21.4727(21.0571) | Bit/dim 3.4284(3.4255) | Xent 2.3026(2.3026) | Loss 3.4284(3.4255) | Error 0.8878(0.9009) Steps 922(920.25) | Grad Norm 1.6123(2.0823) | Total Time 14.00(14.00)\n",
      "Iter 8770 | Time 20.8874(21.0521) | Bit/dim 3.4491(3.4268) | Xent 2.3026(2.3026) | Loss 3.4491(3.4268) | Error 0.8956(0.9004) Steps 910(919.69) | Grad Norm 1.5658(1.9773) | Total Time 14.00(14.00)\n",
      "Iter 8780 | Time 21.7851(21.1455) | Bit/dim 3.3821(3.4245) | Xent 2.3026(2.3026) | Loss 3.3821(3.4245) | Error 0.8922(0.8987) Steps 916(919.54) | Grad Norm 2.1128(2.1613) | Total Time 14.00(14.00)\n",
      "Iter 8790 | Time 21.1976(21.1488) | Bit/dim 3.4600(3.4262) | Xent 2.3026(2.3026) | Loss 3.4600(3.4262) | Error 0.8911(0.8990) Steps 910(919.90) | Grad Norm 2.6985(2.1171) | Total Time 14.00(14.00)\n",
      "Iter 8800 | Time 21.4498(21.1294) | Bit/dim 3.4074(3.4244) | Xent 2.3026(2.3026) | Loss 3.4074(3.4244) | Error 0.9056(0.9000) Steps 910(921.11) | Grad Norm 2.2654(2.0384) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 110.7422, Epoch Time 1290.6719(1257.0475), Bit/dim 3.4300(best: 3.4277), Xent 2.3026, Loss 3.4300, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 21.8889(21.1897) | Bit/dim 3.4122(3.4268) | Xent 2.3026(2.3026) | Loss 3.4122(3.4268) | Error 0.9067(0.8997) Steps 928(922.94) | Grad Norm 2.2710(2.1041) | Total Time 14.00(14.00)\n",
      "Iter 8820 | Time 21.1883(21.1499) | Bit/dim 3.4516(3.4261) | Xent 2.3026(2.3026) | Loss 3.4516(3.4261) | Error 0.8889(0.9009) Steps 940(922.63) | Grad Norm 1.5287(2.0494) | Total Time 14.00(14.00)\n",
      "Iter 8830 | Time 22.0883(21.1794) | Bit/dim 3.4277(3.4238) | Xent 2.3026(2.3026) | Loss 3.4277(3.4238) | Error 0.9100(0.9014) Steps 952(925.14) | Grad Norm 2.8674(2.2251) | Total Time 14.00(14.00)\n",
      "Iter 8840 | Time 20.7476(21.1510) | Bit/dim 3.3943(3.4214) | Xent 2.3026(2.3026) | Loss 3.3943(3.4214) | Error 0.8978(0.8998) Steps 910(923.68) | Grad Norm 2.5402(2.1352) | Total Time 14.00(14.00)\n",
      "Iter 8850 | Time 21.7641(21.1242) | Bit/dim 3.4300(3.4233) | Xent 2.3026(2.3026) | Loss 3.4300(3.4233) | Error 0.9133(0.9002) Steps 934(924.32) | Grad Norm 1.6966(2.2465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 109.5169, Epoch Time 1291.0307(1258.0670), Bit/dim 3.4245(best: 3.4277), Xent 2.3026, Loss 3.4245, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 21.5979(21.1482) | Bit/dim 3.4179(3.4246) | Xent 2.3026(2.3026) | Loss 3.4179(3.4246) | Error 0.9089(0.8999) Steps 940(923.41) | Grad Norm 0.9700(2.1179) | Total Time 14.00(14.00)\n",
      "Iter 8870 | Time 20.7120(21.1866) | Bit/dim 3.3906(3.4234) | Xent 2.3026(2.3026) | Loss 3.3906(3.4234) | Error 0.9078(0.9012) Steps 922(925.22) | Grad Norm 2.1534(2.0300) | Total Time 14.00(14.00)\n",
      "Iter 8880 | Time 20.6328(21.1957) | Bit/dim 3.4178(3.4254) | Xent 2.3026(2.3026) | Loss 3.4178(3.4254) | Error 0.8922(0.9005) Steps 922(924.09) | Grad Norm 1.7715(2.0564) | Total Time 14.00(14.00)\n",
      "Iter 8890 | Time 20.3260(21.1778) | Bit/dim 3.3922(3.4246) | Xent 2.3026(2.3026) | Loss 3.3922(3.4246) | Error 0.9044(0.8995) Steps 922(923.23) | Grad Norm 2.3021(2.1783) | Total Time 14.00(14.00)\n",
      "Iter 8900 | Time 20.6221(21.1715) | Bit/dim 3.4024(3.4234) | Xent 2.3026(2.3026) | Loss 3.4024(3.4234) | Error 0.8967(0.8987) Steps 916(922.57) | Grad Norm 2.3050(2.1461) | Total Time 14.00(14.00)\n",
      "Iter 8910 | Time 20.9954(21.1457) | Bit/dim 3.4349(3.4231) | Xent 2.3026(2.3026) | Loss 3.4349(3.4231) | Error 0.9122(0.8997) Steps 910(922.22) | Grad Norm 3.2382(2.1822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 110.9157, Epoch Time 1290.6222(1259.0437), Bit/dim 3.4312(best: 3.4245), Xent 2.3026, Loss 3.4312, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 20.4740(21.1585) | Bit/dim 3.4314(3.4229) | Xent 2.3026(2.3026) | Loss 3.4314(3.4229) | Error 0.8944(0.8997) Steps 904(922.85) | Grad Norm 2.5217(2.2237) | Total Time 14.00(14.00)\n",
      "Iter 8930 | Time 20.2392(21.1026) | Bit/dim 3.4393(3.4242) | Xent 2.3026(2.3026) | Loss 3.4393(3.4242) | Error 0.8911(0.9000) Steps 916(922.79) | Grad Norm 1.6588(2.0813) | Total Time 14.00(14.00)\n",
      "Iter 8940 | Time 21.2413(21.0929) | Bit/dim 3.4247(3.4214) | Xent 2.3026(2.3026) | Loss 3.4247(3.4214) | Error 0.8944(0.8992) Steps 922(921.27) | Grad Norm 1.6901(2.0655) | Total Time 14.00(14.00)\n",
      "Iter 8950 | Time 21.3868(21.1530) | Bit/dim 3.3878(3.4225) | Xent 2.3026(2.3026) | Loss 3.3878(3.4225) | Error 0.8967(0.8986) Steps 916(921.34) | Grad Norm 1.2407(2.1212) | Total Time 14.00(14.00)\n",
      "Iter 8960 | Time 21.4190(21.1145) | Bit/dim 3.4339(3.4228) | Xent 2.3026(2.3026) | Loss 3.4339(3.4228) | Error 0.9089(0.9000) Steps 934(921.41) | Grad Norm 2.6036(2.1655) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 110.2867, Epoch Time 1286.5479(1259.8688), Bit/dim 3.4277(best: 3.4245), Xent 2.3026, Loss 3.4277, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 20.7440(21.1005) | Bit/dim 3.4453(3.4236) | Xent 2.3026(2.3026) | Loss 3.4453(3.4236) | Error 0.8978(0.9002) Steps 940(922.06) | Grad Norm 1.5006(2.1952) | Total Time 14.00(14.00)\n",
      "Iter 8980 | Time 20.9822(21.1085) | Bit/dim 3.4208(3.4260) | Xent 2.3026(2.3026) | Loss 3.4208(3.4260) | Error 0.8967(0.9013) Steps 928(921.83) | Grad Norm 1.9235(2.1620) | Total Time 14.00(14.00)\n",
      "Iter 8990 | Time 20.9725(21.1092) | Bit/dim 3.4155(3.4251) | Xent 2.3026(2.3026) | Loss 3.4155(3.4251) | Error 0.9056(0.9001) Steps 904(921.80) | Grad Norm 1.8712(2.2456) | Total Time 14.00(14.00)\n",
      "Iter 9000 | Time 21.4090(21.1116) | Bit/dim 3.3643(3.4221) | Xent 2.3026(2.3026) | Loss 3.3643(3.4221) | Error 0.8889(0.8997) Steps 922(920.78) | Grad Norm 2.3617(2.1977) | Total Time 14.00(14.00)\n",
      "Iter 9010 | Time 20.7512(21.1172) | Bit/dim 3.4290(3.4208) | Xent 2.3026(2.3026) | Loss 3.4290(3.4208) | Error 0.8800(0.8995) Steps 916(920.80) | Grad Norm 1.6337(2.2674) | Total Time 14.00(14.00)\n",
      "Iter 9020 | Time 21.0727(21.1282) | Bit/dim 3.4505(3.4221) | Xent 2.3026(2.3026) | Loss 3.4505(3.4221) | Error 0.9044(0.9001) Steps 910(921.56) | Grad Norm 1.4324(2.1669) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 110.1712, Epoch Time 1288.6719(1260.7329), Bit/dim 3.4238(best: 3.4245), Xent 2.3026, Loss 3.4238, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 21.1089(21.1210) | Bit/dim 3.4239(3.4231) | Xent 2.3026(2.3026) | Loss 3.4239(3.4231) | Error 0.8944(0.8999) Steps 934(923.02) | Grad Norm 2.0872(2.0638) | Total Time 14.00(14.00)\n",
      "Iter 9040 | Time 21.4606(21.2249) | Bit/dim 3.4176(3.4217) | Xent 2.3026(2.3026) | Loss 3.4176(3.4217) | Error 0.8933(0.8995) Steps 952(925.44) | Grad Norm 1.8637(2.1318) | Total Time 14.00(14.00)\n",
      "Iter 9050 | Time 21.3307(21.1585) | Bit/dim 3.3797(3.4212) | Xent 2.3026(2.3026) | Loss 3.3797(3.4212) | Error 0.8689(0.8985) Steps 922(924.39) | Grad Norm 2.6013(2.1017) | Total Time 14.00(14.00)\n",
      "Iter 9060 | Time 20.5859(21.1465) | Bit/dim 3.4461(3.4217) | Xent 2.3026(2.3026) | Loss 3.4461(3.4217) | Error 0.9011(0.8991) Steps 928(923.51) | Grad Norm 2.0693(2.0808) | Total Time 14.00(14.00)\n",
      "Iter 9070 | Time 21.3177(21.1974) | Bit/dim 3.4061(3.4208) | Xent 2.3026(2.3026) | Loss 3.4061(3.4208) | Error 0.9056(0.9002) Steps 934(924.55) | Grad Norm 2.1821(2.1680) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 108.4988, Epoch Time 1290.7724(1261.6341), Bit/dim 3.4279(best: 3.4238), Xent 2.3026, Loss 3.4279, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 21.3524(21.1734) | Bit/dim 3.4574(3.4219) | Xent 2.3026(2.3026) | Loss 3.4574(3.4219) | Error 0.9067(0.9004) Steps 916(925.17) | Grad Norm 1.7409(2.2059) | Total Time 14.00(14.00)\n",
      "Iter 9090 | Time 21.0327(21.2253) | Bit/dim 3.4556(3.4222) | Xent 2.3026(2.3026) | Loss 3.4556(3.4222) | Error 0.9100(0.9008) Steps 934(924.33) | Grad Norm 1.2800(2.2700) | Total Time 14.00(14.00)\n",
      "Iter 9100 | Time 21.3970(21.2728) | Bit/dim 3.4172(3.4204) | Xent 2.3026(2.3026) | Loss 3.4172(3.4204) | Error 0.9044(0.8996) Steps 922(922.78) | Grad Norm 2.0104(2.2456) | Total Time 14.00(14.00)\n",
      "Iter 9110 | Time 21.0133(21.2432) | Bit/dim 3.4236(3.4199) | Xent 2.3026(2.3026) | Loss 3.4236(3.4199) | Error 0.8956(0.9001) Steps 934(924.07) | Grad Norm 2.6169(2.1617) | Total Time 14.00(14.00)\n",
      "Iter 9120 | Time 21.4471(21.1897) | Bit/dim 3.4195(3.4200) | Xent 2.3026(2.3026) | Loss 3.4195(3.4200) | Error 0.9200(0.9016) Steps 922(924.53) | Grad Norm 1.7071(2.1516) | Total Time 14.00(14.00)\n",
      "Iter 9130 | Time 21.0064(21.2029) | Bit/dim 3.4272(3.4210) | Xent 2.3026(2.3026) | Loss 3.4272(3.4210) | Error 0.9067(0.9004) Steps 916(923.44) | Grad Norm 2.6333(2.1235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 110.3337, Epoch Time 1294.0919(1262.6078), Bit/dim 3.4280(best: 3.4238), Xent 2.3026, Loss 3.4280, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 21.4095(21.2512) | Bit/dim 3.4361(3.4198) | Xent 2.3026(2.3026) | Loss 3.4361(3.4198) | Error 0.8922(0.9003) Steps 934(925.49) | Grad Norm 1.3719(2.1990) | Total Time 14.00(14.00)\n",
      "Iter 9150 | Time 21.4441(21.2251) | Bit/dim 3.4135(3.4220) | Xent 2.3026(2.3026) | Loss 3.4135(3.4220) | Error 0.9000(0.9007) Steps 922(925.19) | Grad Norm 2.3548(2.2272) | Total Time 14.00(14.00)\n",
      "Iter 9160 | Time 21.1627(21.2818) | Bit/dim 3.4354(3.4185) | Xent 2.3026(2.3026) | Loss 3.4354(3.4185) | Error 0.8922(0.9000) Steps 922(924.53) | Grad Norm 2.7134(2.1649) | Total Time 14.00(14.00)\n",
      "Iter 9170 | Time 21.3464(21.2538) | Bit/dim 3.4363(3.4171) | Xent 2.3026(2.3026) | Loss 3.4363(3.4171) | Error 0.9156(0.8995) Steps 934(925.73) | Grad Norm 1.8224(2.0965) | Total Time 14.00(14.00)\n",
      "Iter 9180 | Time 21.2296(21.2335) | Bit/dim 3.4790(3.4196) | Xent 2.3026(2.3026) | Loss 3.4790(3.4196) | Error 0.8933(0.8999) Steps 928(924.63) | Grad Norm 2.6969(2.1143) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 112.9803, Epoch Time 1300.0893(1263.7322), Bit/dim 3.4234(best: 3.4238), Xent 2.3026, Loss 3.4234, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 21.0522(21.2868) | Bit/dim 3.4321(3.4199) | Xent 2.3026(2.3026) | Loss 3.4321(3.4199) | Error 0.8878(0.9000) Steps 940(925.28) | Grad Norm 1.4725(2.1363) | Total Time 14.00(14.00)\n",
      "Iter 9200 | Time 20.7981(21.2733) | Bit/dim 3.4143(3.4203) | Xent 2.3026(2.3026) | Loss 3.4143(3.4203) | Error 0.8944(0.9008) Steps 934(925.10) | Grad Norm 1.6974(2.0745) | Total Time 14.00(14.00)\n",
      "Iter 9210 | Time 21.3503(21.2929) | Bit/dim 3.4504(3.4212) | Xent 2.3026(2.3026) | Loss 3.4504(3.4212) | Error 0.9033(0.8991) Steps 910(925.33) | Grad Norm 3.0844(2.0441) | Total Time 14.00(14.00)\n",
      "Iter 9220 | Time 21.6567(21.3063) | Bit/dim 3.4643(3.4208) | Xent 2.3026(2.3026) | Loss 3.4643(3.4208) | Error 0.9100(0.8985) Steps 952(926.15) | Grad Norm 2.7283(2.0930) | Total Time 14.00(14.00)\n",
      "Iter 9230 | Time 20.9093(21.2280) | Bit/dim 3.4299(3.4233) | Xent 2.3026(2.3026) | Loss 3.4299(3.4233) | Error 0.9167(0.9005) Steps 928(925.32) | Grad Norm 1.8989(2.1857) | Total Time 14.00(14.00)\n",
      "Iter 9240 | Time 20.4790(21.2214) | Bit/dim 3.4166(3.4183) | Xent 2.3026(2.3026) | Loss 3.4166(3.4183) | Error 0.9033(0.9005) Steps 928(925.54) | Grad Norm 2.9645(2.1920) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 108.5643, Epoch Time 1293.4126(1264.6227), Bit/dim 3.4277(best: 3.4234), Xent 2.3026, Loss 3.4277, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 21.7270(21.2209) | Bit/dim 3.3887(3.4181) | Xent 2.3026(2.3026) | Loss 3.3887(3.4181) | Error 0.9056(0.9002) Steps 934(925.78) | Grad Norm 1.2637(2.1308) | Total Time 14.00(14.00)\n",
      "Iter 9260 | Time 21.7840(21.3111) | Bit/dim 3.4069(3.4168) | Xent 2.3026(2.3026) | Loss 3.4069(3.4168) | Error 0.8967(0.9002) Steps 934(926.98) | Grad Norm 2.3745(2.1535) | Total Time 14.00(14.00)\n",
      "Iter 9270 | Time 21.0870(21.3228) | Bit/dim 3.4370(3.4222) | Xent 2.3026(2.3026) | Loss 3.4370(3.4222) | Error 0.8911(0.9007) Steps 922(926.09) | Grad Norm 1.8933(2.2301) | Total Time 14.00(14.00)\n",
      "Iter 9280 | Time 21.8825(21.3379) | Bit/dim 3.3850(3.4215) | Xent 2.3026(2.3026) | Loss 3.3850(3.4215) | Error 0.8811(0.8996) Steps 940(926.61) | Grad Norm 1.8638(2.1566) | Total Time 14.00(14.00)\n",
      "Iter 9290 | Time 20.7897(21.2626) | Bit/dim 3.4395(3.4184) | Xent 2.3026(2.3026) | Loss 3.4395(3.4184) | Error 0.8978(0.9001) Steps 910(926.42) | Grad Norm 2.0261(2.0714) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 109.4797, Epoch Time 1298.3779(1265.6353), Bit/dim 3.4214(best: 3.4234), Xent 2.3026, Loss 3.4214, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 21.5259(21.3078) | Bit/dim 3.4038(3.4175) | Xent 2.3026(2.3026) | Loss 3.4038(3.4175) | Error 0.9033(0.9008) Steps 946(927.02) | Grad Norm 1.7922(2.0589) | Total Time 14.00(14.00)\n",
      "Iter 9310 | Time 20.7646(21.3877) | Bit/dim 3.4515(3.4172) | Xent 2.3026(2.3026) | Loss 3.4515(3.4172) | Error 0.9078(0.9005) Steps 916(926.27) | Grad Norm 2.2977(2.0821) | Total Time 14.00(14.00)\n",
      "Iter 9320 | Time 20.0718(21.3183) | Bit/dim 3.4241(3.4186) | Xent 2.3026(2.3026) | Loss 3.4241(3.4186) | Error 0.9000(0.9003) Steps 916(925.66) | Grad Norm 1.8219(2.0728) | Total Time 14.00(14.00)\n",
      "Iter 9330 | Time 21.5378(21.2630) | Bit/dim 3.4593(3.4179) | Xent 2.3026(2.3026) | Loss 3.4593(3.4179) | Error 0.9033(0.9006) Steps 928(924.29) | Grad Norm 1.9880(2.0652) | Total Time 14.00(14.00)\n",
      "Iter 9340 | Time 21.5131(21.2460) | Bit/dim 3.4354(3.4177) | Xent 2.3026(2.3026) | Loss 3.4354(3.4177) | Error 0.8944(0.8998) Steps 934(922.90) | Grad Norm 2.3770(1.9329) | Total Time 14.00(14.00)\n",
      "Iter 9350 | Time 20.9882(21.2633) | Bit/dim 3.4281(3.4179) | Xent 2.3026(2.3026) | Loss 3.4281(3.4179) | Error 0.9089(0.9000) Steps 922(923.34) | Grad Norm 2.1319(1.9684) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 108.9643, Epoch Time 1296.5069(1266.5615), Bit/dim 3.4193(best: 3.4214), Xent 2.3026, Loss 3.4193, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 21.5141(21.3550) | Bit/dim 3.3916(3.4156) | Xent 2.3026(2.3026) | Loss 3.3916(3.4156) | Error 0.8722(0.8989) Steps 922(923.52) | Grad Norm 1.7195(2.0259) | Total Time 14.00(14.00)\n",
      "Iter 9370 | Time 21.3321(21.3430) | Bit/dim 3.4208(3.4161) | Xent 2.3026(2.3026) | Loss 3.4208(3.4161) | Error 0.8978(0.8993) Steps 916(923.33) | Grad Norm 2.9643(2.0130) | Total Time 14.00(14.00)\n",
      "Iter 9380 | Time 21.1361(21.4423) | Bit/dim 3.4085(3.4175) | Xent 2.3026(2.3026) | Loss 3.4085(3.4175) | Error 0.9156(0.8995) Steps 922(923.60) | Grad Norm 2.3967(2.1274) | Total Time 14.00(14.00)\n",
      "Iter 9390 | Time 21.3932(21.4210) | Bit/dim 3.4336(3.4203) | Xent 2.3026(2.3026) | Loss 3.4336(3.4203) | Error 0.9022(0.9000) Steps 928(923.99) | Grad Norm 1.8655(2.0407) | Total Time 14.00(14.00)\n",
      "Iter 9400 | Time 21.1342(21.3783) | Bit/dim 3.4238(3.4195) | Xent 2.3026(2.3026) | Loss 3.4238(3.4195) | Error 0.8967(0.9001) Steps 934(927.29) | Grad Norm 2.7503(2.1211) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 110.8941, Epoch Time 1307.6168(1267.7931), Bit/dim 3.4178(best: 3.4193), Xent 2.3026, Loss 3.4178, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 20.8560(21.3915) | Bit/dim 3.4127(3.4183) | Xent 2.3026(2.3026) | Loss 3.4127(3.4183) | Error 0.8933(0.8999) Steps 922(926.58) | Grad Norm 3.3088(2.1941) | Total Time 14.00(14.00)\n",
      "Iter 9420 | Time 21.4637(21.3643) | Bit/dim 3.4159(3.4176) | Xent 2.3026(2.3026) | Loss 3.4159(3.4176) | Error 0.9056(0.9008) Steps 910(924.58) | Grad Norm 2.5113(2.1920) | Total Time 14.00(14.00)\n",
      "Iter 9430 | Time 21.3206(21.3506) | Bit/dim 3.4339(3.4186) | Xent 2.3026(2.3026) | Loss 3.4339(3.4186) | Error 0.9144(0.9013) Steps 940(923.63) | Grad Norm 1.6822(2.1777) | Total Time 14.00(14.00)\n",
      "Iter 9440 | Time 21.3276(21.3354) | Bit/dim 3.3772(3.4147) | Xent 2.3026(2.3026) | Loss 3.3772(3.4147) | Error 0.9000(0.8999) Steps 922(925.28) | Grad Norm 1.8923(2.0175) | Total Time 14.00(14.00)\n",
      "Iter 9450 | Time 21.0154(21.3222) | Bit/dim 3.4365(3.4176) | Xent 2.3026(2.3026) | Loss 3.4365(3.4176) | Error 0.9089(0.9000) Steps 922(925.78) | Grad Norm 2.5880(1.9817) | Total Time 14.00(14.00)\n",
      "Iter 9460 | Time 20.9893(21.3165) | Bit/dim 3.4394(3.4179) | Xent 2.3026(2.3026) | Loss 3.4394(3.4179) | Error 0.9111(0.9001) Steps 916(925.18) | Grad Norm 1.6549(2.1375) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 110.7380, Epoch Time 1298.6914(1268.7201), Bit/dim 3.4233(best: 3.4178), Xent 2.3026, Loss 3.4233, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 21.2122(21.3298) | Bit/dim 3.4214(3.4170) | Xent 2.3026(2.3026) | Loss 3.4214(3.4170) | Error 0.9056(0.9002) Steps 928(924.95) | Grad Norm 1.5492(2.0679) | Total Time 14.00(14.00)\n",
      "Iter 9480 | Time 21.3427(21.4191) | Bit/dim 3.3657(3.4145) | Xent 2.3026(2.3026) | Loss 3.3657(3.4145) | Error 0.8922(0.8995) Steps 940(928.59) | Grad Norm 2.6279(2.1721) | Total Time 14.00(14.00)\n",
      "Iter 9490 | Time 21.6659(21.4465) | Bit/dim 3.4340(3.4154) | Xent 2.3026(2.3026) | Loss 3.4340(3.4154) | Error 0.9089(0.8999) Steps 934(930.22) | Grad Norm 1.5273(2.0526) | Total Time 14.00(14.00)\n",
      "Iter 9500 | Time 21.6854(21.4537) | Bit/dim 3.4800(3.4172) | Xent 2.3026(2.3026) | Loss 3.4800(3.4172) | Error 0.9189(0.8999) Steps 958(931.52) | Grad Norm 3.5527(2.2491) | Total Time 14.00(14.00)\n",
      "Iter 9510 | Time 21.9743(21.5209) | Bit/dim 3.4221(3.4199) | Xent 2.3026(2.3026) | Loss 3.4221(3.4199) | Error 0.9056(0.9000) Steps 928(931.14) | Grad Norm 2.1708(2.2898) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 112.8674, Epoch Time 1314.4746(1270.0927), Bit/dim 3.4200(best: 3.4178), Xent 2.3026, Loss 3.4200, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 20.7692(21.4920) | Bit/dim 3.4048(3.4182) | Xent 2.3026(2.3026) | Loss 3.4048(3.4182) | Error 0.9000(0.8995) Steps 916(930.95) | Grad Norm 1.9426(2.1471) | Total Time 14.00(14.00)\n",
      "Iter 9530 | Time 21.5925(21.4914) | Bit/dim 3.4211(3.4164) | Xent 2.3026(2.3026) | Loss 3.4211(3.4164) | Error 0.9111(0.8999) Steps 928(929.97) | Grad Norm 2.1016(2.1710) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_015625_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.015625 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
