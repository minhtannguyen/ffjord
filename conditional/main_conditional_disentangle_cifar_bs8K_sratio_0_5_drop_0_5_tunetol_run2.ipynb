{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        for i in range(1,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run2/epoch_250_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 21.5835(22.4317) | Bit/dim 3.5952(3.5739) | Xent 0.1608(0.2164) | Loss 3.6756(3.6821) | Error 0.0456(0.0757) Steps 898(894.63) | Grad Norm 1.3370(3.8687) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 21.1694(22.2319) | Bit/dim 3.5880(3.5710) | Xent 0.1158(0.1982) | Loss 3.6460(3.6701) | Error 0.0344(0.0690) Steps 880(894.63) | Grad Norm 1.0456(3.2276) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 21.5531(22.0684) | Bit/dim 3.5504(3.5664) | Xent 0.1236(0.1824) | Loss 3.6122(3.6576) | Error 0.0478(0.0637) Steps 880(890.82) | Grad Norm 1.2966(2.7322) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 21.7826(21.9666) | Bit/dim 3.5337(3.5645) | Xent 0.1402(0.1674) | Loss 3.6038(3.6482) | Error 0.0478(0.0581) Steps 898(891.65) | Grad Norm 1.5684(2.3941) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 21.8964(21.9652) | Bit/dim 3.5569(3.5613) | Xent 0.1410(0.1572) | Loss 3.6275(3.6399) | Error 0.0500(0.0547) Steps 874(892.76) | Grad Norm 1.5311(2.0916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 108.0770, Epoch Time 1349.6821(1246.3189), Bit/dim 3.5630(best: inf), Xent 0.8816, Loss 4.0038, Error 0.2214(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 22.4564(21.9833) | Bit/dim 3.5580(3.5608) | Xent 0.1193(0.1497) | Loss 3.6177(3.6356) | Error 0.0400(0.0516) Steps 892(891.26) | Grad Norm 1.2586(1.8822) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 21.3430(21.9497) | Bit/dim 3.5865(3.5579) | Xent 0.1259(0.1428) | Loss 3.6495(3.6293) | Error 0.0444(0.0492) Steps 880(889.38) | Grad Norm 1.2979(1.7365) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 22.1537(21.8978) | Bit/dim 3.5598(3.5557) | Xent 0.0976(0.1368) | Loss 3.6086(3.6241) | Error 0.0300(0.0473) Steps 880(889.81) | Grad Norm 1.1531(1.5957) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 21.4202(21.7892) | Bit/dim 3.5774(3.5555) | Xent 0.1292(0.1317) | Loss 3.6420(3.6214) | Error 0.0500(0.0455) Steps 874(886.80) | Grad Norm 1.6488(1.5296) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 21.3946(21.6890) | Bit/dim 3.5277(3.5542) | Xent 0.1095(0.1301) | Loss 3.5825(3.6193) | Error 0.0333(0.0451) Steps 880(884.29) | Grad Norm 1.1153(1.4698) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 22.5232(21.7145) | Bit/dim 3.5365(3.5535) | Xent 0.1166(0.1270) | Loss 3.5948(3.6170) | Error 0.0422(0.0440) Steps 886(886.40) | Grad Norm 1.2235(1.4196) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 101.5808, Epoch Time 1313.7823(1248.3428), Bit/dim 3.5595(best: 3.5630), Xent 0.9099, Loss 4.0145, Error 0.2221(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 21.0147(21.7249) | Bit/dim 3.5639(3.5536) | Xent 0.1292(0.1233) | Loss 3.6285(3.6152) | Error 0.0467(0.0429) Steps 856(886.85) | Grad Norm 1.1537(1.3596) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 21.3580(21.6575) | Bit/dim 3.5594(3.5503) | Xent 0.1242(0.1213) | Loss 3.6215(3.6109) | Error 0.0356(0.0419) Steps 892(885.26) | Grad Norm 1.5291(1.3375) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 22.1083(21.7003) | Bit/dim 3.5548(3.5507) | Xent 0.1070(0.1194) | Loss 3.6083(3.6105) | Error 0.0300(0.0412) Steps 910(886.25) | Grad Norm 1.2906(1.3559) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 22.1908(21.7000) | Bit/dim 3.5671(3.5515) | Xent 0.1032(0.1181) | Loss 3.6187(3.6105) | Error 0.0389(0.0406) Steps 868(885.80) | Grad Norm 0.9886(1.3356) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 22.0048(21.6753) | Bit/dim 3.5465(3.5521) | Xent 0.1277(0.1158) | Loss 3.6103(3.6101) | Error 0.0467(0.0395) Steps 874(884.07) | Grad Norm 1.2480(1.3073) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 103.6013, Epoch Time 1315.5592(1250.3593), Bit/dim 3.5589(best: 3.5595), Xent 0.9194, Loss 4.0186, Error 0.2233(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 21.2429(21.6959) | Bit/dim 3.5456(3.5518) | Xent 0.1193(0.1158) | Loss 3.6053(3.6097) | Error 0.0389(0.0394) Steps 904(885.79) | Grad Norm 1.2169(1.2685) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 21.4315(21.6897) | Bit/dim 3.5447(3.5510) | Xent 0.0845(0.1137) | Loss 3.5869(3.6079) | Error 0.0244(0.0386) Steps 886(887.94) | Grad Norm 1.1385(1.2306) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 21.5610(21.7636) | Bit/dim 3.5527(3.5513) | Xent 0.1266(0.1157) | Loss 3.6160(3.6092) | Error 0.0456(0.0393) Steps 868(888.92) | Grad Norm 1.3361(1.2602) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 21.7502(21.8375) | Bit/dim 3.5560(3.5526) | Xent 0.1222(0.1147) | Loss 3.6172(3.6100) | Error 0.0444(0.0393) Steps 880(889.38) | Grad Norm 1.3531(1.2567) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 20.7326(21.8830) | Bit/dim 3.5745(3.5522) | Xent 0.1060(0.1138) | Loss 3.6275(3.6091) | Error 0.0367(0.0389) Steps 904(889.14) | Grad Norm 1.3603(1.2991) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 22.0423(21.9223) | Bit/dim 3.5383(3.5485) | Xent 0.1170(0.1159) | Loss 3.5968(3.6065) | Error 0.0433(0.0400) Steps 898(888.79) | Grad Norm 1.3178(1.3122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 103.6475, Epoch Time 1328.7207(1252.7102), Bit/dim 3.5597(best: 3.5589), Xent 0.9299, Loss 4.0246, Error 0.2214(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 22.0892(21.9327) | Bit/dim 3.5466(3.5481) | Xent 0.0877(0.1138) | Loss 3.5905(3.6051) | Error 0.0311(0.0391) Steps 892(890.66) | Grad Norm 1.0389(1.3065) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 21.7646(21.8528) | Bit/dim 3.5465(3.5477) | Xent 0.1337(0.1136) | Loss 3.6133(3.6045) | Error 0.0411(0.0388) Steps 898(890.74) | Grad Norm 1.3517(1.2975) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 21.7539(21.8223) | Bit/dim 3.5692(3.5522) | Xent 0.1030(0.1129) | Loss 3.6207(3.6087) | Error 0.0333(0.0384) Steps 874(890.10) | Grad Norm 1.2562(1.2932) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 21.0289(21.7977) | Bit/dim 3.5618(3.5535) | Xent 0.1086(0.1095) | Loss 3.6162(3.6083) | Error 0.0344(0.0374) Steps 874(888.87) | Grad Norm 1.2391(1.2891) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 21.2921(21.7455) | Bit/dim 3.5157(3.5476) | Xent 0.1052(0.1102) | Loss 3.5683(3.6027) | Error 0.0389(0.0380) Steps 886(888.06) | Grad Norm 1.1762(1.3366) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 104.2395, Epoch Time 1318.6189(1254.6874), Bit/dim 3.5577(best: 3.5589), Xent 0.9403, Loss 4.0279, Error 0.2247(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 21.7471(21.7579) | Bit/dim 3.5639(3.5487) | Xent 0.1055(0.1077) | Loss 3.6166(3.6026) | Error 0.0389(0.0371) Steps 874(887.86) | Grad Norm 1.6173(1.3309) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 22.5294(21.8634) | Bit/dim 3.5422(3.5477) | Xent 0.0951(0.1058) | Loss 3.5898(3.6006) | Error 0.0367(0.0368) Steps 904(889.78) | Grad Norm 1.1675(1.2803) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 22.1928(21.9801) | Bit/dim 3.5409(3.5487) | Xent 0.0952(0.1052) | Loss 3.5885(3.6012) | Error 0.0322(0.0363) Steps 922(892.61) | Grad Norm 1.1026(1.2737) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 21.6156(21.9080) | Bit/dim 3.5645(3.5485) | Xent 0.0825(0.1081) | Loss 3.6058(3.6026) | Error 0.0300(0.0374) Steps 892(892.08) | Grad Norm 1.2555(1.3074) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 21.6208(21.8391) | Bit/dim 3.5725(3.5492) | Xent 0.1171(0.1093) | Loss 3.6311(3.6039) | Error 0.0389(0.0374) Steps 886(890.89) | Grad Norm 1.6741(1.3453) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 20.8629(21.8258) | Bit/dim 3.5745(3.5483) | Xent 0.1009(0.1081) | Loss 3.6250(3.6023) | Error 0.0378(0.0369) Steps 880(891.90) | Grad Norm 1.9124(1.3658) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 103.9522, Epoch Time 1328.1174(1256.8903), Bit/dim 3.5577(best: 3.5577), Xent 0.9583, Loss 4.0368, Error 0.2255(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 22.2833(21.8684) | Bit/dim 3.5612(3.5488) | Xent 0.0970(0.1076) | Loss 3.6097(3.6026) | Error 0.0278(0.0362) Steps 904(891.66) | Grad Norm 1.3488(1.3695) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 21.5552(21.8906) | Bit/dim 3.5732(3.5477) | Xent 0.1104(0.1067) | Loss 3.6284(3.6011) | Error 0.0356(0.0356) Steps 880(890.86) | Grad Norm 1.4210(1.3434) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 22.0254(21.8991) | Bit/dim 3.5687(3.5501) | Xent 0.1313(0.1074) | Loss 3.6344(3.6038) | Error 0.0433(0.0360) Steps 862(888.74) | Grad Norm 1.7380(1.3584) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 22.1813(21.8261) | Bit/dim 3.5295(3.5480) | Xent 0.1019(0.1070) | Loss 3.5804(3.6015) | Error 0.0400(0.0360) Steps 898(888.37) | Grad Norm 1.2446(1.3929) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 21.9249(21.9242) | Bit/dim 3.5343(3.5484) | Xent 0.1248(0.1088) | Loss 3.5967(3.6028) | Error 0.0356(0.0364) Steps 904(890.64) | Grad Norm 1.6155(1.3987) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 104.4670, Epoch Time 1330.3649(1259.0946), Bit/dim 3.5559(best: 3.5577), Xent 0.9430, Loss 4.0274, Error 0.2240(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 21.9571(21.9341) | Bit/dim 3.5364(3.5465) | Xent 0.1209(0.1092) | Loss 3.5968(3.6011) | Error 0.0367(0.0366) Steps 898(891.12) | Grad Norm 1.1868(1.4189) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 21.8287(21.8800) | Bit/dim 3.5581(3.5491) | Xent 0.1131(0.1082) | Loss 3.6147(3.6032) | Error 0.0311(0.0360) Steps 922(891.05) | Grad Norm 1.6107(1.4079) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 22.0216(21.8544) | Bit/dim 3.5655(3.5483) | Xent 0.1181(0.1081) | Loss 3.6245(3.6023) | Error 0.0400(0.0363) Steps 886(892.32) | Grad Norm 1.3631(1.4087) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 21.9293(21.7856) | Bit/dim 3.5442(3.5476) | Xent 0.0948(0.1056) | Loss 3.5916(3.6004) | Error 0.0244(0.0351) Steps 886(891.85) | Grad Norm 1.3988(1.3870) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 21.8098(21.8194) | Bit/dim 3.5038(3.5463) | Xent 0.1117(0.1056) | Loss 3.5597(3.5991) | Error 0.0378(0.0353) Steps 886(892.71) | Grad Norm 1.5754(1.3821) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 22.5026(21.9134) | Bit/dim 3.5425(3.5461) | Xent 0.1325(0.1058) | Loss 3.6088(3.5990) | Error 0.0511(0.0355) Steps 916(895.56) | Grad Norm 1.5795(1.3781) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 105.4223, Epoch Time 1326.5400(1261.1179), Bit/dim 3.5577(best: 3.5559), Xent 0.9865, Loss 4.0509, Error 0.2254(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 21.9176(21.8865) | Bit/dim 3.5366(3.5465) | Xent 0.1035(0.1053) | Loss 3.5884(3.5991) | Error 0.0344(0.0354) Steps 922(896.25) | Grad Norm 1.4761(1.4031) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 21.8456(21.8576) | Bit/dim 3.5332(3.5451) | Xent 0.1135(0.1055) | Loss 3.5899(3.5979) | Error 0.0378(0.0353) Steps 916(896.09) | Grad Norm 1.0915(1.4134) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 22.0295(21.8433) | Bit/dim 3.5621(3.5461) | Xent 0.1214(0.1069) | Loss 3.6228(3.5995) | Error 0.0411(0.0362) Steps 898(895.44) | Grad Norm 2.0043(1.4307) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 22.0519(21.8423) | Bit/dim 3.5563(3.5482) | Xent 0.1205(0.1069) | Loss 3.6165(3.6016) | Error 0.0400(0.0362) Steps 904(896.79) | Grad Norm 1.6716(1.4635) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 20.7397(21.8389) | Bit/dim 3.5558(3.5466) | Xent 0.1136(0.1090) | Loss 3.6126(3.6011) | Error 0.0344(0.0370) Steps 898(896.32) | Grad Norm 1.4204(1.4763) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 104.9159, Epoch Time 1323.2681(1262.9824), Bit/dim 3.5574(best: 3.5559), Xent 0.9772, Loss 4.0460, Error 0.2257(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 21.4413(21.7455) | Bit/dim 3.5452(3.5472) | Xent 0.1150(0.1059) | Loss 3.6027(3.6002) | Error 0.0433(0.0362) Steps 916(894.97) | Grad Norm 1.4550(1.4486) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 21.4091(21.7094) | Bit/dim 3.5290(3.5470) | Xent 0.1180(0.1065) | Loss 3.5880(3.6002) | Error 0.0389(0.0364) Steps 880(893.87) | Grad Norm 1.9418(1.4465) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 21.6612(21.7514) | Bit/dim 3.5364(3.5459) | Xent 0.1111(0.1043) | Loss 3.5920(3.5981) | Error 0.0356(0.0353) Steps 898(894.92) | Grad Norm 1.4133(1.4106) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 21.7064(21.7678) | Bit/dim 3.5248(3.5453) | Xent 0.0931(0.1049) | Loss 3.5714(3.5978) | Error 0.0289(0.0353) Steps 898(895.04) | Grad Norm 1.4345(1.4064) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 22.3688(21.8714) | Bit/dim 3.5639(3.5468) | Xent 0.1116(0.1043) | Loss 3.6197(3.5989) | Error 0.0378(0.0351) Steps 916(896.62) | Grad Norm 1.5521(1.4255) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 22.5417(21.9156) | Bit/dim 3.5492(3.5462) | Xent 0.1044(0.1035) | Loss 3.6014(3.5980) | Error 0.0344(0.0345) Steps 892(897.99) | Grad Norm 1.4041(1.4708) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 105.7589, Epoch Time 1326.1346(1264.8770), Bit/dim 3.5565(best: 3.5559), Xent 0.9856, Loss 4.0492, Error 0.2250(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 21.2022(21.8143) | Bit/dim 3.5422(3.5468) | Xent 0.1086(0.1049) | Loss 3.5965(3.5993) | Error 0.0467(0.0357) Steps 904(895.84) | Grad Norm 1.2699(1.4477) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 21.7005(21.8750) | Bit/dim 3.5357(3.5456) | Xent 0.1457(0.1062) | Loss 3.6086(3.5987) | Error 0.0467(0.0365) Steps 898(898.06) | Grad Norm 1.4475(1.4524) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 22.0036(21.9199) | Bit/dim 3.5362(3.5447) | Xent 0.1030(0.1051) | Loss 3.5877(3.5972) | Error 0.0411(0.0362) Steps 916(900.44) | Grad Norm 1.5748(1.4375) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 22.3268(21.9068) | Bit/dim 3.5701(3.5475) | Xent 0.0933(0.1048) | Loss 3.6168(3.5999) | Error 0.0300(0.0355) Steps 886(897.15) | Grad Norm 1.2295(1.3929) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 21.5673(21.9143) | Bit/dim 3.5627(3.5465) | Xent 0.1139(0.1042) | Loss 3.6196(3.5986) | Error 0.0378(0.0349) Steps 886(898.48) | Grad Norm 1.1420(1.3632) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 105.2091, Epoch Time 1328.8121(1266.7951), Bit/dim 3.5555(best: 3.5559), Xent 0.9942, Loss 4.0526, Error 0.2300(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 21.6835(21.9058) | Bit/dim 3.5428(3.5463) | Xent 0.1034(0.1034) | Loss 3.5945(3.5980) | Error 0.0300(0.0343) Steps 928(898.22) | Grad Norm 1.1599(1.3415) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 21.4997(21.9038) | Bit/dim 3.5511(3.5471) | Xent 0.0985(0.1015) | Loss 3.6004(3.5978) | Error 0.0267(0.0334) Steps 892(898.62) | Grad Norm 1.9145(1.3611) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 21.8435(21.8894) | Bit/dim 3.5785(3.5467) | Xent 0.1013(0.1003) | Loss 3.6292(3.5968) | Error 0.0389(0.0337) Steps 892(898.65) | Grad Norm 1.2918(1.3642) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 21.5863(21.9798) | Bit/dim 3.5341(3.5461) | Xent 0.0785(0.1008) | Loss 3.5734(3.5964) | Error 0.0211(0.0337) Steps 898(898.74) | Grad Norm 0.9955(1.3347) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 22.5614(21.9901) | Bit/dim 3.5738(3.5475) | Xent 0.1252(0.1034) | Loss 3.6364(3.5992) | Error 0.0378(0.0343) Steps 898(898.41) | Grad Norm 2.7167(1.3975) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 23.1086(22.1067) | Bit/dim 3.5203(3.5454) | Xent 0.0894(0.1030) | Loss 3.5650(3.5969) | Error 0.0311(0.0341) Steps 934(902.13) | Grad Norm 1.5025(1.4524) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 106.4532, Epoch Time 1338.0474(1268.9326), Bit/dim 3.5573(best: 3.5555), Xent 0.9925, Loss 4.0536, Error 0.2266(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 22.3254(22.1045) | Bit/dim 3.5552(3.5461) | Xent 0.0974(0.1019) | Loss 3.6039(3.5970) | Error 0.0333(0.0340) Steps 922(903.59) | Grad Norm 1.2214(1.4711) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 22.0305(22.0506) | Bit/dim 3.5302(3.5463) | Xent 0.1012(0.1015) | Loss 3.5808(3.5970) | Error 0.0311(0.0339) Steps 886(903.03) | Grad Norm 1.1046(1.4637) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 22.1965(22.0420) | Bit/dim 3.5343(3.5478) | Xent 0.1200(0.1026) | Loss 3.5943(3.5991) | Error 0.0433(0.0344) Steps 922(905.20) | Grad Norm 1.6761(1.4466) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 21.5379(22.0118) | Bit/dim 3.5398(3.5476) | Xent 0.1385(0.1034) | Loss 3.6091(3.5993) | Error 0.0478(0.0351) Steps 904(904.39) | Grad Norm 1.8143(1.4356) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 21.9577(22.0131) | Bit/dim 3.5160(3.5431) | Xent 0.0850(0.1024) | Loss 3.5585(3.5942) | Error 0.0322(0.0344) Steps 898(903.34) | Grad Norm 1.6766(1.4781) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 107.1355, Epoch Time 1334.6577(1270.9044), Bit/dim 3.5561(best: 3.5555), Xent 0.9876, Loss 4.0499, Error 0.2273(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 21.7064(21.9720) | Bit/dim 3.5595(3.5458) | Xent 0.1003(0.1031) | Loss 3.6097(3.5974) | Error 0.0389(0.0353) Steps 898(902.64) | Grad Norm 1.4111(1.4972) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 22.3328(22.0598) | Bit/dim 3.5300(3.5454) | Xent 0.1018(0.1020) | Loss 3.5809(3.5964) | Error 0.0378(0.0352) Steps 922(903.76) | Grad Norm 1.4213(1.4726) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 21.8375(22.0315) | Bit/dim 3.5494(3.5470) | Xent 0.0825(0.1008) | Loss 3.5907(3.5974) | Error 0.0289(0.0345) Steps 916(905.76) | Grad Norm 1.3914(1.5160) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 21.6823(21.9729) | Bit/dim 3.5846(3.5470) | Xent 0.0918(0.0999) | Loss 3.6305(3.5969) | Error 0.0300(0.0338) Steps 898(904.67) | Grad Norm 1.1788(1.5390) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 21.4066(21.9081) | Bit/dim 3.5539(3.5446) | Xent 0.0841(0.1007) | Loss 3.5960(3.5950) | Error 0.0244(0.0341) Steps 892(903.79) | Grad Norm 1.3732(1.4990) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 23.9567(21.8686) | Bit/dim 3.5668(3.5441) | Xent 0.0876(0.1006) | Loss 3.6105(3.5944) | Error 0.0356(0.0342) Steps 904(902.32) | Grad Norm 1.1535(1.5276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 105.6614, Epoch Time 1329.9848(1272.6768), Bit/dim 3.5553(best: 3.5555), Xent 1.0163, Loss 4.0635, Error 0.2317(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 21.9745(21.9008) | Bit/dim 3.5192(3.5424) | Xent 0.1050(0.1028) | Loss 3.5717(3.5938) | Error 0.0389(0.0354) Steps 892(903.15) | Grad Norm 1.3083(1.5644) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 21.6960(21.9156) | Bit/dim 3.5143(3.5422) | Xent 0.1077(0.0989) | Loss 3.5682(3.5916) | Error 0.0356(0.0343) Steps 898(902.24) | Grad Norm 1.7471(1.5097) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 21.2581(21.8077) | Bit/dim 3.5460(3.5434) | Xent 0.1022(0.0996) | Loss 3.5971(3.5932) | Error 0.0333(0.0345) Steps 892(899.33) | Grad Norm 1.0903(1.4848) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 22.1811(21.9442) | Bit/dim 3.5608(3.5451) | Xent 0.0904(0.1007) | Loss 3.6060(3.5954) | Error 0.0311(0.0348) Steps 892(901.96) | Grad Norm 2.2217(1.5563) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 22.0269(21.9756) | Bit/dim 3.5739(3.5448) | Xent 0.1086(0.0998) | Loss 3.6282(3.5947) | Error 0.0344(0.0343) Steps 892(903.12) | Grad Norm 1.4908(1.5407) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 106.2490, Epoch Time 1333.4790(1274.5009), Bit/dim 3.5550(best: 3.5553), Xent 1.0272, Loss 4.0686, Error 0.2312(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 21.1404(21.9205) | Bit/dim 3.5269(3.5450) | Xent 0.0869(0.0997) | Loss 3.5703(3.5948) | Error 0.0289(0.0344) Steps 886(901.67) | Grad Norm 1.2267(1.4991) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 22.3932(21.9515) | Bit/dim 3.5514(3.5455) | Xent 0.1036(0.0998) | Loss 3.6031(3.5954) | Error 0.0411(0.0347) Steps 898(900.65) | Grad Norm 1.5505(1.4743) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 22.8410(22.0194) | Bit/dim 3.5235(3.5448) | Xent 0.0839(0.1000) | Loss 3.5655(3.5949) | Error 0.0311(0.0346) Steps 892(902.02) | Grad Norm 1.7859(1.5161) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 22.0616(22.0732) | Bit/dim 3.5582(3.5456) | Xent 0.0896(0.0984) | Loss 3.6030(3.5949) | Error 0.0311(0.0337) Steps 874(900.14) | Grad Norm 1.1779(1.5051) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 21.7529(22.0448) | Bit/dim 3.5326(3.5465) | Xent 0.0973(0.0989) | Loss 3.5812(3.5959) | Error 0.0367(0.0339) Steps 898(898.22) | Grad Norm 1.4572(1.5528) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 21.8141(21.9613) | Bit/dim 3.5471(3.5455) | Xent 0.0828(0.1000) | Loss 3.5885(3.5955) | Error 0.0244(0.0343) Steps 886(897.29) | Grad Norm 1.7052(1.5905) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 105.4411, Epoch Time 1333.8472(1276.2812), Bit/dim 3.5561(best: 3.5550), Xent 1.0165, Loss 4.0643, Error 0.2283(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 21.5323(21.9607) | Bit/dim 3.5308(3.5459) | Xent 0.0812(0.0994) | Loss 3.5714(3.5956) | Error 0.0256(0.0337) Steps 898(898.59) | Grad Norm 1.0585(1.5572) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 22.0669(21.9822) | Bit/dim 3.5610(3.5455) | Xent 0.1029(0.0990) | Loss 3.6124(3.5951) | Error 0.0367(0.0333) Steps 880(896.98) | Grad Norm 1.3964(1.5714) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 21.7606(21.8800) | Bit/dim 3.5422(3.5471) | Xent 0.0827(0.0984) | Loss 3.5835(3.5963) | Error 0.0278(0.0333) Steps 922(898.16) | Grad Norm 1.6167(1.5430) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 22.0539(21.9108) | Bit/dim 3.5410(3.5469) | Xent 0.1140(0.0998) | Loss 3.5980(3.5968) | Error 0.0444(0.0342) Steps 910(899.32) | Grad Norm 1.2954(1.5468) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 21.9395(21.8631) | Bit/dim 3.5556(3.5444) | Xent 0.0902(0.1002) | Loss 3.6007(3.5945) | Error 0.0244(0.0339) Steps 904(899.82) | Grad Norm 1.2815(1.5600) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 106.1504, Epoch Time 1329.4464(1277.8762), Bit/dim 3.5537(best: 3.5550), Xent 1.0136, Loss 4.0605, Error 0.2280(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 22.2884(21.8695) | Bit/dim 3.5344(3.5447) | Xent 0.0866(0.0997) | Loss 3.5777(3.5946) | Error 0.0278(0.0338) Steps 886(900.27) | Grad Norm 1.5626(1.6062) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 21.6286(21.9063) | Bit/dim 3.5280(3.5455) | Xent 0.1170(0.0998) | Loss 3.5865(3.5954) | Error 0.0433(0.0343) Steps 910(903.45) | Grad Norm 1.6590(1.6231) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 22.4575(21.9222) | Bit/dim 3.5654(3.5487) | Xent 0.1064(0.0985) | Loss 3.6186(3.5979) | Error 0.0322(0.0340) Steps 898(903.77) | Grad Norm 1.5371(1.5778) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 22.0417(21.9318) | Bit/dim 3.5388(3.5449) | Xent 0.1016(0.0971) | Loss 3.5896(3.5934) | Error 0.0333(0.0332) Steps 898(901.51) | Grad Norm 1.2106(1.5441) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 21.8634(21.9393) | Bit/dim 3.5188(3.5432) | Xent 0.0998(0.0971) | Loss 3.5687(3.5917) | Error 0.0378(0.0329) Steps 898(901.80) | Grad Norm 2.3317(1.5875) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 21.5431(21.9443) | Bit/dim 3.5580(3.5430) | Xent 0.1160(0.0967) | Loss 3.6159(3.5914) | Error 0.0444(0.0325) Steps 892(902.12) | Grad Norm 1.4076(1.5747) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 106.5265, Epoch Time 1332.3897(1279.5116), Bit/dim 3.5543(best: 3.5537), Xent 1.0243, Loss 4.0664, Error 0.2268(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 22.3285(21.9637) | Bit/dim 3.5180(3.5432) | Xent 0.0702(0.0967) | Loss 3.5532(3.5916) | Error 0.0256(0.0323) Steps 898(901.99) | Grad Norm 1.5877(1.5768) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 22.3154(21.9777) | Bit/dim 3.5397(3.5438) | Xent 0.1162(0.0964) | Loss 3.5978(3.5920) | Error 0.0411(0.0319) Steps 898(901.78) | Grad Norm 1.3429(1.5963) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 21.2566(21.8988) | Bit/dim 3.5551(3.5428) | Xent 0.0743(0.0964) | Loss 3.5923(3.5910) | Error 0.0222(0.0321) Steps 886(900.68) | Grad Norm 1.2742(1.5661) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 22.0173(21.8227) | Bit/dim 3.5596(3.5460) | Xent 0.0887(0.0955) | Loss 3.6039(3.5937) | Error 0.0289(0.0321) Steps 898(899.32) | Grad Norm 1.2591(1.5396) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 22.3184(21.8927) | Bit/dim 3.5491(3.5431) | Xent 0.0965(0.0969) | Loss 3.5973(3.5916) | Error 0.0356(0.0330) Steps 886(898.17) | Grad Norm 2.0114(1.5541) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 105.7948, Epoch Time 1327.0172(1280.9368), Bit/dim 3.5573(best: 3.5537), Xent 1.0613, Loss 4.0879, Error 0.2284(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 22.4012(21.8797) | Bit/dim 3.5608(3.5430) | Xent 0.0815(0.0951) | Loss 3.6015(3.5906) | Error 0.0244(0.0319) Steps 910(898.75) | Grad Norm 1.4537(1.5142) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 21.8229(21.9126) | Bit/dim 3.5467(3.5420) | Xent 0.0993(0.0972) | Loss 3.5964(3.5906) | Error 0.0333(0.0331) Steps 898(899.71) | Grad Norm 1.5203(1.5474) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 22.1447(21.9573) | Bit/dim 3.5355(3.5441) | Xent 0.0954(0.0956) | Loss 3.5832(3.5919) | Error 0.0344(0.0329) Steps 898(901.18) | Grad Norm 1.5578(1.4964) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 21.9133(22.0539) | Bit/dim 3.5502(3.5467) | Xent 0.1040(0.0970) | Loss 3.6021(3.5952) | Error 0.0333(0.0329) Steps 910(902.44) | Grad Norm 1.7034(1.4766) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 21.8640(22.1297) | Bit/dim 3.5513(3.5460) | Xent 0.1009(0.0967) | Loss 3.6017(3.5944) | Error 0.0344(0.0328) Steps 898(902.05) | Grad Norm 1.3735(1.4674) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 22.0137(22.0772) | Bit/dim 3.5444(3.5429) | Xent 0.0794(0.0973) | Loss 3.5841(3.5916) | Error 0.0244(0.0333) Steps 898(902.07) | Grad Norm 1.4300(1.4933) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 107.2954, Epoch Time 1343.1325(1282.8026), Bit/dim 3.5539(best: 3.5537), Xent 1.0388, Loss 4.0733, Error 0.2322(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 22.0440(22.0115) | Bit/dim 3.5558(3.5433) | Xent 0.1117(0.0977) | Loss 3.6116(3.5921) | Error 0.0456(0.0335) Steps 898(901.88) | Grad Norm 2.0513(1.5184) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 22.5754(22.0702) | Bit/dim 3.5252(3.5416) | Xent 0.0974(0.0963) | Loss 3.5740(3.5897) | Error 0.0378(0.0329) Steps 886(903.24) | Grad Norm 1.4116(1.4945) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 21.3407(22.0579) | Bit/dim 3.5313(3.5404) | Xent 0.1016(0.0975) | Loss 3.5821(3.5892) | Error 0.0356(0.0330) Steps 910(904.58) | Grad Norm 1.6855(1.5315) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 22.0409(22.0458) | Bit/dim 3.5370(3.5430) | Xent 0.0718(0.0969) | Loss 3.5729(3.5915) | Error 0.0222(0.0329) Steps 898(904.45) | Grad Norm 1.1922(1.5207) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 21.7720(21.9959) | Bit/dim 3.5796(3.5460) | Xent 0.0872(0.0970) | Loss 3.6232(3.5945) | Error 0.0333(0.0334) Steps 910(905.36) | Grad Norm 1.7115(1.5688) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 105.7162, Epoch Time 1332.5507(1284.2951), Bit/dim 3.5548(best: 3.5537), Xent 1.0138, Loss 4.0617, Error 0.2283(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 21.5464(21.9519) | Bit/dim 3.5654(3.5455) | Xent 0.1046(0.0976) | Loss 3.6177(3.5944) | Error 0.0422(0.0342) Steps 892(904.82) | Grad Norm 1.4477(1.5601) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 21.3315(21.8697) | Bit/dim 3.5527(3.5442) | Xent 0.1047(0.0972) | Loss 3.6050(3.5928) | Error 0.0344(0.0335) Steps 910(904.08) | Grad Norm 1.5090(1.5622) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 21.6664(21.9221) | Bit/dim 3.5671(3.5462) | Xent 0.0974(0.0968) | Loss 3.6159(3.5946) | Error 0.0289(0.0334) Steps 916(903.64) | Grad Norm 1.5470(1.5967) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 21.5899(21.9379) | Bit/dim 3.5405(3.5462) | Xent 0.1007(0.0961) | Loss 3.5909(3.5942) | Error 0.0289(0.0329) Steps 910(904.03) | Grad Norm 1.3448(1.6158) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 21.7996(21.9209) | Bit/dim 3.5289(3.5415) | Xent 0.0958(0.0975) | Loss 3.5768(3.5902) | Error 0.0289(0.0336) Steps 904(903.82) | Grad Norm 2.0650(1.6829) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 22.6126(21.9635) | Bit/dim 3.5602(3.5428) | Xent 0.1075(0.0982) | Loss 3.6139(3.5919) | Error 0.0322(0.0333) Steps 886(904.38) | Grad Norm 1.9290(1.7635) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 106.3500, Epoch Time 1331.9481(1285.7247), Bit/dim 3.5540(best: 3.5537), Xent 1.0353, Loss 4.0716, Error 0.2284(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 21.5892(22.0215) | Bit/dim 3.5635(3.5405) | Xent 0.0882(0.0969) | Loss 3.6076(3.5889) | Error 0.0289(0.0332) Steps 898(905.09) | Grad Norm 1.7710(1.7415) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 22.0166(22.0650) | Bit/dim 3.5564(3.5426) | Xent 0.1021(0.0955) | Loss 3.6075(3.5904) | Error 0.0333(0.0325) Steps 910(905.69) | Grad Norm 1.8864(1.7347) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 22.2262(22.0830) | Bit/dim 3.5649(3.5424) | Xent 0.0861(0.0963) | Loss 3.6079(3.5905) | Error 0.0367(0.0325) Steps 922(907.73) | Grad Norm 1.6608(1.7511) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 21.3218(22.0427) | Bit/dim 3.5330(3.5421) | Xent 0.1020(0.0970) | Loss 3.5840(3.5906) | Error 0.0400(0.0328) Steps 904(905.83) | Grad Norm 1.9164(1.7233) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 23.0578(22.0409) | Bit/dim 3.5549(3.5443) | Xent 0.1052(0.0982) | Loss 3.6076(3.5934) | Error 0.0378(0.0338) Steps 928(905.20) | Grad Norm 1.6989(1.6563) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 107.4304, Epoch Time 1341.9277(1287.4108), Bit/dim 3.5532(best: 3.5537), Xent 1.0518, Loss 4.0791, Error 0.2298(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 21.8786(22.1175) | Bit/dim 3.5549(3.5439) | Xent 0.0702(0.0958) | Loss 3.5900(3.5918) | Error 0.0233(0.0321) Steps 892(904.14) | Grad Norm 1.1881(1.5664) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 21.8736(22.1439) | Bit/dim 3.5119(3.5408) | Xent 0.0861(0.0947) | Loss 3.5550(3.5881) | Error 0.0300(0.0320) Steps 922(904.63) | Grad Norm 1.3818(1.5193) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 22.7707(22.1531) | Bit/dim 3.5574(3.5406) | Xent 0.0872(0.0947) | Loss 3.6010(3.5879) | Error 0.0311(0.0327) Steps 910(902.54) | Grad Norm 2.7257(1.6246) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 21.9456(22.1755) | Bit/dim 3.5390(3.5416) | Xent 0.1000(0.0967) | Loss 3.5890(3.5900) | Error 0.0411(0.0332) Steps 892(903.82) | Grad Norm 1.7490(1.6393) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 22.2657(22.1730) | Bit/dim 3.5540(3.5437) | Xent 0.1017(0.0945) | Loss 3.6048(3.5910) | Error 0.0367(0.0328) Steps 892(904.06) | Grad Norm 1.4026(1.6307) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 22.4757(22.1860) | Bit/dim 3.5652(3.5448) | Xent 0.0795(0.0931) | Loss 3.6050(3.5913) | Error 0.0278(0.0323) Steps 886(903.89) | Grad Norm 1.2755(1.6242) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 107.8277, Epoch Time 1349.5862(1289.2760), Bit/dim 3.5519(best: 3.5532), Xent 1.0395, Loss 4.0717, Error 0.2309(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 22.5343(22.1220) | Bit/dim 3.5486(3.5429) | Xent 0.0807(0.0935) | Loss 3.5889(3.5896) | Error 0.0267(0.0322) Steps 946(906.31) | Grad Norm 1.2246(1.6281) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 22.0768(22.0885) | Bit/dim 3.5760(3.5423) | Xent 0.0668(0.0928) | Loss 3.6094(3.5887) | Error 0.0278(0.0319) Steps 910(904.21) | Grad Norm 1.4021(1.5780) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 23.1717(22.1590) | Bit/dim 3.5374(3.5455) | Xent 0.1087(0.0947) | Loss 3.5918(3.5928) | Error 0.0378(0.0322) Steps 934(906.54) | Grad Norm 1.9849(1.6104) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 22.0912(22.2746) | Bit/dim 3.5524(3.5437) | Xent 0.0926(0.0950) | Loss 3.5987(3.5912) | Error 0.0333(0.0319) Steps 898(906.76) | Grad Norm 2.4114(1.6294) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 22.8930(22.3240) | Bit/dim 3.5605(3.5439) | Xent 0.0914(0.0959) | Loss 3.6062(3.5919) | Error 0.0344(0.0334) Steps 934(909.11) | Grad Norm 1.5356(1.6862) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 107.5252, Epoch Time 1351.8142(1291.1522), Bit/dim 3.5529(best: 3.5519), Xent 1.0424, Loss 4.0740, Error 0.2313(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 22.2256(22.2506) | Bit/dim 3.5520(3.5441) | Xent 0.0917(0.0980) | Loss 3.5979(3.5931) | Error 0.0300(0.0341) Steps 910(906.42) | Grad Norm 1.6278(1.7582) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 20.9456(22.1438) | Bit/dim 3.5508(3.5439) | Xent 0.0923(0.0989) | Loss 3.5970(3.5934) | Error 0.0322(0.0344) Steps 904(906.42) | Grad Norm 1.4771(1.7664) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 22.6145(22.2008) | Bit/dim 3.5119(3.5421) | Xent 0.1006(0.0964) | Loss 3.5622(3.5903) | Error 0.0300(0.0334) Steps 946(909.16) | Grad Norm 1.7465(1.6932) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 22.1251(22.1826) | Bit/dim 3.5303(3.5421) | Xent 0.1034(0.0957) | Loss 3.5820(3.5899) | Error 0.0300(0.0331) Steps 928(910.05) | Grad Norm 2.2049(1.6902) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 22.8214(22.2015) | Bit/dim 3.5623(3.5438) | Xent 0.0890(0.0940) | Loss 3.6068(3.5908) | Error 0.0344(0.0324) Steps 898(908.39) | Grad Norm 1.7868(1.6656) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 21.6096(22.2557) | Bit/dim 3.5217(3.5434) | Xent 0.0772(0.0954) | Loss 3.5603(3.5910) | Error 0.0311(0.0331) Steps 910(912.13) | Grad Norm 1.4368(1.6795) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 106.6502, Epoch Time 1344.3723(1292.7488), Bit/dim 3.5526(best: 3.5519), Xent 1.0452, Loss 4.0752, Error 0.2306(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 22.6519(22.2988) | Bit/dim 3.5357(3.5448) | Xent 0.1000(0.0939) | Loss 3.5857(3.5918) | Error 0.0256(0.0319) Steps 898(911.11) | Grad Norm 1.9488(1.6661) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 21.8379(22.2468) | Bit/dim 3.5450(3.5444) | Xent 0.0677(0.0933) | Loss 3.5789(3.5910) | Error 0.0200(0.0316) Steps 916(912.11) | Grad Norm 1.2515(1.6312) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 21.7461(22.2122) | Bit/dim 3.5687(3.5441) | Xent 0.0906(0.0924) | Loss 3.6140(3.5903) | Error 0.0300(0.0310) Steps 886(911.54) | Grad Norm 2.5616(1.6362) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 22.0381(22.2193) | Bit/dim 3.4725(3.5379) | Xent 0.0998(0.0915) | Loss 3.5224(3.5836) | Error 0.0344(0.0312) Steps 910(911.47) | Grad Norm 1.7044(1.6322) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 22.5941(22.1716) | Bit/dim 3.5862(3.5423) | Xent 0.1131(0.0930) | Loss 3.6427(3.5888) | Error 0.0400(0.0320) Steps 916(910.55) | Grad Norm 1.8027(1.6741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 108.5908, Epoch Time 1348.4413(1294.4196), Bit/dim 3.5521(best: 3.5519), Xent 1.0573, Loss 4.0808, Error 0.2323(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 23.0098(22.2834) | Bit/dim 3.5827(3.5432) | Xent 0.0986(0.0930) | Loss 3.6320(3.5897) | Error 0.0344(0.0318) Steps 928(914.39) | Grad Norm 1.6805(1.6083) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 22.0380(22.2938) | Bit/dim 3.5272(3.5426) | Xent 0.0892(0.0929) | Loss 3.5718(3.5891) | Error 0.0322(0.0317) Steps 922(913.91) | Grad Norm 1.8566(1.5946) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 22.4465(22.3152) | Bit/dim 3.5588(3.5426) | Xent 0.1032(0.0930) | Loss 3.6104(3.5892) | Error 0.0333(0.0313) Steps 916(914.67) | Grad Norm 2.4107(1.7199) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 22.8712(22.3954) | Bit/dim 3.5300(3.5428) | Xent 0.0775(0.0927) | Loss 3.5688(3.5892) | Error 0.0233(0.0310) Steps 922(913.43) | Grad Norm 1.5486(1.7003) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 22.0173(22.3701) | Bit/dim 3.5530(3.5404) | Xent 0.1026(0.0930) | Loss 3.6043(3.5869) | Error 0.0322(0.0309) Steps 886(911.03) | Grad Norm 1.5358(1.6545) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 22.8724(22.4349) | Bit/dim 3.5511(3.5431) | Xent 0.1076(0.0942) | Loss 3.6049(3.5902) | Error 0.0378(0.0319) Steps 904(913.49) | Grad Norm 1.8800(1.6537) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 107.4613, Epoch Time 1362.9441(1296.4753), Bit/dim 3.5539(best: 3.5519), Xent 1.1008, Loss 4.1043, Error 0.2313(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 21.7402(22.3024) | Bit/dim 3.5742(3.5449) | Xent 0.0776(0.0936) | Loss 3.6129(3.5917) | Error 0.0211(0.0317) Steps 898(913.42) | Grad Norm 1.7457(1.7305) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 22.1673(22.3352) | Bit/dim 3.5315(3.5437) | Xent 0.0757(0.0926) | Loss 3.5693(3.5900) | Error 0.0233(0.0317) Steps 904(911.79) | Grad Norm 1.9416(1.6879) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 22.7381(22.3491) | Bit/dim 3.5525(3.5436) | Xent 0.1057(0.0931) | Loss 3.6054(3.5901) | Error 0.0344(0.0317) Steps 910(912.24) | Grad Norm 1.7665(1.7164) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 22.1973(22.4120) | Bit/dim 3.5368(3.5434) | Xent 0.1086(0.0926) | Loss 3.5911(3.5898) | Error 0.0344(0.0312) Steps 904(910.13) | Grad Norm 1.3658(1.6799) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 22.4840(22.4431) | Bit/dim 3.5523(3.5412) | Xent 0.0892(0.0928) | Loss 3.5969(3.5876) | Error 0.0333(0.0315) Steps 916(910.21) | Grad Norm 2.6996(1.7189) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 107.5341, Epoch Time 1359.3877(1298.3627), Bit/dim 3.5517(best: 3.5519), Xent 1.0716, Loss 4.0875, Error 0.2304(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 22.1998(22.4598) | Bit/dim 3.5793(3.5421) | Xent 0.0981(0.0924) | Loss 3.6283(3.5883) | Error 0.0378(0.0315) Steps 904(909.43) | Grad Norm 1.8622(1.6680) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 22.1846(22.4338) | Bit/dim 3.5427(3.5419) | Xent 0.0849(0.0895) | Loss 3.5852(3.5867) | Error 0.0267(0.0303) Steps 916(910.16) | Grad Norm 1.7409(1.6106) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 22.6153(22.3939) | Bit/dim 3.5527(3.5398) | Xent 0.1141(0.0896) | Loss 3.6097(3.5846) | Error 0.0378(0.0303) Steps 922(913.80) | Grad Norm 1.8101(1.6400) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 22.2675(22.3600) | Bit/dim 3.5425(3.5405) | Xent 0.0766(0.0902) | Loss 3.5808(3.5856) | Error 0.0267(0.0306) Steps 922(915.40) | Grad Norm 1.3652(1.6993) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 22.1484(22.3508) | Bit/dim 3.5349(3.5417) | Xent 0.0973(0.0911) | Loss 3.5835(3.5872) | Error 0.0356(0.0309) Steps 898(915.34) | Grad Norm 1.6983(1.6443) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 22.8766(22.4051) | Bit/dim 3.5558(3.5428) | Xent 0.0972(0.0922) | Loss 3.6044(3.5889) | Error 0.0333(0.0311) Steps 940(915.79) | Grad Norm 2.2299(1.6677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 108.9054, Epoch Time 1357.1637(1300.1267), Bit/dim 3.5531(best: 3.5517), Xent 1.0710, Loss 4.0886, Error 0.2309(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 22.5454(22.4254) | Bit/dim 3.5052(3.5408) | Xent 0.0926(0.0931) | Loss 3.5515(3.5874) | Error 0.0344(0.0314) Steps 916(916.19) | Grad Norm 2.2338(1.7334) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 22.8068(22.4258) | Bit/dim 3.5414(3.5421) | Xent 0.0874(0.0935) | Loss 3.5851(3.5889) | Error 0.0311(0.0318) Steps 928(913.99) | Grad Norm 1.5676(1.7571) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 21.5343(22.2627) | Bit/dim 3.5377(3.5439) | Xent 0.0990(0.0948) | Loss 3.5872(3.5913) | Error 0.0356(0.0321) Steps 892(910.54) | Grad Norm 1.7947(1.7870) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 22.7222(22.3317) | Bit/dim 3.5470(3.5427) | Xent 0.0878(0.0963) | Loss 3.5909(3.5908) | Error 0.0356(0.0337) Steps 922(914.87) | Grad Norm 2.1839(1.8808) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 22.0647(22.2834) | Bit/dim 3.5294(3.5424) | Xent 0.1043(0.0952) | Loss 3.5816(3.5900) | Error 0.0344(0.0338) Steps 916(915.42) | Grad Norm 1.6761(1.8197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 107.7931, Epoch Time 1352.1575(1301.6876), Bit/dim 3.5515(best: 3.5517), Xent 1.0772, Loss 4.0901, Error 0.2298(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 22.0300(22.2822) | Bit/dim 3.5479(3.5408) | Xent 0.1015(0.0945) | Loss 3.5986(3.5881) | Error 0.0289(0.0329) Steps 934(917.31) | Grad Norm 1.9331(1.8396) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 21.7958(22.3067) | Bit/dim 3.5301(3.5409) | Xent 0.1054(0.0958) | Loss 3.5828(3.5888) | Error 0.0444(0.0335) Steps 910(918.57) | Grad Norm 1.6735(1.8211) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 22.3355(22.3093) | Bit/dim 3.5161(3.5435) | Xent 0.0855(0.0930) | Loss 3.5588(3.5900) | Error 0.0311(0.0321) Steps 910(918.42) | Grad Norm 2.0200(1.7686) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 22.1163(22.3712) | Bit/dim 3.5267(3.5424) | Xent 0.0975(0.0925) | Loss 3.5754(3.5887) | Error 0.0322(0.0320) Steps 910(917.29) | Grad Norm 1.3079(1.7019) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 23.3549(22.4032) | Bit/dim 3.5458(3.5429) | Xent 0.0736(0.0910) | Loss 3.5826(3.5884) | Error 0.0244(0.0316) Steps 910(919.04) | Grad Norm 1.5328(1.6363) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 22.5985(22.4342) | Bit/dim 3.5288(3.5418) | Xent 0.0923(0.0922) | Loss 3.5749(3.5879) | Error 0.0389(0.0323) Steps 934(917.45) | Grad Norm 1.6176(1.6029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 107.9024, Epoch Time 1360.5166(1303.4525), Bit/dim 3.5516(best: 3.5515), Xent 1.0613, Loss 4.0823, Error 0.2279(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 22.3044(22.4648) | Bit/dim 3.5745(3.5426) | Xent 0.0920(0.0901) | Loss 3.6205(3.5876) | Error 0.0311(0.0312) Steps 946(919.80) | Grad Norm 1.5707(1.5902) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 22.6773(22.4228) | Bit/dim 3.5255(3.5422) | Xent 0.0936(0.0920) | Loss 3.5723(3.5882) | Error 0.0300(0.0321) Steps 928(917.44) | Grad Norm 2.1316(1.6472) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 21.8017(22.3496) | Bit/dim 3.5468(3.5418) | Xent 0.0815(0.0909) | Loss 3.5876(3.5872) | Error 0.0333(0.0317) Steps 916(915.93) | Grad Norm 1.7070(1.6320) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 22.7584(22.3578) | Bit/dim 3.5620(3.5428) | Xent 0.0915(0.0916) | Loss 3.6078(3.5886) | Error 0.0311(0.0314) Steps 910(915.92) | Grad Norm 1.4594(1.6294) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 22.7491(22.4490) | Bit/dim 3.5293(3.5413) | Xent 0.0920(0.0905) | Loss 3.5753(3.5866) | Error 0.0333(0.0308) Steps 946(918.89) | Grad Norm 1.4189(1.6743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 107.8607, Epoch Time 1361.1729(1305.1841), Bit/dim 3.5537(best: 3.5515), Xent 1.0826, Loss 4.0950, Error 0.2308(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 22.7498(22.4625) | Bit/dim 3.5275(3.5419) | Xent 0.0622(0.0910) | Loss 3.5586(3.5874) | Error 0.0200(0.0311) Steps 946(919.37) | Grad Norm 1.5739(1.6801) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 22.7052(22.3957) | Bit/dim 3.5592(3.5440) | Xent 0.0781(0.0880) | Loss 3.5983(3.5880) | Error 0.0256(0.0293) Steps 892(918.48) | Grad Norm 2.0955(1.6768) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 22.5446(22.4159) | Bit/dim 3.5347(3.5436) | Xent 0.1034(0.0884) | Loss 3.5864(3.5877) | Error 0.0322(0.0293) Steps 928(919.01) | Grad Norm 1.4463(1.6414) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 22.4702(22.3468) | Bit/dim 3.5875(3.5443) | Xent 0.0901(0.0880) | Loss 3.6326(3.5883) | Error 0.0300(0.0295) Steps 934(919.53) | Grad Norm 1.6122(1.6236) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 23.2934(22.4688) | Bit/dim 3.5244(3.5411) | Xent 0.0861(0.0885) | Loss 3.5674(3.5854) | Error 0.0300(0.0303) Steps 946(921.02) | Grad Norm 1.5423(1.6248) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 23.1008(22.5408) | Bit/dim 3.5616(3.5403) | Xent 0.0999(0.0897) | Loss 3.6116(3.5852) | Error 0.0389(0.0306) Steps 922(918.42) | Grad Norm 1.7688(1.6314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 107.9942, Epoch Time 1362.0207(1306.8892), Bit/dim 3.5537(best: 3.5515), Xent 1.1017, Loss 4.1046, Error 0.2295(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 23.1520(22.5422) | Bit/dim 3.5452(3.5402) | Xent 0.0815(0.0912) | Loss 3.5860(3.5858) | Error 0.0300(0.0315) Steps 928(919.48) | Grad Norm 2.6820(1.6949) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 22.0285(22.4858) | Bit/dim 3.5379(3.5398) | Xent 0.0815(0.0904) | Loss 3.5786(3.5850) | Error 0.0267(0.0310) Steps 922(919.92) | Grad Norm 2.1440(1.8352) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 22.2902(22.4616) | Bit/dim 3.5412(3.5414) | Xent 0.0798(0.0907) | Loss 3.5811(3.5867) | Error 0.0289(0.0315) Steps 910(920.28) | Grad Norm 1.8982(1.8743) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 22.9088(22.4405) | Bit/dim 3.5120(3.5414) | Xent 0.0945(0.0916) | Loss 3.5593(3.5872) | Error 0.0389(0.0318) Steps 928(919.18) | Grad Norm 1.9025(1.8498) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 22.8206(22.4755) | Bit/dim 3.5587(3.5417) | Xent 0.0856(0.0914) | Loss 3.6015(3.5874) | Error 0.0311(0.0321) Steps 934(920.14) | Grad Norm 1.5791(1.8232) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 106.0793, Epoch Time 1357.0514(1308.3941), Bit/dim 3.5516(best: 3.5515), Xent 1.0719, Loss 4.0875, Error 0.2292(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 22.3338(22.3773) | Bit/dim 3.5423(3.5430) | Xent 0.0713(0.0897) | Loss 3.5779(3.5879) | Error 0.0267(0.0315) Steps 922(919.77) | Grad Norm 1.5105(1.7559) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 22.0185(22.3641) | Bit/dim 3.5413(3.5423) | Xent 0.0922(0.0916) | Loss 3.5874(3.5881) | Error 0.0356(0.0319) Steps 946(920.02) | Grad Norm 1.6255(1.7094) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 21.6315(22.3769) | Bit/dim 3.5217(3.5434) | Xent 0.1120(0.0925) | Loss 3.5777(3.5896) | Error 0.0344(0.0316) Steps 928(919.03) | Grad Norm 1.7225(1.7326) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 22.9443(22.3106) | Bit/dim 3.5501(3.5411) | Xent 0.1241(0.0925) | Loss 3.6121(3.5873) | Error 0.0456(0.0318) Steps 940(917.86) | Grad Norm 2.6609(1.8003) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 22.5582(22.3792) | Bit/dim 3.5300(3.5412) | Xent 0.0999(0.0927) | Loss 3.5799(3.5875) | Error 0.0356(0.0321) Steps 928(917.90) | Grad Norm 2.4127(1.9572) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 22.8041(22.3508) | Bit/dim 3.5502(3.5415) | Xent 0.0962(0.0921) | Loss 3.5983(3.5875) | Error 0.0333(0.0317) Steps 916(920.09) | Grad Norm 2.0780(1.9984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 108.8787, Epoch Time 1356.5743(1309.8395), Bit/dim 3.5528(best: 3.5515), Xent 1.1001, Loss 4.1028, Error 0.2322(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 22.5940(22.3530) | Bit/dim 3.5305(3.5398) | Xent 0.0861(0.0924) | Loss 3.5736(3.5860) | Error 0.0322(0.0319) Steps 928(921.23) | Grad Norm 2.4055(2.0188) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 23.0617(22.3882) | Bit/dim 3.5646(3.5401) | Xent 0.0936(0.0912) | Loss 3.6114(3.5857) | Error 0.0322(0.0310) Steps 934(920.03) | Grad Norm 1.9528(1.9853) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 22.4830(22.3707) | Bit/dim 3.5432(3.5374) | Xent 0.0969(0.0906) | Loss 3.5916(3.5827) | Error 0.0356(0.0307) Steps 928(919.17) | Grad Norm 2.0926(1.9502) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 22.0307(22.4056) | Bit/dim 3.5560(3.5407) | Xent 0.0745(0.0900) | Loss 3.5933(3.5857) | Error 0.0200(0.0307) Steps 922(919.86) | Grad Norm 1.0356(1.8782) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 22.1151(22.4493) | Bit/dim 3.5799(3.5419) | Xent 0.1046(0.0897) | Loss 3.6322(3.5868) | Error 0.0344(0.0306) Steps 916(921.00) | Grad Norm 1.6294(1.7891) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 106.9916, Epoch Time 1359.2356(1311.3213), Bit/dim 3.5516(best: 3.5515), Xent 1.0826, Loss 4.0929, Error 0.2317(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 22.9194(22.4265) | Bit/dim 3.5410(3.5426) | Xent 0.0893(0.0896) | Loss 3.5856(3.5873) | Error 0.0289(0.0305) Steps 916(921.52) | Grad Norm 2.0778(1.8025) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 22.3702(22.3747) | Bit/dim 3.5466(3.5432) | Xent 0.0774(0.0878) | Loss 3.5853(3.5872) | Error 0.0267(0.0296) Steps 904(919.99) | Grad Norm 1.2141(1.8055) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 22.5694(22.3205) | Bit/dim 3.5523(3.5423) | Xent 0.0908(0.0865) | Loss 3.5977(3.5855) | Error 0.0356(0.0298) Steps 928(920.06) | Grad Norm 1.8473(1.7772) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 22.3560(22.3231) | Bit/dim 3.5420(3.5414) | Xent 0.0919(0.0883) | Loss 3.5880(3.5856) | Error 0.0344(0.0309) Steps 952(921.78) | Grad Norm 1.7684(1.8823) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 22.9353(22.3643) | Bit/dim 3.5640(3.5411) | Xent 0.0906(0.0881) | Loss 3.6093(3.5852) | Error 0.0322(0.0307) Steps 904(921.80) | Grad Norm 2.3869(1.8785) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 22.1585(22.3971) | Bit/dim 3.5236(3.5408) | Xent 0.0917(0.0892) | Loss 3.5695(3.5854) | Error 0.0311(0.0311) Steps 910(922.07) | Grad Norm 1.4901(1.8749) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 107.4678, Epoch Time 1356.2258(1312.6685), Bit/dim 3.5528(best: 3.5515), Xent 1.1196, Loss 4.1126, Error 0.2337(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 23.7223(22.4314) | Bit/dim 3.5426(3.5390) | Xent 0.0777(0.0893) | Loss 3.5815(3.5836) | Error 0.0244(0.0308) Steps 928(921.61) | Grad Norm 1.2090(1.8584) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 22.5655(22.4548) | Bit/dim 3.5485(3.5378) | Xent 0.1078(0.0898) | Loss 3.6024(3.5827) | Error 0.0444(0.0315) Steps 898(921.12) | Grad Norm 1.9682(1.8312) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 22.1793(22.4177) | Bit/dim 3.5142(3.5406) | Xent 0.0785(0.0898) | Loss 3.5534(3.5855) | Error 0.0256(0.0317) Steps 916(922.34) | Grad Norm 2.0563(1.8881) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 22.6978(22.4314) | Bit/dim 3.5653(3.5407) | Xent 0.0933(0.0888) | Loss 3.6119(3.5851) | Error 0.0322(0.0312) Steps 928(921.29) | Grad Norm 2.1456(1.9402) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 22.2000(22.4926) | Bit/dim 3.5559(3.5416) | Xent 0.0819(0.0898) | Loss 3.5969(3.5865) | Error 0.0300(0.0317) Steps 916(920.91) | Grad Norm 1.7969(1.9724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 108.1412, Epoch Time 1363.3718(1314.1896), Bit/dim 3.5507(best: 3.5515), Xent 1.1022, Loss 4.1018, Error 0.2330(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 23.0487(22.4852) | Bit/dim 3.5541(3.5429) | Xent 0.0690(0.0904) | Loss 3.5886(3.5881) | Error 0.0222(0.0315) Steps 916(920.21) | Grad Norm 1.5854(1.9345) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 21.6399(22.4628) | Bit/dim 3.5361(3.5418) | Xent 0.0712(0.0890) | Loss 3.5717(3.5863) | Error 0.0233(0.0308) Steps 916(920.26) | Grad Norm 1.7279(1.8749) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 23.1009(22.4877) | Bit/dim 3.5568(3.5401) | Xent 0.0664(0.0897) | Loss 3.5900(3.5850) | Error 0.0256(0.0307) Steps 910(919.24) | Grad Norm 1.2889(1.7973) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 22.3583(22.4669) | Bit/dim 3.5421(3.5407) | Xent 0.0917(0.0895) | Loss 3.5879(3.5855) | Error 0.0267(0.0309) Steps 928(918.55) | Grad Norm 1.5209(1.7844) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 22.8637(22.4972) | Bit/dim 3.5289(3.5401) | Xent 0.0831(0.0896) | Loss 3.5705(3.5849) | Error 0.0289(0.0307) Steps 916(919.03) | Grad Norm 1.7598(1.8081) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 23.0135(22.4897) | Bit/dim 3.5570(3.5403) | Xent 0.0832(0.0897) | Loss 3.5986(3.5852) | Error 0.0322(0.0309) Steps 928(920.68) | Grad Norm 1.5950(1.7780) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 107.3832, Epoch Time 1363.7454(1315.6763), Bit/dim 3.5513(best: 3.5507), Xent 1.1112, Loss 4.1069, Error 0.2284(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 22.7850(22.5335) | Bit/dim 3.5562(3.5412) | Xent 0.0954(0.0885) | Loss 3.6039(3.5854) | Error 0.0344(0.0306) Steps 916(920.42) | Grad Norm 1.4463(1.7782) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 22.5653(22.5517) | Bit/dim 3.5195(3.5424) | Xent 0.1101(0.0900) | Loss 3.5746(3.5874) | Error 0.0411(0.0314) Steps 922(920.12) | Grad Norm 1.9408(1.7689) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 22.8287(22.5563) | Bit/dim 3.4834(3.5408) | Xent 0.0913(0.0910) | Loss 3.5290(3.5863) | Error 0.0311(0.0316) Steps 928(921.09) | Grad Norm 1.8832(1.7940) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 22.7349(22.6220) | Bit/dim 3.5181(3.5390) | Xent 0.1079(0.0919) | Loss 3.5721(3.5850) | Error 0.0400(0.0314) Steps 922(920.36) | Grad Norm 1.5769(1.7634) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 22.9074(22.6264) | Bit/dim 3.5259(3.5377) | Xent 0.0835(0.0921) | Loss 3.5676(3.5837) | Error 0.0300(0.0316) Steps 910(920.51) | Grad Norm 1.5991(1.7931) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 107.5293, Epoch Time 1371.7077(1317.3572), Bit/dim 3.5515(best: 3.5507), Xent 1.1050, Loss 4.1040, Error 0.2271(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 22.4023(22.5911) | Bit/dim 3.5604(3.5388) | Xent 0.0717(0.0899) | Loss 3.5963(3.5838) | Error 0.0233(0.0312) Steps 934(921.30) | Grad Norm 1.6012(1.7483) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 21.9112(22.4931) | Bit/dim 3.5145(3.5414) | Xent 0.0650(0.0898) | Loss 3.5470(3.5862) | Error 0.0222(0.0312) Steps 922(921.40) | Grad Norm 1.7340(1.8010) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 22.3652(22.5758) | Bit/dim 3.5337(3.5418) | Xent 0.0653(0.0888) | Loss 3.5664(3.5861) | Error 0.0233(0.0310) Steps 916(924.60) | Grad Norm 1.5854(1.7530) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 22.6107(22.5740) | Bit/dim 3.5511(3.5414) | Xent 0.1040(0.0895) | Loss 3.6031(3.5862) | Error 0.0344(0.0304) Steps 922(922.62) | Grad Norm 1.7807(1.7382) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 22.1886(22.5514) | Bit/dim 3.5726(3.5399) | Xent 0.1019(0.0896) | Loss 3.6235(3.5847) | Error 0.0367(0.0308) Steps 910(923.71) | Grad Norm 1.5391(1.7444) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 22.4965(22.5046) | Bit/dim 3.5263(3.5395) | Xent 0.0755(0.0887) | Loss 3.5641(3.5839) | Error 0.0244(0.0301) Steps 910(921.86) | Grad Norm 1.7552(1.7621) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 107.8025, Epoch Time 1364.2730(1318.7647), Bit/dim 3.5495(best: 3.5507), Xent 1.1241, Loss 4.1115, Error 0.2327(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 22.9273(22.4659) | Bit/dim 3.5723(3.5399) | Xent 0.0702(0.0868) | Loss 3.6074(3.5833) | Error 0.0256(0.0296) Steps 910(920.95) | Grad Norm 2.5029(1.7918) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 22.9943(22.5336) | Bit/dim 3.5307(3.5387) | Xent 0.0823(0.0870) | Loss 3.5718(3.5822) | Error 0.0289(0.0299) Steps 910(917.95) | Grad Norm 2.6000(1.8372) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 22.0745(22.5682) | Bit/dim 3.5297(3.5408) | Xent 0.0940(0.0882) | Loss 3.5767(3.5848) | Error 0.0233(0.0302) Steps 928(917.26) | Grad Norm 2.0076(1.9683) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 22.9099(22.6294) | Bit/dim 3.5561(3.5408) | Xent 0.0957(0.0878) | Loss 3.6039(3.5847) | Error 0.0333(0.0300) Steps 904(917.96) | Grad Norm 1.6185(1.9245) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 22.3010(22.6238) | Bit/dim 3.5481(3.5391) | Xent 0.0875(0.0872) | Loss 3.5919(3.5827) | Error 0.0322(0.0294) Steps 916(920.41) | Grad Norm 1.4556(1.8604) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 107.6059, Epoch Time 1369.6511(1320.2913), Bit/dim 3.5513(best: 3.5495), Xent 1.1066, Loss 4.1046, Error 0.2318(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 22.3800(22.5867) | Bit/dim 3.5172(3.5372) | Xent 0.0803(0.0883) | Loss 3.5574(3.5814) | Error 0.0267(0.0301) Steps 934(921.46) | Grad Norm 1.7183(1.9188) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 22.1452(22.5584) | Bit/dim 3.5312(3.5384) | Xent 0.1015(0.0867) | Loss 3.5820(3.5817) | Error 0.0389(0.0298) Steps 892(920.17) | Grad Norm 2.1809(2.0078) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 22.4897(22.4946) | Bit/dim 3.5024(3.5378) | Xent 0.0900(0.0857) | Loss 3.5474(3.5807) | Error 0.0289(0.0295) Steps 928(921.04) | Grad Norm 2.0552(2.1084) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 22.2754(22.5081) | Bit/dim 3.5262(3.5378) | Xent 0.0974(0.0854) | Loss 3.5749(3.5805) | Error 0.0344(0.0292) Steps 928(921.94) | Grad Norm 2.0229(2.0539) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 21.7132(22.5027) | Bit/dim 3.5565(3.5397) | Xent 0.0916(0.0869) | Loss 3.6022(3.5832) | Error 0.0311(0.0296) Steps 916(922.93) | Grad Norm 2.1392(1.9613) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 22.9249(22.4486) | Bit/dim 3.5265(3.5393) | Xent 0.0903(0.0868) | Loss 3.5717(3.5827) | Error 0.0311(0.0298) Steps 922(922.42) | Grad Norm 1.6706(1.9207) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 107.3315, Epoch Time 1361.0073(1321.5127), Bit/dim 3.5496(best: 3.5495), Xent 1.1030, Loss 4.1012, Error 0.2327(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 22.6807(22.4418) | Bit/dim 3.5310(3.5398) | Xent 0.0947(0.0866) | Loss 3.5784(3.5831) | Error 0.0344(0.0295) Steps 904(922.20) | Grad Norm 2.5439(1.8865) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 22.3910(22.4564) | Bit/dim 3.5260(3.5368) | Xent 0.0710(0.0864) | Loss 3.5616(3.5800) | Error 0.0244(0.0297) Steps 928(924.49) | Grad Norm 1.2408(2.0182) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 22.2803(22.4818) | Bit/dim 3.5564(3.5389) | Xent 0.1033(0.0866) | Loss 3.6081(3.5821) | Error 0.0322(0.0296) Steps 916(923.86) | Grad Norm 2.3594(2.0935) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 22.0879(22.4717) | Bit/dim 3.5199(3.5404) | Xent 0.1161(0.0861) | Loss 3.5779(3.5834) | Error 0.0389(0.0294) Steps 916(923.26) | Grad Norm 1.6784(2.0249) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 22.4342(22.5090) | Bit/dim 3.5135(3.5382) | Xent 0.0818(0.0872) | Loss 3.5544(3.5818) | Error 0.0300(0.0303) Steps 940(923.78) | Grad Norm 1.8801(1.9494) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 107.9682, Epoch Time 1365.4901(1322.8321), Bit/dim 3.5497(best: 3.5495), Xent 1.1268, Loss 4.1131, Error 0.2323(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 23.1580(22.5734) | Bit/dim 3.5061(3.5390) | Xent 0.0910(0.0883) | Loss 3.5516(3.5832) | Error 0.0333(0.0310) Steps 946(924.56) | Grad Norm 2.0393(1.9392) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 22.9616(22.6183) | Bit/dim 3.5058(3.5369) | Xent 0.0915(0.0875) | Loss 3.5516(3.5807) | Error 0.0333(0.0309) Steps 928(923.85) | Grad Norm 1.7090(1.8818) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 22.0817(22.5557) | Bit/dim 3.5140(3.5372) | Xent 0.1005(0.0872) | Loss 3.5642(3.5808) | Error 0.0344(0.0307) Steps 934(921.73) | Grad Norm 1.5061(1.8670) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 21.5054(22.4666) | Bit/dim 3.5637(3.5410) | Xent 0.0757(0.0872) | Loss 3.6015(3.5845) | Error 0.0244(0.0309) Steps 892(921.09) | Grad Norm 2.4422(1.8582) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 22.6141(22.5076) | Bit/dim 3.5377(3.5423) | Xent 0.0663(0.0862) | Loss 3.5709(3.5853) | Error 0.0267(0.0305) Steps 904(923.49) | Grad Norm 1.6221(1.8489) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 23.0903(22.5635) | Bit/dim 3.5226(3.5409) | Xent 0.0845(0.0855) | Loss 3.5649(3.5837) | Error 0.0367(0.0301) Steps 910(923.42) | Grad Norm 1.6878(1.7809) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 109.0516, Epoch Time 1369.6274(1324.2359), Bit/dim 3.5493(best: 3.5495), Xent 1.0997, Loss 4.0991, Error 0.2324(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 22.3352(22.5299) | Bit/dim 3.5250(3.5414) | Xent 0.1020(0.0856) | Loss 3.5761(3.5842) | Error 0.0344(0.0304) Steps 910(923.88) | Grad Norm 2.1067(1.8349) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 22.8056(22.5695) | Bit/dim 3.5376(3.5391) | Xent 0.0530(0.0847) | Loss 3.5641(3.5815) | Error 0.0178(0.0298) Steps 922(924.55) | Grad Norm 1.7712(1.8274) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 23.1491(22.5908) | Bit/dim 3.5275(3.5409) | Xent 0.0953(0.0854) | Loss 3.5752(3.5836) | Error 0.0289(0.0299) Steps 952(926.70) | Grad Norm 1.6635(1.8241) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 22.2133(22.5520) | Bit/dim 3.5348(3.5408) | Xent 0.0689(0.0850) | Loss 3.5692(3.5833) | Error 0.0189(0.0295) Steps 922(926.66) | Grad Norm 1.6231(1.8577) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 23.0730(22.5896) | Bit/dim 3.5246(3.5401) | Xent 0.0986(0.0858) | Loss 3.5739(3.5830) | Error 0.0356(0.0301) Steps 940(926.80) | Grad Norm 2.1961(1.9485) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 108.3981, Epoch Time 1367.9549(1325.5475), Bit/dim 3.5510(best: 3.5493), Xent 1.1360, Loss 4.1190, Error 0.2305(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 21.8013(22.5407) | Bit/dim 3.5478(3.5391) | Xent 0.0835(0.0866) | Loss 3.5896(3.5824) | Error 0.0333(0.0303) Steps 910(924.84) | Grad Norm 2.0571(2.1043) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 22.7416(22.5105) | Bit/dim 3.5128(3.5367) | Xent 0.1220(0.0877) | Loss 3.5738(3.5805) | Error 0.0433(0.0307) Steps 916(924.77) | Grad Norm 2.3802(2.1203) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 22.7017(22.6468) | Bit/dim 3.4971(3.5365) | Xent 0.0674(0.0861) | Loss 3.5308(3.5796) | Error 0.0233(0.0299) Steps 964(926.86) | Grad Norm 1.6391(2.0488) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 22.7154(22.6930) | Bit/dim 3.5494(3.5396) | Xent 0.0986(0.0872) | Loss 3.5987(3.5832) | Error 0.0378(0.0306) Steps 928(926.66) | Grad Norm 1.7904(2.0212) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 23.0470(22.6872) | Bit/dim 3.5229(3.5395) | Xent 0.0833(0.0860) | Loss 3.5645(3.5825) | Error 0.0211(0.0301) Steps 934(925.98) | Grad Norm 1.3481(1.9342) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 22.3367(22.6864) | Bit/dim 3.5236(3.5408) | Xent 0.0792(0.0846) | Loss 3.5632(3.5831) | Error 0.0267(0.0294) Steps 946(923.69) | Grad Norm 2.1338(1.8974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 108.5058, Epoch Time 1376.4931(1327.0759), Bit/dim 3.5505(best: 3.5493), Xent 1.1322, Loss 4.1166, Error 0.2326(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 22.3287(22.6211) | Bit/dim 3.5608(3.5413) | Xent 0.0943(0.0874) | Loss 3.6080(3.5850) | Error 0.0344(0.0302) Steps 916(923.34) | Grad Norm 2.2236(1.9159) | Total Time 14.00(14.00)\n",
      "Iter 16410 | Time 22.0088(22.5691) | Bit/dim 3.5212(3.5396) | Xent 0.0776(0.0880) | Loss 3.5600(3.5836) | Error 0.0244(0.0301) Steps 916(923.76) | Grad Norm 2.2899(1.9249) | Total Time 14.00(14.00)\n",
      "Iter 16420 | Time 22.9048(22.5332) | Bit/dim 3.5058(3.5401) | Xent 0.0941(0.0880) | Loss 3.5529(3.5841) | Error 0.0378(0.0305) Steps 928(922.27) | Grad Norm 2.3387(1.9818) | Total Time 14.00(14.00)\n",
      "Iter 16430 | Time 22.5129(22.5032) | Bit/dim 3.5181(3.5377) | Xent 0.0799(0.0877) | Loss 3.5581(3.5815) | Error 0.0278(0.0308) Steps 934(920.94) | Grad Norm 1.5346(2.0106) | Total Time 14.00(14.00)\n",
      "Iter 16440 | Time 22.4440(22.5566) | Bit/dim 3.5340(3.5391) | Xent 0.1147(0.0877) | Loss 3.5913(3.5829) | Error 0.0433(0.0309) Steps 916(921.66) | Grad Norm 1.8469(1.9288) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 108.6192, Epoch Time 1365.6543(1328.2332), Bit/dim 3.5515(best: 3.5493), Xent 1.1534, Loss 4.1282, Error 0.2303(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 22.3522(22.5951) | Bit/dim 3.5252(3.5391) | Xent 0.0960(0.0878) | Loss 3.5732(3.5830) | Error 0.0333(0.0312) Steps 916(923.62) | Grad Norm 1.5822(1.8888) | Total Time 14.00(14.00)\n",
      "Iter 16460 | Time 22.5943(22.5625) | Bit/dim 3.5394(3.5421) | Xent 0.0766(0.0866) | Loss 3.5777(3.5854) | Error 0.0244(0.0304) Steps 952(923.19) | Grad Norm 1.5524(1.8667) | Total Time 14.00(14.00)\n",
      "Iter 16470 | Time 22.3175(22.5905) | Bit/dim 3.5315(3.5415) | Xent 0.0714(0.0859) | Loss 3.5672(3.5844) | Error 0.0211(0.0302) Steps 916(922.55) | Grad Norm 1.3483(1.8281) | Total Time 14.00(14.00)\n",
      "Iter 16480 | Time 22.5318(22.5803) | Bit/dim 3.5583(3.5395) | Xent 0.0910(0.0861) | Loss 3.6038(3.5825) | Error 0.0344(0.0302) Steps 928(922.58) | Grad Norm 1.8444(1.8268) | Total Time 14.00(14.00)\n",
      "Iter 16490 | Time 22.2347(22.4858) | Bit/dim 3.5268(3.5392) | Xent 0.0608(0.0852) | Loss 3.5571(3.5818) | Error 0.0200(0.0300) Steps 934(923.91) | Grad Norm 1.8949(1.8032) | Total Time 14.00(14.00)\n",
      "Iter 16500 | Time 23.0595(22.5247) | Bit/dim 3.5608(3.5387) | Xent 0.0935(0.0859) | Loss 3.6076(3.5817) | Error 0.0311(0.0298) Steps 946(925.42) | Grad Norm 1.7775(1.7813) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 109.8816, Epoch Time 1367.8450(1329.4216), Bit/dim 3.5488(best: 3.5493), Xent 1.1368, Loss 4.1172, Error 0.2327(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 23.7824(22.6262) | Bit/dim 3.5123(3.5387) | Xent 0.0887(0.0856) | Loss 3.5567(3.5815) | Error 0.0344(0.0296) Steps 964(928.55) | Grad Norm 1.4687(1.7724) | Total Time 14.00(14.00)\n",
      "Iter 16520 | Time 22.2842(22.6602) | Bit/dim 3.5096(3.5404) | Xent 0.0816(0.0845) | Loss 3.5503(3.5826) | Error 0.0267(0.0288) Steps 928(927.07) | Grad Norm 1.9339(1.8013) | Total Time 14.00(14.00)\n",
      "Iter 16530 | Time 22.1488(22.6394) | Bit/dim 3.5221(3.5391) | Xent 0.0722(0.0837) | Loss 3.5582(3.5810) | Error 0.0267(0.0286) Steps 916(926.21) | Grad Norm 1.7659(1.7944) | Total Time 14.00(14.00)\n",
      "Iter 16540 | Time 21.9518(22.6017) | Bit/dim 3.5403(3.5409) | Xent 0.0891(0.0860) | Loss 3.5849(3.5840) | Error 0.0311(0.0292) Steps 928(926.23) | Grad Norm 2.3927(1.8921) | Total Time 14.00(14.00)\n",
      "Iter 16550 | Time 23.0912(22.5612) | Bit/dim 3.5726(3.5379) | Xent 0.0748(0.0876) | Loss 3.6100(3.5817) | Error 0.0267(0.0302) Steps 904(926.20) | Grad Norm 2.6237(2.0134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 108.3320, Epoch Time 1372.0459(1330.7003), Bit/dim 3.5511(best: 3.5488), Xent 1.1451, Loss 4.1237, Error 0.2323(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 22.9172(22.6037) | Bit/dim 3.5545(3.5402) | Xent 0.0912(0.0866) | Loss 3.6001(3.5834) | Error 0.0356(0.0304) Steps 934(926.70) | Grad Norm 2.2776(2.0882) | Total Time 14.00(14.00)\n",
      "Iter 16570 | Time 23.0099(22.6510) | Bit/dim 3.5143(3.5396) | Xent 0.0643(0.0852) | Loss 3.5464(3.5822) | Error 0.0178(0.0297) Steps 916(929.91) | Grad Norm 2.3448(2.1064) | Total Time 14.00(14.00)\n",
      "Iter 16580 | Time 22.9963(22.6363) | Bit/dim 3.5531(3.5415) | Xent 0.0890(0.0845) | Loss 3.5976(3.5837) | Error 0.0344(0.0290) Steps 916(928.21) | Grad Norm 1.7132(2.0814) | Total Time 14.00(14.00)\n",
      "Iter 16590 | Time 23.3697(22.6516) | Bit/dim 3.5398(3.5423) | Xent 0.0878(0.0856) | Loss 3.5837(3.5851) | Error 0.0222(0.0289) Steps 916(928.49) | Grad Norm 2.4026(2.0286) | Total Time 14.00(14.00)\n",
      "Iter 16600 | Time 22.5672(22.5634) | Bit/dim 3.5131(3.5389) | Xent 0.0921(0.0876) | Loss 3.5592(3.5827) | Error 0.0333(0.0296) Steps 928(926.07) | Grad Norm 2.1458(2.0583) | Total Time 14.00(14.00)\n",
      "Iter 16610 | Time 22.5113(22.5693) | Bit/dim 3.5469(3.5371) | Xent 0.0749(0.0892) | Loss 3.5844(3.5817) | Error 0.0256(0.0308) Steps 922(925.78) | Grad Norm 1.6443(2.1344) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 107.9442, Epoch Time 1371.2251(1331.9160), Bit/dim 3.5509(best: 3.5488), Xent 1.1334, Loss 4.1176, Error 0.2337(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 21.6626(22.6060) | Bit/dim 3.5181(3.5365) | Xent 0.0679(0.0857) | Loss 3.5520(3.5793) | Error 0.0244(0.0295) Steps 946(929.80) | Grad Norm 2.0557(2.0638) | Total Time 14.00(14.00)\n",
      "Iter 16630 | Time 22.6653(22.6514) | Bit/dim 3.5495(3.5369) | Xent 0.0905(0.0865) | Loss 3.5947(3.5802) | Error 0.0256(0.0297) Steps 958(932.45) | Grad Norm 2.4152(2.0966) | Total Time 14.00(14.00)\n",
      "Iter 16640 | Time 22.9978(22.6304) | Bit/dim 3.5455(3.5383) | Xent 0.0789(0.0849) | Loss 3.5849(3.5808) | Error 0.0267(0.0291) Steps 916(931.18) | Grad Norm 2.4391(2.0759) | Total Time 14.00(14.00)\n",
      "Iter 16650 | Time 22.3332(22.5699) | Bit/dim 3.5619(3.5385) | Xent 0.0826(0.0846) | Loss 3.6032(3.5808) | Error 0.0267(0.0288) Steps 964(930.20) | Grad Norm 2.8491(2.0380) | Total Time 14.00(14.00)\n",
      "Iter 16660 | Time 22.2479(22.5194) | Bit/dim 3.5311(3.5377) | Xent 0.0894(0.0845) | Loss 3.5758(3.5800) | Error 0.0356(0.0290) Steps 928(929.44) | Grad Norm 2.0581(1.9895) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 107.3627, Epoch Time 1364.9729(1332.9078), Bit/dim 3.5506(best: 3.5488), Xent 1.1534, Loss 4.1273, Error 0.2351(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 22.4777(22.4440) | Bit/dim 3.4957(3.5374) | Xent 0.0749(0.0840) | Loss 3.5331(3.5794) | Error 0.0222(0.0291) Steps 910(928.64) | Grad Norm 1.5864(1.9196) | Total Time 14.00(14.00)\n",
      "Iter 16680 | Time 21.8096(22.3848) | Bit/dim 3.5607(3.5357) | Xent 0.0827(0.0838) | Loss 3.6021(3.5776) | Error 0.0289(0.0290) Steps 940(929.01) | Grad Norm 2.4966(1.9181) | Total Time 14.00(14.00)\n",
      "Iter 16690 | Time 22.6979(22.4487) | Bit/dim 3.5382(3.5367) | Xent 0.0811(0.0859) | Loss 3.5787(3.5797) | Error 0.0278(0.0294) Steps 922(927.61) | Grad Norm 3.0692(2.0824) | Total Time 14.00(14.00)\n",
      "Iter 16700 | Time 22.5886(22.5315) | Bit/dim 3.5425(3.5383) | Xent 0.0615(0.0838) | Loss 3.5733(3.5802) | Error 0.0189(0.0283) Steps 916(926.35) | Grad Norm 1.8899(2.1516) | Total Time 14.00(14.00)\n",
      "Iter 16710 | Time 22.7284(22.6100) | Bit/dim 3.5197(3.5390) | Xent 0.0625(0.0842) | Loss 3.5510(3.5811) | Error 0.0222(0.0283) Steps 916(925.64) | Grad Norm 1.4785(2.1437) | Total Time 14.00(14.00)\n",
      "Iter 16720 | Time 22.8816(22.6055) | Bit/dim 3.5560(3.5395) | Xent 0.0965(0.0846) | Loss 3.6042(3.5818) | Error 0.0322(0.0291) Steps 928(926.05) | Grad Norm 2.9628(2.2138) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 107.9867, Epoch Time 1370.1329(1334.0245), Bit/dim 3.5497(best: 3.5488), Xent 1.1655, Loss 4.1324, Error 0.2343(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 22.1322(22.5665) | Bit/dim 3.5295(3.5386) | Xent 0.0929(0.0843) | Loss 3.5759(3.5808) | Error 0.0322(0.0291) Steps 928(924.95) | Grad Norm 1.8721(2.1795) | Total Time 14.00(14.00)\n",
      "Iter 16740 | Time 22.5903(22.6328) | Bit/dim 3.5217(3.5379) | Xent 0.0693(0.0841) | Loss 3.5564(3.5800) | Error 0.0211(0.0291) Steps 952(925.98) | Grad Norm 1.7180(2.2065) | Total Time 14.00(14.00)\n",
      "Iter 16750 | Time 23.2416(22.6462) | Bit/dim 3.5194(3.5398) | Xent 0.0797(0.0830) | Loss 3.5593(3.5812) | Error 0.0300(0.0290) Steps 916(927.65) | Grad Norm 1.7173(2.1423) | Total Time 14.00(14.00)\n",
      "Iter 16760 | Time 22.4752(22.6108) | Bit/dim 3.5704(3.5392) | Xent 0.0677(0.0824) | Loss 3.6042(3.5804) | Error 0.0222(0.0288) Steps 934(928.21) | Grad Norm 1.5558(2.0402) | Total Time 14.00(14.00)\n",
      "Iter 16770 | Time 23.6298(22.6299) | Bit/dim 3.5232(3.5378) | Xent 0.0783(0.0836) | Loss 3.5623(3.5796) | Error 0.0278(0.0289) Steps 952(929.95) | Grad Norm 1.1498(1.9349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 108.3602, Epoch Time 1370.5131(1335.1192), Bit/dim 3.5512(best: 3.5488), Xent 1.1662, Loss 4.1344, Error 0.2323(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 22.8306(22.6080) | Bit/dim 3.5306(3.5396) | Xent 0.0654(0.0819) | Loss 3.5634(3.5805) | Error 0.0222(0.0283) Steps 946(929.05) | Grad Norm 1.3213(1.8545) | Total Time 14.00(14.00)\n",
      "Iter 16790 | Time 22.4185(22.6112) | Bit/dim 3.5599(3.5401) | Xent 0.0949(0.0826) | Loss 3.6074(3.5814) | Error 0.0367(0.0289) Steps 934(927.76) | Grad Norm 1.8060(1.8378) | Total Time 14.00(14.00)\n",
      "Iter 16800 | Time 22.3612(22.6134) | Bit/dim 3.5410(3.5399) | Xent 0.1185(0.0823) | Loss 3.6003(3.5811) | Error 0.0422(0.0289) Steps 946(929.70) | Grad Norm 2.1103(1.8169) | Total Time 14.00(14.00)\n",
      "Iter 16810 | Time 22.9011(22.6485) | Bit/dim 3.5222(3.5375) | Xent 0.0783(0.0799) | Loss 3.5613(3.5775) | Error 0.0278(0.0280) Steps 922(929.48) | Grad Norm 1.8569(1.8013) | Total Time 14.00(14.00)\n",
      "Iter 16820 | Time 22.3977(22.6348) | Bit/dim 3.5949(3.5389) | Xent 0.0728(0.0815) | Loss 3.6313(3.5796) | Error 0.0200(0.0285) Steps 922(929.60) | Grad Norm 2.1067(1.8625) | Total Time 14.00(14.00)\n",
      "Iter 16830 | Time 22.4247(22.6844) | Bit/dim 3.5205(3.5365) | Xent 0.0910(0.0824) | Loss 3.5660(3.5777) | Error 0.0333(0.0286) Steps 922(930.63) | Grad Norm 2.2988(1.9654) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 108.4821, Epoch Time 1375.2730(1336.3238), Bit/dim 3.5484(best: 3.5488), Xent 1.1596, Loss 4.1282, Error 0.2341(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 22.4791(22.5979) | Bit/dim 3.5318(3.5392) | Xent 0.0751(0.0819) | Loss 3.5694(3.5802) | Error 0.0267(0.0280) Steps 934(927.16) | Grad Norm 1.5620(1.9911) | Total Time 14.00(14.00)\n",
      "Iter 16850 | Time 22.2774(22.5604) | Bit/dim 3.5265(3.5370) | Xent 0.0574(0.0819) | Loss 3.5552(3.5779) | Error 0.0178(0.0278) Steps 934(925.99) | Grad Norm 1.1102(1.9005) | Total Time 14.00(14.00)\n",
      "Iter 16860 | Time 22.2040(22.5630) | Bit/dim 3.5657(3.5369) | Xent 0.0680(0.0825) | Loss 3.5997(3.5782) | Error 0.0256(0.0282) Steps 910(924.55) | Grad Norm 1.6853(1.8352) | Total Time 14.00(14.00)\n",
      "Iter 16870 | Time 22.4614(22.5651) | Bit/dim 3.5446(3.5391) | Xent 0.1178(0.0840) | Loss 3.6035(3.5812) | Error 0.0500(0.0295) Steps 922(924.82) | Grad Norm 3.0672(1.8325) | Total Time 14.00(14.00)\n",
      "Iter 16880 | Time 23.3022(22.6222) | Bit/dim 3.5580(3.5382) | Xent 0.0585(0.0819) | Loss 3.5872(3.5792) | Error 0.0211(0.0289) Steps 952(926.53) | Grad Norm 1.7374(1.8500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 108.1702, Epoch Time 1367.8302(1337.2690), Bit/dim 3.5492(best: 3.5484), Xent 1.1763, Loss 4.1373, Error 0.2316(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 22.7223(22.6073) | Bit/dim 3.5383(3.5360) | Xent 0.0677(0.0828) | Loss 3.5722(3.5774) | Error 0.0233(0.0292) Steps 928(926.26) | Grad Norm 1.5608(1.9220) | Total Time 14.00(14.00)\n",
      "Iter 16900 | Time 23.5149(22.6636) | Bit/dim 3.5466(3.5359) | Xent 0.0841(0.0826) | Loss 3.5886(3.5772) | Error 0.0267(0.0292) Steps 958(930.40) | Grad Norm 1.9401(1.9588) | Total Time 14.00(14.00)\n",
      "Iter 16910 | Time 22.9465(22.6622) | Bit/dim 3.5119(3.5385) | Xent 0.0894(0.0823) | Loss 3.5566(3.5796) | Error 0.0322(0.0285) Steps 946(930.52) | Grad Norm 2.8102(1.9391) | Total Time 14.00(14.00)\n",
      "Iter 16920 | Time 22.2824(22.6048) | Bit/dim 3.5477(3.5402) | Xent 0.0735(0.0829) | Loss 3.5844(3.5816) | Error 0.0256(0.0282) Steps 922(929.82) | Grad Norm 1.2107(1.9324) | Total Time 14.00(14.00)\n",
      "Iter 16930 | Time 22.7335(22.6149) | Bit/dim 3.5128(3.5380) | Xent 0.0755(0.0841) | Loss 3.5506(3.5800) | Error 0.0300(0.0287) Steps 904(926.97) | Grad Norm 2.3546(2.0435) | Total Time 14.00(14.00)\n",
      "Iter 16940 | Time 22.3597(22.6378) | Bit/dim 3.5173(3.5379) | Xent 0.0791(0.0824) | Loss 3.5568(3.5791) | Error 0.0244(0.0288) Steps 946(928.64) | Grad Norm 2.0890(2.0222) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 108.1546, Epoch Time 1372.3509(1338.3214), Bit/dim 3.5488(best: 3.5484), Xent 1.1676, Loss 4.1326, Error 0.2318(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 22.8034(22.6090) | Bit/dim 3.5513(3.5396) | Xent 0.0823(0.0821) | Loss 3.5924(3.5806) | Error 0.0289(0.0283) Steps 934(928.05) | Grad Norm 1.2188(1.9828) | Total Time 14.00(14.00)\n",
      "Iter 16960 | Time 22.2592(22.5803) | Bit/dim 3.5183(3.5362) | Xent 0.0601(0.0807) | Loss 3.5483(3.5766) | Error 0.0200(0.0280) Steps 946(927.93) | Grad Norm 1.4784(1.9550) | Total Time 14.00(14.00)\n",
      "Iter 16970 | Time 22.4616(22.5522) | Bit/dim 3.5257(3.5359) | Xent 0.1051(0.0808) | Loss 3.5782(3.5763) | Error 0.0367(0.0280) Steps 904(925.23) | Grad Norm 1.5414(1.8531) | Total Time 14.00(14.00)\n",
      "Iter 16980 | Time 22.7406(22.6154) | Bit/dim 3.5328(3.5359) | Xent 0.0630(0.0806) | Loss 3.5643(3.5762) | Error 0.0256(0.0281) Steps 928(926.23) | Grad Norm 1.7983(1.8772) | Total Time 14.00(14.00)\n",
      "Iter 16990 | Time 22.9813(22.6403) | Bit/dim 3.5431(3.5402) | Xent 0.0884(0.0814) | Loss 3.5873(3.5809) | Error 0.0322(0.0283) Steps 952(926.51) | Grad Norm 1.8322(1.9145) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 108.5711, Epoch Time 1371.5871(1339.3194), Bit/dim 3.5484(best: 3.5484), Xent 1.1729, Loss 4.1348, Error 0.2341(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 22.4198(22.6824) | Bit/dim 3.5573(3.5397) | Xent 0.0728(0.0818) | Loss 3.5937(3.5806) | Error 0.0289(0.0288) Steps 928(928.96) | Grad Norm 1.9735(1.9172) | Total Time 14.00(14.00)\n",
      "Iter 17010 | Time 22.4220(22.6118) | Bit/dim 3.5535(3.5401) | Xent 0.1019(0.0832) | Loss 3.6045(3.5817) | Error 0.0367(0.0291) Steps 916(927.69) | Grad Norm 2.4968(1.9471) | Total Time 14.00(14.00)\n",
      "Iter 17020 | Time 22.6145(22.5847) | Bit/dim 3.5519(3.5392) | Xent 0.0716(0.0815) | Loss 3.5877(3.5800) | Error 0.0278(0.0286) Steps 910(925.32) | Grad Norm 1.5779(1.9554) | Total Time 14.00(14.00)\n",
      "Iter 17030 | Time 22.5955(22.5911) | Bit/dim 3.5479(3.5361) | Xent 0.0777(0.0827) | Loss 3.5867(3.5775) | Error 0.0244(0.0284) Steps 922(926.31) | Grad Norm 1.7867(1.9643) | Total Time 14.00(14.00)\n",
      "Iter 17040 | Time 22.4335(22.5664) | Bit/dim 3.5404(3.5367) | Xent 0.0673(0.0840) | Loss 3.5741(3.5787) | Error 0.0256(0.0289) Steps 916(926.43) | Grad Norm 1.8949(1.9805) | Total Time 14.00(14.00)\n",
      "Iter 17050 | Time 22.6969(22.5068) | Bit/dim 3.5298(3.5374) | Xent 0.0674(0.0850) | Loss 3.5635(3.5799) | Error 0.0222(0.0293) Steps 934(925.68) | Grad Norm 1.3288(2.0576) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 108.6269, Epoch Time 1364.5861(1340.0774), Bit/dim 3.5491(best: 3.5484), Xent 1.1729, Loss 4.1355, Error 0.2366(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 22.7376(22.4987) | Bit/dim 3.5426(3.5367) | Xent 0.0761(0.0838) | Loss 3.5806(3.5786) | Error 0.0233(0.0287) Steps 928(924.98) | Grad Norm 2.8745(2.0669) | Total Time 14.00(14.00)\n",
      "Iter 17070 | Time 22.1705(22.5626) | Bit/dim 3.5316(3.5374) | Xent 0.0937(0.0843) | Loss 3.5785(3.5795) | Error 0.0367(0.0284) Steps 940(927.00) | Grad Norm 2.4953(2.1298) | Total Time 14.00(14.00)\n",
      "Iter 17080 | Time 22.4358(22.5631) | Bit/dim 3.5763(3.5377) | Xent 0.0832(0.0828) | Loss 3.6179(3.5791) | Error 0.0289(0.0286) Steps 934(925.14) | Grad Norm 2.2612(2.1368) | Total Time 14.00(14.00)\n",
      "Iter 17090 | Time 22.6585(22.5379) | Bit/dim 3.5210(3.5355) | Xent 0.0730(0.0818) | Loss 3.5575(3.5764) | Error 0.0244(0.0279) Steps 922(926.62) | Grad Norm 1.4909(2.1253) | Total Time 14.00(14.00)\n",
      "Iter 17100 | Time 22.3357(22.5382) | Bit/dim 3.5541(3.5371) | Xent 0.0872(0.0803) | Loss 3.5977(3.5773) | Error 0.0344(0.0274) Steps 922(925.67) | Grad Norm 1.8530(2.0643) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 108.3222, Epoch Time 1367.7373(1340.9072), Bit/dim 3.5483(best: 3.5484), Xent 1.1896, Loss 4.1431, Error 0.2329(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 22.5494(22.5588) | Bit/dim 3.5091(3.5363) | Xent 0.0755(0.0808) | Loss 3.5468(3.5767) | Error 0.0311(0.0280) Steps 928(922.83) | Grad Norm 1.9863(2.0484) | Total Time 14.00(14.00)\n",
      "Iter 17120 | Time 23.1901(22.6935) | Bit/dim 3.5257(3.5329) | Xent 0.0930(0.0804) | Loss 3.5721(3.5730) | Error 0.0267(0.0276) Steps 928(924.50) | Grad Norm 2.0762(2.0950) | Total Time 14.00(14.00)\n",
      "Iter 17130 | Time 23.1792(22.7015) | Bit/dim 3.5210(3.5354) | Xent 0.0894(0.0818) | Loss 3.5657(3.5763) | Error 0.0300(0.0281) Steps 952(928.41) | Grad Norm 2.1818(2.0943) | Total Time 14.00(14.00)\n",
      "Iter 17140 | Time 23.2143(22.7281) | Bit/dim 3.5125(3.5365) | Xent 0.0772(0.0801) | Loss 3.5511(3.5765) | Error 0.0289(0.0276) Steps 934(926.86) | Grad Norm 2.3233(2.0618) | Total Time 14.00(14.00)\n",
      "Iter 17150 | Time 22.3319(22.6213) | Bit/dim 3.5305(3.5377) | Xent 0.0919(0.0808) | Loss 3.5764(3.5781) | Error 0.0344(0.0281) Steps 922(926.38) | Grad Norm 1.5352(2.0415) | Total Time 14.00(14.00)\n",
      "Iter 17160 | Time 22.7428(22.5644) | Bit/dim 3.5395(3.5380) | Xent 0.0956(0.0826) | Loss 3.5874(3.5793) | Error 0.0367(0.0290) Steps 946(927.53) | Grad Norm 1.6438(2.0339) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 108.1102, Epoch Time 1373.5474(1341.8864), Bit/dim 3.5488(best: 3.5483), Xent 1.1860, Loss 4.1418, Error 0.2324(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 22.4891(22.5993) | Bit/dim 3.5357(3.5396) | Xent 0.0745(0.0815) | Loss 3.5729(3.5803) | Error 0.0244(0.0283) Steps 928(927.85) | Grad Norm 2.4329(2.0387) | Total Time 14.00(14.00)\n",
      "Iter 17180 | Time 22.2419(22.6190) | Bit/dim 3.5450(3.5378) | Xent 0.0982(0.0819) | Loss 3.5941(3.5788) | Error 0.0356(0.0282) Steps 934(927.91) | Grad Norm 2.1972(2.0050) | Total Time 14.00(14.00)\n",
      "Iter 17190 | Time 22.1777(22.6050) | Bit/dim 3.5545(3.5390) | Xent 0.0868(0.0834) | Loss 3.5979(3.5807) | Error 0.0311(0.0285) Steps 928(927.84) | Grad Norm 3.1358(2.1088) | Total Time 14.00(14.00)\n",
      "Iter 17200 | Time 22.4485(22.6252) | Bit/dim 3.5411(3.5380) | Xent 0.0787(0.0831) | Loss 3.5804(3.5795) | Error 0.0311(0.0286) Steps 928(931.14) | Grad Norm 2.2294(2.0667) | Total Time 14.00(14.00)\n",
      "Iter 17210 | Time 22.1107(22.6218) | Bit/dim 3.5194(3.5359) | Xent 0.0855(0.0830) | Loss 3.5621(3.5774) | Error 0.0311(0.0291) Steps 922(929.63) | Grad Norm 1.7751(2.0397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 109.3797, Epoch Time 1374.7659(1342.8728), Bit/dim 3.5491(best: 3.5483), Xent 1.1828, Loss 4.1405, Error 0.2322(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 21.9351(22.6659) | Bit/dim 3.5452(3.5353) | Xent 0.1006(0.0830) | Loss 3.5955(3.5768) | Error 0.0378(0.0295) Steps 916(929.41) | Grad Norm 2.7378(2.0538) | Total Time 14.00(14.00)\n",
      "Iter 17230 | Time 21.9478(22.7106) | Bit/dim 3.5521(3.5374) | Xent 0.0752(0.0811) | Loss 3.5897(3.5780) | Error 0.0267(0.0287) Steps 952(930.18) | Grad Norm 2.7255(1.9900) | Total Time 14.00(14.00)\n",
      "Iter 17240 | Time 22.7118(22.7167) | Bit/dim 3.5526(3.5403) | Xent 0.0838(0.0802) | Loss 3.5945(3.5804) | Error 0.0267(0.0284) Steps 940(931.19) | Grad Norm 1.4679(1.8994) | Total Time 14.00(14.00)\n",
      "Iter 17250 | Time 22.3422(22.6585) | Bit/dim 3.5159(3.5363) | Xent 0.0833(0.0795) | Loss 3.5575(3.5760) | Error 0.0300(0.0282) Steps 910(929.72) | Grad Norm 1.5187(1.8802) | Total Time 14.00(14.00)\n",
      "Iter 17260 | Time 22.8887(22.6724) | Bit/dim 3.5752(3.5355) | Xent 0.0672(0.0808) | Loss 3.6087(3.5759) | Error 0.0222(0.0286) Steps 940(929.87) | Grad Norm 1.3880(1.8398) | Total Time 14.00(14.00)\n",
      "Iter 17270 | Time 21.9965(22.5745) | Bit/dim 3.5781(3.5367) | Xent 0.0860(0.0811) | Loss 3.6211(3.5773) | Error 0.0300(0.0286) Steps 922(929.83) | Grad Norm 1.9005(1.8892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 108.8191, Epoch Time 1373.2528(1343.7842), Bit/dim 3.5491(best: 3.5483), Xent 1.1909, Loss 4.1446, Error 0.2275(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 22.5515(22.5971) | Bit/dim 3.5430(3.5369) | Xent 0.0901(0.0808) | Loss 3.5880(3.5773) | Error 0.0289(0.0282) Steps 928(928.54) | Grad Norm 2.2670(1.8970) | Total Time 14.00(14.00)\n",
      "Iter 17290 | Time 22.2339(22.6465) | Bit/dim 3.5500(3.5361) | Xent 0.0797(0.0799) | Loss 3.5899(3.5760) | Error 0.0311(0.0279) Steps 922(930.65) | Grad Norm 1.8258(1.8029) | Total Time 14.00(14.00)\n",
      "Iter 17300 | Time 22.5253(22.6386) | Bit/dim 3.5281(3.5387) | Xent 0.1069(0.0816) | Loss 3.5815(3.5795) | Error 0.0422(0.0284) Steps 922(931.71) | Grad Norm 2.3893(1.8784) | Total Time 14.00(14.00)\n",
      "Iter 17310 | Time 22.1613(22.5697) | Bit/dim 3.5557(3.5410) | Xent 0.0811(0.0825) | Loss 3.5962(3.5822) | Error 0.0267(0.0288) Steps 928(930.13) | Grad Norm 2.2170(1.9628) | Total Time 14.00(14.00)\n",
      "Iter 17320 | Time 22.7297(22.5304) | Bit/dim 3.5289(3.5368) | Xent 0.0764(0.0830) | Loss 3.5671(3.5783) | Error 0.0278(0.0291) Steps 922(927.94) | Grad Norm 1.8007(1.9361) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 107.5805, Epoch Time 1368.3748(1344.5219), Bit/dim 3.5471(best: 3.5483), Xent 1.1770, Loss 4.1356, Error 0.2332(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 22.1957(22.5410) | Bit/dim 3.5302(3.5385) | Xent 0.1099(0.0830) | Loss 3.5851(3.5800) | Error 0.0411(0.0285) Steps 934(927.85) | Grad Norm 2.4020(1.9230) | Total Time 14.00(14.00)\n",
      "Iter 17340 | Time 22.3481(22.4504) | Bit/dim 3.5063(3.5340) | Xent 0.0804(0.0818) | Loss 3.5465(3.5749) | Error 0.0311(0.0284) Steps 928(928.46) | Grad Norm 2.2426(1.9682) | Total Time 14.00(14.00)\n",
      "Iter 17350 | Time 22.7325(22.4027) | Bit/dim 3.5420(3.5342) | Xent 0.0862(0.0794) | Loss 3.5851(3.5739) | Error 0.0289(0.0276) Steps 934(927.11) | Grad Norm 2.2588(1.9388) | Total Time 14.00(14.00)\n",
      "Iter 17360 | Time 22.1854(22.3627) | Bit/dim 3.5593(3.5370) | Xent 0.0740(0.0784) | Loss 3.5963(3.5762) | Error 0.0244(0.0270) Steps 922(926.32) | Grad Norm 1.5395(1.9309) | Total Time 14.00(14.00)\n",
      "Iter 17370 | Time 22.5194(22.3617) | Bit/dim 3.5331(3.5364) | Xent 0.0779(0.0782) | Loss 3.5721(3.5755) | Error 0.0244(0.0269) Steps 940(926.33) | Grad Norm 1.8215(1.9166) | Total Time 14.00(14.00)\n",
      "Iter 17380 | Time 21.5512(22.2740) | Bit/dim 3.5485(3.5371) | Xent 0.0820(0.0787) | Loss 3.5895(3.5764) | Error 0.0267(0.0272) Steps 922(926.28) | Grad Norm 1.8433(1.8988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 109.1271, Epoch Time 1352.0728(1344.7484), Bit/dim 3.5465(best: 3.5471), Xent 1.1877, Loss 4.1404, Error 0.2325(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 22.2855(22.2738) | Bit/dim 3.5515(3.5380) | Xent 0.0806(0.0794) | Loss 3.5918(3.5777) | Error 0.0244(0.0274) Steps 928(926.93) | Grad Norm 1.7465(1.8825) | Total Time 14.00(14.00)\n",
      "Iter 17400 | Time 22.2585(22.2592) | Bit/dim 3.5301(3.5361) | Xent 0.1007(0.0786) | Loss 3.5804(3.5754) | Error 0.0400(0.0274) Steps 910(925.10) | Grad Norm 1.7839(1.8411) | Total Time 14.00(14.00)\n",
      "Iter 17410 | Time 22.3969(22.2856) | Bit/dim 3.5325(3.5379) | Xent 0.0773(0.0788) | Loss 3.5712(3.5773) | Error 0.0233(0.0277) Steps 928(926.50) | Grad Norm 1.8990(1.8322) | Total Time 14.00(14.00)\n",
      "Iter 17420 | Time 22.3782(22.2698) | Bit/dim 3.5233(3.5359) | Xent 0.0840(0.0792) | Loss 3.5653(3.5755) | Error 0.0311(0.0278) Steps 916(925.21) | Grad Norm 2.5438(1.8760) | Total Time 14.00(14.00)\n",
      "Iter 17430 | Time 22.1660(22.2588) | Bit/dim 3.5217(3.5348) | Xent 0.0910(0.0803) | Loss 3.5672(3.5750) | Error 0.0256(0.0282) Steps 922(924.43) | Grad Norm 1.7161(1.9676) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 109.7511, Epoch Time 1352.7187(1344.9875), Bit/dim 3.5481(best: 3.5465), Xent 1.1917, Loss 4.1439, Error 0.2293(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 22.1561(22.2475) | Bit/dim 3.5334(3.5357) | Xent 0.0700(0.0801) | Loss 3.5684(3.5758) | Error 0.0311(0.0279) Steps 916(926.15) | Grad Norm 1.4441(1.9227) | Total Time 14.00(14.00)\n",
      "Iter 17450 | Time 22.1097(22.2475) | Bit/dim 3.5144(3.5352) | Xent 0.0608(0.0809) | Loss 3.5448(3.5756) | Error 0.0200(0.0285) Steps 952(927.42) | Grad Norm 1.2747(1.9043) | Total Time 14.00(14.00)\n",
      "Iter 17460 | Time 21.8903(22.2488) | Bit/dim 3.5090(3.5346) | Xent 0.0776(0.0816) | Loss 3.5478(3.5754) | Error 0.0244(0.0285) Steps 940(927.79) | Grad Norm 2.8701(1.9624) | Total Time 14.00(14.00)\n",
      "Iter 17470 | Time 22.3844(22.2377) | Bit/dim 3.5764(3.5381) | Xent 0.0854(0.0820) | Loss 3.6191(3.5791) | Error 0.0256(0.0286) Steps 940(930.63) | Grad Norm 1.9715(1.9991) | Total Time 14.00(14.00)\n",
      "Iter 17480 | Time 22.2749(22.2310) | Bit/dim 3.5363(3.5375) | Xent 0.0742(0.0805) | Loss 3.5734(3.5778) | Error 0.0256(0.0282) Steps 904(927.87) | Grad Norm 2.4298(2.0231) | Total Time 14.00(14.00)\n",
      "Iter 17490 | Time 22.0846(22.2625) | Bit/dim 3.5383(3.5367) | Xent 0.0496(0.0788) | Loss 3.5631(3.5761) | Error 0.0144(0.0276) Steps 922(924.58) | Grad Norm 1.4983(2.0083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 110.5357, Epoch Time 1354.1477(1345.2623), Bit/dim 3.5470(best: 3.5465), Xent 1.2154, Loss 4.1547, Error 0.2352(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 22.1957(22.2099) | Bit/dim 3.5156(3.5358) | Xent 0.0799(0.0799) | Loss 3.5555(3.5758) | Error 0.0256(0.0280) Steps 928(928.20) | Grad Norm 1.6714(2.0015) | Total Time 14.00(14.00)\n",
      "Iter 17510 | Time 22.7077(22.1551) | Bit/dim 3.5422(3.5373) | Xent 0.0865(0.0795) | Loss 3.5855(3.5770) | Error 0.0300(0.0281) Steps 946(927.22) | Grad Norm 1.9709(1.9655) | Total Time 14.00(14.00)\n",
      "Iter 17520 | Time 21.8939(22.1416) | Bit/dim 3.5178(3.5374) | Xent 0.1051(0.0797) | Loss 3.5703(3.5773) | Error 0.0378(0.0279) Steps 940(926.15) | Grad Norm 3.5521(1.9883) | Total Time 14.00(14.00)\n",
      "Iter 17530 | Time 22.1062(22.1604) | Bit/dim 3.5429(3.5386) | Xent 0.0883(0.0814) | Loss 3.5870(3.5793) | Error 0.0389(0.0290) Steps 916(924.63) | Grad Norm 3.2988(2.1832) | Total Time 14.00(14.00)\n",
      "Iter 17540 | Time 22.5861(22.2550) | Bit/dim 3.5065(3.5377) | Xent 0.0785(0.0809) | Loss 3.5457(3.5782) | Error 0.0311(0.0287) Steps 934(925.39) | Grad Norm 2.2648(2.1389) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 109.7563, Epoch Time 1350.7100(1345.4258), Bit/dim 3.5489(best: 3.5465), Xent 1.2124, Loss 4.1551, Error 0.2328(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 22.1496(22.3354) | Bit/dim 3.5395(3.5376) | Xent 0.0900(0.0807) | Loss 3.5845(3.5780) | Error 0.0322(0.0286) Steps 928(925.20) | Grad Norm 1.3808(2.0265) | Total Time 14.00(14.00)\n",
      "Iter 17560 | Time 23.1846(22.3718) | Bit/dim 3.5178(3.5370) | Xent 0.1036(0.0826) | Loss 3.5696(3.5783) | Error 0.0344(0.0293) Steps 922(925.75) | Grad Norm 2.5753(2.1036) | Total Time 14.00(14.00)\n",
      "Iter 17570 | Time 22.4353(22.3570) | Bit/dim 3.5369(3.5371) | Xent 0.0815(0.0819) | Loss 3.5777(3.5781) | Error 0.0289(0.0294) Steps 928(926.02) | Grad Norm 1.8881(2.1182) | Total Time 14.00(14.00)\n",
      "Iter 17580 | Time 22.0175(22.3238) | Bit/dim 3.5220(3.5370) | Xent 0.0780(0.0811) | Loss 3.5610(3.5775) | Error 0.0311(0.0291) Steps 928(925.69) | Grad Norm 2.0525(2.1539) | Total Time 14.00(14.00)\n",
      "Iter 17590 | Time 22.5320(22.3841) | Bit/dim 3.5689(3.5389) | Xent 0.1024(0.0800) | Loss 3.6201(3.5789) | Error 0.0344(0.0286) Steps 916(928.61) | Grad Norm 2.2821(2.1207) | Total Time 14.00(14.00)\n",
      "Iter 17600 | Time 22.9703(22.3878) | Bit/dim 3.5352(3.5350) | Xent 0.0743(0.0804) | Loss 3.5724(3.5753) | Error 0.0311(0.0291) Steps 928(926.38) | Grad Norm 2.5573(2.1048) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 109.9229, Epoch Time 1360.9040(1345.8901), Bit/dim 3.5499(best: 3.5465), Xent 1.2190, Loss 4.1594, Error 0.2327(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 21.9246(22.2715) | Bit/dim 3.5615(3.5345) | Xent 0.0722(0.0800) | Loss 3.5976(3.5745) | Error 0.0256(0.0289) Steps 940(927.06) | Grad Norm 1.5990(2.1288) | Total Time 14.00(14.00)\n",
      "Iter 17620 | Time 22.1886(22.2058) | Bit/dim 3.5748(3.5355) | Xent 0.0888(0.0788) | Loss 3.6191(3.5750) | Error 0.0289(0.0282) Steps 910(926.67) | Grad Norm 2.2918(2.1258) | Total Time 14.00(14.00)\n",
      "Iter 17630 | Time 22.3332(22.2228) | Bit/dim 3.5282(3.5379) | Xent 0.0824(0.0804) | Loss 3.5694(3.5781) | Error 0.0344(0.0289) Steps 910(925.61) | Grad Norm 1.7749(2.1344) | Total Time 14.00(14.00)\n",
      "Iter 17640 | Time 21.8415(22.2195) | Bit/dim 3.5173(3.5362) | Xent 0.0849(0.0809) | Loss 3.5597(3.5767) | Error 0.0300(0.0289) Steps 934(926.66) | Grad Norm 2.6652(2.1414) | Total Time 14.00(14.00)\n",
      "Iter 17650 | Time 22.1540(22.2678) | Bit/dim 3.5374(3.5362) | Xent 0.0758(0.0805) | Loss 3.5753(3.5764) | Error 0.0233(0.0283) Steps 916(927.29) | Grad Norm 1.9953(2.1021) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 109.7323, Epoch Time 1349.9449(1346.0118), Bit/dim 3.5467(best: 3.5465), Xent 1.1991, Loss 4.1463, Error 0.2316(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 22.5975(22.2800) | Bit/dim 3.5191(3.5352) | Xent 0.0720(0.0808) | Loss 3.5551(3.5757) | Error 0.0222(0.0278) Steps 904(928.33) | Grad Norm 2.0767(2.0433) | Total Time 14.00(14.00)\n",
      "Iter 17670 | Time 22.7254(22.3337) | Bit/dim 3.5212(3.5364) | Xent 0.0638(0.0788) | Loss 3.5531(3.5758) | Error 0.0233(0.0274) Steps 904(930.42) | Grad Norm 1.7172(2.0127) | Total Time 14.00(14.00)\n",
      "Iter 17680 | Time 23.1715(22.3510) | Bit/dim 3.5405(3.5376) | Xent 0.0565(0.0769) | Loss 3.5687(3.5761) | Error 0.0189(0.0265) Steps 922(931.42) | Grad Norm 1.5628(1.9148) | Total Time 14.00(14.00)\n",
      "Iter 17690 | Time 23.2142(22.3466) | Bit/dim 3.5318(3.5384) | Xent 0.0827(0.0786) | Loss 3.5732(3.5777) | Error 0.0356(0.0267) Steps 928(929.25) | Grad Norm 1.9804(1.9057) | Total Time 14.00(14.00)\n",
      "Iter 17700 | Time 22.0429(22.3516) | Bit/dim 3.5658(3.5380) | Xent 0.0685(0.0804) | Loss 3.6001(3.5782) | Error 0.0256(0.0279) Steps 940(927.59) | Grad Norm 2.4254(2.0078) | Total Time 14.00(14.00)\n",
      "Iter 17710 | Time 22.2324(22.2376) | Bit/dim 3.5455(3.5354) | Xent 0.0739(0.0806) | Loss 3.5824(3.5756) | Error 0.0267(0.0278) Steps 934(926.03) | Grad Norm 1.6765(1.9547) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 111.8864, Epoch Time 1356.7809(1346.3348), Bit/dim 3.5478(best: 3.5465), Xent 1.2027, Loss 4.1492, Error 0.2337(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 22.3135(22.1950) | Bit/dim 3.5624(3.5371) | Xent 0.0773(0.0801) | Loss 3.6010(3.5771) | Error 0.0233(0.0278) Steps 934(927.26) | Grad Norm 1.8958(1.9882) | Total Time 14.00(14.00)\n",
      "Iter 17730 | Time 22.7858(22.1653) | Bit/dim 3.5340(3.5399) | Xent 0.1022(0.0813) | Loss 3.5851(3.5805) | Error 0.0356(0.0284) Steps 934(928.80) | Grad Norm 2.8913(2.0625) | Total Time 14.00(14.00)\n",
      "Iter 17740 | Time 22.9112(22.2938) | Bit/dim 3.5303(3.5385) | Xent 0.0920(0.0811) | Loss 3.5763(3.5790) | Error 0.0300(0.0289) Steps 940(930.45) | Grad Norm 2.5763(2.1176) | Total Time 14.00(14.00)\n",
      "Iter 17750 | Time 21.3588(22.2950) | Bit/dim 3.5266(3.5374) | Xent 0.0796(0.0811) | Loss 3.5664(3.5779) | Error 0.0311(0.0285) Steps 946(930.52) | Grad Norm 3.0298(2.1975) | Total Time 14.00(14.00)\n",
      "Iter 17760 | Time 21.8265(22.2718) | Bit/dim 3.5400(3.5362) | Xent 0.0660(0.0804) | Loss 3.5731(3.5764) | Error 0.0178(0.0279) Steps 940(928.87) | Grad Norm 1.4710(2.1693) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 109.6019, Epoch Time 1354.0556(1346.5665), Bit/dim 3.5475(best: 3.5465), Xent 1.2369, Loss 4.1660, Error 0.2362(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 22.9335(22.3438) | Bit/dim 3.5342(3.5347) | Xent 0.0780(0.0803) | Loss 3.5732(3.5749) | Error 0.0300(0.0283) Steps 916(929.61) | Grad Norm 4.0906(2.1823) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run2/epoch_250_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
