{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.125, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_125_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=768, bias=True)\n",
      "  (project_class): LinearZeros(in_features=384, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1372726\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.9582(33.2800) | Bit/dim 8.9616(9.2193) | Xent 2.3026(2.3026) | Loss 8.9616(9.2193) | Error 0.9111(0.8952) Steps 574(574.00) | Grad Norm 12.8259(17.2720) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.1687(28.0730) | Bit/dim 8.6397(9.0981) | Xent 2.3026(2.3026) | Loss 8.6397(9.0981) | Error 0.8967(0.8963) Steps 574(574.00) | Grad Norm 4.7806(14.8375) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 13.3897(24.2174) | Bit/dim 8.4646(8.9338) | Xent 2.3026(2.3026) | Loss 8.4646(8.9338) | Error 0.9100(0.8979) Steps 574(574.00) | Grad Norm 3.5643(11.9353) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.7933(21.3869) | Bit/dim 8.2173(8.7636) | Xent 2.3026(2.3026) | Loss 8.2173(8.7636) | Error 0.9033(0.8976) Steps 574(574.00) | Grad Norm 2.9296(9.6059) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.4602(19.3143) | Bit/dim 7.9768(8.5828) | Xent 2.3026(2.3026) | Loss 7.9768(8.5828) | Error 0.9067(0.8978) Steps 574(574.00) | Grad Norm 2.7420(7.8448) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 83.7251, Epoch Time 879.7321(879.7321), Bit/dim 7.7962(best: inf), Xent 2.3026, Loss 7.7962, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.2708(17.8140) | Bit/dim 7.7101(8.3767) | Xent 2.3026(2.3026) | Loss 7.7101(8.3767) | Error 0.8933(0.8998) Steps 574(574.00) | Grad Norm 2.8898(6.4762) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.3587(16.6383) | Bit/dim 7.3756(8.1474) | Xent 2.3026(2.3026) | Loss 7.3756(8.1474) | Error 0.8956(0.8981) Steps 574(574.00) | Grad Norm 2.3195(5.4041) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 13.3862(15.7753) | Bit/dim 7.1729(7.9121) | Xent 2.3026(2.3026) | Loss 7.1729(7.9121) | Error 0.8900(0.8981) Steps 574(574.00) | Grad Norm 1.4067(4.4171) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.2316(15.2912) | Bit/dim 7.0581(7.7012) | Xent 2.3026(2.3026) | Loss 7.0581(7.7012) | Error 0.8744(0.8980) Steps 598(577.62) | Grad Norm 0.8738(3.5391) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.4755(15.3333) | Bit/dim 7.0032(7.5254) | Xent 2.3026(2.3026) | Loss 7.0032(7.5254) | Error 0.9067(0.8998) Steps 610(585.57) | Grad Norm 0.4913(2.7951) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.8359(15.4213) | Bit/dim 6.9739(7.3867) | Xent 2.3026(2.3026) | Loss 6.9739(7.3867) | Error 0.9056(0.9003) Steps 622(593.65) | Grad Norm 0.3893(2.1903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 83.0316, Epoch Time 886.6550(879.9398), Bit/dim 6.9771(best: 7.7962), Xent 2.3026, Loss 6.9771, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 16.7288(15.6337) | Bit/dim 6.9362(7.2732) | Xent 2.3026(2.3026) | Loss 6.9362(7.2732) | Error 0.9089(0.9006) Steps 628(601.91) | Grad Norm 0.5810(1.7478) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.0147(15.7943) | Bit/dim 6.8778(7.1786) | Xent 2.3026(2.3026) | Loss 6.8778(7.1786) | Error 0.8944(0.9002) Steps 628(608.76) | Grad Norm 0.3831(1.4329) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 17.5834(16.0729) | Bit/dim 6.8580(7.0975) | Xent 2.3026(2.3026) | Loss 6.8580(7.0975) | Error 0.9022(0.8999) Steps 634(615.11) | Grad Norm 0.4522(1.1897) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 17.2319(16.3136) | Bit/dim 6.7331(7.0155) | Xent 2.3026(2.3026) | Loss 6.7331(7.0155) | Error 0.8956(0.8995) Steps 640(620.42) | Grad Norm 0.5589(1.0391) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 17.1894(16.4783) | Bit/dim 6.6273(6.9261) | Xent 2.3026(2.3026) | Loss 6.6273(6.9261) | Error 0.9078(0.9006) Steps 640(625.56) | Grad Norm 4.0434(1.1767) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 84.6993, Epoch Time 1019.7263(884.1334), Bit/dim 6.4727(best: 6.9771), Xent 2.3026, Loss 6.4727, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 17.4158(16.5550) | Bit/dim 6.5711(6.8158) | Xent 2.3026(2.3026) | Loss 6.5711(6.8158) | Error 0.9056(0.9000) Steps 640(629.19) | Grad Norm 51.6331(4.3903) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 16.5406(16.6065) | Bit/dim 6.2135(6.6918) | Xent 2.3026(2.3026) | Loss 6.2135(6.6918) | Error 0.9022(0.8990) Steps 640(632.03) | Grad Norm 8.9343(9.2202) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.2974(16.7078) | Bit/dim 6.0470(6.5414) | Xent 2.3026(2.3026) | Loss 6.0470(6.5414) | Error 0.9000(0.8983) Steps 646(634.81) | Grad Norm 19.4266(11.0180) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.6132(16.9660) | Bit/dim 5.8837(6.3825) | Xent 2.3026(2.3026) | Loss 5.8837(6.3825) | Error 0.8956(0.8987) Steps 658(641.22) | Grad Norm 13.7501(11.1667) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.8291(17.1366) | Bit/dim 5.7362(6.2282) | Xent 2.3026(2.3026) | Loss 5.7362(6.2282) | Error 0.9056(0.9003) Steps 664(646.02) | Grad Norm 5.7603(10.0702) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 18.1394(17.2987) | Bit/dim 5.6479(6.0896) | Xent 2.3026(2.3026) | Loss 5.6479(6.0896) | Error 0.9167(0.9011) Steps 676(652.84) | Grad Norm 3.7212(8.9535) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 90.8411, Epoch Time 1059.8218(889.4041), Bit/dim 5.6632(best: 6.4727), Xent 2.3026, Loss 5.6632, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 18.3208(17.4675) | Bit/dim 5.6219(5.9696) | Xent 2.3026(2.3026) | Loss 5.6219(5.9696) | Error 0.8800(0.9012) Steps 688(660.19) | Grad Norm 1.2752(6.9680) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.7324(17.6696) | Bit/dim 5.7267(5.9036) | Xent 2.3026(2.3026) | Loss 5.7267(5.9036) | Error 0.9000(0.9003) Steps 652(663.76) | Grad Norm 19.7929(12.6237) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 16.0745(17.4806) | Bit/dim 5.6171(5.8365) | Xent 2.3026(2.3026) | Loss 5.6171(5.8365) | Error 0.9000(0.9006) Steps 640(659.90) | Grad Norm 3.6929(11.0714) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.0664(17.1395) | Bit/dim 5.6064(5.7757) | Xent 2.3026(2.3026) | Loss 5.6064(5.7757) | Error 0.8978(0.8989) Steps 628(653.57) | Grad Norm 2.7587(9.0724) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.8402(16.8890) | Bit/dim 5.5672(5.7172) | Xent 2.3026(2.3026) | Loss 5.5672(5.7172) | Error 0.9056(0.9002) Steps 640(648.56) | Grad Norm 2.1351(7.2433) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 85.7499, Epoch Time 1041.7511(893.9745), Bit/dim 5.5303(best: 5.6632), Xent 2.3026, Loss 5.5303, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.7665(16.7674) | Bit/dim 5.5054(5.6709) | Xent 2.3026(2.3026) | Loss 5.5054(5.6709) | Error 0.9000(0.9010) Steps 640(646.31) | Grad Norm 1.5532(5.9148) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 16.4107(16.6896) | Bit/dim 5.5115(5.6275) | Xent 2.3026(2.3026) | Loss 5.5115(5.6275) | Error 0.9111(0.9011) Steps 640(644.65) | Grad Norm 1.6964(4.9854) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 16.7070(16.7231) | Bit/dim 5.4459(5.5818) | Xent 2.3026(2.3026) | Loss 5.4459(5.5818) | Error 0.9000(0.9002) Steps 658(647.05) | Grad Norm 4.1554(4.8188) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.0164(16.7656) | Bit/dim 5.4337(5.5375) | Xent 2.3026(2.3026) | Loss 5.4337(5.5375) | Error 0.8833(0.8996) Steps 664(651.07) | Grad Norm 1.2640(4.1786) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 17.0404(16.8604) | Bit/dim 5.3740(5.4973) | Xent 2.3026(2.3026) | Loss 5.3740(5.4973) | Error 0.8989(0.8989) Steps 664(654.32) | Grad Norm 2.1738(4.2583) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.2791(16.9494) | Bit/dim 5.3430(5.4598) | Xent 2.3026(2.3026) | Loss 5.3430(5.4598) | Error 0.9011(0.9004) Steps 676(657.72) | Grad Norm 10.5408(4.7694) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 86.7267, Epoch Time 1032.1578(898.1200), Bit/dim 5.3434(best: 5.5303), Xent 2.3026, Loss 5.3434, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 16.9912(17.0375) | Bit/dim 5.3566(5.4145) | Xent 2.3026(2.3026) | Loss 5.3566(5.4145) | Error 0.9033(0.9000) Steps 676(661.81) | Grad Norm 13.6704(5.0510) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 17.0235(17.1340) | Bit/dim 5.3491(5.3798) | Xent 2.3026(2.3026) | Loss 5.3491(5.3798) | Error 0.8956(0.9011) Steps 676(665.11) | Grad Norm 22.9827(7.5955) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 17.4592(17.1540) | Bit/dim 5.1479(5.3289) | Xent 2.3026(2.3026) | Loss 5.1479(5.3289) | Error 0.9122(0.9015) Steps 676(667.17) | Grad Norm 5.1706(7.0941) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 16.4384(17.0510) | Bit/dim 5.1157(5.2765) | Xent 2.3026(2.3026) | Loss 5.1157(5.2765) | Error 0.8967(0.9011) Steps 664(667.73) | Grad Norm 10.3426(6.5155) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 16.4734(17.0163) | Bit/dim 5.0869(5.2312) | Xent 2.3026(2.3026) | Loss 5.0869(5.2312) | Error 0.8944(0.8995) Steps 658(667.14) | Grad Norm 0.6832(5.9115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 88.9523, Epoch Time 1045.9695(902.5555), Bit/dim 5.0954(best: 5.3434), Xent 2.3026, Loss 5.0954, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 16.9917(16.9166) | Bit/dim 5.0955(5.1897) | Xent 2.3026(2.3026) | Loss 5.0955(5.1897) | Error 0.9122(0.9004) Steps 658(664.74) | Grad Norm 5.1939(6.2653) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.4436(16.8230) | Bit/dim 5.0892(5.1570) | Xent 2.3026(2.3026) | Loss 5.0892(5.1570) | Error 0.9078(0.9012) Steps 646(661.47) | Grad Norm 18.0269(6.7842) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 16.6108(16.7227) | Bit/dim 5.0188(5.1183) | Xent 2.3026(2.3026) | Loss 5.0188(5.1183) | Error 0.8889(0.9000) Steps 646(657.54) | Grad Norm 3.7910(7.3388) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 16.6009(16.6271) | Bit/dim 5.0046(5.0783) | Xent 2.3026(2.3026) | Loss 5.0046(5.0783) | Error 0.9011(0.9007) Steps 640(653.83) | Grad Norm 13.6178(6.9606) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 16.8864(16.5695) | Bit/dim 4.9810(5.0413) | Xent 2.3026(2.3026) | Loss 4.9810(5.0413) | Error 0.8856(0.8998) Steps 652(650.87) | Grad Norm 16.7183(7.5026) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 17.1228(16.5797) | Bit/dim 4.8399(5.0023) | Xent 2.3026(2.3026) | Loss 4.8399(5.0023) | Error 0.9078(0.8993) Steps 652(649.63) | Grad Norm 7.4006(7.9390) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 86.6037, Epoch Time 1011.6523(905.8284), Bit/dim 4.8825(best: 5.0954), Xent 2.3026, Loss 4.8825, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 16.9384(16.6509) | Bit/dim 4.8640(4.9676) | Xent 2.3026(2.3026) | Loss 4.8640(4.9676) | Error 0.9044(0.8994) Steps 664(651.65) | Grad Norm 5.6958(7.5884) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 17.5737(16.8411) | Bit/dim 4.8209(4.9321) | Xent 2.3026(2.3026) | Loss 4.8209(4.9321) | Error 0.9122(0.8990) Steps 676(656.51) | Grad Norm 6.5100(7.7907) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 16.8365(17.0010) | Bit/dim 5.2900(4.9257) | Xent 2.3026(2.3026) | Loss 5.2900(4.9257) | Error 0.8978(0.8997) Steps 670(662.40) | Grad Norm 25.2654(10.2353) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 16.7442(16.9760) | Bit/dim 4.9671(4.9726) | Xent 2.3026(2.3026) | Loss 4.9671(4.9726) | Error 0.8967(0.9001) Steps 670(664.22) | Grad Norm 5.8420(10.9099) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 17.3282(17.0637) | Bit/dim 4.8475(4.9550) | Xent 2.3026(2.3026) | Loss 4.8475(4.9550) | Error 0.8822(0.9008) Steps 676(666.76) | Grad Norm 3.2302(9.5231) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 91.9305, Epoch Time 1056.0800(910.3359), Bit/dim 4.8106(best: 4.8825), Xent 2.3026, Loss 4.8106, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 17.2021(17.1463) | Bit/dim 4.8013(4.9154) | Xent 2.3026(2.3026) | Loss 4.8013(4.9154) | Error 0.9067(0.9000) Steps 670(668.17) | Grad Norm 2.4876(7.8377) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 17.5053(17.2026) | Bit/dim 4.7547(4.8735) | Xent 2.3026(2.3026) | Loss 4.7547(4.8735) | Error 0.8889(0.9005) Steps 682(670.41) | Grad Norm 1.2776(6.4367) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 17.8850(17.3324) | Bit/dim 4.7017(4.8328) | Xent 2.3026(2.3026) | Loss 4.7017(4.8328) | Error 0.9222(0.9004) Steps 694(673.84) | Grad Norm 5.2030(5.6033) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 17.8129(17.4628) | Bit/dim 4.6862(4.7971) | Xent 2.3026(2.3026) | Loss 4.6862(4.7971) | Error 0.9133(0.9011) Steps 694(679.49) | Grad Norm 3.5906(5.0258) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.1633(17.6099) | Bit/dim 4.6983(4.7651) | Xent 2.3026(2.3026) | Loss 4.6983(4.7651) | Error 0.9133(0.9014) Steps 706(686.56) | Grad Norm 4.5758(4.5186) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 17.9309(17.7421) | Bit/dim 4.6910(4.7375) | Xent 2.3026(2.3026) | Loss 4.6910(4.7375) | Error 0.9033(0.8994) Steps 700(692.11) | Grad Norm 14.8674(5.8002) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 96.8648, Epoch Time 1091.4735(915.7700), Bit/dim 4.6591(best: 4.8106), Xent 2.3026, Loss 4.6591, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 17.6840(17.8078) | Bit/dim 4.6060(4.7099) | Xent 2.3026(2.3026) | Loss 4.6060(4.7099) | Error 0.8944(0.8987) Steps 712(696.45) | Grad Norm 3.9519(5.9678) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 17.9614(17.8753) | Bit/dim 4.6096(4.6823) | Xent 2.3026(2.3026) | Loss 4.6096(4.6823) | Error 0.9033(0.9004) Steps 706(699.90) | Grad Norm 7.0530(5.8741) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.5538(17.9829) | Bit/dim 4.6263(4.6632) | Xent 2.3026(2.3026) | Loss 4.6263(4.6632) | Error 0.8956(0.8989) Steps 700(701.74) | Grad Norm 7.1884(7.2875) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.3679(18.0186) | Bit/dim 4.6366(4.6517) | Xent 2.3026(2.3026) | Loss 4.6366(4.6517) | Error 0.8956(0.8984) Steps 712(704.00) | Grad Norm 14.2326(8.4722) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 17.9113(18.0658) | Bit/dim 4.5699(4.6348) | Xent 2.3026(2.3026) | Loss 4.5699(4.6348) | Error 0.9200(0.9004) Steps 712(706.10) | Grad Norm 12.5595(8.9934) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 98.4298, Epoch Time 1113.6375(921.7061), Bit/dim 4.5445(best: 4.6591), Xent 2.3026, Loss 4.5445, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 17.9790(18.0396) | Bit/dim 4.5345(4.6133) | Xent 2.3026(2.3026) | Loss 4.5345(4.6133) | Error 0.8956(0.9015) Steps 712(708.39) | Grad Norm 6.4363(8.6764) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 18.0239(18.0933) | Bit/dim 4.5433(4.5925) | Xent 2.3026(2.3026) | Loss 4.5433(4.5925) | Error 0.8956(0.9005) Steps 706(709.00) | Grad Norm 9.3217(8.2501) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 18.8519(18.2081) | Bit/dim 4.5375(4.5743) | Xent 2.3026(2.3026) | Loss 4.5375(4.5743) | Error 0.9167(0.9011) Steps 718(710.53) | Grad Norm 12.3011(8.0783) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 18.3212(18.2912) | Bit/dim 4.4968(4.5527) | Xent 2.3026(2.3026) | Loss 4.4968(4.5527) | Error 0.8800(0.9005) Steps 700(712.04) | Grad Norm 8.8530(7.7646) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 18.1614(18.3270) | Bit/dim 4.4672(4.5371) | Xent 2.3026(2.3026) | Loss 4.4672(4.5371) | Error 0.9067(0.8998) Steps 694(709.08) | Grad Norm 2.3880(7.7580) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.9679(18.3813) | Bit/dim 4.6151(4.5298) | Xent 2.3026(2.3026) | Loss 4.6151(4.5298) | Error 0.8778(0.8997) Steps 694(706.31) | Grad Norm 18.2757(8.2079) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 92.9320, Epoch Time 1122.9921(927.7446), Bit/dim 4.4732(best: 4.5445), Xent 2.3026, Loss 4.4732, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 18.2523(18.2471) | Bit/dim 4.5948(4.5640) | Xent 2.3026(2.3026) | Loss 4.5948(4.5640) | Error 0.8911(0.8996) Steps 688(702.13) | Grad Norm 9.9988(9.7583) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 17.3224(18.1369) | Bit/dim 4.5066(4.5588) | Xent 2.3026(2.3026) | Loss 4.5066(4.5588) | Error 0.9000(0.9001) Steps 676(697.37) | Grad Norm 4.1363(8.6221) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 17.8399(18.0055) | Bit/dim 4.4655(4.5370) | Xent 2.3026(2.3026) | Loss 4.4655(4.5370) | Error 0.9000(0.9008) Steps 682(691.88) | Grad Norm 3.1503(7.1993) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 17.3809(17.9084) | Bit/dim 4.4411(4.5131) | Xent 2.3026(2.3026) | Loss 4.4411(4.5131) | Error 0.8911(0.9001) Steps 658(685.29) | Grad Norm 0.8523(5.7432) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 17.8712(17.8573) | Bit/dim 4.4037(4.4884) | Xent 2.3026(2.3026) | Loss 4.4037(4.4884) | Error 0.9033(0.8998) Steps 676(680.87) | Grad Norm 1.4804(4.6190) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 89.1913, Epoch Time 1083.4778(932.4166), Bit/dim 4.3936(best: 4.4732), Xent 2.3026, Loss 4.3936, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 17.9714(17.8227) | Bit/dim 4.3872(4.4630) | Xent 2.3026(2.3026) | Loss 4.3872(4.4630) | Error 0.9256(0.9002) Steps 676(679.58) | Grad Norm 1.2560(3.7517) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 18.2869(17.8918) | Bit/dim 4.3688(4.4431) | Xent 2.3026(2.3026) | Loss 4.3688(4.4431) | Error 0.8989(0.9006) Steps 688(680.45) | Grad Norm 4.5418(3.2451) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 17.6447(17.9802) | Bit/dim 4.3745(4.4288) | Xent 2.3026(2.3026) | Loss 4.3745(4.4288) | Error 0.8900(0.8997) Steps 688(682.51) | Grad Norm 5.6761(4.5700) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 17.2731(17.8620) | Bit/dim 4.3602(4.4110) | Xent 2.3026(2.3026) | Loss 4.3602(4.4110) | Error 0.8944(0.9007) Steps 676(682.62) | Grad Norm 3.3020(4.7789) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.0797(17.9340) | Bit/dim 4.5242(4.4136) | Xent 2.3026(2.3026) | Loss 4.5242(4.4136) | Error 0.9089(0.8993) Steps 718(684.46) | Grad Norm 20.6262(6.6008) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 17.9366(17.9764) | Bit/dim 4.3570(4.4077) | Xent 2.3026(2.3026) | Loss 4.3570(4.4077) | Error 0.8956(0.8995) Steps 682(685.22) | Grad Norm 3.2198(6.5342) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 91.0073, Epoch Time 1097.9184(937.3817), Bit/dim 4.3738(best: 4.3936), Xent 2.3026, Loss 4.3738, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.3555(17.9636) | Bit/dim 4.3469(4.3901) | Xent 2.3026(2.3026) | Loss 4.3469(4.3901) | Error 0.9100(0.8996) Steps 676(683.79) | Grad Norm 2.5536(5.9206) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 18.2357(17.9395) | Bit/dim 4.2970(4.3710) | Xent 2.3026(2.3026) | Loss 4.2970(4.3710) | Error 0.8978(0.9007) Steps 694(683.27) | Grad Norm 2.5574(4.9199) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 18.5428(18.0009) | Bit/dim 4.2801(4.3496) | Xent 2.3026(2.3026) | Loss 4.2801(4.3496) | Error 0.8967(0.8996) Steps 706(688.43) | Grad Norm 1.9332(4.3426) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 18.3354(18.0789) | Bit/dim 4.3148(4.3385) | Xent 2.3026(2.3026) | Loss 4.3148(4.3385) | Error 0.9044(0.9001) Steps 718(694.23) | Grad Norm 2.5876(5.2064) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 18.9814(18.3021) | Bit/dim 4.2511(4.3287) | Xent 2.3026(2.3026) | Loss 4.2511(4.3287) | Error 0.9033(0.9004) Steps 742(705.85) | Grad Norm 6.2611(5.8148) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 91.0769, Epoch Time 1112.6530(942.6398), Bit/dim 4.2605(best: 4.3738), Xent 2.3026, Loss 4.2605, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 17.6617(18.2796) | Bit/dim 4.2843(4.3160) | Xent 2.3026(2.3026) | Loss 4.2843(4.3160) | Error 0.8978(0.8992) Steps 694(704.31) | Grad Norm 4.3558(5.7837) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 17.9353(18.3298) | Bit/dim 4.3407(4.3067) | Xent 2.3026(2.3026) | Loss 4.3407(4.3067) | Error 0.8933(0.9003) Steps 712(706.77) | Grad Norm 10.4357(6.5826) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 19.2285(18.3625) | Bit/dim 4.2671(4.3032) | Xent 2.3026(2.3026) | Loss 4.2671(4.3032) | Error 0.8978(0.8999) Steps 712(708.41) | Grad Norm 5.5396(6.9409) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 17.9195(18.3428) | Bit/dim 4.2460(4.2882) | Xent 2.3026(2.3026) | Loss 4.2460(4.2882) | Error 0.8844(0.8994) Steps 706(707.74) | Grad Norm 3.5917(6.4793) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 19.3486(18.5207) | Bit/dim 4.2000(4.2679) | Xent 2.3026(2.3026) | Loss 4.2000(4.2679) | Error 0.8989(0.8995) Steps 736(711.19) | Grad Norm 2.0158(5.7406) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 18.6542(18.7110) | Bit/dim 4.2388(4.2609) | Xent 2.3026(2.3026) | Loss 4.2388(4.2609) | Error 0.8967(0.9000) Steps 724(719.41) | Grad Norm 4.9148(6.2536) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 94.7357, Epoch Time 1137.9359(948.4987), Bit/dim 4.2896(best: 4.2605), Xent 2.3026, Loss 4.2896, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 19.2759(18.8221) | Bit/dim 4.2041(4.2507) | Xent 2.3026(2.3026) | Loss 4.2041(4.2507) | Error 0.8878(0.8999) Steps 736(726.63) | Grad Norm 3.6761(6.1571) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 19.3276(18.9531) | Bit/dim 4.1654(4.2325) | Xent 2.3026(2.3026) | Loss 4.1654(4.2325) | Error 0.9011(0.9008) Steps 742(730.63) | Grad Norm 2.5811(5.2924) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 18.6915(19.0761) | Bit/dim 4.2618(4.2236) | Xent 2.3026(2.3026) | Loss 4.2618(4.2236) | Error 0.9067(0.9013) Steps 730(735.49) | Grad Norm 13.4182(5.6007) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 19.1970(19.1878) | Bit/dim 4.1705(4.2241) | Xent 2.3026(2.3026) | Loss 4.1705(4.2241) | Error 0.9056(0.9008) Steps 736(737.12) | Grad Norm 4.3023(6.3566) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 19.4965(19.2209) | Bit/dim 4.1886(4.2099) | Xent 2.3026(2.3026) | Loss 4.1886(4.2099) | Error 0.8822(0.8997) Steps 736(736.02) | Grad Norm 4.3095(5.9301) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 96.0640, Epoch Time 1178.4980(955.3987), Bit/dim 4.1349(best: 4.2605), Xent 2.3026, Loss 4.1349, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 20.1668(19.3139) | Bit/dim 4.1474(4.1904) | Xent 2.3026(2.3026) | Loss 4.1474(4.1904) | Error 0.8956(0.9001) Steps 772(740.13) | Grad Norm 5.7245(5.4381) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 19.6198(19.3801) | Bit/dim 4.1238(4.1788) | Xent 2.3026(2.3026) | Loss 4.1238(4.1788) | Error 0.8833(0.8996) Steps 766(745.92) | Grad Norm 6.1054(5.8040) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 19.9500(19.5307) | Bit/dim 4.1218(4.1595) | Xent 2.3026(2.3026) | Loss 4.1218(4.1595) | Error 0.9089(0.8994) Steps 784(754.08) | Grad Norm 6.7995(5.3089) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 19.5618(19.5723) | Bit/dim 4.0878(4.1498) | Xent 2.3026(2.3026) | Loss 4.0878(4.1498) | Error 0.8844(0.8999) Steps 778(758.72) | Grad Norm 6.5525(5.8039) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 19.3090(19.5992) | Bit/dim 4.0939(4.1368) | Xent 2.3026(2.3026) | Loss 4.0939(4.1368) | Error 0.8989(0.8999) Steps 766(762.02) | Grad Norm 3.4999(5.8785) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 19.2194(19.6409) | Bit/dim 4.2124(4.1260) | Xent 2.3026(2.3026) | Loss 4.2124(4.1260) | Error 0.8978(0.8996) Steps 778(765.17) | Grad Norm 13.6298(6.2470) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 95.4983, Epoch Time 1198.4510(962.6903), Bit/dim 4.1137(best: 4.1349), Xent 2.3026, Loss 4.1137, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 19.8308(19.5535) | Bit/dim 4.0741(4.1273) | Xent 2.3026(2.3026) | Loss 4.0741(4.1273) | Error 0.8922(0.8987) Steps 778(761.45) | Grad Norm 5.4493(6.5797) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 20.0070(19.6043) | Bit/dim 4.0886(4.1161) | Xent 2.3026(2.3026) | Loss 4.0886(4.1161) | Error 0.9056(0.9002) Steps 736(761.24) | Grad Norm 2.5073(5.7574) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 19.5962(19.6684) | Bit/dim 4.0481(4.1019) | Xent 2.3026(2.3026) | Loss 4.0481(4.1019) | Error 0.8856(0.8997) Steps 778(765.49) | Grad Norm 3.0420(5.0266) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 20.2174(19.6878) | Bit/dim 4.0587(4.0864) | Xent 2.3026(2.3026) | Loss 4.0587(4.0864) | Error 0.9044(0.9001) Steps 778(769.05) | Grad Norm 6.0259(5.1352) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 20.0459(19.7844) | Bit/dim 4.0439(4.0711) | Xent 2.3026(2.3026) | Loss 4.0439(4.0711) | Error 0.9078(0.9003) Steps 778(771.54) | Grad Norm 6.0236(5.1999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 100.6709, Epoch Time 1202.9116(969.8969), Bit/dim 4.0324(best: 4.1137), Xent 2.3026, Loss 4.0324, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 20.2094(19.7412) | Bit/dim 4.0480(4.0603) | Xent 2.3026(2.3026) | Loss 4.0480(4.0603) | Error 0.9144(0.9015) Steps 790(774.05) | Grad Norm 2.7341(4.9680) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 19.8434(19.8076) | Bit/dim 4.0012(4.0448) | Xent 2.3026(2.3026) | Loss 4.0012(4.0448) | Error 0.9111(0.9010) Steps 778(775.89) | Grad Norm 2.5015(4.6884) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 19.6484(19.9275) | Bit/dim 4.0004(4.0383) | Xent 2.3026(2.3026) | Loss 4.0004(4.0383) | Error 0.8922(0.9010) Steps 790(778.71) | Grad Norm 4.2869(5.5381) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 20.6056(20.0027) | Bit/dim 3.9857(4.0272) | Xent 2.3026(2.3026) | Loss 3.9857(4.0272) | Error 0.8856(0.8988) Steps 790(778.84) | Grad Norm 1.5911(5.2907) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 20.7514(20.0663) | Bit/dim 4.0071(4.0204) | Xent 2.3026(2.3026) | Loss 4.0071(4.0204) | Error 0.9044(0.8994) Steps 778(780.10) | Grad Norm 2.7260(5.1896) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 19.7255(20.0610) | Bit/dim 3.9460(4.0129) | Xent 2.3026(2.3026) | Loss 3.9460(4.0129) | Error 0.8989(0.8990) Steps 778(780.47) | Grad Norm 5.0145(5.2042) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 99.8786, Epoch Time 1223.9854(977.5196), Bit/dim 3.9769(best: 4.0324), Xent 2.3026, Loss 3.9769, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 20.4220(20.1113) | Bit/dim 4.0339(4.0081) | Xent 2.3026(2.3026) | Loss 4.0339(4.0081) | Error 0.8900(0.8999) Steps 802(781.79) | Grad Norm 6.6922(5.3726) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 20.2120(20.1974) | Bit/dim 3.9558(3.9974) | Xent 2.3026(2.3026) | Loss 3.9558(3.9974) | Error 0.9122(0.9004) Steps 784(782.06) | Grad Norm 2.6794(5.1118) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 20.6866(20.2482) | Bit/dim 4.0018(3.9868) | Xent 2.3026(2.3026) | Loss 4.0018(3.9868) | Error 0.9044(0.8995) Steps 796(784.93) | Grad Norm 7.2949(5.0870) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 20.1947(20.3380) | Bit/dim 3.9382(3.9770) | Xent 2.3026(2.3026) | Loss 3.9382(3.9770) | Error 0.9033(0.8989) Steps 796(786.39) | Grad Norm 4.3537(4.8634) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 20.3554(20.3675) | Bit/dim 3.9263(3.9694) | Xent 2.3026(2.3026) | Loss 3.9263(3.9694) | Error 0.9078(0.9004) Steps 784(789.86) | Grad Norm 2.5215(4.7552) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 101.6238, Epoch Time 1242.9488(985.4824), Bit/dim 3.9422(best: 3.9769), Xent 2.3026, Loss 3.9422, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 20.5520(20.3514) | Bit/dim 3.9374(3.9617) | Xent 2.3026(2.3026) | Loss 3.9374(3.9617) | Error 0.9044(0.9009) Steps 802(791.85) | Grad Norm 4.4716(4.5691) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 20.0310(20.3248) | Bit/dim 3.9502(3.9564) | Xent 2.3026(2.3026) | Loss 3.9502(3.9564) | Error 0.9078(0.9002) Steps 796(792.22) | Grad Norm 6.8427(5.0349) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 20.5527(20.2878) | Bit/dim 3.9765(3.9524) | Xent 2.3026(2.3026) | Loss 3.9765(3.9524) | Error 0.9022(0.9006) Steps 802(792.59) | Grad Norm 5.7080(5.3187) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.3333(20.2811) | Bit/dim 3.9050(3.9449) | Xent 2.3026(2.3026) | Loss 3.9050(3.9449) | Error 0.8922(0.9000) Steps 790(792.57) | Grad Norm 2.4384(4.6069) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 20.0841(20.2731) | Bit/dim 3.9217(3.9409) | Xent 2.3026(2.3026) | Loss 3.9217(3.9409) | Error 0.9200(0.8994) Steps 796(792.21) | Grad Norm 7.4904(5.1132) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 19.7987(20.3127) | Bit/dim 3.8864(3.9381) | Xent 2.3026(2.3026) | Loss 3.8864(3.9381) | Error 0.8956(0.9001) Steps 790(792.51) | Grad Norm 2.8595(5.3948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 102.1465, Epoch Time 1235.2366(992.9751), Bit/dim 3.9235(best: 3.9422), Xent 2.3026, Loss 3.9235, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 20.9387(20.3164) | Bit/dim 3.8930(3.9293) | Xent 2.3026(2.3026) | Loss 3.8930(3.9293) | Error 0.9078(0.8998) Steps 796(793.02) | Grad Norm 1.8735(4.9196) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 20.1130(20.3159) | Bit/dim 3.9017(3.9230) | Xent 2.3026(2.3026) | Loss 3.9017(3.9230) | Error 0.8978(0.8991) Steps 796(792.18) | Grad Norm 4.9885(4.5089) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 21.3629(20.3421) | Bit/dim 3.9109(3.9170) | Xent 2.3026(2.3026) | Loss 3.9109(3.9170) | Error 0.8967(0.8991) Steps 808(792.28) | Grad Norm 6.3115(4.5686) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 19.7293(20.2930) | Bit/dim 3.8894(3.9118) | Xent 2.3026(2.3026) | Loss 3.8894(3.9118) | Error 0.9100(0.8996) Steps 784(794.84) | Grad Norm 6.2284(4.9122) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 19.6267(20.2248) | Bit/dim 3.8914(3.9062) | Xent 2.3026(2.3026) | Loss 3.8914(3.9062) | Error 0.9067(0.9012) Steps 790(794.94) | Grad Norm 3.6041(4.6460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 100.9270, Epoch Time 1232.4652(1000.1598), Bit/dim 3.8860(best: 3.9235), Xent 2.3026, Loss 3.8860, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 19.4674(20.1523) | Bit/dim 3.9112(3.8988) | Xent 2.3026(2.3026) | Loss 3.9112(3.8988) | Error 0.9111(0.9023) Steps 790(792.42) | Grad Norm 4.4893(4.7310) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 20.2857(20.0930) | Bit/dim 3.8902(3.8931) | Xent 2.3026(2.3026) | Loss 3.8902(3.8931) | Error 0.8944(0.9009) Steps 790(792.91) | Grad Norm 3.6677(4.6961) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 20.0780(20.0412) | Bit/dim 3.8716(3.8884) | Xent 2.3026(2.3026) | Loss 3.8716(3.8884) | Error 0.9033(0.9010) Steps 802(792.67) | Grad Norm 3.3024(4.7715) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 19.9931(20.0460) | Bit/dim 3.8761(3.8829) | Xent 2.3026(2.3026) | Loss 3.8761(3.8829) | Error 0.9033(0.9011) Steps 784(793.29) | Grad Norm 3.5615(4.4669) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 20.4145(20.1314) | Bit/dim 3.8444(3.8801) | Xent 2.3026(2.3026) | Loss 3.8444(3.8801) | Error 0.8978(0.8995) Steps 790(794.40) | Grad Norm 2.7668(4.8513) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 20.1816(20.1414) | Bit/dim 3.8423(3.8725) | Xent 2.3026(2.3026) | Loss 3.8423(3.8725) | Error 0.8911(0.9000) Steps 790(795.07) | Grad Norm 2.5044(4.1770) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 101.8511, Epoch Time 1222.9236(1006.8427), Bit/dim 3.8538(best: 3.8860), Xent 2.3026, Loss 3.8538, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 19.5116(20.1243) | Bit/dim 3.8637(3.8702) | Xent 2.3026(2.3026) | Loss 3.8637(3.8702) | Error 0.9056(0.9021) Steps 796(797.07) | Grad Norm 6.0359(4.6710) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 20.6125(20.1213) | Bit/dim 3.8642(3.8668) | Xent 2.3026(2.3026) | Loss 3.8642(3.8668) | Error 0.8867(0.9013) Steps 808(799.56) | Grad Norm 3.2295(4.6450) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 21.0397(20.1482) | Bit/dim 3.8452(3.8602) | Xent 2.3026(2.3026) | Loss 3.8452(3.8602) | Error 0.9033(0.9000) Steps 826(800.42) | Grad Norm 3.4161(4.2966) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 20.4301(20.1356) | Bit/dim 3.8311(3.8549) | Xent 2.3026(2.3026) | Loss 3.8311(3.8549) | Error 0.8811(0.8992) Steps 814(802.04) | Grad Norm 3.5888(4.3642) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 20.9174(20.1564) | Bit/dim 3.8090(3.8517) | Xent 2.3026(2.3026) | Loss 3.8090(3.8517) | Error 0.8800(0.8982) Steps 808(802.15) | Grad Norm 2.9960(4.8007) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 100.8567, Epoch Time 1226.3665(1013.4284), Bit/dim 3.8353(best: 3.8538), Xent 2.3026, Loss 3.8353, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 19.5422(20.0911) | Bit/dim 3.8240(3.8503) | Xent 2.3026(2.3026) | Loss 3.8240(3.8503) | Error 0.8833(0.8995) Steps 808(799.59) | Grad Norm 3.8782(4.6763) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 20.0434(20.0930) | Bit/dim 3.8166(3.8397) | Xent 2.3026(2.3026) | Loss 3.8166(3.8397) | Error 0.8989(0.8993) Steps 790(799.86) | Grad Norm 2.2335(4.6462) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 20.2825(20.0800) | Bit/dim 3.8155(3.8387) | Xent 2.3026(2.3026) | Loss 3.8155(3.8387) | Error 0.8922(0.8993) Steps 808(798.88) | Grad Norm 3.1489(5.1799) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 20.4795(20.1040) | Bit/dim 3.8114(3.8350) | Xent 2.3026(2.3026) | Loss 3.8114(3.8350) | Error 0.8878(0.8987) Steps 796(796.86) | Grad Norm 3.0548(4.8573) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 19.9119(20.0919) | Bit/dim 3.8494(3.8325) | Xent 2.3026(2.3026) | Loss 3.8494(3.8325) | Error 0.9144(0.9000) Steps 790(794.96) | Grad Norm 4.0141(4.2469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 100.2746, Epoch Time 1221.4580(1019.6693), Bit/dim 3.8150(best: 3.8353), Xent 2.3026, Loss 3.8150, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 19.9708(19.9756) | Bit/dim 3.7939(3.8269) | Xent 2.3026(2.3026) | Loss 3.7939(3.8269) | Error 0.9011(0.9000) Steps 796(794.45) | Grad Norm 4.4043(4.0322) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 20.9240(20.0017) | Bit/dim 3.8294(3.8237) | Xent 2.3026(2.3026) | Loss 3.8294(3.8237) | Error 0.9100(0.8998) Steps 814(794.97) | Grad Norm 2.5315(4.2600) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 19.5812(19.9976) | Bit/dim 3.7954(3.8196) | Xent 2.3026(2.3026) | Loss 3.7954(3.8196) | Error 0.8944(0.8989) Steps 790(794.89) | Grad Norm 5.7185(4.3036) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 20.3332(20.0361) | Bit/dim 3.8199(3.8159) | Xent 2.3026(2.3026) | Loss 3.8199(3.8159) | Error 0.9222(0.9003) Steps 796(795.25) | Grad Norm 5.0829(4.4640) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 19.8062(19.9941) | Bit/dim 3.8096(3.8104) | Xent 2.3026(2.3026) | Loss 3.8096(3.8104) | Error 0.9156(0.8995) Steps 796(795.50) | Grad Norm 3.0964(4.6360) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 99.6439, Epoch Time 1215.1102(1025.5325), Bit/dim 3.7932(best: 3.8150), Xent 2.3026, Loss 3.7932, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 20.3372(19.9643) | Bit/dim 3.8001(3.8077) | Xent 2.3026(2.3026) | Loss 3.8001(3.8077) | Error 0.9078(0.9005) Steps 796(795.64) | Grad Norm 7.9320(4.4617) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 19.8003(19.9688) | Bit/dim 3.8023(3.8071) | Xent 2.3026(2.3026) | Loss 3.8023(3.8071) | Error 0.9167(0.9014) Steps 790(795.90) | Grad Norm 5.1545(4.7723) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 20.3437(19.9968) | Bit/dim 3.8021(3.8023) | Xent 2.3026(2.3026) | Loss 3.8021(3.8023) | Error 0.8967(0.9002) Steps 808(797.11) | Grad Norm 3.9533(4.4412) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 19.9666(20.0384) | Bit/dim 3.8054(3.7968) | Xent 2.3026(2.3026) | Loss 3.8054(3.7968) | Error 0.9067(0.8997) Steps 796(797.37) | Grad Norm 4.4913(4.5851) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 20.0645(19.9764) | Bit/dim 3.7941(3.7957) | Xent 2.3026(2.3026) | Loss 3.7941(3.7957) | Error 0.9011(0.8996) Steps 796(796.56) | Grad Norm 4.5839(4.4810) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 19.9811(19.9423) | Bit/dim 3.8006(3.7943) | Xent 2.3026(2.3026) | Loss 3.8006(3.7943) | Error 0.8967(0.9000) Steps 796(796.76) | Grad Norm 3.9316(4.7407) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 96.9457, Epoch Time 1212.5436(1031.1428), Bit/dim 3.7917(best: 3.7932), Xent 2.3026, Loss 3.7917, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 20.0455(19.9132) | Bit/dim 3.7714(3.7911) | Xent 2.3026(2.3026) | Loss 3.7714(3.7911) | Error 0.8944(0.8998) Steps 796(796.19) | Grad Norm 2.3022(4.4345) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 20.5532(19.9392) | Bit/dim 3.7933(3.7890) | Xent 2.3026(2.3026) | Loss 3.7933(3.7890) | Error 0.8978(0.9002) Steps 796(795.33) | Grad Norm 4.8279(4.3108) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 20.3131(20.0355) | Bit/dim 3.8130(3.7892) | Xent 2.3026(2.3026) | Loss 3.8130(3.7892) | Error 0.9122(0.9002) Steps 772(793.97) | Grad Norm 4.0407(4.7506) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 19.7679(20.0256) | Bit/dim 3.7865(3.7888) | Xent 2.3026(2.3026) | Loss 3.7865(3.7888) | Error 0.9033(0.9005) Steps 784(792.01) | Grad Norm 4.6879(4.9575) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 20.0958(20.0219) | Bit/dim 3.7793(3.7827) | Xent 2.3026(2.3026) | Loss 3.7793(3.7827) | Error 0.9067(0.9008) Steps 790(790.86) | Grad Norm 1.4235(4.3498) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 97.7812, Epoch Time 1215.8887(1036.6852), Bit/dim 3.7721(best: 3.7917), Xent 2.3026, Loss 3.7721, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 19.7422(19.9328) | Bit/dim 3.7765(3.7776) | Xent 2.3026(2.3026) | Loss 3.7765(3.7776) | Error 0.9089(0.8995) Steps 790(790.05) | Grad Norm 6.0381(4.3476) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 19.1125(19.8322) | Bit/dim 3.7637(3.7749) | Xent 2.3026(2.3026) | Loss 3.7637(3.7749) | Error 0.9022(0.9000) Steps 772(790.78) | Grad Norm 3.6518(4.4276) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 20.4557(19.9520) | Bit/dim 3.7574(3.7718) | Xent 2.3026(2.3026) | Loss 3.7574(3.7718) | Error 0.8778(0.8995) Steps 796(791.60) | Grad Norm 4.3863(4.3957) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 19.5232(19.9435) | Bit/dim 3.7422(3.7692) | Xent 2.3026(2.3026) | Loss 3.7422(3.7692) | Error 0.9011(0.8994) Steps 802(792.49) | Grad Norm 5.9741(4.3157) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 19.9794(20.0008) | Bit/dim 3.7288(3.7664) | Xent 2.3026(2.3026) | Loss 3.7288(3.7664) | Error 0.9011(0.8998) Steps 796(793.24) | Grad Norm 2.0981(4.1220) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 19.5853(20.0663) | Bit/dim 3.7920(3.7638) | Xent 2.3026(2.3026) | Loss 3.7920(3.7638) | Error 0.8889(0.9001) Steps 790(793.68) | Grad Norm 6.2559(4.2076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 99.1773, Epoch Time 1217.6119(1042.1130), Bit/dim 3.7570(best: 3.7721), Xent 2.3026, Loss 3.7570, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 19.4115(20.0308) | Bit/dim 3.7580(3.7663) | Xent 2.3026(2.3026) | Loss 3.7580(3.7663) | Error 0.9044(0.9003) Steps 784(792.94) | Grad Norm 5.8145(4.6617) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_125_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.125 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
