{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      "\n",
      "    Args:\n",
      "        root (string): Root directory of dataset where ``processed/training.pt``\n",
      "            and  ``processed/test.pt`` exist.\n",
      "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "            otherwise from ``test.pt``.\n",
      "        download (bool, optional): If true, downloads the dataset from the internet and\n",
      "            puts it in root directory. If dataset is already downloaded, it is not\n",
      "            downloaded again.\n",
      "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index], self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index], self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, 10)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, cond_nn='mlp', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_25_drop_0_5_rl_stdscale_6_2cond_mlp_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=10, out_features=588, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=588, out_features=1176, bias=True)\n",
      "  )\n",
      "  (project_ycond_color): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=10, out_features=588, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=588, out_features=1176, bias=True)\n",
      "  )\n",
      "  (project_class): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=588, out_features=294, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=294, out_features=10, bias=True)\n",
      "  )\n",
      "  (project_color): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=588, out_features=294, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=294, out_features=10, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 2663948\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 7.4664(21.9844) | Bit/dim 25.4152(27.3626) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 48.5387(52.0582) | Error 0.8989(0.9056) | Error Color 0.9067(0.8916) |Steps 290(329.51) | Grad Norm 259.8885(276.2212) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.3963(18.1195) | Bit/dim 20.1510(26.1021) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 39.2363(49.7937) | Error 0.8800(0.8997) | Error Color 0.9111(0.8933) |Steps 326(323.58) | Grad Norm 217.5948(265.8461) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.9363(15.3308) | Bit/dim 13.8903(23.5858) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 27.9903(45.2459) | Error 0.8789(0.8969) | Error Color 0.8978(0.8931) |Steps 314(319.96) | Grad Norm 159.9506(244.2895) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.2976(13.4238) | Bit/dim 9.0146(20.2494) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 19.2451(39.2576) | Error 0.8856(0.8942) | Error Color 0.9078(0.8958) |Steps 368(323.57) | Grad Norm 95.7045(212.4260) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.2549(12.2303) | Bit/dim 6.7292(16.8939) | Xent 2.3025(2.3026) | Xent Color 2.3026(2.3026) | Loss 15.1064(33.2582) | Error 0.8978(0.8925) | Error Color 0.8722(0.8959) |Steps 404(336.35) | Grad Norm 33.6770(172.0463) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.1941(11.4707) | Bit/dim 6.1786(14.1200) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 13.9607(28.2804) | Error 0.8933(0.8915) | Error Color 0.9111(0.8977) |Steps 380(351.87) | Grad Norm 25.0975(132.8569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 55.3418, Epoch Time 638.1115(638.1115), Bit/dim 5.7457(best: inf), Xent 2.3025, Xent Color 2.3026. Loss 6.8970, Error 0.8865(best: inf), Error Color 0.9008(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.9239(10.8773) | Bit/dim 4.2979(11.7904) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 10.6920(24.5515) | Error 0.8933(0.8908) | Error Color 0.8967(0.8974) |Steps 386(359.01) | Grad Norm 16.4540(103.6881) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.7378(10.4024) | Bit/dim 4.0218(9.7817) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 10.0409(20.8257) | Error 0.8978(0.8907) | Error Color 0.9156(0.8989) |Steps 380(364.35) | Grad Norm 12.0023(79.9181) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.2044(10.0470) | Bit/dim 3.8025(8.2365) | Xent 2.3025(2.3025) | Xent Color 2.3026(2.3026) | Loss 9.8295(17.9598) | Error 0.8956(0.8907) | Error Color 0.9033(0.8999) |Steps 380(369.73) | Grad Norm 9.5453(61.9283) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.8169(9.7797) | Bit/dim 3.5035(7.0259) | Xent 2.3026(2.3025) | Xent Color 2.3026(2.3026) | Loss 9.2056(15.7152) | Error 0.8933(0.8904) | Error Color 0.9011(0.8995) |Steps 368(371.26) | Grad Norm 7.4804(47.6646) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.8509(9.5537) | Bit/dim 3.2376(6.0665) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 8.7127(13.9402) | Error 0.8656(0.8883) | Error Color 0.8878(0.8982) |Steps 380(374.49) | Grad Norm 6.7234(37.0294) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 9.2563(9.4046) | Bit/dim 2.9885(5.2920) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 8.2220(12.5087) | Error 0.8711(0.8870) | Error Color 0.9011(0.8992) |Steps 362(376.24) | Grad Norm 6.6762(29.1247) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 9.3168(9.3169) | Bit/dim 2.7636(4.6559) | Xent 2.3025(2.3025) | Xent Color 2.3026(2.3026) | Loss 7.9798(11.3388) | Error 0.8989(0.8874) | Error Color 0.8944(0.8994) |Steps 404(378.08) | Grad Norm 5.8006(23.1051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 54.9012, Epoch Time 674.4503(639.2017), Bit/dim 2.6917(best: 5.7457), Xent 2.3024, Xent Color 2.3026. Loss 3.8430, Error 0.8865(best: 0.8865), Error Color 0.9019(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.4580(9.3026) | Bit/dim 2.1781(4.0384) | Xent 2.3023(2.3025) | Xent Color 2.3026(2.3026) | Loss 6.9152(10.6476) | Error 0.8878(0.8880) | Error Color 0.9100(0.8994) |Steps 416(382.59) | Grad Norm 4.0486(18.4575) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.4339(9.3214) | Bit/dim 2.0573(3.5294) | Xent 2.3021(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.6706(9.6241) | Error 0.8767(0.8886) | Error Color 0.8956(0.9006) |Steps 392(386.72) | Grad Norm 3.9255(14.6118) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.6611(9.3751) | Bit/dim 1.9728(3.1281) | Xent 2.3022(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.4821(8.8114) | Error 0.8700(0.8876) | Error Color 0.8967(0.8993) |Steps 410(391.04) | Grad Norm 3.1446(11.7370) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.5515(9.4254) | Bit/dim 1.8704(2.8089) | Xent 2.3020(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.3945(8.1713) | Error 0.8722(0.8873) | Error Color 0.9122(0.8994) |Steps 404(393.04) | Grad Norm 3.1141(9.4780) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.4687(9.4704) | Bit/dim 1.8118(2.5545) | Xent 2.3026(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.2112(7.6617) | Error 0.8956(0.8878) | Error Color 0.9044(0.9007) |Steps 416(394.87) | Grad Norm 2.6816(7.6737) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.9474(9.5189) | Bit/dim 1.7697(2.3528) | Xent 2.3024(2.3023) | Xent Color 2.3026(2.3026) | Loss 6.0943(7.2694) | Error 0.8967(0.8879) | Error Color 0.8889(0.8997) |Steps 422(397.60) | Grad Norm 1.7875(6.2127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 57.2044, Epoch Time 709.0111(641.2960), Bit/dim 1.7242(best: 2.6917), Xent 2.3022, Xent Color 2.3026. Loss 2.8754, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.3448(9.5562) | Bit/dim 1.4653(2.1756) | Xent 2.3018(2.3023) | Xent Color 2.3026(2.3026) | Loss 5.5300(7.4425) | Error 0.8933(0.8877) | Error Color 0.8900(0.8990) |Steps 410(400.38) | Grad Norm 1.4916(5.0611) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 10.3085(9.6158) | Bit/dim 1.4383(1.9858) | Xent 2.3023(2.3023) | Xent Color 2.3026(2.3026) | Loss 5.6187(6.9474) | Error 0.8889(0.8875) | Error Color 0.8900(0.8969) |Steps 404(402.51) | Grad Norm 1.6632(4.1564) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.9782(9.6056) | Bit/dim 1.4297(1.8384) | Xent 2.3021(2.3022) | Xent Color 2.3025(2.3026) | Loss 5.5787(6.5755) | Error 0.8700(0.8856) | Error Color 0.8911(0.8973) |Steps 422(404.70) | Grad Norm 0.9177(3.3540) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.7606(9.6289) | Bit/dim 1.4017(1.7258) | Xent 2.3020(2.3021) | Xent Color 2.3024(2.3026) | Loss 5.4453(6.2826) | Error 0.8733(0.8867) | Error Color 0.8900(0.8980) |Steps 416(404.08) | Grad Norm 0.7820(2.7388) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.6651(9.6451) | Bit/dim 1.3951(1.6376) | Xent 2.3017(2.3021) | Xent Color 2.3026(2.3026) | Loss 5.3614(6.0603) | Error 0.8800(0.8858) | Error Color 0.9156(0.8986) |Steps 374(404.19) | Grad Norm 0.7280(2.2225) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.0835(9.5825) | Bit/dim 1.3796(1.5696) | Xent 2.3022(2.3021) | Xent Color 2.3026(2.3026) | Loss 5.3403(5.8859) | Error 0.8800(0.8865) | Error Color 0.9033(0.8990) |Steps 386(402.66) | Grad Norm 0.8641(1.8556) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.8348(9.5565) | Bit/dim 1.3595(1.5183) | Xent 2.3018(2.3021) | Xent Color 2.3025(2.3026) | Loss 5.3591(5.7573) | Error 0.8811(0.8882) | Error Color 0.8778(0.8986) |Steps 392(402.26) | Grad Norm 0.6865(1.5528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 56.2660, Epoch Time 712.3449(643.4274), Bit/dim 1.3604(best: 1.7242), Xent 2.3019, Xent Color 2.3026. Loss 2.5115, Error 0.8865(best: 0.8865), Error Color 0.9023(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.5585(9.5516) | Bit/dim 1.1536(1.4416) | Xent 2.3015(2.3021) | Xent Color 2.3026(2.3026) | Loss 4.9881(6.0509) | Error 0.8878(0.8883) | Error Color 0.9133(0.9007) |Steps 398(399.49) | Grad Norm 0.9216(1.3353) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.5926(9.5210) | Bit/dim 1.1378(1.3622) | Xent 2.3030(2.3020) | Xent Color 2.3026(2.3026) | Loss 4.9967(5.7747) | Error 0.8967(0.8875) | Error Color 0.9100(0.9007) |Steps 380(398.53) | Grad Norm 0.5803(1.1772) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.1785(9.4470) | Bit/dim 1.1168(1.3021) | Xent 2.3016(2.3020) | Xent Color 2.3026(2.3026) | Loss 4.9687(5.5654) | Error 0.8811(0.8879) | Error Color 0.8956(0.9006) |Steps 410(397.12) | Grad Norm 0.6381(1.0278) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.0986(9.3736) | Bit/dim 1.1147(1.2560) | Xent 2.3013(2.3020) | Xent Color 2.3024(2.3026) | Loss 4.9158(5.3962) | Error 0.8811(0.8888) | Error Color 0.8822(0.8988) |Steps 374(394.49) | Grad Norm 0.4481(0.8895) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.6265(9.3687) | Bit/dim 1.1204(1.2214) | Xent 2.3019(2.3019) | Xent Color 2.3027(2.3026) | Loss 5.0424(5.2803) | Error 0.8778(0.8873) | Error Color 0.9033(0.8999) |Steps 398(394.67) | Grad Norm 0.6544(0.8109) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.0390(9.3002) | Bit/dim 1.1145(1.1942) | Xent 2.3013(2.3017) | Xent Color 2.3027(2.3026) | Loss 4.8291(5.1777) | Error 0.8778(0.8872) | Error Color 0.9144(0.9003) |Steps 386(392.50) | Grad Norm 0.2958(0.7263) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.1528(9.3000) | Bit/dim 1.1153(1.1752) | Xent 2.3020(2.3018) | Xent Color 2.3026(2.3026) | Loss 4.9355(5.1186) | Error 0.8944(0.8881) | Error Color 0.9122(0.9006) |Steps 404(392.21) | Grad Norm 0.3132(0.6387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 55.7091, Epoch Time 692.2875(644.8932), Bit/dim 1.1076(best: 1.3604), Xent 2.3016, Xent Color 2.3025. Loss 2.2587, Error 0.8865(best: 0.8865), Error Color 0.8988(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.1177(9.2666) | Bit/dim 0.9284(1.1125) | Xent 2.3003(2.3018) | Xent Color 2.3026(2.3026) | Loss 4.5636(5.3878) | Error 0.8833(0.8895) | Error Color 0.9044(0.9017) |Steps 380(389.46) | Grad Norm 0.7115(0.6571) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 8.8557(9.2363) | Bit/dim 0.9257(1.0655) | Xent 2.3015(2.3018) | Xent Color 2.3025(2.3026) | Loss 4.5579(5.1816) | Error 0.8922(0.8898) | Error Color 0.8900(0.9006) |Steps 374(389.55) | Grad Norm 0.3866(0.6263) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 9.1496(9.2006) | Bit/dim 0.9260(1.0287) | Xent 2.3023(2.3018) | Xent Color 2.3025(2.3026) | Loss 4.6589(5.0208) | Error 0.8844(0.8883) | Error Color 0.8756(0.8995) |Steps 410(389.46) | Grad Norm 0.3644(0.5835) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.0693(9.1430) | Bit/dim 0.9286(1.0013) | Xent 2.3011(2.3017) | Xent Color 2.3027(2.3026) | Loss 4.6253(4.8976) | Error 0.8833(0.8872) | Error Color 0.8900(0.8989) |Steps 398(389.01) | Grad Norm 0.3332(0.5403) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 8.8569(9.1455) | Bit/dim 0.9204(0.9789) | Xent 2.3024(2.3016) | Xent Color 2.3024(2.3026) | Loss 4.5180(4.8062) | Error 0.8978(0.8862) | Error Color 0.9033(0.9001) |Steps 362(387.84) | Grad Norm 0.3629(0.5178) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 8.8354(9.1483) | Bit/dim 0.9300(0.9638) | Xent 2.3006(2.3015) | Xent Color 2.3025(2.3026) | Loss 4.5283(4.7446) | Error 0.8844(0.8863) | Error Color 0.9067(0.9012) |Steps 398(388.47) | Grad Norm 0.2530(0.5232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 55.1184, Epoch Time 679.9178(645.9440), Bit/dim 0.9093(best: 1.1076), Xent 2.3013, Xent Color 2.3026. Loss 2.0603, Error 0.8865(best: 0.8865), Error Color 0.8991(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.0647(9.1370) | Bit/dim 0.7940(0.9377) | Xent 2.3024(2.3016) | Xent Color 2.3024(2.3026) | Loss 4.3621(5.1433) | Error 0.9000(0.8876) | Error Color 0.8911(0.9011) |Steps 392(387.66) | Grad Norm 1.2462(0.5675) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.0322(9.1160) | Bit/dim 0.7834(0.8972) | Xent 2.3032(2.3015) | Xent Color 2.3026(2.3026) | Loss 4.3367(4.9235) | Error 0.9022(0.8870) | Error Color 0.8911(0.9009) |Steps 392(387.33) | Grad Norm 0.7067(0.6283) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.5561(9.1446) | Bit/dim 0.7822(0.8664) | Xent 2.3026(2.3014) | Xent Color 2.3026(2.3026) | Loss 4.3422(4.7666) | Error 0.8989(0.8869) | Error Color 0.9078(0.8997) |Steps 398(389.53) | Grad Norm 0.5651(0.6317) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.1617(9.1923) | Bit/dim 0.7809(0.8428) | Xent 2.3004(2.3014) | Xent Color 2.3028(2.3026) | Loss 4.4267(4.6551) | Error 0.8656(0.8867) | Error Color 0.9144(0.9002) |Steps 416(391.25) | Grad Norm 1.4357(0.6441) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.0800(9.1644) | Bit/dim 0.7723(0.8249) | Xent 2.2993(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.3112(4.5717) | Error 0.8722(0.8859) | Error Color 0.9022(0.9013) |Steps 386(392.49) | Grad Norm 1.8846(0.7665) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.3157(9.2116) | Bit/dim 0.7754(0.8111) | Xent 2.3006(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.4152(4.5147) | Error 0.8800(0.8868) | Error Color 0.8811(0.9001) |Steps 398(395.55) | Grad Norm 1.9459(1.1095) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 8.9453(9.2029) | Bit/dim 0.7686(0.8006) | Xent 2.2993(2.3013) | Xent Color 2.3024(2.3025) | Loss 4.2826(4.4580) | Error 0.8778(0.8877) | Error Color 0.9144(0.8998) |Steps 392(395.99) | Grad Norm 2.4076(1.3435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 56.3565, Epoch Time 686.7071(647.1669), Bit/dim 0.7644(best: 0.9093), Xent 2.3011, Xent Color 2.3027. Loss 1.9153, Error 0.8865(best: 0.8865), Error Color 0.9026(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.4610(9.2061) | Bit/dim 0.6950(0.7759) | Xent 2.3013(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.2120(4.8344) | Error 0.8900(0.8858) | Error Color 0.9067(0.9013) |Steps 404(397.90) | Grad Norm 6.3127(2.0748) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.3425(9.2294) | Bit/dim 0.6872(0.7519) | Xent 2.3008(2.3012) | Xent Color 2.3031(2.3026) | Loss 4.1417(4.6499) | Error 0.8967(0.8872) | Error Color 0.9100(0.9015) |Steps 386(395.40) | Grad Norm 3.5162(2.3566) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.6327(9.2805) | Bit/dim 0.6813(0.7349) | Xent 2.3030(2.3012) | Xent Color 2.3019(2.3026) | Loss 4.1807(4.5284) | Error 0.9033(0.8875) | Error Color 0.8867(0.9018) |Steps 398(396.62) | Grad Norm 3.0224(3.4313) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.2287(9.3020) | Bit/dim 0.6808(0.7205) | Xent 2.3009(2.3013) | Xent Color 2.3029(2.3026) | Loss 4.0857(4.4219) | Error 0.8778(0.8876) | Error Color 0.9089(0.9025) |Steps 398(396.17) | Grad Norm 4.5465(3.8189) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.0470(9.2623) | Bit/dim 0.6796(0.7105) | Xent 2.3030(2.3014) | Xent Color 2.3025(2.3026) | Loss 4.0907(4.3467) | Error 0.8989(0.8878) | Error Color 0.8956(0.9028) |Steps 380(394.59) | Grad Norm 8.2513(4.6746) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.1406(9.2408) | Bit/dim 0.6933(0.7027) | Xent 2.3005(2.3014) | Xent Color 2.3028(2.3026) | Loss 4.2424(4.2998) | Error 0.8789(0.8881) | Error Color 0.9000(0.9032) |Steps 416(396.17) | Grad Norm 9.3972(4.8900) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 55.8689, Epoch Time 691.2764(648.4901), Bit/dim 0.6720(best: 0.7644), Xent 2.3011, Xent Color 2.3026. Loss 1.8229, Error 0.8865(best: 0.8865), Error Color 0.9029(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.5824(9.2585) | Bit/dim 0.6428(0.6946) | Xent 2.3008(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.1012(4.7805) | Error 0.8867(0.8880) | Error Color 0.9133(0.9024) |Steps 398(395.78) | Grad Norm 7.9997(5.1272) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.4259(9.2512) | Bit/dim 0.6300(0.6790) | Xent 2.3003(2.3010) | Xent Color 2.3026(2.3026) | Loss 4.1228(4.5915) | Error 0.8856(0.8865) | Error Color 0.8944(0.9020) |Steps 404(395.86) | Grad Norm 1.5245(5.1522) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.5888(9.2740) | Bit/dim 0.6377(0.6698) | Xent 2.3021(2.3013) | Xent Color 2.3027(2.3026) | Loss 4.0933(4.4592) | Error 0.8889(0.8885) | Error Color 0.9078(0.9005) |Steps 374(394.90) | Grad Norm 6.3513(7.3548) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.1825(9.2709) | Bit/dim 0.6354(0.6607) | Xent 2.3033(2.3013) | Xent Color 2.3026(2.3026) | Loss 4.1040(4.3587) | Error 0.8989(0.8882) | Error Color 0.9011(0.9000) |Steps 386(393.20) | Grad Norm 6.7881(7.5070) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.1775(9.2583) | Bit/dim 0.6281(0.6532) | Xent 2.3008(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.9919(4.2765) | Error 0.8878(0.8885) | Error Color 0.9167(0.8992) |Steps 368(391.74) | Grad Norm 2.1428(6.8908) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.0670(9.2620) | Bit/dim 0.6272(0.6466) | Xent 2.3023(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.9842(4.2147) | Error 0.8844(0.8868) | Error Color 0.9011(0.9007) |Steps 368(393.23) | Grad Norm 5.5000(6.4588) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.4344(9.2735) | Bit/dim 0.6228(0.6424) | Xent 2.3029(2.3012) | Xent Color 2.3025(2.3026) | Loss 4.0951(4.1709) | Error 0.9022(0.8876) | Error Color 0.9056(0.9014) |Steps 380(392.66) | Grad Norm 2.5929(5.8600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 55.7351, Epoch Time 691.8925(649.7922), Bit/dim 0.6235(best: 0.6720), Xent 2.3010, Xent Color 2.3025. Loss 1.7744, Error 0.8865(best: 0.8865), Error Color 0.8995(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.0429(9.3058) | Bit/dim 0.6568(0.6370) | Xent 2.3020(2.3011) | Xent Color 2.3025(2.3026) | Loss 4.1783(4.5821) | Error 0.8944(0.8872) | Error Color 0.8900(0.8997) |Steps 404(393.14) | Grad Norm 39.0447(6.7318) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.4641(9.3707) | Bit/dim 0.6549(0.6531) | Xent 2.3039(2.3013) | Xent Color 2.3027(2.3026) | Loss 4.0636(4.4812) | Error 0.9044(0.8881) | Error Color 0.8978(0.9006) |Steps 404(393.31) | Grad Norm 13.2659(13.4023) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 8.9625(9.4075) | Bit/dim 0.6285(0.6479) | Xent 2.2984(2.3010) | Xent Color 2.3028(2.3026) | Loss 4.0724(4.3807) | Error 0.8744(0.8867) | Error Color 0.9167(0.9011) |Steps 398(395.24) | Grad Norm 8.5051(12.4316) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.7929(9.4590) | Bit/dim 0.6157(0.6414) | Xent 2.3004(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.0551(4.3036) | Error 0.8789(0.8872) | Error Color 0.9167(0.9001) |Steps 428(398.86) | Grad Norm 2.5824(10.5035) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.0354(9.4891) | Bit/dim 0.6134(0.6350) | Xent 2.3001(2.3012) | Xent Color 2.3026(2.3026) | Loss 4.0271(4.2400) | Error 0.8911(0.8883) | Error Color 0.8956(0.9001) |Steps 380(398.00) | Grad Norm 1.7079(8.5438) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.7486(9.5558) | Bit/dim 0.6108(0.6289) | Xent 2.3015(2.3012) | Xent Color 2.3025(2.3026) | Loss 4.0416(4.1964) | Error 0.8889(0.8882) | Error Color 0.9022(0.8992) |Steps 410(399.00) | Grad Norm 1.6731(6.7824) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.6784(9.5026) | Bit/dim 0.6073(0.6237) | Xent 2.3001(2.3013) | Xent Color 2.3027(2.3026) | Loss 4.0209(4.1568) | Error 0.8789(0.8881) | Error Color 0.9000(0.8996) |Steps 398(396.07) | Grad Norm 0.6654(5.3037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 57.1341, Epoch Time 709.7994(651.5924), Bit/dim 0.6057(best: 0.6235), Xent 2.3010, Xent Color 2.3026. Loss 1.7566, Error 0.8865(best: 0.8865), Error Color 0.9038(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.0304(9.4333) | Bit/dim 0.5932(0.6164) | Xent 2.3029(2.3014) | Xent Color 2.3025(2.3026) | Loss 4.0063(4.5481) | Error 0.8933(0.8892) | Error Color 0.8956(0.9003) |Steps 380(395.39) | Grad Norm 0.6589(4.1095) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.0534(9.3855) | Bit/dim 0.5935(0.6107) | Xent 2.2990(2.3013) | Xent Color 2.3024(2.3026) | Loss 4.0580(4.4097) | Error 0.8889(0.8885) | Error Color 0.8844(0.9008) |Steps 392(392.99) | Grad Norm 0.9677(3.2168) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.8379(9.3864) | Bit/dim 0.5925(0.6060) | Xent 2.3003(2.3012) | Xent Color 2.3027(2.3026) | Loss 4.0650(4.3099) | Error 0.8889(0.8882) | Error Color 0.9000(0.9009) |Steps 416(393.98) | Grad Norm 0.7365(2.5297) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.1718(9.3576) | Bit/dim 0.5967(0.6024) | Xent 2.3012(2.3014) | Xent Color 2.3025(2.3026) | Loss 4.0017(4.2365) | Error 0.8989(0.8884) | Error Color 0.9089(0.8998) |Steps 374(395.22) | Grad Norm 4.2966(2.3243) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.1732(9.3245) | Bit/dim 0.5938(0.5998) | Xent 2.2993(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.0399(4.1813) | Error 0.8889(0.8881) | Error Color 0.8944(0.8996) |Steps 404(394.84) | Grad Norm 5.7956(3.2313) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 8.7506(9.3029) | Bit/dim 0.5885(0.5970) | Xent 2.3019(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.8962(4.1399) | Error 0.8878(0.8877) | Error Color 0.8978(0.8985) |Steps 398(396.84) | Grad Norm 3.5151(3.1606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 56.2225, Epoch Time 690.9391(652.7728), Bit/dim 0.5877(best: 0.6057), Xent 2.3010, Xent Color 2.3027. Loss 1.7387, Error 0.8865(best: 0.8865), Error Color 0.9014(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 8.8574(9.2711) | Bit/dim 0.5855(0.5945) | Xent 2.3018(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.9618(4.6110) | Error 0.8989(0.8879) | Error Color 0.8922(0.8987) |Steps 380(397.37) | Grad Norm 7.7045(3.4350) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.1453(9.2885) | Bit/dim 0.5827(0.5917) | Xent 2.3022(2.3012) | Xent Color 2.3022(2.3026) | Loss 3.9173(4.4445) | Error 0.8978(0.8877) | Error Color 0.8922(0.9006) |Steps 374(395.40) | Grad Norm 4.0836(3.9269) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.5087(9.3295) | Bit/dim 0.5812(0.5895) | Xent 2.3018(2.3010) | Xent Color 2.3027(2.3026) | Loss 4.0605(4.3339) | Error 0.8856(0.8861) | Error Color 0.9233(0.9010) |Steps 422(395.23) | Grad Norm 5.0021(4.3199) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.2672(9.3353) | Bit/dim 0.5799(0.5873) | Xent 2.3014(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.9821(4.2419) | Error 0.8800(0.8869) | Error Color 0.8867(0.8996) |Steps 368(392.98) | Grad Norm 3.3710(4.0857) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.4059(9.3523) | Bit/dim 0.5824(0.5856) | Xent 2.2987(2.3010) | Xent Color 2.3027(2.3026) | Loss 4.1187(4.1865) | Error 0.8778(0.8866) | Error Color 0.8922(0.8996) |Steps 410(394.78) | Grad Norm 5.4699(4.4493) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.0873(9.4071) | Bit/dim 0.5779(0.5837) | Xent 2.3019(2.3011) | Xent Color 2.3029(2.3026) | Loss 3.9954(4.1474) | Error 0.8911(0.8872) | Error Color 0.9233(0.9013) |Steps 374(396.94) | Grad Norm 6.2777(4.4860) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.2832(9.3817) | Bit/dim 0.5737(0.5814) | Xent 2.3015(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.9038(4.1022) | Error 0.8933(0.8872) | Error Color 0.9078(0.9018) |Steps 398(396.32) | Grad Norm 2.4493(3.9699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 56.2035, Epoch Time 699.1880(654.1653), Bit/dim 0.5792(best: 0.5877), Xent 2.3010, Xent Color 2.3026. Loss 1.7301, Error 0.8865(best: 0.8865), Error Color 0.9000(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.5266(9.3397) | Bit/dim 0.6839(0.5857) | Xent 2.3017(2.3012) | Xent Color 2.3023(2.3026) | Loss 4.2100(4.5347) | Error 0.8844(0.8875) | Error Color 0.9022(0.9002) |Steps 410(396.45) | Grad Norm 25.4196(6.8071) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.5759(9.3782) | Bit/dim 0.5944(0.5920) | Xent 2.2983(2.3011) | Xent Color 2.3028(2.3026) | Loss 4.0274(4.4062) | Error 0.8844(0.8877) | Error Color 0.9122(0.9000) |Steps 386(394.69) | Grad Norm 8.2444(8.1603) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.4605(9.3878) | Bit/dim 0.5741(0.5891) | Xent 2.3017(2.3011) | Xent Color 2.3022(2.3026) | Loss 4.0340(4.3088) | Error 0.8856(0.8875) | Error Color 0.8811(0.8997) |Steps 410(394.59) | Grad Norm 3.6260(7.5655) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.4720(9.3244) | Bit/dim 0.5658(0.5841) | Xent 2.3015(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.9946(4.2272) | Error 0.8922(0.8871) | Error Color 0.8978(0.8996) |Steps 380(392.66) | Grad Norm 3.3704(6.7621) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.1305(9.3505) | Bit/dim 0.5579(0.5782) | Xent 2.3014(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.9892(4.1667) | Error 0.8733(0.8873) | Error Color 0.9033(0.8991) |Steps 386(394.72) | Grad Norm 6.4138(6.3952) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 8.8146(9.3501) | Bit/dim 0.5517(0.5795) | Xent 2.3007(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.8800(4.1232) | Error 0.8811(0.8869) | Error Color 0.8933(0.8995) |Steps 344(392.08) | Grad Norm 4.8242(9.5604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 57.7634, Epoch Time 696.7664(655.4433), Bit/dim 0.5565(best: 0.5792), Xent 2.3010, Xent Color 2.3026. Loss 1.7074, Error 0.8865(best: 0.8865), Error Color 0.9018(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 8.8548(9.3153) | Bit/dim 0.5540(0.5734) | Xent 2.2971(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.8801(4.6323) | Error 0.8778(0.8882) | Error Color 0.9144(0.9008) |Steps 362(388.81) | Grad Norm 13.1785(9.9480) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.4375(9.3396) | Bit/dim 0.5296(0.5636) | Xent 2.2990(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.9597(4.4442) | Error 0.8756(0.8874) | Error Color 0.8856(0.8990) |Steps 392(391.28) | Grad Norm 10.3614(9.4501) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.6079(9.2820) | Bit/dim 0.5017(0.5500) | Xent 2.2987(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.9538(4.2922) | Error 0.8789(0.8867) | Error Color 0.9044(0.8985) |Steps 386(389.54) | Grad Norm 7.8347(8.9504) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 8.8716(9.2739) | Bit/dim 0.4805(0.5345) | Xent 2.2999(2.3009) | Xent Color 2.3022(2.3026) | Loss 3.7736(4.1697) | Error 0.8956(0.8869) | Error Color 0.8811(0.8982) |Steps 410(387.73) | Grad Norm 9.6019(9.0681) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.3625(9.2207) | Bit/dim 0.4675(0.5187) | Xent 2.2986(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.7620(4.0727) | Error 0.8589(0.8871) | Error Color 0.8922(0.8974) |Steps 350(386.79) | Grad Norm 10.7810(9.6085) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.3526(9.2194) | Bit/dim 0.4603(0.5024) | Xent 2.3018(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.8313(3.9970) | Error 0.8967(0.8879) | Error Color 0.8967(0.8982) |Steps 398(389.38) | Grad Norm 14.9238(9.3205) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.6612(9.2647) | Bit/dim 0.6424(0.5045) | Xent 2.3018(2.3014) | Xent Color 2.3024(2.3026) | Loss 4.0698(3.9657) | Error 0.8911(0.8878) | Error Color 0.8911(0.8990) |Steps 356(391.10) | Grad Norm 36.0377(12.4349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 57.2650, Epoch Time 689.0993(656.4530), Bit/dim 0.6290(best: 0.5565), Xent 2.3010, Xent Color 2.3027. Loss 1.7799, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.5247(9.2946) | Bit/dim 0.5224(0.5274) | Xent 2.3029(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.8672(4.4580) | Error 0.8956(0.8879) | Error Color 0.9033(0.8988) |Steps 428(394.44) | Grad Norm 6.4510(15.6805) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.0325(9.4053) | Bit/dim 0.4807(0.5204) | Xent 2.2969(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.8284(4.3106) | Error 0.8622(0.8879) | Error Color 0.8911(0.8986) |Steps 362(399.67) | Grad Norm 4.1079(13.4037) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.7788(9.5104) | Bit/dim 0.4625(0.5075) | Xent 2.2997(2.3014) | Xent Color 2.3023(2.3026) | Loss 3.9022(4.2065) | Error 0.8844(0.8900) | Error Color 0.8733(0.8996) |Steps 404(403.66) | Grad Norm 3.4697(10.8348) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 8.9481(9.5733) | Bit/dim 0.4444(0.4927) | Xent 2.3005(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.7081(4.1119) | Error 0.8856(0.8888) | Error Color 0.8956(0.9000) |Steps 392(404.85) | Grad Norm 2.2925(8.7009) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.0139(9.5062) | Bit/dim 0.4352(0.4784) | Xent 2.3017(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.8071(4.0344) | Error 0.8844(0.8878) | Error Color 0.8778(0.8991) |Steps 404(404.24) | Grad Norm 1.6860(6.8649) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.7706(9.5129) | Bit/dim 0.4238(0.4651) | Xent 2.3030(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.7136(3.9653) | Error 0.8778(0.8863) | Error Color 0.8967(0.8979) |Steps 368(404.43) | Grad Norm 1.0613(5.3556) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.0328(9.4493) | Bit/dim 0.4155(0.4532) | Xent 2.3014(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.7230(3.9119) | Error 0.8856(0.8869) | Error Color 0.8989(0.8994) |Steps 398(402.73) | Grad Norm 1.8752(4.5106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 58.2631, Epoch Time 712.4932(658.1342), Bit/dim 0.4143(best: 0.5565), Xent 2.3010, Xent Color 2.3026. Loss 1.5652, Error 0.8865(best: 0.8865), Error Color 0.8998(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.1217(9.4078) | Bit/dim 0.4483(0.4465) | Xent 2.3016(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.7381(4.3223) | Error 0.8922(0.8883) | Error Color 0.8900(0.8988) |Steps 356(399.83) | Grad Norm 17.7540(5.9243) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.4389(9.3845) | Bit/dim 0.5009(0.4606) | Xent 2.2985(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.8791(4.2066) | Error 0.8756(0.8883) | Error Color 0.9067(0.8998) |Steps 380(399.67) | Grad Norm 22.0997(10.1349) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.1465(9.3574) | Bit/dim 0.4221(0.4578) | Xent 2.3007(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.7194(4.0841) | Error 0.8844(0.8881) | Error Color 0.9033(0.8997) |Steps 398(396.93) | Grad Norm 3.1851(10.1790) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.3642(9.2994) | Bit/dim 0.4029(0.4468) | Xent 2.3005(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.7702(3.9877) | Error 0.8800(0.8872) | Error Color 0.8967(0.8995) |Steps 374(390.42) | Grad Norm 2.4204(9.0892) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 8.8034(9.2928) | Bit/dim 0.3765(0.4313) | Xent 2.3004(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.6340(3.9217) | Error 0.8756(0.8864) | Error Color 0.8933(0.9005) |Steps 422(394.64) | Grad Norm 2.0023(7.5840) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.2838(9.2664) | Bit/dim 0.3509(0.4130) | Xent 2.3041(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.7049(3.8507) | Error 0.9078(0.8875) | Error Color 0.8944(0.8995) |Steps 410(392.12) | Grad Norm 0.9356(6.1846) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 59.7506, Epoch Time 694.4393(659.2234), Bit/dim 0.3332(best: 0.4143), Xent 2.3010, Xent Color 2.3026. Loss 1.4841, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.6010(9.3325) | Bit/dim 0.3273(0.3931) | Xent 2.3035(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.6472(4.3199) | Error 0.8989(0.8881) | Error Color 0.9033(0.8990) |Steps 428(396.41) | Grad Norm 3.3283(5.1050) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.0373(9.3511) | Bit/dim 0.3221(0.3793) | Xent 2.3028(2.3009) | Xent Color 2.3030(2.3026) | Loss 3.5462(4.1398) | Error 0.8844(0.8858) | Error Color 0.9122(0.8996) |Steps 386(398.67) | Grad Norm 7.8478(7.2020) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.3519(9.3668) | Bit/dim 0.3666(0.3743) | Xent 2.3003(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.5969(4.0060) | Error 0.8733(0.8864) | Error Color 0.9022(0.8997) |Steps 410(400.13) | Grad Norm 17.9771(9.7418) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.5033(9.3447) | Bit/dim 0.3126(0.3613) | Xent 2.3005(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.5426(3.8848) | Error 0.8722(0.8869) | Error Color 0.9056(0.8999) |Steps 398(399.46) | Grad Norm 8.3853(9.4817) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.1414(9.2868) | Bit/dim 0.2935(0.3457) | Xent 2.3031(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.4656(3.7807) | Error 0.8911(0.8867) | Error Color 0.9011(0.9005) |Steps 386(395.50) | Grad Norm 3.5214(8.2773) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.2550(9.2336) | Bit/dim 0.2781(0.3297) | Xent 2.3003(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.5805(3.7057) | Error 0.8833(0.8872) | Error Color 0.8956(0.9001) |Steps 422(394.74) | Grad Norm 2.8217(7.0135) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.5327(9.2141) | Bit/dim 0.2782(0.3146) | Xent 2.3009(2.3013) | Xent Color 2.3023(2.3026) | Loss 3.5511(3.6471) | Error 0.8833(0.8881) | Error Color 0.8944(0.8998) |Steps 428(395.73) | Grad Norm 12.9345(6.4815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 53.7523, Epoch Time 687.9502(660.0852), Bit/dim 0.3403(best: 0.3332), Xent 2.3010, Xent Color 2.3027. Loss 1.4912, Error 0.8865(best: 0.8865), Error Color 0.9032(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 8.7594(9.2245) | Bit/dim 0.2928(0.3256) | Xent 2.3006(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.3036(4.0168) | Error 0.8922(0.8878) | Error Color 0.8956(0.8994) |Steps 356(392.81) | Grad Norm 10.0949(10.3694) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 8.5796(9.1428) | Bit/dim 0.2803(0.3167) | Xent 2.3000(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.3339(3.8618) | Error 0.8711(0.8872) | Error Color 0.8800(0.8983) |Steps 344(389.67) | Grad Norm 5.9808(9.6177) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 9.1328(9.2086) | Bit/dim 0.2459(0.3013) | Xent 2.3002(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.3286(3.7495) | Error 0.8767(0.8881) | Error Color 0.8756(0.8972) |Steps 392(392.52) | Grad Norm 2.5461(8.2285) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.0991(9.2168) | Bit/dim 0.2244(0.2838) | Xent 2.3013(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.4079(3.6535) | Error 0.8911(0.8876) | Error Color 0.9100(0.8983) |Steps 404(392.61) | Grad Norm 2.9204(6.9717) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 9.7093(9.2353) | Bit/dim 0.2056(0.2653) | Xent 2.2988(2.3011) | Xent Color 2.3023(2.3025) | Loss 3.3291(3.5742) | Error 0.8667(0.8876) | Error Color 0.8978(0.8968) |Steps 380(392.64) | Grad Norm 2.3397(5.8566) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 8.9193(9.2095) | Bit/dim 0.2141(0.2506) | Xent 2.2989(2.3011) | Xent Color 2.3023(2.3025) | Loss 3.2469(3.5040) | Error 0.8878(0.8875) | Error Color 0.9078(0.8977) |Steps 350(390.65) | Grad Norm 12.5863(6.8787) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 54.5752, Epoch Time 685.3012(660.8416), Bit/dim 0.2449(best: 0.3332), Xent 2.3010, Xent Color 2.3026. Loss 1.3959, Error 0.8865(best: 0.8865), Error Color 0.8980(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.5496(9.2143) | Bit/dim 0.2923(0.2537) | Xent 2.3003(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.6024(3.9443) | Error 0.8756(0.8878) | Error Color 0.8967(0.8982) |Steps 416(389.51) | Grad Norm 25.2249(10.6626) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 9.5173(9.2149) | Bit/dim 0.2146(0.2468) | Xent 2.3029(2.3014) | Xent Color 2.3020(2.3025) | Loss 3.2624(3.7727) | Error 0.9000(0.8889) | Error Color 0.9022(0.8969) |Steps 386(388.56) | Grad Norm 6.8614(9.7331) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.2359(9.1314) | Bit/dim 0.1883(0.2339) | Xent 2.3019(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.2878(3.6240) | Error 0.9000(0.8888) | Error Color 0.8911(0.8981) |Steps 404(385.72) | Grad Norm 4.6681(8.6094) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 8.9371(9.1503) | Bit/dim 0.1726(0.2199) | Xent 2.2983(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2282(3.5354) | Error 0.8744(0.8884) | Error Color 0.8911(0.8984) |Steps 392(387.20) | Grad Norm 2.5138(7.4319) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 8.7184(9.1102) | Bit/dim 0.1909(0.2097) | Xent 2.2993(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.2908(3.4590) | Error 0.8733(0.8870) | Error Color 0.9000(0.8979) |Steps 386(385.78) | Grad Norm 12.5124(7.9032) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 9.4774(9.1274) | Bit/dim 0.1682(0.1989) | Xent 2.3017(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.2597(3.4037) | Error 0.8867(0.8870) | Error Color 0.8822(0.8975) |Steps 434(387.33) | Grad Norm 7.4782(7.4332) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.2486(9.1328) | Bit/dim 0.1881(0.2021) | Xent 2.3030(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.2554(3.3823) | Error 0.9000(0.8883) | Error Color 0.8944(0.8989) |Steps 398(388.66) | Grad Norm 9.5740(9.9148) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 56.7984, Epoch Time 681.3299(661.4563), Bit/dim 0.2607(best: 0.2449), Xent 2.3010, Xent Color 2.3026. Loss 1.4116, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.0516(9.1408) | Bit/dim 0.2051(0.2061) | Xent 2.3036(2.3014) | Xent Color 2.3024(2.3026) | Loss 3.3032(3.8055) | Error 0.8956(0.8881) | Error Color 0.8978(0.8999) |Steps 422(390.74) | Grad Norm 9.8174(10.4458) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 8.4995(9.1238) | Bit/dim 0.1690(0.1999) | Xent 2.3011(2.3012) | Xent Color 2.3030(2.3027) | Loss 3.1849(3.6631) | Error 0.8800(0.8876) | Error Color 0.9033(0.9008) |Steps 404(390.61) | Grad Norm 3.3521(9.2433) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 9.6278(9.1186) | Bit/dim 0.1641(0.1916) | Xent 2.2988(2.3014) | Xent Color 2.3028(2.3027) | Loss 3.2730(3.5701) | Error 0.8656(0.8876) | Error Color 0.9144(0.9014) |Steps 410(392.45) | Grad Norm 7.4337(8.1493) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.4090(9.1238) | Bit/dim 0.1595(0.1832) | Xent 2.3012(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.2163(3.4891) | Error 0.8922(0.8869) | Error Color 0.8900(0.9005) |Steps 380(393.81) | Grad Norm 3.3583(7.3120) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 8.8929(9.1044) | Bit/dim 0.1490(0.1748) | Xent 2.3017(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1076(3.4156) | Error 0.8889(0.8873) | Error Color 0.9200(0.9019) |Steps 368(391.50) | Grad Norm 3.6704(6.3351) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.1516(9.0610) | Bit/dim 0.1934(0.1767) | Xent 2.3019(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.3503(3.3795) | Error 0.8878(0.8861) | Error Color 0.8956(0.9010) |Steps 398(390.59) | Grad Norm 11.7373(8.3920) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.1991(9.0937) | Bit/dim 0.1669(0.1791) | Xent 2.3032(2.3012) | Xent Color 2.3029(2.3026) | Loss 3.3203(3.3586) | Error 0.8989(0.8880) | Error Color 0.8978(0.9014) |Steps 368(388.63) | Grad Norm 9.3422(10.3545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 60.4997, Epoch Time 683.8077(662.1268), Bit/dim 0.1642(best: 0.2449), Xent 2.3010, Xent Color 2.3027. Loss 1.3151, Error 0.8865(best: 0.8865), Error Color 0.9024(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.0467(9.0685) | Bit/dim 0.1565(0.1737) | Xent 2.3023(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.2124(3.7593) | Error 0.9022(0.8893) | Error Color 0.8911(0.9009) |Steps 368(387.49) | Grad Norm 6.1857(9.4713) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 8.7257(9.0238) | Bit/dim 0.1461(0.1670) | Xent 2.3016(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.1400(3.6095) | Error 0.8867(0.8884) | Error Color 0.9067(0.9011) |Steps 368(387.95) | Grad Norm 3.9755(7.9635) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 9.0838(9.0163) | Bit/dim 0.1422(0.1609) | Xent 2.3019(2.3011) | Xent Color 2.3028(2.3025) | Loss 3.2110(3.5071) | Error 0.8844(0.8868) | Error Color 0.8978(0.9003) |Steps 398(391.24) | Grad Norm 1.6370(6.4564) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 9.2419(9.0472) | Bit/dim 0.1359(0.1554) | Xent 2.3017(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.1387(3.4206) | Error 0.8944(0.8881) | Error Color 0.8789(0.9012) |Steps 368(387.90) | Grad Norm 2.2801(5.3021) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 9.0606(9.0919) | Bit/dim 0.1355(0.1519) | Xent 2.3004(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.1855(3.3623) | Error 0.8878(0.8883) | Error Color 0.9011(0.9021) |Steps 380(390.07) | Grad Norm 5.1538(6.1947) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.3120(9.1013) | Bit/dim 0.1598(0.1509) | Xent 2.2998(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1534(3.3148) | Error 0.8744(0.8875) | Error Color 0.8900(0.9006) |Steps 392(389.63) | Grad Norm 17.4922(7.7385) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 57.2994, Epoch Time 679.4787(662.6474), Bit/dim 0.4438(best: 0.1642), Xent 2.3010, Xent Color 2.3027. Loss 1.5947, Error 0.8865(best: 0.8865), Error Color 0.9005(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 8.9505(9.1395) | Bit/dim 0.4255(0.1928) | Xent 2.3017(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.7803(3.8427) | Error 0.8889(0.8873) | Error Color 0.8978(0.9010) |Steps 416(391.88) | Grad Norm 11.4060(12.0736) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.2816(9.2069) | Bit/dim 0.2608(0.2212) | Xent 2.3005(2.3013) | Xent Color 2.3023(2.3026) | Loss 3.5171(3.7679) | Error 0.8889(0.8898) | Error Color 0.8889(0.9016) |Steps 428(396.70) | Grad Norm 8.5713(11.6840) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 8.9518(9.2315) | Bit/dim 0.1883(0.2181) | Xent 2.3011(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.3226(3.6658) | Error 0.8956(0.8897) | Error Color 0.8967(0.9011) |Steps 404(398.74) | Grad Norm 7.8028(10.4944) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.0829(9.2496) | Bit/dim 0.1571(0.2044) | Xent 2.2991(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.3241(3.5816) | Error 0.8622(0.8896) | Error Color 0.8956(0.8998) |Steps 416(401.15) | Grad Norm 3.5562(9.0769) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 9.3376(9.2670) | Bit/dim 0.1448(0.1900) | Xent 2.3005(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.3868(3.5186) | Error 0.8800(0.8882) | Error Color 0.9167(0.9004) |Steps 410(402.28) | Grad Norm 2.5124(7.4572) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 10.0219(9.3357) | Bit/dim 0.1345(0.1765) | Xent 2.2981(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.3330(3.4633) | Error 0.8744(0.8865) | Error Color 0.8989(0.9007) |Steps 446(405.49) | Grad Norm 1.7437(6.0476) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.0236(9.3146) | Bit/dim 0.1356(0.1654) | Xent 2.3039(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.2646(3.4151) | Error 0.8944(0.8865) | Error Color 0.8989(0.9003) |Steps 392(404.77) | Grad Norm 1.7978(4.9339) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 61.3946, Epoch Time 701.1158(663.8014), Bit/dim 0.1315(best: 0.1642), Xent 2.3010, Xent Color 2.3026. Loss 1.2825, Error 0.8865(best: 0.8865), Error Color 0.8985(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 9.4101(9.3651) | Bit/dim 0.1245(0.1559) | Xent 2.3026(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.2162(3.8634) | Error 0.8889(0.8861) | Error Color 0.9022(0.9006) |Steps 416(403.95) | Grad Norm 1.2102(4.0414) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 9.5817(9.4234) | Bit/dim 0.2783(0.1573) | Xent 2.2998(2.3010) | Xent Color 2.3027(2.3026) | Loss 3.3141(3.7193) | Error 0.8800(0.8860) | Error Color 0.9044(0.9007) |Steps 416(407.58) | Grad Norm 42.8615(6.0834) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.7009(9.4879) | Bit/dim 0.3101(0.2061) | Xent 2.2965(2.3008) | Xent Color 2.3027(2.3026) | Loss 3.5915(3.6989) | Error 0.8667(0.8848) | Error Color 0.9033(0.9006) |Steps 422(412.90) | Grad Norm 10.4868(8.0236) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 8.8762(9.4136) | Bit/dim 0.2394(0.2208) | Xent 2.3058(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.3061(3.6272) | Error 0.8956(0.8858) | Error Color 0.8944(0.9012) |Steps 392(407.84) | Grad Norm 6.2717(7.8799) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 9.4147(9.3274) | Bit/dim 0.2101(0.2210) | Xent 2.3017(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.3303(3.5521) | Error 0.8900(0.8876) | Error Color 0.8922(0.9014) |Steps 404(406.29) | Grad Norm 5.8007(7.1198) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 8.8587(9.2933) | Bit/dim 0.1872(0.2143) | Xent 2.3009(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2649(3.4808) | Error 0.8856(0.8883) | Error Color 0.9067(0.9013) |Steps 362(399.10) | Grad Norm 5.4745(6.2452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 58.7929, Epoch Time 700.1820(664.8929), Bit/dim 0.1661(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.3170, Error 0.8865(best: 0.8865), Error Color 0.9066(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 8.4982(9.2778) | Bit/dim 0.1636(0.2032) | Xent 2.3002(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1886(3.9496) | Error 0.8789(0.8874) | Error Color 0.8989(0.9021) |Steps 392(399.77) | Grad Norm 2.6237(5.3483) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 8.9215(9.2408) | Bit/dim 0.1501(0.1909) | Xent 2.2984(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.1404(3.7483) | Error 0.8700(0.8857) | Error Color 0.8833(0.9010) |Steps 380(394.73) | Grad Norm 1.5781(4.4624) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 9.4460(9.2302) | Bit/dim 0.1403(0.1784) | Xent 2.3010(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.1876(3.5972) | Error 0.8833(0.8863) | Error Color 0.8989(0.9011) |Steps 410(392.78) | Grad Norm 4.6049(3.8440) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 9.1436(9.2397) | Bit/dim 0.1482(0.1702) | Xent 2.3030(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1872(3.4872) | Error 0.8911(0.8865) | Error Color 0.8889(0.8997) |Steps 398(390.13) | Grad Norm 14.5365(5.6497) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 8.5648(9.2171) | Bit/dim 0.2663(0.1923) | Xent 2.3048(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.3257(3.4428) | Error 0.9211(0.8881) | Error Color 0.9122(0.9005) |Steps 368(384.54) | Grad Norm 21.2946(9.8605) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 9.8081(9.2579) | Bit/dim 0.1904(0.1951) | Xent 2.3041(2.3014) | Xent Color 2.3025(2.3026) | Loss 3.3395(3.4215) | Error 0.8989(0.8892) | Error Color 0.9022(0.9002) |Steps 416(390.34) | Grad Norm 7.9060(9.8787) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 9.9671(9.3877) | Bit/dim 0.1557(0.1871) | Xent 2.3004(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.3638(3.3923) | Error 0.8811(0.8886) | Error Color 0.8889(0.8998) |Steps 458(397.98) | Grad Norm 3.2797(8.7870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 58.9565, Epoch Time 697.5929(665.8739), Bit/dim 0.1418(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.2927, Error 0.8865(best: 0.8865), Error Color 0.8997(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 9.6944(9.4732) | Bit/dim 0.1364(0.1752) | Xent 2.3011(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2476(3.8362) | Error 0.8800(0.8877) | Error Color 0.9067(0.9001) |Steps 422(401.72) | Grad Norm 1.8233(7.5187) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.2577(9.4936) | Bit/dim 0.1232(0.1628) | Xent 2.3001(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.1813(3.6692) | Error 0.8889(0.8888) | Error Color 0.9167(0.9006) |Steps 398(402.58) | Grad Norm 2.1429(6.2123) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 9.8948(9.5223) | Bit/dim 0.1210(0.1518) | Xent 2.3020(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.1530(3.5451) | Error 0.8822(0.8885) | Error Color 0.9133(0.9005) |Steps 344(400.74) | Grad Norm 2.0222(5.0883) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.4235(9.4946) | Bit/dim 0.1134(0.1423) | Xent 2.3030(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1215(3.4459) | Error 0.8956(0.8873) | Error Color 0.8944(0.8993) |Steps 380(399.21) | Grad Norm 1.0425(4.2270) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 9.2987(9.5100) | Bit/dim 0.3448(0.1440) | Xent 2.2987(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.6463(3.3965) | Error 0.8789(0.8877) | Error Color 0.9067(0.8987) |Steps 416(402.24) | Grad Norm 40.8505(5.7748) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 9.5514(9.5200) | Bit/dim 0.4015(0.2122) | Xent 2.3030(2.3012) | Xent Color 2.3022(2.3026) | Loss 3.6922(3.4865) | Error 0.9078(0.8884) | Error Color 0.8844(0.8978) |Steps 404(404.29) | Grad Norm 7.8823(9.1191) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 9.8776(9.5749) | Bit/dim 0.3170(0.2467) | Xent 2.3042(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.7020(3.5430) | Error 0.9000(0.8876) | Error Color 0.8900(0.8980) |Steps 440(410.26) | Grad Norm 5.7677(8.9750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 64.1265, Epoch Time 719.0311(667.4686), Bit/dim 0.3162(best: 0.1315), Xent 2.3010, Xent Color 2.3025. Loss 1.4671, Error 0.8865(best: 0.8865), Error Color 0.8940(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.7359(9.6272) | Bit/dim 0.2921(0.2609) | Xent 2.2993(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.5965(4.0389) | Error 0.8767(0.8874) | Error Color 0.9122(0.8987) |Steps 428(415.86) | Grad Norm 3.9104(7.9413) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 9.3463(9.5586) | Bit/dim 0.2751(0.2663) | Xent 2.3018(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.4879(3.9084) | Error 0.8878(0.8867) | Error Color 0.9056(0.8989) |Steps 428(414.11) | Grad Norm 2.6011(6.6791) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.1374(9.5294) | Bit/dim 0.2631(0.2672) | Xent 2.3061(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.4661(3.7980) | Error 0.9011(0.8865) | Error Color 0.9033(0.8990) |Steps 440(414.95) | Grad Norm 2.0753(5.4910) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 9.6099(9.5260) | Bit/dim 0.2543(0.2651) | Xent 2.3022(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.5068(3.7122) | Error 0.8944(0.8873) | Error Color 0.9044(0.9007) |Steps 398(413.51) | Grad Norm 1.6904(4.4608) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 9.4203(9.5240) | Bit/dim 0.2493(0.2615) | Xent 2.3013(2.3014) | Xent Color 2.3028(2.3026) | Loss 3.4196(3.6411) | Error 0.9000(0.8884) | Error Color 0.8944(0.9001) |Steps 398(411.17) | Grad Norm 1.6908(3.5801) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 9.2682(9.5735) | Bit/dim 0.2411(0.2566) | Xent 2.3042(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.3307(3.5807) | Error 0.9122(0.8882) | Error Color 0.9000(0.8996) |Steps 404(411.97) | Grad Norm 1.8568(3.0226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 62.4221, Epoch Time 717.1758(668.9598), Bit/dim 0.2555(best: 0.1315), Xent 2.3010, Xent Color 2.3025. Loss 1.4064, Error 0.8865(best: 0.8865), Error Color 0.9005(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 10.0387(9.5971) | Bit/dim 0.2384(0.2536) | Xent 2.2997(2.3009) | Xent Color 2.3024(2.3026) | Loss 3.3810(4.0493) | Error 0.8833(0.8877) | Error Color 0.8900(0.8997) |Steps 398(413.65) | Grad Norm 3.7189(4.0672) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.5105(9.5568) | Bit/dim 0.2363(0.2546) | Xent 2.2982(2.3009) | Xent Color 2.3025(2.3026) | Loss 3.3998(3.8896) | Error 0.8833(0.8871) | Error Color 0.9033(0.9003) |Steps 428(412.98) | Grad Norm 7.9808(6.5997) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 9.5710(9.5323) | Bit/dim 0.2337(0.2517) | Xent 2.3012(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.4060(3.7575) | Error 0.8844(0.8870) | Error Color 0.8867(0.9007) |Steps 404(409.47) | Grad Norm 8.7326(7.6075) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 9.8696(9.5372) | Bit/dim 0.2174(0.2454) | Xent 2.3007(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.3542(3.6579) | Error 0.8756(0.8872) | Error Color 0.9011(0.8999) |Steps 404(410.98) | Grad Norm 5.2779(7.6811) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 9.7255(9.5237) | Bit/dim 0.2004(0.2350) | Xent 2.3009(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.3853(3.5754) | Error 0.8978(0.8879) | Error Color 0.8867(0.8996) |Steps 428(410.68) | Grad Norm 6.1868(7.0096) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 9.9150(9.5510) | Bit/dim 0.1849(0.2234) | Xent 2.3018(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.3660(3.5085) | Error 0.9011(0.8879) | Error Color 0.9133(0.8993) |Steps 440(412.32) | Grad Norm 6.1432(6.3767) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 9.0324(9.5276) | Bit/dim 0.1620(0.2112) | Xent 2.2998(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2054(3.4471) | Error 0.8844(0.8877) | Error Color 0.9022(0.8999) |Steps 398(409.51) | Grad Norm 2.2114(6.5208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 60.6612, Epoch Time 711.6233(670.2397), Bit/dim 0.1624(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.3133, Error 0.8865(best: 0.8865), Error Color 0.8981(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 9.5797(9.4881) | Bit/dim 0.1568(0.1987) | Xent 2.3020(2.3014) | Xent Color 2.3024(2.3026) | Loss 3.2252(3.8303) | Error 0.8967(0.8891) | Error Color 0.8878(0.8995) |Steps 380(408.51) | Grad Norm 9.1327(6.9574) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 9.2848(9.4882) | Bit/dim 0.2782(0.2029) | Xent 2.3001(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.3622(3.6955) | Error 0.8967(0.8871) | Error Color 0.9000(0.8997) |Steps 404(407.89) | Grad Norm 31.1218(9.9875) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 10.0049(9.5038) | Bit/dim 0.3068(0.2256) | Xent 2.3009(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.5563(3.6471) | Error 0.8911(0.8889) | Error Color 0.9022(0.8992) |Steps 404(408.35) | Grad Norm 5.1797(11.3098) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 9.6745(9.5244) | Bit/dim 0.2735(0.2419) | Xent 2.3028(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.5250(3.6110) | Error 0.8900(0.8885) | Error Color 0.8933(0.8992) |Steps 404(410.15) | Grad Norm 2.2939(9.4603) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 9.3888(9.5162) | Bit/dim 0.2567(0.2475) | Xent 2.3021(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.4463(3.5767) | Error 0.8811(0.8877) | Error Color 0.9000(0.8987) |Steps 398(411.43) | Grad Norm 4.5834(7.8764) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.1522(9.5170) | Bit/dim 0.2346(0.2461) | Xent 2.3031(2.3010) | Xent Color 2.3027(2.3026) | Loss 3.4134(3.5427) | Error 0.8967(0.8863) | Error Color 0.9044(0.8998) |Steps 428(413.31) | Grad Norm 3.7073(6.4619) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 63.7613, Epoch Time 713.9790(671.5519), Bit/dim 0.2210(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.3719, Error 0.8865(best: 0.8865), Error Color 0.8972(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.4913(9.4915) | Bit/dim 0.2346(0.2400) | Xent 2.3017(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.4621(4.0645) | Error 0.8878(0.8878) | Error Color 0.8922(0.8977) |Steps 416(412.07) | Grad Norm 11.5850(5.7824) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 9.1133(9.5042) | Bit/dim 0.2364(0.2514) | Xent 2.3028(2.3013) | Xent Color 2.3030(2.3026) | Loss 3.3819(3.9175) | Error 0.9022(0.8886) | Error Color 0.9033(0.8987) |Steps 398(412.71) | Grad Norm 6.0002(7.5961) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 9.9388(9.5960) | Bit/dim 0.2150(0.2460) | Xent 2.2992(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.3995(3.7900) | Error 0.8856(0.8880) | Error Color 0.9111(0.8997) |Steps 446(418.51) | Grad Norm 10.6490(7.7593) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.6488(9.6254) | Bit/dim 0.2504(0.2403) | Xent 2.3001(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.4875(3.6866) | Error 0.8789(0.8877) | Error Color 0.8967(0.9006) |Steps 404(418.65) | Grad Norm 20.5163(9.2242) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.3402(9.5022) | Bit/dim 0.1840(0.2290) | Xent 2.3006(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2369(3.5783) | Error 0.8911(0.8877) | Error Color 0.9133(0.8989) |Steps 368(411.48) | Grad Norm 6.4004(8.7467) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 9.5916(9.4694) | Bit/dim 0.1693(0.2151) | Xent 2.2984(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.2386(3.4954) | Error 0.8689(0.8877) | Error Color 0.8944(0.8997) |Steps 410(407.54) | Grad Norm 3.5065(7.3845) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 9.5960(9.4420) | Bit/dim 0.1579(0.2015) | Xent 2.3021(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.2843(3.4254) | Error 0.8978(0.8860) | Error Color 0.8944(0.9006) |Steps 434(408.03) | Grad Norm 2.2313(6.1087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 60.3960, Epoch Time 710.3796(672.7167), Bit/dim 0.1558(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.3067, Error 0.8865(best: 0.8865), Error Color 0.9007(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 8.9360(9.4311) | Bit/dim 0.1520(0.1894) | Xent 2.3002(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.1423(3.8380) | Error 0.8789(0.8871) | Error Color 0.8900(0.8994) |Steps 404(405.84) | Grad Norm 4.8400(5.4233) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 9.3800(9.3810) | Bit/dim 0.1397(0.1789) | Xent 2.3020(2.3012) | Xent Color 2.3031(2.3026) | Loss 3.1259(3.6666) | Error 0.8933(0.8863) | Error Color 0.9178(0.9004) |Steps 386(404.49) | Grad Norm 1.0008(4.7190) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 9.3134(9.3457) | Bit/dim 0.1416(0.1700) | Xent 2.3023(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.1729(3.5366) | Error 0.8922(0.8875) | Error Color 0.8922(0.8999) |Steps 374(402.02) | Grad Norm 2.2039(4.5427) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 9.8523(9.3755) | Bit/dim 0.1608(0.1662) | Xent 2.2972(2.3011) | Xent Color 2.3029(2.3026) | Loss 3.2693(3.4538) | Error 0.8700(0.8864) | Error Color 0.9067(0.9007) |Steps 386(401.65) | Grad Norm 15.8076(6.1926) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 8.8854(9.3398) | Bit/dim 0.3516(0.2181) | Xent 2.3014(2.3012) | Xent Color 2.3029(2.3026) | Loss 3.5774(3.4843) | Error 0.8989(0.8879) | Error Color 0.9167(0.9026) |Steps 386(398.83) | Grad Norm 11.0144(11.1264) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 8.7098(9.3155) | Bit/dim 0.2666(0.2385) | Xent 2.3003(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.4255(3.4867) | Error 0.8800(0.8872) | Error Color 0.9189(0.9044) |Steps 380(399.75) | Grad Norm 6.3473(10.5009) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 9.4250(9.3631) | Bit/dim 0.2095(0.2369) | Xent 2.3003(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.3912(3.4725) | Error 0.8811(0.8877) | Error Color 0.9067(0.9040) |Steps 398(402.07) | Grad Norm 5.4571(9.1239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 63.8473, Epoch Time 702.4614(673.6090), Bit/dim 0.1977(best: 0.1315), Xent 2.3010, Xent Color 2.3025. Loss 1.3486, Error 0.8865(best: 0.8865), Error Color 0.8966(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 9.6855(9.4388) | Bit/dim 0.1744(0.2242) | Xent 2.2990(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.3218(3.9404) | Error 0.8744(0.8879) | Error Color 0.9022(0.9033) |Steps 404(407.19) | Grad Norm 2.4924(7.7057) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 9.9231(9.4891) | Bit/dim 0.1593(0.2090) | Xent 2.2978(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.2977(3.7650) | Error 0.8678(0.8876) | Error Color 0.8811(0.9026) |Steps 446(409.75) | Grad Norm 3.3307(6.5478) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 9.6052(9.4617) | Bit/dim 0.1504(0.1941) | Xent 2.3018(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.1957(3.6222) | Error 0.8856(0.8873) | Error Color 0.9000(0.9021) |Steps 380(409.02) | Grad Norm 2.7093(5.4457) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 9.3548(9.4738) | Bit/dim 0.1470(0.1814) | Xent 2.2995(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.2774(3.5154) | Error 0.8867(0.8865) | Error Color 0.8989(0.9016) |Steps 422(407.45) | Grad Norm 1.2911(4.4002) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.6531(9.4922) | Bit/dim 0.1380(0.1704) | Xent 2.3007(2.3010) | Xent Color 2.3022(2.3026) | Loss 3.2637(3.4409) | Error 0.8944(0.8869) | Error Color 0.8911(0.9005) |Steps 422(406.80) | Grad Norm 2.2812(3.7820) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 10.0370(9.5596) | Bit/dim 0.5562(0.1844) | Xent 2.3001(2.3010) | Xent Color 2.3025(2.3026) | Loss 4.0211(3.4330) | Error 0.8878(0.8869) | Error Color 0.9022(0.8995) |Steps 404(408.08) | Grad Norm 60.1346(7.3361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 70.3519, Epoch Time 729.8767(675.2971), Bit/dim 0.3684(best: 0.1315), Xent 2.3010, Xent Color 2.3025. Loss 1.5193, Error 0.8865(best: 0.8865), Error Color 0.9011(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.4946(9.7906) | Bit/dim 0.3869(0.2458) | Xent 2.3014(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.8776(4.2187) | Error 0.8833(0.8880) | Error Color 0.8967(0.8993) |Steps 488(423.89) | Grad Norm 15.3977(10.7010) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 11.2589(10.1688) | Bit/dim 0.2677(0.2598) | Xent 2.3032(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.7009(4.0906) | Error 0.8933(0.8872) | Error Color 0.8978(0.8991) |Steps 500(443.74) | Grad Norm 3.6194(9.7213) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.5636(10.1857) | Bit/dim 0.2272(0.2549) | Xent 2.3009(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.4295(3.9387) | Error 0.8889(0.8869) | Error Color 0.9111(0.8989) |Steps 422(444.79) | Grad Norm 4.7216(8.2833) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 10.3848(10.1216) | Bit/dim 0.2009(0.2427) | Xent 2.3023(2.3010) | Xent Color 2.3027(2.3026) | Loss 3.3560(3.8010) | Error 0.8922(0.8875) | Error Color 0.8989(0.8991) |Steps 416(439.91) | Grad Norm 5.2591(7.2299) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 9.8312(10.0039) | Bit/dim 0.1808(0.2289) | Xent 2.3038(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.3081(3.6801) | Error 0.8900(0.8878) | Error Color 0.8922(0.8990) |Steps 410(431.48) | Grad Norm 2.0448(6.1331) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.3175(9.9285) | Bit/dim 0.1712(0.2146) | Xent 2.3031(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2828(3.5711) | Error 0.9044(0.8884) | Error Color 0.8922(0.8995) |Steps 386(425.54) | Grad Norm 0.9984(4.9948) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 9.4518(9.8612) | Bit/dim 0.1658(0.2017) | Xent 2.3065(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.2672(3.4928) | Error 0.9156(0.8879) | Error Color 0.8789(0.8995) |Steps 392(419.56) | Grad Norm 2.6971(4.3365) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 62.4644, Epoch Time 752.2235(677.6049), Bit/dim 0.1621(best: 0.1315), Xent 2.3010, Xent Color 2.3025. Loss 1.3130, Error 0.8865(best: 0.8865), Error Color 0.8941(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 9.7417(9.8046) | Bit/dim 0.1524(0.1900) | Xent 2.3005(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2981(3.9037) | Error 0.8856(0.8871) | Error Color 0.8944(0.8998) |Steps 428(417.98) | Grad Norm 2.6673(4.1157) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 10.0332(9.7897) | Bit/dim 0.1490(0.1807) | Xent 2.3010(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.2664(3.7330) | Error 0.8833(0.8869) | Error Color 0.9022(0.9006) |Steps 440(416.87) | Grad Norm 1.2982(3.9427) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 9.8664(9.8049) | Bit/dim 0.1437(0.1726) | Xent 2.3004(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.2994(3.6057) | Error 0.8800(0.8868) | Error Color 0.9011(0.9002) |Steps 446(418.16) | Grad Norm 3.1668(3.7912) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 9.1333(9.7387) | Bit/dim 0.1707(0.1689) | Xent 2.2996(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2500(3.5127) | Error 0.8811(0.8866) | Error Color 0.8911(0.9014) |Steps 410(417.06) | Grad Norm 17.9924(5.7755) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.8852(9.7029) | Bit/dim 0.1649(0.1672) | Xent 2.2985(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.3229(3.4469) | Error 0.8800(0.8874) | Error Color 0.9000(0.9026) |Steps 410(413.53) | Grad Norm 13.9919(7.7921) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 9.7931(9.7010) | Bit/dim 0.1431(0.1644) | Xent 2.3020(2.3013) | Xent Color 2.3023(2.3026) | Loss 3.2505(3.3953) | Error 0.8956(0.8889) | Error Color 0.8956(0.9024) |Steps 422(411.81) | Grad Norm 6.2812(8.5241) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 61.9601, Epoch Time 724.5490(679.0132), Bit/dim 0.1419(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.2928, Error 0.8865(best: 0.8865), Error Color 0.9014(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 10.0244(9.7142) | Bit/dim 0.1382(0.1585) | Xent 2.3018(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.2893(3.8892) | Error 0.8944(0.8886) | Error Color 0.9100(0.9027) |Steps 440(413.12) | Grad Norm 3.4628(8.1329) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 9.5744(9.7126) | Bit/dim 0.1404(0.1526) | Xent 2.2999(2.3014) | Xent Color 2.3023(2.3026) | Loss 3.2022(3.7103) | Error 0.8789(0.8890) | Error Color 0.8833(0.9017) |Steps 392(412.03) | Grad Norm 9.5162(7.3915) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 9.9322(9.7264) | Bit/dim 0.1445(0.1480) | Xent 2.3005(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.3020(3.5838) | Error 0.8867(0.8882) | Error Color 0.8933(0.9005) |Steps 428(410.76) | Grad Norm 18.9121(8.0721) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 9.9242(9.7414) | Bit/dim 0.1382(0.1459) | Xent 2.2981(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.2300(3.4846) | Error 0.8844(0.8874) | Error Color 0.8889(0.9005) |Steps 428(410.29) | Grad Norm 13.8309(9.4097) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 9.3831(9.7116) | Bit/dim 0.1628(0.1503) | Xent 2.3018(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.2330(3.4226) | Error 0.8878(0.8879) | Error Color 0.9011(0.9004) |Steps 380(410.22) | Grad Norm 21.4719(11.5857) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 9.6049(9.6822) | Bit/dim 0.1429(0.1481) | Xent 2.3025(2.3012) | Xent Color 2.3025(2.3025) | Loss 3.2277(3.3680) | Error 0.8878(0.8876) | Error Color 0.8944(0.8987) |Steps 386(405.89) | Grad Norm 7.8727(11.1392) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 9.3860(9.7034) | Bit/dim 0.1248(0.1426) | Xent 2.3007(2.3011) | Xent Color 2.3027(2.3025) | Loss 3.1137(3.3217) | Error 0.8978(0.8879) | Error Color 0.9033(0.8975) |Steps 380(405.26) | Grad Norm 3.7947(9.6992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 61.5754, Epoch Time 726.0145(680.4232), Bit/dim 0.1277(best: 0.1315), Xent 2.3010, Xent Color 2.3026. Loss 1.2786, Error 0.8865(best: 0.8865), Error Color 0.8982(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 10.0739(9.6868) | Bit/dim 0.1369(0.1385) | Xent 2.2986(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.2524(3.7833) | Error 0.8767(0.8873) | Error Color 0.9189(0.8980) |Steps 416(407.09) | Grad Norm 10.5700(8.8791) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 9.9880(9.6735) | Bit/dim 0.1272(0.1359) | Xent 2.3045(2.3014) | Xent Color 2.3034(2.3026) | Loss 3.2745(3.6264) | Error 0.9011(0.8901) | Error Color 0.9278(0.8978) |Steps 446(407.07) | Grad Norm 12.1391(9.1172) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.7041(9.6950) | Bit/dim 0.1350(0.1341) | Xent 2.3032(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.2620(3.5195) | Error 0.8911(0.8890) | Error Color 0.8989(0.8983) |Steps 416(408.04) | Grad Norm 16.8138(9.8702) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.2873(9.6919) | Bit/dim 0.3835(0.1631) | Xent 2.3013(2.3014) | Xent Color 2.3025(2.3026) | Loss 3.6378(3.4969) | Error 0.8956(0.8889) | Error Color 0.8978(0.8982) |Steps 380(409.66) | Grad Norm 25.3643(12.8765) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.7218(9.7060) | Bit/dim 0.2403(0.1876) | Xent 2.3016(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.4730(3.4869) | Error 0.8856(0.8871) | Error Color 0.9000(0.8987) |Steps 434(411.55) | Grad Norm 12.2838(13.3732) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 9.2756(9.6147) | Bit/dim 0.1685(0.1885) | Xent 2.3020(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2971(3.4428) | Error 0.8844(0.8873) | Error Color 0.9056(0.9000) |Steps 416(410.21) | Grad Norm 5.9376(11.7487) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.1669(9.5552) | Bit/dim 0.1395(0.1768) | Xent 2.3020(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1736(3.3757) | Error 0.8822(0.8866) | Error Color 0.9067(0.9003) |Steps 404(408.51) | Grad Norm 7.0499(10.5402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 59.7209, Epoch Time 716.8068(681.5147), Bit/dim 0.1336(best: 0.1277), Xent 2.3010, Xent Color 2.3025. Loss 1.2845, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.0046(9.5863) | Bit/dim 0.1215(0.1640) | Xent 2.3006(2.3008) | Xent Color 2.3028(2.3026) | Loss 3.2427(3.7405) | Error 0.8822(0.8861) | Error Color 0.8933(0.8996) |Steps 410(407.56) | Grad Norm 2.2573(9.1198) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 9.5968(9.6577) | Bit/dim 0.1191(0.1528) | Xent 2.2984(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.2023(3.6001) | Error 0.8722(0.8865) | Error Color 0.8900(0.8979) |Steps 422(409.51) | Grad Norm 3.7691(7.7467) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 9.5592(9.6137) | Bit/dim 0.1159(0.1431) | Xent 2.3015(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1214(3.4858) | Error 0.8844(0.8864) | Error Color 0.8989(0.8985) |Steps 416(409.77) | Grad Norm 0.8388(6.2207) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.4049(9.6510) | Bit/dim 0.1130(0.1351) | Xent 2.3014(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.0914(3.4015) | Error 0.8878(0.8866) | Error Color 0.8900(0.8989) |Steps 410(411.18) | Grad Norm 2.1843(4.9805) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.5280(9.7091) | Bit/dim 0.1092(0.1285) | Xent 2.2990(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1557(3.3449) | Error 0.8822(0.8869) | Error Color 0.8989(0.8987) |Steps 392(412.97) | Grad Norm 4.4422(4.4775) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.7616(9.7048) | Bit/dim 0.1398(0.1267) | Xent 2.3053(2.3014) | Xent Color 2.3024(2.3026) | Loss 3.2327(3.2994) | Error 0.9100(0.8892) | Error Color 0.8878(0.8986) |Steps 428(412.39) | Grad Norm 12.3759(5.5089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 60.6653, Epoch Time 725.1609(682.8241), Bit/dim 0.1639(best: 0.1277), Xent 2.3010, Xent Color 2.3026. Loss 1.3148, Error 0.8865(best: 0.8865), Error Color 0.9039(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 10.5211(9.7127) | Bit/dim 0.2482(0.1374) | Xent 2.3037(2.3015) | Xent Color 2.3026(2.3026) | Loss 3.3662(3.8129) | Error 0.9011(0.8890) | Error Color 0.8900(0.8988) |Steps 422(413.22) | Grad Norm 44.0623(9.2377) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 11.1567(10.0583) | Bit/dim 0.3067(0.1810) | Xent 2.3003(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.7841(3.7848) | Error 0.8944(0.8884) | Error Color 0.8889(0.9004) |Steps 488(428.98) | Grad Norm 21.9332(12.9240) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 11.3690(10.1601) | Bit/dim 0.1725(0.1893) | Xent 2.3043(2.3015) | Xent Color 2.3026(2.3026) | Loss 3.5299(3.6985) | Error 0.8978(0.8890) | Error Color 0.8989(0.9011) |Steps 488(435.43) | Grad Norm 4.6988(12.1893) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 10.0379(10.3230) | Bit/dim 0.1400(0.1804) | Xent 2.2989(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2184(3.6039) | Error 0.8767(0.8877) | Error Color 0.9033(0.9014) |Steps 422(443.04) | Grad Norm 6.8292(10.9712) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 9.4465(10.1494) | Bit/dim 0.1150(0.1655) | Xent 2.3038(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.0607(3.4782) | Error 0.9100(0.8879) | Error Color 0.9044(0.9005) |Steps 398(433.19) | Grad Norm 2.5625(9.2848) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 9.6946(10.0490) | Bit/dim 0.1111(0.1516) | Xent 2.3015(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1426(3.3883) | Error 0.8867(0.8878) | Error Color 0.8989(0.8991) |Steps 416(428.03) | Grad Norm 1.6489(7.6144) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 9.5940(9.9289) | Bit/dim 0.1036(0.1403) | Xent 2.3011(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.0395(3.3125) | Error 0.8844(0.8882) | Error Color 0.9100(0.8994) |Steps 404(421.65) | Grad Norm 2.2507(6.1505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 59.2930, Epoch Time 753.1375(684.9335), Bit/dim 0.1074(best: 0.1277), Xent 2.3010, Xent Color 2.3026. Loss 1.2583, Error 0.8865(best: 0.8865), Error Color 0.9044(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 9.4418(9.8257) | Bit/dim 0.1075(0.1317) | Xent 2.3008(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.0188(3.6616) | Error 0.8911(0.8871) | Error Color 0.8989(0.8989) |Steps 398(417.31) | Grad Norm 1.9689(4.9640) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 9.7527(9.7892) | Bit/dim 0.1039(0.1249) | Xent 2.3001(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.1196(3.5058) | Error 0.8756(0.8862) | Error Color 0.8922(0.8986) |Steps 410(413.30) | Grad Norm 2.2298(4.0194) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 9.6161(9.7287) | Bit/dim 0.1016(0.1190) | Xent 2.3011(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.1184(3.3944) | Error 0.8844(0.8873) | Error Color 0.9144(0.8972) |Steps 422(409.76) | Grad Norm 1.1566(3.4433) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 9.2328(9.6777) | Bit/dim 0.1080(0.1144) | Xent 2.3006(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.1494(3.3131) | Error 0.8767(0.8866) | Error Color 0.9044(0.8981) |Steps 404(408.37) | Grad Norm 10.7456(3.7714) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 10.0111(9.6803) | Bit/dim 0.1047(0.1113) | Xent 2.3012(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1660(3.2587) | Error 0.8889(0.8878) | Error Color 0.8933(0.8975) |Steps 422(407.08) | Grad Norm 12.1739(4.6210) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 10.9051(9.7303) | Bit/dim 0.1596(0.1112) | Xent 2.3011(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.4243(3.2289) | Error 0.8833(0.8886) | Error Color 0.9011(0.8980) |Steps 470(408.97) | Grad Norm 21.5175(6.2870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 69.8217, Epoch Time 743.5039(686.6906), Bit/dim 0.6315(best: 0.1074), Xent 2.3010, Xent Color 2.3027. Loss 1.7824, Error 0.8865(best: 0.8865), Error Color 0.9020(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.5377(10.1896) | Bit/dim 0.6571(0.2544) | Xent 2.2987(2.3013) | Xent Color 2.3026(2.3026) | Loss 4.4845(4.2797) | Error 0.8822(0.8882) | Error Color 0.9011(0.9000) |Steps 524(432.03) | Grad Norm 17.6439(10.8869) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 10.1454(10.4255) | Bit/dim 0.4495(0.3155) | Xent 2.2982(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.9459(4.2226) | Error 0.8722(0.8884) | Error Color 0.8944(0.8993) |Steps 446(444.89) | Grad Norm 6.4794(10.1328) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 9.8599(10.3488) | Bit/dim 0.3523(0.3336) | Xent 2.3044(2.3013) | Xent Color 2.3022(2.3026) | Loss 3.6011(4.0990) | Error 0.8956(0.8896) | Error Color 0.8778(0.8978) |Steps 428(445.07) | Grad Norm 2.3867(8.3751) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 9.7742(10.2219) | Bit/dim 0.3169(0.3332) | Xent 2.2995(2.3014) | Xent Color 2.3023(2.3026) | Loss 3.5290(3.9552) | Error 0.8756(0.8888) | Error Color 0.8922(0.8983) |Steps 392(438.86) | Grad Norm 2.2275(6.8051) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 9.5833(10.0796) | Bit/dim 0.2859(0.3240) | Xent 2.3008(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.4093(3.8258) | Error 0.8767(0.8871) | Error Color 0.8900(0.8996) |Steps 404(434.08) | Grad Norm 2.1232(5.4800) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 9.5187(9.9511) | Bit/dim 0.2514(0.3093) | Xent 2.3014(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.3016(3.7031) | Error 0.8944(0.8867) | Error Color 0.9122(0.9008) |Steps 392(426.23) | Grad Norm 0.8349(4.3862) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 9.6913(9.8327) | Bit/dim 0.2260(0.2904) | Xent 2.3016(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2801(3.5981) | Error 0.8900(0.8876) | Error Color 0.8867(0.9006) |Steps 404(418.35) | Grad Norm 1.5017(3.5307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 55.0158, Epoch Time 738.5136(688.2453), Bit/dim 0.2094(best: 0.1074), Xent 2.3010, Xent Color 2.3026. Loss 1.3603, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.4355(9.7487) | Bit/dim 0.1944(0.2695) | Xent 2.3002(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1620(3.9405) | Error 0.8856(0.8875) | Error Color 0.8911(0.9001) |Steps 374(410.92) | Grad Norm 5.9847(3.9260) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.6721(9.7535) | Bit/dim 0.1675(0.2475) | Xent 2.3034(2.3014) | Xent Color 2.3029(2.3026) | Loss 3.2080(3.7594) | Error 0.8933(0.8880) | Error Color 0.9300(0.9011) |Steps 380(408.54) | Grad Norm 1.9515(4.8687) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 9.3395(9.6753) | Bit/dim 0.1579(0.2254) | Xent 2.3014(2.3015) | Xent Color 2.3025(2.3026) | Loss 3.0658(3.5967) | Error 0.8956(0.8885) | Error Color 0.9089(0.9006) |Steps 398(402.45) | Grad Norm 7.8419(4.8644) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 9.2878(9.5934) | Bit/dim 0.1394(0.2050) | Xent 2.3014(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.0496(3.4692) | Error 0.8900(0.8880) | Error Color 0.8789(0.9001) |Steps 362(398.19) | Grad Norm 2.3890(4.3787) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 10.0371(9.6240) | Bit/dim 0.1386(0.1879) | Xent 2.3009(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1188(3.3760) | Error 0.8911(0.8878) | Error Color 0.9056(0.8995) |Steps 368(395.10) | Grad Norm 3.5432(3.9326) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 10.4810(9.5682) | Bit/dim 0.1657(0.1756) | Xent 2.3012(2.3011) | Xent Color 2.3030(2.3026) | Loss 3.2900(3.3099) | Error 0.8933(0.8875) | Error Color 0.9067(0.8997) |Steps 428(394.96) | Grad Norm 14.6071(4.4542) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 9.8724(9.5279) | Bit/dim 0.1383(0.1664) | Xent 2.3014(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.1620(3.2588) | Error 0.8867(0.8873) | Error Color 0.9044(0.8997) |Steps 416(393.75) | Grad Norm 4.1069(5.3339) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 54.1583, Epoch Time 705.4736(688.7622), Bit/dim 0.1552(best: 0.1074), Xent 2.3010, Xent Color 2.3027. Loss 1.3061, Error 0.8865(best: 0.8865), Error Color 0.9028(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 10.1355(9.6058) | Bit/dim 0.1455(0.1664) | Xent 2.3022(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.1584(3.6275) | Error 0.8889(0.8873) | Error Color 0.8967(0.8999) |Steps 410(395.98) | Grad Norm 6.2008(8.2295) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 9.3477(9.6096) | Bit/dim 0.1370(0.1601) | Xent 2.2991(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1466(3.5045) | Error 0.8822(0.8870) | Error Color 0.8911(0.9000) |Steps 392(398.24) | Grad Norm 4.6283(8.0654) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 9.1986(9.5281) | Bit/dim 0.1264(0.1523) | Xent 2.3011(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1023(3.3971) | Error 0.8944(0.8868) | Error Color 0.9111(0.8998) |Steps 404(395.51) | Grad Norm 4.5001(7.3755) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 9.1310(9.4570) | Bit/dim 0.1165(0.1449) | Xent 2.3042(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.0561(3.3142) | Error 0.8989(0.8868) | Error Color 0.8933(0.8986) |Steps 392(392.32) | Grad Norm 2.0814(6.3372) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 9.7035(9.3978) | Bit/dim 0.1212(0.1389) | Xent 2.3008(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.1093(3.2504) | Error 0.8844(0.8873) | Error Color 0.8900(0.8973) |Steps 416(391.94) | Grad Norm 4.3688(5.8343) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 9.0427(9.3532) | Bit/dim 0.1201(0.1335) | Xent 2.2989(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.0448(3.1968) | Error 0.8856(0.8874) | Error Color 0.8944(0.8970) |Steps 374(387.86) | Grad Norm 8.9859(5.7100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 55.7251, Epoch Time 699.0677(689.0713), Bit/dim 0.1135(best: 0.1074), Xent 2.3010, Xent Color 2.3027. Loss 1.2645, Error 0.8865(best: 0.8865), Error Color 0.9008(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 9.5445(9.3254) | Bit/dim 0.1183(0.1289) | Xent 2.2996(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.0444(3.5964) | Error 0.8700(0.8873) | Error Color 0.8856(0.8983) |Steps 380(386.00) | Grad Norm 6.8133(5.4298) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.3643(9.3602) | Bit/dim 0.2877(0.1415) | Xent 2.3031(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.3928(3.4926) | Error 0.8978(0.8871) | Error Color 0.9044(0.8997) |Steps 374(387.78) | Grad Norm 42.9737(8.7600) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 9.8067(9.5333) | Bit/dim 0.1855(0.1578) | Xent 2.2985(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2898(3.4504) | Error 0.8856(0.8874) | Error Color 0.9000(0.8994) |Steps 398(394.40) | Grad Norm 5.3079(8.8673) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.5148(9.5137) | Bit/dim 0.1619(0.1609) | Xent 2.3003(2.3010) | Xent Color 2.3024(2.3026) | Loss 3.1256(3.3845) | Error 0.8844(0.8873) | Error Color 0.8989(0.9001) |Steps 356(393.18) | Grad Norm 3.6608(7.6525) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 9.0559(9.4485) | Bit/dim 0.1380(0.1567) | Xent 2.3045(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.0437(3.3038) | Error 0.9100(0.8879) | Error Color 0.8833(0.8974) |Steps 374(387.91) | Grad Norm 1.5925(6.4202) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 8.7032(9.3642) | Bit/dim 0.1145(0.1474) | Xent 2.3036(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.0114(3.2352) | Error 0.9022(0.8879) | Error Color 0.8867(0.8976) |Steps 374(385.24) | Grad Norm 2.8493(5.5672) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 9.2214(9.3493) | Bit/dim 0.1120(0.1383) | Xent 2.3010(2.3014) | Xent Color 2.3028(2.3026) | Loss 3.0383(3.1838) | Error 0.8911(0.8889) | Error Color 0.9189(0.8989) |Steps 392(384.23) | Grad Norm 3.1746(4.7228) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 56.4891, Epoch Time 701.8306(689.4541), Bit/dim 0.1083(best: 0.1074), Xent 2.3010, Xent Color 2.3027. Loss 1.2592, Error 0.8865(best: 0.8865), Error Color 0.9043(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 9.6301(9.3360) | Bit/dim 0.1131(0.1308) | Xent 2.2983(2.3013) | Xent Color 2.3029(2.3026) | Loss 3.0894(3.5487) | Error 0.8789(0.8882) | Error Color 0.9111(0.9003) |Steps 386(384.49) | Grad Norm 9.0568(4.5412) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 9.6284(9.3380) | Bit/dim 0.1032(0.1254) | Xent 2.3010(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.0340(3.4136) | Error 0.8822(0.8887) | Error Color 0.9033(0.8999) |Steps 398(384.76) | Grad Norm 5.9418(5.3406) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 9.4284(9.4136) | Bit/dim 0.1058(0.1200) | Xent 2.3017(2.3012) | Xent Color 2.3020(2.3026) | Loss 3.0347(3.3169) | Error 0.8911(0.8876) | Error Color 0.8700(0.8981) |Steps 386(387.45) | Grad Norm 10.1428(5.8561) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 9.1790(9.3937) | Bit/dim 0.1109(0.1183) | Xent 2.3005(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.0709(3.2442) | Error 0.8922(0.8867) | Error Color 0.8967(0.8998) |Steps 386(387.39) | Grad Norm 10.8311(7.4678) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 11.8670(9.6528) | Bit/dim 0.3309(0.1471) | Xent 2.3027(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.7464(3.2877) | Error 0.8900(0.8865) | Error Color 0.8833(0.9005) |Steps 476(398.25) | Grad Norm 13.0996(10.2484) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 10.1666(9.9176) | Bit/dim 0.1766(0.1707) | Xent 2.3024(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.2858(3.3253) | Error 0.8922(0.8874) | Error Color 0.9000(0.8991) |Steps 464(413.55) | Grad Norm 3.9783(9.6714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 58.5733, Epoch Time 732.0430(690.7318), Bit/dim 0.1552(best: 0.1074), Xent 2.3010, Xent Color 2.3027. Loss 1.3062, Error 0.8865(best: 0.8865), Error Color 0.8997(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.1609(9.9618) | Bit/dim 0.1482(0.1702) | Xent 2.3019(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1236(3.8242) | Error 0.8867(0.8880) | Error Color 0.9067(0.8993) |Steps 386(414.11) | Grad Norm 4.4714(8.5108) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 9.8955(9.9731) | Bit/dim 0.1137(0.1589) | Xent 2.3013(2.3013) | Xent Color 2.3029(2.3027) | Loss 3.1725(3.6564) | Error 0.8856(0.8882) | Error Color 0.9100(0.9005) |Steps 428(417.71) | Grad Norm 3.3846(7.1703) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 10.4233(9.9955) | Bit/dim 0.1081(0.1463) | Xent 2.3009(2.3008) | Xent Color 2.3026(2.3026) | Loss 3.1183(3.5166) | Error 0.8822(0.8858) | Error Color 0.9033(0.9004) |Steps 428(416.80) | Grad Norm 3.5776(6.1066) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 10.4282(9.9494) | Bit/dim 0.1043(0.1353) | Xent 2.3019(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.1673(3.4084) | Error 0.8911(0.8872) | Error Color 0.8900(0.9010) |Steps 416(414.33) | Grad Norm 7.9328(5.4591) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 11.0417(10.1107) | Bit/dim 0.2611(0.1783) | Xent 2.3027(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.6148(3.4455) | Error 0.8844(0.8874) | Error Color 0.9078(0.9008) |Steps 476(420.56) | Grad Norm 8.3891(8.6984) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 10.8091(10.1176) | Bit/dim 0.2129(0.1933) | Xent 2.3024(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.4327(3.4404) | Error 0.9100(0.8891) | Error Color 0.8944(0.9001) |Steps 434(424.49) | Grad Norm 4.3807(8.2693) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_25_drop_0_5_rl_stdscale_6_2cond_mlp_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn mlp\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
