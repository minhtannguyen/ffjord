{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run3/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 26.4673(27.2702) | Bit/dim 3.5389(3.5641) | Xent 0.1262(0.1722) | Loss 3.6020(3.6502) | Error 0.0400(0.0611) Steps 1054(1076.26) | Grad Norm 1.5858(3.0835) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 27.3470(27.1736) | Bit/dim 3.5560(3.5637) | Xent 0.0902(0.1607) | Loss 3.6011(3.6441) | Error 0.0322(0.0563) Steps 1054(1078.46) | Grad Norm 1.4201(2.6979) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 26.6180(27.0921) | Bit/dim 3.5648(3.5614) | Xent 0.1109(0.1484) | Loss 3.6203(3.6356) | Error 0.0356(0.0521) Steps 1060(1076.94) | Grad Norm 1.2837(2.3610) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 26.6205(26.9657) | Bit/dim 3.5387(3.5573) | Xent 0.1318(0.1410) | Loss 3.6046(3.6278) | Error 0.0411(0.0491) Steps 1084(1077.94) | Grad Norm 1.5163(2.0902) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 26.9875(26.9319) | Bit/dim 3.5276(3.5530) | Xent 0.1333(0.1326) | Loss 3.5943(3.6193) | Error 0.0411(0.0460) Steps 1060(1075.77) | Grad Norm 1.1535(1.8551) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 132.0545, Epoch Time 1643.7559(1550.4736), Bit/dim 3.5538(best: inf), Xent 0.9178, Loss 4.0127, Error 0.2220(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 26.6277(26.9046) | Bit/dim 3.5484(3.5505) | Xent 0.1391(0.1273) | Loss 3.6179(3.6142) | Error 0.0489(0.0441) Steps 1078(1073.43) | Grad Norm 1.1180(1.6554) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 26.9642(26.8421) | Bit/dim 3.5425(3.5486) | Xent 0.1198(0.1234) | Loss 3.6024(3.6103) | Error 0.0422(0.0423) Steps 1090(1074.96) | Grad Norm 1.5819(1.5504) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 26.4458(26.7495) | Bit/dim 3.5571(3.5476) | Xent 0.1097(0.1198) | Loss 3.6119(3.6075) | Error 0.0356(0.0406) Steps 1090(1076.11) | Grad Norm 1.3062(1.4419) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 27.0639(26.7663) | Bit/dim 3.5541(3.5460) | Xent 0.0827(0.1154) | Loss 3.5955(3.6037) | Error 0.0200(0.0393) Steps 1078(1075.13) | Grad Norm 1.0441(1.3697) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 26.6466(26.8124) | Bit/dim 3.5537(3.5478) | Xent 0.1084(0.1130) | Loss 3.6079(3.6043) | Error 0.0378(0.0388) Steps 1066(1075.55) | Grad Norm 1.4591(1.3402) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 27.6753(26.8307) | Bit/dim 3.5315(3.5462) | Xent 0.0901(0.1118) | Loss 3.5765(3.6021) | Error 0.0311(0.0383) Steps 1072(1077.61) | Grad Norm 1.2942(1.3356) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 125.0862, Epoch Time 1615.5287(1552.4253), Bit/dim 3.5540(best: 3.5538), Xent 0.9364, Loss 4.0222, Error 0.2203(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.3830(26.8692) | Bit/dim 3.5730(3.5447) | Xent 0.1039(0.1095) | Loss 3.6250(3.5995) | Error 0.0333(0.0373) Steps 1066(1077.86) | Grad Norm 1.1776(1.3050) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 26.9580(26.8670) | Bit/dim 3.5402(3.5468) | Xent 0.0943(0.1091) | Loss 3.5873(3.6013) | Error 0.0267(0.0371) Steps 1090(1077.29) | Grad Norm 1.0003(1.2664) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 27.0263(26.8644) | Bit/dim 3.5369(3.5449) | Xent 0.1057(0.1067) | Loss 3.5897(3.5983) | Error 0.0333(0.0364) Steps 1066(1077.13) | Grad Norm 1.2314(1.2262) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 26.6386(26.9262) | Bit/dim 3.5408(3.5443) | Xent 0.1008(0.1067) | Loss 3.5912(3.5976) | Error 0.0333(0.0365) Steps 1048(1076.14) | Grad Norm 1.1911(1.2457) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 25.8418(26.8060) | Bit/dim 3.5709(3.5423) | Xent 0.0975(0.1088) | Loss 3.6196(3.5967) | Error 0.0267(0.0369) Steps 1072(1075.45) | Grad Norm 1.1759(1.2473) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 127.0062, Epoch Time 1623.9373(1554.5706), Bit/dim 3.5524(best: 3.5538), Xent 0.9430, Loss 4.0239, Error 0.2192(best: 0.2203)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 27.1587(26.8678) | Bit/dim 3.5567(3.5433) | Xent 0.1058(0.1093) | Loss 3.6096(3.5980) | Error 0.0333(0.0368) Steps 1096(1076.05) | Grad Norm 1.8379(1.2689) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 28.0104(26.9571) | Bit/dim 3.5269(3.5439) | Xent 0.1105(0.1088) | Loss 3.5821(3.5983) | Error 0.0400(0.0370) Steps 1078(1077.43) | Grad Norm 1.3239(1.2556) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 27.2600(27.0282) | Bit/dim 3.5075(3.5432) | Xent 0.1154(0.1085) | Loss 3.5652(3.5974) | Error 0.0400(0.0371) Steps 1084(1079.13) | Grad Norm 1.3647(1.2651) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 27.3907(27.1817) | Bit/dim 3.5648(3.5431) | Xent 0.1030(0.1069) | Loss 3.6162(3.5966) | Error 0.0333(0.0366) Steps 1102(1082.32) | Grad Norm 2.1016(1.2868) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 27.1735(27.2059) | Bit/dim 3.5444(3.5429) | Xent 0.1268(0.1067) | Loss 3.6078(3.5963) | Error 0.0411(0.0366) Steps 1078(1081.84) | Grad Norm 1.3783(1.2845) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 27.2770(27.1729) | Bit/dim 3.5260(3.5423) | Xent 0.1195(0.1077) | Loss 3.5858(3.5961) | Error 0.0456(0.0371) Steps 1066(1081.47) | Grad Norm 1.6560(1.2634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 126.8335, Epoch Time 1643.8436(1557.2488), Bit/dim 3.5522(best: 3.5524), Xent 0.9547, Loss 4.0295, Error 0.2208(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 26.9767(27.1324) | Bit/dim 3.5520(3.5415) | Xent 0.1173(0.1079) | Loss 3.6106(3.5955) | Error 0.0400(0.0376) Steps 1072(1081.93) | Grad Norm 1.0641(1.2929) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 26.7113(27.1131) | Bit/dim 3.5568(3.5439) | Xent 0.0979(0.1057) | Loss 3.6057(3.5967) | Error 0.0333(0.0365) Steps 1078(1083.19) | Grad Norm 1.1011(1.2801) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 27.7306(27.2060) | Bit/dim 3.5456(3.5432) | Xent 0.1094(0.1062) | Loss 3.6003(3.5963) | Error 0.0411(0.0366) Steps 1096(1082.50) | Grad Norm 1.4587(1.2919) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 27.3226(27.2463) | Bit/dim 3.5366(3.5428) | Xent 0.1068(0.1063) | Loss 3.5900(3.5960) | Error 0.0311(0.0364) Steps 1096(1080.56) | Grad Norm 1.5426(1.3430) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 26.8590(27.2416) | Bit/dim 3.5398(3.5412) | Xent 0.1087(0.1058) | Loss 3.5941(3.5941) | Error 0.0422(0.0364) Steps 1066(1081.24) | Grad Norm 1.4949(1.3277) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 127.1772, Epoch Time 1642.6630(1559.8112), Bit/dim 3.5511(best: 3.5522), Xent 0.9541, Loss 4.0281, Error 0.2199(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 27.2121(27.2518) | Bit/dim 3.5545(3.5431) | Xent 0.0999(0.1034) | Loss 3.6044(3.5948) | Error 0.0333(0.0358) Steps 1072(1079.80) | Grad Norm 1.1331(1.3091) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 27.4291(27.2274) | Bit/dim 3.5152(3.5427) | Xent 0.1075(0.1028) | Loss 3.5689(3.5941) | Error 0.0311(0.0352) Steps 1072(1080.18) | Grad Norm 1.1100(1.2972) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 26.5727(27.1801) | Bit/dim 3.5299(3.5406) | Xent 0.1063(0.1040) | Loss 3.5831(3.5926) | Error 0.0411(0.0356) Steps 1054(1079.31) | Grad Norm 1.7821(1.3235) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 27.4176(27.1543) | Bit/dim 3.5289(3.5418) | Xent 0.1034(0.1031) | Loss 3.5806(3.5933) | Error 0.0400(0.0355) Steps 1066(1077.36) | Grad Norm 1.2663(1.3035) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 26.8065(27.1480) | Bit/dim 3.5499(3.5425) | Xent 0.0924(0.1017) | Loss 3.5961(3.5934) | Error 0.0289(0.0346) Steps 1078(1078.33) | Grad Norm 1.2906(1.3055) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 27.1372(27.1455) | Bit/dim 3.5718(3.5431) | Xent 0.1078(0.1022) | Loss 3.6257(3.5942) | Error 0.0378(0.0348) Steps 1078(1079.17) | Grad Norm 1.1582(1.3110) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 126.3665, Epoch Time 1637.3972(1562.1388), Bit/dim 3.5501(best: 3.5511), Xent 0.9612, Loss 4.0308, Error 0.2219(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 26.6308(27.0809) | Bit/dim 3.5431(3.5425) | Xent 0.1086(0.1045) | Loss 3.5974(3.5948) | Error 0.0378(0.0355) Steps 1078(1077.50) | Grad Norm 1.5836(1.3638) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 27.8949(27.1976) | Bit/dim 3.5211(3.5407) | Xent 0.0914(0.1028) | Loss 3.5668(3.5921) | Error 0.0344(0.0353) Steps 1090(1077.46) | Grad Norm 1.1965(1.3792) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 26.9882(27.2875) | Bit/dim 3.5651(3.5414) | Xent 0.0965(0.1039) | Loss 3.6133(3.5933) | Error 0.0322(0.0359) Steps 1090(1078.24) | Grad Norm 1.0291(1.4149) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 27.7560(27.3828) | Bit/dim 3.5584(3.5423) | Xent 0.1123(0.1034) | Loss 3.6145(3.5940) | Error 0.0400(0.0358) Steps 1084(1079.11) | Grad Norm 1.8598(1.4566) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 27.0246(27.3079) | Bit/dim 3.5155(3.5425) | Xent 0.0955(0.1026) | Loss 3.5633(3.5938) | Error 0.0289(0.0355) Steps 1078(1080.89) | Grad Norm 1.2272(1.4586) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 126.7730, Epoch Time 1647.2461(1564.6920), Bit/dim 3.5523(best: 3.5501), Xent 0.9719, Loss 4.0382, Error 0.2241(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 27.0103(27.1821) | Bit/dim 3.5663(3.5424) | Xent 0.1080(0.1026) | Loss 3.6203(3.5937) | Error 0.0344(0.0351) Steps 1078(1078.93) | Grad Norm 1.0833(1.4129) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 27.0303(27.1454) | Bit/dim 3.5518(3.5403) | Xent 0.1095(0.1005) | Loss 3.6066(3.5906) | Error 0.0433(0.0346) Steps 1090(1079.37) | Grad Norm 2.2043(1.4326) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 26.7441(27.1250) | Bit/dim 3.5214(3.5405) | Xent 0.1205(0.0998) | Loss 3.5817(3.5904) | Error 0.0311(0.0335) Steps 1066(1079.16) | Grad Norm 1.0019(1.3736) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 27.1802(27.0856) | Bit/dim 3.5535(3.5409) | Xent 0.1155(0.1000) | Loss 3.6112(3.5910) | Error 0.0444(0.0339) Steps 1072(1078.56) | Grad Norm 1.2281(1.3670) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 27.0798(27.1354) | Bit/dim 3.5217(3.5426) | Xent 0.1215(0.1018) | Loss 3.5825(3.5935) | Error 0.0444(0.0345) Steps 1054(1078.17) | Grad Norm 1.5172(1.3724) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 26.7257(27.1742) | Bit/dim 3.5522(3.5417) | Xent 0.1133(0.1017) | Loss 3.6088(3.5926) | Error 0.0411(0.0343) Steps 1066(1078.07) | Grad Norm 1.1551(1.3889) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 126.2852, Epoch Time 1634.4768(1566.7856), Bit/dim 3.5500(best: 3.5501), Xent 0.9644, Loss 4.0322, Error 0.2224(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 26.7003(27.0239) | Bit/dim 3.5320(3.5413) | Xent 0.1012(0.1004) | Loss 3.5826(3.5915) | Error 0.0378(0.0340) Steps 1060(1078.60) | Grad Norm 1.2800(1.3596) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 26.5092(26.9675) | Bit/dim 3.5587(3.5423) | Xent 0.1034(0.1009) | Loss 3.6104(3.5928) | Error 0.0311(0.0347) Steps 1090(1079.23) | Grad Norm 1.1657(1.3662) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 27.8781(27.0361) | Bit/dim 3.5607(3.5435) | Xent 0.0964(0.0983) | Loss 3.6088(3.5926) | Error 0.0333(0.0339) Steps 1060(1078.47) | Grad Norm 1.9584(1.3734) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 27.1737(27.0528) | Bit/dim 3.5401(3.5418) | Xent 0.0810(0.0980) | Loss 3.5806(3.5908) | Error 0.0256(0.0337) Steps 1054(1078.27) | Grad Norm 1.5767(1.3729) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 26.4673(27.0370) | Bit/dim 3.5490(3.5392) | Xent 0.1260(0.1004) | Loss 3.6120(3.5894) | Error 0.0444(0.0343) Steps 1060(1077.72) | Grad Norm 1.7311(1.3887) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 127.7820, Epoch Time 1628.0908(1568.6247), Bit/dim 3.5513(best: 3.5500), Xent 0.9628, Loss 4.0327, Error 0.2189(best: 0.2192)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 27.3865(27.0362) | Bit/dim 3.5401(3.5394) | Xent 0.0959(0.0992) | Loss 3.5880(3.5890) | Error 0.0300(0.0342) Steps 1108(1079.01) | Grad Norm 1.1961(1.3393) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 27.1612(27.0362) | Bit/dim 3.5244(3.5404) | Xent 0.0883(0.0982) | Loss 3.5685(3.5895) | Error 0.0278(0.0337) Steps 1084(1080.35) | Grad Norm 2.0923(1.3427) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 27.7390(27.1002) | Bit/dim 3.5654(3.5406) | Xent 0.0941(0.0974) | Loss 3.6124(3.5893) | Error 0.0356(0.0337) Steps 1060(1080.04) | Grad Norm 1.2278(1.3908) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 26.8861(27.1600) | Bit/dim 3.5370(3.5394) | Xent 0.0906(0.0967) | Loss 3.5823(3.5877) | Error 0.0300(0.0334) Steps 1084(1083.79) | Grad Norm 1.5963(1.4105) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 27.7606(27.1747) | Bit/dim 3.5277(3.5410) | Xent 0.0855(0.0975) | Loss 3.5705(3.5897) | Error 0.0244(0.0332) Steps 1096(1087.09) | Grad Norm 1.5306(1.4719) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 26.7930(27.2035) | Bit/dim 3.5473(3.5412) | Xent 0.0896(0.0972) | Loss 3.5921(3.5898) | Error 0.0311(0.0336) Steps 1072(1085.97) | Grad Norm 1.2093(1.4633) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 126.4750, Epoch Time 1642.0345(1570.8270), Bit/dim 3.5500(best: 3.5500), Xent 0.9698, Loss 4.0349, Error 0.2198(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 26.6353(27.0624) | Bit/dim 3.5716(3.5427) | Xent 0.0869(0.0971) | Loss 3.6151(3.5913) | Error 0.0333(0.0331) Steps 1066(1084.37) | Grad Norm 1.0293(1.4140) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 26.7062(27.0480) | Bit/dim 3.5431(3.5436) | Xent 0.0716(0.0988) | Loss 3.5788(3.5930) | Error 0.0300(0.0338) Steps 1096(1085.02) | Grad Norm 1.1615(1.4040) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 26.4060(27.1070) | Bit/dim 3.5611(3.5405) | Xent 0.0933(0.0987) | Loss 3.6077(3.5899) | Error 0.0333(0.0342) Steps 1090(1085.66) | Grad Norm 1.4759(1.4081) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 27.0713(27.1545) | Bit/dim 3.5198(3.5398) | Xent 0.1176(0.0978) | Loss 3.5786(3.5887) | Error 0.0378(0.0342) Steps 1084(1084.31) | Grad Norm 1.2550(1.3913) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 27.2632(27.1556) | Bit/dim 3.5286(3.5382) | Xent 0.1048(0.0982) | Loss 3.5810(3.5873) | Error 0.0344(0.0344) Steps 1078(1084.60) | Grad Norm 1.3245(1.3641) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 126.5654, Epoch Time 1634.5418(1572.7385), Bit/dim 3.5508(best: 3.5500), Xent 0.9891, Loss 4.0454, Error 0.2214(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 26.7707(27.1710) | Bit/dim 3.5897(3.5417) | Xent 0.1025(0.0977) | Loss 3.6409(3.5906) | Error 0.0289(0.0335) Steps 1090(1085.95) | Grad Norm 1.1987(1.3520) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 27.1322(27.3213) | Bit/dim 3.5567(3.5413) | Xent 0.0950(0.0975) | Loss 3.6042(3.5901) | Error 0.0322(0.0334) Steps 1096(1086.57) | Grad Norm 1.5624(1.3956) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 27.5051(27.3277) | Bit/dim 3.5730(3.5429) | Xent 0.0802(0.0957) | Loss 3.6131(3.5908) | Error 0.0244(0.0331) Steps 1054(1085.28) | Grad Norm 1.1848(1.4110) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 27.1880(27.2679) | Bit/dim 3.5289(3.5424) | Xent 0.0945(0.0957) | Loss 3.5761(3.5902) | Error 0.0311(0.0330) Steps 1090(1084.14) | Grad Norm 1.2434(1.4387) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 27.5319(27.3053) | Bit/dim 3.5275(3.5383) | Xent 0.0748(0.0958) | Loss 3.5649(3.5862) | Error 0.0222(0.0324) Steps 1090(1082.68) | Grad Norm 1.0630(1.4054) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 26.5226(27.3238) | Bit/dim 3.5193(3.5381) | Xent 0.0945(0.0966) | Loss 3.5665(3.5864) | Error 0.0356(0.0324) Steps 1060(1083.75) | Grad Norm 1.2788(1.3710) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 126.4108, Epoch Time 1650.9084(1575.0836), Bit/dim 3.5499(best: 3.5500), Xent 0.9808, Loss 4.0403, Error 0.2202(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 26.7474(27.2261) | Bit/dim 3.5165(3.5419) | Xent 0.0753(0.0956) | Loss 3.5541(3.5896) | Error 0.0267(0.0321) Steps 1084(1081.60) | Grad Norm 0.9563(1.3273) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 26.6876(27.1643) | Bit/dim 3.5442(3.5399) | Xent 0.0966(0.0959) | Loss 3.5925(3.5879) | Error 0.0311(0.0324) Steps 1048(1080.16) | Grad Norm 1.7166(1.3512) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 27.5490(27.1060) | Bit/dim 3.5211(3.5359) | Xent 0.1228(0.0971) | Loss 3.5825(3.5845) | Error 0.0356(0.0331) Steps 1060(1081.08) | Grad Norm 1.3970(1.3654) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 26.5755(27.0552) | Bit/dim 3.5374(3.5394) | Xent 0.1080(0.0958) | Loss 3.5914(3.5874) | Error 0.0422(0.0330) Steps 1084(1079.17) | Grad Norm 1.6703(1.3794) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 26.4834(27.0473) | Bit/dim 3.5633(3.5383) | Xent 0.0933(0.0970) | Loss 3.6099(3.5867) | Error 0.0333(0.0335) Steps 1096(1079.61) | Grad Norm 1.0648(1.4533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 126.1316, Epoch Time 1629.0198(1576.7016), Bit/dim 3.5497(best: 3.5499), Xent 1.0023, Loss 4.0509, Error 0.2203(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 27.1048(27.0529) | Bit/dim 3.5145(3.5393) | Xent 0.0740(0.0964) | Loss 3.5515(3.5875) | Error 0.0189(0.0328) Steps 1096(1079.16) | Grad Norm 1.2545(1.4319) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 27.0869(27.0879) | Bit/dim 3.5599(3.5385) | Xent 0.0774(0.0954) | Loss 3.5986(3.5863) | Error 0.0267(0.0322) Steps 1066(1082.84) | Grad Norm 1.6295(1.4263) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 26.4942(27.0857) | Bit/dim 3.5628(3.5405) | Xent 0.1013(0.0955) | Loss 3.6134(3.5882) | Error 0.0400(0.0327) Steps 1078(1080.54) | Grad Norm 1.9068(1.4075) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 26.6555(27.0148) | Bit/dim 3.5171(3.5381) | Xent 0.0947(0.0954) | Loss 3.5644(3.5858) | Error 0.0356(0.0327) Steps 1072(1080.53) | Grad Norm 1.2819(1.4092) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 26.8122(26.9526) | Bit/dim 3.5253(3.5369) | Xent 0.1046(0.0958) | Loss 3.5776(3.5848) | Error 0.0400(0.0330) Steps 1084(1079.90) | Grad Norm 1.6932(1.4233) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 27.5550(26.9876) | Bit/dim 3.5821(3.5402) | Xent 0.0881(0.0957) | Loss 3.6262(3.5881) | Error 0.0289(0.0328) Steps 1102(1079.33) | Grad Norm 1.3312(1.4092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 128.1020, Epoch Time 1630.3725(1578.3118), Bit/dim 3.5503(best: 3.5497), Xent 1.0159, Loss 4.0583, Error 0.2238(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 27.2679(26.9846) | Bit/dim 3.5591(3.5413) | Xent 0.0956(0.0951) | Loss 3.6069(3.5888) | Error 0.0344(0.0326) Steps 1084(1081.29) | Grad Norm 1.4684(1.4244) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 26.6130(27.0188) | Bit/dim 3.5407(3.5405) | Xent 0.0844(0.0952) | Loss 3.5830(3.5881) | Error 0.0300(0.0324) Steps 1078(1080.02) | Grad Norm 1.1977(1.4090) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 26.9119(27.0826) | Bit/dim 3.5566(3.5431) | Xent 0.0926(0.0929) | Loss 3.6029(3.5895) | Error 0.0278(0.0318) Steps 1090(1079.83) | Grad Norm 1.5755(1.3853) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 27.2189(27.1216) | Bit/dim 3.5199(3.5403) | Xent 0.0986(0.0934) | Loss 3.5692(3.5870) | Error 0.0300(0.0315) Steps 1090(1082.00) | Grad Norm 1.6432(1.3825) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 27.0078(27.0452) | Bit/dim 3.5314(3.5387) | Xent 0.0936(0.0945) | Loss 3.5782(3.5859) | Error 0.0344(0.0323) Steps 1090(1082.99) | Grad Norm 2.2694(1.4523) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 127.3438, Epoch Time 1634.8753(1580.0087), Bit/dim 3.5500(best: 3.5497), Xent 1.0172, Loss 4.0586, Error 0.2208(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 27.4470(27.0614) | Bit/dim 3.5537(3.5392) | Xent 0.0823(0.0922) | Loss 3.5949(3.5853) | Error 0.0322(0.0316) Steps 1102(1082.28) | Grad Norm 1.7042(1.4976) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 26.8443(27.1728) | Bit/dim 3.5162(3.5380) | Xent 0.1176(0.0943) | Loss 3.5749(3.5851) | Error 0.0422(0.0323) Steps 1084(1083.75) | Grad Norm 1.7042(1.5095) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 26.4813(27.1687) | Bit/dim 3.5500(3.5376) | Xent 0.0974(0.0940) | Loss 3.5987(3.5846) | Error 0.0311(0.0320) Steps 1066(1083.46) | Grad Norm 1.6943(1.4886) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 27.2841(27.1797) | Bit/dim 3.5417(3.5375) | Xent 0.0861(0.0938) | Loss 3.5847(3.5843) | Error 0.0289(0.0317) Steps 1096(1083.51) | Grad Norm 1.2852(1.4754) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 27.3701(27.1976) | Bit/dim 3.5869(3.5394) | Xent 0.0978(0.0922) | Loss 3.6358(3.5854) | Error 0.0400(0.0316) Steps 1078(1081.98) | Grad Norm 1.5178(1.4669) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 26.9692(27.2417) | Bit/dim 3.5358(3.5397) | Xent 0.0918(0.0927) | Loss 3.5817(3.5861) | Error 0.0300(0.0318) Steps 1078(1083.39) | Grad Norm 1.1756(1.4470) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 126.2830, Epoch Time 1645.6857(1581.9790), Bit/dim 3.5495(best: 3.5497), Xent 1.0191, Loss 4.0591, Error 0.2215(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 27.0417(27.1417) | Bit/dim 3.5321(3.5390) | Xent 0.1059(0.0928) | Loss 3.5851(3.5854) | Error 0.0333(0.0313) Steps 1078(1081.38) | Grad Norm 1.4567(1.4306) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 26.4185(27.1074) | Bit/dim 3.5318(3.5378) | Xent 0.1066(0.0955) | Loss 3.5850(3.5856) | Error 0.0411(0.0325) Steps 1066(1081.13) | Grad Norm 1.4115(1.4557) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 27.2165(27.1472) | Bit/dim 3.5148(3.5387) | Xent 0.0780(0.0950) | Loss 3.5539(3.5862) | Error 0.0267(0.0322) Steps 1120(1082.83) | Grad Norm 1.6196(1.4837) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 26.5037(27.1154) | Bit/dim 3.5427(3.5394) | Xent 0.0882(0.0945) | Loss 3.5868(3.5866) | Error 0.0278(0.0323) Steps 1066(1081.65) | Grad Norm 1.7188(1.4867) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 27.1823(27.0825) | Bit/dim 3.5206(3.5399) | Xent 0.1110(0.0939) | Loss 3.5761(3.5868) | Error 0.0322(0.0321) Steps 1066(1082.02) | Grad Norm 1.4952(1.4750) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 126.3869, Epoch Time 1633.2316(1583.5166), Bit/dim 3.5490(best: 3.5495), Xent 1.0226, Loss 4.0603, Error 0.2214(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 27.0169(27.1754) | Bit/dim 3.5633(3.5388) | Xent 0.0739(0.0921) | Loss 3.6003(3.5849) | Error 0.0244(0.0316) Steps 1102(1082.00) | Grad Norm 1.3611(1.4506) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 27.1192(27.1461) | Bit/dim 3.5292(3.5397) | Xent 0.1082(0.0922) | Loss 3.5833(3.5858) | Error 0.0433(0.0322) Steps 1102(1083.09) | Grad Norm 1.4190(1.4311) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 26.8165(27.1884) | Bit/dim 3.5271(3.5397) | Xent 0.0762(0.0916) | Loss 3.5652(3.5855) | Error 0.0311(0.0320) Steps 1084(1084.33) | Grad Norm 1.0528(1.3908) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 28.1941(27.1583) | Bit/dim 3.5515(3.5393) | Xent 0.1081(0.0910) | Loss 3.6055(3.5849) | Error 0.0344(0.0315) Steps 1084(1083.31) | Grad Norm 1.4753(1.3914) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 26.8440(27.1635) | Bit/dim 3.5511(3.5394) | Xent 0.1093(0.0930) | Loss 3.6057(3.5859) | Error 0.0444(0.0324) Steps 1090(1083.04) | Grad Norm 2.4435(1.4499) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 27.2214(27.2018) | Bit/dim 3.5547(3.5378) | Xent 0.0907(0.0938) | Loss 3.6000(3.5847) | Error 0.0300(0.0325) Steps 1096(1084.55) | Grad Norm 1.1085(1.5222) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 127.5950, Epoch Time 1642.3913(1585.2828), Bit/dim 3.5476(best: 3.5490), Xent 1.0065, Loss 4.0508, Error 0.2229(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 26.6119(27.1420) | Bit/dim 3.5559(3.5386) | Xent 0.0946(0.0918) | Loss 3.6032(3.5846) | Error 0.0289(0.0316) Steps 1078(1082.70) | Grad Norm 1.3649(1.5006) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 26.6235(27.0941) | Bit/dim 3.5366(3.5378) | Xent 0.1004(0.0928) | Loss 3.5868(3.5842) | Error 0.0356(0.0320) Steps 1054(1082.10) | Grad Norm 1.2495(1.5002) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 26.7504(27.0370) | Bit/dim 3.5590(3.5381) | Xent 0.0879(0.0922) | Loss 3.6030(3.5843) | Error 0.0300(0.0318) Steps 1060(1081.11) | Grad Norm 1.2167(1.4704) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 27.0855(27.0598) | Bit/dim 3.5510(3.5378) | Xent 0.1061(0.0927) | Loss 3.6040(3.5842) | Error 0.0389(0.0322) Steps 1084(1082.46) | Grad Norm 1.4969(1.4566) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 27.2046(27.0884) | Bit/dim 3.5546(3.5376) | Xent 0.1060(0.0934) | Loss 3.6077(3.5843) | Error 0.0444(0.0330) Steps 1084(1083.12) | Grad Norm 2.6459(1.5043) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 126.0099, Epoch Time 1632.7273(1586.7061), Bit/dim 3.5491(best: 3.5476), Xent 1.0200, Loss 4.0591, Error 0.2210(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 26.9862(27.1027) | Bit/dim 3.5437(3.5382) | Xent 0.0733(0.0946) | Loss 3.5803(3.5855) | Error 0.0256(0.0333) Steps 1096(1083.76) | Grad Norm 1.5025(1.5932) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 27.0267(27.1396) | Bit/dim 3.5388(3.5375) | Xent 0.1027(0.0935) | Loss 3.5901(3.5843) | Error 0.0400(0.0331) Steps 1102(1084.41) | Grad Norm 1.6683(1.5888) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 26.5743(27.1750) | Bit/dim 3.5126(3.5381) | Xent 0.0820(0.0938) | Loss 3.5536(3.5850) | Error 0.0278(0.0329) Steps 1090(1084.36) | Grad Norm 1.4359(1.6150) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 26.4015(27.1832) | Bit/dim 3.5309(3.5399) | Xent 0.0867(0.0920) | Loss 3.5743(3.5859) | Error 0.0233(0.0322) Steps 1090(1086.09) | Grad Norm 1.6095(1.5999) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 27.5132(27.2543) | Bit/dim 3.5334(3.5383) | Xent 0.0759(0.0925) | Loss 3.5713(3.5845) | Error 0.0300(0.0325) Steps 1066(1082.80) | Grad Norm 1.3342(1.5554) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 27.6212(27.2567) | Bit/dim 3.5353(3.5376) | Xent 0.0838(0.0923) | Loss 3.5772(3.5837) | Error 0.0333(0.0320) Steps 1102(1084.69) | Grad Norm 1.7021(1.5058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 127.2790, Epoch Time 1644.3448(1588.4353), Bit/dim 3.5474(best: 3.5476), Xent 1.0293, Loss 4.0620, Error 0.2242(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 26.7991(27.2535) | Bit/dim 3.5292(3.5371) | Xent 0.0940(0.0933) | Loss 3.5762(3.5837) | Error 0.0278(0.0321) Steps 1066(1085.82) | Grad Norm 1.1135(1.4956) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 26.8237(27.2616) | Bit/dim 3.5178(3.5365) | Xent 0.0877(0.0917) | Loss 3.5617(3.5823) | Error 0.0300(0.0315) Steps 1078(1086.83) | Grad Norm 1.3880(1.5000) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 27.7390(27.2182) | Bit/dim 3.5501(3.5384) | Xent 0.0893(0.0916) | Loss 3.5947(3.5842) | Error 0.0289(0.0314) Steps 1072(1086.33) | Grad Norm 1.3348(1.4999) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 27.5347(27.2460) | Bit/dim 3.5285(3.5359) | Xent 0.0984(0.0922) | Loss 3.5777(3.5820) | Error 0.0278(0.0316) Steps 1072(1083.84) | Grad Norm 1.8905(1.5280) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 26.9070(27.2696) | Bit/dim 3.5700(3.5379) | Xent 0.0903(0.0913) | Loss 3.6151(3.5836) | Error 0.0278(0.0312) Steps 1078(1084.43) | Grad Norm 1.3662(1.5425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 127.5700, Epoch Time 1645.6361(1590.1513), Bit/dim 3.5485(best: 3.5474), Xent 1.0351, Loss 4.0660, Error 0.2242(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 28.2655(27.2784) | Bit/dim 3.5630(3.5399) | Xent 0.0719(0.0907) | Loss 3.5990(3.5852) | Error 0.0189(0.0306) Steps 1120(1085.15) | Grad Norm 0.9749(1.5479) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 26.9284(27.2776) | Bit/dim 3.5196(3.5339) | Xent 0.0704(0.0921) | Loss 3.5547(3.5799) | Error 0.0267(0.0313) Steps 1072(1083.11) | Grad Norm 1.3890(1.5787) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 26.7582(27.2574) | Bit/dim 3.5183(3.5361) | Xent 0.1052(0.0909) | Loss 3.5709(3.5815) | Error 0.0378(0.0308) Steps 1084(1083.21) | Grad Norm 2.1153(1.6236) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 27.7477(27.2692) | Bit/dim 3.5231(3.5388) | Xent 0.0853(0.0891) | Loss 3.5658(3.5833) | Error 0.0322(0.0302) Steps 1078(1083.07) | Grad Norm 1.2166(1.6282) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 27.8513(27.2577) | Bit/dim 3.5148(3.5392) | Xent 0.0976(0.0901) | Loss 3.5636(3.5843) | Error 0.0344(0.0304) Steps 1090(1084.18) | Grad Norm 1.7767(1.5874) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 26.8206(27.2057) | Bit/dim 3.5273(3.5401) | Xent 0.0798(0.0908) | Loss 3.5672(3.5855) | Error 0.0278(0.0311) Steps 1066(1084.92) | Grad Norm 1.1190(1.5760) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 127.4891, Epoch Time 1642.7443(1591.7291), Bit/dim 3.5457(best: 3.5474), Xent 1.0256, Loss 4.0585, Error 0.2229(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 27.3108(27.1755) | Bit/dim 3.5392(3.5385) | Xent 0.0886(0.0891) | Loss 3.5835(3.5831) | Error 0.0378(0.0308) Steps 1084(1084.24) | Grad Norm 1.4379(1.5496) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 27.4632(27.1257) | Bit/dim 3.5485(3.5374) | Xent 0.0755(0.0890) | Loss 3.5863(3.5819) | Error 0.0233(0.0311) Steps 1096(1083.05) | Grad Norm 1.4019(1.5377) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 26.9962(27.1386) | Bit/dim 3.5351(3.5393) | Xent 0.1080(0.0905) | Loss 3.5890(3.5845) | Error 0.0389(0.0313) Steps 1078(1084.80) | Grad Norm 1.5299(1.5476) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 27.6101(27.1557) | Bit/dim 3.5557(3.5384) | Xent 0.0801(0.0912) | Loss 3.5958(3.5840) | Error 0.0289(0.0317) Steps 1090(1084.59) | Grad Norm 1.1032(1.5459) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 27.0299(27.2100) | Bit/dim 3.5616(3.5400) | Xent 0.1106(0.0935) | Loss 3.6169(3.5868) | Error 0.0433(0.0323) Steps 1090(1085.93) | Grad Norm 2.2972(1.5543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 126.8069, Epoch Time 1638.8892(1593.1439), Bit/dim 3.5479(best: 3.5457), Xent 1.0356, Loss 4.0658, Error 0.2211(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 26.3388(27.1418) | Bit/dim 3.5418(3.5352) | Xent 0.0979(0.0933) | Loss 3.5907(3.5818) | Error 0.0333(0.0318) Steps 1108(1085.59) | Grad Norm 2.1925(1.6078) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 26.5473(27.1848) | Bit/dim 3.5569(3.5369) | Xent 0.0760(0.0913) | Loss 3.5949(3.5826) | Error 0.0244(0.0311) Steps 1090(1085.33) | Grad Norm 1.8665(1.6035) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 26.9198(27.2074) | Bit/dim 3.5377(3.5362) | Xent 0.0937(0.0899) | Loss 3.5845(3.5812) | Error 0.0300(0.0306) Steps 1102(1085.80) | Grad Norm 1.3445(1.5851) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 26.4828(27.1834) | Bit/dim 3.5358(3.5393) | Xent 0.0929(0.0899) | Loss 3.5822(3.5842) | Error 0.0289(0.0308) Steps 1072(1084.94) | Grad Norm 1.6172(1.5720) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 27.3669(27.2519) | Bit/dim 3.5734(3.5384) | Xent 0.1077(0.0908) | Loss 3.6272(3.5838) | Error 0.0378(0.0315) Steps 1066(1085.51) | Grad Norm 1.6567(1.5662) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 26.9952(27.2860) | Bit/dim 3.5421(3.5383) | Xent 0.1293(0.0920) | Loss 3.6068(3.5843) | Error 0.0400(0.0322) Steps 1078(1086.73) | Grad Norm 1.9610(1.5953) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 126.9318, Epoch Time 1644.5727(1594.6868), Bit/dim 3.5460(best: 3.5457), Xent 1.0342, Loss 4.0631, Error 0.2248(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 27.3526(27.3040) | Bit/dim 3.5628(3.5404) | Xent 0.0941(0.0907) | Loss 3.6098(3.5857) | Error 0.0322(0.0317) Steps 1078(1086.34) | Grad Norm 1.3625(1.5771) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 27.4445(27.3016) | Bit/dim 3.5217(3.5382) | Xent 0.0999(0.0907) | Loss 3.5716(3.5836) | Error 0.0300(0.0317) Steps 1090(1084.94) | Grad Norm 2.3080(1.5762) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 26.7877(27.3395) | Bit/dim 3.5528(3.5391) | Xent 0.0729(0.0881) | Loss 3.5893(3.5832) | Error 0.0200(0.0305) Steps 1090(1086.48) | Grad Norm 1.3264(1.5255) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 27.7530(27.2828) | Bit/dim 3.5216(3.5356) | Xent 0.0854(0.0888) | Loss 3.5643(3.5800) | Error 0.0244(0.0302) Steps 1102(1088.12) | Grad Norm 1.5362(1.4901) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 26.5414(27.2453) | Bit/dim 3.5340(3.5377) | Xent 0.0947(0.0909) | Loss 3.5813(3.5831) | Error 0.0333(0.0312) Steps 1072(1086.49) | Grad Norm 1.2017(1.5273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 128.3223, Epoch Time 1646.0350(1596.2272), Bit/dim 3.5461(best: 3.5457), Xent 1.0264, Loss 4.0593, Error 0.2255(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 26.7108(27.1606) | Bit/dim 3.5449(3.5368) | Xent 0.0983(0.0916) | Loss 3.5941(3.5826) | Error 0.0278(0.0312) Steps 1102(1085.11) | Grad Norm 1.5905(1.5232) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 27.7026(27.2140) | Bit/dim 3.5518(3.5385) | Xent 0.0701(0.0900) | Loss 3.5869(3.5835) | Error 0.0211(0.0303) Steps 1096(1087.06) | Grad Norm 1.0640(1.4822) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 27.4241(27.2278) | Bit/dim 3.5301(3.5356) | Xent 0.0920(0.0895) | Loss 3.5761(3.5803) | Error 0.0344(0.0303) Steps 1096(1087.37) | Grad Norm 1.6618(1.4865) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 27.3369(27.2960) | Bit/dim 3.5150(3.5354) | Xent 0.1011(0.0895) | Loss 3.5656(3.5801) | Error 0.0356(0.0307) Steps 1066(1086.50) | Grad Norm 1.8607(1.5372) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 28.0231(27.2665) | Bit/dim 3.5123(3.5363) | Xent 0.0997(0.0873) | Loss 3.5622(3.5799) | Error 0.0367(0.0299) Steps 1108(1087.34) | Grad Norm 2.0585(1.5544) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 26.3885(27.2240) | Bit/dim 3.5849(3.5368) | Xent 0.0932(0.0868) | Loss 3.6315(3.5802) | Error 0.0322(0.0300) Steps 1078(1087.56) | Grad Norm 1.1664(1.5105) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 127.8544, Epoch Time 1644.3725(1597.6716), Bit/dim 3.5467(best: 3.5457), Xent 1.0426, Loss 4.0680, Error 0.2238(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 26.9821(27.1971) | Bit/dim 3.5460(3.5351) | Xent 0.1097(0.0890) | Loss 3.6009(3.5796) | Error 0.0344(0.0305) Steps 1096(1086.97) | Grad Norm 1.4257(1.4883) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 27.1438(27.1632) | Bit/dim 3.5186(3.5348) | Xent 0.0909(0.0881) | Loss 3.5640(3.5788) | Error 0.0333(0.0302) Steps 1078(1086.06) | Grad Norm 1.3109(1.4880) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 28.1824(27.2686) | Bit/dim 3.5141(3.5366) | Xent 0.0994(0.0876) | Loss 3.5638(3.5804) | Error 0.0311(0.0299) Steps 1096(1085.85) | Grad Norm 1.3668(1.5506) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 26.6656(27.2348) | Bit/dim 3.5266(3.5337) | Xent 0.1147(0.0881) | Loss 3.5839(3.5778) | Error 0.0389(0.0300) Steps 1078(1086.16) | Grad Norm 1.5907(1.5546) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 27.4305(27.3580) | Bit/dim 3.5314(3.5342) | Xent 0.0875(0.0877) | Loss 3.5751(3.5781) | Error 0.0311(0.0296) Steps 1090(1087.44) | Grad Norm 1.9529(1.5823) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 127.1285, Epoch Time 1647.2966(1599.1603), Bit/dim 3.5486(best: 3.5457), Xent 1.0571, Loss 4.0771, Error 0.2239(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.1853(27.3673) | Bit/dim 3.5473(3.5384) | Xent 0.0831(0.0874) | Loss 3.5889(3.5822) | Error 0.0244(0.0293) Steps 1090(1085.39) | Grad Norm 1.3097(1.5872) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 27.1280(27.3574) | Bit/dim 3.5842(3.5397) | Xent 0.0886(0.0878) | Loss 3.6286(3.5836) | Error 0.0289(0.0291) Steps 1066(1084.72) | Grad Norm 1.1914(1.5254) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 26.7335(27.3204) | Bit/dim 3.5406(3.5398) | Xent 0.0790(0.0877) | Loss 3.5801(3.5837) | Error 0.0278(0.0295) Steps 1096(1083.97) | Grad Norm 1.4786(1.5408) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 27.0863(27.3244) | Bit/dim 3.5402(3.5410) | Xent 0.0885(0.0892) | Loss 3.5845(3.5856) | Error 0.0300(0.0303) Steps 1090(1084.47) | Grad Norm 1.6589(1.5424) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 28.3442(27.3509) | Bit/dim 3.5037(3.5377) | Xent 0.0902(0.0891) | Loss 3.5488(3.5822) | Error 0.0333(0.0303) Steps 1084(1084.26) | Grad Norm 1.1947(1.5238) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 27.6387(27.4200) | Bit/dim 3.5469(3.5352) | Xent 0.0727(0.0879) | Loss 3.5833(3.5791) | Error 0.0233(0.0297) Steps 1096(1087.52) | Grad Norm 1.1925(1.5300) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 127.6733, Epoch Time 1652.8784(1600.7719), Bit/dim 3.5466(best: 3.5457), Xent 1.0602, Loss 4.0767, Error 0.2246(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 27.1118(27.4287) | Bit/dim 3.5394(3.5375) | Xent 0.0965(0.0872) | Loss 3.5876(3.5811) | Error 0.0356(0.0294) Steps 1096(1088.46) | Grad Norm 1.3303(1.5112) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 27.7886(27.4088) | Bit/dim 3.5129(3.5378) | Xent 0.0986(0.0875) | Loss 3.5622(3.5816) | Error 0.0333(0.0296) Steps 1090(1086.90) | Grad Norm 1.3313(1.5041) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 26.7336(27.4226) | Bit/dim 3.5343(3.5332) | Xent 0.1009(0.0875) | Loss 3.5847(3.5770) | Error 0.0300(0.0295) Steps 1066(1084.72) | Grad Norm 1.8619(1.5689) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 27.2248(27.3758) | Bit/dim 3.5379(3.5349) | Xent 0.0981(0.0872) | Loss 3.5870(3.5785) | Error 0.0378(0.0296) Steps 1084(1083.08) | Grad Norm 1.5008(1.5370) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 27.5419(27.3836) | Bit/dim 3.5135(3.5356) | Xent 0.1144(0.0873) | Loss 3.5707(3.5793) | Error 0.0400(0.0295) Steps 1090(1083.95) | Grad Norm 2.3395(1.5247) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 127.4567, Epoch Time 1651.4936(1602.2935), Bit/dim 3.5459(best: 3.5457), Xent 1.0584, Loss 4.0751, Error 0.2241(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 27.8565(27.3775) | Bit/dim 3.5371(3.5370) | Xent 0.0903(0.0878) | Loss 3.5823(3.5808) | Error 0.0333(0.0297) Steps 1066(1084.13) | Grad Norm 1.3116(1.5583) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 27.4723(27.3858) | Bit/dim 3.5511(3.5371) | Xent 0.0872(0.0859) | Loss 3.5947(3.5801) | Error 0.0300(0.0292) Steps 1084(1083.43) | Grad Norm 1.3465(1.5370) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 26.6874(27.3763) | Bit/dim 3.5431(3.5361) | Xent 0.0845(0.0862) | Loss 3.5854(3.5792) | Error 0.0256(0.0299) Steps 1072(1085.04) | Grad Norm 1.5264(1.5454) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 27.5046(27.4022) | Bit/dim 3.4949(3.5354) | Xent 0.0874(0.0842) | Loss 3.5386(3.5775) | Error 0.0311(0.0296) Steps 1096(1084.83) | Grad Norm 1.4954(1.5603) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 27.0447(27.3694) | Bit/dim 3.5582(3.5360) | Xent 0.1043(0.0846) | Loss 3.6104(3.5783) | Error 0.0311(0.0293) Steps 1084(1085.36) | Grad Norm 1.3102(1.5585) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 27.9761(27.3877) | Bit/dim 3.5148(3.5357) | Xent 0.0788(0.0849) | Loss 3.5542(3.5781) | Error 0.0267(0.0291) Steps 1072(1083.25) | Grad Norm 1.8243(1.5845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 127.3125, Epoch Time 1653.3571(1603.8254), Bit/dim 3.5475(best: 3.5457), Xent 1.0766, Loss 4.0857, Error 0.2274(best: 0.2189)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run3/epoch_250_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
