{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4900 | Time 16.7010(17.6460) | Bit/dim 3.6507(3.6841) | Xent 0.6361(0.6743) | Loss 8.4584(9.3368) | Error 0.2167(0.2418) Steps 0(0.00) | Grad Norm 8.5651(8.1940) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.8063(17.4505) | Bit/dim 3.6661(3.6822) | Xent 0.6476(0.6708) | Loss 8.6582(9.1432) | Error 0.2278(0.2392) Steps 0(0.00) | Grad Norm 6.1777(7.7785) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 18.0579(17.4244) | Bit/dim 3.6456(3.6814) | Xent 0.6957(0.6643) | Loss 8.5295(9.0003) | Error 0.2411(0.2384) Steps 0(0.00) | Grad Norm 5.9551(7.1761) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 17.1717(17.4538) | Bit/dim 3.6889(3.6798) | Xent 0.7156(0.6590) | Loss 8.6009(8.8965) | Error 0.2522(0.2361) Steps 0(0.00) | Grad Norm 10.8835(7.3201) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 16.9046(17.4834) | Bit/dim 3.6407(3.6782) | Xent 0.6119(0.6559) | Loss 8.6468(8.8260) | Error 0.2411(0.2345) Steps 0(0.00) | Grad Norm 6.8944(7.7023) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.8011(17.5897) | Bit/dim 3.6660(3.6791) | Xent 0.6771(0.6503) | Loss 8.6727(8.7655) | Error 0.2433(0.2327) Steps 0(0.00) | Grad Norm 8.0721(7.5154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 96.5888, Epoch Time 1100.0947(962.1008), Bit/dim 3.6723(best: inf), Xent 0.7291, Loss 4.0368, Error 0.2510(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.2213(17.5169) | Bit/dim 3.6332(3.6731) | Xent 0.6406(0.6468) | Loss 8.3233(9.1932) | Error 0.2167(0.2316) Steps 0(0.00) | Grad Norm 5.8121(7.1725) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 16.6113(17.4299) | Bit/dim 3.6789(3.6753) | Xent 0.6817(0.6465) | Loss 8.6215(9.0328) | Error 0.2478(0.2313) Steps 0(0.00) | Grad Norm 9.0920(7.5643) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.1110(17.4906) | Bit/dim 3.6781(3.6763) | Xent 0.6093(0.6434) | Loss 8.5290(8.9174) | Error 0.2178(0.2306) Steps 0(0.00) | Grad Norm 5.9796(7.4743) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 17.5018(17.4189) | Bit/dim 3.6718(3.6766) | Xent 0.6951(0.6432) | Loss 8.6897(8.8226) | Error 0.2456(0.2312) Steps 0(0.00) | Grad Norm 16.7651(7.5047) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.1458(17.4496) | Bit/dim 3.6819(3.6785) | Xent 0.7277(0.6512) | Loss 8.8313(8.7798) | Error 0.2489(0.2330) Steps 0(0.00) | Grad Norm 8.7639(7.8393) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 91.9396, Epoch Time 1066.5687(965.2348), Bit/dim 3.6764(best: 3.6723), Xent 0.7147, Loss 4.0338, Error 0.2497(best: 0.2510)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 17.9161(17.3794) | Bit/dim 3.6646(3.6778) | Xent 0.6036(0.6511) | Loss 8.5852(9.3460) | Error 0.2122(0.2333) Steps 0(0.00) | Grad Norm 10.4217(8.0402) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.5173(17.5685) | Bit/dim 3.6970(3.6777) | Xent 0.5753(0.6450) | Loss 8.3524(9.1524) | Error 0.2056(0.2309) Steps 0(0.00) | Grad Norm 4.6570(7.4792) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 18.7804(17.6005) | Bit/dim 3.6841(3.6772) | Xent 0.6100(0.6353) | Loss 8.6053(8.9912) | Error 0.2178(0.2269) Steps 0(0.00) | Grad Norm 5.6782(6.9335) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.6093(17.6486) | Bit/dim 3.6804(3.6744) | Xent 0.6982(0.6359) | Loss 8.6255(8.8833) | Error 0.2600(0.2285) Steps 0(0.00) | Grad Norm 9.5583(7.5848) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 18.6009(17.7529) | Bit/dim 3.7041(3.6758) | Xent 0.6478(0.6333) | Loss 8.6968(8.8064) | Error 0.2300(0.2265) Steps 0(0.00) | Grad Norm 8.1690(7.0774) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.7662(17.6770) | Bit/dim 3.6601(3.6728) | Xent 0.6680(0.6423) | Loss 8.6108(8.7647) | Error 0.2400(0.2292) Steps 0(0.00) | Grad Norm 10.9273(7.3107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 91.8299, Epoch Time 1085.6238(968.8465), Bit/dim 3.6745(best: 3.6723), Xent 0.7245, Loss 4.0367, Error 0.2503(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 16.5870(17.7214) | Bit/dim 3.6775(3.6751) | Xent 0.6045(0.6426) | Loss 8.4429(9.2377) | Error 0.2100(0.2287) Steps 0(0.00) | Grad Norm 5.6510(7.9529) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 16.3288(17.6251) | Bit/dim 3.6411(3.6747) | Xent 0.6213(0.6386) | Loss 8.4357(9.0714) | Error 0.2100(0.2290) Steps 0(0.00) | Grad Norm 9.1912(7.8767) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 17.6584(17.6124) | Bit/dim 3.6980(3.6761) | Xent 0.6287(0.6394) | Loss 8.6687(8.9621) | Error 0.2233(0.2289) Steps 0(0.00) | Grad Norm 6.1546(7.9283) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 19.3988(17.8256) | Bit/dim 3.6539(3.6763) | Xent 0.6069(0.6379) | Loss 8.3632(8.8573) | Error 0.2156(0.2288) Steps 0(0.00) | Grad Norm 8.4941(7.8914) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 19.4029(17.8197) | Bit/dim 3.6766(3.6738) | Xent 0.6876(0.6391) | Loss 8.7092(8.7970) | Error 0.2378(0.2296) Steps 0(0.00) | Grad Norm 9.9147(7.9464) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 89.9374, Epoch Time 1085.6374(972.3502), Bit/dim 3.6760(best: 3.6723), Xent 0.7253, Loss 4.0386, Error 0.2537(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 19.3831(17.8778) | Bit/dim 3.6511(3.6667) | Xent 0.5824(0.6362) | Loss 8.6981(9.3026) | Error 0.1867(0.2277) Steps 0(0.00) | Grad Norm 7.3050(7.9631) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 18.3924(17.9195) | Bit/dim 3.7147(3.6723) | Xent 0.6902(0.6304) | Loss 8.5613(9.1051) | Error 0.2378(0.2250) Steps 0(0.00) | Grad Norm 11.6763(7.8583) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 17.9186(17.9269) | Bit/dim 3.6963(3.6731) | Xent 0.6013(0.6250) | Loss 8.6005(8.9674) | Error 0.2111(0.2229) Steps 0(0.00) | Grad Norm 4.4574(7.3250) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 17.5587(17.7869) | Bit/dim 3.6878(3.6718) | Xent 0.6610(0.6238) | Loss 8.5639(8.8538) | Error 0.2322(0.2220) Steps 0(0.00) | Grad Norm 6.3576(7.1743) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 16.3315(17.7604) | Bit/dim 3.6994(3.6719) | Xent 0.6762(0.6247) | Loss 8.8700(8.8030) | Error 0.2444(0.2226) Steps 0(0.00) | Grad Norm 8.0562(7.3567) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 17.5736(17.8458) | Bit/dim 3.6687(3.6705) | Xent 0.6977(0.6284) | Loss 8.6822(8.7478) | Error 0.2578(0.2246) Steps 0(0.00) | Grad Norm 7.4306(7.1295) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 91.1491, Epoch Time 1093.4995(975.9847), Bit/dim 3.6724(best: 3.6723), Xent 0.7266, Loss 4.0357, Error 0.2599(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 18.2461(18.0869) | Bit/dim 3.6519(3.6697) | Xent 0.6259(0.6277) | Loss 8.5632(9.1850) | Error 0.2367(0.2247) Steps 0(0.00) | Grad Norm 10.3489(7.3898) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 17.9098(18.0576) | Bit/dim 3.6561(3.6695) | Xent 0.6413(0.6266) | Loss 8.6103(9.0384) | Error 0.2333(0.2246) Steps 0(0.00) | Grad Norm 6.4075(7.3213) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 19.1707(18.1183) | Bit/dim 3.6590(3.6683) | Xent 0.6020(0.6282) | Loss 8.7598(8.9307) | Error 0.2233(0.2257) Steps 0(0.00) | Grad Norm 8.0064(7.1674) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 19.2282(18.1698) | Bit/dim 3.6855(3.6686) | Xent 0.5944(0.6286) | Loss 8.6437(8.8441) | Error 0.2178(0.2257) Steps 0(0.00) | Grad Norm 6.5507(7.5289) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 18.2000(18.0043) | Bit/dim 3.6352(3.6669) | Xent 0.6553(0.6327) | Loss 8.5403(8.7680) | Error 0.2267(0.2273) Steps 0(0.00) | Grad Norm 8.2008(7.6699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 92.1664, Epoch Time 1107.9828(979.9446), Bit/dim 3.6774(best: 3.6723), Xent 0.7354, Loss 4.0451, Error 0.2597(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 16.4445(17.9432) | Bit/dim 3.6781(3.6713) | Xent 0.5306(0.6293) | Loss 8.2575(9.3129) | Error 0.1933(0.2260) Steps 0(0.00) | Grad Norm 8.5166(7.6327) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 18.5536(18.1250) | Bit/dim 3.6497(3.6718) | Xent 0.6019(0.6239) | Loss 8.4750(9.1349) | Error 0.2211(0.2235) Steps 0(0.00) | Grad Norm 5.9538(7.7296) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 18.4857(18.0135) | Bit/dim 3.6431(3.6675) | Xent 0.5672(0.6173) | Loss 8.5443(8.9764) | Error 0.2078(0.2209) Steps 0(0.00) | Grad Norm 6.4835(7.4116) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.2190(18.2201) | Bit/dim 3.6992(3.6681) | Xent 0.4922(0.6139) | Loss 8.3331(8.8635) | Error 0.1656(0.2180) Steps 0(0.00) | Grad Norm 4.2138(6.9127) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 18.2382(18.1812) | Bit/dim 3.6484(3.6664) | Xent 0.6307(0.6193) | Loss 8.4754(8.7941) | Error 0.2233(0.2197) Steps 0(0.00) | Grad Norm 8.9459(7.1535) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 16.5525(18.1347) | Bit/dim 3.6931(3.6663) | Xent 0.6742(0.6297) | Loss 8.6849(8.7369) | Error 0.2378(0.2236) Steps 0(0.00) | Grad Norm 11.7709(7.7417) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 92.0710, Epoch Time 1110.2564(983.8540), Bit/dim 3.6699(best: 3.6723), Xent 0.7223, Loss 4.0310, Error 0.2536(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.3216(18.1389) | Bit/dim 3.6811(3.6696) | Xent 0.5705(0.6243) | Loss 8.4800(9.1751) | Error 0.2167(0.2225) Steps 0(0.00) | Grad Norm 3.9271(7.5832) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 17.8522(18.0393) | Bit/dim 3.6687(3.6688) | Xent 0.6390(0.6172) | Loss 8.6709(9.0226) | Error 0.2256(0.2207) Steps 0(0.00) | Grad Norm 6.4621(7.1950) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 21.4489(18.2194) | Bit/dim 3.6502(3.6643) | Xent 0.5895(0.6169) | Loss 8.5666(8.9057) | Error 0.2022(0.2200) Steps 0(0.00) | Grad Norm 5.4321(6.9640) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 19.8124(18.2694) | Bit/dim 3.6497(3.6644) | Xent 0.6158(0.6160) | Loss 8.5634(8.8171) | Error 0.2133(0.2198) Steps 0(0.00) | Grad Norm 6.2437(6.7390) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 17.9457(18.2802) | Bit/dim 3.6438(3.6637) | Xent 0.6336(0.6153) | Loss 8.6232(8.7503) | Error 0.2244(0.2184) Steps 0(0.00) | Grad Norm 6.6862(6.6742) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 90.2085, Epoch Time 1112.3030(987.7074), Bit/dim 3.6673(best: 3.6699), Xent 0.6961, Loss 4.0153, Error 0.2417(best: 0.2497)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 17.9009(18.2785) | Bit/dim 3.6509(3.6642) | Xent 0.6602(0.6130) | Loss 8.6365(9.3074) | Error 0.2256(0.2181) Steps 0(0.00) | Grad Norm 13.6571(6.8275) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 20.1330(18.1861) | Bit/dim 3.6656(3.6679) | Xent 0.6319(0.6154) | Loss 8.6919(9.1146) | Error 0.2300(0.2189) Steps 0(0.00) | Grad Norm 5.3387(7.4687) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 18.9481(18.2873) | Bit/dim 3.6452(3.6636) | Xent 0.6447(0.6067) | Loss 8.7320(8.9829) | Error 0.2256(0.2157) Steps 0(0.00) | Grad Norm 6.4524(7.2013) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 17.3908(18.5306) | Bit/dim 3.6744(3.6636) | Xent 0.6442(0.6091) | Loss 8.5080(8.8683) | Error 0.2367(0.2159) Steps 0(0.00) | Grad Norm 9.7234(7.1933) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 18.8973(18.4550) | Bit/dim 3.6419(3.6606) | Xent 0.5956(0.6094) | Loss 8.5718(8.7820) | Error 0.2000(0.2169) Steps 0(0.00) | Grad Norm 9.3096(7.4208) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.9854(18.3182) | Bit/dim 3.6931(3.6655) | Xent 0.6229(0.6081) | Loss 8.6598(8.7299) | Error 0.2167(0.2174) Steps 0(0.00) | Grad Norm 7.8358(7.4099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 92.8705, Epoch Time 1119.3636(991.6571), Bit/dim 3.6696(best: 3.6673), Xent 0.7116, Loss 4.0254, Error 0.2490(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 17.6598(18.2691) | Bit/dim 3.6925(3.6618) | Xent 0.4907(0.5967) | Loss 8.3425(9.1060) | Error 0.1756(0.2137) Steps 0(0.00) | Grad Norm 5.2078(7.4086) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 18.0610(18.3618) | Bit/dim 3.6668(3.6641) | Xent 0.5257(0.5950) | Loss 8.5030(8.9705) | Error 0.1789(0.2135) Steps 0(0.00) | Grad Norm 6.7974(7.5211) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 18.8270(18.4957) | Bit/dim 3.6416(3.6615) | Xent 0.6586(0.5977) | Loss 8.7089(8.8794) | Error 0.2367(0.2141) Steps 0(0.00) | Grad Norm 11.3357(7.1897) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 17.6732(18.5643) | Bit/dim 3.6861(3.6636) | Xent 0.6726(0.6093) | Loss 8.7508(8.8116) | Error 0.2356(0.2181) Steps 0(0.00) | Grad Norm 8.8920(7.9480) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 18.5963(18.5937) | Bit/dim 3.6961(3.6671) | Xent 0.6016(0.6162) | Loss 8.7394(8.7559) | Error 0.2089(0.2187) Steps 0(0.00) | Grad Norm 6.3435(7.7259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 93.3880, Epoch Time 1138.0903(996.0501), Bit/dim 3.6653(best: 3.6673), Xent 0.6844, Loss 4.0075, Error 0.2434(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 17.4283(18.6944) | Bit/dim 3.6606(3.6658) | Xent 0.5348(0.6053) | Loss 8.4938(9.3086) | Error 0.1911(0.2150) Steps 0(0.00) | Grad Norm 6.4702(7.6050) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 18.2697(18.5887) | Bit/dim 3.6367(3.6639) | Xent 0.6005(0.6018) | Loss 8.5647(9.1097) | Error 0.2300(0.2131) Steps 0(0.00) | Grad Norm 9.0785(7.5315) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 18.6089(18.4404) | Bit/dim 3.6552(3.6626) | Xent 0.5905(0.5986) | Loss 8.6758(8.9570) | Error 0.2189(0.2120) Steps 0(0.00) | Grad Norm 5.5960(7.2296) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 18.6455(18.3418) | Bit/dim 3.6673(3.6631) | Xent 0.5877(0.6031) | Loss 8.7259(8.8689) | Error 0.2167(0.2141) Steps 0(0.00) | Grad Norm 11.6513(7.6083) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 18.1235(18.2346) | Bit/dim 3.6576(3.6644) | Xent 0.5663(0.5990) | Loss 8.5174(8.7779) | Error 0.2033(0.2132) Steps 0(0.00) | Grad Norm 4.9958(7.4731) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 17.9220(18.1796) | Bit/dim 3.6272(3.6637) | Xent 0.6188(0.6029) | Loss 8.4726(8.7168) | Error 0.2233(0.2153) Steps 0(0.00) | Grad Norm 6.9127(7.7345) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 92.7742, Epoch Time 1108.7039(999.4297), Bit/dim 3.6679(best: 3.6653), Xent 0.7160, Loss 4.0259, Error 0.2485(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 18.5389(18.3694) | Bit/dim 3.6672(3.6648) | Xent 0.5313(0.5982) | Loss 8.7031(9.1975) | Error 0.1967(0.2143) Steps 0(0.00) | Grad Norm 10.7961(7.6032) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.9145(18.4799) | Bit/dim 3.6432(3.6625) | Xent 0.5648(0.5917) | Loss 8.1539(9.0330) | Error 0.1967(0.2105) Steps 0(0.00) | Grad Norm 5.4996(7.3648) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 18.0510(18.4398) | Bit/dim 3.6456(3.6633) | Xent 0.6485(0.5877) | Loss 8.4091(8.9011) | Error 0.2356(0.2094) Steps 0(0.00) | Grad Norm 9.4205(7.0317) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 18.2281(18.4987) | Bit/dim 3.6505(3.6587) | Xent 0.5536(0.5916) | Loss 8.4861(8.8126) | Error 0.2044(0.2100) Steps 0(0.00) | Grad Norm 8.7136(7.3612) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 18.3719(18.4379) | Bit/dim 3.6424(3.6622) | Xent 0.5961(0.5932) | Loss 8.5754(8.7535) | Error 0.2033(0.2106) Steps 0(0.00) | Grad Norm 7.4934(7.9167) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 93.8359, Epoch Time 1134.4157(1003.4793), Bit/dim 3.6659(best: 3.6653), Xent 0.6993, Loss 4.0156, Error 0.2481(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 17.3386(18.4400) | Bit/dim 3.7007(3.6639) | Xent 0.5842(0.5902) | Loss 8.6971(9.2779) | Error 0.2111(0.2083) Steps 0(0.00) | Grad Norm 5.5656(7.7381) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 18.4240(18.5492) | Bit/dim 3.6118(3.6628) | Xent 0.5772(0.5890) | Loss 8.4776(9.0915) | Error 0.1922(0.2070) Steps 0(0.00) | Grad Norm 8.2221(7.5481) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.7116(18.4123) | Bit/dim 3.6705(3.6620) | Xent 0.5748(0.5892) | Loss 8.3814(8.9467) | Error 0.2189(0.2068) Steps 0(0.00) | Grad Norm 10.0339(7.6587) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.8635(18.6178) | Bit/dim 3.6530(3.6622) | Xent 0.6803(0.6074) | Loss 8.5003(8.8605) | Error 0.2456(0.2132) Steps 0(0.00) | Grad Norm 12.8037(8.4047) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 17.4619(18.5573) | Bit/dim 3.6340(3.6645) | Xent 0.6150(0.6213) | Loss 8.4633(8.7974) | Error 0.2111(0.2183) Steps 0(0.00) | Grad Norm 6.2370(8.7458) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 19.3577(18.6695) | Bit/dim 3.6484(3.6671) | Xent 0.5680(0.6171) | Loss 8.4669(8.7624) | Error 0.2011(0.2175) Steps 0(0.00) | Grad Norm 6.1687(8.3106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 96.4241, Epoch Time 1141.6337(1007.6239), Bit/dim 3.6719(best: 3.6653), Xent 0.7077, Loss 4.0258, Error 0.2450(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 17.9791(18.7447) | Bit/dim 3.6687(3.6677) | Xent 0.5511(0.6112) | Loss 8.5554(9.2217) | Error 0.1967(0.2164) Steps 0(0.00) | Grad Norm 4.7519(8.0991) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 19.4829(18.7278) | Bit/dim 3.6637(3.6668) | Xent 0.6007(0.6023) | Loss 8.6476(9.0340) | Error 0.2211(0.2138) Steps 0(0.00) | Grad Norm 11.1763(8.0727) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 18.1409(18.8373) | Bit/dim 3.6548(3.6659) | Xent 0.6035(0.5988) | Loss 8.3648(8.9191) | Error 0.2178(0.2125) Steps 0(0.00) | Grad Norm 6.1444(8.2591) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 17.5350(18.6997) | Bit/dim 3.6812(3.6669) | Xent 0.6418(0.6030) | Loss 8.7212(8.8300) | Error 0.2267(0.2133) Steps 0(0.00) | Grad Norm 10.9779(8.4561) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.7138(18.7095) | Bit/dim 3.6553(3.6657) | Xent 0.5489(0.5994) | Loss 8.3895(8.7625) | Error 0.2000(0.2136) Steps 0(0.00) | Grad Norm 9.8806(7.9972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 93.0927, Epoch Time 1141.4989(1011.6402), Bit/dim 3.6550(best: 3.6653), Xent 0.6807, Loss 3.9953, Error 0.2404(best: 0.2417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 19.5505(18.7137) | Bit/dim 3.6618(3.6641) | Xent 0.5646(0.5925) | Loss 8.6281(9.3003) | Error 0.2022(0.2116) Steps 0(0.00) | Grad Norm 5.0930(7.3685) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 18.4255(19.0170) | Bit/dim 3.6459(3.6622) | Xent 0.5405(0.5813) | Loss 8.5150(9.1134) | Error 0.1933(0.2078) Steps 0(0.00) | Grad Norm 6.4594(6.9826) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 18.8307(18.9623) | Bit/dim 3.6580(3.6616) | Xent 0.5409(0.5800) | Loss 8.5154(8.9651) | Error 0.1867(0.2064) Steps 0(0.00) | Grad Norm 5.4011(7.0616) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 21.1949(19.0160) | Bit/dim 3.6513(3.6579) | Xent 0.6346(0.5854) | Loss 8.6859(8.8574) | Error 0.2267(0.2080) Steps 0(0.00) | Grad Norm 6.8177(7.2104) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 20.3906(18.9464) | Bit/dim 3.6693(3.6549) | Xent 0.6099(0.5850) | Loss 8.6310(8.7746) | Error 0.2222(0.2087) Steps 0(0.00) | Grad Norm 6.8685(7.1266) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 18.7976(18.9804) | Bit/dim 3.6577(3.6582) | Xent 0.5148(0.5832) | Loss 8.5371(8.7289) | Error 0.1822(0.2084) Steps 0(0.00) | Grad Norm 4.6735(7.0761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 95.1369, Epoch Time 1165.7067(1016.2622), Bit/dim 3.6696(best: 3.6550), Xent 0.6861, Loss 4.0127, Error 0.2352(best: 0.2404)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 19.8589(18.9945) | Bit/dim 3.6697(3.6585) | Xent 0.5730(0.5787) | Loss 8.7912(9.2109) | Error 0.1967(0.2076) Steps 0(0.00) | Grad Norm 6.3128(7.2700) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 21.1664(19.0482) | Bit/dim 3.6629(3.6571) | Xent 0.5488(0.5779) | Loss 8.5124(9.0371) | Error 0.2011(0.2063) Steps 0(0.00) | Grad Norm 6.2939(7.3966) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.1457(19.0962) | Bit/dim 3.6819(3.6584) | Xent 0.5016(0.5735) | Loss 8.5093(8.9184) | Error 0.1867(0.2057) Steps 0(0.00) | Grad Norm 4.5182(7.3377) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 19.4389(19.0321) | Bit/dim 3.6419(3.6575) | Xent 0.5907(0.5740) | Loss 8.6727(8.8219) | Error 0.2167(0.2059) Steps 0(0.00) | Grad Norm 13.8113(7.8789) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 19.1002(18.8313) | Bit/dim 3.6763(3.6587) | Xent 0.5478(0.5723) | Loss 8.6955(8.7586) | Error 0.2033(0.2049) Steps 0(0.00) | Grad Norm 6.4597(7.5561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 94.6029, Epoch Time 1150.1883(1020.2800), Bit/dim 3.6650(best: 3.6550), Xent 0.7097, Loss 4.0198, Error 0.2453(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 19.9550(18.7878) | Bit/dim 3.6648(3.6583) | Xent 0.6291(0.5867) | Loss 8.4488(9.3379) | Error 0.2200(0.2096) Steps 0(0.00) | Grad Norm 9.5785(8.4756) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 19.1529(18.7347) | Bit/dim 3.6459(3.6620) | Xent 0.6375(0.5914) | Loss 8.6921(9.1514) | Error 0.2378(0.2112) Steps 0(0.00) | Grad Norm 11.6734(8.7489) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 17.5983(18.7066) | Bit/dim 3.6552(3.6593) | Xent 0.5676(0.5892) | Loss 8.4304(8.9961) | Error 0.2022(0.2105) Steps 0(0.00) | Grad Norm 5.9674(8.3965) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.4112(18.9680) | Bit/dim 3.6461(3.6593) | Xent 0.5514(0.5846) | Loss 8.4817(8.8969) | Error 0.1889(0.2085) Steps 0(0.00) | Grad Norm 5.8323(8.1194) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 19.6911(18.9848) | Bit/dim 3.6155(3.6575) | Xent 0.5747(0.5814) | Loss 8.2650(8.7777) | Error 0.1956(0.2059) Steps 0(0.00) | Grad Norm 5.5109(7.7342) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 18.5457(18.9493) | Bit/dim 3.6475(3.6560) | Xent 0.5626(0.5767) | Loss 8.4197(8.7214) | Error 0.1933(0.2037) Steps 0(0.00) | Grad Norm 7.7117(7.7020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 94.1945, Epoch Time 1153.4294(1024.2745), Bit/dim 3.6569(best: 3.6550), Xent 0.7065, Loss 4.0101, Error 0.2472(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 18.9342(18.8981) | Bit/dim 3.6518(3.6554) | Xent 0.5569(0.5718) | Loss 8.3440(9.1660) | Error 0.1944(0.2027) Steps 0(0.00) | Grad Norm 4.8502(7.5025) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 18.2538(18.9301) | Bit/dim 3.6222(3.6523) | Xent 0.5514(0.5728) | Loss 8.4061(9.0027) | Error 0.1911(0.2030) Steps 0(0.00) | Grad Norm 6.2796(7.2411) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 20.0196(18.9857) | Bit/dim 3.6767(3.6545) | Xent 0.5605(0.5729) | Loss 8.5753(8.8614) | Error 0.1922(0.2036) Steps 0(0.00) | Grad Norm 6.1255(7.4475) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 19.6061(19.0363) | Bit/dim 3.6687(3.6571) | Xent 0.5724(0.5711) | Loss 8.5326(8.7897) | Error 0.2200(0.2043) Steps 0(0.00) | Grad Norm 9.9557(7.7620) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 19.0370(18.9225) | Bit/dim 3.6593(3.6591) | Xent 0.6050(0.5772) | Loss 8.5877(8.7205) | Error 0.2167(0.2056) Steps 0(0.00) | Grad Norm 10.5734(8.1162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 96.2061, Epoch Time 1156.7310(1028.2481), Bit/dim 3.6564(best: 3.6550), Xent 0.6978, Loss 4.0053, Error 0.2422(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 18.7597(18.9230) | Bit/dim 3.6357(3.6584) | Xent 0.5261(0.5740) | Loss 8.4858(9.2889) | Error 0.1856(0.2042) Steps 0(0.00) | Grad Norm 4.8360(7.7168) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 17.9725(18.8658) | Bit/dim 3.6733(3.6570) | Xent 0.5704(0.5693) | Loss 8.4799(9.1023) | Error 0.1989(0.2017) Steps 0(0.00) | Grad Norm 8.8529(7.6183) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 18.4178(18.8799) | Bit/dim 3.6081(3.6575) | Xent 0.5303(0.5684) | Loss 8.3792(8.9498) | Error 0.1900(0.2014) Steps 0(0.00) | Grad Norm 5.0639(7.5933) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 18.6277(18.8613) | Bit/dim 3.6279(3.6553) | Xent 0.4772(0.5623) | Loss 8.3850(8.8374) | Error 0.1756(0.2001) Steps 0(0.00) | Grad Norm 5.9269(7.3558) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 18.3859(18.9067) | Bit/dim 3.6878(3.6542) | Xent 0.5959(0.5651) | Loss 8.7188(8.7709) | Error 0.2122(0.2015) Steps 0(0.00) | Grad Norm 5.6541(7.3802) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.3314(18.8630) | Bit/dim 3.6634(3.6546) | Xent 0.5938(0.5670) | Loss 8.3981(8.6986) | Error 0.2022(0.2013) Steps 0(0.00) | Grad Norm 9.4624(7.3372) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 95.0204, Epoch Time 1149.1226(1031.8744), Bit/dim 3.6553(best: 3.6550), Xent 0.7266, Loss 4.0186, Error 0.2490(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 18.0981(18.9348) | Bit/dim 3.6794(3.6552) | Xent 0.4950(0.5669) | Loss 8.3121(9.1769) | Error 0.1722(0.2010) Steps 0(0.00) | Grad Norm 3.9292(7.4345) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 19.5056(19.0275) | Bit/dim 3.6420(3.6513) | Xent 0.5387(0.5579) | Loss 8.5334(9.0130) | Error 0.1889(0.1977) Steps 0(0.00) | Grad Norm 7.8137(7.1116) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 18.7241(19.1149) | Bit/dim 3.6579(3.6520) | Xent 0.5894(0.5581) | Loss 8.7125(8.9055) | Error 0.2044(0.1975) Steps 0(0.00) | Grad Norm 12.8813(7.4215) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 19.3871(19.1025) | Bit/dim 3.6774(3.6525) | Xent 0.5593(0.5613) | Loss 8.6486(8.7969) | Error 0.2244(0.1997) Steps 0(0.00) | Grad Norm 7.1160(7.2669) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 18.0396(19.0921) | Bit/dim 3.6375(3.6536) | Xent 0.5275(0.5635) | Loss 8.3092(8.7202) | Error 0.1967(0.2005) Steps 0(0.00) | Grad Norm 5.1165(7.3484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 95.4797, Epoch Time 1172.5214(1036.0938), Bit/dim 3.6550(best: 3.6550), Xent 0.7044, Loss 4.0072, Error 0.2423(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 19.2097(19.2514) | Bit/dim 3.6662(3.6527) | Xent 0.5513(0.5656) | Loss 8.5412(9.2851) | Error 0.2044(0.2012) Steps 0(0.00) | Grad Norm 10.3846(7.6835) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 19.4372(19.2628) | Bit/dim 3.6156(3.6548) | Xent 0.5371(0.5692) | Loss 8.4861(9.0966) | Error 0.1933(0.2031) Steps 0(0.00) | Grad Norm 6.7982(7.9442) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 19.2549(19.0770) | Bit/dim 3.6402(3.6553) | Xent 0.5666(0.5626) | Loss 8.6333(8.9483) | Error 0.2067(0.2002) Steps 0(0.00) | Grad Norm 10.8765(8.0129) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 18.1554(18.9337) | Bit/dim 3.6581(3.6565) | Xent 0.6196(0.5670) | Loss 8.4134(8.8446) | Error 0.2200(0.2011) Steps 0(0.00) | Grad Norm 8.8728(8.2791) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 19.6608(18.9769) | Bit/dim 3.6287(3.6543) | Xent 0.4928(0.5675) | Loss 8.4269(8.7576) | Error 0.1811(0.2010) Steps 0(0.00) | Grad Norm 4.7345(7.9084) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 18.1358(19.1697) | Bit/dim 3.6394(3.6519) | Xent 0.5391(0.5615) | Loss 8.3006(8.6732) | Error 0.1933(0.1986) Steps 0(0.00) | Grad Norm 4.0457(7.5433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 94.2772, Epoch Time 1162.5094(1039.8863), Bit/dim 3.6598(best: 3.6550), Xent 0.6915, Loss 4.0055, Error 0.2370(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 18.8647(19.1780) | Bit/dim 3.6653(3.6561) | Xent 0.5412(0.5640) | Loss 8.5654(9.1785) | Error 0.1978(0.2003) Steps 0(0.00) | Grad Norm 5.1300(7.6892) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 19.2130(19.1784) | Bit/dim 3.6973(3.6554) | Xent 0.5284(0.5581) | Loss 8.5142(9.0081) | Error 0.1989(0.1988) Steps 0(0.00) | Grad Norm 5.9943(7.4021) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.4185(19.1262) | Bit/dim 3.6491(3.6543) | Xent 0.5413(0.5548) | Loss 8.5081(8.8693) | Error 0.1989(0.1984) Steps 0(0.00) | Grad Norm 4.4118(7.2690) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 19.9485(19.1026) | Bit/dim 3.6243(3.6511) | Xent 0.5834(0.5518) | Loss 8.5987(8.7752) | Error 0.2167(0.1967) Steps 0(0.00) | Grad Norm 9.7534(7.1173) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 19.1335(19.0539) | Bit/dim 3.6399(3.6510) | Xent 0.6029(0.5603) | Loss 8.5377(8.7110) | Error 0.2156(0.1998) Steps 0(0.00) | Grad Norm 12.4922(7.7804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 95.1484, Epoch Time 1163.4767(1043.5940), Bit/dim 3.6550(best: 3.6550), Xent 0.6998, Loss 4.0049, Error 0.2418(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 18.2004(18.9546) | Bit/dim 3.6490(3.6498) | Xent 0.5980(0.5589) | Loss 8.4625(9.2727) | Error 0.2078(0.1988) Steps 0(0.00) | Grad Norm 6.7097(7.8587) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 18.7130(18.8433) | Bit/dim 3.6213(3.6465) | Xent 0.5548(0.5678) | Loss 8.5334(9.0756) | Error 0.1967(0.2007) Steps 0(0.00) | Grad Norm 6.4827(7.9136) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 17.6107(18.9409) | Bit/dim 3.6425(3.6468) | Xent 0.5617(0.5705) | Loss 7.9801(8.9029) | Error 0.2078(0.2018) Steps 0(0.00) | Grad Norm 9.3727(8.3145) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 18.6922(18.8926) | Bit/dim 3.6332(3.6488) | Xent 0.5415(0.5616) | Loss 8.3358(8.7954) | Error 0.1911(0.1986) Steps 0(0.00) | Grad Norm 5.3189(7.8329) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 19.7194(19.0170) | Bit/dim 3.6621(3.6508) | Xent 0.5774(0.5595) | Loss 8.6589(8.7363) | Error 0.2033(0.1976) Steps 0(0.00) | Grad Norm 5.6803(7.5179) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 19.4573(19.0597) | Bit/dim 3.6775(3.6510) | Xent 0.6717(0.5627) | Loss 8.6139(8.6687) | Error 0.2400(0.1992) Steps 0(0.00) | Grad Norm 11.8568(7.6561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 95.0748, Epoch Time 1154.5914(1046.9239), Bit/dim 3.6584(best: 3.6550), Xent 0.6903, Loss 4.0036, Error 0.2413(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 20.2779(19.1557) | Bit/dim 3.6010(3.6500) | Xent 0.5167(0.5585) | Loss 8.4948(9.1556) | Error 0.1722(0.1978) Steps 0(0.00) | Grad Norm 6.0004(7.5720) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 18.8071(19.1457) | Bit/dim 3.6281(3.6514) | Xent 0.5571(0.5520) | Loss 8.4588(8.9957) | Error 0.1878(0.1960) Steps 0(0.00) | Grad Norm 10.4374(7.4430) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 18.6797(19.2035) | Bit/dim 3.6413(3.6509) | Xent 0.5513(0.5543) | Loss 8.7400(8.8869) | Error 0.1889(0.1959) Steps 0(0.00) | Grad Norm 10.2517(7.6944) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 18.0021(19.2860) | Bit/dim 3.6762(3.6520) | Xent 0.5712(0.5561) | Loss 8.6698(8.7790) | Error 0.2011(0.1968) Steps 0(0.00) | Grad Norm 6.6870(7.5451) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 18.9478(19.2549) | Bit/dim 3.6345(3.6526) | Xent 0.5552(0.5576) | Loss 8.4487(8.7076) | Error 0.1911(0.1971) Steps 0(0.00) | Grad Norm 4.5702(7.1443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 94.2423, Epoch Time 1174.2306(1050.7431), Bit/dim 3.6592(best: 3.6550), Xent 0.7062, Loss 4.0123, Error 0.2433(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 20.1007(19.2339) | Bit/dim 3.6660(3.6513) | Xent 0.5025(0.5494) | Loss 8.6079(9.2642) | Error 0.1867(0.1954) Steps 0(0.00) | Grad Norm 4.5403(7.0663) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 18.2804(19.2302) | Bit/dim 3.6259(3.6521) | Xent 0.5957(0.5481) | Loss 8.5343(9.0834) | Error 0.2189(0.1960) Steps 0(0.00) | Grad Norm 9.0262(7.2893) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 19.0822(19.1417) | Bit/dim 3.6647(3.6505) | Xent 0.5265(0.5440) | Loss 8.6324(8.9205) | Error 0.1922(0.1942) Steps 0(0.00) | Grad Norm 6.9694(7.1612) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 19.3689(19.2172) | Bit/dim 3.6450(3.6497) | Xent 0.4796(0.5403) | Loss 8.5286(8.8158) | Error 0.1689(0.1923) Steps 0(0.00) | Grad Norm 4.3920(6.6751) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.9411(19.3269) | Bit/dim 3.6369(3.6503) | Xent 0.5934(0.5369) | Loss 8.5851(8.7453) | Error 0.2044(0.1910) Steps 0(0.00) | Grad Norm 7.0410(6.4042) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 19.1070(19.3508) | Bit/dim 3.6200(3.6468) | Xent 0.5509(0.5420) | Loss 8.4174(8.6945) | Error 0.2122(0.1933) Steps 0(0.00) | Grad Norm 7.1658(6.5056) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 97.6320, Epoch Time 1178.0955(1054.5637), Bit/dim 3.6547(best: 3.6550), Xent 0.6938, Loss 4.0016, Error 0.2377(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.7765(19.3438) | Bit/dim 3.6393(3.6445) | Xent 0.5590(0.5385) | Loss 8.6384(9.1528) | Error 0.1933(0.1916) Steps 0(0.00) | Grad Norm 6.9422(6.6208) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 19.8424(19.3050) | Bit/dim 3.6391(3.6429) | Xent 0.5393(0.5389) | Loss 8.4973(8.9744) | Error 0.2000(0.1915) Steps 0(0.00) | Grad Norm 4.8756(6.8332) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 19.3038(19.1364) | Bit/dim 3.6447(3.6460) | Xent 0.5628(0.5397) | Loss 8.4927(8.8482) | Error 0.2144(0.1923) Steps 0(0.00) | Grad Norm 9.2777(6.7902) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 19.2550(19.2213) | Bit/dim 3.6500(3.6465) | Xent 0.5628(0.5374) | Loss 8.6467(8.7666) | Error 0.1789(0.1907) Steps 0(0.00) | Grad Norm 7.6806(6.8341) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 19.1844(19.4522) | Bit/dim 3.6349(3.6471) | Xent 0.5419(0.5346) | Loss 8.4855(8.7074) | Error 0.1989(0.1899) Steps 0(0.00) | Grad Norm 7.7470(6.8310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 95.5778, Epoch Time 1177.3583(1058.2475), Bit/dim 3.6474(best: 3.6547), Xent 0.6782, Loss 3.9865, Error 0.2351(best: 0.2352)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 18.9399(19.4984) | Bit/dim 3.5891(3.6460) | Xent 0.5542(0.5334) | Loss 8.2597(9.2431) | Error 0.2078(0.1887) Steps 0(0.00) | Grad Norm 8.7028(6.8068) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 18.2483(19.4881) | Bit/dim 3.6397(3.6458) | Xent 0.4347(0.5307) | Loss 8.2375(9.0571) | Error 0.1467(0.1882) Steps 0(0.00) | Grad Norm 5.3333(6.9946) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 18.7006(19.3903) | Bit/dim 3.6455(3.6454) | Xent 0.5061(0.5265) | Loss 8.4453(8.9018) | Error 0.1689(0.1878) Steps 0(0.00) | Grad Norm 6.3744(6.9640) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 20.3013(19.5742) | Bit/dim 3.6251(3.6454) | Xent 0.5214(0.5296) | Loss 8.5163(8.8007) | Error 0.1767(0.1886) Steps 0(0.00) | Grad Norm 8.0197(7.3413) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 19.7833(19.6591) | Bit/dim 3.6367(3.6481) | Xent 0.5935(0.5382) | Loss 8.5476(8.7502) | Error 0.2178(0.1923) Steps 0(0.00) | Grad Norm 9.1820(7.3497) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 19.2395(19.6006) | Bit/dim 3.6435(3.6496) | Xent 0.5546(0.5382) | Loss 8.4847(8.6810) | Error 0.1911(0.1923) Steps 0(0.00) | Grad Norm 5.8340(7.3834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 95.1980, Epoch Time 1193.2073(1062.2963), Bit/dim 3.6492(best: 3.6474), Xent 0.6741, Loss 3.9863, Error 0.2328(best: 0.2351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 19.2519(19.5660) | Bit/dim 3.6417(3.6489) | Xent 0.5505(0.5390) | Loss 8.4665(9.1651) | Error 0.2078(0.1929) Steps 0(0.00) | Grad Norm 6.3938(7.2455) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.3124(19.7393) | Bit/dim 3.6406(3.6488) | Xent 0.5176(0.5425) | Loss 8.6433(9.0134) | Error 0.1811(0.1949) Steps 0(0.00) | Grad Norm 6.5407(7.6296) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 18.9937(19.5968) | Bit/dim 3.6633(3.6455) | Xent 0.5359(0.5396) | Loss 8.5851(8.8742) | Error 0.1722(0.1927) Steps 0(0.00) | Grad Norm 10.4677(7.6438) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 17.5753(19.8035) | Bit/dim 3.6377(3.6457) | Xent 0.5324(0.5345) | Loss 8.3326(8.7854) | Error 0.2011(0.1920) Steps 0(0.00) | Grad Norm 7.4316(7.3051) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 18.8181(19.7672) | Bit/dim 3.6384(3.6447) | Xent 0.4979(0.5260) | Loss 8.3194(8.6995) | Error 0.1978(0.1884) Steps 0(0.00) | Grad Norm 3.3917(6.8949) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 95.1377, Epoch Time 1197.9694(1066.3665), Bit/dim 3.6429(best: 3.6474), Xent 0.7003, Loss 3.9931, Error 0.2382(best: 0.2328)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 21.5067(19.7297) | Bit/dim 3.6360(3.6448) | Xent 0.5512(0.5257) | Loss 8.5384(9.2908) | Error 0.2189(0.1885) Steps 0(0.00) | Grad Norm 5.3835(6.7920) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 19.1294(19.6357) | Bit/dim 3.6573(3.6460) | Xent 0.5155(0.5211) | Loss 8.5267(9.0858) | Error 0.1933(0.1869) Steps 0(0.00) | Grad Norm 6.1034(7.0132) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 19.2479(19.6608) | Bit/dim 3.6062(3.6449) | Xent 0.5396(0.5159) | Loss 8.4701(8.9207) | Error 0.1911(0.1845) Steps 0(0.00) | Grad Norm 7.4634(6.7355) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 19.7118(19.7243) | Bit/dim 3.6612(3.6444) | Xent 0.4865(0.5164) | Loss 8.3880(8.8181) | Error 0.1644(0.1838) Steps 0(0.00) | Grad Norm 4.6925(6.7552) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 19.6919(19.6948) | Bit/dim 3.6072(3.6423) | Xent 0.5303(0.5196) | Loss 8.3868(8.7292) | Error 0.1844(0.1844) Steps 0(0.00) | Grad Norm 7.2503(6.6997) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 19.4960(19.6073) | Bit/dim 3.6338(3.6433) | Xent 0.4903(0.5214) | Loss 8.5133(8.6603) | Error 0.1711(0.1854) Steps 0(0.00) | Grad Norm 5.5604(6.8231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 96.8030, Epoch Time 1195.9479(1070.2539), Bit/dim 3.6485(best: 3.6429), Xent 0.6889, Loss 3.9930, Error 0.2376(best: 0.2328)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.2681(19.5113) | Bit/dim 3.6467(3.6435) | Xent 0.4685(0.5129) | Loss 8.5362(9.1546) | Error 0.1656(0.1826) Steps 0(0.00) | Grad Norm 6.7191(6.6432) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 18.5166(19.3223) | Bit/dim 3.6372(3.6438) | Xent 0.5260(0.5186) | Loss 8.3939(8.9595) | Error 0.1833(0.1844) Steps 0(0.00) | Grad Norm 9.9197(7.4350) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 19.6485(19.5527) | Bit/dim 3.6654(3.6454) | Xent 0.5337(0.5231) | Loss 8.5496(8.8634) | Error 0.1978(0.1874) Steps 0(0.00) | Grad Norm 6.1592(7.4233) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 20.3579(19.5732) | Bit/dim 3.6481(3.6438) | Xent 0.6089(0.5284) | Loss 8.5827(8.7543) | Error 0.2144(0.1884) Steps 0(0.00) | Grad Norm 8.4544(7.3499) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.7749(19.5556) | Bit/dim 3.6539(3.6445) | Xent 0.5305(0.5342) | Loss 8.4751(8.6879) | Error 0.1900(0.1900) Steps 0(0.00) | Grad Norm 4.6187(7.1090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 94.8798, Epoch Time 1183.3497(1073.6468), Bit/dim 3.6554(best: 3.6429), Xent 0.7153, Loss 4.0130, Error 0.2441(best: 0.2328)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.4368(19.4835) | Bit/dim 3.6654(3.6464) | Xent 0.5546(0.5346) | Loss 8.3917(9.2468) | Error 0.1878(0.1900) Steps 0(0.00) | Grad Norm 8.2049(7.2687) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 19.5415(19.5250) | Bit/dim 3.6573(3.6452) | Xent 0.4950(0.5233) | Loss 8.5191(9.0359) | Error 0.1633(0.1862) Steps 0(0.00) | Grad Norm 6.0349(7.1047) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 20.6394(19.4309) | Bit/dim 3.6494(3.6463) | Xent 0.4879(0.5198) | Loss 8.4596(8.8710) | Error 0.1622(0.1857) Steps 0(0.00) | Grad Norm 4.8366(7.3066) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 20.1700(19.3944) | Bit/dim 3.6618(3.6465) | Xent 0.5231(0.5227) | Loss 8.5628(8.7760) | Error 0.1844(0.1863) Steps 0(0.00) | Grad Norm 8.4679(7.6362) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 20.1494(19.4779) | Bit/dim 3.6434(3.6524) | Xent 0.5504(0.5264) | Loss 8.3959(8.7175) | Error 0.1833(0.1878) Steps 0(0.00) | Grad Norm 8.2467(7.9937) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 21.7431(19.5003) | Bit/dim 3.6180(3.6462) | Xent 0.5121(0.5286) | Loss 8.4996(8.6569) | Error 0.1856(0.1883) Steps 0(0.00) | Grad Norm 8.1259(7.8809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 96.9416, Epoch Time 1183.4897(1076.9421), Bit/dim 3.6522(best: 3.6429), Xent 0.6433, Loss 3.9738, Error 0.2207(best: 0.2328)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 20.1148(19.6137) | Bit/dim 3.6354(3.6468) | Xent 0.5050(0.5154) | Loss 8.3647(9.1253) | Error 0.1833(0.1835) Steps 0(0.00) | Grad Norm 6.6499(7.2061) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 19.1355(19.6408) | Bit/dim 3.6755(3.6490) | Xent 0.5218(0.5131) | Loss 8.3804(8.9641) | Error 0.1833(0.1818) Steps 0(0.00) | Grad Norm 8.6148(7.2611) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 19.0616(19.6416) | Bit/dim 3.6436(3.6470) | Xent 0.5103(0.5139) | Loss 8.3084(8.8312) | Error 0.1800(0.1826) Steps 0(0.00) | Grad Norm 5.3763(7.3891) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 21.8710(19.8390) | Bit/dim 3.6619(3.6466) | Xent 0.4615(0.5119) | Loss 8.5477(8.7505) | Error 0.1600(0.1809) Steps 0(0.00) | Grad Norm 5.9576(7.1236) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 20.0047(19.9127) | Bit/dim 3.6312(3.6448) | Xent 0.5071(0.5112) | Loss 8.4529(8.6912) | Error 0.1811(0.1818) Steps 0(0.00) | Grad Norm 6.1948(6.8671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 97.1920, Epoch Time 1210.6760(1080.9541), Bit/dim 3.6374(best: 3.6429), Xent 0.6753, Loss 3.9750, Error 0.2310(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 19.5431(19.8044) | Bit/dim 3.6624(3.6423) | Xent 0.4383(0.5106) | Loss 8.6857(9.2660) | Error 0.1644(0.1823) Steps 0(0.00) | Grad Norm 7.3242(6.9393) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 20.1109(19.8830) | Bit/dim 3.6360(3.6450) | Xent 0.4799(0.5087) | Loss 8.5704(9.0671) | Error 0.1633(0.1803) Steps 0(0.00) | Grad Norm 6.5061(7.2940) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 19.1263(19.7477) | Bit/dim 3.6444(3.6431) | Xent 0.4960(0.5059) | Loss 8.5037(8.8821) | Error 0.1856(0.1807) Steps 0(0.00) | Grad Norm 4.9789(6.9676) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 20.5427(19.7634) | Bit/dim 3.6251(3.6434) | Xent 0.4919(0.5066) | Loss 8.5612(8.7837) | Error 0.1867(0.1808) Steps 0(0.00) | Grad Norm 6.0916(7.1514) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 20.8359(19.7633) | Bit/dim 3.6651(3.6414) | Xent 0.4799(0.5046) | Loss 8.5047(8.6927) | Error 0.1600(0.1797) Steps 0(0.00) | Grad Norm 9.3577(7.3308) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.8111(19.6511) | Bit/dim 3.6778(3.6425) | Xent 0.5329(0.5065) | Loss 8.5811(8.6196) | Error 0.1911(0.1798) Steps 0(0.00) | Grad Norm 5.6428(6.8992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 97.1313, Epoch Time 1196.1519(1084.4100), Bit/dim 3.6408(best: 3.6374), Xent 0.7188, Loss 4.0002, Error 0.2440(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 19.8533(19.6687) | Bit/dim 3.6379(3.6423) | Xent 0.5310(0.5110) | Loss 8.5910(9.1262) | Error 0.1922(0.1818) Steps 0(0.00) | Grad Norm 8.9608(7.4552) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 18.3442(19.6186) | Bit/dim 3.6493(3.6412) | Xent 0.5671(0.5155) | Loss 8.3457(8.9559) | Error 0.1967(0.1824) Steps 0(0.00) | Grad Norm 9.4613(7.5315) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 21.3655(19.7215) | Bit/dim 3.6423(3.6440) | Xent 0.5296(0.5185) | Loss 8.7244(8.8325) | Error 0.1811(0.1842) Steps 0(0.00) | Grad Norm 10.3211(8.1053) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 19.7487(19.6586) | Bit/dim 3.6290(3.6440) | Xent 0.5170(0.5209) | Loss 8.6890(8.7422) | Error 0.1756(0.1845) Steps 0(0.00) | Grad Norm 5.1967(8.0111) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 19.3480(19.8177) | Bit/dim 3.6213(3.6433) | Xent 0.5018(0.5144) | Loss 8.3619(8.6664) | Error 0.1878(0.1824) Steps 0(0.00) | Grad Norm 6.4614(7.6818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 98.0151, Epoch Time 1201.9749(1087.9370), Bit/dim 3.6414(best: 3.6374), Xent 0.6729, Loss 3.9779, Error 0.2303(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 19.8080(19.8927) | Bit/dim 3.6666(3.6459) | Xent 0.4525(0.5041) | Loss 8.4257(9.2432) | Error 0.1533(0.1787) Steps 0(0.00) | Grad Norm 5.1170(7.0045) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.9134(19.8298) | Bit/dim 3.6622(3.6484) | Xent 0.5569(0.5037) | Loss 8.6554(9.0284) | Error 0.1978(0.1783) Steps 0(0.00) | Grad Norm 10.5031(7.1702) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 21.1354(19.8942) | Bit/dim 3.6417(3.6465) | Xent 0.5436(0.5098) | Loss 8.5224(8.8912) | Error 0.1922(0.1811) Steps 0(0.00) | Grad Norm 7.1480(7.7546) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 21.0735(20.0973) | Bit/dim 3.6423(3.6457) | Xent 0.5447(0.5083) | Loss 8.4982(8.7821) | Error 0.1933(0.1805) Steps 0(0.00) | Grad Norm 12.3634(7.7978) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 19.7516(19.9344) | Bit/dim 3.6128(3.6426) | Xent 0.4773(0.5115) | Loss 8.3611(8.6994) | Error 0.1656(0.1811) Steps 0(0.00) | Grad Norm 6.2776(7.5852) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 19.4010(20.0177) | Bit/dim 3.6159(3.6426) | Xent 0.5403(0.5229) | Loss 8.3781(8.6415) | Error 0.1867(0.1858) Steps 0(0.00) | Grad Norm 9.2699(7.6469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 96.7105, Epoch Time 1219.0973(1091.8718), Bit/dim 3.6457(best: 3.6374), Xent 0.7082, Loss 3.9997, Error 0.2418(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.9113(20.2408) | Bit/dim 3.6116(3.6424) | Xent 0.5218(0.5243) | Loss 8.3919(9.1483) | Error 0.1933(0.1871) Steps 0(0.00) | Grad Norm 7.8408(7.8841) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 19.3736(20.1689) | Bit/dim 3.6233(3.6421) | Xent 0.4811(0.5166) | Loss 8.5020(8.9666) | Error 0.1767(0.1848) Steps 0(0.00) | Grad Norm 4.7558(7.5822) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 19.4122(20.0940) | Bit/dim 3.6492(3.6446) | Xent 0.4807(0.5162) | Loss 8.1489(8.8426) | Error 0.1733(0.1841) Steps 0(0.00) | Grad Norm 6.1807(7.4192) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 21.0314(19.9549) | Bit/dim 3.6338(3.6408) | Xent 0.4229(0.5121) | Loss 8.2446(8.7239) | Error 0.1456(0.1822) Steps 0(0.00) | Grad Norm 4.1837(7.2256) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.6912(19.9159) | Bit/dim 3.6278(3.6402) | Xent 0.4953(0.5062) | Loss 8.1541(8.6443) | Error 0.1744(0.1797) Steps 0(0.00) | Grad Norm 7.5547(6.9089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 98.1086, Epoch Time 1222.6220(1095.7943), Bit/dim 3.6341(best: 3.6374), Xent 0.6731, Loss 3.9707, Error 0.2315(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 19.2663(20.1029) | Bit/dim 3.6480(3.6410) | Xent 0.4906(0.5033) | Loss 8.5102(9.2256) | Error 0.1789(0.1790) Steps 0(0.00) | Grad Norm 7.6112(6.9516) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 20.6119(20.2165) | Bit/dim 3.6480(3.6437) | Xent 0.4541(0.4981) | Loss 8.4600(9.0435) | Error 0.1811(0.1771) Steps 0(0.00) | Grad Norm 6.8294(7.0690) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 19.1152(20.2922) | Bit/dim 3.6659(3.6440) | Xent 0.4862(0.4929) | Loss 8.5960(8.8988) | Error 0.1822(0.1765) Steps 0(0.00) | Grad Norm 9.5815(7.1046) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 20.0576(20.1339) | Bit/dim 3.6028(3.6423) | Xent 0.4773(0.4919) | Loss 8.1581(8.7624) | Error 0.1811(0.1758) Steps 0(0.00) | Grad Norm 6.0901(7.0032) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 21.1019(20.4935) | Bit/dim 3.6132(3.6389) | Xent 0.5040(0.4945) | Loss 8.3882(8.6847) | Error 0.1778(0.1761) Steps 0(0.00) | Grad Norm 6.3284(7.1464) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 21.9362(20.4529) | Bit/dim 3.6446(3.6408) | Xent 0.5532(0.5007) | Loss 8.6975(8.6341) | Error 0.1967(0.1775) Steps 0(0.00) | Grad Norm 9.8296(7.2545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 99.2777, Epoch Time 1244.9377(1100.2686), Bit/dim 3.6424(best: 3.6341), Xent 0.6690, Loss 3.9769, Error 0.2328(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 19.8834(20.4635) | Bit/dim 3.6162(3.6410) | Xent 0.4522(0.4994) | Loss 8.4746(9.1358) | Error 0.1467(0.1760) Steps 0(0.00) | Grad Norm 3.3898(6.9743) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 21.2965(20.3653) | Bit/dim 3.6531(3.6403) | Xent 0.4318(0.4914) | Loss 8.5963(8.9516) | Error 0.1467(0.1732) Steps 0(0.00) | Grad Norm 5.9726(6.7240) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 19.0381(20.3720) | Bit/dim 3.6261(3.6387) | Xent 0.4865(0.4866) | Loss 8.1814(8.8305) | Error 0.1900(0.1727) Steps 0(0.00) | Grad Norm 8.4516(6.4416) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 19.2315(20.3218) | Bit/dim 3.6572(3.6421) | Xent 0.4901(0.4877) | Loss 8.5665(8.7388) | Error 0.1756(0.1730) Steps 0(0.00) | Grad Norm 5.8903(6.6677) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 18.6960(20.2258) | Bit/dim 3.6328(3.6421) | Xent 0.4755(0.4849) | Loss 8.4323(8.6736) | Error 0.1722(0.1730) Steps 0(0.00) | Grad Norm 4.2159(6.4575) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 98.4969, Epoch Time 1226.8770(1104.0669), Bit/dim 3.6412(best: 3.6341), Xent 0.6782, Loss 3.9803, Error 0.2274(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 20.6234(20.2117) | Bit/dim 3.6671(3.6384) | Xent 0.4896(0.4803) | Loss 8.5927(9.2518) | Error 0.1778(0.1716) Steps 0(0.00) | Grad Norm 6.6210(6.3347) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.5437(20.1454) | Bit/dim 3.6351(3.6377) | Xent 0.4720(0.4778) | Loss 8.2713(9.0333) | Error 0.1722(0.1719) Steps 0(0.00) | Grad Norm 7.6262(6.6226) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 20.2364(20.0316) | Bit/dim 3.6142(3.6371) | Xent 0.5112(0.4729) | Loss 8.3957(8.8606) | Error 0.2000(0.1705) Steps 0(0.00) | Grad Norm 7.1579(6.5714) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 21.7581(20.0130) | Bit/dim 3.6810(3.6361) | Xent 0.5683(0.4752) | Loss 8.6798(8.7622) | Error 0.1967(0.1708) Steps 0(0.00) | Grad Norm 11.8822(6.8323) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 20.5024(20.0398) | Bit/dim 3.6709(3.6374) | Xent 0.5067(0.4874) | Loss 8.6964(8.6953) | Error 0.1744(0.1746) Steps 0(0.00) | Grad Norm 7.7925(7.1019) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 19.8130(20.1260) | Bit/dim 3.6252(3.6377) | Xent 0.4941(0.4914) | Loss 8.3380(8.6252) | Error 0.1900(0.1769) Steps 0(0.00) | Grad Norm 6.5315(6.8561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 99.0618, Epoch Time 1221.2100(1107.5812), Bit/dim 3.6403(best: 3.6341), Xent 0.6728, Loss 3.9767, Error 0.2268(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 21.5985(20.4732) | Bit/dim 3.6526(3.6372) | Xent 0.4771(0.4864) | Loss 8.7231(9.1389) | Error 0.1700(0.1750) Steps 0(0.00) | Grad Norm 12.1894(6.7782) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 20.7611(20.5100) | Bit/dim 3.6350(3.6406) | Xent 0.4499(0.4885) | Loss 8.3920(8.9719) | Error 0.1467(0.1750) Steps 0(0.00) | Grad Norm 9.6518(7.3958) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 20.5586(20.4076) | Bit/dim 3.6396(3.6414) | Xent 0.4254(0.4938) | Loss 8.4370(8.8434) | Error 0.1544(0.1761) Steps 0(0.00) | Grad Norm 5.1703(7.4411) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 20.7855(20.5343) | Bit/dim 3.6524(3.6398) | Xent 0.4777(0.4883) | Loss 8.6248(8.7542) | Error 0.1756(0.1737) Steps 0(0.00) | Grad Norm 9.1544(7.1075) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 20.6487(20.6089) | Bit/dim 3.6360(3.6392) | Xent 0.5114(0.4932) | Loss 8.4200(8.7055) | Error 0.1722(0.1743) Steps 0(0.00) | Grad Norm 6.7471(7.2953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 96.1146, Epoch Time 1255.0086(1112.0040), Bit/dim 3.6450(best: 3.6341), Xent 0.7022, Loss 3.9960, Error 0.2334(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 20.6260(20.5035) | Bit/dim 3.6386(3.6381) | Xent 0.4826(0.4834) | Loss 8.4076(9.2286) | Error 0.1633(0.1718) Steps 0(0.00) | Grad Norm 3.9163(7.1346) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 20.2386(20.4944) | Bit/dim 3.5992(3.6346) | Xent 0.5024(0.4794) | Loss 8.5169(9.0117) | Error 0.1789(0.1697) Steps 0(0.00) | Grad Norm 6.7848(6.8895) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 20.0905(20.5247) | Bit/dim 3.6330(3.6340) | Xent 0.4528(0.4771) | Loss 8.4612(8.8721) | Error 0.1533(0.1687) Steps 0(0.00) | Grad Norm 4.9686(6.8888) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 20.0507(20.4043) | Bit/dim 3.6509(3.6335) | Xent 0.4813(0.4828) | Loss 8.4169(8.7523) | Error 0.1722(0.1716) Steps 0(0.00) | Grad Norm 7.7780(6.9728) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 21.4019(20.4387) | Bit/dim 3.6564(3.6347) | Xent 0.5579(0.4820) | Loss 8.7659(8.6623) | Error 0.1944(0.1712) Steps 0(0.00) | Grad Norm 12.7192(6.9212) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 21.4433(20.4632) | Bit/dim 3.6545(3.6401) | Xent 0.4663(0.4974) | Loss 8.5850(8.6558) | Error 0.1489(0.1761) Steps 0(0.00) | Grad Norm 5.5638(7.8032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 98.3913, Epoch Time 1239.1507(1115.8184), Bit/dim 3.6583(best: 3.6341), Xent 0.7112, Loss 4.0139, Error 0.2370(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.8047(20.2570) | Bit/dim 3.6522(3.6403) | Xent 0.4461(0.4918) | Loss 8.4560(9.1365) | Error 0.1567(0.1744) Steps 0(0.00) | Grad Norm 4.0978(7.3261) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 19.5114(20.1548) | Bit/dim 3.6207(3.6400) | Xent 0.4748(0.4873) | Loss 8.3703(8.9460) | Error 0.1589(0.1728) Steps 0(0.00) | Grad Norm 6.9515(7.1753) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 20.2411(20.0913) | Bit/dim 3.6832(3.6430) | Xent 0.4655(0.4839) | Loss 8.6956(8.8143) | Error 0.1700(0.1716) Steps 0(0.00) | Grad Norm 5.4556(6.8281) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 20.3134(20.1574) | Bit/dim 3.6333(3.6383) | Xent 0.5316(0.4833) | Loss 8.6815(8.7233) | Error 0.1900(0.1718) Steps 0(0.00) | Grad Norm 8.3343(6.7753) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 20.3423(20.2080) | Bit/dim 3.6247(3.6370) | Xent 0.4287(0.4744) | Loss 8.4277(8.6499) | Error 0.1433(0.1681) Steps 0(0.00) | Grad Norm 3.6990(6.3485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 97.8330, Epoch Time 1218.2271(1118.8906), Bit/dim 3.6363(best: 3.6341), Xent 0.6804, Loss 3.9765, Error 0.2281(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 21.3437(20.3203) | Bit/dim 3.6206(3.6360) | Xent 0.4594(0.4661) | Loss 8.2462(9.2245) | Error 0.1811(0.1657) Steps 0(0.00) | Grad Norm 5.8489(6.0012) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 19.2788(20.2749) | Bit/dim 3.6579(3.6363) | Xent 0.4267(0.4613) | Loss 8.3535(9.0061) | Error 0.1589(0.1637) Steps 0(0.00) | Grad Norm 7.3983(5.9767) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.3981(20.3236) | Bit/dim 3.6493(3.6363) | Xent 0.5385(0.4626) | Loss 8.5549(8.8725) | Error 0.1822(0.1637) Steps 0(0.00) | Grad Norm 11.4354(6.2630) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 20.6859(20.4893) | Bit/dim 3.6169(3.6350) | Xent 0.4426(0.4604) | Loss 8.5999(8.7614) | Error 0.1600(0.1634) Steps 0(0.00) | Grad Norm 4.3201(6.1283) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 19.6085(20.3923) | Bit/dim 3.6444(3.6356) | Xent 0.4866(0.4662) | Loss 8.4293(8.6888) | Error 0.1778(0.1655) Steps 0(0.00) | Grad Norm 5.0157(6.4467) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 19.4624(20.4895) | Bit/dim 3.6303(3.6337) | Xent 0.5026(0.4762) | Loss 8.5021(8.6494) | Error 0.1689(0.1695) Steps 0(0.00) | Grad Norm 7.9864(6.6331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 98.8668, Epoch Time 1245.5615(1122.6908), Bit/dim 3.6343(best: 3.6341), Xent 0.7148, Loss 3.9916, Error 0.2389(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 19.5600(20.4198) | Bit/dim 3.6534(3.6344) | Xent 0.4474(0.4688) | Loss 8.4718(9.1406) | Error 0.1556(0.1658) Steps 0(0.00) | Grad Norm 5.7287(6.7397) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 20.5588(20.3758) | Bit/dim 3.6468(3.6367) | Xent 0.4783(0.4686) | Loss 8.6295(8.9684) | Error 0.1789(0.1665) Steps 0(0.00) | Grad Norm 8.5678(6.6900) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 19.3450(20.1484) | Bit/dim 3.5948(3.6345) | Xent 0.4476(0.4671) | Loss 8.2959(8.8150) | Error 0.1611(0.1660) Steps 0(0.00) | Grad Norm 6.1825(6.6133) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 20.0336(20.2902) | Bit/dim 3.6152(3.6321) | Xent 0.4448(0.4608) | Loss 8.2867(8.7055) | Error 0.1556(0.1634) Steps 0(0.00) | Grad Norm 5.0856(6.3476) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 20.4124(20.3285) | Bit/dim 3.6152(3.6284) | Xent 0.4977(0.4608) | Loss 8.4512(8.6296) | Error 0.1911(0.1632) Steps 0(0.00) | Grad Norm 6.4791(6.4217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 100.8606, Epoch Time 1228.8177(1125.8746), Bit/dim 3.6344(best: 3.6341), Xent 0.6564, Loss 3.9626, Error 0.2246(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 20.8241(20.2099) | Bit/dim 3.6229(3.6294) | Xent 0.4337(0.4610) | Loss 8.3352(9.2413) | Error 0.1567(0.1633) Steps 0(0.00) | Grad Norm 7.0720(6.6214) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 19.9472(20.1558) | Bit/dim 3.6554(3.6307) | Xent 0.4450(0.4507) | Loss 8.3606(9.0150) | Error 0.1533(0.1600) Steps 0(0.00) | Grad Norm 7.6245(6.2806) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 21.2354(20.3382) | Bit/dim 3.6204(3.6315) | Xent 0.5002(0.4623) | Loss 8.4732(8.8870) | Error 0.1811(0.1646) Steps 0(0.00) | Grad Norm 7.2767(7.0549) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 20.1998(20.2599) | Bit/dim 3.6650(3.6312) | Xent 0.5204(0.4743) | Loss 8.6311(8.7887) | Error 0.1622(0.1677) Steps 0(0.00) | Grad Norm 12.2551(7.6546) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 20.9601(20.2933) | Bit/dim 3.6588(3.6370) | Xent 0.5549(0.4884) | Loss 8.6688(8.7243) | Error 0.2011(0.1728) Steps 0(0.00) | Grad Norm 9.4170(7.8909) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 22.6867(20.4218) | Bit/dim 3.6449(3.6396) | Xent 0.4365(0.4916) | Loss 8.4151(8.6626) | Error 0.1467(0.1740) Steps 0(0.00) | Grad Norm 8.0180(7.9929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 99.3977, Epoch Time 1237.5325(1129.2243), Bit/dim 3.6504(best: 3.6341), Xent 0.6851, Loss 3.9929, Error 0.2294(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 19.7602(20.4592) | Bit/dim 3.6315(3.6394) | Xent 0.4810(0.4869) | Loss 8.4928(9.1590) | Error 0.1689(0.1727) Steps 0(0.00) | Grad Norm 7.1272(7.9035) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 20.3108(20.4370) | Bit/dim 3.6351(3.6392) | Xent 0.5059(0.4864) | Loss 8.5536(8.9753) | Error 0.1733(0.1728) Steps 0(0.00) | Grad Norm 9.5220(7.6754) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 20.2414(20.4465) | Bit/dim 3.6593(3.6399) | Xent 0.4486(0.4758) | Loss 8.5677(8.8374) | Error 0.1589(0.1682) Steps 0(0.00) | Grad Norm 4.6192(7.0944) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 21.0457(20.4300) | Bit/dim 3.6039(3.6384) | Xent 0.4299(0.4708) | Loss 8.3876(8.7391) | Error 0.1589(0.1663) Steps 0(0.00) | Grad Norm 7.3611(6.7108) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 19.9807(20.3654) | Bit/dim 3.6045(3.6353) | Xent 0.4831(0.4636) | Loss 8.4186(8.6449) | Error 0.1622(0.1630) Steps 0(0.00) | Grad Norm 6.0426(6.4367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 99.8116, Epoch Time 1241.5282(1132.5934), Bit/dim 3.6348(best: 3.6341), Xent 0.7158, Loss 3.9926, Error 0.2333(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 21.2868(20.5828) | Bit/dim 3.6765(3.6334) | Xent 0.4702(0.4685) | Loss 8.4353(9.2625) | Error 0.1611(0.1652) Steps 0(0.00) | Grad Norm 14.4565(7.2390) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 20.2831(20.6463) | Bit/dim 3.6462(3.6371) | Xent 0.4840(0.4613) | Loss 8.5152(9.0497) | Error 0.1656(0.1630) Steps 0(0.00) | Grad Norm 6.2293(7.2488) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 20.8627(20.5773) | Bit/dim 3.6370(3.6370) | Xent 0.4558(0.4570) | Loss 8.4506(8.8795) | Error 0.1644(0.1616) Steps 0(0.00) | Grad Norm 4.9813(7.0655) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 21.0357(20.5018) | Bit/dim 3.6398(3.6352) | Xent 0.4986(0.4574) | Loss 8.5415(8.7594) | Error 0.1778(0.1630) Steps 0(0.00) | Grad Norm 11.9442(6.9916) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 21.0653(20.4695) | Bit/dim 3.6299(3.6347) | Xent 0.4542(0.4662) | Loss 8.5264(8.6919) | Error 0.1656(0.1665) Steps 0(0.00) | Grad Norm 8.0829(7.6220) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 20.1596(20.6004) | Bit/dim 3.6246(3.6318) | Xent 0.5133(0.4675) | Loss 8.5678(8.6406) | Error 0.1733(0.1681) Steps 0(0.00) | Grad Norm 11.8356(7.6453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 98.4528, Epoch Time 1251.4988(1136.1606), Bit/dim 3.6364(best: 3.6341), Xent 0.7169, Loss 3.9949, Error 0.2440(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 23.3540(20.6329) | Bit/dim 3.6560(3.6289) | Xent 0.4839(0.4678) | Loss 8.3961(9.1435) | Error 0.1611(0.1679) Steps 0(0.00) | Grad Norm 6.3552(7.3559) | Total Time 0.00(0.00)\n",
      "Iter 7500 | Time 20.6289(20.6500) | Bit/dim 3.6299(3.6294) | Xent 0.4179(0.4658) | Loss 8.4644(8.9550) | Error 0.1589(0.1676) Steps 0(0.00) | Grad Norm 4.1826(7.1644) | Total Time 0.00(0.00)\n",
      "Iter 7510 | Time 21.1128(20.5370) | Bit/dim 3.5943(3.6312) | Xent 0.5083(0.4621) | Loss 8.4859(8.8256) | Error 0.1778(0.1666) Steps 0(0.00) | Grad Norm 8.5091(7.2559) | Total Time 0.00(0.00)\n",
      "Iter 7520 | Time 19.6232(20.5488) | Bit/dim 3.6580(3.6306) | Xent 0.4572(0.4593) | Loss 8.3965(8.7188) | Error 0.1622(0.1645) Steps 0(0.00) | Grad Norm 7.7018(6.8977) | Total Time 0.00(0.00)\n",
      "Iter 7530 | Time 20.4016(20.4945) | Bit/dim 3.6316(3.6307) | Xent 0.4384(0.4614) | Loss 8.5843(8.6338) | Error 0.1656(0.1656) Steps 0(0.00) | Grad Norm 7.3955(7.2088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 99.9086, Epoch Time 1246.3509(1139.4663), Bit/dim 3.6420(best: 3.6341), Xent 0.7324, Loss 4.0082, Error 0.2421(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 20.0111(20.4483) | Bit/dim 3.6075(3.6310) | Xent 0.5380(0.4793) | Loss 8.4297(9.2114) | Error 0.1967(0.1725) Steps 0(0.00) | Grad Norm 7.6592(7.4825) | Total Time 0.00(0.00)\n",
      "Iter 7550 | Time 20.6940(20.4729) | Bit/dim 3.6793(3.6355) | Xent 0.4606(0.4743) | Loss 8.6462(9.0140) | Error 0.1622(0.1702) Steps 0(0.00) | Grad Norm 4.2730(7.0399) | Total Time 0.00(0.00)\n",
      "Iter 7560 | Time 20.4622(20.5989) | Bit/dim 3.6141(3.6338) | Xent 0.4462(0.4663) | Loss 8.3250(8.8554) | Error 0.1600(0.1672) Steps 0(0.00) | Grad Norm 6.8585(6.6784) | Total Time 0.00(0.00)\n",
      "Iter 7570 | Time 21.7741(20.8135) | Bit/dim 3.6442(3.6369) | Xent 0.3982(0.4622) | Loss 8.3627(8.7632) | Error 0.1389(0.1660) Steps 0(0.00) | Grad Norm 3.6322(6.5455) | Total Time 0.00(0.00)\n",
      "Iter 7580 | Time 20.0000(20.7697) | Bit/dim 3.6356(3.6352) | Xent 0.4495(0.4553) | Loss 8.5946(8.6846) | Error 0.1600(0.1630) Steps 0(0.00) | Grad Norm 6.5802(6.2278) | Total Time 0.00(0.00)\n",
      "Iter 7590 | Time 20.6332(20.7262) | Bit/dim 3.6572(3.6317) | Xent 0.4276(0.4542) | Loss 8.5092(8.6208) | Error 0.1456(0.1618) Steps 0(0.00) | Grad Norm 7.5353(6.3065) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 100.5898, Epoch Time 1261.5473(1143.1287), Bit/dim 3.6329(best: 3.6341), Xent 0.6832, Loss 3.9745, Error 0.2282(best: 0.2207)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 20.4719(20.8266) | Bit/dim 3.6204(3.6336) | Xent 0.4196(0.4482) | Loss 8.4180(9.1339) | Error 0.1489(0.1588) Steps 0(0.00) | Grad Norm 6.2409(6.3613) | Total Time 0.00(0.00)\n",
      "Iter 7610 | Time 20.4931(20.9639) | Bit/dim 3.6240(3.6332) | Xent 0.3828(0.4435) | Loss 8.3581(8.9537) | Error 0.1422(0.1564) Steps 0(0.00) | Grad Norm 6.3518(6.2261) | Total Time 0.00(0.00)\n",
      "Iter 7620 | Time 19.5595(20.8288) | Bit/dim 3.6302(3.6330) | Xent 0.5453(0.4610) | Loss 8.4931(8.8263) | Error 0.1833(0.1618) Steps 0(0.00) | Grad Norm 10.1478(7.1616) | Total Time 0.00(0.00)\n",
      "Iter 7630 | Time 21.0056(20.8315) | Bit/dim 3.6192(3.6320) | Xent 0.4565(0.4683) | Loss 8.4314(8.7345) | Error 0.1656(0.1646) Steps 0(0.00) | Grad Norm 6.8700(7.2465) | Total Time 0.00(0.00)\n",
      "Iter 7640 | Time 21.0235(20.7561) | Bit/dim 3.6499(3.6311) | Xent 0.4465(0.4743) | Loss 8.4932(8.6762) | Error 0.1633(0.1669) Steps 0(0.00) | Grad Norm 7.9856(7.3890) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
