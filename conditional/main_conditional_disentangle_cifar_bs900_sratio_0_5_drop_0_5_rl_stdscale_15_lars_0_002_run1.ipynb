{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_002_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_002_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.002, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 2590 | Time 15.3594(15.3985) | Bit/dim 3.7819(3.8228) | Xent 0.9365(0.9664) | Loss 9.3133(10.1142) | Error 0.3456(0.3435) Steps 598(573.52) | Grad Norm 9.2144(8.7709) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 14.0776(15.2262) | Bit/dim 3.8078(3.8195) | Xent 0.9091(0.9618) | Loss 9.3626(9.9354) | Error 0.3189(0.3425) Steps 568(573.10) | Grad Norm 7.2872(8.2131) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.1882(15.1434) | Bit/dim 3.7997(3.8195) | Xent 1.0320(0.9623) | Loss 9.5647(9.8179) | Error 0.3556(0.3420) Steps 598(572.74) | Grad Norm 13.1851(8.5693) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 13.9218(14.9811) | Bit/dim 3.8158(3.8189) | Xent 0.9603(0.9598) | Loss 9.4476(9.7176) | Error 0.3378(0.3416) Steps 562(570.08) | Grad Norm 7.3698(9.0206) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.5322(14.9669) | Bit/dim 3.7750(3.8155) | Xent 0.8633(0.9562) | Loss 9.3164(9.6550) | Error 0.3089(0.3398) Steps 568(570.32) | Grad Norm 6.6891(8.7881) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 14.7939(14.9502) | Bit/dim 3.8098(3.8169) | Xent 0.9628(0.9551) | Loss 9.4286(9.6051) | Error 0.3233(0.3389) Steps 562(571.73) | Grad Norm 9.6205(8.8265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 80.6630, Epoch Time 936.4516(838.8518), Bit/dim 3.8182(best: inf), Xent 0.9455, Loss 4.2910, Error 0.3356(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.5162(14.9765) | Bit/dim 3.8047(3.8162) | Xent 0.9210(0.9545) | Loss 9.3213(10.0160) | Error 0.3244(0.3397) Steps 556(570.83) | Grad Norm 10.0045(9.0047) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.5211(14.9989) | Bit/dim 3.8108(3.8134) | Xent 0.9833(0.9486) | Loss 9.5964(9.8678) | Error 0.3422(0.3379) Steps 544(572.51) | Grad Norm 6.8153(8.4592) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 14.7630(14.9788) | Bit/dim 3.7735(3.8100) | Xent 0.9013(0.9464) | Loss 9.2566(9.7539) | Error 0.3289(0.3370) Steps 586(574.76) | Grad Norm 6.2924(8.8150) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 14.6008(14.9475) | Bit/dim 3.8077(3.8091) | Xent 0.9623(0.9429) | Loss 9.4905(9.6707) | Error 0.3500(0.3357) Steps 568(575.88) | Grad Norm 9.0351(8.4855) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.3346(14.9963) | Bit/dim 3.8291(3.8097) | Xent 0.8628(0.9364) | Loss 9.4088(9.6069) | Error 0.3078(0.3331) Steps 574(574.68) | Grad Norm 7.2874(8.4299) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 78.1304, Epoch Time 920.8559(841.3119), Bit/dim 3.8042(best: 3.8182), Xent 0.9660, Loss 4.2872, Error 0.3437(best: 0.3356)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 14.8454(14.9985) | Bit/dim 3.7894(3.8076) | Xent 0.9600(0.9390) | Loss 9.3142(10.1005) | Error 0.3578(0.3342) Steps 568(579.33) | Grad Norm 9.4546(8.5911) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.7597(15.0106) | Bit/dim 3.7960(3.8100) | Xent 0.9444(0.9414) | Loss 9.5773(9.9384) | Error 0.3178(0.3345) Steps 568(579.61) | Grad Norm 8.3437(9.0267) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.1427(15.0369) | Bit/dim 3.8355(3.8097) | Xent 0.9843(0.9423) | Loss 9.4987(9.8041) | Error 0.3489(0.3348) Steps 616(579.71) | Grad Norm 9.7953(9.3364) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.2852(15.0011) | Bit/dim 3.8189(3.8078) | Xent 0.9614(0.9453) | Loss 9.4282(9.7102) | Error 0.3400(0.3354) Steps 580(581.27) | Grad Norm 8.4435(9.5606) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 14.3393(14.9318) | Bit/dim 3.7656(3.8071) | Xent 0.9149(0.9401) | Loss 9.3766(9.6283) | Error 0.3344(0.3350) Steps 586(581.15) | Grad Norm 6.0581(9.2532) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.3731(14.9676) | Bit/dim 3.7849(3.8045) | Xent 0.9181(0.9334) | Loss 9.3011(9.5604) | Error 0.3356(0.3323) Steps 574(582.82) | Grad Norm 5.9685(8.6834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 77.8262, Epoch Time 919.3770(843.6539), Bit/dim 3.8054(best: 3.8042), Xent 0.8919, Loss 4.2514, Error 0.3160(best: 0.3356)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 14.9532(15.0420) | Bit/dim 3.7991(3.8030) | Xent 0.8579(0.9199) | Loss 9.2739(9.9644) | Error 0.3011(0.3267) Steps 586(581.84) | Grad Norm 10.5405(8.4586) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.0263(15.1003) | Bit/dim 3.7996(3.8032) | Xent 1.0084(0.9249) | Loss 9.6092(9.8370) | Error 0.3556(0.3289) Steps 574(582.39) | Grad Norm 8.8674(9.0636) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.9526(15.1198) | Bit/dim 3.8306(3.7993) | Xent 0.9771(0.9276) | Loss 9.5158(9.7217) | Error 0.3300(0.3297) Steps 598(581.99) | Grad Norm 13.3625(9.0731) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.1482(15.1207) | Bit/dim 3.7809(3.7991) | Xent 0.9175(0.9228) | Loss 9.3626(9.6340) | Error 0.3244(0.3280) Steps 568(581.91) | Grad Norm 7.7073(8.9289) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.6420(15.1616) | Bit/dim 3.7830(3.8001) | Xent 0.9280(0.9374) | Loss 9.4670(9.5767) | Error 0.3267(0.3324) Steps 616(580.12) | Grad Norm 8.7011(8.9982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 78.2772, Epoch Time 933.7482(846.3567), Bit/dim 3.7969(best: 3.8042), Xent 0.8735, Loss 4.2337, Error 0.3049(best: 0.3160)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.4455(15.1825) | Bit/dim 3.7820(3.7977) | Xent 0.9222(0.9290) | Loss 9.4756(10.0524) | Error 0.3378(0.3302) Steps 574(582.50) | Grad Norm 11.5439(8.6476) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.3330(15.2095) | Bit/dim 3.7831(3.7982) | Xent 0.8994(0.9260) | Loss 9.3157(9.8852) | Error 0.3122(0.3281) Steps 580(584.61) | Grad Norm 6.1112(8.5522) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.1939(15.1796) | Bit/dim 3.7801(3.7976) | Xent 0.9153(0.9200) | Loss 9.2552(9.7558) | Error 0.3222(0.3258) Steps 556(584.09) | Grad Norm 10.9910(8.4662) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.3849(15.1442) | Bit/dim 3.7291(3.7937) | Xent 0.8526(0.9149) | Loss 9.2132(9.6370) | Error 0.2778(0.3216) Steps 598(584.34) | Grad Norm 3.9820(8.4063) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.0287(15.2280) | Bit/dim 3.8249(3.7927) | Xent 0.8940(0.9116) | Loss 9.3020(9.5669) | Error 0.2944(0.3196) Steps 574(583.43) | Grad Norm 7.3982(8.0614) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.3960(15.2209) | Bit/dim 3.7705(3.7882) | Xent 1.0022(0.9154) | Loss 9.4281(9.5142) | Error 0.3444(0.3220) Steps 568(583.31) | Grad Norm 9.9040(8.1671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 79.0389, Epoch Time 933.0846(848.9586), Bit/dim 3.7921(best: 3.7969), Xent 0.9804, Loss 4.2823, Error 0.3507(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 14.9191(15.1949) | Bit/dim 3.7807(3.7876) | Xent 0.8421(0.9206) | Loss 9.1837(9.9429) | Error 0.2867(0.3242) Steps 592(586.94) | Grad Norm 8.3217(8.6708) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 15.0697(15.1419) | Bit/dim 3.8135(3.7864) | Xent 0.9212(0.9124) | Loss 9.4194(9.7887) | Error 0.3422(0.3249) Steps 580(587.75) | Grad Norm 13.3496(8.7710) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.4980(15.1627) | Bit/dim 3.8049(3.7889) | Xent 0.9417(0.9161) | Loss 9.5723(9.6870) | Error 0.3200(0.3259) Steps 580(586.83) | Grad Norm 6.4099(8.8089) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.0566(15.2441) | Bit/dim 3.7740(3.7876) | Xent 0.7973(0.9068) | Loss 9.1096(9.5833) | Error 0.2811(0.3220) Steps 580(587.35) | Grad Norm 5.8323(8.2967) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.4029(15.2223) | Bit/dim 3.7838(3.7872) | Xent 1.0153(0.9155) | Loss 9.5216(9.5342) | Error 0.3622(0.3248) Steps 568(585.75) | Grad Norm 14.9356(8.8848) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 78.8126, Epoch Time 934.4809(851.5242), Bit/dim 3.7932(best: 3.7921), Xent 0.8968, Loss 4.2416, Error 0.3185(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 14.9441(15.2239) | Bit/dim 3.8059(3.7878) | Xent 0.8638(0.9142) | Loss 9.2703(10.0236) | Error 0.3144(0.3250) Steps 604(588.33) | Grad Norm 13.4243(9.1777) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 14.9045(15.1735) | Bit/dim 3.7395(3.7877) | Xent 0.8931(0.9062) | Loss 9.1742(9.8406) | Error 0.3200(0.3221) Steps 610(587.78) | Grad Norm 5.7290(8.7542) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 14.6060(15.2011) | Bit/dim 3.7596(3.7863) | Xent 0.9330(0.9033) | Loss 9.2548(9.7078) | Error 0.3167(0.3198) Steps 580(588.81) | Grad Norm 5.3276(8.3560) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 14.4911(15.1856) | Bit/dim 3.7684(3.7823) | Xent 0.8564(0.9006) | Loss 9.3434(9.6125) | Error 0.3200(0.3191) Steps 574(588.44) | Grad Norm 4.7069(8.1658) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.2854(15.1764) | Bit/dim 3.7617(3.7850) | Xent 0.8885(0.8940) | Loss 9.3552(9.5418) | Error 0.3056(0.3176) Steps 592(588.47) | Grad Norm 8.2160(8.0477) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.7422(15.2446) | Bit/dim 3.7847(3.7836) | Xent 0.8739(0.8864) | Loss 9.3585(9.4931) | Error 0.3211(0.3159) Steps 592(588.98) | Grad Norm 11.3149(8.0372) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 81.1108, Epoch Time 934.2864(854.0071), Bit/dim 3.7738(best: 3.7921), Xent 0.8777, Loss 4.2127, Error 0.3108(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.3507(15.1890) | Bit/dim 3.7552(3.7798) | Xent 0.8930(0.8812) | Loss 9.3102(9.9074) | Error 0.3400(0.3139) Steps 568(588.29) | Grad Norm 5.5429(8.0977) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.5176(15.2203) | Bit/dim 3.7836(3.7817) | Xent 0.9466(0.8818) | Loss 9.3567(9.7535) | Error 0.3222(0.3122) Steps 604(589.06) | Grad Norm 14.9550(8.5421) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.6289(15.2332) | Bit/dim 3.7585(3.7801) | Xent 0.9293(0.8909) | Loss 9.2973(9.6459) | Error 0.3278(0.3158) Steps 580(590.10) | Grad Norm 8.2526(8.9402) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 16.5850(15.2958) | Bit/dim 3.7594(3.7783) | Xent 0.8553(0.8864) | Loss 9.3011(9.5554) | Error 0.3100(0.3160) Steps 580(588.97) | Grad Norm 7.9563(8.5257) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 14.6311(15.2799) | Bit/dim 3.7668(3.7779) | Xent 0.8437(0.8856) | Loss 9.1604(9.4933) | Error 0.3000(0.3152) Steps 568(589.08) | Grad Norm 9.0341(8.3363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 79.2193, Epoch Time 936.8194(856.4915), Bit/dim 3.7759(best: 3.7738), Xent 0.8692, Loss 4.2106, Error 0.3049(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.5676(15.3008) | Bit/dim 3.7789(3.7772) | Xent 0.8802(0.8859) | Loss 9.3864(9.9644) | Error 0.3267(0.3149) Steps 580(587.74) | Grad Norm 9.5015(8.5317) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.5782(15.2863) | Bit/dim 3.7530(3.7738) | Xent 0.9080(0.8820) | Loss 9.3242(9.7934) | Error 0.3056(0.3129) Steps 628(591.81) | Grad Norm 10.1475(8.2804) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.4310(15.3297) | Bit/dim 3.7812(3.7733) | Xent 0.8298(0.8809) | Loss 9.2819(9.6766) | Error 0.3000(0.3119) Steps 610(592.81) | Grad Norm 8.5180(8.3098) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.2096(15.2530) | Bit/dim 3.7816(3.7745) | Xent 0.9563(0.8794) | Loss 9.3491(9.5733) | Error 0.3256(0.3109) Steps 598(592.97) | Grad Norm 11.4654(8.1647) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 15.7473(15.2294) | Bit/dim 3.7929(3.7749) | Xent 0.8912(0.8860) | Loss 9.3821(9.5165) | Error 0.3089(0.3127) Steps 622(594.94) | Grad Norm 7.7610(8.3503) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 14.9591(15.1783) | Bit/dim 3.7849(3.7772) | Xent 0.8443(0.8820) | Loss 9.2869(9.4720) | Error 0.3056(0.3127) Steps 586(592.33) | Grad Norm 7.9538(8.1884) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 80.2739, Epoch Time 934.4886(858.8314), Bit/dim 3.7845(best: 3.7738), Xent 0.8569, Loss 4.2130, Error 0.3063(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 15.1965(15.1449) | Bit/dim 3.7713(3.7745) | Xent 0.7851(0.8785) | Loss 9.2232(9.8838) | Error 0.2789(0.3114) Steps 586(591.23) | Grad Norm 8.1235(8.2312) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 16.1198(15.1703) | Bit/dim 3.8071(3.7749) | Xent 0.9302(0.8872) | Loss 9.2172(9.7338) | Error 0.3267(0.3151) Steps 574(590.59) | Grad Norm 11.5536(9.1129) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 14.5180(15.1795) | Bit/dim 3.7868(3.7759) | Xent 0.8143(0.8875) | Loss 9.2263(9.6330) | Error 0.3044(0.3160) Steps 610(592.94) | Grad Norm 8.5151(9.0877) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 14.7604(15.1845) | Bit/dim 3.7732(3.7758) | Xent 0.8811(0.8812) | Loss 9.2105(9.5385) | Error 0.3044(0.3127) Steps 598(592.29) | Grad Norm 9.3971(8.8654) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.8433(15.2501) | Bit/dim 3.7943(3.7746) | Xent 0.9458(0.8859) | Loss 9.4663(9.4881) | Error 0.3233(0.3130) Steps 592(590.57) | Grad Norm 10.3498(9.0236) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 80.2117, Epoch Time 935.9649(861.1454), Bit/dim 3.7773(best: 3.7738), Xent 0.8432, Loss 4.1989, Error 0.2975(best: 0.3049)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 14.9952(15.2716) | Bit/dim 3.7569(3.7734) | Xent 0.8146(0.8752) | Loss 9.1380(9.9707) | Error 0.2833(0.3098) Steps 604(592.01) | Grad Norm 6.4766(8.6770) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.5431(15.2748) | Bit/dim 3.7379(3.7734) | Xent 0.8261(0.8657) | Loss 9.1791(9.7916) | Error 0.3044(0.3088) Steps 586(595.00) | Grad Norm 7.2616(8.6138) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 16.1187(15.3302) | Bit/dim 3.7248(3.7701) | Xent 0.8449(0.8662) | Loss 9.2464(9.6577) | Error 0.3111(0.3082) Steps 604(595.15) | Grad Norm 6.1943(8.3962) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.1716(15.3321) | Bit/dim 3.7502(3.7703) | Xent 0.8657(0.8677) | Loss 9.3340(9.5695) | Error 0.3022(0.3081) Steps 604(597.55) | Grad Norm 6.9258(8.3396) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.7376(15.3264) | Bit/dim 3.7695(3.7672) | Xent 0.8879(0.8637) | Loss 9.4238(9.4909) | Error 0.3100(0.3070) Steps 592(596.25) | Grad Norm 6.4405(7.8920) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.4187(15.2880) | Bit/dim 3.7143(3.7640) | Xent 0.9069(0.8664) | Loss 9.2806(9.4406) | Error 0.3078(0.3080) Steps 610(596.57) | Grad Norm 8.3578(8.0375) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 79.7649, Epoch Time 939.8710(863.5072), Bit/dim 3.7596(best: 3.7738), Xent 0.8606, Loss 4.1898, Error 0.3021(best: 0.2975)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.4125(15.3076) | Bit/dim 3.7225(3.7638) | Xent 0.8324(0.8552) | Loss 9.2242(9.8617) | Error 0.2967(0.3044) Steps 580(595.19) | Grad Norm 6.7762(7.8333) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.3675(15.3569) | Bit/dim 3.7660(3.7606) | Xent 0.8022(0.8444) | Loss 9.2054(9.7021) | Error 0.2967(0.3001) Steps 610(597.17) | Grad Norm 7.0777(7.3789) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 16.0458(15.3980) | Bit/dim 3.7595(3.7624) | Xent 0.9021(0.8515) | Loss 9.3768(9.6067) | Error 0.3311(0.3025) Steps 610(598.36) | Grad Norm 8.7874(7.5407) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.9393(15.4576) | Bit/dim 3.7364(3.7631) | Xent 0.8352(0.8457) | Loss 9.2653(9.5161) | Error 0.3078(0.3012) Steps 622(601.54) | Grad Norm 6.3967(7.3508) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.6748(15.4794) | Bit/dim 3.7711(3.7630) | Xent 0.9099(0.8468) | Loss 9.3446(9.4561) | Error 0.3311(0.3021) Steps 568(599.51) | Grad Norm 9.5179(7.6171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 81.6040, Epoch Time 950.1353(866.1060), Bit/dim 3.7601(best: 3.7596), Xent 0.8239, Loss 4.1720, Error 0.2900(best: 0.2975)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.2457(15.4458) | Bit/dim 3.7708(3.7604) | Xent 0.8658(0.8492) | Loss 9.3789(9.9550) | Error 0.3111(0.3022) Steps 586(598.60) | Grad Norm 7.6645(7.9897) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.9377(15.4782) | Bit/dim 3.7644(3.7589) | Xent 0.8310(0.8365) | Loss 9.2360(9.7611) | Error 0.2733(0.2980) Steps 622(599.84) | Grad Norm 11.5651(7.9487) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 16.0902(15.5399) | Bit/dim 3.7553(3.7570) | Xent 0.8733(0.8417) | Loss 9.4091(9.6311) | Error 0.3278(0.3005) Steps 622(603.08) | Grad Norm 7.1847(7.8458) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.7662(15.5600) | Bit/dim 3.7548(3.7559) | Xent 0.8031(0.8406) | Loss 9.2418(9.5407) | Error 0.2922(0.2995) Steps 616(604.68) | Grad Norm 6.1531(7.3263) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.6727(15.5592) | Bit/dim 3.7880(3.7586) | Xent 0.8207(0.8408) | Loss 9.2137(9.4616) | Error 0.2811(0.2986) Steps 604(602.76) | Grad Norm 8.4401(7.4393) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.0690(15.4889) | Bit/dim 3.7449(3.7557) | Xent 0.8292(0.8378) | Loss 9.1889(9.3942) | Error 0.3022(0.2989) Steps 580(601.50) | Grad Norm 5.1075(7.1491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 80.1836, Epoch Time 952.3193(868.6924), Bit/dim 3.7567(best: 3.7596), Xent 0.8282, Loss 4.1708, Error 0.2895(best: 0.2900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 16.6399(15.5045) | Bit/dim 3.7292(3.7538) | Xent 0.8433(0.8304) | Loss 9.2195(9.8160) | Error 0.3133(0.2968) Steps 658(601.19) | Grad Norm 6.0663(6.8231) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.0968(15.5346) | Bit/dim 3.7347(3.7519) | Xent 0.9077(0.8300) | Loss 9.1621(9.6572) | Error 0.3211(0.2964) Steps 610(599.80) | Grad Norm 11.5355(7.0905) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.2524(15.6112) | Bit/dim 3.7440(3.7559) | Xent 0.8870(0.8387) | Loss 9.3293(9.5672) | Error 0.3200(0.3003) Steps 610(599.70) | Grad Norm 10.9206(7.8324) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 14.9054(15.5684) | Bit/dim 3.7730(3.7570) | Xent 0.8450(0.8427) | Loss 9.3419(9.4959) | Error 0.2956(0.2994) Steps 598(599.87) | Grad Norm 8.4776(8.4433) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.0079(15.5537) | Bit/dim 3.8004(3.7591) | Xent 0.9384(0.8514) | Loss 9.4219(9.4486) | Error 0.3289(0.3019) Steps 586(600.24) | Grad Norm 11.9016(9.1761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 80.4677, Epoch Time 956.1625(871.3165), Bit/dim 3.7685(best: 3.7567), Xent 0.8368, Loss 4.1869, Error 0.2937(best: 0.2895)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 14.9699(15.6018) | Bit/dim 3.7535(3.7619) | Xent 0.8478(0.8515) | Loss 9.1587(9.9708) | Error 0.2922(0.3024) Steps 586(599.49) | Grad Norm 10.2840(9.3539) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.7450(15.5807) | Bit/dim 3.7468(3.7596) | Xent 0.8921(0.8483) | Loss 9.3622(9.7768) | Error 0.3000(0.3000) Steps 592(598.57) | Grad Norm 8.2022(9.1873) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.3514(15.6251) | Bit/dim 3.7460(3.7568) | Xent 0.7836(0.8437) | Loss 9.2107(9.6372) | Error 0.2744(0.2988) Steps 574(601.64) | Grad Norm 5.9343(8.6764) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.1898(15.6814) | Bit/dim 3.7901(3.7558) | Xent 0.8028(0.8349) | Loss 9.3511(9.5269) | Error 0.2967(0.2968) Steps 634(603.54) | Grad Norm 7.4811(8.0855) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.6669(15.6869) | Bit/dim 3.7540(3.7530) | Xent 0.8827(0.8357) | Loss 9.1865(9.4489) | Error 0.3111(0.2960) Steps 616(606.52) | Grad Norm 8.2333(8.1368) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 16.0865(15.6482) | Bit/dim 3.7359(3.7525) | Xent 0.7763(0.8276) | Loss 9.2099(9.3881) | Error 0.2856(0.2935) Steps 622(605.95) | Grad Norm 7.3846(7.7275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 80.2443, Epoch Time 960.3659(873.9880), Bit/dim 3.7486(best: 3.7567), Xent 0.8053, Loss 4.1512, Error 0.2819(best: 0.2895)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 16.0190(15.7182) | Bit/dim 3.7391(3.7526) | Xent 0.8541(0.8200) | Loss 9.1096(9.8246) | Error 0.2944(0.2904) Steps 634(608.19) | Grad Norm 7.3123(7.6429) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.6616(15.7679) | Bit/dim 3.7622(3.7494) | Xent 0.8385(0.8186) | Loss 9.2859(9.6664) | Error 0.3233(0.2907) Steps 610(608.25) | Grad Norm 9.2422(7.4395) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.8463(15.7586) | Bit/dim 3.7582(3.7462) | Xent 0.7613(0.8215) | Loss 9.1406(9.5490) | Error 0.3044(0.2922) Steps 622(607.32) | Grad Norm 7.2898(7.6927) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.7701(15.7619) | Bit/dim 3.7296(3.7455) | Xent 0.7475(0.8139) | Loss 9.1285(9.4510) | Error 0.2689(0.2899) Steps 622(606.91) | Grad Norm 5.5812(7.6255) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.1839(15.7598) | Bit/dim 3.7519(3.7448) | Xent 0.7763(0.8135) | Loss 9.1740(9.3897) | Error 0.2767(0.2894) Steps 604(607.99) | Grad Norm 7.7459(7.7474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 81.0778, Epoch Time 970.7197(876.8899), Bit/dim 3.7451(best: 3.7486), Xent 0.8163, Loss 4.1532, Error 0.2862(best: 0.2819)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 16.3270(15.8120) | Bit/dim 3.7083(3.7428) | Xent 0.7617(0.8109) | Loss 9.1976(9.8902) | Error 0.2667(0.2881) Steps 604(608.30) | Grad Norm 6.0889(7.8018) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 16.1868(15.8657) | Bit/dim 3.7004(3.7439) | Xent 0.8863(0.8184) | Loss 9.2643(9.7244) | Error 0.3122(0.2904) Steps 592(611.33) | Grad Norm 10.1223(8.2713) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.8043(15.9043) | Bit/dim 3.7357(3.7425) | Xent 0.7743(0.8175) | Loss 9.3058(9.5944) | Error 0.2900(0.2888) Steps 604(613.64) | Grad Norm 10.0262(8.3141) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.2409(15.8828) | Bit/dim 3.7668(3.7428) | Xent 0.8560(0.8193) | Loss 9.3505(9.4905) | Error 0.3111(0.2909) Steps 604(616.98) | Grad Norm 9.7130(8.3612) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.6971(15.8785) | Bit/dim 3.7365(3.7433) | Xent 0.8525(0.8163) | Loss 9.3095(9.4199) | Error 0.3000(0.2891) Steps 598(615.86) | Grad Norm 10.6438(8.2665) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.5706(15.8542) | Bit/dim 3.7445(3.7472) | Xent 0.8587(0.8313) | Loss 9.2207(9.3962) | Error 0.3111(0.2948) Steps 634(617.89) | Grad Norm 6.8029(8.7671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 82.5921, Epoch Time 974.6282(879.8221), Bit/dim 3.7548(best: 3.7451), Xent 0.8615, Loss 4.1855, Error 0.3010(best: 0.2819)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 16.8602(15.9164) | Bit/dim 3.7527(3.7491) | Xent 0.8266(0.8305) | Loss 9.3130(9.8376) | Error 0.2778(0.2943) Steps 622(617.55) | Grad Norm 6.9112(8.4391) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.8484(15.9444) | Bit/dim 3.7181(3.7472) | Xent 0.7754(0.8234) | Loss 9.1906(9.6712) | Error 0.2944(0.2929) Steps 622(618.82) | Grad Norm 6.6741(8.2370) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.3543(16.0007) | Bit/dim 3.7205(3.7449) | Xent 0.7683(0.8186) | Loss 9.1118(9.5479) | Error 0.2900(0.2906) Steps 634(619.46) | Grad Norm 6.1273(8.2356) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.4895(16.0335) | Bit/dim 3.7464(3.7411) | Xent 0.7682(0.8215) | Loss 9.2137(9.4670) | Error 0.2800(0.2920) Steps 670(620.21) | Grad Norm 10.0090(8.0405) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.0103(15.9658) | Bit/dim 3.7015(3.7447) | Xent 0.8433(0.8194) | Loss 9.1600(9.4034) | Error 0.2833(0.2904) Steps 598(620.62) | Grad Norm 8.4318(8.3440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 82.0913, Epoch Time 982.1906(882.8931), Bit/dim 3.7420(best: 3.7451), Xent 0.7858, Loss 4.1349, Error 0.2756(best: 0.2819)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 16.2387(16.0433) | Bit/dim 3.7068(3.7439) | Xent 0.8580(0.8088) | Loss 9.2129(9.9023) | Error 0.3100(0.2869) Steps 598(620.64) | Grad Norm 8.3746(7.9414) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.9765(16.0610) | Bit/dim 3.6963(3.7408) | Xent 0.7858(0.8044) | Loss 9.0919(9.7119) | Error 0.2778(0.2852) Steps 634(622.05) | Grad Norm 5.5039(7.6791) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.7636(16.0436) | Bit/dim 3.7399(3.7399) | Xent 0.7723(0.8046) | Loss 9.2131(9.5833) | Error 0.2711(0.2848) Steps 604(621.71) | Grad Norm 7.1916(7.9593) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.3349(16.0240) | Bit/dim 3.7562(3.7429) | Xent 0.7459(0.8011) | Loss 9.1779(9.4887) | Error 0.2533(0.2835) Steps 604(621.10) | Grad Norm 7.5335(7.8196) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 16.1550(15.9632) | Bit/dim 3.7313(3.7412) | Xent 0.8479(0.7965) | Loss 9.2165(9.3996) | Error 0.2956(0.2826) Steps 646(620.15) | Grad Norm 5.3836(7.4761) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.1228(15.9658) | Bit/dim 3.7141(3.7385) | Xent 0.7812(0.7941) | Loss 8.9673(9.3427) | Error 0.2967(0.2816) Steps 634(621.42) | Grad Norm 7.2011(7.2996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 82.1408, Epoch Time 980.1924(885.8121), Bit/dim 3.7402(best: 3.7420), Xent 0.7863, Loss 4.1334, Error 0.2754(best: 0.2756)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.9025(16.0552) | Bit/dim 3.7209(3.7397) | Xent 0.7585(0.7851) | Loss 9.1960(9.7857) | Error 0.2811(0.2789) Steps 652(623.88) | Grad Norm 5.5864(7.2139) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.0247(16.0319) | Bit/dim 3.7381(3.7367) | Xent 0.8513(0.7950) | Loss 9.2893(9.6384) | Error 0.3000(0.2831) Steps 646(625.45) | Grad Norm 9.4664(7.5550) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.4529(16.1411) | Bit/dim 3.7568(3.7383) | Xent 0.8452(0.7979) | Loss 9.2205(9.5265) | Error 0.2989(0.2840) Steps 622(626.07) | Grad Norm 9.4610(7.8350) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.0531(16.0879) | Bit/dim 3.7577(3.7393) | Xent 0.7766(0.8026) | Loss 9.2667(9.4514) | Error 0.2878(0.2851) Steps 610(627.08) | Grad Norm 6.8963(8.0705) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.0159(16.1175) | Bit/dim 3.7531(3.7402) | Xent 0.8125(0.8025) | Loss 9.3023(9.3958) | Error 0.2878(0.2847) Steps 628(626.02) | Grad Norm 8.2707(8.1370) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 82.2343, Epoch Time 989.5604(888.9246), Bit/dim 3.7303(best: 3.7402), Xent 0.7877, Loss 4.1242, Error 0.2764(best: 0.2754)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.6096(16.0847) | Bit/dim 3.7205(3.7367) | Xent 0.7522(0.7974) | Loss 9.0931(9.9092) | Error 0.2811(0.2836) Steps 652(627.21) | Grad Norm 6.3365(7.9883) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 16.3312(16.0791) | Bit/dim 3.7605(3.7368) | Xent 0.7826(0.7982) | Loss 9.2424(9.7173) | Error 0.2833(0.2839) Steps 604(624.18) | Grad Norm 7.4039(7.8485) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 17.3829(16.2218) | Bit/dim 3.7253(3.7354) | Xent 0.8197(0.7991) | Loss 9.1451(9.5841) | Error 0.3078(0.2839) Steps 652(626.19) | Grad Norm 11.1256(8.2962) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.3346(16.1942) | Bit/dim 3.7000(3.7336) | Xent 0.8092(0.7964) | Loss 9.1376(9.4865) | Error 0.2989(0.2839) Steps 616(626.68) | Grad Norm 9.0092(8.2134) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.7255(16.2484) | Bit/dim 3.7350(3.7332) | Xent 0.8248(0.7916) | Loss 9.1783(9.4077) | Error 0.2833(0.2818) Steps 634(629.01) | Grad Norm 7.3733(7.9382) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 16.5767(16.3423) | Bit/dim 3.7376(3.7347) | Xent 0.8646(0.7866) | Loss 9.2067(9.3424) | Error 0.2900(0.2793) Steps 616(630.24) | Grad Norm 9.6444(8.0453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 83.5594, Epoch Time 998.8684(892.2229), Bit/dim 3.7328(best: 3.7303), Xent 0.7654, Loss 4.1155, Error 0.2677(best: 0.2754)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.4853(16.2768) | Bit/dim 3.7320(3.7377) | Xent 0.7587(0.7831) | Loss 9.1787(9.8064) | Error 0.2633(0.2763) Steps 592(630.37) | Grad Norm 8.2975(7.9868) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.3422(16.3001) | Bit/dim 3.7294(3.7368) | Xent 0.7525(0.7781) | Loss 9.1926(9.6424) | Error 0.2622(0.2756) Steps 682(631.48) | Grad Norm 12.5073(7.9418) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 17.1411(16.3211) | Bit/dim 3.7046(3.7331) | Xent 0.8230(0.7783) | Loss 9.2401(9.5181) | Error 0.2989(0.2754) Steps 628(632.30) | Grad Norm 12.3204(7.8828) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.1814(16.2961) | Bit/dim 3.7162(3.7296) | Xent 0.8355(0.7766) | Loss 9.2436(9.4210) | Error 0.2878(0.2752) Steps 634(632.23) | Grad Norm 5.3749(7.6263) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.6380(16.3614) | Bit/dim 3.6906(3.7270) | Xent 0.9005(0.7797) | Loss 9.2102(9.3481) | Error 0.3067(0.2771) Steps 634(632.11) | Grad Norm 16.2194(7.7437) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 83.0460, Epoch Time 1000.5785(895.4735), Bit/dim 3.7393(best: 3.7303), Xent 0.8039, Loss 4.1413, Error 0.2826(best: 0.2677)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 15.6782(16.3977) | Bit/dim 3.7504(3.7289) | Xent 0.8514(0.7915) | Loss 9.3598(9.8617) | Error 0.3078(0.2819) Steps 640(634.62) | Grad Norm 9.8499(8.4362) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 17.3652(16.4551) | Bit/dim 3.6864(3.7289) | Xent 0.7559(0.7927) | Loss 9.0734(9.6865) | Error 0.2767(0.2817) Steps 622(636.36) | Grad Norm 6.2850(8.4579) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.1990(16.4459) | Bit/dim 3.7044(3.7277) | Xent 0.7430(0.7871) | Loss 8.9738(9.5414) | Error 0.2556(0.2786) Steps 652(639.32) | Grad Norm 8.7427(8.1933) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 17.3059(16.4556) | Bit/dim 3.7292(3.7274) | Xent 0.7060(0.7782) | Loss 9.1662(9.4428) | Error 0.2589(0.2761) Steps 652(640.15) | Grad Norm 3.7020(8.0809) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 16.3350(16.5424) | Bit/dim 3.7052(3.7291) | Xent 0.7110(0.7770) | Loss 9.0162(9.3830) | Error 0.2478(0.2757) Steps 616(641.02) | Grad Norm 6.8055(7.8017) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 17.4053(16.6006) | Bit/dim 3.6872(3.7266) | Xent 0.8157(0.7758) | Loss 9.1698(9.3406) | Error 0.2789(0.2755) Steps 658(643.01) | Grad Norm 7.9685(7.5492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 83.1418, Epoch Time 1013.8853(899.0259), Bit/dim 3.7260(best: 3.7303), Xent 0.8187, Loss 4.1354, Error 0.2926(best: 0.2677)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 17.6492(16.6851) | Bit/dim 3.7652(3.7263) | Xent 0.8167(0.7810) | Loss 9.3923(9.8181) | Error 0.2956(0.2772) Steps 652(645.01) | Grad Norm 9.6706(7.5890) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.8206(16.6495) | Bit/dim 3.7498(3.7263) | Xent 0.9005(0.7867) | Loss 9.3440(9.6557) | Error 0.3200(0.2791) Steps 634(644.84) | Grad Norm 15.4857(8.0918) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.0442(16.6288) | Bit/dim 3.7304(3.7266) | Xent 0.7852(0.7880) | Loss 9.2549(9.5413) | Error 0.2722(0.2785) Steps 664(647.34) | Grad Norm 5.5156(8.1358) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 17.5896(16.6810) | Bit/dim 3.6893(3.7258) | Xent 0.7205(0.7810) | Loss 9.1216(9.4421) | Error 0.2522(0.2770) Steps 658(646.94) | Grad Norm 5.5652(7.6400) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.7826(16.6405) | Bit/dim 3.7558(3.7244) | Xent 0.7816(0.7740) | Loss 9.3404(9.3638) | Error 0.2900(0.2736) Steps 646(648.23) | Grad Norm 7.9321(7.4199) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 83.5024, Epoch Time 1017.3335(902.5751), Bit/dim 3.7322(best: 3.7260), Xent 0.7583, Loss 4.1113, Error 0.2624(best: 0.2677)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 17.3265(16.6445) | Bit/dim 3.6989(3.7248) | Xent 0.6949(0.7629) | Loss 9.0786(9.8732) | Error 0.2578(0.2701) Steps 658(649.51) | Grad Norm 5.5384(7.0653) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 16.8995(16.6568) | Bit/dim 3.7276(3.7207) | Xent 0.7072(0.7531) | Loss 9.0836(9.6609) | Error 0.2567(0.2672) Steps 652(649.89) | Grad Norm 11.1232(6.9819) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 16.8189(16.7732) | Bit/dim 3.7682(3.7218) | Xent 0.8099(0.7546) | Loss 9.2853(9.5260) | Error 0.2844(0.2676) Steps 670(652.86) | Grad Norm 12.7026(7.2720) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 16.7851(16.7270) | Bit/dim 3.7478(3.7256) | Xent 0.8574(0.7655) | Loss 9.3304(9.4501) | Error 0.3011(0.2713) Steps 664(652.32) | Grad Norm 10.8578(7.7795) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.4245(16.6525) | Bit/dim 3.7640(3.7265) | Xent 0.7793(0.7645) | Loss 9.1519(9.3733) | Error 0.2833(0.2713) Steps 640(651.04) | Grad Norm 7.4095(7.9077) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.2579(16.6975) | Bit/dim 3.7305(3.7268) | Xent 0.8290(0.7642) | Loss 9.2540(9.3213) | Error 0.3000(0.2725) Steps 634(648.38) | Grad Norm 11.0456(8.0604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 82.8577, Epoch Time 1021.3887(906.1395), Bit/dim 3.7198(best: 3.7260), Xent 0.7986, Loss 4.1191, Error 0.2790(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 17.2755(16.6879) | Bit/dim 3.7100(3.7250) | Xent 0.7038(0.7569) | Loss 9.0866(9.7578) | Error 0.2611(0.2692) Steps 640(648.50) | Grad Norm 4.9205(7.9857) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 16.2489(16.6773) | Bit/dim 3.6913(3.7218) | Xent 0.7484(0.7533) | Loss 9.1538(9.5920) | Error 0.2622(0.2679) Steps 664(648.63) | Grad Norm 7.2179(7.6301) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.9835(16.7890) | Bit/dim 3.7096(3.7238) | Xent 0.8126(0.7545) | Loss 9.1779(9.4763) | Error 0.3056(0.2694) Steps 634(649.41) | Grad Norm 10.3123(7.7245) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 16.7374(16.7660) | Bit/dim 3.7318(3.7243) | Xent 0.7791(0.7550) | Loss 9.1739(9.3886) | Error 0.2800(0.2702) Steps 634(648.18) | Grad Norm 7.8813(7.8288) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 16.9201(16.8549) | Bit/dim 3.6691(3.7193) | Xent 0.7921(0.7580) | Loss 9.1316(9.3186) | Error 0.2744(0.2710) Steps 670(649.76) | Grad Norm 6.2355(7.7563) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 84.4241, Epoch Time 1028.1593(909.8001), Bit/dim 3.7219(best: 3.7198), Xent 0.7521, Loss 4.0980, Error 0.2654(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.7243(16.8429) | Bit/dim 3.6822(3.7166) | Xent 0.7764(0.7541) | Loss 9.0183(9.8366) | Error 0.2611(0.2694) Steps 688(652.73) | Grad Norm 7.2515(7.3022) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 17.0244(16.9737) | Bit/dim 3.6960(3.7172) | Xent 0.7753(0.7510) | Loss 9.0863(9.6682) | Error 0.2578(0.2679) Steps 622(655.64) | Grad Norm 7.3576(7.5124) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 17.5480(17.0300) | Bit/dim 3.7033(3.7162) | Xent 0.7343(0.7460) | Loss 9.1711(9.5294) | Error 0.2611(0.2659) Steps 670(656.37) | Grad Norm 5.2974(7.2309) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 16.5732(16.9767) | Bit/dim 3.6628(3.7140) | Xent 0.7812(0.7458) | Loss 9.0729(9.4186) | Error 0.2744(0.2648) Steps 646(656.23) | Grad Norm 7.0720(6.7692) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.5549(16.9802) | Bit/dim 3.7180(3.7145) | Xent 0.7404(0.7413) | Loss 9.0671(9.3352) | Error 0.2544(0.2624) Steps 658(657.05) | Grad Norm 12.5097(7.0243) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 17.3950(16.8957) | Bit/dim 3.7338(3.7172) | Xent 0.7379(0.7431) | Loss 9.2177(9.2892) | Error 0.2744(0.2631) Steps 670(654.56) | Grad Norm 9.6357(7.1928) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 84.1144, Epoch Time 1036.0441(913.5874), Bit/dim 3.7130(best: 3.7198), Xent 0.7659, Loss 4.0959, Error 0.2713(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 17.3778(16.8674) | Bit/dim 3.7015(3.7175) | Xent 0.7746(0.7447) | Loss 9.0738(9.7486) | Error 0.2622(0.2631) Steps 628(655.74) | Grad Norm 10.5087(7.7809) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.3410(16.8850) | Bit/dim 3.7179(3.7167) | Xent 0.7380(0.7387) | Loss 9.1178(9.5824) | Error 0.2567(0.2612) Steps 640(655.89) | Grad Norm 5.3782(7.6346) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 17.3629(17.0094) | Bit/dim 3.7270(3.7179) | Xent 0.6733(0.7348) | Loss 9.1795(9.4767) | Error 0.2311(0.2599) Steps 676(662.73) | Grad Norm 6.2561(7.3450) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 16.5821(17.0071) | Bit/dim 3.7042(3.7180) | Xent 0.7866(0.7445) | Loss 9.0916(9.3963) | Error 0.2878(0.2639) Steps 640(663.57) | Grad Norm 9.3633(7.5819) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 17.5380(16.9868) | Bit/dim 3.6999(3.7148) | Xent 0.7851(0.7460) | Loss 9.1529(9.3261) | Error 0.2767(0.2649) Steps 700(663.85) | Grad Norm 9.2746(7.8519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 84.5401, Epoch Time 1037.6243(917.3085), Bit/dim 3.7181(best: 3.7130), Xent 0.7896, Loss 4.1129, Error 0.2768(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.5916(17.0110) | Bit/dim 3.7242(3.7162) | Xent 0.7378(0.7513) | Loss 9.2405(9.8599) | Error 0.2533(0.2669) Steps 646(665.72) | Grad Norm 5.8536(8.1409) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 17.7165(16.9738) | Bit/dim 3.7463(3.7154) | Xent 0.7131(0.7472) | Loss 9.2208(9.6725) | Error 0.2489(0.2643) Steps 652(662.69) | Grad Norm 7.4752(7.7886) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.8757(17.0282) | Bit/dim 3.7143(3.7161) | Xent 0.7450(0.7370) | Loss 9.1800(9.5215) | Error 0.2622(0.2612) Steps 658(663.93) | Grad Norm 8.9958(7.6558) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 17.1148(17.0310) | Bit/dim 3.7214(3.7134) | Xent 0.7833(0.7365) | Loss 9.2050(9.4184) | Error 0.2767(0.2597) Steps 688(664.24) | Grad Norm 9.4988(7.6476) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.9115(17.0093) | Bit/dim 3.7256(3.7149) | Xent 0.7561(0.7400) | Loss 9.1654(9.3565) | Error 0.2833(0.2622) Steps 682(661.27) | Grad Norm 6.8434(7.6356) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 16.7137(16.9988) | Bit/dim 3.7092(3.7129) | Xent 0.7135(0.7368) | Loss 9.1017(9.2947) | Error 0.2689(0.2630) Steps 652(661.74) | Grad Norm 6.0445(7.1502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 85.2634, Epoch Time 1038.8856(920.9559), Bit/dim 3.7137(best: 3.7130), Xent 0.7572, Loss 4.0924, Error 0.2659(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 16.4031(17.0205) | Bit/dim 3.7067(3.7115) | Xent 0.7528(0.7319) | Loss 9.1374(9.7405) | Error 0.2511(0.2606) Steps 652(662.19) | Grad Norm 5.2743(7.2949) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 16.3424(16.9879) | Bit/dim 3.7200(3.7087) | Xent 0.7381(0.7309) | Loss 9.2393(9.5719) | Error 0.2478(0.2609) Steps 634(659.14) | Grad Norm 5.6213(6.8376) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 17.2055(16.9737) | Bit/dim 3.7231(3.7077) | Xent 0.8045(0.7333) | Loss 9.2520(9.4625) | Error 0.2778(0.2617) Steps 646(659.01) | Grad Norm 8.0748(7.1645) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.4837(16.9814) | Bit/dim 3.7138(3.7095) | Xent 0.7134(0.7311) | Loss 9.1426(9.3728) | Error 0.2389(0.2597) Steps 652(658.46) | Grad Norm 6.5820(7.2507) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 17.2259(16.9841) | Bit/dim 3.7213(3.7085) | Xent 0.7556(0.7286) | Loss 9.1665(9.3019) | Error 0.2711(0.2585) Steps 628(658.09) | Grad Norm 6.4881(7.2122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 83.3983, Epoch Time 1035.6405(924.3964), Bit/dim 3.7177(best: 3.7130), Xent 0.7692, Loss 4.1023, Error 0.2721(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 17.1693(16.9418) | Bit/dim 3.7560(3.7098) | Xent 0.7508(0.7323) | Loss 9.1726(9.8177) | Error 0.2578(0.2590) Steps 664(659.27) | Grad Norm 9.8054(7.4035) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.5574(17.0076) | Bit/dim 3.7116(3.7089) | Xent 0.7356(0.7227) | Loss 9.2131(9.6178) | Error 0.2733(0.2568) Steps 706(659.74) | Grad Norm 10.5012(7.5135) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 16.6298(16.9645) | Bit/dim 3.6976(3.7107) | Xent 0.7703(0.7218) | Loss 9.1552(9.4801) | Error 0.2656(0.2570) Steps 658(660.93) | Grad Norm 5.7765(7.4540) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.0907(17.0307) | Bit/dim 3.6776(3.7091) | Xent 0.8078(0.7264) | Loss 9.1505(9.3893) | Error 0.2856(0.2589) Steps 658(661.86) | Grad Norm 9.5801(7.6761) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 17.3886(17.0563) | Bit/dim 3.7403(3.7091) | Xent 0.7416(0.7289) | Loss 9.1871(9.3165) | Error 0.2611(0.2590) Steps 694(662.55) | Grad Norm 5.5742(7.6129) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.7225(17.0841) | Bit/dim 3.6966(3.7053) | Xent 0.7006(0.7347) | Loss 9.0186(9.2623) | Error 0.2444(0.2603) Steps 664(663.81) | Grad Norm 5.4050(7.5496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 85.8441, Epoch Time 1042.8644(927.9504), Bit/dim 3.7095(best: 3.7130), Xent 0.7534, Loss 4.0862, Error 0.2634(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.9002(17.1002) | Bit/dim 3.7030(3.7076) | Xent 0.7207(0.7209) | Loss 9.1621(9.7393) | Error 0.2467(0.2551) Steps 664(662.58) | Grad Norm 7.4669(7.0982) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 17.9495(17.1309) | Bit/dim 3.7327(3.7080) | Xent 0.7262(0.7176) | Loss 9.2103(9.5741) | Error 0.2678(0.2542) Steps 718(667.01) | Grad Norm 10.1931(7.2359) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.4607(17.1876) | Bit/dim 3.7057(3.7047) | Xent 0.7848(0.7136) | Loss 9.1147(9.4580) | Error 0.2744(0.2522) Steps 652(669.97) | Grad Norm 9.5001(7.1459) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.4240(17.2148) | Bit/dim 3.7201(3.7047) | Xent 0.7197(0.7198) | Loss 9.2366(9.3879) | Error 0.2533(0.2544) Steps 670(671.05) | Grad Norm 10.7838(7.7430) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 17.1682(17.1304) | Bit/dim 3.7054(3.7064) | Xent 0.7583(0.7195) | Loss 9.1271(9.3177) | Error 0.2767(0.2555) Steps 646(668.58) | Grad Norm 5.8385(7.4973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 87.7735, Epoch Time 1048.4732(931.5661), Bit/dim 3.7031(best: 3.7095), Xent 0.7469, Loss 4.0765, Error 0.2608(best: 0.2624)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 17.3126(17.0905) | Bit/dim 3.6984(3.7083) | Xent 0.7068(0.7114) | Loss 9.1467(9.8714) | Error 0.2622(0.2530) Steps 682(666.80) | Grad Norm 5.8089(7.2768) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.8540(17.1689) | Bit/dim 3.6497(3.7074) | Xent 0.7302(0.7137) | Loss 9.0232(9.6755) | Error 0.2500(0.2536) Steps 676(668.71) | Grad Norm 5.1745(7.4258) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 17.6564(17.0915) | Bit/dim 3.6837(3.7033) | Xent 0.8025(0.7173) | Loss 9.2092(9.5305) | Error 0.2767(0.2532) Steps 670(667.99) | Grad Norm 17.4583(7.8674) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.8807(17.1045) | Bit/dim 3.7272(3.7056) | Xent 0.6933(0.7224) | Loss 9.1285(9.4431) | Error 0.2367(0.2566) Steps 670(669.40) | Grad Norm 9.2817(8.0200) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 17.5657(17.0986) | Bit/dim 3.7024(3.7066) | Xent 0.7339(0.7187) | Loss 9.1380(9.3636) | Error 0.2578(0.2552) Steps 712(669.34) | Grad Norm 9.0737(7.9581) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 17.0919(17.1810) | Bit/dim 3.6955(3.7093) | Xent 0.8121(0.7211) | Loss 9.2646(9.3142) | Error 0.2844(0.2548) Steps 670(670.73) | Grad Norm 9.7740(8.0082) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 87.5756, Epoch Time 1050.2361(935.1262), Bit/dim 3.7139(best: 3.7031), Xent 0.7520, Loss 4.0898, Error 0.2614(best: 0.2608)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.8876(17.1133) | Bit/dim 3.7225(3.7101) | Xent 0.7007(0.7174) | Loss 9.1861(9.7738) | Error 0.2489(0.2540) Steps 670(669.84) | Grad Norm 7.0194(7.8335) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.2889(17.2748) | Bit/dim 3.7284(3.7100) | Xent 0.7061(0.7103) | Loss 9.1431(9.6126) | Error 0.2433(0.2514) Steps 676(675.36) | Grad Norm 6.0081(7.4884) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 17.5686(17.3746) | Bit/dim 3.6685(3.7058) | Xent 0.7464(0.7057) | Loss 8.9701(9.4803) | Error 0.2722(0.2504) Steps 670(675.89) | Grad Norm 6.1705(7.0943) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.3363(17.4064) | Bit/dim 3.7069(3.7034) | Xent 0.7070(0.7070) | Loss 9.1370(9.3863) | Error 0.2489(0.2514) Steps 682(677.83) | Grad Norm 9.0882(7.3511) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.7752(17.3910) | Bit/dim 3.6871(3.7027) | Xent 0.6611(0.7097) | Loss 8.9879(9.3236) | Error 0.2433(0.2507) Steps 652(677.04) | Grad Norm 5.7676(7.3413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 86.3453, Epoch Time 1062.4138(938.9448), Bit/dim 3.7045(best: 3.7031), Xent 0.7807, Loss 4.0949, Error 0.2754(best: 0.2608)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 17.0908(17.3505) | Bit/dim 3.6749(3.7009) | Xent 0.6953(0.7055) | Loss 8.9753(9.8506) | Error 0.2444(0.2496) Steps 664(677.46) | Grad Norm 8.2414(7.3806) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.4846(17.3429) | Bit/dim 3.6983(3.7028) | Xent 0.6693(0.7074) | Loss 9.0110(9.6631) | Error 0.2244(0.2502) Steps 676(675.72) | Grad Norm 4.2136(7.6487) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 17.1538(17.3726) | Bit/dim 3.7433(3.6993) | Xent 0.6756(0.7029) | Loss 9.1098(9.5106) | Error 0.2356(0.2479) Steps 658(678.38) | Grad Norm 7.1967(7.4307) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.4501(17.3977) | Bit/dim 3.6832(3.6973) | Xent 0.7174(0.6944) | Loss 9.1859(9.3974) | Error 0.2578(0.2458) Steps 682(679.09) | Grad Norm 6.3217(6.7679) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.3129(17.3559) | Bit/dim 3.7030(3.6991) | Xent 0.6955(0.6982) | Loss 9.1015(9.3309) | Error 0.2467(0.2463) Steps 658(676.64) | Grad Norm 11.1194(7.4163) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 16.3354(17.3006) | Bit/dim 3.7005(3.7029) | Xent 0.6835(0.7047) | Loss 9.0154(9.2847) | Error 0.2467(0.2483) Steps 658(674.79) | Grad Norm 8.6593(7.6922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 90.1240, Epoch Time 1060.2280(942.5833), Bit/dim 3.7017(best: 3.7031), Xent 0.7446, Loss 4.0740, Error 0.2597(best: 0.2608)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 17.3317(17.3259) | Bit/dim 3.6871(3.7029) | Xent 0.6742(0.7037) | Loss 9.0083(9.7752) | Error 0.2622(0.2494) Steps 670(679.46) | Grad Norm 6.7981(7.7521) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 16.8105(17.3155) | Bit/dim 3.7157(3.7056) | Xent 0.7020(0.7014) | Loss 9.1436(9.6114) | Error 0.2622(0.2488) Steps 646(677.66) | Grad Norm 7.2347(7.7266) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.0517(17.2989) | Bit/dim 3.6996(3.7054) | Xent 0.6765(0.7005) | Loss 9.0478(9.4820) | Error 0.2356(0.2474) Steps 640(677.65) | Grad Norm 6.6125(7.4042) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 18.4411(17.3610) | Bit/dim 3.7040(3.7033) | Xent 0.7066(0.7021) | Loss 9.2136(9.3998) | Error 0.2633(0.2492) Steps 694(677.80) | Grad Norm 13.0375(7.6933) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.8233(17.3677) | Bit/dim 3.6926(3.7037) | Xent 0.7668(0.7064) | Loss 9.0969(9.3409) | Error 0.2433(0.2491) Steps 658(677.86) | Grad Norm 6.2162(7.6657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 88.8594, Epoch Time 1062.6358(946.1849), Bit/dim 3.6919(best: 3.7017), Xent 0.7314, Loss 4.0576, Error 0.2547(best: 0.2597)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.3181(17.3949) | Bit/dim 3.6964(3.6985) | Xent 0.6571(0.6945) | Loss 8.9987(9.8931) | Error 0.2267(0.2444) Steps 658(676.85) | Grad Norm 6.1460(7.0668) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 16.8212(17.3834) | Bit/dim 3.6588(3.6957) | Xent 0.6715(0.6871) | Loss 8.9822(9.6881) | Error 0.2233(0.2415) Steps 694(679.46) | Grad Norm 7.6706(6.7636) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 17.3196(17.4768) | Bit/dim 3.7203(3.6962) | Xent 0.7038(0.6897) | Loss 9.2031(9.5460) | Error 0.2378(0.2432) Steps 676(680.05) | Grad Norm 8.6234(7.2430) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 18.7875(17.4729) | Bit/dim 3.7175(3.6987) | Xent 0.6977(0.6869) | Loss 9.1770(9.4371) | Error 0.2556(0.2426) Steps 646(679.91) | Grad Norm 6.0955(7.1191) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 16.8596(17.4574) | Bit/dim 3.7053(3.6988) | Xent 0.7416(0.6918) | Loss 9.0831(9.3523) | Error 0.2544(0.2444) Steps 664(677.48) | Grad Norm 12.5983(7.4011) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.1603(17.3812) | Bit/dim 3.7329(3.6999) | Xent 0.6410(0.6886) | Loss 9.1218(9.2960) | Error 0.2167(0.2422) Steps 682(677.69) | Grad Norm 5.7656(7.2155) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 90.5316, Epoch Time 1067.7579(949.8321), Bit/dim 3.6963(best: 3.6919), Xent 0.7251, Loss 4.0588, Error 0.2545(best: 0.2547)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 17.6001(17.4253) | Bit/dim 3.6974(3.6985) | Xent 0.6648(0.6840) | Loss 9.1742(9.7879) | Error 0.2300(0.2404) Steps 688(680.92) | Grad Norm 6.8057(7.0138) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 16.2389(17.2768) | Bit/dim 3.6798(3.6979) | Xent 0.6504(0.6780) | Loss 8.9540(9.6074) | Error 0.2200(0.2377) Steps 676(681.34) | Grad Norm 7.8161(6.9405) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 18.6760(17.3097) | Bit/dim 3.7039(3.6985) | Xent 0.7405(0.7018) | Loss 9.1277(9.5117) | Error 0.2556(0.2480) Steps 688(680.73) | Grad Norm 9.3260(8.1731) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 16.4386(17.3237) | Bit/dim 3.7279(3.6976) | Xent 0.7108(0.7055) | Loss 9.1093(9.4127) | Error 0.2656(0.2500) Steps 658(683.45) | Grad Norm 6.8638(8.1316) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 17.1779(17.3138) | Bit/dim 3.7029(3.6985) | Xent 0.6373(0.6938) | Loss 9.1089(9.3281) | Error 0.2244(0.2461) Steps 688(683.88) | Grad Norm 5.4903(7.4991) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 90.0370, Epoch Time 1060.2831(953.1456), Bit/dim 3.7024(best: 3.6919), Xent 0.7195, Loss 4.0621, Error 0.2467(best: 0.2545)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.8878(17.3612) | Bit/dim 3.7089(3.6983) | Xent 0.6339(0.6814) | Loss 9.1922(9.8788) | Error 0.2167(0.2415) Steps 724(685.08) | Grad Norm 5.8970(6.9270) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 17.6831(17.3842) | Bit/dim 3.6799(3.6993) | Xent 0.6273(0.6767) | Loss 8.9893(9.6775) | Error 0.2256(0.2400) Steps 664(682.15) | Grad Norm 9.4876(7.2893) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 17.3480(17.4039) | Bit/dim 3.6942(3.6981) | Xent 0.6478(0.6726) | Loss 9.0666(9.5220) | Error 0.2389(0.2386) Steps 694(681.16) | Grad Norm 5.4650(7.3234) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 17.3672(17.4629) | Bit/dim 3.7190(3.6964) | Xent 0.7578(0.6745) | Loss 9.2776(9.4145) | Error 0.2711(0.2389) Steps 706(681.38) | Grad Norm 7.2563(7.2459) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 18.5261(17.4601) | Bit/dim 3.7137(3.6947) | Xent 0.6262(0.6757) | Loss 9.0964(9.3278) | Error 0.2089(0.2391) Steps 712(684.61) | Grad Norm 7.2373(7.3728) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.5872(17.4715) | Bit/dim 3.7026(3.6967) | Xent 0.7456(0.6868) | Loss 9.1122(9.2793) | Error 0.2578(0.2432) Steps 652(684.40) | Grad Norm 7.8786(7.7506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 91.1503, Epoch Time 1071.2855(956.6898), Bit/dim 3.7069(best: 3.6919), Xent 0.7575, Loss 4.0856, Error 0.2625(best: 0.2467)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 17.5911(17.5152) | Bit/dim 3.7386(3.7014) | Xent 0.6889(0.6837) | Loss 9.2364(9.7572) | Error 0.2500(0.2420) Steps 700(690.40) | Grad Norm 7.8638(7.9544) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 18.2631(17.5293) | Bit/dim 3.6604(3.7032) | Xent 0.7320(0.6896) | Loss 9.2400(9.6048) | Error 0.2478(0.2432) Steps 724(690.91) | Grad Norm 9.9558(8.0910) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_002_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_lars_0_002_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0 --trust_coefficient 0.002 --clip True\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
