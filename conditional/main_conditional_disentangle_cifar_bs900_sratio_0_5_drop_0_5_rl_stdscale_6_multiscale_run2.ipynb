{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_multiscale.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z_sup, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_sup = torch.cat(z_sup, 1)\n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup = model.module.dropout(z_sup)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "            \n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_multiscale_run2/epoch_250_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_multiscale_run2', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 13760 | Time 20.5028(21.1084) | Bit/dim 3.5369(3.5491) | Xent 0.0082(0.0380) | Loss 8.9508(9.6944) | Error 0.0011(0.0113) Steps 778(781.75) | Grad Norm 1.8155(4.2270) | Total Time 0.00(0.00)\n",
      "Iter 13770 | Time 20.7256(20.9431) | Bit/dim 3.4945(3.5451) | Xent 0.0075(0.0329) | Loss 8.7382(9.4660) | Error 0.0022(0.0099) Steps 820(784.08) | Grad Norm 1.1028(3.7360) | Total Time 0.00(0.00)\n",
      "Iter 13780 | Time 20.0379(20.7680) | Bit/dim 3.5508(3.5420) | Xent 0.0197(0.0289) | Loss 8.8789(9.2925) | Error 0.0044(0.0085) Steps 760(783.39) | Grad Norm 2.7086(3.2564) | Total Time 0.00(0.00)\n",
      "Iter 13790 | Time 20.5628(20.6962) | Bit/dim 3.5142(3.5405) | Xent 0.0205(0.0245) | Loss 8.5955(9.1690) | Error 0.0089(0.0073) Steps 790(785.48) | Grad Norm 2.4199(2.8043) | Total Time 0.00(0.00)\n",
      "Iter 13800 | Time 19.7337(20.6250) | Bit/dim 3.5429(3.5384) | Xent 0.0135(0.0215) | Loss 8.8505(9.0753) | Error 0.0033(0.0061) Steps 772(784.06) | Grad Norm 1.0756(2.4502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 101.0753, Epoch Time 1270.3488(1194.5779), Bit/dim 3.5527(best: inf), Xent 1.9310, Loss 4.5182, Error 0.3246(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 19.3576(20.5829) | Bit/dim 3.4931(3.5335) | Xent 0.0070(0.0192) | Loss 8.6673(9.7853) | Error 0.0011(0.0054) Steps 778(785.37) | Grad Norm 1.1676(2.2258) | Total Time 0.00(0.00)\n",
      "Iter 13820 | Time 20.4574(20.5021) | Bit/dim 3.5467(3.5304) | Xent 0.0089(0.0163) | Loss 8.8499(9.5225) | Error 0.0022(0.0043) Steps 784(784.58) | Grad Norm 1.3203(1.9330) | Total Time 0.00(0.00)\n",
      "Iter 13830 | Time 20.5208(20.4744) | Bit/dim 3.5621(3.5318) | Xent 0.0066(0.0149) | Loss 8.8425(9.3470) | Error 0.0022(0.0040) Steps 784(786.10) | Grad Norm 1.1870(1.8100) | Total Time 0.00(0.00)\n",
      "Iter 13840 | Time 19.5021(20.4409) | Bit/dim 3.5316(3.5305) | Xent 0.0087(0.0131) | Loss 8.7568(9.1964) | Error 0.0011(0.0034) Steps 772(785.30) | Grad Norm 1.5281(1.6654) | Total Time 0.00(0.00)\n",
      "Iter 13850 | Time 20.3980(20.4483) | Bit/dim 3.5242(3.5297) | Xent 0.0080(0.0123) | Loss 8.7700(9.0906) | Error 0.0033(0.0032) Steps 784(784.91) | Grad Norm 1.5667(1.5543) | Total Time 0.00(0.00)\n",
      "Iter 13860 | Time 20.7449(20.4092) | Bit/dim 3.5165(3.5265) | Xent 0.0047(0.0113) | Loss 8.8625(9.0064) | Error 0.0000(0.0029) Steps 778(783.29) | Grad Norm 0.8842(1.4715) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 97.8322, Epoch Time 1243.3813(1196.0420), Bit/dim 3.5449(best: 3.5527), Xent 1.9749, Loss 4.5324, Error 0.3248(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 20.8277(20.4770) | Bit/dim 3.5283(3.5256) | Xent 0.0040(0.0108) | Loss 8.7580(9.6236) | Error 0.0000(0.0026) Steps 754(780.96) | Grad Norm 0.7850(1.4232) | Total Time 0.00(0.00)\n",
      "Iter 13880 | Time 21.3668(20.4974) | Bit/dim 3.5541(3.5265) | Xent 0.0086(0.0100) | Loss 8.9047(9.4077) | Error 0.0022(0.0025) Steps 814(782.85) | Grad Norm 1.0702(1.3409) | Total Time 0.00(0.00)\n",
      "Iter 13890 | Time 20.4400(20.5040) | Bit/dim 3.5048(3.5254) | Xent 0.0108(0.0095) | Loss 8.7326(9.2431) | Error 0.0033(0.0024) Steps 766(780.66) | Grad Norm 1.3583(1.3405) | Total Time 0.00(0.00)\n",
      "Iter 13900 | Time 20.0734(20.4763) | Bit/dim 3.5106(3.5218) | Xent 0.0076(0.0090) | Loss 8.8169(9.1181) | Error 0.0022(0.0023) Steps 772(780.66) | Grad Norm 2.7341(1.3431) | Total Time 0.00(0.00)\n",
      "Iter 13910 | Time 20.8279(20.5422) | Bit/dim 3.5302(3.5230) | Xent 0.0059(0.0088) | Loss 8.7530(9.0403) | Error 0.0011(0.0024) Steps 784(781.54) | Grad Norm 1.6357(1.3849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 98.1386, Epoch Time 1251.0170(1197.6912), Bit/dim 3.5448(best: 3.5449), Xent 1.9966, Loss 4.5431, Error 0.3256(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 21.0437(20.5969) | Bit/dim 3.5577(3.5247) | Xent 0.0034(0.0083) | Loss 8.8939(9.7835) | Error 0.0000(0.0021) Steps 790(778.95) | Grad Norm 0.4700(1.3229) | Total Time 0.00(0.00)\n",
      "Iter 13930 | Time 20.8349(20.6140) | Bit/dim 3.4676(3.5212) | Xent 0.0050(0.0081) | Loss 8.6282(9.5185) | Error 0.0011(0.0020) Steps 766(781.64) | Grad Norm 1.1687(1.3578) | Total Time 0.00(0.00)\n",
      "Iter 13940 | Time 21.0337(20.6093) | Bit/dim 3.5339(3.5214) | Xent 0.0044(0.0074) | Loss 8.7885(9.3222) | Error 0.0011(0.0018) Steps 808(781.84) | Grad Norm 0.5656(1.2497) | Total Time 0.00(0.00)\n",
      "Iter 13950 | Time 21.1927(20.6199) | Bit/dim 3.4985(3.5206) | Xent 0.0109(0.0073) | Loss 8.7307(9.1782) | Error 0.0033(0.0018) Steps 784(780.71) | Grad Norm 1.9848(1.2186) | Total Time 0.00(0.00)\n",
      "Iter 13960 | Time 20.4043(20.5776) | Bit/dim 3.5489(3.5236) | Xent 0.0080(0.0073) | Loss 8.7868(9.0722) | Error 0.0022(0.0017) Steps 784(780.83) | Grad Norm 0.9064(1.1975) | Total Time 0.00(0.00)\n",
      "Iter 13970 | Time 19.9530(20.5498) | Bit/dim 3.5390(3.5240) | Xent 0.0114(0.0069) | Loss 8.8575(9.0005) | Error 0.0022(0.0016) Steps 778(782.37) | Grad Norm 1.0582(1.1319) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 98.4994, Epoch Time 1253.2738(1199.3587), Bit/dim 3.5453(best: 3.5448), Xent 2.0564, Loss 4.5735, Error 0.3274(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 20.5052(20.4901) | Bit/dim 3.5324(3.5264) | Xent 0.0033(0.0066) | Loss 8.8552(9.6255) | Error 0.0000(0.0014) Steps 772(781.60) | Grad Norm 0.4846(1.1063) | Total Time 0.00(0.00)\n",
      "Iter 13990 | Time 20.5132(20.4781) | Bit/dim 3.5307(3.5254) | Xent 0.0035(0.0064) | Loss 8.8616(9.4148) | Error 0.0000(0.0014) Steps 790(784.17) | Grad Norm 0.8494(1.0720) | Total Time 0.00(0.00)\n",
      "Iter 14000 | Time 21.2568(20.4690) | Bit/dim 3.5551(3.5234) | Xent 0.0070(0.0061) | Loss 8.9524(9.2388) | Error 0.0022(0.0013) Steps 820(784.24) | Grad Norm 1.5458(1.0382) | Total Time 0.00(0.00)\n",
      "Iter 14010 | Time 19.9319(20.4664) | Bit/dim 3.4989(3.5213) | Xent 0.0119(0.0064) | Loss 8.7545(9.1078) | Error 0.0033(0.0015) Steps 766(784.16) | Grad Norm 2.2677(1.1232) | Total Time 0.00(0.00)\n",
      "Iter 14020 | Time 20.5364(20.4884) | Bit/dim 3.5354(3.5201) | Xent 0.0038(0.0061) | Loss 8.8621(9.0086) | Error 0.0011(0.0014) Steps 778(783.86) | Grad Norm 0.9970(1.1679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 97.0665, Epoch Time 1242.4090(1200.6502), Bit/dim 3.5454(best: 3.5448), Xent 2.0765, Loss 4.5837, Error 0.3279(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 20.7543(20.5021) | Bit/dim 3.5133(3.5193) | Xent 0.0025(0.0064) | Loss 8.7137(9.7398) | Error 0.0011(0.0016) Steps 784(783.39) | Grad Norm 0.6555(1.2160) | Total Time 0.00(0.00)\n",
      "Iter 14040 | Time 20.2099(20.4938) | Bit/dim 3.4779(3.5194) | Xent 0.0070(0.0061) | Loss 8.7626(9.4933) | Error 0.0033(0.0015) Steps 808(784.10) | Grad Norm 1.5608(1.1685) | Total Time 0.00(0.00)\n",
      "Iter 14050 | Time 19.8912(20.4814) | Bit/dim 3.5122(3.5191) | Xent 0.0039(0.0060) | Loss 8.8595(9.3043) | Error 0.0011(0.0014) Steps 796(786.53) | Grad Norm 0.9796(1.1121) | Total Time 0.00(0.00)\n",
      "Iter 14060 | Time 20.6861(20.5463) | Bit/dim 3.5195(3.5195) | Xent 0.0061(0.0062) | Loss 8.7815(9.1686) | Error 0.0022(0.0015) Steps 778(783.79) | Grad Norm 1.0069(1.1669) | Total Time 0.00(0.00)\n",
      "Iter 14070 | Time 19.7682(20.4667) | Bit/dim 3.4991(3.5212) | Xent 0.0075(0.0064) | Loss 8.6166(9.0602) | Error 0.0022(0.0017) Steps 766(782.52) | Grad Norm 1.9419(1.1978) | Total Time 0.00(0.00)\n",
      "Iter 14080 | Time 20.2814(20.4310) | Bit/dim 3.5131(3.5209) | Xent 0.0032(0.0058) | Loss 8.6430(8.9789) | Error 0.0000(0.0014) Steps 760(781.10) | Grad Norm 0.4932(1.1335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 97.0283, Epoch Time 1243.1806(1201.9261), Bit/dim 3.5432(best: 3.5448), Xent 2.1005, Loss 4.5935, Error 0.3283(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 20.8432(20.4707) | Bit/dim 3.4734(3.5195) | Xent 0.0024(0.0055) | Loss 8.6491(9.5996) | Error 0.0000(0.0014) Steps 790(783.44) | Grad Norm 0.4025(1.0938) | Total Time 0.00(0.00)\n",
      "Iter 14100 | Time 20.2683(20.4838) | Bit/dim 3.5518(3.5197) | Xent 0.0060(0.0055) | Loss 8.7790(9.3770) | Error 0.0011(0.0012) Steps 790(783.04) | Grad Norm 0.8083(1.0958) | Total Time 0.00(0.00)\n",
      "Iter 14110 | Time 19.6945(20.4201) | Bit/dim 3.4922(3.5207) | Xent 0.0054(0.0055) | Loss 8.5761(9.2155) | Error 0.0011(0.0013) Steps 790(782.08) | Grad Norm 0.8777(1.1002) | Total Time 0.00(0.00)\n",
      "Iter 14120 | Time 19.9207(20.4504) | Bit/dim 3.5032(3.5193) | Xent 0.0029(0.0060) | Loss 8.6766(9.0972) | Error 0.0000(0.0015) Steps 760(782.40) | Grad Norm 0.4855(1.1366) | Total Time 0.00(0.00)\n",
      "Iter 14130 | Time 20.9742(20.4938) | Bit/dim 3.4827(3.5193) | Xent 0.0061(0.0056) | Loss 8.6095(9.0080) | Error 0.0011(0.0014) Steps 760(778.11) | Grad Norm 0.6926(1.0623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 97.4689, Epoch Time 1244.7330(1203.2103), Bit/dim 3.5410(best: 3.5432), Xent 2.1061, Loss 4.5941, Error 0.3264(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 20.9340(20.5047) | Bit/dim 3.5588(3.5190) | Xent 0.0107(0.0058) | Loss 8.9240(9.7375) | Error 0.0033(0.0015) Steps 772(780.11) | Grad Norm 1.7018(1.1177) | Total Time 0.00(0.00)\n",
      "Iter 14150 | Time 20.3899(20.5279) | Bit/dim 3.5312(3.5193) | Xent 0.0023(0.0051) | Loss 8.6780(9.4830) | Error 0.0000(0.0011) Steps 784(781.15) | Grad Norm 0.4967(1.0053) | Total Time 0.00(0.00)\n",
      "Iter 14160 | Time 20.3115(20.4903) | Bit/dim 3.4506(3.5187) | Xent 0.0054(0.0053) | Loss 8.7239(9.2940) | Error 0.0011(0.0012) Steps 808(779.91) | Grad Norm 1.1465(1.0903) | Total Time 0.00(0.00)\n",
      "Iter 14170 | Time 20.5769(20.6119) | Bit/dim 3.5601(3.5180) | Xent 0.0075(0.0057) | Loss 8.8634(9.1560) | Error 0.0011(0.0013) Steps 784(780.15) | Grad Norm 1.2259(1.1623) | Total Time 0.00(0.00)\n",
      "Iter 14180 | Time 20.6760(20.6358) | Bit/dim 3.5123(3.5173) | Xent 0.0020(0.0053) | Loss 8.7744(9.0472) | Error 0.0000(0.0011) Steps 772(779.18) | Grad Norm 0.4446(1.0728) | Total Time 0.00(0.00)\n",
      "Iter 14190 | Time 20.4918(20.5745) | Bit/dim 3.4949(3.5168) | Xent 0.0063(0.0049) | Loss 8.6573(8.9681) | Error 0.0022(0.0010) Steps 790(781.65) | Grad Norm 1.3526(0.9509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 97.8022, Epoch Time 1253.0448(1204.7054), Bit/dim 3.5442(best: 3.5410), Xent 2.1408, Loss 4.6147, Error 0.3306(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 21.2220(20.5722) | Bit/dim 3.4865(3.5165) | Xent 0.0017(0.0046) | Loss 8.6954(9.5773) | Error 0.0000(0.0009) Steps 754(780.94) | Grad Norm 0.5284(0.8972) | Total Time 0.00(0.00)\n",
      "Iter 14210 | Time 20.2105(20.5160) | Bit/dim 3.5148(3.5159) | Xent 0.0056(0.0046) | Loss 8.8738(9.3675) | Error 0.0011(0.0009) Steps 772(782.18) | Grad Norm 2.1386(0.9307) | Total Time 0.00(0.00)\n",
      "Iter 14220 | Time 19.8057(20.4399) | Bit/dim 3.4897(3.5162) | Xent 0.0057(0.0045) | Loss 8.6760(9.2138) | Error 0.0011(0.0008) Steps 766(779.91) | Grad Norm 1.1121(0.9353) | Total Time 0.00(0.00)\n",
      "Iter 14230 | Time 20.3599(20.4140) | Bit/dim 3.5336(3.5194) | Xent 0.0034(0.0046) | Loss 8.6289(9.0857) | Error 0.0011(0.0010) Steps 766(779.38) | Grad Norm 0.5412(0.9058) | Total Time 0.00(0.00)\n",
      "Iter 14240 | Time 20.8852(20.4005) | Bit/dim 3.5237(3.5151) | Xent 0.0069(0.0052) | Loss 8.7849(8.9946) | Error 0.0011(0.0011) Steps 808(779.48) | Grad Norm 0.9776(1.0746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 95.9472, Epoch Time 1236.6029(1205.6623), Bit/dim 3.5441(best: 3.5410), Xent 2.1766, Loss 4.6324, Error 0.3313(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 19.8349(20.3987) | Bit/dim 3.5206(3.5187) | Xent 0.0035(0.0050) | Loss 8.7835(9.7289) | Error 0.0011(0.0010) Steps 766(778.85) | Grad Norm 1.3466(1.1444) | Total Time 0.00(0.00)\n",
      "Iter 14260 | Time 20.4528(20.4320) | Bit/dim 3.5261(3.5204) | Xent 0.0101(0.0049) | Loss 8.6895(9.4724) | Error 0.0044(0.0011) Steps 772(778.21) | Grad Norm 1.7416(1.1396) | Total Time 0.00(0.00)\n",
      "Iter 14270 | Time 19.9400(20.4802) | Bit/dim 3.4574(3.5177) | Xent 0.0069(0.0051) | Loss 8.7379(9.2848) | Error 0.0022(0.0012) Steps 778(778.96) | Grad Norm 2.2356(1.1744) | Total Time 0.00(0.00)\n",
      "Iter 14280 | Time 20.6540(20.5825) | Bit/dim 3.5495(3.5206) | Xent 0.0077(0.0050) | Loss 8.8214(9.1486) | Error 0.0022(0.0011) Steps 796(780.47) | Grad Norm 1.1677(1.1475) | Total Time 0.00(0.00)\n",
      "Iter 14290 | Time 20.3174(20.5798) | Bit/dim 3.4660(3.5164) | Xent 0.0040(0.0051) | Loss 8.8149(9.0468) | Error 0.0000(0.0011) Steps 772(780.85) | Grad Norm 1.3932(1.1521) | Total Time 0.00(0.00)\n",
      "Iter 14300 | Time 20.8545(20.6283) | Bit/dim 3.5628(3.5154) | Xent 0.0020(0.0047) | Loss 8.8633(8.9663) | Error 0.0011(0.0010) Steps 814(781.59) | Grad Norm 0.7920(1.1201) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 97.4617, Epoch Time 1258.6147(1207.2509), Bit/dim 3.5402(best: 3.5410), Xent 2.1950, Loss 4.6377, Error 0.3321(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 21.0432(20.6654) | Bit/dim 3.4960(3.5159) | Xent 0.0019(0.0044) | Loss 8.6844(9.5884) | Error 0.0000(0.0010) Steps 778(781.49) | Grad Norm 0.7470(1.0619) | Total Time 0.00(0.00)\n",
      "Iter 14320 | Time 20.7032(20.6841) | Bit/dim 3.4924(3.5144) | Xent 0.0012(0.0045) | Loss 8.6954(9.3657) | Error 0.0000(0.0010) Steps 784(782.05) | Grad Norm 0.6807(1.0722) | Total Time 0.00(0.00)\n",
      "Iter 14330 | Time 20.8805(20.6840) | Bit/dim 3.4944(3.5140) | Xent 0.0045(0.0044) | Loss 8.7208(9.2096) | Error 0.0011(0.0011) Steps 772(781.76) | Grad Norm 0.9921(1.1072) | Total Time 0.00(0.00)\n",
      "Iter 14340 | Time 20.1481(20.5757) | Bit/dim 3.5221(3.5152) | Xent 0.0054(0.0042) | Loss 8.8437(9.0909) | Error 0.0011(0.0010) Steps 772(779.85) | Grad Norm 1.2733(1.1132) | Total Time 0.00(0.00)\n",
      "Iter 14350 | Time 21.0503(20.5642) | Bit/dim 3.5224(3.5169) | Xent 0.0032(0.0039) | Loss 8.6398(8.9992) | Error 0.0000(0.0008) Steps 778(777.75) | Grad Norm 0.7582(1.0142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 97.7072, Epoch Time 1251.9530(1208.5919), Bit/dim 3.5430(best: 3.5402), Xent 2.2172, Loss 4.6516, Error 0.3268(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 20.3915(20.5454) | Bit/dim 3.5192(3.5149) | Xent 0.0047(0.0037) | Loss 8.7917(9.7272) | Error 0.0022(0.0008) Steps 784(777.82) | Grad Norm 1.1919(0.9960) | Total Time 0.00(0.00)\n",
      "Iter 14370 | Time 20.3864(20.4535) | Bit/dim 3.5192(3.5158) | Xent 0.0099(0.0037) | Loss 8.8232(9.4726) | Error 0.0033(0.0009) Steps 802(778.60) | Grad Norm 1.4047(0.9655) | Total Time 0.00(0.00)\n",
      "Iter 14380 | Time 21.1668(20.5074) | Bit/dim 3.5440(3.5131) | Xent 0.0077(0.0040) | Loss 8.9349(9.2823) | Error 0.0022(0.0009) Steps 784(779.40) | Grad Norm 1.9394(1.0510) | Total Time 0.00(0.00)\n",
      "Iter 14390 | Time 20.8739(20.5592) | Bit/dim 3.5233(3.5150) | Xent 0.0037(0.0045) | Loss 8.6481(9.1505) | Error 0.0011(0.0010) Steps 778(781.78) | Grad Norm 2.3945(1.0913) | Total Time 0.00(0.00)\n",
      "Iter 14400 | Time 20.1810(20.5043) | Bit/dim 3.5227(3.5155) | Xent 0.0039(0.0043) | Loss 8.7236(9.0557) | Error 0.0022(0.0010) Steps 742(779.98) | Grad Norm 1.2839(1.0467) | Total Time 0.00(0.00)\n",
      "Iter 14410 | Time 20.3848(20.5070) | Bit/dim 3.5346(3.5165) | Xent 0.0107(0.0049) | Loss 8.7879(8.9648) | Error 0.0022(0.0011) Steps 796(782.37) | Grad Norm 0.8340(1.1004) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 96.8193, Epoch Time 1245.1441(1209.6885), Bit/dim 3.5441(best: 3.5402), Xent 2.2279, Loss 4.6581, Error 0.3318(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 20.7043(20.6040) | Bit/dim 3.5156(3.5176) | Xent 0.0081(0.0046) | Loss 8.8577(9.6073) | Error 0.0033(0.0011) Steps 802(784.59) | Grad Norm 2.2426(1.0992) | Total Time 0.00(0.00)\n",
      "Iter 14430 | Time 20.8580(20.6082) | Bit/dim 3.4872(3.5161) | Xent 0.0042(0.0044) | Loss 8.6600(9.3766) | Error 0.0022(0.0011) Steps 754(784.66) | Grad Norm 1.0382(1.0981) | Total Time 0.00(0.00)\n",
      "Iter 14440 | Time 20.9467(20.6944) | Bit/dim 3.5095(3.5159) | Xent 0.0028(0.0045) | Loss 8.5674(9.2104) | Error 0.0011(0.0012) Steps 814(785.16) | Grad Norm 1.3243(1.1324) | Total Time 0.00(0.00)\n",
      "Iter 14450 | Time 20.0242(20.5947) | Bit/dim 3.5124(3.5164) | Xent 0.0021(0.0041) | Loss 8.7102(9.0843) | Error 0.0000(0.0010) Steps 772(783.59) | Grad Norm 0.5266(1.0697) | Total Time 0.00(0.00)\n",
      "Iter 14460 | Time 20.3301(20.5538) | Bit/dim 3.5037(3.5127) | Xent 0.0045(0.0041) | Loss 8.7166(8.9895) | Error 0.0011(0.0010) Steps 790(784.56) | Grad Norm 1.4624(1.2095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 98.0276, Epoch Time 1251.4342(1210.9409), Bit/dim 3.5456(best: 3.5402), Xent 2.2566, Loss 4.6739, Error 0.3282(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 21.2233(20.5476) | Bit/dim 3.4958(3.5128) | Xent 0.0018(0.0040) | Loss 8.7766(9.7006) | Error 0.0000(0.0010) Steps 802(782.85) | Grad Norm 0.7326(1.2106) | Total Time 0.00(0.00)\n",
      "Iter 14480 | Time 20.2510(20.5936) | Bit/dim 3.5484(3.5153) | Xent 0.0093(0.0047) | Loss 8.8828(9.4520) | Error 0.0022(0.0011) Steps 796(782.02) | Grad Norm 1.9117(1.2769) | Total Time 0.00(0.00)\n",
      "Iter 14490 | Time 20.1321(20.5516) | Bit/dim 3.5092(3.5151) | Xent 0.0020(0.0043) | Loss 8.8075(9.2631) | Error 0.0000(0.0010) Steps 796(781.34) | Grad Norm 0.7683(1.1628) | Total Time 0.00(0.00)\n",
      "Iter 14500 | Time 20.4461(20.5403) | Bit/dim 3.4930(3.5149) | Xent 0.0084(0.0041) | Loss 8.5695(9.1325) | Error 0.0011(0.0009) Steps 772(782.07) | Grad Norm 0.5576(1.1618) | Total Time 0.00(0.00)\n",
      "Iter 14510 | Time 20.2391(20.5731) | Bit/dim 3.5514(3.5151) | Xent 0.0055(0.0041) | Loss 8.7783(9.0233) | Error 0.0022(0.0009) Steps 790(782.75) | Grad Norm 1.4731(1.1238) | Total Time 0.00(0.00)\n",
      "Iter 14520 | Time 21.0498(20.5692) | Bit/dim 3.4789(3.5146) | Xent 0.0058(0.0044) | Loss 8.7531(8.9505) | Error 0.0011(0.0010) Steps 808(783.03) | Grad Norm 0.7410(1.1114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 97.9896, Epoch Time 1252.2780(1212.1810), Bit/dim 3.5438(best: 3.5402), Xent 2.2614, Loss 4.6745, Error 0.3308(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 20.5664(20.5764) | Bit/dim 3.5417(3.5155) | Xent 0.0034(0.0039) | Loss 8.8598(9.5591) | Error 0.0011(0.0009) Steps 826(784.30) | Grad Norm 0.8802(0.9951) | Total Time 0.00(0.00)\n",
      "Iter 14540 | Time 20.0152(20.5302) | Bit/dim 3.5118(3.5149) | Xent 0.0040(0.0038) | Loss 8.6813(9.3386) | Error 0.0011(0.0008) Steps 778(784.40) | Grad Norm 1.4278(0.9396) | Total Time 0.00(0.00)\n",
      "Iter 14550 | Time 20.9356(20.6002) | Bit/dim 3.4845(3.5148) | Xent 0.0026(0.0037) | Loss 8.8204(9.1846) | Error 0.0000(0.0008) Steps 814(783.94) | Grad Norm 0.8904(0.9478) | Total Time 0.00(0.00)\n",
      "Iter 14560 | Time 20.2319(20.6270) | Bit/dim 3.4892(3.5166) | Xent 0.0030(0.0037) | Loss 8.6638(9.0770) | Error 0.0000(0.0008) Steps 784(782.52) | Grad Norm 1.0765(1.0080) | Total Time 0.00(0.00)\n",
      "Iter 14570 | Time 20.3567(20.6062) | Bit/dim 3.5002(3.5144) | Xent 0.0023(0.0036) | Loss 8.6350(8.9860) | Error 0.0000(0.0007) Steps 754(781.62) | Grad Norm 0.4802(0.9867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 96.3092, Epoch Time 1255.6375(1213.4847), Bit/dim 3.5453(best: 3.5402), Xent 2.2918, Loss 4.6912, Error 0.3349(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 20.3465(20.6190) | Bit/dim 3.5301(3.5167) | Xent 0.0023(0.0034) | Loss 8.7875(9.6968) | Error 0.0011(0.0007) Steps 790(782.79) | Grad Norm 0.7700(0.9379) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 19.7606(20.5646) | Bit/dim 3.5127(3.5181) | Xent 0.0048(0.0034) | Loss 8.8091(9.4556) | Error 0.0033(0.0007) Steps 790(784.85) | Grad Norm 1.4647(0.9571) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 20.5972(20.4424) | Bit/dim 3.5442(3.5155) | Xent 0.0042(0.0037) | Loss 8.8705(9.2762) | Error 0.0022(0.0010) Steps 784(785.21) | Grad Norm 1.7628(1.0543) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 20.5887(20.4550) | Bit/dim 3.5317(3.5129) | Xent 0.0019(0.0037) | Loss 8.7929(9.1341) | Error 0.0000(0.0010) Steps 784(785.28) | Grad Norm 0.5932(1.0885) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 20.1665(20.4407) | Bit/dim 3.5159(3.5152) | Xent 0.0039(0.0035) | Loss 8.7408(9.0394) | Error 0.0011(0.0009) Steps 766(784.30) | Grad Norm 0.9797(1.0723) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 20.0982(20.4337) | Bit/dim 3.4844(3.5120) | Xent 0.0089(0.0039) | Loss 8.7191(8.9622) | Error 0.0011(0.0009) Steps 784(783.35) | Grad Norm 1.5028(1.2049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 96.9228, Epoch Time 1238.6565(1214.2398), Bit/dim 3.5398(best: 3.5402), Xent 2.3130, Loss 4.6963, Error 0.3328(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 20.4957(20.4075) | Bit/dim 3.5045(3.5110) | Xent 0.0048(0.0041) | Loss 8.7488(9.5787) | Error 0.0011(0.0009) Steps 778(781.65) | Grad Norm 1.1255(1.2953) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 20.3714(20.4340) | Bit/dim 3.5495(3.5125) | Xent 0.0016(0.0039) | Loss 8.8854(9.3677) | Error 0.0000(0.0009) Steps 796(783.77) | Grad Norm 0.5226(1.2566) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 20.1495(20.4869) | Bit/dim 3.5068(3.5147) | Xent 0.0164(0.0041) | Loss 8.7380(9.2005) | Error 0.0044(0.0009) Steps 796(782.97) | Grad Norm 2.6608(1.2215) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 21.4183(20.5471) | Bit/dim 3.5077(3.5135) | Xent 0.0018(0.0044) | Loss 8.7520(9.0786) | Error 0.0000(0.0010) Steps 754(781.45) | Grad Norm 1.1331(1.2373) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 20.1965(20.5058) | Bit/dim 3.5341(3.5128) | Xent 0.0021(0.0040) | Loss 8.9260(8.9845) | Error 0.0000(0.0009) Steps 790(781.73) | Grad Norm 0.6443(1.2106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 96.2479, Epoch Time 1247.3733(1215.2338), Bit/dim 3.5404(best: 3.5398), Xent 2.3067, Loss 4.6938, Error 0.3323(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 20.0646(20.4843) | Bit/dim 3.5179(3.5137) | Xent 0.0098(0.0042) | Loss 8.6865(9.6953) | Error 0.0022(0.0010) Steps 760(782.04) | Grad Norm 2.1584(1.2710) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 20.4234(20.4454) | Bit/dim 3.4781(3.5136) | Xent 0.0022(0.0041) | Loss 8.6891(9.4500) | Error 0.0000(0.0010) Steps 760(780.69) | Grad Norm 0.7249(1.2329) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 19.6349(20.4001) | Bit/dim 3.5240(3.5148) | Xent 0.0037(0.0039) | Loss 8.8144(9.2654) | Error 0.0011(0.0010) Steps 778(781.03) | Grad Norm 1.5718(1.2310) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 20.3403(20.4086) | Bit/dim 3.4984(3.5122) | Xent 0.0038(0.0037) | Loss 8.7389(9.1180) | Error 0.0022(0.0010) Steps 808(782.07) | Grad Norm 1.2518(1.2264) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 20.5693(20.4252) | Bit/dim 3.5425(3.5143) | Xent 0.0012(0.0043) | Loss 8.6752(9.0277) | Error 0.0000(0.0011) Steps 772(781.67) | Grad Norm 1.0223(1.3213) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 19.6592(20.4510) | Bit/dim 3.4736(3.5143) | Xent 0.0075(0.0043) | Loss 8.7137(8.9572) | Error 0.0011(0.0011) Steps 778(781.56) | Grad Norm 2.0885(1.3353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 97.5990, Epoch Time 1241.1294(1216.0107), Bit/dim 3.5410(best: 3.5398), Xent 2.3429, Loss 4.7124, Error 0.3350(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 20.5763(20.4558) | Bit/dim 3.5162(3.5134) | Xent 0.0026(0.0039) | Loss 8.8618(9.5817) | Error 0.0011(0.0009) Steps 784(783.40) | Grad Norm 0.8622(1.3337) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 20.4414(20.4744) | Bit/dim 3.5132(3.5133) | Xent 0.0029(0.0040) | Loss 8.6728(9.3548) | Error 0.0011(0.0010) Steps 796(784.26) | Grad Norm 0.9661(1.3869) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 20.5018(20.5144) | Bit/dim 3.5209(3.5133) | Xent 0.0013(0.0036) | Loss 8.7594(9.1952) | Error 0.0000(0.0009) Steps 796(785.36) | Grad Norm 0.4885(1.2845) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 20.8263(20.6083) | Bit/dim 3.5203(3.5115) | Xent 0.0022(0.0037) | Loss 8.7558(9.0679) | Error 0.0011(0.0009) Steps 766(783.21) | Grad Norm 1.0153(1.2363) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 20.6130(20.5414) | Bit/dim 3.4984(3.5137) | Xent 0.0043(0.0036) | Loss 8.5771(8.9824) | Error 0.0011(0.0009) Steps 784(784.51) | Grad Norm 1.8872(1.2101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 98.3007, Epoch Time 1250.3845(1217.0419), Bit/dim 3.5420(best: 3.5398), Xent 2.3659, Loss 4.7250, Error 0.3337(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 20.5452(20.5350) | Bit/dim 3.5073(3.5140) | Xent 0.0036(0.0036) | Loss 8.7589(9.6947) | Error 0.0022(0.0008) Steps 790(785.63) | Grad Norm 1.2319(1.1930) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 20.1386(20.6021) | Bit/dim 3.5162(3.5149) | Xent 0.0031(0.0036) | Loss 8.6834(9.4521) | Error 0.0011(0.0009) Steps 766(786.59) | Grad Norm 1.6341(1.2400) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 19.9308(20.6206) | Bit/dim 3.4719(3.5135) | Xent 0.0077(0.0041) | Loss 8.7254(9.2592) | Error 0.0022(0.0011) Steps 784(785.00) | Grad Norm 1.9853(1.2916) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 20.9560(20.6410) | Bit/dim 3.5225(3.5120) | Xent 0.0145(0.0044) | Loss 8.8143(9.1206) | Error 0.0033(0.0011) Steps 790(784.66) | Grad Norm 2.0423(1.2458) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 19.7014(20.6503) | Bit/dim 3.5036(3.5110) | Xent 0.0078(0.0044) | Loss 8.6752(9.0291) | Error 0.0022(0.0011) Steps 772(785.51) | Grad Norm 2.2360(1.2978) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 19.7633(20.5821) | Bit/dim 3.5031(3.5130) | Xent 0.0076(0.0042) | Loss 8.6758(8.9544) | Error 0.0022(0.0010) Steps 784(784.33) | Grad Norm 5.5348(1.3800) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 97.0893, Epoch Time 1255.4099(1218.1929), Bit/dim 3.5434(best: 3.5398), Xent 2.3568, Loss 4.7218, Error 0.3323(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 20.1857(20.6011) | Bit/dim 3.5134(3.5134) | Xent 0.0017(0.0044) | Loss 8.7389(9.5749) | Error 0.0000(0.0011) Steps 796(785.06) | Grad Norm 0.9053(1.4770) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 20.9371(20.5939) | Bit/dim 3.5291(3.5135) | Xent 0.0027(0.0044) | Loss 8.8270(9.3630) | Error 0.0000(0.0011) Steps 796(783.23) | Grad Norm 1.2931(1.4507) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 21.6938(20.6506) | Bit/dim 3.5405(3.5141) | Xent 0.0018(0.0047) | Loss 8.8450(9.2095) | Error 0.0011(0.0012) Steps 820(785.94) | Grad Norm 1.2045(1.4802) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 20.1878(20.6961) | Bit/dim 3.5061(3.5112) | Xent 0.0032(0.0046) | Loss 8.6737(9.0809) | Error 0.0011(0.0011) Steps 796(786.40) | Grad Norm 1.0167(1.4135) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 20.6958(20.7500) | Bit/dim 3.5175(3.5110) | Xent 0.0041(0.0040) | Loss 8.7563(9.0032) | Error 0.0022(0.0010) Steps 808(788.60) | Grad Norm 1.6462(1.2941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 96.9976, Epoch Time 1257.7331(1219.3792), Bit/dim 3.5408(best: 3.5398), Xent 2.3914, Loss 4.7365, Error 0.3340(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 21.0202(20.7318) | Bit/dim 3.5128(3.5130) | Xent 0.0012(0.0040) | Loss 8.7701(9.7416) | Error 0.0000(0.0011) Steps 802(788.76) | Grad Norm 0.7701(1.2788) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 21.0256(20.7142) | Bit/dim 3.5228(3.5136) | Xent 0.0008(0.0035) | Loss 8.8492(9.4869) | Error 0.0000(0.0009) Steps 808(788.00) | Grad Norm 0.4167(1.1748) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 20.6563(20.7676) | Bit/dim 3.5512(3.5149) | Xent 0.0020(0.0038) | Loss 8.8424(9.2942) | Error 0.0011(0.0010) Steps 802(788.33) | Grad Norm 0.5632(1.2244) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 20.4387(20.7713) | Bit/dim 3.5426(3.5137) | Xent 0.0071(0.0038) | Loss 8.8098(9.1563) | Error 0.0022(0.0010) Steps 808(787.63) | Grad Norm 1.4924(1.2247) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 20.2275(20.8194) | Bit/dim 3.5091(3.5144) | Xent 0.0018(0.0036) | Loss 8.6938(9.0512) | Error 0.0000(0.0008) Steps 784(788.59) | Grad Norm 1.2234(1.2147) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 20.9584(20.7965) | Bit/dim 3.5053(3.5092) | Xent 0.0040(0.0036) | Loss 8.8460(8.9532) | Error 0.0011(0.0009) Steps 802(789.56) | Grad Norm 1.0725(1.1917) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 97.1890, Epoch Time 1263.1081(1220.6910), Bit/dim 3.5420(best: 3.5398), Xent 2.3887, Loss 4.7363, Error 0.3343(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 20.9343(20.7969) | Bit/dim 3.5275(3.5105) | Xent 0.0098(0.0041) | Loss 8.8409(9.5873) | Error 0.0011(0.0010) Steps 796(791.18) | Grad Norm 2.1397(1.3267) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 21.0504(20.7878) | Bit/dim 3.5172(3.5116) | Xent 0.0048(0.0040) | Loss 8.7454(9.3704) | Error 0.0011(0.0011) Steps 778(788.11) | Grad Norm 1.3729(1.3537) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 21.2058(20.8216) | Bit/dim 3.5048(3.5127) | Xent 0.0023(0.0042) | Loss 8.6359(9.2013) | Error 0.0011(0.0010) Steps 808(786.00) | Grad Norm 0.8030(1.3855) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 20.3350(20.7962) | Bit/dim 3.4868(3.5089) | Xent 0.0009(0.0038) | Loss 8.7477(9.0688) | Error 0.0000(0.0009) Steps 772(785.88) | Grad Norm 0.6495(1.3568) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 21.2495(20.7035) | Bit/dim 3.4972(3.5090) | Xent 0.0014(0.0037) | Loss 8.8123(8.9821) | Error 0.0000(0.0009) Steps 814(785.89) | Grad Norm 0.9221(1.4649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 97.5072, Epoch Time 1257.9768(1221.8096), Bit/dim 3.5411(best: 3.5398), Xent 2.4039, Loss 4.7431, Error 0.3333(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 19.7337(20.6478) | Bit/dim 3.4678(3.5123) | Xent 0.0014(0.0034) | Loss 8.6675(9.7211) | Error 0.0000(0.0008) Steps 766(784.83) | Grad Norm 0.8675(1.4772) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 21.3059(20.6674) | Bit/dim 3.5222(3.5132) | Xent 0.0010(0.0037) | Loss 8.8207(9.4716) | Error 0.0000(0.0008) Steps 778(784.97) | Grad Norm 0.8157(1.5251) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 20.3752(20.6359) | Bit/dim 3.5185(3.5136) | Xent 0.0157(0.0049) | Loss 8.7550(9.2827) | Error 0.0022(0.0011) Steps 760(784.49) | Grad Norm 1.1719(1.7158) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 21.6009(20.6429) | Bit/dim 3.4965(3.5114) | Xent 0.0008(0.0044) | Loss 8.7420(9.1468) | Error 0.0000(0.0010) Steps 754(783.07) | Grad Norm 0.6924(1.6436) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 20.5532(20.6544) | Bit/dim 3.5007(3.5091) | Xent 0.0080(0.0042) | Loss 8.7058(9.0391) | Error 0.0022(0.0010) Steps 790(785.25) | Grad Norm 2.7401(1.5708) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 21.3691(20.6870) | Bit/dim 3.5275(3.5104) | Xent 0.0110(0.0043) | Loss 8.9014(8.9605) | Error 0.0011(0.0009) Steps 808(786.13) | Grad Norm 1.0346(1.5298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 97.4436, Epoch Time 1254.3458(1222.7857), Bit/dim 3.5399(best: 3.5398), Xent 2.4074, Loss 4.7436, Error 0.3311(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 20.5154(20.6109) | Bit/dim 3.5188(3.5116) | Xent 0.0090(0.0040) | Loss 8.7216(9.5657) | Error 0.0022(0.0008) Steps 790(786.86) | Grad Norm 2.6449(1.4660) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 20.4299(20.5825) | Bit/dim 3.5035(3.5103) | Xent 0.0073(0.0038) | Loss 8.7131(9.3636) | Error 0.0033(0.0008) Steps 766(786.19) | Grad Norm 1.8940(1.3910) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 20.1245(20.5715) | Bit/dim 3.5111(3.5093) | Xent 0.0026(0.0040) | Loss 8.6794(9.1987) | Error 0.0011(0.0009) Steps 790(784.26) | Grad Norm 1.2929(1.4214) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 20.0146(20.5309) | Bit/dim 3.4879(3.5098) | Xent 0.0023(0.0044) | Loss 8.5732(9.0801) | Error 0.0011(0.0010) Steps 784(786.23) | Grad Norm 0.8406(1.4593) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 20.1718(20.5160) | Bit/dim 3.5026(3.5121) | Xent 0.0010(0.0041) | Loss 8.7567(8.9903) | Error 0.0000(0.0009) Steps 790(785.00) | Grad Norm 0.6950(1.4192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 98.1052, Epoch Time 1244.3051(1223.4313), Bit/dim 3.5403(best: 3.5398), Xent 2.3976, Loss 4.7391, Error 0.3321(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 20.3079(20.4901) | Bit/dim 3.4643(3.5095) | Xent 0.0026(0.0042) | Loss 8.6926(9.7003) | Error 0.0011(0.0010) Steps 778(782.98) | Grad Norm 1.6559(1.4553) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 19.8745(20.5010) | Bit/dim 3.5195(3.5079) | Xent 0.0017(0.0040) | Loss 8.6669(9.4423) | Error 0.0000(0.0009) Steps 784(783.28) | Grad Norm 1.0433(1.3754) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 19.8717(20.5834) | Bit/dim 3.5211(3.5103) | Xent 0.0012(0.0038) | Loss 8.8109(9.2720) | Error 0.0000(0.0010) Steps 784(785.47) | Grad Norm 0.6830(1.3441) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 20.6461(20.5719) | Bit/dim 3.4793(3.5084) | Xent 0.0017(0.0035) | Loss 8.6617(9.1239) | Error 0.0000(0.0009) Steps 808(785.79) | Grad Norm 0.9144(1.2718) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 20.5343(20.5298) | Bit/dim 3.5834(3.5131) | Xent 0.0112(0.0039) | Loss 8.8422(9.0294) | Error 0.0022(0.0009) Steps 760(783.20) | Grad Norm 1.1991(1.3601) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 20.6411(20.5414) | Bit/dim 3.5180(3.5133) | Xent 0.0072(0.0039) | Loss 8.7131(8.9509) | Error 0.0033(0.0010) Steps 790(784.58) | Grad Norm 1.8666(1.3660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 97.0969, Epoch Time 1249.1294(1224.2022), Bit/dim 3.5415(best: 3.5398), Xent 2.4119, Loss 4.7474, Error 0.3337(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 20.5154(20.5352) | Bit/dim 3.4717(3.5113) | Xent 0.0030(0.0036) | Loss 8.5851(9.5540) | Error 0.0011(0.0009) Steps 772(783.27) | Grad Norm 0.9985(1.2668) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 20.6435(20.5192) | Bit/dim 3.5211(3.5111) | Xent 0.0017(0.0035) | Loss 8.7344(9.3425) | Error 0.0000(0.0009) Steps 790(783.01) | Grad Norm 1.2236(1.2845) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 19.4704(20.5060) | Bit/dim 3.4582(3.5091) | Xent 0.0009(0.0034) | Loss 8.5838(9.1730) | Error 0.0000(0.0008) Steps 784(782.72) | Grad Norm 0.7628(1.2446) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 19.9286(20.4431) | Bit/dim 3.5003(3.5105) | Xent 0.0010(0.0035) | Loss 8.6775(9.0528) | Error 0.0000(0.0009) Steps 796(783.29) | Grad Norm 0.4956(1.2375) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 19.7522(20.6393) | Bit/dim 3.5372(3.5107) | Xent 0.0046(0.0036) | Loss 8.8059(8.9671) | Error 0.0011(0.0009) Steps 778(786.75) | Grad Norm 2.0802(1.3589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 97.7536, Epoch Time 1252.5151(1225.0516), Bit/dim 3.5431(best: 3.5398), Xent 2.4163, Loss 4.7512, Error 0.3325(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 20.8162(20.6631) | Bit/dim 3.4854(3.5100) | Xent 0.0084(0.0040) | Loss 8.7902(9.6995) | Error 0.0022(0.0011) Steps 796(786.14) | Grad Norm 2.6332(1.4480) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 20.3728(20.6584) | Bit/dim 3.5106(3.5101) | Xent 0.0019(0.0040) | Loss 8.6252(9.4422) | Error 0.0011(0.0011) Steps 772(784.14) | Grad Norm 1.0864(1.4152) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 20.5772(20.6837) | Bit/dim 3.5003(3.5096) | Xent 0.0035(0.0037) | Loss 8.7541(9.2545) | Error 0.0011(0.0010) Steps 778(783.64) | Grad Norm 1.4793(1.3695) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 20.7872(20.6130) | Bit/dim 3.5481(3.5115) | Xent 0.0011(0.0037) | Loss 8.8742(9.1206) | Error 0.0000(0.0010) Steps 760(780.41) | Grad Norm 0.8107(1.3468) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 19.9998(20.6650) | Bit/dim 3.4592(3.5076) | Xent 0.0036(0.0037) | Loss 8.5212(9.0023) | Error 0.0011(0.0009) Steps 772(781.58) | Grad Norm 1.1421(1.2717) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 19.9197(20.6994) | Bit/dim 3.5217(3.5116) | Xent 0.0013(0.0037) | Loss 8.7765(8.9503) | Error 0.0000(0.0009) Steps 778(782.85) | Grad Norm 0.6332(1.2206) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 97.7126, Epoch Time 1257.1235(1226.0137), Bit/dim 3.5378(best: 3.5398), Xent 2.4194, Loss 4.7475, Error 0.3298(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 20.7943(20.6778) | Bit/dim 3.5089(3.5106) | Xent 0.0037(0.0035) | Loss 8.7013(9.5787) | Error 0.0011(0.0008) Steps 790(782.08) | Grad Norm 1.2419(1.1995) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 20.0946(20.5924) | Bit/dim 3.5349(3.5106) | Xent 0.0013(0.0035) | Loss 8.8459(9.3597) | Error 0.0000(0.0008) Steps 784(784.44) | Grad Norm 1.0151(1.2222) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 20.9336(20.6119) | Bit/dim 3.5208(3.5137) | Xent 0.0010(0.0034) | Loss 8.7502(9.2008) | Error 0.0000(0.0008) Steps 778(785.82) | Grad Norm 0.7502(1.2735) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 20.2045(20.6026) | Bit/dim 3.4826(3.5094) | Xent 0.0016(0.0035) | Loss 8.5490(9.0662) | Error 0.0000(0.0008) Steps 760(783.02) | Grad Norm 0.8678(1.2882) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 19.8737(20.5969) | Bit/dim 3.4839(3.5092) | Xent 0.0037(0.0032) | Loss 8.6360(8.9802) | Error 0.0011(0.0008) Steps 790(784.41) | Grad Norm 2.3439(1.2591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 97.0181, Epoch Time 1247.1448(1226.6477), Bit/dim 3.5368(best: 3.5378), Xent 2.4539, Loss 4.7638, Error 0.3341(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 20.3301(20.6061) | Bit/dim 3.4630(3.5083) | Xent 0.0062(0.0036) | Loss 8.6721(9.6864) | Error 0.0022(0.0009) Steps 808(787.24) | Grad Norm 1.4906(1.3593) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 20.5100(20.5835) | Bit/dim 3.5128(3.5072) | Xent 0.0012(0.0033) | Loss 8.8266(9.4324) | Error 0.0000(0.0008) Steps 802(787.79) | Grad Norm 0.8336(1.3013) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 20.5607(20.6074) | Bit/dim 3.5264(3.5085) | Xent 0.0024(0.0034) | Loss 8.8577(9.2534) | Error 0.0011(0.0008) Steps 826(789.84) | Grad Norm 1.0550(1.3665) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 20.2350(20.5309) | Bit/dim 3.5156(3.5106) | Xent 0.0029(0.0031) | Loss 8.7319(9.1243) | Error 0.0011(0.0007) Steps 778(787.45) | Grad Norm 1.6276(1.2870) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 20.8608(20.5979) | Bit/dim 3.5327(3.5103) | Xent 0.0036(0.0028) | Loss 8.8034(9.0272) | Error 0.0011(0.0007) Steps 784(788.51) | Grad Norm 1.1212(1.2141) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 20.5332(20.6376) | Bit/dim 3.5159(3.5106) | Xent 0.0014(0.0030) | Loss 8.7708(8.9495) | Error 0.0000(0.0007) Steps 784(786.86) | Grad Norm 0.8912(1.2014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 96.4267, Epoch Time 1252.8299(1227.4331), Bit/dim 3.5360(best: 3.5368), Xent 2.4730, Loss 4.7725, Error 0.3357(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 21.4730(20.6503) | Bit/dim 3.4998(3.5143) | Xent 0.0094(0.0032) | Loss 8.7132(9.5592) | Error 0.0011(0.0007) Steps 790(786.74) | Grad Norm 1.5971(1.2346) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 21.0374(20.6374) | Bit/dim 3.5016(3.5124) | Xent 0.0015(0.0030) | Loss 8.6524(9.3438) | Error 0.0000(0.0006) Steps 796(783.03) | Grad Norm 0.9272(1.2753) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 20.7400(20.6151) | Bit/dim 3.5143(3.5099) | Xent 0.0016(0.0032) | Loss 8.7821(9.1860) | Error 0.0000(0.0008) Steps 796(784.73) | Grad Norm 1.3498(1.5901) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 21.0655(20.5968) | Bit/dim 3.5101(3.5111) | Xent 0.0049(0.0037) | Loss 8.6698(9.0770) | Error 0.0022(0.0010) Steps 790(784.89) | Grad Norm 2.2737(1.7497) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 20.2107(20.5845) | Bit/dim 3.5098(3.5087) | Xent 0.0011(0.0037) | Loss 8.8319(8.9889) | Error 0.0000(0.0009) Steps 802(787.20) | Grad Norm 0.5716(1.6561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 98.4111, Epoch Time 1252.5952(1228.1880), Bit/dim 3.5380(best: 3.5360), Xent 2.4424, Loss 4.7592, Error 0.3309(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 20.5760(20.6153) | Bit/dim 3.5016(3.5080) | Xent 0.0012(0.0035) | Loss 8.7371(9.7011) | Error 0.0000(0.0009) Steps 778(785.72) | Grad Norm 0.7919(1.5984) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 20.2076(20.6070) | Bit/dim 3.5150(3.5088) | Xent 0.0015(0.0033) | Loss 8.5808(9.4415) | Error 0.0000(0.0008) Steps 778(785.32) | Grad Norm 0.6053(1.4504) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 19.9334(20.6923) | Bit/dim 3.4794(3.5090) | Xent 0.0059(0.0036) | Loss 8.6914(9.2730) | Error 0.0011(0.0009) Steps 790(786.22) | Grad Norm 1.6811(1.4517) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 20.8150(20.7256) | Bit/dim 3.5210(3.5113) | Xent 0.0008(0.0036) | Loss 8.7104(9.1434) | Error 0.0000(0.0009) Steps 814(787.48) | Grad Norm 1.0196(1.5155) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 20.7543(20.7371) | Bit/dim 3.5099(3.5098) | Xent 0.0027(0.0045) | Loss 8.7557(9.0368) | Error 0.0011(0.0011) Steps 796(784.95) | Grad Norm 1.3244(1.6078) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 20.6949(20.7163) | Bit/dim 3.5144(3.5084) | Xent 0.0055(0.0042) | Loss 8.6734(8.9480) | Error 0.0033(0.0010) Steps 778(783.06) | Grad Norm 2.4359(1.6178) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 96.3410, Epoch Time 1258.1759(1229.0876), Bit/dim 3.5349(best: 3.5360), Xent 2.4958, Loss 4.7828, Error 0.3352(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 21.3329(20.7171) | Bit/dim 3.5111(3.5107) | Xent 0.0035(0.0048) | Loss 8.7192(9.5676) | Error 0.0011(0.0012) Steps 796(783.98) | Grad Norm 2.3257(1.7266) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 20.7366(20.7366) | Bit/dim 3.5388(3.5119) | Xent 0.0013(0.0049) | Loss 8.8420(9.3586) | Error 0.0000(0.0012) Steps 784(786.49) | Grad Norm 0.7207(1.7819) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 20.2082(20.7010) | Bit/dim 3.5434(3.5134) | Xent 0.0016(0.0044) | Loss 8.6769(9.1941) | Error 0.0000(0.0010) Steps 790(786.76) | Grad Norm 0.7649(1.6736) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 20.6435(20.6321) | Bit/dim 3.5042(3.5121) | Xent 0.0046(0.0041) | Loss 8.7807(9.0667) | Error 0.0011(0.0009) Steps 778(785.46) | Grad Norm 2.0973(1.6830) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 20.8815(20.5960) | Bit/dim 3.4697(3.5074) | Xent 0.0028(0.0045) | Loss 8.5928(8.9848) | Error 0.0011(0.0011) Steps 784(784.99) | Grad Norm 2.1875(1.7218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 97.5342, Epoch Time 1253.1994(1229.8110), Bit/dim 3.5409(best: 3.5349), Xent 2.4710, Loss 4.7764, Error 0.3354(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 19.7969(20.5795) | Bit/dim 3.5238(3.5072) | Xent 0.0099(0.0052) | Loss 8.7759(9.7027) | Error 0.0022(0.0013) Steps 772(784.00) | Grad Norm 3.3469(1.8241) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 20.6459(20.6150) | Bit/dim 3.5248(3.5073) | Xent 0.0016(0.0046) | Loss 8.7953(9.4518) | Error 0.0000(0.0011) Steps 796(783.66) | Grad Norm 1.9195(1.8171) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 20.3989(20.5819) | Bit/dim 3.5463(3.5088) | Xent 0.0124(0.0048) | Loss 8.7030(9.2656) | Error 0.0022(0.0011) Steps 790(784.12) | Grad Norm 2.7846(1.7969) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 20.4044(20.5787) | Bit/dim 3.5118(3.5110) | Xent 0.0019(0.0043) | Loss 8.6998(9.1302) | Error 0.0000(0.0009) Steps 772(783.93) | Grad Norm 0.9373(1.7048) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 20.2031(20.5886) | Bit/dim 3.4837(3.5085) | Xent 0.0059(0.0041) | Loss 8.6758(9.0229) | Error 0.0011(0.0009) Steps 796(784.93) | Grad Norm 1.4204(1.6420) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 20.9466(20.5957) | Bit/dim 3.5172(3.5084) | Xent 0.0044(0.0039) | Loss 8.7512(8.9469) | Error 0.0022(0.0010) Steps 802(782.59) | Grad Norm 0.8871(1.6103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 98.1227, Epoch Time 1251.0396(1230.4479), Bit/dim 3.5363(best: 3.5349), Xent 2.4742, Loss 4.7734, Error 0.3338(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 20.8748(20.7039) | Bit/dim 3.5489(3.5070) | Xent 0.0014(0.0035) | Loss 8.8371(9.5634) | Error 0.0000(0.0009) Steps 778(782.39) | Grad Norm 1.1434(1.5283) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 20.4946(20.6781) | Bit/dim 3.4931(3.5088) | Xent 0.0049(0.0032) | Loss 8.7277(9.3554) | Error 0.0022(0.0009) Steps 772(781.87) | Grad Norm 1.8736(1.4251) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 20.3757(20.6723) | Bit/dim 3.4956(3.5073) | Xent 0.0037(0.0032) | Loss 8.5764(9.1905) | Error 0.0011(0.0009) Steps 802(784.53) | Grad Norm 1.7422(1.4183) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 20.4056(20.6535) | Bit/dim 3.5066(3.5088) | Xent 0.0012(0.0032) | Loss 8.6798(9.0667) | Error 0.0000(0.0009) Steps 772(784.27) | Grad Norm 0.7778(1.4248) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 20.7300(20.6709) | Bit/dim 3.5059(3.5099) | Xent 0.0013(0.0033) | Loss 8.7134(8.9793) | Error 0.0000(0.0009) Steps 760(784.09) | Grad Norm 0.7628(1.5108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 96.6554, Epoch Time 1255.8454(1231.2098), Bit/dim 3.5389(best: 3.5349), Xent 2.5144, Loss 4.7961, Error 0.3324(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 20.2167(20.6554) | Bit/dim 3.5314(3.5105) | Xent 0.0022(0.0033) | Loss 8.7639(9.6825) | Error 0.0011(0.0009) Steps 796(785.65) | Grad Norm 1.5033(1.4387) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 20.3112(20.6359) | Bit/dim 3.5134(3.5082) | Xent 0.0046(0.0034) | Loss 8.7112(9.4294) | Error 0.0011(0.0010) Steps 772(785.21) | Grad Norm 0.7019(1.3871) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 19.8956(20.5906) | Bit/dim 3.5159(3.5068) | Xent 0.0014(0.0036) | Loss 8.6368(9.2396) | Error 0.0000(0.0010) Steps 772(782.10) | Grad Norm 1.1163(1.4174) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 20.6432(20.5655) | Bit/dim 3.5325(3.5100) | Xent 0.0028(0.0038) | Loss 8.7192(9.1021) | Error 0.0011(0.0010) Steps 802(782.68) | Grad Norm 1.5479(1.5602) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 20.0820(20.5616) | Bit/dim 3.5048(3.5105) | Xent 0.0007(0.0043) | Loss 8.6743(9.0085) | Error 0.0000(0.0011) Steps 778(783.15) | Grad Norm 0.8552(1.6827) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 20.9914(20.5863) | Bit/dim 3.4868(3.5088) | Xent 0.0017(0.0041) | Loss 8.7585(8.9342) | Error 0.0011(0.0012) Steps 772(782.14) | Grad Norm 2.4964(1.6791) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 98.7214, Epoch Time 1251.2691(1231.8116), Bit/dim 3.5362(best: 3.5349), Xent 2.4923, Loss 4.7824, Error 0.3362(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 20.3080(20.5978) | Bit/dim 3.4889(3.5072) | Xent 0.0023(0.0038) | Loss 8.6455(9.5496) | Error 0.0011(0.0012) Steps 766(783.02) | Grad Norm 1.7668(1.6664) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 20.7155(20.5248) | Bit/dim 3.4926(3.5082) | Xent 0.0058(0.0041) | Loss 8.7516(9.3314) | Error 0.0011(0.0012) Steps 796(784.77) | Grad Norm 1.5520(1.6115) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 20.9948(20.4806) | Bit/dim 3.5016(3.5055) | Xent 0.0055(0.0037) | Loss 8.7749(9.1643) | Error 0.0011(0.0010) Steps 802(786.63) | Grad Norm 1.7990(1.4814) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 20.7430(20.5076) | Bit/dim 3.4721(3.5071) | Xent 0.0020(0.0035) | Loss 8.5523(9.0452) | Error 0.0000(0.0009) Steps 778(784.93) | Grad Norm 1.3374(1.5541) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 20.4904(20.6078) | Bit/dim 3.5318(3.5087) | Xent 0.0011(0.0036) | Loss 8.7572(8.9737) | Error 0.0000(0.0010) Steps 754(785.27) | Grad Norm 1.0529(1.7337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 97.2693, Epoch Time 1249.1663(1232.3322), Bit/dim 3.5401(best: 3.5349), Xent 2.4904, Loss 4.7853, Error 0.3283(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 21.4951(20.6126) | Bit/dim 3.5132(3.5097) | Xent 0.0021(0.0038) | Loss 8.7401(9.6666) | Error 0.0000(0.0011) Steps 802(785.99) | Grad Norm 1.8669(1.7752) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 20.5053(20.6433) | Bit/dim 3.4843(3.5088) | Xent 0.0013(0.0036) | Loss 8.7261(9.4196) | Error 0.0000(0.0010) Steps 790(785.10) | Grad Norm 0.8578(1.7126) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 20.9216(20.6872) | Bit/dim 3.5128(3.5076) | Xent 0.0014(0.0033) | Loss 8.7597(9.2458) | Error 0.0000(0.0009) Steps 778(784.93) | Grad Norm 1.4661(1.6401) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 20.3910(20.6513) | Bit/dim 3.5070(3.5075) | Xent 0.0047(0.0037) | Loss 8.7224(9.1144) | Error 0.0033(0.0010) Steps 766(784.32) | Grad Norm 2.3625(1.7308) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 20.4922(20.6148) | Bit/dim 3.5019(3.5058) | Xent 0.0011(0.0037) | Loss 8.6780(9.0021) | Error 0.0000(0.0010) Steps 778(783.40) | Grad Norm 1.1272(1.7687) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 20.4589(20.5825) | Bit/dim 3.5104(3.5058) | Xent 0.0018(0.0040) | Loss 8.7840(8.9273) | Error 0.0011(0.0011) Steps 802(784.05) | Grad Norm 1.6665(1.7983) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 96.6543, Epoch Time 1252.2550(1232.9299), Bit/dim 3.5393(best: 3.5349), Xent 2.4750, Loss 4.7768, Error 0.3323(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 20.2274(20.6100) | Bit/dim 3.4737(3.5027) | Xent 0.0034(0.0046) | Loss 8.6121(9.5479) | Error 0.0011(0.0013) Steps 784(782.75) | Grad Norm 2.3180(1.8956) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 20.3249(20.5950) | Bit/dim 3.4786(3.5052) | Xent 0.0094(0.0045) | Loss 8.5373(9.3308) | Error 0.0033(0.0012) Steps 784(783.60) | Grad Norm 3.1470(1.8638) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 20.1681(20.5850) | Bit/dim 3.5090(3.5084) | Xent 0.0013(0.0041) | Loss 8.6123(9.1703) | Error 0.0000(0.0011) Steps 796(783.35) | Grad Norm 1.0615(1.7759) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 21.5538(20.6034) | Bit/dim 3.5201(3.5106) | Xent 0.0014(0.0038) | Loss 8.7331(9.0589) | Error 0.0000(0.0011) Steps 742(781.82) | Grad Norm 1.1933(1.7654) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 20.8086(20.6080) | Bit/dim 3.5259(3.5105) | Xent 0.0016(0.0036) | Loss 8.7650(8.9718) | Error 0.0000(0.0010) Steps 802(784.20) | Grad Norm 1.8201(1.7522) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 96.4727, Epoch Time 1251.8656(1233.4980), Bit/dim 3.5364(best: 3.5349), Xent 2.5475, Loss 4.8101, Error 0.3401(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 22.2421(20.6774) | Bit/dim 3.4777(3.5089) | Xent 0.0020(0.0040) | Loss 8.7174(9.7073) | Error 0.0000(0.0011) Steps 808(784.33) | Grad Norm 1.2464(1.8092) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 20.5208(20.6751) | Bit/dim 3.5203(3.5102) | Xent 0.0015(0.0040) | Loss 8.7606(9.4524) | Error 0.0000(0.0011) Steps 772(785.69) | Grad Norm 1.0496(1.7930) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 20.1371(20.6534) | Bit/dim 3.5443(3.5110) | Xent 0.0060(0.0038) | Loss 8.8262(9.2722) | Error 0.0011(0.0010) Steps 784(785.09) | Grad Norm 3.7581(1.9067) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 20.4300(20.6350) | Bit/dim 3.4916(3.5083) | Xent 0.0028(0.0036) | Loss 8.7817(9.1292) | Error 0.0011(0.0010) Steps 754(784.00) | Grad Norm 1.1853(1.8693) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 20.8058(20.6206) | Bit/dim 3.5069(3.5077) | Xent 0.0007(0.0039) | Loss 8.8036(9.0247) | Error 0.0000(0.0010) Steps 778(782.03) | Grad Norm 0.7559(1.7394) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 19.8471(20.5931) | Bit/dim 3.5195(3.5071) | Xent 0.0023(0.0035) | Loss 8.7235(8.9421) | Error 0.0000(0.0009) Steps 790(782.06) | Grad Norm 0.8571(1.5872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 96.1177, Epoch Time 1251.8956(1234.0499), Bit/dim 3.5351(best: 3.5349), Xent 2.5491, Loss 4.8097, Error 0.3382(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 20.4474(20.6014) | Bit/dim 3.5037(3.5058) | Xent 0.0010(0.0041) | Loss 8.6930(9.5394) | Error 0.0000(0.0011) Steps 784(781.52) | Grad Norm 0.8419(1.7002) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 20.8295(20.5991) | Bit/dim 3.5407(3.5084) | Xent 0.0018(0.0038) | Loss 8.7743(9.3318) | Error 0.0011(0.0010) Steps 784(780.56) | Grad Norm 1.5941(1.6759) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 21.3022(20.6357) | Bit/dim 3.4936(3.5052) | Xent 0.0006(0.0036) | Loss 8.7646(9.1689) | Error 0.0000(0.0010) Steps 772(781.92) | Grad Norm 1.1525(1.7947) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 20.7093(20.6137) | Bit/dim 3.5254(3.5071) | Xent 0.0019(0.0038) | Loss 8.7205(9.0484) | Error 0.0000(0.0010) Steps 778(780.41) | Grad Norm 1.0412(1.6276) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 20.7574(20.6979) | Bit/dim 3.5042(3.5082) | Xent 0.0083(0.0037) | Loss 8.6129(8.9611) | Error 0.0022(0.0009) Steps 772(779.89) | Grad Norm 1.4130(1.5516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 96.6972, Epoch Time 1254.6802(1234.6688), Bit/dim 3.5352(best: 3.5349), Xent 2.5359, Loss 4.8032, Error 0.3344(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 21.1916(20.6917) | Bit/dim 3.5150(3.5051) | Xent 0.0053(0.0038) | Loss 8.6844(9.6961) | Error 0.0022(0.0009) Steps 814(780.44) | Grad Norm 3.8121(1.6140) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 20.9002(20.6892) | Bit/dim 3.5041(3.5054) | Xent 0.0006(0.0034) | Loss 8.7314(9.4328) | Error 0.0000(0.0009) Steps 802(783.77) | Grad Norm 0.7374(1.5541) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 20.3583(20.7039) | Bit/dim 3.5253(3.5092) | Xent 0.0010(0.0042) | Loss 8.7440(9.2429) | Error 0.0000(0.0010) Steps 790(783.75) | Grad Norm 0.7491(1.6864) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 20.1425(20.6973) | Bit/dim 3.4973(3.5079) | Xent 0.0013(0.0041) | Loss 8.6833(9.1116) | Error 0.0000(0.0009) Steps 796(783.83) | Grad Norm 1.4205(1.6632) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 21.0092(20.7340) | Bit/dim 3.5116(3.5051) | Xent 0.0012(0.0038) | Loss 8.7452(9.0109) | Error 0.0000(0.0008) Steps 796(784.25) | Grad Norm 0.9136(1.5716) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 20.6865(20.7453) | Bit/dim 3.5232(3.5071) | Xent 0.0060(0.0040) | Loss 8.7347(8.9419) | Error 0.0033(0.0010) Steps 784(784.67) | Grad Norm 2.0101(1.5517) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 96.2662, Epoch Time 1258.8344(1235.3938), Bit/dim 3.5381(best: 3.5349), Xent 2.5269, Loss 4.8016, Error 0.3387(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 20.4845(20.6875) | Bit/dim 3.4838(3.5069) | Xent 0.0022(0.0046) | Loss 8.7315(9.5616) | Error 0.0000(0.0011) Steps 796(784.53) | Grad Norm 1.4168(1.8827) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 20.4900(20.6426) | Bit/dim 3.5245(3.5064) | Xent 0.0060(0.0042) | Loss 8.6473(9.3343) | Error 0.0011(0.0010) Steps 772(781.86) | Grad Norm 1.7399(1.7862) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 20.2644(20.6564) | Bit/dim 3.4883(3.5047) | Xent 0.0016(0.0037) | Loss 8.6727(9.1765) | Error 0.0000(0.0009) Steps 790(781.87) | Grad Norm 0.7636(1.6733) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 20.9592(20.6629) | Bit/dim 3.5194(3.5084) | Xent 0.0027(0.0037) | Loss 8.7576(9.0684) | Error 0.0011(0.0010) Steps 784(781.42) | Grad Norm 1.3119(1.5926) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 20.6439(20.7142) | Bit/dim 3.4924(3.5043) | Xent 0.0035(0.0039) | Loss 8.7590(8.9815) | Error 0.0011(0.0012) Steps 778(783.97) | Grad Norm 1.6744(1.6889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 96.5390, Epoch Time 1252.6559(1235.9116), Bit/dim 3.5382(best: 3.5349), Xent 2.5406, Loss 4.8086, Error 0.3313(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 20.5896(20.6278) | Bit/dim 3.5216(3.5064) | Xent 0.0014(0.0035) | Loss 8.8579(9.6902) | Error 0.0000(0.0010) Steps 790(782.78) | Grad Norm 1.5461(1.6603) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 21.1129(20.6037) | Bit/dim 3.5070(3.5068) | Xent 0.0018(0.0034) | Loss 8.7176(9.4352) | Error 0.0000(0.0010) Steps 778(780.61) | Grad Norm 0.9060(1.5835) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 20.2897(20.5850) | Bit/dim 3.4922(3.5070) | Xent 0.0013(0.0032) | Loss 8.7296(9.2439) | Error 0.0000(0.0010) Steps 772(780.34) | Grad Norm 0.9945(1.5917) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 20.9548(20.6221) | Bit/dim 3.5113(3.5071) | Xent 0.0007(0.0033) | Loss 8.8248(9.1214) | Error 0.0000(0.0010) Steps 802(783.48) | Grad Norm 1.0373(1.6238) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 20.4457(20.5981) | Bit/dim 3.5148(3.5090) | Xent 0.0045(0.0035) | Loss 8.6525(9.0300) | Error 0.0011(0.0011) Steps 784(785.32) | Grad Norm 2.1381(1.8518) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 21.0712(20.6251) | Bit/dim 3.5060(3.5058) | Xent 0.0024(0.0037) | Loss 8.6705(8.9418) | Error 0.0000(0.0011) Steps 772(785.74) | Grad Norm 1.5159(1.8652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 97.3894, Epoch Time 1250.8661(1236.3603), Bit/dim 3.5371(best: 3.5349), Xent 2.5896, Loss 4.8319, Error 0.3394(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 19.9152(20.6405) | Bit/dim 3.5539(3.5053) | Xent 0.0011(0.0033) | Loss 8.8266(9.5315) | Error 0.0000(0.0009) Steps 772(785.89) | Grad Norm 0.8506(1.7372) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 20.8536(20.6906) | Bit/dim 3.5112(3.5077) | Xent 0.0104(0.0036) | Loss 8.7450(9.3222) | Error 0.0022(0.0009) Steps 796(783.05) | Grad Norm 1.9756(1.8187) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 20.8878(20.7357) | Bit/dim 3.4866(3.5054) | Xent 0.0033(0.0043) | Loss 8.6979(9.1568) | Error 0.0011(0.0012) Steps 772(782.27) | Grad Norm 1.5525(1.9454) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 20.7977(20.7270) | Bit/dim 3.5194(3.5066) | Xent 0.0026(0.0038) | Loss 8.7869(9.0454) | Error 0.0011(0.0011) Steps 778(783.30) | Grad Norm 1.4654(1.8843) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 20.6981(20.6817) | Bit/dim 3.5130(3.5040) | Xent 0.0015(0.0037) | Loss 8.6949(8.9460) | Error 0.0000(0.0010) Steps 796(782.41) | Grad Norm 1.3800(1.9512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 97.2480, Epoch Time 1258.0983(1237.0124), Bit/dim 3.5358(best: 3.5349), Xent 2.4992, Loss 4.7854, Error 0.3311(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 20.6435(20.7033) | Bit/dim 3.5125(3.5057) | Xent 0.0058(0.0034) | Loss 8.5948(9.6810) | Error 0.0011(0.0009) Steps 784(783.86) | Grad Norm 5.8850(2.0120) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 21.1333(20.7027) | Bit/dim 3.5227(3.5088) | Xent 0.0014(0.0034) | Loss 8.7979(9.4397) | Error 0.0000(0.0009) Steps 790(784.69) | Grad Norm 1.5807(1.9923) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 20.3240(20.7002) | Bit/dim 3.4870(3.5076) | Xent 0.0015(0.0031) | Loss 8.7319(9.2590) | Error 0.0000(0.0007) Steps 772(787.79) | Grad Norm 1.4165(1.9085) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 20.7944(20.6581) | Bit/dim 3.4977(3.5047) | Xent 0.0015(0.0032) | Loss 8.7790(9.1155) | Error 0.0000(0.0008) Steps 778(785.57) | Grad Norm 1.1011(1.8967) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 19.9738(20.6129) | Bit/dim 3.4573(3.5042) | Xent 0.0012(0.0030) | Loss 8.5789(9.0068) | Error 0.0000(0.0008) Steps 784(787.06) | Grad Norm 0.8647(1.9402) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 20.5122(20.5927) | Bit/dim 3.4997(3.5042) | Xent 0.0033(0.0028) | Loss 8.6685(8.9299) | Error 0.0011(0.0007) Steps 772(786.32) | Grad Norm 1.5904(1.7777) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 97.4260, Epoch Time 1252.0646(1237.4640), Bit/dim 3.5368(best: 3.5349), Xent 2.5705, Loss 4.8221, Error 0.3382(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 20.6851(20.6204) | Bit/dim 3.5445(3.5070) | Xent 0.0026(0.0030) | Loss 8.6961(9.5257) | Error 0.0000(0.0007) Steps 772(786.20) | Grad Norm 1.1964(1.6682) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 20.4284(20.6498) | Bit/dim 3.4747(3.5049) | Xent 0.0100(0.0032) | Loss 8.7166(9.3126) | Error 0.0022(0.0007) Steps 796(786.14) | Grad Norm 3.1185(1.6452) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 20.5071(20.6448) | Bit/dim 3.5309(3.5055) | Xent 0.0041(0.0034) | Loss 8.6341(9.1499) | Error 0.0011(0.0008) Steps 784(786.62) | Grad Norm 1.2549(1.6355) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 20.5402(20.6047) | Bit/dim 3.5255(3.5057) | Xent 0.0016(0.0035) | Loss 8.8217(9.0472) | Error 0.0000(0.0009) Steps 784(786.55) | Grad Norm 0.8579(1.7392) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 21.4072(20.6026) | Bit/dim 3.4874(3.5050) | Xent 0.0013(0.0032) | Loss 8.7526(8.9730) | Error 0.0000(0.0009) Steps 808(787.13) | Grad Norm 1.1486(1.6036) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 98.2029, Epoch Time 1254.4681(1237.9741), Bit/dim 3.5345(best: 3.5349), Xent 2.5863, Loss 4.8276, Error 0.3393(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 20.2760(20.6194) | Bit/dim 3.5089(3.5045) | Xent 0.0010(0.0037) | Loss 8.6960(9.6897) | Error 0.0000(0.0009) Steps 772(785.81) | Grad Norm 0.7731(1.6861) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 20.6281(20.6600) | Bit/dim 3.4970(3.5043) | Xent 0.0027(0.0039) | Loss 8.5464(9.4335) | Error 0.0011(0.0010) Steps 790(784.28) | Grad Norm 0.9186(1.6889) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 20.1885(20.6475) | Bit/dim 3.4919(3.5033) | Xent 0.0053(0.0041) | Loss 8.6800(9.2464) | Error 0.0022(0.0011) Steps 772(784.85) | Grad Norm 1.8961(1.7457) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 20.4590(20.6763) | Bit/dim 3.4842(3.5025) | Xent 0.0021(0.0043) | Loss 8.5610(9.1067) | Error 0.0000(0.0011) Steps 778(782.83) | Grad Norm 0.9225(1.7154) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 20.3912(20.6935) | Bit/dim 3.5070(3.5043) | Xent 0.0010(0.0041) | Loss 8.8000(9.0172) | Error 0.0000(0.0012) Steps 784(784.12) | Grad Norm 1.5378(1.8171) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 21.3190(20.6975) | Bit/dim 3.5041(3.5072) | Xent 0.0009(0.0041) | Loss 8.6913(8.9449) | Error 0.0000(0.0012) Steps 796(786.59) | Grad Norm 2.6069(1.9807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 96.6487, Epoch Time 1256.5468(1238.5313), Bit/dim 3.5385(best: 3.5345), Xent 2.5370, Loss 4.8070, Error 0.3355(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 21.7803(20.7603) | Bit/dim 3.4965(3.5057) | Xent 0.0026(0.0044) | Loss 8.7838(9.5596) | Error 0.0011(0.0013) Steps 778(787.18) | Grad Norm 1.5593(2.0383) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 20.5814(20.8273) | Bit/dim 3.5246(3.5066) | Xent 0.0115(0.0045) | Loss 8.8024(9.3497) | Error 0.0022(0.0013) Steps 796(787.00) | Grad Norm 1.9867(2.0216) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 20.4451(20.8241) | Bit/dim 3.5203(3.5062) | Xent 0.0035(0.0042) | Loss 8.6733(9.1816) | Error 0.0011(0.0012) Steps 766(785.29) | Grad Norm 1.7978(1.9188) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 20.5110(20.7962) | Bit/dim 3.4979(3.5050) | Xent 0.0016(0.0044) | Loss 8.5675(9.0546) | Error 0.0011(0.0012) Steps 778(782.48) | Grad Norm 2.1798(1.9164) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 20.5556(20.7548) | Bit/dim 3.5086(3.5061) | Xent 0.0110(0.0048) | Loss 8.8294(8.9777) | Error 0.0022(0.0013) Steps 790(784.95) | Grad Norm 5.1118(1.9765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 97.7505, Epoch Time 1262.2211(1239.2420), Bit/dim 3.5348(best: 3.5345), Xent 2.5136, Loss 4.7917, Error 0.3308(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 19.6842(20.6842) | Bit/dim 3.5221(3.5080) | Xent 0.0067(0.0045) | Loss 8.7446(9.7079) | Error 0.0033(0.0012) Steps 778(786.44) | Grad Norm 4.0148(1.9371) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 20.1163(20.6096) | Bit/dim 3.5119(3.5076) | Xent 0.0039(0.0040) | Loss 8.7594(9.4442) | Error 0.0011(0.0010) Steps 766(785.25) | Grad Norm 2.5312(1.7882) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 19.9075(20.6067) | Bit/dim 3.4887(3.5059) | Xent 0.0022(0.0042) | Loss 8.6656(9.2397) | Error 0.0011(0.0011) Steps 784(782.91) | Grad Norm 0.9453(1.7624) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 20.7524(20.5559) | Bit/dim 3.4680(3.5044) | Xent 0.0012(0.0037) | Loss 8.6446(9.0950) | Error 0.0000(0.0010) Steps 802(783.81) | Grad Norm 1.1566(1.8236) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 20.7900(20.5834) | Bit/dim 3.5346(3.5056) | Xent 0.0146(0.0040) | Loss 8.7193(8.9940) | Error 0.0044(0.0011) Steps 766(784.23) | Grad Norm 3.2960(1.8859) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 21.3335(20.6180) | Bit/dim 3.5338(3.5065) | Xent 0.0032(0.0047) | Loss 8.8252(8.9286) | Error 0.0011(0.0012) Steps 808(784.15) | Grad Norm 2.3003(2.0476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 97.7681, Epoch Time 1249.8817(1239.5612), Bit/dim 3.5400(best: 3.5345), Xent 2.5250, Loss 4.8024, Error 0.3329(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 21.3462(20.6644) | Bit/dim 3.5113(3.5064) | Xent 0.0016(0.0043) | Loss 8.7820(9.5501) | Error 0.0000(0.0011) Steps 784(781.55) | Grad Norm 1.6080(1.9816) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 20.7088(20.6659) | Bit/dim 3.4988(3.5053) | Xent 0.0010(0.0043) | Loss 8.7394(9.3366) | Error 0.0000(0.0012) Steps 790(782.74) | Grad Norm 0.8738(2.1452) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 20.7254(20.7093) | Bit/dim 3.4767(3.5038) | Xent 0.0032(0.0042) | Loss 8.7029(9.1746) | Error 0.0022(0.0011) Steps 778(782.12) | Grad Norm 2.2231(2.0517) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 20.4238(20.7213) | Bit/dim 3.4807(3.5068) | Xent 0.0041(0.0046) | Loss 8.6854(9.0564) | Error 0.0022(0.0014) Steps 796(785.42) | Grad Norm 2.2028(2.1718) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 20.0858(20.6976) | Bit/dim 3.4935(3.5053) | Xent 0.0010(0.0046) | Loss 8.6325(8.9625) | Error 0.0000(0.0012) Steps 772(783.60) | Grad Norm 1.0102(2.0061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 97.6185, Epoch Time 1257.9786(1240.1137), Bit/dim 3.5335(best: 3.5345), Xent 2.5606, Loss 4.8138, Error 0.3386(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 20.8327(20.6972) | Bit/dim 3.5571(3.5089) | Xent 0.0010(0.0045) | Loss 8.8602(9.6912) | Error 0.0000(0.0012) Steps 802(785.72) | Grad Norm 0.7741(1.8957) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 20.4143(20.7337) | Bit/dim 3.5180(3.5038) | Xent 0.0047(0.0040) | Loss 8.6111(9.4240) | Error 0.0011(0.0011) Steps 760(784.04) | Grad Norm 2.6242(1.8091) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 21.1953(20.7583) | Bit/dim 3.5056(3.5033) | Xent 0.0023(0.0035) | Loss 8.8125(9.2372) | Error 0.0011(0.0010) Steps 802(785.58) | Grad Norm 1.2927(1.6961) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 21.7021(20.7409) | Bit/dim 3.5115(3.5040) | Xent 0.0008(0.0036) | Loss 8.6980(9.1046) | Error 0.0000(0.0010) Steps 766(785.81) | Grad Norm 0.6816(1.7457) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 20.0142(20.7000) | Bit/dim 3.5006(3.5056) | Xent 0.0028(0.0040) | Loss 8.6549(9.0000) | Error 0.0011(0.0011) Steps 778(785.74) | Grad Norm 1.6892(2.0224) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 20.0891(20.6582) | Bit/dim 3.4937(3.5052) | Xent 0.0079(0.0051) | Loss 8.7576(8.9272) | Error 0.0033(0.0013) Steps 772(785.45) | Grad Norm 3.6157(2.2240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 97.6239, Epoch Time 1257.8442(1240.6456), Bit/dim 3.5356(best: 3.5335), Xent 2.5788, Loss 4.8250, Error 0.3341(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 20.6403(20.7221) | Bit/dim 3.4832(3.5015) | Xent 0.0023(0.0046) | Loss 8.8047(9.5365) | Error 0.0011(0.0011) Steps 802(784.77) | Grad Norm 1.3437(2.0494) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 20.4817(20.6128) | Bit/dim 3.5082(3.5037) | Xent 0.0015(0.0042) | Loss 8.7767(9.3203) | Error 0.0000(0.0010) Steps 766(783.89) | Grad Norm 1.1969(1.9377) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 20.4997(20.6222) | Bit/dim 3.5203(3.5066) | Xent 0.0028(0.0035) | Loss 8.7881(9.1678) | Error 0.0011(0.0009) Steps 760(780.63) | Grad Norm 1.4165(1.7373) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 20.6624(20.5877) | Bit/dim 3.4912(3.5020) | Xent 0.0011(0.0035) | Loss 8.7198(9.0472) | Error 0.0000(0.0008) Steps 766(781.46) | Grad Norm 1.0409(1.7383) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 21.7470(20.6921) | Bit/dim 3.4540(3.5017) | Xent 0.0008(0.0035) | Loss 8.5632(8.9504) | Error 0.0000(0.0009) Steps 814(782.03) | Grad Norm 0.7529(1.6873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 96.6818, Epoch Time 1259.0236(1241.1969), Bit/dim 3.5324(best: 3.5335), Xent 2.5992, Loss 4.8320, Error 0.3382(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 20.9340(20.8176) | Bit/dim 3.4627(3.5025) | Xent 0.0156(0.0038) | Loss 8.7807(9.6950) | Error 0.0022(0.0009) Steps 814(785.72) | Grad Norm 2.7218(1.6648) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 21.0603(20.7467) | Bit/dim 3.5021(3.5051) | Xent 0.0009(0.0035) | Loss 8.6301(9.4435) | Error 0.0000(0.0009) Steps 796(787.68) | Grad Norm 0.7977(1.6424) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 20.1081(20.7140) | Bit/dim 3.5031(3.5056) | Xent 0.0015(0.0032) | Loss 8.7471(9.2614) | Error 0.0000(0.0008) Steps 790(787.60) | Grad Norm 0.7754(1.6382) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 20.8419(20.7711) | Bit/dim 3.5102(3.5065) | Xent 0.0083(0.0033) | Loss 8.7296(9.1112) | Error 0.0011(0.0008) Steps 802(784.60) | Grad Norm 1.2753(1.6259) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 21.4139(20.7385) | Bit/dim 3.4792(3.5066) | Xent 0.0029(0.0031) | Loss 8.7152(9.0046) | Error 0.0011(0.0007) Steps 784(783.11) | Grad Norm 3.4177(1.6420) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 20.9738(20.6812) | Bit/dim 3.5085(3.5038) | Xent 0.0051(0.0036) | Loss 8.7652(8.9252) | Error 0.0011(0.0009) Steps 760(782.61) | Grad Norm 2.4470(1.7407) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 97.3701, Epoch Time 1254.9402(1241.6092), Bit/dim 3.5334(best: 3.5324), Xent 2.5672, Loss 4.8170, Error 0.3308(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 20.6215(20.6507) | Bit/dim 3.5102(3.5062) | Xent 0.0006(0.0037) | Loss 8.7048(9.5277) | Error 0.0000(0.0009) Steps 808(783.46) | Grad Norm 0.9860(1.7376) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 21.2296(20.7559) | Bit/dim 3.5138(3.5055) | Xent 0.0028(0.0047) | Loss 8.8037(9.3260) | Error 0.0022(0.0013) Steps 784(785.11) | Grad Norm 2.7959(2.0989) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 21.0473(20.7960) | Bit/dim 3.4817(3.5027) | Xent 0.0016(0.0043) | Loss 8.6253(9.1625) | Error 0.0000(0.0012) Steps 784(785.19) | Grad Norm 1.5113(2.0466) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 20.6425(20.8110) | Bit/dim 3.4853(3.5023) | Xent 0.0044(0.0039) | Loss 8.6406(9.0384) | Error 0.0022(0.0011) Steps 778(786.91) | Grad Norm 2.0126(2.0195) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 20.5761(20.7193) | Bit/dim 3.5232(3.5015) | Xent 0.0124(0.0037) | Loss 8.8324(8.9600) | Error 0.0022(0.0010) Steps 796(786.74) | Grad Norm 2.5510(1.8767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 96.6120, Epoch Time 1257.7278(1242.0928), Bit/dim 3.5310(best: 3.5324), Xent 2.5986, Loss 4.8303, Error 0.3353(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 20.6222(20.6796) | Bit/dim 3.5135(3.5024) | Xent 0.0011(0.0039) | Loss 8.7704(9.6765) | Error 0.0000(0.0010) Steps 814(787.74) | Grad Norm 1.4977(1.8921) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 20.5267(20.6676) | Bit/dim 3.5305(3.5052) | Xent 0.0011(0.0036) | Loss 8.7185(9.4349) | Error 0.0000(0.0009) Steps 748(786.55) | Grad Norm 0.8938(1.8254) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 20.5678(20.6143) | Bit/dim 3.4902(3.5027) | Xent 0.0123(0.0042) | Loss 8.7516(9.2409) | Error 0.0011(0.0011) Steps 790(785.99) | Grad Norm 2.1636(1.9152) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 20.3827(20.5405) | Bit/dim 3.5485(3.5035) | Xent 0.0012(0.0038) | Loss 8.7786(9.0926) | Error 0.0000(0.0009) Steps 778(784.84) | Grad Norm 0.7523(1.7525) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 20.7231(20.5911) | Bit/dim 3.4784(3.5013) | Xent 0.0045(0.0038) | Loss 8.6839(8.9924) | Error 0.0011(0.0010) Steps 778(785.56) | Grad Norm 3.6281(1.8922) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 20.0888(20.5710) | Bit/dim 3.4883(3.5022) | Xent 0.0008(0.0039) | Loss 8.6062(8.9202) | Error 0.0000(0.0009) Steps 760(783.94) | Grad Norm 1.0094(1.7947) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 97.6296, Epoch Time 1249.6378(1242.3191), Bit/dim 3.5343(best: 3.5310), Xent 2.5689, Loss 4.8188, Error 0.3332(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 20.3082(20.5161) | Bit/dim 3.4878(3.5038) | Xent 0.0030(0.0041) | Loss 8.6664(9.5302) | Error 0.0011(0.0009) Steps 790(783.07) | Grad Norm 1.6681(1.6657) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 20.7223(20.5615) | Bit/dim 3.5051(3.5015) | Xent 0.0007(0.0039) | Loss 8.7287(9.3090) | Error 0.0000(0.0009) Steps 772(784.00) | Grad Norm 2.1020(1.7589) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 21.0098(20.6108) | Bit/dim 3.5110(3.5010) | Xent 0.0012(0.0040) | Loss 8.7759(9.1466) | Error 0.0000(0.0010) Steps 814(783.56) | Grad Norm 1.1679(1.8347) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 20.6019(20.6926) | Bit/dim 3.4905(3.5024) | Xent 0.0014(0.0034) | Loss 8.6680(9.0261) | Error 0.0000(0.0009) Steps 766(780.60) | Grad Norm 1.6485(1.6758) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 20.0644(20.6584) | Bit/dim 3.4725(3.5025) | Xent 0.0054(0.0037) | Loss 8.6662(8.9380) | Error 0.0011(0.0008) Steps 796(780.45) | Grad Norm 1.7806(1.6083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 97.0106, Epoch Time 1253.9665(1242.6686), Bit/dim 3.5321(best: 3.5310), Xent 2.5903, Loss 4.8272, Error 0.3325(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 20.3342(20.6525) | Bit/dim 3.5449(3.5025) | Xent 0.0012(0.0034) | Loss 8.7438(9.6331) | Error 0.0000(0.0008) Steps 784(781.40) | Grad Norm 1.0483(1.5951) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 20.9860(20.6590) | Bit/dim 3.5231(3.5011) | Xent 0.0102(0.0037) | Loss 8.7475(9.3925) | Error 0.0022(0.0009) Steps 784(783.02) | Grad Norm 2.4441(1.7033) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 20.8780(20.6517) | Bit/dim 3.4897(3.5015) | Xent 0.0075(0.0037) | Loss 8.6667(9.2152) | Error 0.0022(0.0010) Steps 802(783.11) | Grad Norm 5.0319(1.8535) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 20.3996(20.6368) | Bit/dim 3.5335(3.5042) | Xent 0.0034(0.0043) | Loss 8.6462(9.0818) | Error 0.0011(0.0011) Steps 778(783.21) | Grad Norm 1.8722(2.0919) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 20.4768(20.6552) | Bit/dim 3.5236(3.5027) | Xent 0.0091(0.0045) | Loss 8.7236(8.9875) | Error 0.0022(0.0011) Steps 784(784.49) | Grad Norm 3.2755(2.1228) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 21.5242(20.7102) | Bit/dim 3.5116(3.5066) | Xent 0.0017(0.0047) | Loss 8.7476(8.9287) | Error 0.0000(0.0013) Steps 790(785.58) | Grad Norm 1.4338(2.1958) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 98.3805, Epoch Time 1257.6307(1243.1174), Bit/dim 3.5317(best: 3.5310), Xent 2.5584, Loss 4.8109, Error 0.3332(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 19.8912(20.7079) | Bit/dim 3.5299(3.5079) | Xent 0.0085(0.0050) | Loss 8.7190(9.5569) | Error 0.0022(0.0014) Steps 742(787.10) | Grad Norm 4.7969(2.3663) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 21.2905(20.7406) | Bit/dim 3.5114(3.5073) | Xent 0.0027(0.0048) | Loss 8.7625(9.3392) | Error 0.0000(0.0013) Steps 796(785.69) | Grad Norm 2.6195(2.2863) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 20.7283(20.6945) | Bit/dim 3.4960(3.5083) | Xent 0.0072(0.0045) | Loss 8.6979(9.1861) | Error 0.0011(0.0011) Steps 754(783.67) | Grad Norm 2.3449(2.1656) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 20.3189(20.6332) | Bit/dim 3.4939(3.5062) | Xent 0.0011(0.0049) | Loss 8.6984(9.0636) | Error 0.0000(0.0012) Steps 784(782.86) | Grad Norm 0.9526(2.1604) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 20.6755(20.6711) | Bit/dim 3.5028(3.5040) | Xent 0.0019(0.0045) | Loss 8.7983(8.9729) | Error 0.0000(0.0011) Steps 778(782.82) | Grad Norm 1.3455(2.0524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 97.2218, Epoch Time 1256.2177(1243.5104), Bit/dim 3.5357(best: 3.5310), Xent 2.6078, Loss 4.8396, Error 0.3342(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 20.2307(20.7374) | Bit/dim 3.4685(3.5020) | Xent 0.0097(0.0047) | Loss 8.6350(9.6736) | Error 0.0011(0.0011) Steps 784(783.22) | Grad Norm 2.0732(2.1117) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 20.9405(20.7690) | Bit/dim 3.4924(3.5040) | Xent 0.0021(0.0044) | Loss 8.6079(9.4300) | Error 0.0000(0.0010) Steps 766(782.13) | Grad Norm 1.7179(2.0918) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 20.6716(20.7273) | Bit/dim 3.5097(3.5045) | Xent 0.0025(0.0043) | Loss 8.7207(9.2367) | Error 0.0000(0.0011) Steps 796(783.34) | Grad Norm 1.6842(2.0253) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 21.2974(20.7071) | Bit/dim 3.4697(3.5008) | Xent 0.0028(0.0042) | Loss 8.6548(9.0949) | Error 0.0011(0.0010) Steps 802(783.61) | Grad Norm 2.1167(1.9442) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 21.0342(20.6404) | Bit/dim 3.4924(3.5006) | Xent 0.0063(0.0040) | Loss 8.7317(8.9820) | Error 0.0022(0.0009) Steps 796(783.25) | Grad Norm 2.9101(1.8433) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 20.2577(20.5983) | Bit/dim 3.5034(3.5035) | Xent 0.0029(0.0036) | Loss 8.6127(8.9016) | Error 0.0011(0.0009) Steps 790(783.57) | Grad Norm 1.4677(1.8930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 95.9438, Epoch Time 1252.9425(1243.7934), Bit/dim 3.5333(best: 3.5310), Xent 2.6088, Loss 4.8376, Error 0.3362(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 20.7994(20.6113) | Bit/dim 3.4748(3.5026) | Xent 0.0010(0.0034) | Loss 8.6734(9.4994) | Error 0.0000(0.0009) Steps 802(786.30) | Grad Norm 1.1157(1.9170) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 20.6340(20.6209) | Bit/dim 3.5112(3.5028) | Xent 0.0010(0.0031) | Loss 8.6646(9.2823) | Error 0.0000(0.0008) Steps 802(787.59) | Grad Norm 0.6042(1.8200) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 20.4952(20.5813) | Bit/dim 3.5122(3.5034) | Xent 0.0073(0.0033) | Loss 8.7314(9.1318) | Error 0.0011(0.0008) Steps 778(785.06) | Grad Norm 2.0195(1.7936) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 20.6363(20.6076) | Bit/dim 3.4929(3.5027) | Xent 0.0050(0.0036) | Loss 8.6936(9.0264) | Error 0.0011(0.0009) Steps 814(785.58) | Grad Norm 2.4107(1.8732) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 20.6404(20.6238) | Bit/dim 3.4805(3.5022) | Xent 0.0043(0.0037) | Loss 8.5914(8.9389) | Error 0.0011(0.0009) Steps 754(783.09) | Grad Norm 2.2521(1.7893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 96.1737, Epoch Time 1252.8352(1244.0647), Bit/dim 3.5318(best: 3.5310), Xent 2.6059, Loss 4.8348, Error 0.3363(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 20.2906(20.6544) | Bit/dim 3.5084(3.5034) | Xent 0.0053(0.0038) | Loss 8.6542(9.6649) | Error 0.0011(0.0009) Steps 772(781.94) | Grad Norm 4.7286(1.8808) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 20.7737(20.7116) | Bit/dim 3.5050(3.5048) | Xent 0.0069(0.0037) | Loss 8.8269(9.4157) | Error 0.0022(0.0009) Steps 778(781.68) | Grad Norm 3.1300(1.9177) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 20.9477(20.7505) | Bit/dim 3.5162(3.5039) | Xent 0.0072(0.0039) | Loss 8.7643(9.2307) | Error 0.0033(0.0009) Steps 796(783.07) | Grad Norm 2.6150(1.9335) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 20.3737(20.7405) | Bit/dim 3.5262(3.5031) | Xent 0.0082(0.0044) | Loss 8.7862(9.0978) | Error 0.0011(0.0010) Steps 784(783.79) | Grad Norm 1.8324(1.9066) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 19.7940(20.6483) | Bit/dim 3.5427(3.5000) | Xent 0.0161(0.0043) | Loss 8.7600(8.9926) | Error 0.0044(0.0010) Steps 784(785.93) | Grad Norm 3.3762(1.8286) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 20.7032(20.7171) | Bit/dim 3.4880(3.5010) | Xent 0.0008(0.0039) | Loss 8.6479(8.9246) | Error 0.0000(0.0010) Steps 760(786.32) | Grad Norm 1.0610(1.8059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 95.8810, Epoch Time 1258.2343(1244.4897), Bit/dim 3.5325(best: 3.5310), Xent 2.6170, Loss 4.8410, Error 0.3365(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 20.5542(20.7486) | Bit/dim 3.5017(3.5002) | Xent 0.0014(0.0037) | Loss 8.6176(9.5393) | Error 0.0000(0.0010) Steps 778(786.37) | Grad Norm 0.8686(1.8847) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 20.2618(20.6790) | Bit/dim 3.5305(3.5022) | Xent 0.0042(0.0035) | Loss 8.7842(9.3213) | Error 0.0011(0.0009) Steps 760(784.71) | Grad Norm 1.9231(1.7802) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 20.8073(20.6883) | Bit/dim 3.5127(3.5034) | Xent 0.0025(0.0036) | Loss 8.8203(9.1720) | Error 0.0011(0.0011) Steps 802(786.06) | Grad Norm 1.7035(1.7527) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 20.8937(20.6757) | Bit/dim 3.4721(3.5015) | Xent 0.0078(0.0037) | Loss 8.7283(9.0589) | Error 0.0011(0.0010) Steps 754(785.91) | Grad Norm 2.3576(1.7258) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 20.5378(20.6952) | Bit/dim 3.5128(3.5026) | Xent 0.0009(0.0039) | Loss 8.6801(8.9691) | Error 0.0000(0.0010) Steps 766(784.90) | Grad Norm 1.4011(1.9838) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 96.7946, Epoch Time 1255.8937(1244.8319), Bit/dim 3.5318(best: 3.5310), Xent 2.6096, Loss 4.8366, Error 0.3364(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 21.0581(20.7139) | Bit/dim 3.4906(3.5016) | Xent 0.0013(0.0040) | Loss 8.6376(9.6776) | Error 0.0000(0.0010) Steps 790(784.01) | Grad Norm 1.9439(1.9680) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 20.5726(20.6735) | Bit/dim 3.4994(3.5011) | Xent 0.0039(0.0035) | Loss 8.6666(9.4246) | Error 0.0022(0.0008) Steps 760(783.39) | Grad Norm 2.0278(1.8103) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 20.2441(20.6776) | Bit/dim 3.4807(3.4988) | Xent 0.0006(0.0029) | Loss 8.6261(9.2391) | Error 0.0000(0.0006) Steps 772(784.59) | Grad Norm 0.6774(1.5906) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 21.5212(20.7013) | Bit/dim 3.5125(3.4990) | Xent 0.0041(0.0029) | Loss 8.7040(9.1078) | Error 0.0011(0.0007) Steps 808(784.99) | Grad Norm 1.3308(1.5687) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 20.3539(20.6324) | Bit/dim 3.5174(3.5010) | Xent 0.0007(0.0031) | Loss 8.7047(9.0018) | Error 0.0000(0.0007) Steps 796(785.32) | Grad Norm 0.8219(1.5232) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 20.2971(20.5826) | Bit/dim 3.5144(3.5018) | Xent 0.0025(0.0031) | Loss 8.7149(8.9211) | Error 0.0011(0.0007) Steps 760(784.52) | Grad Norm 1.0074(1.4816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 96.3345, Epoch Time 1250.4912(1245.0016), Bit/dim 3.5301(best: 3.5310), Xent 2.6273, Loss 4.8438, Error 0.3377(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 19.9585(20.5823) | Bit/dim 3.5321(3.5030) | Xent 0.0005(0.0034) | Loss 8.6384(9.5253) | Error 0.0000(0.0009) Steps 748(783.27) | Grad Norm 0.8603(1.6186) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 20.8024(20.6268) | Bit/dim 3.4859(3.5019) | Xent 0.0039(0.0034) | Loss 8.5336(9.3010) | Error 0.0011(0.0009) Steps 784(782.20) | Grad Norm 3.4693(1.6666) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 21.1513(20.6760) | Bit/dim 3.5019(3.4996) | Xent 0.0016(0.0033) | Loss 8.7212(9.1380) | Error 0.0011(0.0009) Steps 742(782.82) | Grad Norm 1.1529(1.6363) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 21.3816(20.7438) | Bit/dim 3.4979(3.5001) | Xent 0.0007(0.0030) | Loss 8.6693(9.0236) | Error 0.0000(0.0008) Steps 766(783.10) | Grad Norm 0.7521(1.5679) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 21.7146(20.7450) | Bit/dim 3.5126(3.5026) | Xent 0.0010(0.0028) | Loss 8.7054(8.9325) | Error 0.0000(0.0008) Steps 808(784.04) | Grad Norm 0.8266(1.5511) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 97.0337, Epoch Time 1260.0885(1245.4543), Bit/dim 3.5313(best: 3.5301), Xent 2.6855, Loss 4.8741, Error 0.3383(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 20.4002(20.7388) | Bit/dim 3.5030(3.5019) | Xent 0.0138(0.0037) | Loss 8.6896(9.6505) | Error 0.0033(0.0010) Steps 802(782.67) | Grad Norm 2.5384(1.6422) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 20.8627(20.7260) | Bit/dim 3.5187(3.5030) | Xent 0.0044(0.0041) | Loss 8.7260(9.4146) | Error 0.0022(0.0011) Steps 748(781.98) | Grad Norm 1.4561(1.7575) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 20.7816(20.6813) | Bit/dim 3.4954(3.5009) | Xent 0.0009(0.0036) | Loss 8.6883(9.2265) | Error 0.0000(0.0010) Steps 802(783.63) | Grad Norm 0.6924(1.6077) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 20.4867(20.7145) | Bit/dim 3.4624(3.5017) | Xent 0.0125(0.0041) | Loss 8.6475(9.0817) | Error 0.0022(0.0011) Steps 790(782.38) | Grad Norm 3.1004(1.7967) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 21.1063(20.6854) | Bit/dim 3.5064(3.5016) | Xent 0.0033(0.0046) | Loss 8.6703(8.9781) | Error 0.0011(0.0012) Steps 802(782.39) | Grad Norm 2.4742(1.9800) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 20.5427(20.7331) | Bit/dim 3.4586(3.5023) | Xent 0.0041(0.0045) | Loss 8.6816(8.9114) | Error 0.0022(0.0012) Steps 790(783.46) | Grad Norm 3.4664(2.0256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 96.4071, Epoch Time 1256.4720(1245.7848), Bit/dim 3.5288(best: 3.5301), Xent 2.6467, Loss 4.8521, Error 0.3378(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 21.1055(20.7000) | Bit/dim 3.4955(3.5008) | Xent 0.0032(0.0042) | Loss 8.7418(9.5224) | Error 0.0022(0.0012) Steps 778(784.29) | Grad Norm 3.9958(2.1480) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 20.7972(20.6888) | Bit/dim 3.5276(3.5012) | Xent 0.0036(0.0039) | Loss 8.7826(9.3096) | Error 0.0011(0.0011) Steps 796(785.59) | Grad Norm 2.4438(2.1605) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 20.1030(20.6261) | Bit/dim 3.5066(3.5007) | Xent 0.0013(0.0038) | Loss 8.8200(9.1573) | Error 0.0000(0.0010) Steps 790(785.99) | Grad Norm 0.8901(2.0782) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 20.7152(20.5765) | Bit/dim 3.5235(3.5032) | Xent 0.0047(0.0035) | Loss 8.7897(9.0451) | Error 0.0011(0.0008) Steps 802(786.37) | Grad Norm 1.2767(1.8965) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 21.3194(20.6049) | Bit/dim 3.5082(3.5012) | Xent 0.0031(0.0038) | Loss 8.7667(8.9601) | Error 0.0022(0.0010) Steps 802(787.84) | Grad Norm 2.2164(2.2031) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 96.1810, Epoch Time 1247.4152(1245.8337), Bit/dim 3.5337(best: 3.5288), Xent 2.6950, Loss 4.8812, Error 0.3396(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 20.5340(20.5314) | Bit/dim 3.5106(3.5032) | Xent 0.0056(0.0041) | Loss 8.7816(9.6835) | Error 0.0022(0.0011) Steps 784(787.15) | Grad Norm 2.2747(2.3261) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 19.9978(20.5802) | Bit/dim 3.5011(3.5027) | Xent 0.0027(0.0044) | Loss 8.7466(9.4246) | Error 0.0011(0.0012) Steps 790(784.78) | Grad Norm 2.0566(2.4013) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 20.8952(20.5842) | Bit/dim 3.5325(3.5023) | Xent 0.0109(0.0044) | Loss 8.7258(9.2398) | Error 0.0033(0.0012) Steps 796(785.44) | Grad Norm 2.9650(2.2728) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 20.6305(20.6306) | Bit/dim 3.5272(3.5014) | Xent 0.0028(0.0042) | Loss 8.8263(9.1014) | Error 0.0011(0.0012) Steps 790(786.76) | Grad Norm 2.1084(2.3462) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 20.8999(20.5807) | Bit/dim 3.5130(3.5007) | Xent 0.0013(0.0042) | Loss 8.7550(8.9926) | Error 0.0000(0.0012) Steps 796(784.55) | Grad Norm 1.8183(2.3265) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 20.1997(20.5458) | Bit/dim 3.5206(3.5040) | Xent 0.0016(0.0039) | Loss 8.6955(8.9253) | Error 0.0000(0.0012) Steps 784(784.11) | Grad Norm 1.3532(2.2281) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 96.7666, Epoch Time 1248.7150(1245.9201), Bit/dim 3.5281(best: 3.5288), Xent 2.6471, Loss 4.8517, Error 0.3387(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 21.1672(20.6224) | Bit/dim 3.5112(3.5025) | Xent 0.0008(0.0036) | Loss 8.7551(9.5370) | Error 0.0000(0.0010) Steps 772(782.70) | Grad Norm 1.0407(2.0380) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 20.1183(20.6170) | Bit/dim 3.5104(3.5029) | Xent 0.0014(0.0038) | Loss 8.5866(9.3233) | Error 0.0000(0.0010) Steps 754(781.76) | Grad Norm 1.5162(1.9518) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 19.8585(20.5800) | Bit/dim 3.5090(3.5033) | Xent 0.0031(0.0035) | Loss 8.8083(9.1620) | Error 0.0011(0.0009) Steps 790(781.67) | Grad Norm 2.3237(2.0083) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 20.9001(20.5974) | Bit/dim 3.4894(3.5029) | Xent 0.0036(0.0037) | Loss 8.6814(9.0402) | Error 0.0022(0.0011) Steps 802(781.03) | Grad Norm 2.6367(2.0042) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 21.1843(20.6139) | Bit/dim 3.4891(3.5020) | Xent 0.0066(0.0034) | Loss 8.5983(8.9572) | Error 0.0033(0.0010) Steps 772(782.26) | Grad Norm 2.7437(2.0751) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 98.1299, Epoch Time 1253.1965(1246.1384), Bit/dim 3.5294(best: 3.5281), Xent 2.6843, Loss 4.8716, Error 0.3373(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 20.2952(20.5706) | Bit/dim 3.5063(3.5035) | Xent 0.0039(0.0031) | Loss 8.6771(9.6459) | Error 0.0022(0.0009) Steps 784(781.23) | Grad Norm 2.5732(2.0043) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 20.3461(20.5708) | Bit/dim 3.5001(3.5019) | Xent 0.0034(0.0033) | Loss 8.5906(9.3935) | Error 0.0022(0.0010) Steps 778(783.28) | Grad Norm 2.1310(1.9773) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 20.4975(20.6523) | Bit/dim 3.5097(3.5025) | Xent 0.0128(0.0039) | Loss 8.7878(9.2322) | Error 0.0011(0.0009) Steps 784(783.25) | Grad Norm 1.7299(1.9689) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 20.7721(20.6886) | Bit/dim 3.5054(3.5038) | Xent 0.0030(0.0038) | Loss 8.7715(9.1003) | Error 0.0011(0.0008) Steps 796(785.01) | Grad Norm 2.0734(2.0096) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 19.6992(20.6603) | Bit/dim 3.5046(3.5014) | Xent 0.0047(0.0037) | Loss 8.6091(8.9888) | Error 0.0011(0.0008) Steps 778(785.94) | Grad Norm 2.7165(1.9417) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 21.1587(20.6538) | Bit/dim 3.5059(3.5002) | Xent 0.0184(0.0041) | Loss 8.8575(8.9128) | Error 0.0022(0.0008) Steps 802(787.70) | Grad Norm 3.0457(1.9284) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 98.1790, Epoch Time 1255.8248(1246.4290), Bit/dim 3.5306(best: 3.5281), Xent 2.6690, Loss 4.8651, Error 0.3371(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 20.5335(20.6507) | Bit/dim 3.4682(3.4968) | Xent 0.0035(0.0051) | Loss 8.6776(9.5250) | Error 0.0022(0.0011) Steps 796(785.81) | Grad Norm 2.0254(2.0532) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 20.9018(20.7135) | Bit/dim 3.5094(3.4976) | Xent 0.0084(0.0058) | Loss 8.6950(9.3038) | Error 0.0022(0.0013) Steps 802(786.13) | Grad Norm 2.9869(2.1742) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 20.7059(20.7062) | Bit/dim 3.5044(3.5016) | Xent 0.0015(0.0051) | Loss 8.6736(9.1607) | Error 0.0000(0.0012) Steps 790(784.23) | Grad Norm 1.0816(2.0334) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 21.1841(20.7774) | Bit/dim 3.5177(3.5022) | Xent 0.0006(0.0045) | Loss 8.8365(9.0475) | Error 0.0000(0.0011) Steps 784(784.10) | Grad Norm 0.7250(1.8809) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 20.8856(20.7542) | Bit/dim 3.4544(3.5006) | Xent 0.0020(0.0041) | Loss 8.5770(8.9618) | Error 0.0000(0.0010) Steps 796(786.55) | Grad Norm 1.7975(1.8439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 96.2804, Epoch Time 1260.4521(1246.8497), Bit/dim 3.5300(best: 3.5281), Xent 2.6777, Loss 4.8688, Error 0.3416(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 20.8536(20.7410) | Bit/dim 3.5176(3.5021) | Xent 0.0015(0.0036) | Loss 8.6761(9.7083) | Error 0.0011(0.0009) Steps 778(786.16) | Grad Norm 1.5042(1.7305) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 20.3004(20.7354) | Bit/dim 3.4936(3.4995) | Xent 0.0019(0.0036) | Loss 8.8193(9.4407) | Error 0.0000(0.0009) Steps 802(786.14) | Grad Norm 1.1821(1.6466) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 20.6025(20.7348) | Bit/dim 3.4755(3.4999) | Xent 0.0017(0.0034) | Loss 8.7185(9.2478) | Error 0.0000(0.0008) Steps 808(786.26) | Grad Norm 2.0126(1.6227) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 21.4665(20.7019) | Bit/dim 3.4595(3.5003) | Xent 0.0006(0.0036) | Loss 8.6450(9.1130) | Error 0.0000(0.0009) Steps 772(788.16) | Grad Norm 1.2402(1.6849) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 19.2137(20.7111) | Bit/dim 3.4966(3.4985) | Xent 0.0010(0.0038) | Loss 8.7256(9.0065) | Error 0.0000(0.0010) Steps 790(789.34) | Grad Norm 1.0143(1.7623) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 20.7654(20.6559) | Bit/dim 3.4810(3.4985) | Xent 0.0121(0.0041) | Loss 8.6776(8.9209) | Error 0.0056(0.0012) Steps 766(788.33) | Grad Norm 4.6249(2.0941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 96.7177, Epoch Time 1253.2032(1247.0403), Bit/dim 3.5335(best: 3.5281), Xent 2.6475, Loss 4.8572, Error 0.3336(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 21.5964(20.6704) | Bit/dim 3.4949(3.4980) | Xent 0.0047(0.0042) | Loss 8.8673(9.5425) | Error 0.0011(0.0012) Steps 796(788.24) | Grad Norm 2.5508(2.2414) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 20.3758(20.6450) | Bit/dim 3.4939(3.4992) | Xent 0.0078(0.0041) | Loss 8.6623(9.3141) | Error 0.0022(0.0012) Steps 778(786.23) | Grad Norm 1.9486(2.1711) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 21.0611(20.6740) | Bit/dim 3.5091(3.4991) | Xent 0.0032(0.0038) | Loss 8.7624(9.1538) | Error 0.0011(0.0010) Steps 778(786.79) | Grad Norm 1.9597(1.9708) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 20.3311(20.6371) | Bit/dim 3.4730(3.4984) | Xent 0.0007(0.0039) | Loss 8.7173(9.0391) | Error 0.0000(0.0010) Steps 802(787.43) | Grad Norm 1.4752(2.0525) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 20.2692(20.6687) | Bit/dim 3.5334(3.5013) | Xent 0.0009(0.0037) | Loss 8.8629(8.9619) | Error 0.0000(0.0009) Steps 790(788.09) | Grad Norm 1.2328(1.9613) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 96.4494, Epoch Time 1256.5998(1247.3271), Bit/dim 3.5274(best: 3.5281), Xent 2.6969, Loss 4.8759, Error 0.3390(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 20.3879(20.7218) | Bit/dim 3.4502(3.4975) | Xent 0.0017(0.0038) | Loss 8.7109(9.6651) | Error 0.0000(0.0010) Steps 784(789.28) | Grad Norm 1.4796(1.8428) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 20.4478(20.6741) | Bit/dim 3.4680(3.4974) | Xent 0.0009(0.0034) | Loss 8.6868(9.4173) | Error 0.0000(0.0009) Steps 784(787.20) | Grad Norm 1.4708(1.7699) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 19.8642(20.6344) | Bit/dim 3.5141(3.4989) | Xent 0.0057(0.0038) | Loss 8.7204(9.2313) | Error 0.0022(0.0010) Steps 778(785.73) | Grad Norm 1.8364(1.7980) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 21.2933(20.6793) | Bit/dim 3.4533(3.4977) | Xent 0.0013(0.0033) | Loss 8.6171(9.1009) | Error 0.0000(0.0009) Steps 790(785.26) | Grad Norm 1.1402(1.6821) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 20.6855(20.6852) | Bit/dim 3.4884(3.5004) | Xent 0.0038(0.0033) | Loss 8.7043(8.9983) | Error 0.0022(0.0009) Steps 790(783.83) | Grad Norm 2.7479(1.8508) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 21.3690(20.7084) | Bit/dim 3.5098(3.5009) | Xent 0.0039(0.0036) | Loss 8.6930(8.9283) | Error 0.0011(0.0009) Steps 790(785.81) | Grad Norm 1.6289(1.9045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 97.0566, Epoch Time 1255.1377(1247.5614), Bit/dim 3.5274(best: 3.5274), Xent 2.6374, Loss 4.8461, Error 0.3361(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 20.5493(20.6550) | Bit/dim 3.4927(3.4992) | Xent 0.0026(0.0035) | Loss 8.6493(9.5418) | Error 0.0011(0.0009) Steps 790(783.81) | Grad Norm 2.6517(1.8812) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 20.5041(20.6647) | Bit/dim 3.5246(3.5027) | Xent 0.0050(0.0037) | Loss 8.7387(9.3305) | Error 0.0011(0.0009) Steps 790(782.73) | Grad Norm 2.5604(1.9694) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 20.7350(20.7249) | Bit/dim 3.4899(3.4992) | Xent 0.0021(0.0036) | Loss 8.8193(9.1622) | Error 0.0011(0.0009) Steps 814(782.89) | Grad Norm 1.8903(1.9507) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 20.9409(20.6820) | Bit/dim 3.5142(3.4990) | Xent 0.0012(0.0033) | Loss 8.9100(9.0414) | Error 0.0000(0.0009) Steps 826(785.07) | Grad Norm 1.4896(1.8564) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 20.5462(20.6624) | Bit/dim 3.5039(3.4994) | Xent 0.0125(0.0037) | Loss 8.6755(8.9459) | Error 0.0033(0.0010) Steps 796(784.32) | Grad Norm 3.6567(2.4293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 97.3995, Epoch Time 1257.4518(1247.8581), Bit/dim 3.5297(best: 3.5274), Xent 2.6763, Loss 4.8679, Error 0.3327(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 21.2224(20.6636) | Bit/dim 3.5141(3.5029) | Xent 0.0048(0.0037) | Loss 8.7464(9.6587) | Error 0.0011(0.0010) Steps 742(783.74) | Grad Norm 1.6227(2.4083) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 21.3063(20.6428) | Bit/dim 3.5026(3.5043) | Xent 0.0070(0.0038) | Loss 8.6891(9.4130) | Error 0.0011(0.0010) Steps 790(784.67) | Grad Norm 1.7974(2.2380) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 20.2437(20.6108) | Bit/dim 3.4777(3.5017) | Xent 0.0012(0.0032) | Loss 8.7154(9.2235) | Error 0.0000(0.0009) Steps 772(783.81) | Grad Norm 1.0180(2.0415) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 20.8105(20.6828) | Bit/dim 3.5119(3.5008) | Xent 0.0053(0.0033) | Loss 8.7137(9.0936) | Error 0.0033(0.0009) Steps 778(784.08) | Grad Norm 2.7453(1.8565) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 19.9070(20.6474) | Bit/dim 3.4782(3.4979) | Xent 0.0072(0.0035) | Loss 8.7685(8.9938) | Error 0.0011(0.0010) Steps 784(784.79) | Grad Norm 1.3314(1.8514) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 21.5143(20.6450) | Bit/dim 3.5373(3.4993) | Xent 0.0013(0.0034) | Loss 8.9047(8.9287) | Error 0.0000(0.0009) Steps 748(784.14) | Grad Norm 1.6363(1.8391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 97.1454, Epoch Time 1252.9466(1248.0108), Bit/dim 3.5282(best: 3.5274), Xent 2.6468, Loss 4.8516, Error 0.3334(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 19.9994(20.6463) | Bit/dim 3.4982(3.4996) | Xent 0.0031(0.0035) | Loss 8.6488(9.5385) | Error 0.0011(0.0009) Steps 772(784.31) | Grad Norm 1.4270(1.9382) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 21.5336(20.6775) | Bit/dim 3.5280(3.5014) | Xent 0.0010(0.0032) | Loss 8.7273(9.3224) | Error 0.0000(0.0009) Steps 790(785.38) | Grad Norm 0.9308(1.8673) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 20.6405(20.6585) | Bit/dim 3.4964(3.4995) | Xent 0.0017(0.0029) | Loss 8.7247(9.1508) | Error 0.0000(0.0008) Steps 808(786.48) | Grad Norm 1.0078(1.7589) | Total Time 0.00(0.00)\n",
      "Iter 17970 | Time 20.7409(20.6260) | Bit/dim 3.4562(3.4984) | Xent 0.0022(0.0027) | Loss 8.6638(9.0318) | Error 0.0011(0.0007) Steps 808(787.17) | Grad Norm 1.8228(1.7728) | Total Time 0.00(0.00)\n",
      "Iter 17980 | Time 20.0022(20.6762) | Bit/dim 3.4813(3.4993) | Xent 0.0033(0.0031) | Loss 8.6603(8.9492) | Error 0.0011(0.0008) Steps 784(786.47) | Grad Norm 2.3620(1.8955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 96.7176, Epoch Time 1256.1115(1248.2538), Bit/dim 3.5279(best: 3.5274), Xent 2.6618, Loss 4.8588, Error 0.3371(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 20.8192(20.7688) | Bit/dim 3.4951(3.5006) | Xent 0.0025(0.0037) | Loss 8.6645(9.6636) | Error 0.0011(0.0010) Steps 766(787.01) | Grad Norm 1.6546(1.9854) | Total Time 0.00(0.00)\n",
      "Iter 18000 | Time 20.2452(20.7697) | Bit/dim 3.4843(3.4995) | Xent 0.0030(0.0035) | Loss 8.6819(9.4193) | Error 0.0011(0.0009) Steps 790(789.44) | Grad Norm 1.4762(1.9080) | Total Time 0.00(0.00)\n",
      "Iter 18010 | Time 20.5494(20.6984) | Bit/dim 3.5174(3.4992) | Xent 0.0050(0.0041) | Loss 8.7774(9.2296) | Error 0.0011(0.0010) Steps 790(788.15) | Grad Norm 2.2578(2.0553) | Total Time 0.00(0.00)\n",
      "Iter 18020 | Time 21.4677(20.7145) | Bit/dim 3.5248(3.5001) | Xent 0.0008(0.0035) | Loss 8.8667(9.1036) | Error 0.0000(0.0009) Steps 772(784.69) | Grad Norm 0.7642(1.8648) | Total Time 0.00(0.00)\n",
      "Iter 18030 | Time 20.8192(20.7673) | Bit/dim 3.5202(3.5002) | Xent 0.0004(0.0032) | Loss 8.8324(9.0137) | Error 0.0000(0.0007) Steps 796(785.81) | Grad Norm 0.8456(1.7254) | Total Time 0.00(0.00)\n",
      "Iter 18040 | Time 20.6134(20.7366) | Bit/dim 3.5266(3.4984) | Xent 0.0101(0.0036) | Loss 8.7690(8.9288) | Error 0.0011(0.0007) Steps 796(787.40) | Grad Norm 1.8944(1.6879) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 97.2100, Epoch Time 1259.3912(1248.5879), Bit/dim 3.5254(best: 3.5274), Xent 2.6535, Loss 4.8521, Error 0.3357(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 20.7411(20.7455) | Bit/dim 3.5374(3.4958) | Xent 0.0014(0.0034) | Loss 8.8863(9.5148) | Error 0.0000(0.0007) Steps 790(786.45) | Grad Norm 0.9186(1.5996) | Total Time 0.00(0.00)\n",
      "Iter 18060 | Time 20.3447(20.7628) | Bit/dim 3.5115(3.4959) | Xent 0.0092(0.0039) | Loss 8.7233(9.3023) | Error 0.0022(0.0009) Steps 790(787.83) | Grad Norm 4.3264(1.8511) | Total Time 0.00(0.00)\n",
      "Iter 18070 | Time 21.1014(20.7399) | Bit/dim 3.4762(3.4965) | Xent 0.0025(0.0040) | Loss 8.6518(9.1412) | Error 0.0000(0.0009) Steps 796(786.90) | Grad Norm 1.5303(1.9441) | Total Time 0.00(0.00)\n",
      "Iter 18080 | Time 21.0283(20.6910) | Bit/dim 3.5360(3.4995) | Xent 0.0120(0.0048) | Loss 8.9139(9.0411) | Error 0.0011(0.0010) Steps 802(786.38) | Grad Norm 1.4545(2.0363) | Total Time 0.00(0.00)\n",
      "Iter 18090 | Time 20.2830(20.6343) | Bit/dim 3.5139(3.4998) | Xent 0.0022(0.0050) | Loss 8.6404(8.9418) | Error 0.0000(0.0012) Steps 778(785.91) | Grad Norm 2.2607(2.2004) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 97.3877, Epoch Time 1254.3726(1248.7615), Bit/dim 3.5328(best: 3.5254), Xent 2.6386, Loss 4.8521, Error 0.3339(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 21.6672(20.6925) | Bit/dim 3.4642(3.4987) | Xent 0.0015(0.0048) | Loss 8.6675(9.6706) | Error 0.0000(0.0012) Steps 802(786.41) | Grad Norm 2.0664(2.3613) | Total Time 0.00(0.00)\n",
      "Iter 18110 | Time 19.8680(20.6833) | Bit/dim 3.5347(3.4975) | Xent 0.0005(0.0041) | Loss 8.7581(9.4250) | Error 0.0000(0.0011) Steps 778(785.17) | Grad Norm 1.0963(2.3459) | Total Time 0.00(0.00)\n",
      "Iter 18120 | Time 20.0328(20.6767) | Bit/dim 3.5277(3.4992) | Xent 0.0008(0.0043) | Loss 8.7287(9.2335) | Error 0.0000(0.0011) Steps 766(784.99) | Grad Norm 1.9604(2.3398) | Total Time 0.00(0.00)\n",
      "Iter 18130 | Time 20.4200(20.7305) | Bit/dim 3.5050(3.4979) | Xent 0.0075(0.0038) | Loss 8.7732(9.0800) | Error 0.0022(0.0010) Steps 790(785.18) | Grad Norm 2.7882(2.1813) | Total Time 0.00(0.00)\n",
      "Iter 18140 | Time 20.8243(20.7126) | Bit/dim 3.4821(3.4994) | Xent 0.0025(0.0039) | Loss 8.6790(8.9873) | Error 0.0011(0.0010) Steps 790(785.99) | Grad Norm 1.6599(2.1772) | Total Time 0.00(0.00)\n",
      "Iter 18150 | Time 20.7789(20.7161) | Bit/dim 3.4984(3.5025) | Xent 0.0019(0.0038) | Loss 8.7075(8.9236) | Error 0.0000(0.0009) Steps 808(787.92) | Grad Norm 1.6882(2.1333) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 96.4118, Epoch Time 1260.9939(1249.1284), Bit/dim 3.5292(best: 3.5254), Xent 2.7149, Loss 4.8867, Error 0.3357(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 20.4131(20.6986) | Bit/dim 3.4881(3.4991) | Xent 0.0025(0.0042) | Loss 8.6328(9.5523) | Error 0.0011(0.0011) Steps 778(787.34) | Grad Norm 1.7164(2.2446) | Total Time 0.00(0.00)\n",
      "Iter 18170 | Time 20.7953(20.6887) | Bit/dim 3.5063(3.4984) | Xent 0.0170(0.0049) | Loss 8.6873(9.3312) | Error 0.0022(0.0011) Steps 784(787.61) | Grad Norm 3.3231(2.2916) | Total Time 0.00(0.00)\n",
      "Iter 18180 | Time 20.4069(20.6546) | Bit/dim 3.5287(3.5002) | Xent 0.0013(0.0050) | Loss 8.7859(9.1656) | Error 0.0000(0.0012) Steps 754(787.84) | Grad Norm 0.8966(2.3987) | Total Time 0.00(0.00)\n",
      "Iter 18190 | Time 20.8138(20.6580) | Bit/dim 3.5086(3.4996) | Xent 0.0009(0.0056) | Loss 8.7993(9.0514) | Error 0.0000(0.0013) Steps 790(787.35) | Grad Norm 0.9036(2.3358) | Total Time 0.00(0.00)\n",
      "Iter 18200 | Time 19.8112(20.6123) | Bit/dim 3.5206(3.5033) | Xent 0.0008(0.0048) | Loss 8.6204(8.9598) | Error 0.0000(0.0012) Steps 778(785.86) | Grad Norm 0.8904(2.1291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 95.9921, Epoch Time 1253.8382(1249.2697), Bit/dim 3.5291(best: 3.5254), Xent 2.6583, Loss 4.8582, Error 0.3327(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 20.7789(20.6573) | Bit/dim 3.4988(3.5001) | Xent 0.0017(0.0045) | Loss 8.6031(9.6411) | Error 0.0011(0.0011) Steps 766(784.23) | Grad Norm 2.0977(2.0291) | Total Time 0.00(0.00)\n",
      "Iter 18220 | Time 20.3950(20.6350) | Bit/dim 3.5215(3.5026) | Xent 0.0159(0.0045) | Loss 8.7232(9.4038) | Error 0.0022(0.0010) Steps 808(784.33) | Grad Norm 2.6462(1.9478) | Total Time 0.00(0.00)\n",
      "Iter 18230 | Time 20.1343(20.6252) | Bit/dim 3.5048(3.5024) | Xent 0.0026(0.0041) | Loss 8.6964(9.2138) | Error 0.0011(0.0011) Steps 760(781.64) | Grad Norm 1.3941(2.1100) | Total Time 0.00(0.00)\n",
      "Iter 18240 | Time 20.4789(20.6678) | Bit/dim 3.4779(3.5019) | Xent 0.0012(0.0041) | Loss 8.6390(9.0857) | Error 0.0000(0.0011) Steps 772(782.44) | Grad Norm 1.1897(2.2088) | Total Time 0.00(0.00)\n",
      "Iter 18250 | Time 20.4448(20.5938) | Bit/dim 3.4897(3.5005) | Xent 0.0021(0.0044) | Loss 8.6870(8.9804) | Error 0.0011(0.0012) Steps 784(783.24) | Grad Norm 1.5596(2.3598) | Total Time 0.00(0.00)\n",
      "Iter 18260 | Time 20.7808(20.5483) | Bit/dim 3.4792(3.5002) | Xent 0.0024(0.0044) | Loss 8.5729(8.9054) | Error 0.0000(0.0012) Steps 760(782.86) | Grad Norm 1.1037(2.2867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 96.4231, Epoch Time 1248.0203(1249.2322), Bit/dim 3.5284(best: 3.5254), Xent 2.7489, Loss 4.9028, Error 0.3429(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 20.6161(20.5344) | Bit/dim 3.5049(3.4996) | Xent 0.0012(0.0042) | Loss 8.7282(9.5155) | Error 0.0000(0.0012) Steps 796(782.99) | Grad Norm 0.9151(2.1642) | Total Time 0.00(0.00)\n",
      "Iter 18280 | Time 20.6181(20.5809) | Bit/dim 3.5068(3.4998) | Xent 0.0018(0.0041) | Loss 8.7157(9.3082) | Error 0.0000(0.0012) Steps 784(783.70) | Grad Norm 2.0265(2.2524) | Total Time 0.00(0.00)\n",
      "Iter 18290 | Time 20.1748(20.5933) | Bit/dim 3.4589(3.5006) | Xent 0.0110(0.0040) | Loss 8.5766(9.1482) | Error 0.0011(0.0011) Steps 778(782.48) | Grad Norm 1.2241(2.0127) | Total Time 0.00(0.00)\n",
      "Iter 18300 | Time 20.5815(20.6652) | Bit/dim 3.5058(3.4990) | Xent 0.0010(0.0035) | Loss 8.7954(9.0226) | Error 0.0000(0.0011) Steps 784(782.62) | Grad Norm 0.9402(1.8924) | Total Time 0.00(0.00)\n",
      "Iter 18310 | Time 20.3262(20.7289) | Bit/dim 3.5055(3.4978) | Xent 0.0142(0.0039) | Loss 8.7824(8.9284) | Error 0.0011(0.0009) Steps 790(782.41) | Grad Norm 0.8539(1.6941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 96.4052, Epoch Time 1263.0707(1249.6474), Bit/dim 3.5244(best: 3.5254), Xent 2.7101, Loss 4.8795, Error 0.3372(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 20.2646(20.8002) | Bit/dim 3.4811(3.4969) | Xent 0.0079(0.0039) | Loss 8.7109(9.6490) | Error 0.0011(0.0009) Steps 790(781.35) | Grad Norm 2.0225(1.6713) | Total Time 0.00(0.00)\n",
      "Iter 18330 | Time 19.8629(20.7138) | Bit/dim 3.4900(3.4982) | Xent 0.0022(0.0037) | Loss 8.6705(9.4005) | Error 0.0011(0.0010) Steps 772(778.94) | Grad Norm 1.6892(1.9361) | Total Time 0.00(0.00)\n",
      "Iter 18340 | Time 20.0610(20.6776) | Bit/dim 3.4612(3.4968) | Xent 0.0076(0.0052) | Loss 8.6227(9.2137) | Error 0.0033(0.0013) Steps 784(778.83) | Grad Norm 1.9647(2.1728) | Total Time 0.00(0.00)\n",
      "Iter 18350 | Time 21.4187(20.6743) | Bit/dim 3.4915(3.4980) | Xent 0.0135(0.0051) | Loss 8.6665(9.0800) | Error 0.0033(0.0013) Steps 784(779.08) | Grad Norm 3.8267(2.2881) | Total Time 0.00(0.00)\n",
      "Iter 18360 | Time 20.8368(20.6450) | Bit/dim 3.5005(3.4985) | Xent 0.0006(0.0042) | Loss 8.7470(8.9838) | Error 0.0000(0.0011) Steps 784(779.05) | Grad Norm 1.1691(2.1445) | Total Time 0.00(0.00)\n",
      "Iter 18370 | Time 19.9443(20.6035) | Bit/dim 3.5111(3.4991) | Xent 0.0060(0.0044) | Loss 8.7512(8.9159) | Error 0.0033(0.0011) Steps 790(780.87) | Grad Norm 2.5787(2.1588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 96.4654, Epoch Time 1251.9617(1249.7168), Bit/dim 3.5307(best: 3.5244), Xent 2.6623, Loss 4.8619, Error 0.3356(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18380 | Time 19.8453(20.6000) | Bit/dim 3.4804(3.4947) | Xent 0.0007(0.0040) | Loss 8.6998(9.5139) | Error 0.0000(0.0010) Steps 802(781.68) | Grad Norm 1.5014(2.0106) | Total Time 0.00(0.00)\n",
      "Iter 18390 | Time 20.6481(20.6561) | Bit/dim 3.5304(3.4968) | Xent 0.0090(0.0041) | Loss 8.7370(9.3001) | Error 0.0011(0.0010) Steps 754(781.76) | Grad Norm 1.2370(1.9137) | Total Time 0.00(0.00)\n",
      "Iter 18400 | Time 21.0380(20.6825) | Bit/dim 3.4759(3.4971) | Xent 0.0007(0.0038) | Loss 8.7079(9.1339) | Error 0.0000(0.0009) Steps 796(782.94) | Grad Norm 1.0546(1.8375) | Total Time 0.00(0.00)\n",
      "Iter 18410 | Time 20.7595(20.6484) | Bit/dim 3.5123(3.4977) | Xent 0.0022(0.0036) | Loss 8.7021(9.0249) | Error 0.0011(0.0009) Steps 754(783.02) | Grad Norm 1.8287(1.7488) | Total Time 0.00(0.00)\n",
      "Iter 18420 | Time 20.6310(20.5965) | Bit/dim 3.4871(3.4986) | Xent 0.0071(0.0039) | Loss 8.6293(8.9344) | Error 0.0022(0.0011) Steps 778(783.13) | Grad Norm 3.7970(2.1036) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 98.2010, Epoch Time 1252.4539(1249.7989), Bit/dim 3.5292(best: 3.5244), Xent 2.6953, Loss 4.8769, Error 0.3415(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18430 | Time 20.8070(20.5732) | Bit/dim 3.4550(3.4980) | Xent 0.0060(0.0036) | Loss 8.5177(9.6277) | Error 0.0033(0.0011) Steps 772(780.64) | Grad Norm 2.5836(2.2324) | Total Time 0.00(0.00)\n",
      "Iter 18440 | Time 20.2814(20.6333) | Bit/dim 3.4805(3.4970) | Xent 0.0071(0.0039) | Loss 8.6228(9.3788) | Error 0.0011(0.0011) Steps 772(781.14) | Grad Norm 1.6270(2.2568) | Total Time 0.00(0.00)\n",
      "Iter 18450 | Time 20.6841(20.7028) | Bit/dim 3.5032(3.4990) | Xent 0.0053(0.0038) | Loss 8.6748(9.2072) | Error 0.0011(0.0010) Steps 772(782.89) | Grad Norm 1.3516(2.1795) | Total Time 0.00(0.00)\n",
      "Iter 18460 | Time 20.4541(20.6663) | Bit/dim 3.4991(3.5007) | Xent 0.0031(0.0041) | Loss 8.6630(9.0716) | Error 0.0011(0.0010) Steps 760(781.26) | Grad Norm 2.3635(2.1470) | Total Time 0.00(0.00)\n",
      "Iter 18470 | Time 20.5719(20.6586) | Bit/dim 3.5145(3.5017) | Xent 0.0042(0.0037) | Loss 8.7596(8.9818) | Error 0.0022(0.0010) Steps 790(781.39) | Grad Norm 2.4531(2.0659) | Total Time 0.00(0.00)\n",
      "Iter 18480 | Time 20.9280(20.7031) | Bit/dim 3.5058(3.4992) | Xent 0.0066(0.0039) | Loss 8.6032(8.9024) | Error 0.0011(0.0010) Steps 790(783.49) | Grad Norm 1.2096(1.9730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 96.1322, Epoch Time 1257.6290(1250.0338), Bit/dim 3.5306(best: 3.5244), Xent 2.7067, Loss 4.8839, Error 0.3370(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18490 | Time 21.7056(20.8122) | Bit/dim 3.5289(3.5017) | Xent 0.0016(0.0038) | Loss 8.7729(9.5128) | Error 0.0000(0.0010) Steps 760(783.01) | Grad Norm 1.3463(1.9366) | Total Time 0.00(0.00)\n",
      "Iter 18500 | Time 20.3739(20.7394) | Bit/dim 3.5177(3.4994) | Xent 0.0025(0.0034) | Loss 8.6818(9.2905) | Error 0.0011(0.0009) Steps 766(781.78) | Grad Norm 1.5409(1.8221) | Total Time 0.00(0.00)\n",
      "Iter 18510 | Time 20.0218(20.6813) | Bit/dim 3.5281(3.4998) | Xent 0.0017(0.0036) | Loss 8.6982(9.1281) | Error 0.0011(0.0010) Steps 778(781.28) | Grad Norm 1.2863(1.9698) | Total Time 0.00(0.00)\n",
      "Iter 18520 | Time 20.7467(20.7208) | Bit/dim 3.4811(3.5000) | Xent 0.0012(0.0031) | Loss 8.5920(9.0128) | Error 0.0000(0.0008) Steps 766(779.89) | Grad Norm 1.5551(1.8936) | Total Time 0.00(0.00)\n",
      "Iter 18530 | Time 20.8509(20.7237) | Bit/dim 3.4666(3.4973) | Xent 0.0006(0.0030) | Loss 8.5191(8.9093) | Error 0.0000(0.0008) Steps 772(777.23) | Grad Norm 0.7077(1.8137) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 97.0377, Epoch Time 1261.9595(1250.3916), Bit/dim 3.5268(best: 3.5244), Xent 2.6942, Loss 4.8739, Error 0.3356(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18540 | Time 20.0221(20.6691) | Bit/dim 3.5166(3.4963) | Xent 0.0013(0.0035) | Loss 8.7319(9.6419) | Error 0.0000(0.0009) Steps 796(777.76) | Grad Norm 1.9487(1.9619) | Total Time 0.00(0.00)\n",
      "Iter 18550 | Time 20.0024(20.6124) | Bit/dim 3.4761(3.4955) | Xent 0.0008(0.0034) | Loss 8.6920(9.3850) | Error 0.0000(0.0008) Steps 760(777.91) | Grad Norm 1.2420(1.9654) | Total Time 0.00(0.00)\n",
      "Iter 18560 | Time 20.6539(20.6724) | Bit/dim 3.5042(3.4962) | Xent 0.0007(0.0035) | Loss 8.7239(9.1999) | Error 0.0000(0.0009) Steps 790(779.60) | Grad Norm 1.3553(2.0041) | Total Time 0.00(0.00)\n",
      "Iter 18570 | Time 21.0575(20.7632) | Bit/dim 3.5469(3.4966) | Xent 0.0132(0.0043) | Loss 8.7755(9.0593) | Error 0.0067(0.0012) Steps 748(778.89) | Grad Norm 3.5434(2.2383) | Total Time 0.00(0.00)\n",
      "Iter 18580 | Time 20.1819(20.7217) | Bit/dim 3.5101(3.4994) | Xent 0.0027(0.0045) | Loss 8.7907(8.9779) | Error 0.0011(0.0012) Steps 754(777.05) | Grad Norm 2.5016(2.3305) | Total Time 0.00(0.00)\n",
      "Iter 18590 | Time 20.5175(20.7064) | Bit/dim 3.4878(3.4986) | Xent 0.0008(0.0040) | Loss 8.7047(8.9008) | Error 0.0000(0.0011) Steps 778(776.54) | Grad Norm 1.0567(2.1480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 96.4045, Epoch Time 1254.4589(1250.5136), Bit/dim 3.5282(best: 3.5244), Xent 2.6999, Loss 4.8781, Error 0.3351(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18600 | Time 20.1717(20.6949) | Bit/dim 3.4721(3.4987) | Xent 0.0056(0.0036) | Loss 8.6144(9.5262) | Error 0.0011(0.0010) Steps 790(780.22) | Grad Norm 2.2884(2.0061) | Total Time 0.00(0.00)\n",
      "Iter 18610 | Time 21.1085(20.6988) | Bit/dim 3.4691(3.4959) | Xent 0.0033(0.0036) | Loss 8.6174(9.3054) | Error 0.0011(0.0010) Steps 760(780.04) | Grad Norm 1.3058(2.1376) | Total Time 0.00(0.00)\n",
      "Iter 18620 | Time 20.5667(20.6866) | Bit/dim 3.4932(3.4963) | Xent 0.0011(0.0035) | Loss 8.6158(9.1430) | Error 0.0000(0.0010) Steps 796(783.55) | Grad Norm 1.4218(2.1061) | Total Time 0.00(0.00)\n",
      "Iter 18630 | Time 20.6227(20.6730) | Bit/dim 3.5325(3.4998) | Xent 0.0022(0.0036) | Loss 8.8033(9.0298) | Error 0.0011(0.0011) Steps 814(781.75) | Grad Norm 1.7847(2.0513) | Total Time 0.00(0.00)\n",
      "Iter 18640 | Time 20.8852(20.7400) | Bit/dim 3.5126(3.4985) | Xent 0.0042(0.0040) | Loss 8.7137(8.9381) | Error 0.0011(0.0013) Steps 802(781.65) | Grad Norm 2.0153(2.1038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 96.4203, Epoch Time 1257.7547(1250.7309), Bit/dim 3.5297(best: 3.5244), Xent 2.7142, Loss 4.8868, Error 0.3344(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18650 | Time 21.0296(20.7707) | Bit/dim 3.5077(3.4988) | Xent 0.0101(0.0041) | Loss 8.7553(9.6567) | Error 0.0011(0.0012) Steps 760(779.53) | Grad Norm 3.7400(2.0064) | Total Time 0.00(0.00)\n",
      "Iter 18660 | Time 20.8821(20.7218) | Bit/dim 3.4769(3.4952) | Xent 0.0121(0.0040) | Loss 8.6228(9.3935) | Error 0.0022(0.0012) Steps 748(779.38) | Grad Norm 4.4969(1.9546) | Total Time 0.00(0.00)\n",
      "Iter 18670 | Time 20.1947(20.6852) | Bit/dim 3.4939(3.4971) | Xent 0.0006(0.0037) | Loss 8.6745(9.2054) | Error 0.0000(0.0011) Steps 796(781.70) | Grad Norm 1.1157(1.8448) | Total Time 0.00(0.00)\n",
      "Iter 18680 | Time 20.9585(20.6315) | Bit/dim 3.4934(3.4974) | Xent 0.0006(0.0031) | Loss 8.7458(9.0770) | Error 0.0000(0.0009) Steps 790(781.74) | Grad Norm 0.6687(1.6697) | Total Time 0.00(0.00)\n",
      "Iter 18690 | Time 20.6641(20.6862) | Bit/dim 3.4931(3.4962) | Xent 0.0012(0.0029) | Loss 8.7401(8.9756) | Error 0.0000(0.0008) Steps 772(780.17) | Grad Norm 1.5326(1.6331) | Total Time 0.00(0.00)\n",
      "Iter 18700 | Time 20.8120(20.6659) | Bit/dim 3.5026(3.4978) | Xent 0.0013(0.0030) | Loss 8.7946(8.9013) | Error 0.0000(0.0007) Steps 772(781.28) | Grad Norm 1.9788(1.7553) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 95.4801, Epoch Time 1251.9261(1250.7667), Bit/dim 3.5245(best: 3.5244), Xent 2.7340, Loss 4.8914, Error 0.3391(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18710 | Time 20.3203(20.6288) | Bit/dim 3.5202(3.4987) | Xent 0.0027(0.0029) | Loss 8.6761(9.5187) | Error 0.0011(0.0007) Steps 778(780.95) | Grad Norm 2.6114(1.8622) | Total Time 0.00(0.00)\n",
      "Iter 18720 | Time 20.5342(20.6024) | Bit/dim 3.4836(3.4978) | Xent 0.0015(0.0029) | Loss 8.7553(9.2959) | Error 0.0000(0.0007) Steps 784(780.70) | Grad Norm 1.2041(1.9914) | Total Time 0.00(0.00)\n",
      "Iter 18730 | Time 20.1285(20.5177) | Bit/dim 3.4726(3.4952) | Xent 0.0100(0.0031) | Loss 8.5695(9.1269) | Error 0.0033(0.0008) Steps 748(781.26) | Grad Norm 1.9417(1.9524) | Total Time 0.00(0.00)\n",
      "Iter 18740 | Time 20.7516(20.5304) | Bit/dim 3.4679(3.4959) | Xent 0.0067(0.0032) | Loss 8.6622(9.0183) | Error 0.0011(0.0008) Steps 784(781.40) | Grad Norm 1.1916(1.9420) | Total Time 0.00(0.00)\n",
      "Iter 18750 | Time 21.7060(20.7012) | Bit/dim 3.4670(3.4946) | Xent 0.0154(0.0035) | Loss 8.7634(8.9319) | Error 0.0033(0.0008) Steps 796(780.66) | Grad Norm 3.4923(1.9127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 95.4828, Epoch Time 1252.5478(1250.8202), Bit/dim 3.5278(best: 3.5244), Xent 2.7565, Loss 4.9061, Error 0.3339(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18760 | Time 20.8210(20.7282) | Bit/dim 3.5227(3.4967) | Xent 0.0012(0.0030) | Loss 8.8052(9.6363) | Error 0.0011(0.0007) Steps 778(781.14) | Grad Norm 1.5185(1.7653) | Total Time 0.00(0.00)\n",
      "Iter 18770 | Time 20.3636(20.7484) | Bit/dim 3.4948(3.4964) | Xent 0.0005(0.0027) | Loss 8.7752(9.3894) | Error 0.0000(0.0007) Steps 796(781.51) | Grad Norm 0.6316(1.6620) | Total Time 0.00(0.00)\n",
      "Iter 18780 | Time 20.7118(20.6694) | Bit/dim 3.4732(3.4970) | Xent 0.0022(0.0027) | Loss 8.7353(9.2051) | Error 0.0011(0.0008) Steps 796(779.77) | Grad Norm 1.0541(1.5637) | Total Time 0.00(0.00)\n",
      "Iter 18790 | Time 20.5154(20.6523) | Bit/dim 3.5185(3.4981) | Xent 0.0035(0.0025) | Loss 8.8080(9.0805) | Error 0.0011(0.0007) Steps 766(777.95) | Grad Norm 3.1080(1.6292) | Total Time 0.00(0.00)\n",
      "Iter 18800 | Time 21.4376(20.6275) | Bit/dim 3.4736(3.4974) | Xent 0.0042(0.0035) | Loss 8.6679(8.9865) | Error 0.0011(0.0008) Steps 814(779.30) | Grad Norm 3.6767(2.0307) | Total Time 0.00(0.00)\n",
      "Iter 18810 | Time 21.2753(20.5633) | Bit/dim 3.5067(3.4978) | Xent 0.0017(0.0041) | Loss 8.7428(8.9115) | Error 0.0000(0.0008) Steps 772(779.83) | Grad Norm 1.6161(2.0942) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 95.7860, Epoch Time 1250.8277(1250.8204), Bit/dim 3.5234(best: 3.5244), Xent 2.7337, Loss 4.8903, Error 0.3363(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18820 | Time 20.7663(20.6624) | Bit/dim 3.5001(3.4959) | Xent 0.0063(0.0039) | Loss 8.6398(9.5130) | Error 0.0011(0.0009) Steps 754(778.91) | Grad Norm 5.3381(2.1159) | Total Time 0.00(0.00)\n",
      "Iter 18830 | Time 20.6551(20.6806) | Bit/dim 3.5072(3.4950) | Xent 0.0034(0.0039) | Loss 8.7429(9.2974) | Error 0.0011(0.0010) Steps 772(778.42) | Grad Norm 2.2195(2.2402) | Total Time 0.00(0.00)\n",
      "Iter 18840 | Time 20.4545(20.6495) | Bit/dim 3.5108(3.4946) | Xent 0.0027(0.0044) | Loss 8.6827(9.1405) | Error 0.0000(0.0011) Steps 754(777.80) | Grad Norm 2.6876(2.3767) | Total Time 0.00(0.00)\n",
      "Iter 18850 | Time 20.8276(20.6606) | Bit/dim 3.5258(3.4966) | Xent 0.0024(0.0048) | Loss 8.6619(9.0267) | Error 0.0011(0.0013) Steps 784(778.81) | Grad Norm 2.0088(2.5908) | Total Time 0.00(0.00)\n",
      "Iter 18860 | Time 20.3707(20.6385) | Bit/dim 3.4903(3.4984) | Xent 0.0042(0.0047) | Loss 8.6497(8.9409) | Error 0.0011(0.0012) Steps 766(778.55) | Grad Norm 1.9176(2.4395) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 96.7222, Epoch Time 1256.0251(1250.9765), Bit/dim 3.5303(best: 3.5234), Xent 2.7347, Loss 4.8976, Error 0.3378(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18870 | Time 21.4484(20.6422) | Bit/dim 3.4686(3.5002) | Xent 0.0060(0.0045) | Loss 8.6098(9.6649) | Error 0.0011(0.0012) Steps 802(779.07) | Grad Norm 2.2964(2.3421) | Total Time 0.00(0.00)\n",
      "Iter 18880 | Time 20.5077(20.5994) | Bit/dim 3.5088(3.4995) | Xent 0.0015(0.0044) | Loss 8.7486(9.4109) | Error 0.0011(0.0012) Steps 790(781.70) | Grad Norm 2.3756(2.2156) | Total Time 0.00(0.00)\n",
      "Iter 18890 | Time 20.5209(20.6242) | Bit/dim 3.4616(3.4985) | Xent 0.0007(0.0048) | Loss 8.6082(9.2282) | Error 0.0000(0.0012) Steps 766(778.87) | Grad Norm 0.9142(2.2314) | Total Time 0.00(0.00)\n",
      "Iter 18900 | Time 20.4586(20.6392) | Bit/dim 3.4972(3.4995) | Xent 0.0060(0.0041) | Loss 8.6545(9.0873) | Error 0.0033(0.0012) Steps 778(778.48) | Grad Norm 3.3022(2.1532) | Total Time 0.00(0.00)\n",
      "Iter 18910 | Time 21.0023(20.7319) | Bit/dim 3.5064(3.4995) | Xent 0.0026(0.0038) | Loss 8.5577(8.9830) | Error 0.0011(0.0012) Steps 766(779.24) | Grad Norm 3.3109(2.2869) | Total Time 0.00(0.00)\n",
      "Iter 18920 | Time 20.4986(20.7543) | Bit/dim 3.5299(3.4984) | Xent 0.0076(0.0037) | Loss 8.8369(8.9036) | Error 0.0022(0.0011) Steps 784(778.72) | Grad Norm 4.6103(2.3640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 97.4479, Epoch Time 1262.0600(1251.3090), Bit/dim 3.5271(best: 3.5234), Xent 2.7685, Loss 4.9114, Error 0.3405(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18930 | Time 20.5451(20.8100) | Bit/dim 3.4968(3.4959) | Xent 0.0081(0.0045) | Loss 8.7276(9.5024) | Error 0.0011(0.0012) Steps 808(781.72) | Grad Norm 2.3233(2.4571) | Total Time 0.00(0.00)\n",
      "Iter 18940 | Time 20.6300(20.8389) | Bit/dim 3.4977(3.4955) | Xent 0.0047(0.0043) | Loss 8.8239(9.2914) | Error 0.0011(0.0011) Steps 772(781.72) | Grad Norm 5.6534(2.5588) | Total Time 0.00(0.00)\n",
      "Iter 18950 | Time 20.4030(20.7851) | Bit/dim 3.4738(3.4967) | Xent 0.0101(0.0042) | Loss 8.5803(9.1388) | Error 0.0022(0.0013) Steps 772(781.15) | Grad Norm 2.2157(2.6125) | Total Time 0.00(0.00)\n",
      "Iter 18960 | Time 20.2412(20.7597) | Bit/dim 3.5211(3.4979) | Xent 0.0010(0.0039) | Loss 8.8252(9.0376) | Error 0.0000(0.0012) Steps 772(780.83) | Grad Norm 1.4895(2.5318) | Total Time 0.00(0.00)\n",
      "Iter 18970 | Time 20.4729(20.7170) | Bit/dim 3.5096(3.5015) | Xent 0.0008(0.0033) | Loss 8.7266(8.9511) | Error 0.0000(0.0010) Steps 772(781.93) | Grad Norm 1.0742(2.2348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 96.4030, Epoch Time 1264.4432(1251.7030), Bit/dim 3.5207(best: 3.5234), Xent 2.7605, Loss 4.9009, Error 0.3427(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18980 | Time 20.0268(20.7242) | Bit/dim 3.4988(3.4977) | Xent 0.0020(0.0029) | Loss 8.7143(9.6500) | Error 0.0000(0.0008) Steps 802(783.07) | Grad Norm 0.8493(1.8959) | Total Time 0.00(0.00)\n",
      "Iter 18990 | Time 21.5490(20.7545) | Bit/dim 3.4513(3.4974) | Xent 0.0029(0.0028) | Loss 8.6676(9.3947) | Error 0.0022(0.0008) Steps 802(782.53) | Grad Norm 1.5875(1.8459) | Total Time 0.00(0.00)\n",
      "Iter 19000 | Time 20.4788(20.6945) | Bit/dim 3.5180(3.4996) | Xent 0.0020(0.0031) | Loss 8.7413(9.2120) | Error 0.0011(0.0009) Steps 754(780.24) | Grad Norm 1.2168(1.8226) | Total Time 0.00(0.00)\n",
      "Iter 19010 | Time 20.5482(20.6777) | Bit/dim 3.4789(3.4977) | Xent 0.0023(0.0030) | Loss 8.7143(9.0769) | Error 0.0011(0.0009) Steps 778(778.66) | Grad Norm 1.0980(1.7202) | Total Time 0.00(0.00)\n",
      "Iter 19020 | Time 20.8804(20.6427) | Bit/dim 3.5037(3.4987) | Xent 0.0067(0.0033) | Loss 8.6694(8.9723) | Error 0.0022(0.0010) Steps 772(779.67) | Grad Norm 2.5080(1.7747) | Total Time 0.00(0.00)\n",
      "Iter 19030 | Time 19.9249(20.6064) | Bit/dim 3.4762(3.4977) | Xent 0.0025(0.0034) | Loss 8.6043(8.8969) | Error 0.0011(0.0009) Steps 766(779.56) | Grad Norm 1.6981(1.7294) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 96.1369, Epoch Time 1249.8148(1251.6464), Bit/dim 3.5256(best: 3.5207), Xent 2.7406, Loss 4.8959, Error 0.3394(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19040 | Time 20.9288(20.6628) | Bit/dim 3.5109(3.4984) | Xent 0.0047(0.0038) | Loss 8.7186(9.5068) | Error 0.0011(0.0010) Steps 754(778.15) | Grad Norm 2.2057(1.8184) | Total Time 0.00(0.00)\n",
      "Iter 19050 | Time 21.1485(20.6644) | Bit/dim 3.4604(3.4976) | Xent 0.0039(0.0046) | Loss 8.6637(9.2924) | Error 0.0022(0.0012) Steps 796(776.58) | Grad Norm 4.0298(2.0867) | Total Time 0.00(0.00)\n",
      "Iter 19060 | Time 20.2419(20.6182) | Bit/dim 3.4837(3.4977) | Xent 0.0005(0.0046) | Loss 8.6844(9.1372) | Error 0.0000(0.0013) Steps 760(778.87) | Grad Norm 1.1926(2.1594) | Total Time 0.00(0.00)\n",
      "Iter 19070 | Time 20.3185(20.5932) | Bit/dim 3.4820(3.4978) | Xent 0.0030(0.0050) | Loss 8.6914(9.0278) | Error 0.0022(0.0013) Steps 784(779.63) | Grad Norm 2.1288(2.1720) | Total Time 0.00(0.00)\n",
      "Iter 19080 | Time 20.8021(20.5861) | Bit/dim 3.4898(3.4966) | Xent 0.0021(0.0044) | Loss 8.6489(8.9345) | Error 0.0011(0.0012) Steps 790(779.91) | Grad Norm 1.5439(1.9878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 94.7294, Epoch Time 1248.5872(1251.5546), Bit/dim 3.5258(best: 3.5207), Xent 2.7473, Loss 4.8995, Error 0.3359(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19090 | Time 21.2622(20.6092) | Bit/dim 3.4723(3.4988) | Xent 0.0004(0.0039) | Loss 8.5468(9.6502) | Error 0.0000(0.0010) Steps 796(778.60) | Grad Norm 0.5775(1.7841) | Total Time 0.00(0.00)\n",
      "Iter 19100 | Time 21.4476(20.6963) | Bit/dim 3.4747(3.4948) | Xent 0.0140(0.0036) | Loss 8.7951(9.3993) | Error 0.0022(0.0008) Steps 820(780.51) | Grad Norm 3.1696(1.6295) | Total Time 0.00(0.00)\n",
      "Iter 19110 | Time 21.6944(20.7353) | Bit/dim 3.5175(3.4959) | Xent 0.0007(0.0035) | Loss 8.6547(9.2130) | Error 0.0000(0.0009) Steps 772(777.58) | Grad Norm 0.8731(1.7495) | Total Time 0.00(0.00)\n",
      "Iter 19120 | Time 21.2928(20.7996) | Bit/dim 3.5035(3.4983) | Xent 0.0047(0.0044) | Loss 8.7119(9.0855) | Error 0.0011(0.0011) Steps 772(780.07) | Grad Norm 2.6330(2.1380) | Total Time 0.00(0.00)\n",
      "Iter 19130 | Time 20.7112(20.7748) | Bit/dim 3.5114(3.4984) | Xent 0.0006(0.0038) | Loss 8.6810(8.9768) | Error 0.0000(0.0009) Steps 796(780.87) | Grad Norm 2.0749(2.2706) | Total Time 0.00(0.00)\n",
      "Iter 19140 | Time 21.0761(20.7402) | Bit/dim 3.4803(3.4973) | Xent 0.0007(0.0033) | Loss 8.6304(8.8952) | Error 0.0000(0.0008) Steps 766(778.84) | Grad Norm 1.8719(2.1829) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 96.8471, Epoch Time 1262.9685(1251.8970), Bit/dim 3.5249(best: 3.5207), Xent 2.7438, Loss 4.8968, Error 0.3383(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19150 | Time 20.4587(20.6277) | Bit/dim 3.5099(3.4964) | Xent 0.0063(0.0041) | Loss 8.8052(9.5221) | Error 0.0022(0.0010) Steps 772(780.55) | Grad Norm 5.4006(2.3975) | Total Time 0.00(0.00)\n",
      "Iter 19160 | Time 20.2890(20.6026) | Bit/dim 3.4811(3.4989) | Xent 0.0061(0.0043) | Loss 8.7421(9.3081) | Error 0.0011(0.0011) Steps 784(780.82) | Grad Norm 1.8283(2.3913) | Total Time 0.00(0.00)\n",
      "Iter 19170 | Time 21.2385(20.6217) | Bit/dim 3.4940(3.4979) | Xent 0.0016(0.0044) | Loss 8.7638(9.1514) | Error 0.0000(0.0010) Steps 796(780.64) | Grad Norm 2.2216(2.3173) | Total Time 0.00(0.00)\n",
      "Iter 19180 | Time 20.8933(20.5567) | Bit/dim 3.5367(3.5000) | Xent 0.0041(0.0045) | Loss 8.7303(9.0297) | Error 0.0011(0.0011) Steps 790(779.39) | Grad Norm 2.6324(2.4461) | Total Time 0.00(0.00)\n",
      "Iter 19190 | Time 20.8267(20.5893) | Bit/dim 3.4897(3.4971) | Xent 0.0038(0.0046) | Loss 8.6882(8.9423) | Error 0.0011(0.0011) Steps 778(778.88) | Grad Norm 4.5005(2.7626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 95.9627, Epoch Time 1245.2951(1251.6990), Bit/dim 3.5243(best: 3.5207), Xent 2.7610, Loss 4.9048, Error 0.3422(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19200 | Time 21.6970(20.6163) | Bit/dim 3.5283(3.4993) | Xent 0.0082(0.0045) | Loss 8.7784(9.6342) | Error 0.0022(0.0011) Steps 784(778.90) | Grad Norm 3.0827(3.0145) | Total Time 0.00(0.00)\n",
      "Iter 19210 | Time 21.2989(20.6188) | Bit/dim 3.4849(3.4957) | Xent 0.0007(0.0042) | Loss 8.7275(9.3900) | Error 0.0000(0.0010) Steps 784(779.39) | Grad Norm 1.2839(2.8806) | Total Time 0.00(0.00)\n",
      "Iter 19220 | Time 20.8846(20.6438) | Bit/dim 3.4585(3.4935) | Xent 0.0079(0.0041) | Loss 8.5853(9.1995) | Error 0.0022(0.0010) Steps 796(778.90) | Grad Norm 5.7379(2.8615) | Total Time 0.00(0.00)\n",
      "Iter 19230 | Time 21.2961(20.6979) | Bit/dim 3.5081(3.4948) | Xent 0.0129(0.0050) | Loss 8.7753(9.0653) | Error 0.0033(0.0012) Steps 784(779.91) | Grad Norm 2.4495(2.7531) | Total Time 0.00(0.00)\n",
      "Iter 19240 | Time 21.1823(20.7321) | Bit/dim 3.5098(3.4976) | Xent 0.0007(0.0049) | Loss 8.7500(8.9682) | Error 0.0000(0.0011) Steps 802(781.40) | Grad Norm 0.9767(2.6304) | Total Time 0.00(0.00)\n",
      "Iter 19250 | Time 21.2396(20.8192) | Bit/dim 3.4821(3.4995) | Xent 0.0006(0.0046) | Loss 8.6853(8.8959) | Error 0.0000(0.0011) Steps 754(781.73) | Grad Norm 0.8372(2.4007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 96.3127, Epoch Time 1262.8628(1252.0339), Bit/dim 3.5291(best: 3.5207), Xent 2.7267, Loss 4.8925, Error 0.3419(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19260 | Time 20.2991(20.8131) | Bit/dim 3.5034(3.4966) | Xent 0.0016(0.0045) | Loss 8.6165(9.4920) | Error 0.0000(0.0011) Steps 760(782.73) | Grad Norm 1.1345(2.1660) | Total Time 0.00(0.00)\n",
      "Iter 19270 | Time 20.9713(20.8042) | Bit/dim 3.5032(3.4962) | Xent 0.0009(0.0040) | Loss 8.7346(9.2823) | Error 0.0000(0.0009) Steps 808(782.66) | Grad Norm 1.1298(2.0147) | Total Time 0.00(0.00)\n",
      "Iter 19280 | Time 20.5997(20.7100) | Bit/dim 3.5046(3.4985) | Xent 0.0012(0.0034) | Loss 8.6608(9.1339) | Error 0.0000(0.0008) Steps 778(780.30) | Grad Norm 0.6889(1.8294) | Total Time 0.00(0.00)\n",
      "Iter 19290 | Time 20.7038(20.7234) | Bit/dim 3.4556(3.4974) | Xent 0.0008(0.0034) | Loss 8.6245(9.0109) | Error 0.0000(0.0008) Steps 790(779.30) | Grad Norm 1.1674(1.7909) | Total Time 0.00(0.00)\n",
      "Iter 19300 | Time 20.7417(20.7683) | Bit/dim 3.4757(3.4965) | Xent 0.0007(0.0030) | Loss 8.6644(8.9236) | Error 0.0000(0.0007) Steps 790(781.82) | Grad Norm 0.6107(1.6435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 95.9812, Epoch Time 1257.3364(1252.1930), Bit/dim 3.5217(best: 3.5207), Xent 2.7542, Loss 4.8988, Error 0.3413(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19310 | Time 20.8874(20.7685) | Bit/dim 3.4818(3.4965) | Xent 0.0007(0.0025) | Loss 8.6398(9.6334) | Error 0.0000(0.0006) Steps 796(781.99) | Grad Norm 0.5971(1.4255) | Total Time 0.00(0.00)\n",
      "Iter 19320 | Time 20.3110(20.6854) | Bit/dim 3.5099(3.5003) | Xent 0.0057(0.0025) | Loss 8.6689(9.3797) | Error 0.0011(0.0006) Steps 784(781.11) | Grad Norm 2.4721(1.4643) | Total Time 0.00(0.00)\n",
      "Iter 19330 | Time 21.0592(20.6568) | Bit/dim 3.5329(3.4967) | Xent 0.0011(0.0026) | Loss 8.7641(9.1939) | Error 0.0000(0.0006) Steps 790(778.82) | Grad Norm 0.7043(1.4528) | Total Time 0.00(0.00)\n",
      "Iter 19340 | Time 21.5309(20.7241) | Bit/dim 3.4810(3.4950) | Xent 0.0021(0.0026) | Loss 8.7037(9.0666) | Error 0.0011(0.0007) Steps 766(778.51) | Grad Norm 1.8431(1.6996) | Total Time 0.00(0.00)\n",
      "Iter 19350 | Time 20.8223(20.6423) | Bit/dim 3.4994(3.4969) | Xent 0.0008(0.0030) | Loss 8.6767(8.9652) | Error 0.0000(0.0007) Steps 760(777.45) | Grad Norm 0.8663(1.8283) | Total Time 0.00(0.00)\n",
      "Iter 19360 | Time 19.6322(20.5909) | Bit/dim 3.4708(3.4933) | Xent 0.0054(0.0031) | Loss 8.5648(8.8786) | Error 0.0022(0.0007) Steps 778(779.29) | Grad Norm 2.5261(1.7798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 96.0551, Epoch Time 1249.5281(1252.1130), Bit/dim 3.5250(best: 3.5207), Xent 2.7735, Loss 4.9118, Error 0.3425(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19370 | Time 20.9113(20.5594) | Bit/dim 3.5259(3.4951) | Xent 0.0023(0.0032) | Loss 8.7834(9.4955) | Error 0.0011(0.0007) Steps 790(779.03) | Grad Norm 1.3419(1.8346) | Total Time 0.00(0.00)\n",
      "Iter 19380 | Time 21.0827(20.6074) | Bit/dim 3.4970(3.4964) | Xent 0.0013(0.0032) | Loss 8.6575(9.2762) | Error 0.0000(0.0007) Steps 802(778.66) | Grad Norm 1.1576(1.8832) | Total Time 0.00(0.00)\n",
      "Iter 19390 | Time 22.3901(20.7364) | Bit/dim 3.4932(3.4947) | Xent 0.0008(0.0028) | Loss 8.5993(9.1057) | Error 0.0000(0.0007) Steps 796(779.54) | Grad Norm 1.4279(1.8584) | Total Time 0.00(0.00)\n",
      "Iter 19400 | Time 20.8694(20.7323) | Bit/dim 3.4907(3.4942) | Xent 0.0022(0.0027) | Loss 8.7882(8.9898) | Error 0.0011(0.0007) Steps 802(779.22) | Grad Norm 5.3552(1.9041) | Total Time 0.00(0.00)\n",
      "Iter 19410 | Time 20.4718(20.6703) | Bit/dim 3.4926(3.4951) | Xent 0.0034(0.0028) | Loss 8.7801(8.9084) | Error 0.0011(0.0008) Steps 796(778.28) | Grad Norm 1.2211(1.8858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 95.8536, Epoch Time 1254.0735(1252.1718), Bit/dim 3.5270(best: 3.5207), Xent 2.8126, Loss 4.9332, Error 0.3411(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19420 | Time 20.7636(20.5927) | Bit/dim 3.5240(3.4958) | Xent 0.0017(0.0035) | Loss 8.8591(9.6211) | Error 0.0011(0.0010) Steps 796(777.67) | Grad Norm 2.8340(2.1764) | Total Time 0.00(0.00)\n",
      "Iter 19430 | Time 20.9575(20.6072) | Bit/dim 3.4707(3.4964) | Xent 0.0021(0.0035) | Loss 8.5646(9.3724) | Error 0.0011(0.0009) Steps 784(778.16) | Grad Norm 2.8181(2.3243) | Total Time 0.00(0.00)\n",
      "Iter 19440 | Time 21.0140(20.6390) | Bit/dim 3.5002(3.4935) | Xent 0.0009(0.0030) | Loss 8.7354(9.1802) | Error 0.0000(0.0008) Steps 760(776.07) | Grad Norm 1.5709(2.2339) | Total Time 0.00(0.00)\n",
      "Iter 19450 | Time 20.8001(20.6350) | Bit/dim 3.5252(3.4955) | Xent 0.0085(0.0033) | Loss 8.7669(9.0570) | Error 0.0022(0.0009) Steps 784(776.97) | Grad Norm 1.8058(2.1694) | Total Time 0.00(0.00)\n",
      "Iter 19460 | Time 20.8636(20.6365) | Bit/dim 3.4939(3.4988) | Xent 0.0012(0.0027) | Loss 8.7239(8.9676) | Error 0.0000(0.0007) Steps 766(776.25) | Grad Norm 1.1584(1.9769) | Total Time 0.00(0.00)\n",
      "Iter 19470 | Time 20.2554(20.5891) | Bit/dim 3.4720(3.4937) | Xent 0.0006(0.0027) | Loss 8.5956(8.8868) | Error 0.0000(0.0007) Steps 784(776.74) | Grad Norm 0.8853(1.9748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 95.7930, Epoch Time 1249.5427(1252.0930), Bit/dim 3.5246(best: 3.5207), Xent 2.7873, Loss 4.9183, Error 0.3378(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19480 | Time 20.0130(20.6589) | Bit/dim 3.4738(3.4935) | Xent 0.0007(0.0027) | Loss 8.5205(9.4859) | Error 0.0000(0.0007) Steps 754(779.21) | Grad Norm 1.3856(2.0457) | Total Time 0.00(0.00)\n",
      "Iter 19490 | Time 20.0176(20.6575) | Bit/dim 3.5061(3.4942) | Xent 0.0089(0.0035) | Loss 8.7165(9.2710) | Error 0.0033(0.0009) Steps 766(779.62) | Grad Norm 5.9652(2.1992) | Total Time 0.00(0.00)\n",
      "Iter 19500 | Time 21.1851(20.6422) | Bit/dim 3.5301(3.4970) | Xent 0.0014(0.0038) | Loss 8.7332(9.1232) | Error 0.0000(0.0010) Steps 760(778.38) | Grad Norm 1.8627(2.2958) | Total Time 0.00(0.00)\n",
      "Iter 19510 | Time 20.4784(20.5631) | Bit/dim 3.5146(3.4985) | Xent 0.0019(0.0037) | Loss 8.8191(9.0089) | Error 0.0000(0.0010) Steps 790(777.91) | Grad Norm 2.2899(2.2810) | Total Time 0.00(0.00)\n",
      "Iter 19520 | Time 20.5749(20.5444) | Bit/dim 3.4831(3.4945) | Xent 0.0106(0.0044) | Loss 8.7415(8.9267) | Error 0.0033(0.0013) Steps 796(778.02) | Grad Norm 4.3361(2.6121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 96.8660, Epoch Time 1251.0576(1252.0619), Bit/dim 3.5289(best: 3.5207), Xent 2.9024, Loss 4.9801, Error 0.3500(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19530 | Time 20.1705(20.5362) | Bit/dim 3.5050(3.4958) | Xent 0.0008(0.0045) | Loss 8.8201(9.6319) | Error 0.0000(0.0011) Steps 790(776.92) | Grad Norm 1.5399(2.5431) | Total Time 0.00(0.00)\n",
      "Iter 19540 | Time 21.1696(20.6273) | Bit/dim 3.5139(3.4971) | Xent 0.0121(0.0051) | Loss 8.7692(9.3888) | Error 0.0011(0.0012) Steps 760(774.88) | Grad Norm 1.8239(2.6395) | Total Time 0.00(0.00)\n",
      "Iter 19550 | Time 20.1073(20.5799) | Bit/dim 3.4875(3.4985) | Xent 0.0006(0.0048) | Loss 8.6745(9.2107) | Error 0.0000(0.0011) Steps 784(774.27) | Grad Norm 1.4114(2.4364) | Total Time 0.00(0.00)\n",
      "Iter 19560 | Time 20.6667(20.5462) | Bit/dim 3.4962(3.4971) | Xent 0.0049(0.0042) | Loss 8.5926(9.0697) | Error 0.0022(0.0010) Steps 754(774.18) | Grad Norm 2.2421(2.2186) | Total Time 0.00(0.00)\n",
      "Iter 19570 | Time 21.4954(20.5736) | Bit/dim 3.5102(3.4952) | Xent 0.0008(0.0040) | Loss 8.6843(8.9621) | Error 0.0000(0.0010) Steps 796(776.43) | Grad Norm 1.1860(2.1880) | Total Time 0.00(0.00)\n",
      "Iter 19580 | Time 20.7225(20.6209) | Bit/dim 3.4831(3.4966) | Xent 0.0036(0.0039) | Loss 8.7279(8.9038) | Error 0.0011(0.0010) Steps 772(778.40) | Grad Norm 3.4230(2.4656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 95.8694, Epoch Time 1250.4643(1252.0140), Bit/dim 3.5261(best: 3.5207), Xent 2.7782, Loss 4.9152, Error 0.3386(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19590 | Time 21.3960(20.6394) | Bit/dim 3.5124(3.4991) | Xent 0.0012(0.0042) | Loss 8.7168(9.4965) | Error 0.0000(0.0011) Steps 808(779.84) | Grad Norm 3.2724(2.8489) | Total Time 0.00(0.00)\n",
      "Iter 19600 | Time 20.9930(20.6368) | Bit/dim 3.4809(3.4972) | Xent 0.0012(0.0040) | Loss 8.6916(9.2866) | Error 0.0000(0.0010) Steps 766(780.68) | Grad Norm 1.5939(2.7778) | Total Time 0.00(0.00)\n",
      "Iter 19610 | Time 20.2066(20.5653) | Bit/dim 3.4991(3.4981) | Xent 0.0037(0.0040) | Loss 8.7442(9.1339) | Error 0.0011(0.0009) Steps 784(778.64) | Grad Norm 2.1951(2.6646) | Total Time 0.00(0.00)\n",
      "Iter 19620 | Time 20.6591(20.5731) | Bit/dim 3.4673(3.4968) | Xent 0.0068(0.0040) | Loss 8.6443(9.0150) | Error 0.0022(0.0010) Steps 754(778.84) | Grad Norm 3.5176(2.6451) | Total Time 0.00(0.00)\n",
      "Iter 19630 | Time 20.4097(20.6548) | Bit/dim 3.4996(3.4965) | Xent 0.0082(0.0039) | Loss 8.6469(8.9183) | Error 0.0022(0.0010) Steps 784(778.71) | Grad Norm 1.7965(2.4960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 96.3597, Epoch Time 1251.3880(1251.9952), Bit/dim 3.5269(best: 3.5207), Xent 2.8284, Loss 4.9412, Error 0.3419(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19640 | Time 20.5516(20.6809) | Bit/dim 3.4663(3.4955) | Xent 0.0025(0.0039) | Loss 8.5535(9.6268) | Error 0.0011(0.0010) Steps 754(779.63) | Grad Norm 1.7788(2.4393) | Total Time 0.00(0.00)\n",
      "Iter 19650 | Time 20.3953(20.6876) | Bit/dim 3.4971(3.4955) | Xent 0.0015(0.0034) | Loss 8.7487(9.3813) | Error 0.0011(0.0008) Steps 766(781.72) | Grad Norm 2.7134(2.2392) | Total Time 0.00(0.00)\n",
      "Iter 19660 | Time 20.9912(20.6785) | Bit/dim 3.4642(3.4935) | Xent 0.0035(0.0030) | Loss 8.5924(9.1951) | Error 0.0011(0.0008) Steps 778(782.55) | Grad Norm 2.3025(2.0468) | Total Time 0.00(0.00)\n",
      "Iter 19670 | Time 20.9132(20.6888) | Bit/dim 3.4933(3.4940) | Xent 0.0017(0.0030) | Loss 8.6858(9.0566) | Error 0.0011(0.0008) Steps 790(780.96) | Grad Norm 1.3898(1.9216) | Total Time 0.00(0.00)\n",
      "Iter 19680 | Time 21.0450(20.6781) | Bit/dim 3.4571(3.4936) | Xent 0.0010(0.0031) | Loss 8.7143(8.9559) | Error 0.0000(0.0008) Steps 796(780.80) | Grad Norm 0.8441(1.7679) | Total Time 0.00(0.00)\n",
      "Iter 19690 | Time 20.7625(20.7019) | Bit/dim 3.5027(3.4951) | Xent 0.0010(0.0031) | Loss 8.6768(8.8890) | Error 0.0000(0.0008) Steps 748(778.12) | Grad Norm 0.9125(1.8121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 95.5357, Epoch Time 1258.4846(1252.1899), Bit/dim 3.5223(best: 3.5207), Xent 2.8356, Loss 4.9401, Error 0.3455(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19700 | Time 20.5248(20.6858) | Bit/dim 3.4804(3.4917) | Xent 0.0007(0.0030) | Loss 8.6534(9.4936) | Error 0.0000(0.0007) Steps 772(778.35) | Grad Norm 0.8820(1.6984) | Total Time 0.00(0.00)\n",
      "Iter 19710 | Time 20.3292(20.6981) | Bit/dim 3.4857(3.4931) | Xent 0.0018(0.0028) | Loss 8.7060(9.2766) | Error 0.0000(0.0006) Steps 802(777.75) | Grad Norm 1.6948(1.6359) | Total Time 0.00(0.00)\n",
      "Iter 19720 | Time 20.8169(20.7292) | Bit/dim 3.5000(3.4936) | Xent 0.0017(0.0023) | Loss 8.7117(9.1245) | Error 0.0000(0.0005) Steps 790(781.51) | Grad Norm 2.3357(1.4947) | Total Time 0.00(0.00)\n",
      "Iter 19730 | Time 20.2723(20.7116) | Bit/dim 3.4728(3.4945) | Xent 0.0017(0.0027) | Loss 8.5438(9.0094) | Error 0.0000(0.0006) Steps 766(780.67) | Grad Norm 1.1489(1.5176) | Total Time 0.00(0.00)\n",
      "Iter 19740 | Time 20.6124(20.6747) | Bit/dim 3.4753(3.4950) | Xent 0.0064(0.0035) | Loss 8.6975(8.9344) | Error 0.0011(0.0008) Steps 802(781.46) | Grad Norm 2.3254(1.6745) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 96.1699, Epoch Time 1253.1182(1252.2177), Bit/dim 3.5224(best: 3.5207), Xent 2.7916, Loss 4.9182, Error 0.3375(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19750 | Time 19.8839(20.5996) | Bit/dim 3.4656(3.4947) | Xent 0.0061(0.0036) | Loss 8.6028(9.6307) | Error 0.0011(0.0009) Steps 772(781.70) | Grad Norm 2.2501(1.7529) | Total Time 0.00(0.00)\n",
      "Iter 19760 | Time 20.3203(20.7172) | Bit/dim 3.4664(3.4944) | Xent 0.0005(0.0034) | Loss 8.5493(9.3783) | Error 0.0000(0.0008) Steps 754(781.29) | Grad Norm 0.7022(1.6580) | Total Time 0.00(0.00)\n",
      "Iter 19770 | Time 20.2887(20.7056) | Bit/dim 3.4849(3.4939) | Xent 0.0008(0.0030) | Loss 8.6569(9.1883) | Error 0.0000(0.0007) Steps 796(781.14) | Grad Norm 1.0138(1.5934) | Total Time 0.00(0.00)\n",
      "Iter 19780 | Time 19.8954(20.7125) | Bit/dim 3.4863(3.4909) | Xent 0.0048(0.0031) | Loss 8.6703(9.0452) | Error 0.0022(0.0009) Steps 772(779.88) | Grad Norm 3.3236(1.6714) | Total Time 0.00(0.00)\n",
      "Iter 19790 | Time 20.8546(20.6761) | Bit/dim 3.5254(3.4923) | Xent 0.0008(0.0027) | Loss 8.7455(8.9451) | Error 0.0000(0.0008) Steps 778(778.73) | Grad Norm 0.6126(1.5480) | Total Time 0.00(0.00)\n",
      "Iter 19800 | Time 20.8880(20.6144) | Bit/dim 3.4802(3.4922) | Xent 0.0101(0.0030) | Loss 8.5081(8.8664) | Error 0.0033(0.0009) Steps 784(777.39) | Grad Norm 2.7182(1.6661) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 96.3368, Epoch Time 1255.4568(1252.3149), Bit/dim 3.5259(best: 3.5207), Xent 2.8339, Loss 4.9428, Error 0.3416(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19810 | Time 19.7528(20.6166) | Bit/dim 3.4625(3.4923) | Xent 0.0006(0.0034) | Loss 8.6450(9.4745) | Error 0.0000(0.0009) Steps 784(778.73) | Grad Norm 1.1326(1.8087) | Total Time 0.00(0.00)\n",
      "Iter 19820 | Time 20.8912(20.6005) | Bit/dim 3.4926(3.4937) | Xent 0.0098(0.0038) | Loss 8.6739(9.2763) | Error 0.0011(0.0010) Steps 766(779.18) | Grad Norm 2.4468(1.8742) | Total Time 0.00(0.00)\n",
      "Iter 19830 | Time 20.3312(20.6736) | Bit/dim 3.5062(3.4959) | Xent 0.0046(0.0038) | Loss 8.6673(9.1338) | Error 0.0011(0.0009) Steps 766(780.41) | Grad Norm 2.4713(1.9359) | Total Time 0.00(0.00)\n",
      "Iter 19840 | Time 20.7511(20.7181) | Bit/dim 3.4894(3.4959) | Xent 0.0077(0.0040) | Loss 8.7343(9.0230) | Error 0.0033(0.0011) Steps 772(781.98) | Grad Norm 4.1804(2.0921) | Total Time 0.00(0.00)\n",
      "Iter 19850 | Time 20.5896(20.6863) | Bit/dim 3.5154(3.4970) | Xent 0.0035(0.0039) | Loss 8.7722(8.9431) | Error 0.0022(0.0012) Steps 772(783.66) | Grad Norm 3.7824(2.3048) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 95.8007, Epoch Time 1254.5400(1252.3817), Bit/dim 3.5255(best: 3.5207), Xent 2.7679, Loss 4.9095, Error 0.3375(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19860 | Time 20.3280(20.6511) | Bit/dim 3.4928(3.4988) | Xent 0.0022(0.0037) | Loss 8.6755(9.6440) | Error 0.0011(0.0011) Steps 790(782.03) | Grad Norm 1.8600(2.2519) | Total Time 0.00(0.00)\n",
      "Iter 19870 | Time 20.3935(20.6235) | Bit/dim 3.4657(3.4991) | Xent 0.0050(0.0035) | Loss 8.6732(9.3996) | Error 0.0011(0.0011) Steps 760(782.19) | Grad Norm 2.1505(2.1656) | Total Time 0.00(0.00)\n",
      "Iter 19880 | Time 20.7112(20.6366) | Bit/dim 3.4815(3.4969) | Xent 0.0061(0.0040) | Loss 8.6529(9.2081) | Error 0.0011(0.0010) Steps 802(782.26) | Grad Norm 2.4216(2.1090) | Total Time 0.00(0.00)\n",
      "Iter 19890 | Time 21.0730(20.6399) | Bit/dim 3.4973(3.4972) | Xent 0.0024(0.0036) | Loss 8.7084(9.0775) | Error 0.0000(0.0009) Steps 772(781.96) | Grad Norm 1.2564(1.9720) | Total Time 0.00(0.00)\n",
      "Iter 19900 | Time 21.2155(20.6477) | Bit/dim 3.4969(3.4937) | Xent 0.0013(0.0038) | Loss 8.8061(8.9744) | Error 0.0000(0.0008) Steps 802(782.09) | Grad Norm 0.9321(1.7466) | Total Time 0.00(0.00)\n",
      "Iter 19910 | Time 20.8700(20.6607) | Bit/dim 3.5019(3.4916) | Xent 0.0038(0.0037) | Loss 8.6720(8.8928) | Error 0.0022(0.0009) Steps 760(781.03) | Grad Norm 2.4528(1.7714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 97.4428, Epoch Time 1253.9420(1252.4285), Bit/dim 3.5214(best: 3.5207), Xent 2.7725, Loss 4.9076, Error 0.3402(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19920 | Time 20.2476(20.6685) | Bit/dim 3.4940(3.4908) | Xent 0.0035(0.0034) | Loss 8.6595(9.4994) | Error 0.0011(0.0009) Steps 766(781.37) | Grad Norm 1.6952(1.7225) | Total Time 0.00(0.00)\n",
      "Iter 19930 | Time 21.1472(20.7546) | Bit/dim 3.4918(3.4893) | Xent 0.0007(0.0031) | Loss 8.6808(9.2812) | Error 0.0000(0.0008) Steps 772(782.80) | Grad Norm 0.8235(1.6513) | Total Time 0.00(0.00)\n",
      "Iter 19940 | Time 21.4201(20.7939) | Bit/dim 3.4897(3.4920) | Xent 0.0019(0.0034) | Loss 8.7833(9.1275) | Error 0.0000(0.0007) Steps 808(784.05) | Grad Norm 1.4074(1.6985) | Total Time 0.00(0.00)\n",
      "Iter 19950 | Time 21.2752(20.8138) | Bit/dim 3.4766(3.4935) | Xent 0.0008(0.0029) | Loss 8.6881(9.0063) | Error 0.0000(0.0007) Steps 766(782.58) | Grad Norm 1.2478(1.6519) | Total Time 0.00(0.00)\n",
      "Iter 19960 | Time 21.4483(20.7932) | Bit/dim 3.5278(3.4945) | Xent 0.0066(0.0035) | Loss 8.7622(8.9226) | Error 0.0011(0.0009) Steps 796(781.97) | Grad Norm 1.8761(1.7896) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 95.6546, Epoch Time 1262.8080(1252.7398), Bit/dim 3.5228(best: 3.5207), Xent 2.8150, Loss 4.9304, Error 0.3387(best: 0.3246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19970 | Time 20.5398(20.7863) | Bit/dim 3.4777(3.4936) | Xent 0.0061(0.0036) | Loss 8.6328(9.6258) | Error 0.0011(0.0009) Steps 766(780.70) | Grad Norm 1.4954(1.9005) | Total Time 0.00(0.00)\n",
      "Iter 19980 | Time 20.0893(20.7781) | Bit/dim 3.4994(3.4935) | Xent 0.0021(0.0038) | Loss 8.5395(9.3761) | Error 0.0011(0.0010) Steps 766(781.14) | Grad Norm 4.0779(2.1649) | Total Time 0.00(0.00)\n",
      "Iter 19990 | Time 21.5093(20.8232) | Bit/dim 3.5167(3.4935) | Xent 0.0004(0.0038) | Loss 8.7971(9.1941) | Error 0.0000(0.0010) Steps 772(783.69) | Grad Norm 1.3559(2.1794) | Total Time 0.00(0.00)\n",
      "Iter 20000 | Time 21.2149(20.8595) | Bit/dim 3.4939(3.4957) | Xent 0.0042(0.0044) | Loss 8.6825(9.0633) | Error 0.0011(0.0011) Steps 772(782.35) | Grad Norm 6.4048(2.4054) | Total Time 0.00(0.00)\n",
      "Iter 20010 | Time 20.3547(20.7733) | Bit/dim 3.4990(3.4958) | Xent 0.0012(0.0044) | Loss 8.6658(8.9660) | Error 0.0000(0.0011) Steps 760(780.54) | Grad Norm 1.6210(2.4889) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_multiscale.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_multiscale_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_multiscale_run2/epoch_250_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
