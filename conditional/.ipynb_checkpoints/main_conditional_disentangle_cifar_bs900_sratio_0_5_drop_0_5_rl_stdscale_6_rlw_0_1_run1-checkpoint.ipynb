{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_1_run1/current_checkpt.pth', rl_weight=0.1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_1_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4020 | Time 16.7961(18.0670) | Bit/dim 3.6724(3.7049) | Xent 0.7057(0.7428) | Loss 55.1656(64.8248) | Error 0.2411(0.2634) Steps 0(0.00) | Grad Norm 45.3902(45.8416) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 17.2383(17.9985) | Bit/dim 3.6942(3.7038) | Xent 0.7382(0.7433) | Loss 57.5977(62.8095) | Error 0.2544(0.2632) Steps 0(0.00) | Grad Norm 43.6773(43.9430) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 18.6780(17.9750) | Bit/dim 3.6724(3.7045) | Xent 0.8286(0.7435) | Loss 56.5143(61.3298) | Error 0.3100(0.2651) Steps 0(0.00) | Grad Norm 61.6592(46.0304) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 16.4379(17.7516) | Bit/dim 3.7041(3.7026) | Xent 0.7773(0.7441) | Loss 55.9481(60.2523) | Error 0.2589(0.2653) Steps 0(0.00) | Grad Norm 42.3038(45.6270) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 17.8770(17.6629) | Bit/dim 3.6711(3.7026) | Xent 0.7554(0.7467) | Loss 58.1688(59.4444) | Error 0.2700(0.2663) Steps 0(0.00) | Grad Norm 70.4405(49.9354) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 17.1546(17.4694) | Bit/dim 3.7096(3.7065) | Xent 0.8412(0.7673) | Loss 57.5897(58.8891) | Error 0.2900(0.2746) Steps 0(0.00) | Grad Norm 40.7159(55.3162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 91.6312, Epoch Time 1095.1238(1028.6273), Bit/dim 3.7203(best: inf), Xent 0.8490, Loss 4.1448, Error 0.2962(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 17.6625(17.4173) | Bit/dim 3.6718(3.7038) | Xent 0.7269(0.7700) | Loss 55.2663(64.0147) | Error 0.2611(0.2756) Steps 0(0.00) | Grad Norm 27.9911(52.8490) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 18.0698(17.3024) | Bit/dim 3.7017(3.7055) | Xent 0.7637(0.7619) | Loss 56.4085(62.1017) | Error 0.2711(0.2704) Steps 0(0.00) | Grad Norm 36.9143(49.4619) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 17.1692(17.3228) | Bit/dim 3.7060(3.7056) | Xent 0.7032(0.7502) | Loss 56.1746(60.7404) | Error 0.2556(0.2669) Steps 0(0.00) | Grad Norm 33.8984(47.7653) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.9723(17.1774) | Bit/dim 3.6946(3.7042) | Xent 0.7137(0.7416) | Loss 57.1607(59.5609) | Error 0.2478(0.2656) Steps 0(0.00) | Grad Norm 37.6571(44.2860) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.0843(17.1552) | Bit/dim 3.7042(3.7040) | Xent 0.7590(0.7429) | Loss 58.0606(58.9499) | Error 0.2633(0.2648) Steps 0(0.00) | Grad Norm 36.9447(47.8932) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 90.0368, Epoch Time 1051.4146(1029.3109), Bit/dim 3.6944(best: 3.7203), Xent 0.7717, Loss 4.0803, Error 0.2676(best: 0.2962)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.4880(17.1422) | Bit/dim 3.6856(3.7024) | Xent 0.6596(0.7402) | Loss 56.5015(65.1777) | Error 0.2467(0.2637) Steps 0(0.00) | Grad Norm 31.3297(47.0980) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.4680(17.1575) | Bit/dim 3.7244(3.7037) | Xent 0.7312(0.7450) | Loss 54.5445(63.1347) | Error 0.2556(0.2643) Steps 0(0.00) | Grad Norm 52.2142(47.9022) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.4152(17.1370) | Bit/dim 3.7075(3.7037) | Xent 0.6992(0.7381) | Loss 57.1828(61.4119) | Error 0.2344(0.2618) Steps 0(0.00) | Grad Norm 22.5903(45.0015) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 18.6596(17.4019) | Bit/dim 3.7018(3.6991) | Xent 0.7510(0.7327) | Loss 56.5905(60.1043) | Error 0.2633(0.2606) Steps 0(0.00) | Grad Norm 50.1998(42.5315) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.6863(17.5535) | Bit/dim 3.7191(3.7002) | Xent 0.7157(0.7276) | Loss 56.9452(59.2236) | Error 0.2511(0.2589) Steps 0(0.00) | Grad Norm 38.0972(42.1198) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 17.6997(17.4941) | Bit/dim 3.6881(3.6976) | Xent 0.7436(0.7287) | Loss 57.4257(58.6702) | Error 0.2644(0.2586) Steps 0(0.00) | Grad Norm 51.5516(43.7381) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 90.5966, Epoch Time 1068.6408(1030.4908), Bit/dim 3.6969(best: 3.6944), Xent 0.7612, Loss 4.0775, Error 0.2674(best: 0.2676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 18.2908(17.4498) | Bit/dim 3.6955(3.6995) | Xent 0.7083(0.7236) | Loss 55.6362(63.9612) | Error 0.2567(0.2564) Steps 0(0.00) | Grad Norm 26.7936(43.4899) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 18.3605(17.4796) | Bit/dim 3.6643(3.6982) | Xent 0.7471(0.7247) | Loss 57.0457(62.2477) | Error 0.2622(0.2594) Steps 0(0.00) | Grad Norm 53.2796(47.5145) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 19.9208(17.5463) | Bit/dim 3.7174(3.6998) | Xent 0.7023(0.7212) | Loss 57.8720(60.9301) | Error 0.2489(0.2570) Steps 0(0.00) | Grad Norm 32.9528(48.0822) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 17.3197(17.3863) | Bit/dim 3.6718(3.6989) | Xent 0.7145(0.7195) | Loss 54.3534(59.7766) | Error 0.2589(0.2567) Steps 0(0.00) | Grad Norm 60.4748(47.8337) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 17.3315(17.4321) | Bit/dim 3.7065(3.6961) | Xent 0.7743(0.7231) | Loss 58.1916(59.1829) | Error 0.2800(0.2578) Steps 0(0.00) | Grad Norm 71.1930(49.2568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 88.0459, Epoch Time 1065.8715(1031.5522), Bit/dim 3.6978(best: 3.6944), Xent 0.7566, Loss 4.0761, Error 0.2646(best: 0.2674)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 19.3323(17.5881) | Bit/dim 3.6728(3.6896) | Xent 0.7123(0.7252) | Loss 58.7129(64.8899) | Error 0.2533(0.2590) Steps 0(0.00) | Grad Norm 88.8072(51.5836) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 18.0946(17.5486) | Bit/dim 3.7292(3.6947) | Xent 0.7404(0.7216) | Loss 55.6916(62.6339) | Error 0.2600(0.2573) Steps 0(0.00) | Grad Norm 33.4697(48.8978) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 19.0187(17.5662) | Bit/dim 3.7200(3.6960) | Xent 0.7207(0.7216) | Loss 57.2723(61.1526) | Error 0.2611(0.2570) Steps 0(0.00) | Grad Norm 41.5615(49.0148) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.1527(17.4269) | Bit/dim 3.7101(3.6951) | Xent 0.6986(0.7223) | Loss 56.1790(59.9149) | Error 0.2422(0.2573) Steps 0(0.00) | Grad Norm 35.0300(49.7415) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 24.8745(17.6197) | Bit/dim 3.7212(3.6955) | Xent 0.7642(0.7175) | Loss 60.3041(59.2540) | Error 0.2633(0.2550) Steps 0(0.00) | Grad Norm 56.2382(48.7398) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 17.3705(17.6025) | Bit/dim 3.6926(3.6932) | Xent 0.7467(0.7174) | Loss 57.5948(58.5426) | Error 0.2567(0.2544) Steps 0(0.00) | Grad Norm 35.1482(47.1213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 91.3331, Epoch Time 1077.9371(1032.9438), Bit/dim 3.6919(best: 3.6944), Xent 0.7597, Loss 4.0717, Error 0.2662(best: 0.2646)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.7581(17.6475) | Bit/dim 3.6709(3.6927) | Xent 0.6841(0.7128) | Loss 56.0575(63.4955) | Error 0.2356(0.2526) Steps 0(0.00) | Grad Norm 53.8127(47.5964) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.6042(17.6337) | Bit/dim 3.6806(3.6915) | Xent 0.7073(0.7059) | Loss 56.2902(61.7645) | Error 0.2578(0.2498) Steps 0(0.00) | Grad Norm 35.3406(44.4198) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.7651(17.6613) | Bit/dim 3.6846(3.6905) | Xent 0.6574(0.7030) | Loss 57.9985(60.4970) | Error 0.2411(0.2502) Steps 0(0.00) | Grad Norm 48.5548(44.1589) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 19.3768(17.8660) | Bit/dim 3.7040(3.6900) | Xent 0.7391(0.7070) | Loss 57.8901(59.5056) | Error 0.2511(0.2518) Steps 0(0.00) | Grad Norm 49.6697(45.3255) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 17.0172(17.9204) | Bit/dim 3.6759(3.6899) | Xent 0.7174(0.7177) | Loss 56.2783(58.8844) | Error 0.2489(0.2553) Steps 0(0.00) | Grad Norm 75.9214(51.8121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 90.5182, Epoch Time 1091.3412(1034.6957), Bit/dim 3.7080(best: 3.6919), Xent 0.8207, Loss 4.1184, Error 0.2881(best: 0.2646)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 14.8532(17.6931) | Bit/dim 3.7146(3.6969) | Xent 0.6724(0.7228) | Loss 53.1704(65.0994) | Error 0.2300(0.2560) Steps 0(0.00) | Grad Norm 36.0073(51.9926) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 18.2134(17.5842) | Bit/dim 3.6816(3.6986) | Xent 0.6647(0.7123) | Loss 56.2812(63.0320) | Error 0.2344(0.2531) Steps 0(0.00) | Grad Norm 47.1324(51.1922) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 21.0890(17.6109) | Bit/dim 3.6654(3.6936) | Xent 0.6882(0.7061) | Loss 58.0091(61.2893) | Error 0.2633(0.2530) Steps 0(0.00) | Grad Norm 35.0411(47.0972) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.5489(17.4817) | Bit/dim 3.7233(3.6946) | Xent 0.7086(0.7131) | Loss 55.2202(60.0430) | Error 0.2644(0.2552) Steps 0(0.00) | Grad Norm 64.9768(48.9489) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 17.2172(17.6208) | Bit/dim 3.6782(3.6943) | Xent 0.6977(0.7143) | Loss 55.7736(59.2455) | Error 0.2411(0.2562) Steps 0(0.00) | Grad Norm 42.4143(50.5780) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 16.9996(17.6897) | Bit/dim 3.7226(3.6937) | Xent 0.7284(0.7197) | Loss 57.3707(58.4943) | Error 0.2622(0.2576) Steps 0(0.00) | Grad Norm 51.6514(51.0397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 91.4957, Epoch Time 1073.6390(1035.8640), Bit/dim 3.6897(best: 3.6919), Xent 0.7670, Loss 4.0732, Error 0.2654(best: 0.2646)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 17.5327(17.7293) | Bit/dim 3.6989(3.6938) | Xent 0.6669(0.7113) | Loss 56.3769(63.4519) | Error 0.2367(0.2531) Steps 0(0.00) | Grad Norm 46.0807(50.2120) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.1391(17.7419) | Bit/dim 3.6871(3.6922) | Xent 0.6639(0.7039) | Loss 57.3004(61.7981) | Error 0.2478(0.2509) Steps 0(0.00) | Grad Norm 26.8731(48.4240) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 18.8103(17.7160) | Bit/dim 3.6710(3.6871) | Xent 0.6895(0.7052) | Loss 57.1661(60.5109) | Error 0.2433(0.2508) Steps 0(0.00) | Grad Norm 30.7122(47.2866) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.2920(17.6970) | Bit/dim 3.6778(3.6879) | Xent 0.7554(0.7076) | Loss 56.6314(59.4518) | Error 0.2589(0.2514) Steps 0(0.00) | Grad Norm 53.1676(48.4615) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 18.9367(17.7963) | Bit/dim 3.6653(3.6887) | Xent 0.7244(0.7103) | Loss 56.9501(58.7515) | Error 0.2633(0.2520) Steps 0(0.00) | Grad Norm 58.9861(51.8231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 89.2076, Epoch Time 1081.7233(1037.2398), Bit/dim 3.6839(best: 3.6897), Xent 0.7377, Loss 4.0527, Error 0.2595(best: 0.2646)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.9184(17.6744) | Bit/dim 3.6715(3.6888) | Xent 0.7497(0.7036) | Loss 56.4224(65.0334) | Error 0.2600(0.2501) Steps 0(0.00) | Grad Norm 41.9212(47.9159) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.2359(17.5461) | Bit/dim 3.6966(3.6913) | Xent 0.7306(0.7080) | Loss 56.7319(62.7957) | Error 0.2456(0.2515) Steps 0(0.00) | Grad Norm 46.3708(51.3761) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 17.4507(17.5573) | Bit/dim 3.6714(3.6880) | Xent 0.7076(0.7001) | Loss 58.5828(61.3709) | Error 0.2511(0.2489) Steps 0(0.00) | Grad Norm 35.1037(49.2435) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.3811(17.5727) | Bit/dim 3.7011(3.6890) | Xent 0.7205(0.7032) | Loss 55.9209(60.0372) | Error 0.2689(0.2503) Steps 0(0.00) | Grad Norm 48.2256(50.9557) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 17.0620(17.6013) | Bit/dim 3.6665(3.6869) | Xent 0.6420(0.6965) | Loss 56.2738(59.1300) | Error 0.2311(0.2488) Steps 0(0.00) | Grad Norm 47.0268(49.1488) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 18.6842(17.5640) | Bit/dim 3.7135(3.6901) | Xent 0.7083(0.6978) | Loss 58.1015(58.5188) | Error 0.2567(0.2495) Steps 0(0.00) | Grad Norm 31.9834(47.6588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 90.4544, Epoch Time 1071.3237(1038.2623), Bit/dim 3.6880(best: 3.6839), Xent 0.7272, Loss 4.0516, Error 0.2570(best: 0.2595)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 19.2670(17.5878) | Bit/dim 3.7127(3.6857) | Xent 0.6298(0.6909) | Loss 54.7374(62.9519) | Error 0.2144(0.2479) Steps 0(0.00) | Grad Norm 42.0084(47.9857) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 18.9014(17.5870) | Bit/dim 3.6914(3.6879) | Xent 0.6754(0.6857) | Loss 56.9934(61.2668) | Error 0.2389(0.2453) Steps 0(0.00) | Grad Norm 73.0135(48.2917) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.9206(17.6529) | Bit/dim 3.6783(3.6864) | Xent 0.8067(0.6922) | Loss 58.4628(60.2586) | Error 0.2656(0.2472) Steps 0(0.00) | Grad Norm 93.5556(49.4037) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 18.9284(17.8085) | Bit/dim 3.7091(3.6880) | Xent 0.7250(0.7036) | Loss 58.5285(59.4001) | Error 0.2711(0.2504) Steps 0(0.00) | Grad Norm 58.7142(50.4894) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 20.8568(17.8060) | Bit/dim 3.7230(3.6928) | Xent 0.7498(0.7182) | Loss 59.2655(58.8015) | Error 0.2667(0.2555) Steps 0(0.00) | Grad Norm 63.9099(55.6641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 90.1798, Epoch Time 1086.9977(1039.7244), Bit/dim 3.6967(best: 3.6839), Xent 0.7578, Loss 4.0756, Error 0.2631(best: 0.2570)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 18.0966(17.7637) | Bit/dim 3.6914(3.6920) | Xent 0.6473(0.7117) | Loss 57.2271(65.2064) | Error 0.2344(0.2537) Steps 0(0.00) | Grad Norm 38.5931(54.2873) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 19.4505(17.7232) | Bit/dim 3.6624(3.6906) | Xent 0.7211(0.7122) | Loss 57.4767(62.9734) | Error 0.2622(0.2546) Steps 0(0.00) | Grad Norm 68.2664(56.8947) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.8794(17.7861) | Bit/dim 3.6788(3.6891) | Xent 0.6764(0.7047) | Loss 57.5723(61.2033) | Error 0.2300(0.2515) Steps 0(0.00) | Grad Norm 34.7281(52.7402) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.1490(17.7113) | Bit/dim 3.6853(3.6881) | Xent 0.6434(0.6962) | Loss 57.4067(60.1083) | Error 0.2378(0.2493) Steps 0(0.00) | Grad Norm 49.2542(49.1639) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 17.4558(17.7103) | Bit/dim 3.6669(3.6873) | Xent 0.6801(0.6894) | Loss 56.4189(59.0638) | Error 0.2400(0.2460) Steps 0(0.00) | Grad Norm 45.9113(46.5717) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 15.7996(17.5927) | Bit/dim 3.6499(3.6860) | Xent 0.7782(0.6947) | Loss 56.6700(58.3144) | Error 0.2711(0.2476) Steps 0(0.00) | Grad Norm 53.3821(47.0400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 91.8750, Epoch Time 1077.4858(1040.8572), Bit/dim 3.6777(best: 3.6839), Xent 0.7629, Loss 4.0592, Error 0.2657(best: 0.2570)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 18.2493(17.6419) | Bit/dim 3.6859(3.6862) | Xent 0.6295(0.6948) | Loss 57.5557(63.7273) | Error 0.2411(0.2487) Steps 0(0.00) | Grad Norm 36.7048(48.4938) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.2437(17.6521) | Bit/dim 3.6641(3.6837) | Xent 0.5968(0.6870) | Loss 52.3304(61.7663) | Error 0.2178(0.2453) Steps 0(0.00) | Grad Norm 34.0976(48.5299) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 16.1786(17.5419) | Bit/dim 3.6683(3.6849) | Xent 0.6807(0.6816) | Loss 54.3850(60.3341) | Error 0.2489(0.2444) Steps 0(0.00) | Grad Norm 36.9071(46.9696) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 16.7042(17.4744) | Bit/dim 3.6799(3.6808) | Xent 0.6024(0.6822) | Loss 55.6772(59.3132) | Error 0.2333(0.2448) Steps 0(0.00) | Grad Norm 42.0800(46.9344) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 17.1520(17.4293) | Bit/dim 3.6655(3.6835) | Xent 0.7041(0.6774) | Loss 57.0112(58.6071) | Error 0.2500(0.2422) Steps 0(0.00) | Grad Norm 60.3039(46.8721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 91.1228, Epoch Time 1067.9510(1041.6700), Bit/dim 3.6789(best: 3.6777), Xent 0.7259, Loss 4.0418, Error 0.2556(best: 0.2570)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.5921(17.4523) | Bit/dim 3.7072(3.6821) | Xent 0.6640(0.6778) | Loss 57.5799(64.3113) | Error 0.2333(0.2424) Steps 0(0.00) | Grad Norm 26.7429(45.1379) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 18.4724(17.6091) | Bit/dim 3.6318(3.6811) | Xent 0.6429(0.6805) | Loss 55.5616(62.2041) | Error 0.2378(0.2436) Steps 0(0.00) | Grad Norm 33.3647(46.8318) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 18.0224(17.6616) | Bit/dim 3.6800(3.6809) | Xent 0.6276(0.6782) | Loss 54.7413(60.6505) | Error 0.2278(0.2436) Steps 0(0.00) | Grad Norm 28.2213(46.6602) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 17.6628(17.5017) | Bit/dim 3.6623(3.6797) | Xent 0.6701(0.6761) | Loss 55.3681(59.4791) | Error 0.2356(0.2431) Steps 0(0.00) | Grad Norm 65.9784(46.8466) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 17.5589(17.4852) | Bit/dim 3.6441(3.6777) | Xent 0.6541(0.6745) | Loss 55.0333(58.5995) | Error 0.2311(0.2417) Steps 0(0.00) | Grad Norm 32.3336(46.4979) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 18.2543(17.5463) | Bit/dim 3.6644(3.6785) | Xent 0.6308(0.6719) | Loss 55.5312(58.2090) | Error 0.2344(0.2410) Steps 0(0.00) | Grad Norm 34.9747(45.1283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 91.5958, Epoch Time 1078.5698(1042.7770), Bit/dim 3.6803(best: 3.6777), Xent 0.7120, Loss 4.0363, Error 0.2483(best: 0.2556)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 18.2014(17.6135) | Bit/dim 3.6759(3.6792) | Xent 0.6320(0.6701) | Loss 56.3372(63.4286) | Error 0.2278(0.2400) Steps 0(0.00) | Grad Norm 23.5479(45.3154) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 18.2442(17.5228) | Bit/dim 3.6800(3.6789) | Xent 0.6604(0.6654) | Loss 57.4400(61.4076) | Error 0.2267(0.2381) Steps 0(0.00) | Grad Norm 43.9685(42.8264) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 15.6861(17.5935) | Bit/dim 3.6650(3.6774) | Xent 0.6636(0.6684) | Loss 54.0699(60.1349) | Error 0.2367(0.2381) Steps 0(0.00) | Grad Norm 21.9941(44.1817) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.2952(17.5890) | Bit/dim 3.6940(3.6788) | Xent 0.6565(0.6656) | Loss 57.6981(59.1052) | Error 0.2422(0.2368) Steps 0(0.00) | Grad Norm 40.4231(45.0281) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 18.2722(17.6355) | Bit/dim 3.6730(3.6779) | Xent 0.6340(0.6666) | Loss 54.6852(58.4340) | Error 0.2322(0.2366) Steps 0(0.00) | Grad Norm 52.2581(45.9858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 93.2103, Epoch Time 1083.6277(1044.0025), Bit/dim 3.6739(best: 3.6777), Xent 0.7172, Loss 4.0324, Error 0.2504(best: 0.2483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 16.2301(17.6528) | Bit/dim 3.6742(3.6783) | Xent 0.6412(0.6630) | Loss 56.5959(64.5863) | Error 0.2411(0.2356) Steps 0(0.00) | Grad Norm 23.1685(45.8067) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.1054(17.7577) | Bit/dim 3.6672(3.6785) | Xent 0.6362(0.6615) | Loss 56.1995(62.5198) | Error 0.2256(0.2355) Steps 0(0.00) | Grad Norm 38.7366(46.8476) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 18.7543(17.5697) | Bit/dim 3.6847(3.6803) | Xent 0.6651(0.6699) | Loss 56.6388(60.9830) | Error 0.2300(0.2381) Steps 0(0.00) | Grad Norm 34.7097(49.6205) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 19.1098(17.6323) | Bit/dim 3.6605(3.6778) | Xent 0.6273(0.6656) | Loss 56.2071(59.6672) | Error 0.2411(0.2365) Steps 0(0.00) | Grad Norm 34.8003(47.1278) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 18.8195(17.5513) | Bit/dim 3.6819(3.6748) | Xent 0.6095(0.6697) | Loss 55.4712(58.7611) | Error 0.2100(0.2376) Steps 0(0.00) | Grad Norm 31.7834(49.7490) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 17.3371(17.6211) | Bit/dim 3.6730(3.6769) | Xent 0.6474(0.6631) | Loss 56.3323(58.0961) | Error 0.2322(0.2345) Steps 0(0.00) | Grad Norm 28.6562(46.2287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 90.4735, Epoch Time 1074.4226(1044.9151), Bit/dim 3.6768(best: 3.6739), Xent 0.7433, Loss 4.0485, Error 0.2577(best: 0.2483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.8490(17.5197) | Bit/dim 3.6898(3.6760) | Xent 0.6228(0.6593) | Loss 57.6735(63.3475) | Error 0.2189(0.2336) Steps 0(0.00) | Grad Norm 46.6234(46.4804) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 17.1075(17.4781) | Bit/dim 3.6738(3.6741) | Xent 0.6212(0.6547) | Loss 55.4054(61.3983) | Error 0.2100(0.2320) Steps 0(0.00) | Grad Norm 33.4959(43.0938) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 16.9168(17.4087) | Bit/dim 3.7009(3.6745) | Xent 0.5680(0.6455) | Loss 56.1309(60.1219) | Error 0.1989(0.2296) Steps 0(0.00) | Grad Norm 53.1978(42.8283) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 18.8365(17.4146) | Bit/dim 3.6615(3.6739) | Xent 0.7309(0.6525) | Loss 58.7476(59.1242) | Error 0.2622(0.2330) Steps 0(0.00) | Grad Norm 85.2744(46.2043) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 18.2628(17.4065) | Bit/dim 3.6950(3.6758) | Xent 0.6111(0.6567) | Loss 57.5247(58.5588) | Error 0.2178(0.2348) Steps 0(0.00) | Grad Norm 31.9829(47.3511) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 91.4014, Epoch Time 1073.4668(1045.7717), Bit/dim 3.6755(best: 3.6739), Xent 0.7290, Loss 4.0400, Error 0.2561(best: 0.2483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.9908(17.7553) | Bit/dim 3.6834(3.6746) | Xent 0.6290(0.6566) | Loss 54.6684(64.7361) | Error 0.2244(0.2346) Steps 0(0.00) | Grad Norm 42.9424(46.5451) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 19.1840(17.7744) | Bit/dim 3.6635(3.6751) | Xent 0.6750(0.6551) | Loss 57.2620(62.5689) | Error 0.2278(0.2338) Steps 0(0.00) | Grad Norm 47.9628(45.5453) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 17.1064(17.8641) | Bit/dim 3.6747(3.6731) | Xent 0.6698(0.6545) | Loss 55.5604(60.9434) | Error 0.2356(0.2333) Steps 0(0.00) | Grad Norm 62.5197(49.6418) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 15.8743(17.8547) | Bit/dim 3.6635(3.6737) | Xent 0.6115(0.6591) | Loss 55.3330(59.8941) | Error 0.2222(0.2345) Steps 0(0.00) | Grad Norm 28.7682(49.4979) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 17.2615(17.6841) | Bit/dim 3.6325(3.6715) | Xent 0.6393(0.6618) | Loss 52.7441(58.5896) | Error 0.2400(0.2361) Steps 0(0.00) | Grad Norm 28.3846(47.4225) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 16.8722(17.5923) | Bit/dim 3.6620(3.6707) | Xent 0.5804(0.6576) | Loss 54.6950(58.0486) | Error 0.2100(0.2356) Steps 0(0.00) | Grad Norm 29.2796(45.1169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 90.8368, Epoch Time 1081.2720(1046.8367), Bit/dim 3.6694(best: 3.6739), Xent 0.7173, Loss 4.0281, Error 0.2525(best: 0.2483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.8016(17.6552) | Bit/dim 3.6604(3.6703) | Xent 0.6148(0.6483) | Loss 54.3583(63.0545) | Error 0.2200(0.2308) Steps 0(0.00) | Grad Norm 28.6055(44.3528) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.7679(17.6561) | Bit/dim 3.6331(3.6675) | Xent 0.7217(0.6531) | Loss 55.6053(61.3450) | Error 0.2533(0.2315) Steps 0(0.00) | Grad Norm 90.7503(46.7493) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 17.6549(17.6696) | Bit/dim 3.6931(3.6710) | Xent 0.6617(0.6597) | Loss 56.1600(59.7853) | Error 0.2233(0.2337) Steps 0(0.00) | Grad Norm 62.5646(52.2212) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 17.4348(17.6216) | Bit/dim 3.6711(3.6727) | Xent 0.6253(0.6577) | Loss 55.8154(58.9877) | Error 0.2278(0.2336) Steps 0(0.00) | Grad Norm 47.8558(50.3531) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 19.3835(17.6261) | Bit/dim 3.6745(3.6730) | Xent 0.6418(0.6569) | Loss 58.1219(58.2023) | Error 0.2167(0.2325) Steps 0(0.00) | Grad Norm 31.3959(48.9895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 91.5503, Epoch Time 1081.3985(1047.8735), Bit/dim 3.6726(best: 3.6694), Xent 0.7196, Loss 4.0323, Error 0.2477(best: 0.2483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 18.0590(17.5940) | Bit/dim 3.6482(3.6720) | Xent 0.6139(0.6502) | Loss 56.8865(64.3108) | Error 0.2211(0.2306) Steps 0(0.00) | Grad Norm 24.0328(46.3827) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.5319(17.6659) | Bit/dim 3.6830(3.6707) | Xent 0.6769(0.6401) | Loss 55.7860(62.3567) | Error 0.2367(0.2262) Steps 0(0.00) | Grad Norm 25.7598(44.7695) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 17.2551(17.6509) | Bit/dim 3.6266(3.6715) | Xent 0.5670(0.6349) | Loss 54.3811(60.6228) | Error 0.2044(0.2242) Steps 0(0.00) | Grad Norm 30.8294(43.3860) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.0960(17.6293) | Bit/dim 3.6365(3.6687) | Xent 0.5497(0.6266) | Loss 55.1526(59.3983) | Error 0.1967(0.2217) Steps 0(0.00) | Grad Norm 25.6526(39.2680) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 16.4694(17.5575) | Bit/dim 3.6992(3.6673) | Xent 0.7000(0.6293) | Loss 57.2511(58.5556) | Error 0.2556(0.2233) Steps 0(0.00) | Grad Norm 58.2600(39.5361) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.9680(17.5096) | Bit/dim 3.6733(3.6677) | Xent 0.6799(0.6354) | Loss 54.5460(57.7677) | Error 0.2478(0.2247) Steps 0(0.00) | Grad Norm 59.2656(40.8630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 91.7851, Epoch Time 1074.0621(1048.6592), Bit/dim 3.6740(best: 3.6694), Xent 0.7372, Loss 4.0426, Error 0.2590(best: 0.2477)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 16.7284(17.7495) | Bit/dim 3.7033(3.6694) | Xent 0.5848(0.6330) | Loss 54.0599(62.9839) | Error 0.2156(0.2242) Steps 0(0.00) | Grad Norm 49.8756(42.7890) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.6445(17.7860) | Bit/dim 3.6575(3.6660) | Xent 0.6485(0.6293) | Loss 56.8480(61.3250) | Error 0.2433(0.2237) Steps 0(0.00) | Grad Norm 52.6915(43.7171) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 17.4535(17.7586) | Bit/dim 3.6714(3.6674) | Xent 0.6429(0.6297) | Loss 56.9476(60.0384) | Error 0.2222(0.2232) Steps 0(0.00) | Grad Norm 41.9537(43.2597) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 19.0119(17.8810) | Bit/dim 3.7025(3.6686) | Xent 0.6672(0.6365) | Loss 57.2842(58.9482) | Error 0.2456(0.2254) Steps 0(0.00) | Grad Norm 55.2774(45.0583) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 16.4210(17.9176) | Bit/dim 3.6586(3.6697) | Xent 0.6091(0.6400) | Loss 54.0768(58.1779) | Error 0.2200(0.2276) Steps 0(0.00) | Grad Norm 38.2165(43.6904) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 91.4170, Epoch Time 1109.4276(1050.4823), Bit/dim 3.6725(best: 3.6694), Xent 0.7432, Loss 4.0441, Error 0.2567(best: 0.2477)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.1918(18.0522) | Bit/dim 3.6777(3.6680) | Xent 0.6786(0.6431) | Loss 56.2283(64.1727) | Error 0.2367(0.2283) Steps 0(0.00) | Grad Norm 74.9227(47.9239) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.3512(17.8671) | Bit/dim 3.6330(3.6687) | Xent 0.6652(0.6452) | Loss 56.5117(62.0955) | Error 0.2378(0.2310) Steps 0(0.00) | Grad Norm 47.4168(47.6798) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 16.7984(17.8268) | Bit/dim 3.6500(3.6690) | Xent 0.6659(0.6364) | Loss 56.5740(60.4801) | Error 0.2444(0.2273) Steps 0(0.00) | Grad Norm 55.6940(46.7655) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 16.3403(17.8927) | Bit/dim 3.6754(3.6699) | Xent 0.6739(0.6319) | Loss 54.2308(59.2492) | Error 0.2522(0.2264) Steps 0(0.00) | Grad Norm 44.7407(44.9093) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 18.1387(17.9176) | Bit/dim 3.6419(3.6683) | Xent 0.5989(0.6323) | Loss 55.3316(58.3954) | Error 0.2100(0.2255) Steps 0(0.00) | Grad Norm 26.2893(44.5963) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 17.5222(17.9043) | Bit/dim 3.6538(3.6661) | Xent 0.6777(0.6340) | Loss 54.4739(57.5798) | Error 0.2356(0.2253) Steps 0(0.00) | Grad Norm 50.5773(45.1526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 90.9108, Epoch Time 1092.3986(1051.7397), Bit/dim 3.6668(best: 3.6694), Xent 0.7060, Loss 4.0198, Error 0.2463(best: 0.2477)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.1545(18.0014) | Bit/dim 3.6799(3.6698) | Xent 0.5920(0.6279) | Loss 55.7375(63.0108) | Error 0.2233(0.2246) Steps 0(0.00) | Grad Norm 23.8628(43.1618) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 17.9247(18.0321) | Bit/dim 3.7073(3.6692) | Xent 0.5781(0.6233) | Loss 55.9621(61.1881) | Error 0.2078(0.2216) Steps 0(0.00) | Grad Norm 33.1032(45.0084) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 17.7171(18.0445) | Bit/dim 3.6579(3.6669) | Xent 0.6504(0.6217) | Loss 56.2411(59.7929) | Error 0.2322(0.2215) Steps 0(0.00) | Grad Norm 33.6087(42.8659) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 18.4787(17.9369) | Bit/dim 3.6350(3.6633) | Xent 0.6488(0.6233) | Loss 57.0651(58.7932) | Error 0.2467(0.2219) Steps 0(0.00) | Grad Norm 37.8364(43.1735) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 17.8084(17.7598) | Bit/dim 3.6466(3.6619) | Xent 0.6305(0.6236) | Loss 55.6123(57.9596) | Error 0.2422(0.2218) Steps 0(0.00) | Grad Norm 45.6025(41.1827) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 92.0122, Epoch Time 1092.6567(1052.9673), Bit/dim 3.6671(best: 3.6668), Xent 0.6964, Loss 4.0153, Error 0.2405(best: 0.2463)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 18.5981(17.8061) | Bit/dim 3.6566(3.6600) | Xent 0.6323(0.6165) | Loss 54.8828(64.0880) | Error 0.2367(0.2191) Steps 0(0.00) | Grad Norm 35.9223(40.8275) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 18.7045(17.8589) | Bit/dim 3.6350(3.6564) | Xent 0.6509(0.6149) | Loss 56.7708(61.8577) | Error 0.2289(0.2178) Steps 0(0.00) | Grad Norm 53.8724(41.4277) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 15.6750(17.7598) | Bit/dim 3.6569(3.6565) | Xent 0.6258(0.6181) | Loss 50.4553(59.9629) | Error 0.2122(0.2186) Steps 0(0.00) | Grad Norm 44.5023(43.5908) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.0102(17.7679) | Bit/dim 3.6496(3.6608) | Xent 0.5440(0.6089) | Loss 54.0492(58.8396) | Error 0.1944(0.2162) Steps 0(0.00) | Grad Norm 29.6179(42.4315) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 18.3312(17.8692) | Bit/dim 3.6835(3.6639) | Xent 0.6637(0.6106) | Loss 57.4375(58.1973) | Error 0.2400(0.2176) Steps 0(0.00) | Grad Norm 77.2787(44.7264) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 20.2045(17.8090) | Bit/dim 3.6856(3.6646) | Xent 0.7161(0.6179) | Loss 56.2572(57.4560) | Error 0.2578(0.2207) Steps 0(0.00) | Grad Norm 82.9957(47.1562) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 91.0388, Epoch Time 1089.2377(1054.0554), Bit/dim 3.6781(best: 3.6668), Xent 0.7082, Loss 4.0322, Error 0.2433(best: 0.2405)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 19.1169(17.8865) | Bit/dim 3.6130(3.6638) | Xent 0.5961(0.6217) | Loss 56.4564(62.8272) | Error 0.2044(0.2220) Steps 0(0.00) | Grad Norm 41.1694(47.9851) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 16.7442(17.6382) | Bit/dim 3.6459(3.6667) | Xent 0.5950(0.6170) | Loss 55.2862(61.0821) | Error 0.2056(0.2200) Steps 0(0.00) | Grad Norm 40.5893(46.2473) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 17.3965(17.5853) | Bit/dim 3.6482(3.6650) | Xent 0.6416(0.6202) | Loss 57.6306(59.8508) | Error 0.2300(0.2213) Steps 0(0.00) | Grad Norm 42.3662(48.3815) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 18.2169(17.5576) | Bit/dim 3.6836(3.6660) | Xent 0.5888(0.6149) | Loss 56.6883(58.6442) | Error 0.2000(0.2187) Steps 0(0.00) | Grad Norm 30.6637(46.4901) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 16.7756(17.6731) | Bit/dim 3.6439(3.6658) | Xent 0.6158(0.6193) | Loss 54.6924(57.8572) | Error 0.2067(0.2188) Steps 0(0.00) | Grad Norm 37.8445(46.9180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 91.1767, Epoch Time 1073.8643(1054.6496), Bit/dim 3.6644(best: 3.6668), Xent 0.7288, Loss 4.0288, Error 0.2453(best: 0.2405)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 17.8612(17.5532) | Bit/dim 3.6773(3.6642) | Xent 0.6130(0.6151) | Loss 56.5961(63.9149) | Error 0.2267(0.2184) Steps 0(0.00) | Grad Norm 61.9818(46.7680) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 17.6193(17.6431) | Bit/dim 3.6405(3.6640) | Xent 0.6485(0.6158) | Loss 56.7996(61.9930) | Error 0.2467(0.2202) Steps 0(0.00) | Grad Norm 49.6004(46.2630) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.0283(17.7050) | Bit/dim 3.6791(3.6620) | Xent 0.6230(0.6123) | Loss 57.4995(60.3435) | Error 0.2089(0.2178) Steps 0(0.00) | Grad Norm 50.5110(46.0075) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 18.1177(17.7969) | Bit/dim 3.6671(3.6626) | Xent 0.5874(0.6161) | Loss 56.4029(59.2512) | Error 0.2167(0.2175) Steps 0(0.00) | Grad Norm 61.2440(45.6066) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 17.7473(17.8367) | Bit/dim 3.6514(3.6653) | Xent 0.7044(0.6231) | Loss 56.7189(58.5477) | Error 0.2411(0.2197) Steps 0(0.00) | Grad Norm 61.3722(48.2659) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 17.7802(17.9045) | Bit/dim 3.6408(3.6630) | Xent 0.6287(0.6286) | Loss 55.6704(57.9882) | Error 0.2178(0.2227) Steps 0(0.00) | Grad Norm 59.1409(49.9987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 91.1142, Epoch Time 1095.1280(1055.8640), Bit/dim 3.6668(best: 3.6644), Xent 0.7006, Loss 4.0171, Error 0.2404(best: 0.2405)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 22.0903(17.9176) | Bit/dim 3.6542(3.6600) | Xent 0.6099(0.6215) | Loss 58.0824(63.0189) | Error 0.2189(0.2210) Steps 0(0.00) | Grad Norm 39.8891(46.8505) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 19.1256(17.8594) | Bit/dim 3.6556(3.6576) | Xent 0.5691(0.6149) | Loss 55.7409(61.0544) | Error 0.2000(0.2186) Steps 0(0.00) | Grad Norm 25.4586(44.9731) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 17.2786(17.7839) | Bit/dim 3.6514(3.6597) | Xent 0.6240(0.6166) | Loss 55.4988(59.6425) | Error 0.2122(0.2192) Steps 0(0.00) | Grad Norm 57.1775(45.4219) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 18.8656(17.8965) | Bit/dim 3.6684(3.6615) | Xent 0.5926(0.6145) | Loss 57.6444(58.8182) | Error 0.2033(0.2180) Steps 0(0.00) | Grad Norm 43.5934(46.6417) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 17.8961(17.9610) | Bit/dim 3.6581(3.6643) | Xent 0.5789(0.6172) | Loss 56.0570(58.2122) | Error 0.2000(0.2189) Steps 0(0.00) | Grad Norm 49.3052(48.0525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 92.8932, Epoch Time 1097.0056(1057.0982), Bit/dim 3.6699(best: 3.6644), Xent 0.7059, Loss 4.0229, Error 0.2452(best: 0.2404)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 16.3791(17.9474) | Bit/dim 3.6109(3.6641) | Xent 0.5517(0.6142) | Loss 53.9415(63.9840) | Error 0.1911(0.2161) Steps 0(0.00) | Grad Norm 29.2704(46.2140) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 17.0488(17.9384) | Bit/dim 3.6538(3.6633) | Xent 0.5497(0.6084) | Loss 53.7539(61.9323) | Error 0.2056(0.2156) Steps 0(0.00) | Grad Norm 25.0521(44.4916) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 16.5159(17.9466) | Bit/dim 3.6570(3.6621) | Xent 0.6237(0.6070) | Loss 55.4228(60.3680) | Error 0.2233(0.2157) Steps 0(0.00) | Grad Norm 74.0121(45.4223) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 19.6631(18.0581) | Bit/dim 3.6373(3.6609) | Xent 0.6086(0.6095) | Loss 57.5752(59.2271) | Error 0.2100(0.2169) Steps 0(0.00) | Grad Norm 43.3428(46.5128) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 17.6830(17.9504) | Bit/dim 3.6540(3.6617) | Xent 0.6258(0.6175) | Loss 55.1206(58.5554) | Error 0.2200(0.2189) Steps 0(0.00) | Grad Norm 34.7653(46.2971) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 16.5612(17.8217) | Bit/dim 3.6518(3.6619) | Xent 0.5997(0.6120) | Loss 55.0295(57.7537) | Error 0.2089(0.2176) Steps 0(0.00) | Grad Norm 33.5885(46.0681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 92.3335, Epoch Time 1090.5090(1058.1006), Bit/dim 3.6582(best: 3.6644), Xent 0.6852, Loss 4.0008, Error 0.2345(best: 0.2404)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 16.6808(17.8039) | Bit/dim 3.6517(3.6597) | Xent 0.5793(0.6056) | Loss 55.2699(62.8926) | Error 0.2133(0.2166) Steps 0(0.00) | Grad Norm 38.9356(43.2482) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 20.5344(17.8985) | Bit/dim 3.6523(3.6592) | Xent 0.5611(0.5990) | Loss 57.6436(61.0922) | Error 0.1933(0.2134) Steps 0(0.00) | Grad Norm 45.4862(44.5443) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 17.7314(17.9273) | Bit/dim 3.6696(3.6552) | Xent 0.6454(0.5985) | Loss 56.6176(59.6816) | Error 0.2256(0.2138) Steps 0(0.00) | Grad Norm 92.4090(45.3172) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 16.8905(17.9945) | Bit/dim 3.6563(3.6579) | Xent 0.6054(0.6016) | Loss 54.3357(58.7543) | Error 0.2267(0.2158) Steps 0(0.00) | Grad Norm 47.5016(48.7968) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 18.1177(18.0158) | Bit/dim 3.6677(3.6587) | Xent 0.6000(0.6036) | Loss 54.4544(57.9637) | Error 0.2044(0.2144) Steps 0(0.00) | Grad Norm 45.6953(47.5292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 91.3611, Epoch Time 1104.5459(1059.4939), Bit/dim 3.6568(best: 3.6582), Xent 0.6788, Loss 3.9962, Error 0.2375(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 19.2935(18.1205) | Bit/dim 3.6478(3.6588) | Xent 0.5684(0.5991) | Loss 56.4074(64.3546) | Error 0.2089(0.2131) Steps 0(0.00) | Grad Norm 31.8274(45.0177) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 16.7401(18.0943) | Bit/dim 3.6696(3.6594) | Xent 0.5826(0.5908) | Loss 55.9774(62.1943) | Error 0.1922(0.2093) Steps 0(0.00) | Grad Norm 56.8242(44.1658) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 16.1324(18.0466) | Bit/dim 3.6186(3.6574) | Xent 0.6223(0.5939) | Loss 55.8074(60.5607) | Error 0.2156(0.2098) Steps 0(0.00) | Grad Norm 59.5098(44.2577) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.4807(18.0082) | Bit/dim 3.6758(3.6565) | Xent 0.5552(0.5952) | Loss 55.0683(59.4815) | Error 0.2044(0.2102) Steps 0(0.00) | Grad Norm 26.2858(42.9511) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 17.5953(18.1264) | Bit/dim 3.6227(3.6553) | Xent 0.5433(0.5967) | Loss 54.7784(58.5042) | Error 0.1978(0.2119) Steps 0(0.00) | Grad Norm 34.4243(43.1862) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 16.8972(18.0303) | Bit/dim 3.6470(3.6561) | Xent 0.5714(0.6005) | Loss 56.0150(57.7545) | Error 0.2044(0.2125) Steps 0(0.00) | Grad Norm 34.7259(45.8928) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 91.5973, Epoch Time 1102.4812(1060.7835), Bit/dim 3.6532(best: 3.6568), Xent 0.6917, Loss 3.9991, Error 0.2382(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 17.9887(17.8768) | Bit/dim 3.6528(3.6552) | Xent 0.5559(0.5889) | Loss 56.3982(63.0840) | Error 0.2078(0.2092) Steps 0(0.00) | Grad Norm 22.0712(42.8021) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 16.6789(17.8124) | Bit/dim 3.6488(3.6546) | Xent 0.5772(0.5838) | Loss 54.8979(60.9287) | Error 0.1989(0.2077) Steps 0(0.00) | Grad Norm 44.5338(43.7601) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 17.5519(17.7838) | Bit/dim 3.6747(3.6564) | Xent 0.6443(0.5863) | Loss 56.2666(59.7776) | Error 0.2167(0.2087) Steps 0(0.00) | Grad Norm 45.4200(45.8552) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 18.3290(17.8506) | Bit/dim 3.6616(3.6547) | Xent 0.5919(0.5898) | Loss 56.8250(58.6570) | Error 0.2122(0.2095) Steps 0(0.00) | Grad Norm 42.4364(46.8425) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 18.2603(17.9872) | Bit/dim 3.6577(3.6548) | Xent 0.6176(0.5946) | Loss 55.3033(57.9578) | Error 0.2311(0.2129) Steps 0(0.00) | Grad Norm 46.3932(46.9237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 92.6606, Epoch Time 1089.5916(1061.6478), Bit/dim 3.6614(best: 3.6532), Xent 0.7119, Loss 4.0173, Error 0.2447(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 18.1111(17.9858) | Bit/dim 3.6690(3.6560) | Xent 0.5968(0.5953) | Loss 54.6125(64.0537) | Error 0.2222(0.2129) Steps 0(0.00) | Grad Norm 43.0496(46.1725) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 17.8844(18.0133) | Bit/dim 3.6694(3.6545) | Xent 0.5545(0.5867) | Loss 55.9235(61.8629) | Error 0.1922(0.2097) Steps 0(0.00) | Grad Norm 57.0883(45.4097) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 16.4810(17.9569) | Bit/dim 3.6606(3.6555) | Xent 0.5450(0.5813) | Loss 54.4386(60.0374) | Error 0.1867(0.2061) Steps 0(0.00) | Grad Norm 41.3827(44.4065) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 17.6057(17.9489) | Bit/dim 3.6556(3.6542) | Xent 0.5996(0.5840) | Loss 56.6105(59.0005) | Error 0.2178(0.2064) Steps 0(0.00) | Grad Norm 28.9740(45.4843) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 17.5784(17.8462) | Bit/dim 3.6500(3.6583) | Xent 0.5871(0.5887) | Loss 54.5521(58.2125) | Error 0.2044(0.2076) Steps 0(0.00) | Grad Norm 54.8572(47.6604) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 17.1930(17.7428) | Bit/dim 3.6238(3.6530) | Xent 0.5260(0.5886) | Loss 55.6306(57.5339) | Error 0.1900(0.2079) Steps 0(0.00) | Grad Norm 42.1909(44.3474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 93.8438, Epoch Time 1091.6527(1062.5479), Bit/dim 3.6585(best: 3.6532), Xent 0.6997, Loss 4.0084, Error 0.2355(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 18.7436(17.6735) | Bit/dim 3.6427(3.6540) | Xent 0.5824(0.5824) | Loss 55.1314(62.7853) | Error 0.1944(0.2065) Steps 0(0.00) | Grad Norm 49.1061(45.2595) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 17.0935(17.6199) | Bit/dim 3.6720(3.6565) | Xent 0.6042(0.5774) | Loss 54.8108(60.9752) | Error 0.2244(0.2053) Steps 0(0.00) | Grad Norm 43.2584(44.4480) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 17.7894(17.5837) | Bit/dim 3.6476(3.6535) | Xent 0.5815(0.5790) | Loss 54.0484(59.5373) | Error 0.1989(0.2065) Steps 0(0.00) | Grad Norm 25.6298(43.5653) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 19.4311(17.7217) | Bit/dim 3.6667(3.6527) | Xent 0.5275(0.5791) | Loss 56.5389(58.6452) | Error 0.1878(0.2066) Steps 0(0.00) | Grad Norm 45.7718(42.4619) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 18.8589(17.9260) | Bit/dim 3.6380(3.6505) | Xent 0.5152(0.5731) | Loss 55.9552(58.0314) | Error 0.1967(0.2050) Steps 0(0.00) | Grad Norm 50.1704(41.1165) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 92.9268, Epoch Time 1091.6650(1063.4214), Bit/dim 3.6476(best: 3.6532), Xent 0.6984, Loss 3.9968, Error 0.2371(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 18.2121(18.0918) | Bit/dim 3.6688(3.6480) | Xent 0.4885(0.5702) | Loss 58.0410(64.3003) | Error 0.1744(0.2034) Steps 0(0.00) | Grad Norm 45.1926(41.6268) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 17.5243(18.0080) | Bit/dim 3.6475(3.6509) | Xent 0.5786(0.5655) | Loss 56.6693(62.0725) | Error 0.2056(0.2013) Steps 0(0.00) | Grad Norm 52.7470(41.9295) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 18.8665(17.8960) | Bit/dim 3.6541(3.6491) | Xent 0.5424(0.5679) | Loss 56.2088(60.1843) | Error 0.1922(0.2036) Steps 0(0.00) | Grad Norm 27.7856(43.3147) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 17.3733(17.9755) | Bit/dim 3.6352(3.6494) | Xent 0.5563(0.5628) | Loss 56.6827(59.0439) | Error 0.1978(0.2017) Steps 0(0.00) | Grad Norm 37.9509(41.0129) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 18.5007(17.9578) | Bit/dim 3.6700(3.6472) | Xent 0.5486(0.5670) | Loss 55.4883(58.1265) | Error 0.1900(0.2023) Steps 0(0.00) | Grad Norm 44.5534(44.2235) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 22.1246(18.0307) | Bit/dim 3.6868(3.6491) | Xent 0.6060(0.5744) | Loss 56.2019(57.3932) | Error 0.2267(0.2047) Steps 0(0.00) | Grad Norm 47.1414(44.7367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 91.4480, Epoch Time 1100.6297(1064.5377), Bit/dim 3.6462(best: 3.6476), Xent 0.6844, Loss 3.9884, Error 0.2368(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 17.5040(18.0116) | Bit/dim 3.6572(3.6498) | Xent 0.6450(0.5736) | Loss 56.8175(62.8138) | Error 0.2178(0.2044) Steps 0(0.00) | Grad Norm 74.5256(44.8024) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 20.3133(18.1237) | Bit/dim 3.6435(3.6483) | Xent 0.5881(0.5740) | Loss 54.5517(60.9379) | Error 0.2000(0.2050) Steps 0(0.00) | Grad Norm 41.3894(46.3436) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 17.8565(18.0281) | Bit/dim 3.6428(3.6510) | Xent 0.5519(0.5773) | Loss 56.9042(59.5015) | Error 0.2033(0.2054) Steps 0(0.00) | Grad Norm 36.7001(49.8132) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 16.4584(17.9081) | Bit/dim 3.6340(3.6496) | Xent 0.5348(0.5762) | Loss 56.9035(58.4915) | Error 0.2044(0.2058) Steps 0(0.00) | Grad Norm 34.5641(47.2389) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 17.0256(17.8551) | Bit/dim 3.6174(3.6464) | Xent 0.5570(0.5717) | Loss 54.2971(57.7001) | Error 0.1967(0.2043) Steps 0(0.00) | Grad Norm 28.6919(42.3858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 91.1401, Epoch Time 1093.1141(1065.3950), Bit/dim 3.6463(best: 3.6462), Xent 0.7096, Loss 4.0011, Error 0.2389(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 18.0100(17.9219) | Bit/dim 3.6677(3.6493) | Xent 0.5344(0.5646) | Loss 55.3097(63.8816) | Error 0.1922(0.2010) Steps 0(0.00) | Grad Norm 30.2205(40.3935) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 18.3605(17.8048) | Bit/dim 3.6653(3.6517) | Xent 0.5222(0.5585) | Loss 55.9270(61.5429) | Error 0.1733(0.1978) Steps 0(0.00) | Grad Norm 32.9726(38.4306) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 17.2616(17.7669) | Bit/dim 3.6399(3.6490) | Xent 0.5407(0.5548) | Loss 55.5787(60.0046) | Error 0.1878(0.1964) Steps 0(0.00) | Grad Norm 45.5937(39.2358) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 16.6588(17.6867) | Bit/dim 3.6430(3.6472) | Xent 0.6439(0.5602) | Loss 55.4244(58.8936) | Error 0.2322(0.1998) Steps 0(0.00) | Grad Norm 73.1116(41.5903) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 16.6300(17.7592) | Bit/dim 3.6284(3.6460) | Xent 0.5338(0.5604) | Loss 54.5461(58.0222) | Error 0.2044(0.2003) Steps 0(0.00) | Grad Norm 56.7851(43.8241) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 18.8486(17.9029) | Bit/dim 3.6203(3.6471) | Xent 0.5426(0.5672) | Loss 55.0277(57.3403) | Error 0.1878(0.2039) Steps 0(0.00) | Grad Norm 43.8586(46.0277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 94.0124, Epoch Time 1091.6554(1066.1828), Bit/dim 3.6508(best: 3.6462), Xent 0.6953, Loss 3.9984, Error 0.2417(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 17.8746(18.0127) | Bit/dim 3.6183(3.6471) | Xent 0.6268(0.5730) | Loss 55.3349(62.9442) | Error 0.2078(0.2057) Steps 0(0.00) | Grad Norm 54.8668(47.1860) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 16.6476(17.9198) | Bit/dim 3.6342(3.6476) | Xent 0.5813(0.5774) | Loss 56.5635(61.0355) | Error 0.1956(0.2059) Steps 0(0.00) | Grad Norm 41.7807(50.2462) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 17.6357(18.0941) | Bit/dim 3.6582(3.6511) | Xent 0.5637(0.5731) | Loss 52.6307(59.6542) | Error 0.1922(0.2036) Steps 0(0.00) | Grad Norm 32.0902(47.3921) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 17.5474(18.0725) | Bit/dim 3.6387(3.6469) | Xent 0.5747(0.5725) | Loss 54.2909(58.4562) | Error 0.2133(0.2030) Steps 0(0.00) | Grad Norm 47.3182(46.8344) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 16.6564(17.9117) | Bit/dim 3.6333(3.6456) | Xent 0.5699(0.5725) | Loss 52.3291(57.6153) | Error 0.2067(0.2034) Steps 0(0.00) | Grad Norm 40.6407(46.2269) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 93.3629, Epoch Time 1104.4009(1067.3293), Bit/dim 3.6490(best: 3.6462), Xent 0.6898, Loss 3.9939, Error 0.2324(best: 0.2345)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 17.4518(17.9620) | Bit/dim 3.6505(3.6464) | Xent 0.6004(0.5704) | Loss 55.7924(63.8248) | Error 0.2122(0.2021) Steps 0(0.00) | Grad Norm 62.8810(43.4813) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 17.3617(17.9205) | Bit/dim 3.6617(3.6501) | Xent 0.5569(0.5691) | Loss 55.5579(61.8550) | Error 0.1944(0.2015) Steps 0(0.00) | Grad Norm 56.2289(42.8124) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 17.3228(17.9182) | Bit/dim 3.6657(3.6496) | Xent 0.5804(0.5656) | Loss 57.0879(60.3024) | Error 0.2200(0.2004) Steps 0(0.00) | Grad Norm 42.5861(41.5426) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 15.8204(17.8556) | Bit/dim 3.6118(3.6472) | Xent 0.5843(0.5630) | Loss 53.3815(58.9244) | Error 0.2178(0.1991) Steps 0(0.00) | Grad Norm 51.5235(41.4309) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 18.0738(17.9789) | Bit/dim 3.6096(3.6435) | Xent 0.5349(0.5611) | Loss 54.7877(58.0142) | Error 0.1822(0.1985) Steps 0(0.00) | Grad Norm 21.1465(42.3557) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 19.3304(18.0878) | Bit/dim 3.6503(3.6450) | Xent 0.5807(0.5603) | Loss 57.0269(57.3849) | Error 0.2022(0.1972) Steps 0(0.00) | Grad Norm 46.8272(42.4152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 93.4703, Epoch Time 1102.0448(1068.3708), Bit/dim 3.6430(best: 3.6462), Xent 0.6755, Loss 3.9808, Error 0.2336(best: 0.2324)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 17.8186(17.9727) | Bit/dim 3.6263(3.6461) | Xent 0.5555(0.5544) | Loss 56.3824(62.6068) | Error 0.1967(0.1949) Steps 0(0.00) | Grad Norm 47.1229(41.4160) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 18.5819(18.0113) | Bit/dim 3.6664(3.6462) | Xent 0.5352(0.5593) | Loss 57.2454(60.7727) | Error 0.1933(0.1971) Steps 0(0.00) | Grad Norm 40.9851(43.1048) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 16.1957(17.9631) | Bit/dim 3.6349(3.6466) | Xent 0.5138(0.5551) | Loss 52.6135(59.4769) | Error 0.1789(0.1958) Steps 0(0.00) | Grad Norm 29.7094(43.5303) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 19.2737(17.9576) | Bit/dim 3.6643(3.6494) | Xent 0.5628(0.5501) | Loss 56.8481(58.4839) | Error 0.2033(0.1939) Steps 0(0.00) | Grad Norm 43.1748(42.3544) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 17.8137(18.0793) | Bit/dim 3.6325(3.6482) | Xent 0.5596(0.5490) | Loss 55.2978(57.8218) | Error 0.1944(0.1943) Steps 0(0.00) | Grad Norm 35.5043(42.3766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 92.9123, Epoch Time 1100.1073(1069.3229), Bit/dim 3.6494(best: 3.6430), Xent 0.7152, Loss 4.0070, Error 0.2426(best: 0.2324)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 17.7744(18.0457) | Bit/dim 3.6751(3.6441) | Xent 0.5717(0.5483) | Loss 56.8944(64.2096) | Error 0.2100(0.1940) Steps 0(0.00) | Grad Norm 55.9643(43.4096) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 18.1290(18.0690) | Bit/dim 3.6424(3.6448) | Xent 0.5237(0.5490) | Loss 54.0925(61.9006) | Error 0.1922(0.1945) Steps 0(0.00) | Grad Norm 38.9017(46.1062) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 17.7452(18.1834) | Bit/dim 3.6254(3.6467) | Xent 0.5702(0.5480) | Loss 55.0958(60.1306) | Error 0.2167(0.1961) Steps 0(0.00) | Grad Norm 45.7883(46.8474) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 17.2082(18.0368) | Bit/dim 3.6737(3.6449) | Xent 0.5445(0.5475) | Loss 56.3517(59.0086) | Error 0.1789(0.1969) Steps 0(0.00) | Grad Norm 44.4760(44.4016) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 17.5787(17.8730) | Bit/dim 3.6669(3.6442) | Xent 0.6074(0.5480) | Loss 57.7887(58.1326) | Error 0.2122(0.1954) Steps 0(0.00) | Grad Norm 61.1514(42.7603) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 17.4267(17.8041) | Bit/dim 3.6367(3.6446) | Xent 0.5503(0.5499) | Loss 54.4949(57.3034) | Error 0.1822(0.1962) Steps 0(0.00) | Grad Norm 38.8127(43.8427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 91.2528, Epoch Time 1092.8820(1070.0297), Bit/dim 3.6495(best: 3.6430), Xent 0.7040, Loss 4.0016, Error 0.2412(best: 0.2324)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 17.9917(17.7714) | Bit/dim 3.6638(3.6447) | Xent 0.5082(0.5475) | Loss 57.5378(62.8118) | Error 0.1733(0.1940) Steps 0(0.00) | Grad Norm 31.4404(44.4635) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 17.8040(17.9258) | Bit/dim 3.6367(3.6473) | Xent 0.4520(0.5432) | Loss 54.5072(60.9819) | Error 0.1456(0.1934) Steps 0(0.00) | Grad Norm 26.0051(43.5234) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 18.8718(18.0374) | Bit/dim 3.6372(3.6473) | Xent 0.4988(0.5477) | Loss 55.6069(59.6005) | Error 0.1733(0.1950) Steps 0(0.00) | Grad Norm 38.1405(44.4490) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 19.1986(18.0925) | Bit/dim 3.6579(3.6451) | Xent 0.4997(0.5417) | Loss 56.9380(58.5713) | Error 0.1667(0.1924) Steps 0(0.00) | Grad Norm 40.6512(43.0569) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 18.2721(18.3243) | Bit/dim 3.6345(3.6442) | Xent 0.5766(0.5469) | Loss 55.3391(57.9764) | Error 0.1967(0.1941) Steps 0(0.00) | Grad Norm 36.9395(45.3522) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 94.2543, Epoch Time 1122.8844(1071.6153), Bit/dim 3.6478(best: 3.6430), Xent 0.6812, Loss 3.9883, Error 0.2301(best: 0.2324)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 17.4616(18.3356) | Bit/dim 3.6480(3.6426) | Xent 0.5354(0.5365) | Loss 54.9391(63.7454) | Error 0.1889(0.1902) Steps 0(0.00) | Grad Norm 24.2839(42.1124) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 25.5737(18.6362) | Bit/dim 3.6087(3.6389) | Xent 0.5616(0.5316) | Loss 57.9170(61.5166) | Error 0.2000(0.1887) Steps 0(0.00) | Grad Norm 45.9305(41.3576) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 17.6170(18.5680) | Bit/dim 3.6441(3.6395) | Xent 0.5337(0.5331) | Loss 55.8183(60.0755) | Error 0.1644(0.1882) Steps 0(0.00) | Grad Norm 42.5842(40.9934) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 16.9242(18.3407) | Bit/dim 3.6597(3.6399) | Xent 0.5720(0.5407) | Loss 55.5419(58.8583) | Error 0.2122(0.1914) Steps 0(0.00) | Grad Norm 40.3456(41.5312) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.3015(18.1919) | Bit/dim 3.6641(3.6417) | Xent 0.5561(0.5357) | Loss 57.7395(57.8348) | Error 0.1978(0.1902) Steps 0(0.00) | Grad Norm 27.0169(39.4777) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 18.0884(18.1464) | Bit/dim 3.6499(3.6435) | Xent 0.5730(0.5457) | Loss 56.3433(57.4875) | Error 0.1967(0.1939) Steps 0(0.00) | Grad Norm 32.3087(41.9966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 92.1173, Epoch Time 1111.8481(1072.8223), Bit/dim 3.6476(best: 3.6430), Xent 0.7864, Loss 4.0408, Error 0.2634(best: 0.2301)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 19.5087(18.1038) | Bit/dim 3.6659(3.6445) | Xent 0.5130(0.5451) | Loss 55.6769(62.5371) | Error 0.1678(0.1923) Steps 0(0.00) | Grad Norm 33.7998(45.7365) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 18.2312(18.2336) | Bit/dim 3.6311(3.6445) | Xent 0.6808(0.5498) | Loss 55.9291(60.6844) | Error 0.2322(0.1950) Steps 0(0.00) | Grad Norm 119.3900(48.5520) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 19.2272(18.4170) | Bit/dim 3.6968(3.6502) | Xent 0.4856(0.5481) | Loss 57.5246(59.3422) | Error 0.1789(0.1935) Steps 0(0.00) | Grad Norm 34.8620(47.3518) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 17.7991(18.2947) | Bit/dim 3.6448(3.6454) | Xent 0.6077(0.5451) | Loss 57.6059(58.3631) | Error 0.2133(0.1926) Steps 0(0.00) | Grad Norm 40.4540(45.4099) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 17.5055(18.3788) | Bit/dim 3.6348(3.6431) | Xent 0.5379(0.5448) | Loss 56.0288(57.7394) | Error 0.1867(0.1924) Steps 0(0.00) | Grad Norm 38.4868(45.3963) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 94.1263, Epoch Time 1127.6146(1074.4661), Bit/dim 3.6462(best: 3.6430), Xent 0.6899, Loss 3.9912, Error 0.2319(best: 0.2301)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 17.3906(18.4027) | Bit/dim 3.6223(3.6428) | Xent 0.5264(0.5388) | Loss 54.3951(64.0158) | Error 0.2011(0.1905) Steps 0(0.00) | Grad Norm 31.9061(43.3684) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 18.5198(18.4513) | Bit/dim 3.6638(3.6422) | Xent 0.4695(0.5344) | Loss 54.3054(61.7339) | Error 0.1644(0.1880) Steps 0(0.00) | Grad Norm 32.4698(41.3353) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 20.3717(18.5954) | Bit/dim 3.6535(3.6413) | Xent 0.5610(0.5292) | Loss 56.3027(60.2564) | Error 0.1989(0.1868) Steps 0(0.00) | Grad Norm 48.3736(40.5240) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 18.8167(18.5223) | Bit/dim 3.6164(3.6398) | Xent 0.5155(0.5306) | Loss 56.2682(59.0355) | Error 0.1856(0.1879) Steps 0(0.00) | Grad Norm 29.1691(41.2952) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 18.5698(18.4963) | Bit/dim 3.6501(3.6411) | Xent 0.6134(0.5378) | Loss 56.1901(58.2518) | Error 0.2089(0.1903) Steps 0(0.00) | Grad Norm 82.9577(44.5518) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 17.0299(18.4025) | Bit/dim 3.6237(3.6392) | Xent 0.4984(0.5337) | Loss 55.2549(57.6438) | Error 0.1811(0.1899) Steps 0(0.00) | Grad Norm 28.8366(44.6054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 93.1485, Epoch Time 1125.7967(1076.0060), Bit/dim 3.6429(best: 3.6430), Xent 0.6577, Loss 3.9717, Error 0.2271(best: 0.2301)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 17.2798(18.4115) | Bit/dim 3.6437(3.6376) | Xent 0.4536(0.5187) | Loss 55.3001(62.8309) | Error 0.1633(0.1843) Steps 0(0.00) | Grad Norm 25.6609(40.7323) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.4242(18.3836) | Bit/dim 3.6494(3.6395) | Xent 0.5070(0.5170) | Loss 57.6525(61.0571) | Error 0.1811(0.1840) Steps 0(0.00) | Grad Norm 62.3559(40.3192) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 17.7414(18.2252) | Bit/dim 3.6033(3.6373) | Xent 0.5134(0.5245) | Loss 54.1088(59.5285) | Error 0.1856(0.1856) Steps 0(0.00) | Grad Norm 35.5187(42.6639) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 16.1715(18.1254) | Bit/dim 3.6166(3.6365) | Xent 0.5070(0.5281) | Loss 54.1320(58.4772) | Error 0.1811(0.1874) Steps 0(0.00) | Grad Norm 28.0594(43.8096) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 17.9210(18.1785) | Bit/dim 3.6232(3.6337) | Xent 0.5493(0.5257) | Loss 55.1533(57.6214) | Error 0.1878(0.1859) Steps 0(0.00) | Grad Norm 50.8390(42.7228) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 94.2739, Epoch Time 1110.8621(1077.0517), Bit/dim 3.6328(best: 3.6429), Xent 0.6694, Loss 3.9675, Error 0.2286(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 17.4047(18.1661) | Bit/dim 3.6245(3.6331) | Xent 0.4845(0.5240) | Loss 54.2421(64.0424) | Error 0.1667(0.1855) Steps 0(0.00) | Grad Norm 43.9261(41.9156) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 16.6680(18.1771) | Bit/dim 3.6572(3.6335) | Xent 0.5236(0.5215) | Loss 54.4708(61.7616) | Error 0.1667(0.1844) Steps 0(0.00) | Grad Norm 45.2238(41.1671) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 17.9822(18.2697) | Bit/dim 3.6210(3.6338) | Xent 0.5587(0.5294) | Loss 55.7351(60.2989) | Error 0.1922(0.1865) Steps 0(0.00) | Grad Norm 40.4054(44.5667) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 18.2758(18.3092) | Bit/dim 3.6463(3.6321) | Xent 0.5419(0.5350) | Loss 56.8104(59.1543) | Error 0.1811(0.1889) Steps 0(0.00) | Grad Norm 37.6432(43.6858) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 18.3434(18.4191) | Bit/dim 3.6537(3.6366) | Xent 0.5081(0.5351) | Loss 56.4580(58.2896) | Error 0.1889(0.1905) Steps 0(0.00) | Grad Norm 25.4339(41.9892) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 18.5954(18.3829) | Bit/dim 3.6389(3.6372) | Xent 0.4965(0.5344) | Loss 54.9013(57.5329) | Error 0.1767(0.1907) Steps 0(0.00) | Grad Norm 44.7835(43.3587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 93.4496, Epoch Time 1124.0081(1078.4604), Bit/dim 3.6406(best: 3.6328), Xent 0.6713, Loss 3.9762, Error 0.2295(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.1145(18.3179) | Bit/dim 3.6209(3.6349) | Xent 0.4914(0.5218) | Loss 56.1101(62.8071) | Error 0.1656(0.1854) Steps 0(0.00) | Grad Norm 30.5372(40.4481) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 20.8633(18.2861) | Bit/dim 3.6225(3.6354) | Xent 0.5119(0.5290) | Loss 57.2102(60.8390) | Error 0.1856(0.1878) Steps 0(0.00) | Grad Norm 37.3290(42.7820) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 18.4853(18.3733) | Bit/dim 3.6609(3.6378) | Xent 0.4928(0.5197) | Loss 57.1908(59.5189) | Error 0.1767(0.1841) Steps 0(0.00) | Grad Norm 41.7568(40.9804) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 20.9368(18.5239) | Bit/dim 3.6137(3.6374) | Xent 0.5297(0.5198) | Loss 55.5330(58.5107) | Error 0.2000(0.1842) Steps 0(0.00) | Grad Norm 50.0380(42.6061) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 18.0652(18.4750) | Bit/dim 3.6076(3.6355) | Xent 0.5421(0.5203) | Loss 55.5973(57.6270) | Error 0.2033(0.1852) Steps 0(0.00) | Grad Norm 34.1118(41.8537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 93.5238, Epoch Time 1124.0333(1079.8275), Bit/dim 3.6348(best: 3.6328), Xent 0.6940, Loss 3.9818, Error 0.2304(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.5672(18.3836) | Bit/dim 3.6612(3.6336) | Xent 0.5194(0.5208) | Loss 54.9876(64.0669) | Error 0.1889(0.1859) Steps 0(0.00) | Grad Norm 51.1851(41.9345) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 18.3142(18.3751) | Bit/dim 3.6462(3.6353) | Xent 0.5225(0.5123) | Loss 55.3641(61.7146) | Error 0.1867(0.1830) Steps 0(0.00) | Grad Norm 40.7375(40.6348) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 19.9611(18.4874) | Bit/dim 3.6365(3.6363) | Xent 0.5633(0.5045) | Loss 56.4319(60.0073) | Error 0.2144(0.1808) Steps 0(0.00) | Grad Norm 59.0958(40.3111) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 18.9368(18.5985) | Bit/dim 3.6428(3.6354) | Xent 0.5259(0.5113) | Loss 55.7473(58.8532) | Error 0.1922(0.1831) Steps 0(0.00) | Grad Norm 46.5748(42.1223) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 18.5494(18.6250) | Bit/dim 3.6250(3.6361) | Xent 0.5325(0.5242) | Loss 55.9438(58.0918) | Error 0.1844(0.1867) Steps 0(0.00) | Grad Norm 48.7317(46.3510) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 19.3896(18.6189) | Bit/dim 3.6251(3.6338) | Xent 0.5479(0.5232) | Loss 56.8723(57.4486) | Error 0.1956(0.1856) Steps 0(0.00) | Grad Norm 39.3361(44.1523) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 93.0115, Epoch Time 1132.9983(1081.4227), Bit/dim 3.6446(best: 3.6328), Xent 0.6766, Loss 3.9829, Error 0.2332(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 18.7336(18.6136) | Bit/dim 3.6580(3.6315) | Xent 0.5304(0.5201) | Loss 54.2670(62.9447) | Error 0.1800(0.1841) Steps 0(0.00) | Grad Norm 48.5847(44.0368) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 25.5438(18.9021) | Bit/dim 3.6414(3.6324) | Xent 0.4663(0.5141) | Loss 57.0657(60.9481) | Error 0.1700(0.1814) Steps 0(0.00) | Grad Norm 47.3165(43.6174) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 17.2805(18.7693) | Bit/dim 3.5920(3.6319) | Xent 0.4926(0.5121) | Loss 55.2002(59.5208) | Error 0.1800(0.1810) Steps 0(0.00) | Grad Norm 43.7105(42.4125) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 17.2900(18.6900) | Bit/dim 3.6652(3.6320) | Xent 0.5167(0.5205) | Loss 55.1830(58.5217) | Error 0.1867(0.1840) Steps 0(0.00) | Grad Norm 37.1468(45.2441) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 20.4855(18.7878) | Bit/dim 3.6360(3.6340) | Xent 0.4589(0.5191) | Loss 56.1151(57.5387) | Error 0.1656(0.1851) Steps 0(0.00) | Grad Norm 38.5949(47.1229) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 93.4166, Epoch Time 1144.9715(1083.3291), Bit/dim 3.6396(best: 3.6328), Xent 0.6740, Loss 3.9766, Error 0.2287(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 17.6602(18.6846) | Bit/dim 3.6090(3.6338) | Xent 0.5528(0.5225) | Loss 54.5604(63.6623) | Error 0.1911(0.1863) Steps 0(0.00) | Grad Norm 36.0267(46.2312) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 17.3638(18.6328) | Bit/dim 3.6703(3.6371) | Xent 0.5592(0.5157) | Loss 57.3203(61.4908) | Error 0.1933(0.1832) Steps 0(0.00) | Grad Norm 41.9803(43.9553) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 18.2011(18.5150) | Bit/dim 3.6232(3.6350) | Xent 0.4665(0.5099) | Loss 54.3744(59.8435) | Error 0.1711(0.1809) Steps 0(0.00) | Grad Norm 33.6551(41.6632) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 18.0659(18.4392) | Bit/dim 3.6479(3.6382) | Xent 0.4426(0.5053) | Loss 54.5627(58.7250) | Error 0.1544(0.1792) Steps 0(0.00) | Grad Norm 28.9301(39.6701) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 19.8987(18.4755) | Bit/dim 3.6392(3.6357) | Xent 0.5313(0.5019) | Loss 57.2591(57.9430) | Error 0.1889(0.1773) Steps 0(0.00) | Grad Norm 56.6610(40.3196) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.9276(18.5094) | Bit/dim 3.6681(3.6333) | Xent 0.5413(0.5171) | Loss 56.2460(57.3753) | Error 0.1822(0.1824) Steps 0(0.00) | Grad Norm 59.5021(43.3262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 94.7141, Epoch Time 1123.9358(1084.5473), Bit/dim 3.6395(best: 3.6328), Xent 0.6879, Loss 3.9835, Error 0.2311(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 21.7747(18.5281) | Bit/dim 3.6359(3.6371) | Xent 0.5300(0.5179) | Loss 56.1509(63.0308) | Error 0.1844(0.1834) Steps 0(0.00) | Grad Norm 60.2574(45.4443) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 18.2098(18.5317) | Bit/dim 3.6282(3.6375) | Xent 0.4458(0.5122) | Loss 55.4270(61.0482) | Error 0.1633(0.1815) Steps 0(0.00) | Grad Norm 25.8819(42.9929) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 17.6372(18.6833) | Bit/dim 3.6240(3.6352) | Xent 0.4498(0.5058) | Loss 55.0236(59.4906) | Error 0.1478(0.1794) Steps 0(0.00) | Grad Norm 47.2183(42.3151) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 18.4919(18.8125) | Bit/dim 3.6196(3.6318) | Xent 0.5617(0.5059) | Loss 55.9128(58.4487) | Error 0.2067(0.1800) Steps 0(0.00) | Grad Norm 81.6735(44.1515) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 18.9466(18.7029) | Bit/dim 3.6472(3.6316) | Xent 0.5146(0.5155) | Loss 56.3226(57.7981) | Error 0.1856(0.1827) Steps 0(0.00) | Grad Norm 42.6091(44.9637) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 94.5746, Epoch Time 1146.0982(1086.3939), Bit/dim 3.6367(best: 3.6328), Xent 0.6829, Loss 3.9782, Error 0.2295(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 18.8136(18.7942) | Bit/dim 3.6310(3.6319) | Xent 0.4410(0.5098) | Loss 55.8334(63.9813) | Error 0.1689(0.1813) Steps 0(0.00) | Grad Norm 28.1081(44.9798) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.7383(18.6883) | Bit/dim 3.6308(3.6312) | Xent 0.5148(0.5048) | Loss 54.6912(61.5728) | Error 0.1844(0.1796) Steps 0(0.00) | Grad Norm 40.2546(41.5342) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 17.2727(18.7804) | Bit/dim 3.6506(3.6334) | Xent 0.4732(0.5016) | Loss 55.1567(60.0744) | Error 0.1822(0.1789) Steps 0(0.00) | Grad Norm 38.2500(41.6225) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.7255(18.6937) | Bit/dim 3.6161(3.6294) | Xent 0.5397(0.5001) | Loss 57.0753(58.9009) | Error 0.1822(0.1783) Steps 0(0.00) | Grad Norm 51.0371(42.5922) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 18.3195(18.6524) | Bit/dim 3.6131(3.6301) | Xent 0.5039(0.5013) | Loss 56.2295(58.1401) | Error 0.1722(0.1788) Steps 0(0.00) | Grad Norm 40.2032(43.3078) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 16.8166(18.6133) | Bit/dim 3.6484(3.6327) | Xent 0.4721(0.5049) | Loss 52.0852(57.4674) | Error 0.1544(0.1792) Steps 0(0.00) | Grad Norm 42.6072(43.9106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 92.4259, Epoch Time 1138.4236(1087.9548), Bit/dim 3.6462(best: 3.6328), Xent 0.6608, Loss 3.9766, Error 0.2226(best: 0.2271)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.7834(18.6419) | Bit/dim 3.6354(3.6341) | Xent 0.4636(0.5017) | Loss 55.6079(62.8084) | Error 0.1667(0.1787) Steps 0(0.00) | Grad Norm 39.2767(43.7419) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 17.0467(18.5139) | Bit/dim 3.6273(3.6346) | Xent 0.4448(0.4999) | Loss 54.4568(60.7414) | Error 0.1644(0.1771) Steps 0(0.00) | Grad Norm 34.3356(43.8892) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 18.4510(18.4687) | Bit/dim 3.6330(3.6335) | Xent 0.5001(0.4950) | Loss 55.1515(59.2452) | Error 0.1811(0.1750) Steps 0(0.00) | Grad Norm 27.5311(44.3754) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 18.3709(18.6055) | Bit/dim 3.6104(3.6316) | Xent 0.4824(0.4902) | Loss 54.9295(58.3312) | Error 0.1756(0.1738) Steps 0(0.00) | Grad Norm 34.7588(42.7815) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.8688(18.6294) | Bit/dim 3.6168(3.6293) | Xent 0.5173(0.4998) | Loss 56.7191(57.4822) | Error 0.1911(0.1767) Steps 0(0.00) | Grad Norm 34.3461(42.7194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 93.0329, Epoch Time 1131.1625(1089.2510), Bit/dim 3.6345(best: 3.6328), Xent 0.6591, Loss 3.9641, Error 0.2254(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 20.3725(18.6041) | Bit/dim 3.6336(3.6280) | Xent 0.4914(0.4984) | Loss 56.5008(63.9814) | Error 0.1733(0.1773) Steps 0(0.00) | Grad Norm 30.7572(41.2431) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 18.0184(18.6827) | Bit/dim 3.6122(3.6282) | Xent 0.4930(0.4996) | Loss 54.5746(61.8147) | Error 0.1678(0.1782) Steps 0(0.00) | Grad Norm 47.6221(44.5132) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 18.6535(18.6176) | Bit/dim 3.6293(3.6310) | Xent 0.5092(0.4942) | Loss 57.0327(60.1570) | Error 0.1822(0.1757) Steps 0(0.00) | Grad Norm 31.3312(41.0205) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 17.7327(18.5740) | Bit/dim 3.6204(3.6293) | Xent 0.4408(0.4979) | Loss 55.6953(58.9619) | Error 0.1522(0.1762) Steps 0(0.00) | Grad Norm 36.4309(40.7494) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 21.7239(18.7427) | Bit/dim 3.6216(3.6287) | Xent 0.4318(0.4904) | Loss 52.9984(57.8571) | Error 0.1500(0.1739) Steps 0(0.00) | Grad Norm 29.0654(38.7162) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 18.7969(18.7622) | Bit/dim 3.6412(3.6292) | Xent 0.5277(0.4881) | Loss 55.1914(57.1110) | Error 0.1711(0.1720) Steps 0(0.00) | Grad Norm 27.4345(38.2202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 93.6648, Epoch Time 1145.5540(1090.9401), Bit/dim 3.6301(best: 3.6328), Xent 0.6908, Loss 3.9755, Error 0.2286(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 18.1588(18.6585) | Bit/dim 3.6578(3.6312) | Xent 0.4807(0.4875) | Loss 55.1759(62.5694) | Error 0.1644(0.1718) Steps 0(0.00) | Grad Norm 37.9192(38.4763) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 19.0925(18.5761) | Bit/dim 3.6431(3.6297) | Xent 0.5064(0.4875) | Loss 55.2960(60.6824) | Error 0.1833(0.1720) Steps 0(0.00) | Grad Norm 31.0110(38.8391) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 17.7048(18.5954) | Bit/dim 3.6268(3.6307) | Xent 0.4851(0.4847) | Loss 55.1606(59.4041) | Error 0.1756(0.1717) Steps 0(0.00) | Grad Norm 35.7865(38.9435) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 19.2539(18.7526) | Bit/dim 3.6166(3.6271) | Xent 0.4775(0.4847) | Loss 55.7621(58.2468) | Error 0.1678(0.1718) Steps 0(0.00) | Grad Norm 51.9348(39.8557) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 18.5334(18.7202) | Bit/dim 3.6396(3.6275) | Xent 0.4855(0.4853) | Loss 55.2244(57.3401) | Error 0.1744(0.1733) Steps 0(0.00) | Grad Norm 60.6051(40.9441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 94.8724, Epoch Time 1139.6586(1092.4016), Bit/dim 3.6414(best: 3.6301), Xent 0.6813, Loss 3.9821, Error 0.2289(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 21.1505(18.8373) | Bit/dim 3.6363(3.6303) | Xent 0.4256(0.4788) | Loss 54.5499(63.8713) | Error 0.1633(0.1705) Steps 0(0.00) | Grad Norm 29.6571(42.4352) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 18.2149(18.8054) | Bit/dim 3.6120(3.6295) | Xent 0.4154(0.4777) | Loss 53.8103(61.5269) | Error 0.1478(0.1695) Steps 0(0.00) | Grad Norm 39.4584(42.4024) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 19.9565(18.8505) | Bit/dim 3.6187(3.6312) | Xent 0.4989(0.4693) | Loss 55.7795(59.7546) | Error 0.1644(0.1662) Steps 0(0.00) | Grad Norm 42.2915(41.6689) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 17.5436(18.5655) | Bit/dim 3.6172(3.6340) | Xent 0.5129(0.4773) | Loss 56.2566(58.6188) | Error 0.1756(0.1704) Steps 0(0.00) | Grad Norm 41.3792(44.6280) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 19.9992(18.6640) | Bit/dim 3.6051(3.6301) | Xent 0.4950(0.4833) | Loss 56.1045(57.9541) | Error 0.1544(0.1717) Steps 0(0.00) | Grad Norm 45.2637(44.1062) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 17.6528(18.6824) | Bit/dim 3.5924(3.6288) | Xent 0.4627(0.4930) | Loss 54.3089(57.3011) | Error 0.1600(0.1752) Steps 0(0.00) | Grad Norm 29.7454(45.9985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 95.5982, Epoch Time 1141.3541(1093.8702), Bit/dim 3.6280(best: 3.6301), Xent 0.6731, Loss 3.9646, Error 0.2327(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 17.8351(18.6958) | Bit/dim 3.6223(3.6295) | Xent 0.4559(0.4889) | Loss 55.2156(62.6744) | Error 0.1556(0.1735) Steps 0(0.00) | Grad Norm 38.1930(42.7197) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 19.3841(18.6912) | Bit/dim 3.6134(3.6282) | Xent 0.4612(0.4806) | Loss 55.5286(60.6651) | Error 0.1678(0.1718) Steps 0(0.00) | Grad Norm 26.8860(40.0379) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 17.7583(18.6736) | Bit/dim 3.6064(3.6274) | Xent 0.4804(0.4772) | Loss 53.7621(59.1397) | Error 0.1678(0.1700) Steps 0(0.00) | Grad Norm 59.3094(40.9304) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 17.5583(18.6526) | Bit/dim 3.6514(3.6267) | Xent 0.4923(0.4812) | Loss 53.6274(58.1814) | Error 0.1700(0.1712) Steps 0(0.00) | Grad Norm 74.4507(44.4155) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 18.0362(18.7526) | Bit/dim 3.6239(3.6257) | Xent 0.5519(0.4845) | Loss 54.1507(57.4084) | Error 0.1944(0.1716) Steps 0(0.00) | Grad Norm 36.0799(45.0159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 94.1424, Epoch Time 1146.7139(1095.4555), Bit/dim 3.6293(best: 3.6280), Xent 0.6701, Loss 3.9643, Error 0.2265(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 18.2638(18.9239) | Bit/dim 3.6276(3.6275) | Xent 0.4585(0.4774) | Loss 53.7931(63.6894) | Error 0.1611(0.1697) Steps 0(0.00) | Grad Norm 58.9324(45.4328) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 18.7428(18.8330) | Bit/dim 3.6217(3.6281) | Xent 0.5472(0.4810) | Loss 55.5518(61.6349) | Error 0.1911(0.1703) Steps 0(0.00) | Grad Norm 29.4100(46.0725) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 18.8433(18.8729) | Bit/dim 3.5993(3.6252) | Xent 0.4958(0.4777) | Loss 56.4892(59.9997) | Error 0.1789(0.1698) Steps 0(0.00) | Grad Norm 38.5542(43.6745) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 19.5031(18.9584) | Bit/dim 3.6356(3.6262) | Xent 0.4669(0.4688) | Loss 56.7760(58.8318) | Error 0.1700(0.1663) Steps 0(0.00) | Grad Norm 37.0706(42.1549) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 17.3588(18.7452) | Bit/dim 3.6165(3.6251) | Xent 0.5383(0.4800) | Loss 54.7300(57.7198) | Error 0.1967(0.1711) Steps 0(0.00) | Grad Norm 58.0265(44.3736) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 20.9072(18.7098) | Bit/dim 3.6388(3.6260) | Xent 0.4735(0.4760) | Loss 56.5336(57.0486) | Error 0.1656(0.1688) Steps 0(0.00) | Grad Norm 30.4680(44.1673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 92.0666, Epoch Time 1141.9073(1096.8491), Bit/dim 3.6373(best: 3.6280), Xent 0.6770, Loss 3.9758, Error 0.2220(best: 0.2226)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 18.9759(18.6771) | Bit/dim 3.5992(3.6233) | Xent 0.4675(0.4725) | Loss 55.3820(62.0201) | Error 0.1789(0.1672) Steps 0(0.00) | Grad Norm 32.6243(43.4805) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 21.4260(18.8493) | Bit/dim 3.6566(3.6255) | Xent 0.5504(0.4715) | Loss 56.7912(60.2348) | Error 0.1956(0.1674) Steps 0(0.00) | Grad Norm 48.3873(42.5614) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 20.9265(18.8086) | Bit/dim 3.6352(3.6276) | Xent 0.4645(0.4725) | Loss 57.1640(59.1291) | Error 0.1589(0.1667) Steps 0(0.00) | Grad Norm 25.5080(40.6680) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 19.0499(18.8785) | Bit/dim 3.6504(3.6247) | Xent 0.4520(0.4686) | Loss 56.4967(58.0548) | Error 0.1500(0.1652) Steps 0(0.00) | Grad Norm 60.4011(41.6816) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 18.9165(18.7854) | Bit/dim 3.6103(3.6267) | Xent 0.4305(0.4725) | Loss 54.5046(57.3801) | Error 0.1544(0.1678) Steps 0(0.00) | Grad Norm 64.5480(43.1813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 91.9823, Epoch Time 1143.4286(1098.2465), Bit/dim 3.6276(best: 3.6280), Xent 0.6581, Loss 3.9567, Error 0.2221(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 18.9347(18.6394) | Bit/dim 3.6239(3.6284) | Xent 0.3928(0.4672) | Loss 52.8023(63.7952) | Error 0.1378(0.1654) Steps 0(0.00) | Grad Norm 28.8274(41.7971) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 18.7169(18.6259) | Bit/dim 3.6178(3.6271) | Xent 0.4360(0.4601) | Loss 56.0837(61.4786) | Error 0.1622(0.1623) Steps 0(0.00) | Grad Norm 51.2650(41.6086) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 21.4858(18.9329) | Bit/dim 3.6089(3.6231) | Xent 0.4559(0.4549) | Loss 56.5374(59.8606) | Error 0.1678(0.1603) Steps 0(0.00) | Grad Norm 38.5691(39.5839) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 19.8279(18.9793) | Bit/dim 3.6415(3.6225) | Xent 0.4513(0.4514) | Loss 54.9346(58.4655) | Error 0.1622(0.1600) Steps 0(0.00) | Grad Norm 32.9399(39.0673) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 19.0213(18.9921) | Bit/dim 3.6496(3.6252) | Xent 0.4667(0.4608) | Loss 55.4725(57.6929) | Error 0.1678(0.1638) Steps 0(0.00) | Grad Norm 42.2028(40.9988) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 18.4020(18.8540) | Bit/dim 3.6356(3.6265) | Xent 0.5178(0.4720) | Loss 55.7267(57.0283) | Error 0.1778(0.1682) Steps 0(0.00) | Grad Norm 71.8162(45.4400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 94.0956, Epoch Time 1151.7727(1099.8522), Bit/dim 3.6347(best: 3.6276), Xent 0.6804, Loss 3.9749, Error 0.2304(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 18.0382(18.8960) | Bit/dim 3.6307(3.6277) | Xent 0.4786(0.4701) | Loss 54.5627(62.4900) | Error 0.1678(0.1687) Steps 0(0.00) | Grad Norm 37.6442(44.9404) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 20.1613(18.9458) | Bit/dim 3.6158(3.6242) | Xent 0.4512(0.4671) | Loss 57.4755(60.4985) | Error 0.1644(0.1671) Steps 0(0.00) | Grad Norm 32.9147(42.8173) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 17.6033(19.0333) | Bit/dim 3.6429(3.6274) | Xent 0.4649(0.4619) | Loss 53.9983(59.2254) | Error 0.1656(0.1652) Steps 0(0.00) | Grad Norm 43.7003(41.1027) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 18.2736(19.0049) | Bit/dim 3.6110(3.6251) | Xent 0.4954(0.4590) | Loss 51.9524(58.1372) | Error 0.1756(0.1628) Steps 0(0.00) | Grad Norm 56.4462(41.0867) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 20.0212(18.9315) | Bit/dim 3.6170(3.6267) | Xent 0.4626(0.4675) | Loss 53.3153(57.3369) | Error 0.1633(0.1663) Steps 0(0.00) | Grad Norm 28.1409(42.8348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 93.9198, Epoch Time 1155.7535(1101.5293), Bit/dim 3.6267(best: 3.6276), Xent 0.6874, Loss 3.9704, Error 0.2251(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7330 | Time 20.7085(18.9816) | Bit/dim 3.6342(3.6270) | Xent 0.4366(0.4676) | Loss 57.4871(61.4346) | Error 0.1556(0.1687) Steps 0(0.00) | Grad Norm 32.1578(44.4114) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 17.7530(18.8327) | Bit/dim 3.6148(3.6243) | Xent 0.4386(0.4599) | Loss 55.0080(59.8185) | Error 0.1522(0.1652) Steps 0(0.00) | Grad Norm 40.0241(41.9846) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 23.7046(19.0651) | Bit/dim 3.5908(3.6234) | Xent 0.4330(0.4549) | Loss 55.6555(58.5122) | Error 0.1522(0.1625) Steps 0(0.00) | Grad Norm 41.7218(40.6848) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 19.2287(18.8673) | Bit/dim 3.6470(3.6260) | Xent 0.4832(0.4560) | Loss 54.2318(57.5769) | Error 0.1611(0.1623) Steps 0(0.00) | Grad Norm 41.8733(40.0560) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 18.6697(18.7585) | Bit/dim 3.5877(3.6256) | Xent 0.4770(0.4568) | Loss 53.2864(56.8017) | Error 0.1578(0.1614) Steps 0(0.00) | Grad Norm 30.8000(40.1139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 95.2302, Epoch Time 1147.7391(1102.9156), Bit/dim 3.6325(best: 3.6267), Xent 0.6689, Loss 3.9669, Error 0.2256(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 17.8584(18.8187) | Bit/dim 3.6172(3.6232) | Xent 0.4250(0.4474) | Loss 54.7542(62.2803) | Error 0.1389(0.1568) Steps 0(0.00) | Grad Norm 45.6537(39.1072) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 20.8328(18.9132) | Bit/dim 3.6020(3.6195) | Xent 0.3778(0.4432) | Loss 55.0992(60.4567) | Error 0.1333(0.1552) Steps 0(0.00) | Grad Norm 37.2800(39.5221) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 18.1302(18.9095) | Bit/dim 3.6324(3.6165) | Xent 0.4137(0.4417) | Loss 54.9101(58.9595) | Error 0.1433(0.1547) Steps 0(0.00) | Grad Norm 37.0258(38.9589) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 18.9942(18.8446) | Bit/dim 3.6484(3.6209) | Xent 0.4331(0.4436) | Loss 55.6099(57.7315) | Error 0.1578(0.1567) Steps 0(0.00) | Grad Norm 65.4180(39.3522) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 16.5373(18.7458) | Bit/dim 3.6434(3.6239) | Xent 0.4191(0.4469) | Loss 52.3853(56.9264) | Error 0.1522(0.1567) Steps 0(0.00) | Grad Norm 34.7884(40.2051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 93.0745, Epoch Time 1150.4410(1104.3413), Bit/dim 3.6318(best: 3.6267), Xent 0.7261, Loss 3.9949, Error 0.2365(best: 0.2220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 21.0399(18.9455) | Bit/dim 3.6087(3.6234) | Xent 0.5615(0.4557) | Loss 54.4759(63.5871) | Error 0.1978(0.1598) Steps 0(0.00) | Grad Norm 89.5064(44.0292) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 18.3475(18.8532) | Bit/dim 3.6513(3.6237) | Xent 0.4314(0.4606) | Loss 55.5062(61.3246) | Error 0.1533(0.1618) Steps 0(0.00) | Grad Norm 40.5772(46.4391) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 18.7080(18.6898) | Bit/dim 3.6494(3.6246) | Xent 0.3924(0.4536) | Loss 53.7657(59.7194) | Error 0.1311(0.1595) Steps 0(0.00) | Grad Norm 23.6778(43.3853) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 20.5720(18.7233) | Bit/dim 3.5859(3.6229) | Xent 0.3995(0.4456) | Loss 55.2791(58.2329) | Error 0.1478(0.1568) Steps 0(0.00) | Grad Norm 27.3763(40.8141) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_1_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 0.1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
