{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_lr_0_01_run1/epoch_100_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_lr_0_01_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0601 | Time 139.7615(86.0109) | Bit/dim 3.8874(3.9366) | Xent 1.2753(1.4161) | Loss 4.5250(4.6447) | Error 0.4596(0.5027) Steps 982(982.83) | Grad Norm 2.6125(4.0858) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 83.8923(85.9474) | Bit/dim 3.8778(3.9349) | Xent 1.3019(1.4127) | Loss 4.5288(4.6412) | Error 0.4644(0.5016) Steps 988(982.99) | Grad Norm 2.0744(4.0255) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 85.6349(85.9380) | Bit/dim 3.8646(3.9328) | Xent 1.2601(1.4081) | Loss 4.4946(4.6368) | Error 0.4463(0.4999) Steps 970(982.60) | Grad Norm 1.4749(3.9489) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 84.5213(85.8955) | Bit/dim 3.8655(3.9308) | Xent 1.2896(1.4045) | Loss 4.5103(4.6330) | Error 0.4513(0.4985) Steps 958(981.86) | Grad Norm 0.9016(3.8575) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 86.8892(85.9253) | Bit/dim 3.8576(3.9286) | Xent 1.2857(1.4010) | Loss 4.5005(4.6290) | Error 0.4597(0.4973) Steps 958(981.15) | Grad Norm 1.5088(3.7871) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 80.7257(85.7693) | Bit/dim 3.8641(3.9266) | Xent 1.2870(1.3976) | Loss 4.5076(4.6254) | Error 0.4523(0.4960) Steps 958(980.45) | Grad Norm 1.5838(3.7210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 47.9411, Epoch Time 625.1721(506.7273), Bit/dim 3.8584(best: inf), Xent 1.2882, Loss 4.5025, Error 0.4607(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 98.0694(86.1383) | Bit/dim 3.8549(3.9245) | Xent 1.2841(1.3942) | Loss 4.4970(4.6215) | Error 0.4579(0.4948) Steps 994(980.86) | Grad Norm 1.5062(3.6545) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 85.0112(86.1045) | Bit/dim 3.8540(3.9224) | Xent 1.2848(1.3909) | Loss 4.4964(4.6178) | Error 0.4503(0.4935) Steps 970(980.53) | Grad Norm 0.9770(3.5742) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 88.2496(86.1689) | Bit/dim 3.8595(3.9205) | Xent 1.2572(1.3869) | Loss 4.4881(4.6139) | Error 0.4474(0.4921) Steps 970(980.22) | Grad Norm 0.8405(3.4922) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 83.8302(86.0987) | Bit/dim 3.8564(3.9186) | Xent 1.2815(1.3837) | Loss 4.4972(4.6104) | Error 0.4554(0.4910) Steps 982(980.27) | Grad Norm 1.0354(3.4185) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 87.2381(86.1329) | Bit/dim 3.8482(3.9164) | Xent 1.2723(1.3804) | Loss 4.4843(4.6066) | Error 0.4527(0.4898) Steps 958(979.60) | Grad Norm 1.2117(3.3523) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 85.6527(86.1185) | Bit/dim 3.8489(3.9144) | Xent 1.2614(1.3768) | Loss 4.4796(4.6028) | Error 0.4487(0.4886) Steps 976(979.49) | Grad Norm 1.1471(3.2861) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 33.1372, Epoch Time 577.3669(508.8465), Bit/dim 3.8503(best: 3.8584), Xent 1.2764, Loss 4.4885, Error 0.4559(best: 0.4607)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 79.3511(85.9155) | Bit/dim 3.8430(3.9123) | Xent 1.2641(1.3734) | Loss 4.4750(4.5990) | Error 0.4493(0.4874) Steps 970(979.21) | Grad Norm 0.7006(3.2086) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 85.4678(85.9020) | Bit/dim 3.8384(3.9101) | Xent 1.2663(1.3702) | Loss 4.4715(4.5952) | Error 0.4514(0.4864) Steps 970(978.93) | Grad Norm 0.6643(3.1322) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 83.8134(85.8394) | Bit/dim 3.8512(3.9083) | Xent 1.2656(1.3671) | Loss 4.4840(4.5918) | Error 0.4523(0.4853) Steps 982(979.02) | Grad Norm 1.0442(3.0696) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 85.6198(85.8328) | Bit/dim 3.8393(3.9062) | Xent 1.2970(1.3650) | Loss 4.4878(4.5887) | Error 0.4565(0.4845) Steps 988(979.29) | Grad Norm 0.8082(3.0018) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 87.4497(85.8813) | Bit/dim 3.8479(3.9045) | Xent 1.2537(1.3616) | Loss 4.4748(4.5853) | Error 0.4431(0.4832) Steps 958(978.65) | Grad Norm 0.6732(2.9319) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 84.4901(85.8396) | Bit/dim 3.8586(3.9031) | Xent 1.2518(1.3583) | Loss 4.4845(4.5823) | Error 0.4425(0.4820) Steps 970(978.40) | Grad Norm 0.6739(2.8642) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 32.8546, Epoch Time 554.4182(510.2136), Bit/dim 3.8438(best: 3.8503), Xent 1.2722, Loss 4.4799, Error 0.4572(best: 0.4559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 85.1327(85.8184) | Bit/dim 3.8502(3.9015) | Xent 1.2617(1.3554) | Loss 4.4810(4.5792) | Error 0.4473(0.4810) Steps 1000(979.04) | Grad Norm 0.6762(2.7985) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 86.2206(85.8304) | Bit/dim 3.8231(3.8992) | Xent 1.2582(1.3525) | Loss 4.4522(4.5754) | Error 0.4375(0.4797) Steps 994(979.49) | Grad Norm 0.6183(2.7331) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 83.8929(85.7723) | Bit/dim 3.8304(3.8971) | Xent 1.2583(1.3497) | Loss 4.4595(4.5719) | Error 0.4463(0.4787) Steps 994(979.93) | Grad Norm 0.6099(2.6694) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 85.5086(85.7644) | Bit/dim 3.8482(3.8956) | Xent 1.2487(1.3467) | Loss 4.4725(4.5690) | Error 0.4476(0.4777) Steps 994(980.35) | Grad Norm 0.5563(2.6060) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 84.2055(85.7176) | Bit/dim 3.8410(3.8940) | Xent 1.2555(1.3439) | Loss 4.4687(4.5659) | Error 0.4429(0.4767) Steps 958(979.68) | Grad Norm 0.5785(2.5452) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 85.5469(85.7125) | Bit/dim 3.8523(3.8927) | Xent 1.2624(1.3415) | Loss 4.4835(4.5635) | Error 0.4511(0.4759) Steps 994(980.11) | Grad Norm 0.7272(2.4907) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 32.6482, Epoch Time 559.0390(511.6784), Bit/dim 3.8408(best: 3.8438), Xent 1.2698, Loss 4.4756, Error 0.4544(best: 0.4559)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 83.0370(85.6322) | Bit/dim 3.8357(3.8910) | Xent 1.2330(1.3382) | Loss 4.4522(4.5601) | Error 0.4366(0.4747) Steps 982(980.17) | Grad Norm 0.6024(2.4340) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 83.9930(85.5831) | Bit/dim 3.8464(3.8897) | Xent 1.2722(1.3362) | Loss 4.4825(4.5578) | Error 0.4513(0.4740) Steps 988(980.40) | Grad Norm 0.5262(2.3768) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 84.6234(85.5543) | Bit/dim 3.8447(3.8883) | Xent 1.2452(1.3335) | Loss 4.4673(4.5551) | Error 0.4439(0.4731) Steps 958(979.73) | Grad Norm 0.4355(2.3185) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 84.6343(85.5267) | Bit/dim 3.8427(3.8870) | Xent 1.2490(1.3310) | Loss 4.4672(4.5525) | Error 0.4464(0.4723) Steps 994(980.16) | Grad Norm 0.5841(2.2665) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 84.8227(85.5055) | Bit/dim 3.8268(3.8852) | Xent 1.2346(1.3281) | Loss 4.4441(4.5492) | Error 0.4340(0.4712) Steps 976(980.03) | Grad Norm 0.4028(2.2106) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 84.1606(85.4652) | Bit/dim 3.8300(3.8835) | Xent 1.2438(1.3255) | Loss 4.4519(4.5463) | Error 0.4424(0.4703) Steps 988(980.27) | Grad Norm 0.4166(2.1568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 32.8939, Epoch Time 553.7640(512.9410), Bit/dim 3.8368(best: 3.8408), Xent 1.2629, Loss 4.4683, Error 0.4533(best: 0.4544)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 81.9610(85.3601) | Bit/dim 3.8446(3.8823) | Xent 1.2492(1.3233) | Loss 4.4692(4.5440) | Error 0.4414(0.4694) Steps 976(980.14) | Grad Norm 0.4445(2.1054) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 82.4824(85.2737) | Bit/dim 3.8459(3.8812) | Xent 1.2467(1.3210) | Loss 4.4692(4.5417) | Error 0.4513(0.4689) Steps 976(980.02) | Grad Norm 0.4525(2.0558) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 82.0906(85.1782) | Bit/dim 3.8354(3.8799) | Xent 1.2465(1.3187) | Loss 4.4586(4.5392) | Error 0.4489(0.4683) Steps 970(979.72) | Grad Norm 0.6076(2.0124) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 81.4186(85.0655) | Bit/dim 3.8288(3.8783) | Xent 1.2481(1.3166) | Loss 4.4528(4.5366) | Error 0.4449(0.4676) Steps 976(979.61) | Grad Norm 0.4749(1.9662) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 83.9913(85.0332) | Bit/dim 3.8274(3.8768) | Xent 1.2426(1.3144) | Loss 4.4487(4.5340) | Error 0.4330(0.4666) Steps 970(979.32) | Grad Norm 0.4907(1.9220) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 84.6688(85.0223) | Bit/dim 3.8249(3.8752) | Xent 1.2358(1.3120) | Loss 4.4428(4.5313) | Error 0.4427(0.4658) Steps 976(979.22) | Grad Norm 0.4433(1.8776) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 32.8518, Epoch Time 545.4875(513.9174), Bit/dim 3.8343(best: 3.8368), Xent 1.2605, Loss 4.4646, Error 0.4523(best: 0.4533)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 81.7341(84.9237) | Bit/dim 3.8429(3.8743) | Xent 1.2420(1.3099) | Loss 4.4639(4.5292) | Error 0.4383(0.4650) Steps 970(978.94) | Grad Norm 0.4988(1.8363) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 82.9090(84.8632) | Bit/dim 3.8336(3.8731) | Xent 1.2352(1.3077) | Loss 4.4512(4.5269) | Error 0.4399(0.4643) Steps 976(978.85) | Grad Norm 0.4738(1.7954) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 84.4739(84.8515) | Bit/dim 3.8344(3.8719) | Xent 1.2281(1.3053) | Loss 4.4484(4.5245) | Error 0.4365(0.4634) Steps 982(978.95) | Grad Norm 0.4436(1.7548) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 85.4124(84.8684) | Bit/dim 3.8321(3.8707) | Xent 1.2351(1.3032) | Loss 4.4496(4.5223) | Error 0.4410(0.4628) Steps 994(979.40) | Grad Norm 0.3675(1.7132) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 83.5350(84.8284) | Bit/dim 3.8243(3.8693) | Xent 1.2334(1.3011) | Loss 4.4410(4.5199) | Error 0.4394(0.4621) Steps 964(978.94) | Grad Norm 0.3584(1.6726) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 83.5110(84.7888) | Bit/dim 3.8330(3.8682) | Xent 1.2267(1.2989) | Loss 4.4463(4.5177) | Error 0.4383(0.4613) Steps 982(979.03) | Grad Norm 0.4771(1.6367) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 32.2420, Epoch Time 549.4339(514.9829), Bit/dim 3.8314(best: 3.8343), Xent 1.2557, Loss 4.4593, Error 0.4493(best: 0.4523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 84.4203(84.7778) | Bit/dim 3.8317(3.8671) | Xent 1.2359(1.2970) | Loss 4.4497(4.5156) | Error 0.4409(0.4607) Steps 976(978.94) | Grad Norm 0.3990(1.5996) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 84.5380(84.7706) | Bit/dim 3.8279(3.8660) | Xent 1.2236(1.2948) | Loss 4.4397(4.5133) | Error 0.4329(0.4599) Steps 964(978.49) | Grad Norm 0.3747(1.5628) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 82.0634(84.6894) | Bit/dim 3.8281(3.8648) | Xent 1.2149(1.2924) | Loss 4.4355(4.5110) | Error 0.4346(0.4591) Steps 970(978.24) | Grad Norm 0.4248(1.5287) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 83.3275(84.6485) | Bit/dim 3.8314(3.8638) | Xent 1.2333(1.2906) | Loss 4.4480(4.5091) | Error 0.4284(0.4582) Steps 982(978.35) | Grad Norm 0.4915(1.4976) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 84.2137(84.6355) | Bit/dim 3.8322(3.8629) | Xent 1.2328(1.2889) | Loss 4.4486(4.5073) | Error 0.4416(0.4577) Steps 982(978.46) | Grad Norm 0.4728(1.4668) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 82.6442(84.5757) | Bit/dim 3.8311(3.8619) | Xent 1.2273(1.2870) | Loss 4.4447(4.5054) | Error 0.4373(0.4571) Steps 958(977.84) | Grad Norm 0.3626(1.4337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 32.7753, Epoch Time 549.8876(516.0300), Bit/dim 3.8307(best: 3.8314), Xent 1.2498, Loss 4.4556, Error 0.4484(best: 0.4493)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 82.2157(84.5049) | Bit/dim 3.8221(3.8607) | Xent 1.2115(1.2848) | Loss 4.4279(4.5031) | Error 0.4250(0.4561) Steps 988(978.15) | Grad Norm 0.4131(1.4031) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 82.9846(84.4593) | Bit/dim 3.8290(3.8598) | Xent 1.2332(1.2832) | Loss 4.4456(4.5014) | Error 0.4421(0.4557) Steps 964(977.72) | Grad Norm 0.4218(1.3736) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 86.7468(84.5280) | Bit/dim 3.8296(3.8589) | Xent 1.2173(1.2812) | Loss 4.4383(4.4995) | Error 0.4283(0.4549) Steps 988(978.03) | Grad Norm 0.4038(1.3446) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 83.3895(84.4938) | Bit/dim 3.8312(3.8580) | Xent 1.2346(1.2798) | Loss 4.4485(4.4979) | Error 0.4383(0.4544) Steps 964(977.61) | Grad Norm 0.3933(1.3160) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 82.9436(84.4473) | Bit/dim 3.8294(3.8572) | Xent 1.2361(1.2785) | Loss 4.4474(4.4964) | Error 0.4403(0.4540) Steps 964(977.20) | Grad Norm 0.4190(1.2891) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 83.3439(84.4142) | Bit/dim 3.8331(3.8564) | Xent 1.2115(1.2765) | Loss 4.4388(4.4947) | Error 0.4237(0.4531) Steps 958(976.63) | Grad Norm 0.3603(1.2612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 32.1609, Epoch Time 549.1524(517.0237), Bit/dim 3.8289(best: 3.8307), Xent 1.2487, Loss 4.4532, Error 0.4478(best: 0.4484)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 85.4250(84.4445) | Bit/dim 3.8291(3.8556) | Xent 1.2235(1.2749) | Loss 4.4408(4.4931) | Error 0.4384(0.4526) Steps 976(976.61) | Grad Norm 0.4699(1.2375) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 78.8762(84.2775) | Bit/dim 3.8246(3.8547) | Xent 1.2417(1.2739) | Loss 4.4455(4.4917) | Error 0.4374(0.4522) Steps 958(976.05) | Grad Norm 0.4408(1.2136) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 82.4159(84.2216) | Bit/dim 3.8228(3.8537) | Xent 1.2281(1.2725) | Loss 4.4369(4.4900) | Error 0.4424(0.4519) Steps 982(976.23) | Grad Norm 0.5357(1.1933) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 82.7204(84.1766) | Bit/dim 3.8309(3.8531) | Xent 1.1993(1.2703) | Loss 4.4305(4.4882) | Error 0.4294(0.4512) Steps 964(975.86) | Grad Norm 0.3464(1.1679) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 83.4986(84.1562) | Bit/dim 3.8341(3.8525) | Xent 1.2195(1.2688) | Loss 4.4438(4.4869) | Error 0.4295(0.4505) Steps 970(975.69) | Grad Norm 0.4294(1.1457) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 85.5582(84.1983) | Bit/dim 3.8187(3.8515) | Xent 1.2509(1.2683) | Loss 4.4442(4.4856) | Error 0.4464(0.4504) Steps 952(974.98) | Grad Norm 0.4076(1.1236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 31.7510, Epoch Time 545.7498(517.8855), Bit/dim 3.8274(best: 3.8289), Xent 1.2440, Loss 4.4494, Error 0.4476(best: 0.4478)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 82.8142(84.1568) | Bit/dim 3.8277(3.8508) | Xent 1.2438(1.2676) | Loss 4.4496(4.4845) | Error 0.4336(0.4499) Steps 946(974.11) | Grad Norm 0.3918(1.1016) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 81.6488(84.0815) | Bit/dim 3.8282(3.8501) | Xent 1.2418(1.2668) | Loss 4.4491(4.4835) | Error 0.4387(0.4496) Steps 970(973.98) | Grad Norm 0.4018(1.0806) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 82.9736(84.0483) | Bit/dim 3.8294(3.8495) | Xent 1.2092(1.2651) | Loss 4.4340(4.4820) | Error 0.4303(0.4490) Steps 952(973.32) | Grad Norm 0.3603(1.0590) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 84.0396(84.0480) | Bit/dim 3.8312(3.8489) | Xent 1.2172(1.2636) | Loss 4.4398(4.4807) | Error 0.4330(0.4485) Steps 982(973.58) | Grad Norm 0.3512(1.0378) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 81.6293(83.9755) | Bit/dim 3.8178(3.8480) | Xent 1.2435(1.2630) | Loss 4.4395(4.4795) | Error 0.4461(0.4484) Steps 958(973.12) | Grad Norm 0.3924(1.0184) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 83.9292(83.9741) | Bit/dim 3.8248(3.8473) | Xent 1.2248(1.2619) | Loss 4.4372(4.4782) | Error 0.4361(0.4481) Steps 988(973.56) | Grad Norm 0.5525(1.0044) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 32.2941, Epoch Time 545.4334(518.7119), Bit/dim 3.8264(best: 3.8274), Xent 1.2413, Loss 4.4470, Error 0.4467(best: 0.4476)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 84.3280(83.9847) | Bit/dim 3.8272(3.8467) | Xent 1.2198(1.2606) | Loss 4.4371(4.4770) | Error 0.4346(0.4477) Steps 964(973.28) | Grad Norm 0.4999(0.9893) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 84.5785(84.0025) | Bit/dim 3.8241(3.8460) | Xent 1.2202(1.2594) | Loss 4.4342(4.4757) | Error 0.4325(0.4472) Steps 964(973.00) | Grad Norm 0.3500(0.9701) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 84.7482(84.0249) | Bit/dim 3.8364(3.8457) | Xent 1.2323(1.2586) | Loss 4.4525(4.4750) | Error 0.4364(0.4469) Steps 976(973.09) | Grad Norm 0.4083(0.9533) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 84.9703(84.0533) | Bit/dim 3.8181(3.8449) | Xent 1.2583(1.2586) | Loss 4.4473(4.4742) | Error 0.4456(0.4469) Steps 958(972.64) | Grad Norm 0.5277(0.9405) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 81.2918(83.9704) | Bit/dim 3.8327(3.8445) | Xent 1.2369(1.2579) | Loss 4.4511(4.4735) | Error 0.4429(0.4467) Steps 940(971.66) | Grad Norm 0.3740(0.9235) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 82.2452(83.9187) | Bit/dim 3.8082(3.8434) | Xent 1.2314(1.2571) | Loss 4.4238(4.4720) | Error 0.4440(0.4467) Steps 988(972.15) | Grad Norm 0.4251(0.9085) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 32.4245, Epoch Time 550.6494(519.6700), Bit/dim 3.8258(best: 3.8264), Xent 1.2372, Loss 4.4444, Error 0.4443(best: 0.4467)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 82.8185(83.8856) | Bit/dim 3.8221(3.8428) | Xent 1.2404(1.2566) | Loss 4.4423(4.4711) | Error 0.4375(0.4464) Steps 970(972.08) | Grad Norm 0.4018(0.8933) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 83.1848(83.8646) | Bit/dim 3.8344(3.8425) | Xent 1.2268(1.2557) | Loss 4.4478(4.4704) | Error 0.4361(0.4461) Steps 964(971.84) | Grad Norm 0.3899(0.8782) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 84.1068(83.8719) | Bit/dim 3.8229(3.8419) | Xent 1.2226(1.2547) | Loss 4.4342(4.4693) | Error 0.4400(0.4459) Steps 970(971.78) | Grad Norm 0.4191(0.8645) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 87.3807(83.9772) | Bit/dim 3.8185(3.8412) | Xent 1.2299(1.2540) | Loss 4.4334(4.4682) | Error 0.4410(0.4457) Steps 964(971.55) | Grad Norm 0.4095(0.8508) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 84.8397(84.0030) | Bit/dim 3.8262(3.8408) | Xent 1.2261(1.2532) | Loss 4.4392(4.4674) | Error 0.4344(0.4454) Steps 988(972.04) | Grad Norm 0.6218(0.8439) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 81.0886(83.9156) | Bit/dim 3.8232(3.8403) | Xent 1.2091(1.2518) | Loss 4.4277(4.4662) | Error 0.4366(0.4451) Steps 976(972.16) | Grad Norm 0.4192(0.8312) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 32.3777, Epoch Time 551.5168(520.6254), Bit/dim 3.8247(best: 3.8258), Xent 1.2345, Loss 4.4420, Error 0.4426(best: 0.4443)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 81.7024(83.8492) | Bit/dim 3.8325(3.8400) | Xent 1.2166(1.2508) | Loss 4.4408(4.4654) | Error 0.4314(0.4447) Steps 964(971.92) | Grad Norm 0.3951(0.8181) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 84.3790(83.8651) | Bit/dim 3.8091(3.8391) | Xent 1.2189(1.2498) | Loss 4.4186(4.4640) | Error 0.4244(0.4441) Steps 958(971.50) | Grad Norm 0.4595(0.8074) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 83.9222(83.8668) | Bit/dim 3.8268(3.8387) | Xent 1.2219(1.2490) | Loss 4.4378(4.4632) | Error 0.4367(0.4439) Steps 958(971.10) | Grad Norm 0.4666(0.7971) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 82.5920(83.8286) | Bit/dim 3.8248(3.8383) | Xent 1.2411(1.2487) | Loss 4.4454(4.4627) | Error 0.4501(0.4441) Steps 952(970.52) | Grad Norm 0.4588(0.7870) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 84.9498(83.8622) | Bit/dim 3.8285(3.8380) | Xent 1.2269(1.2481) | Loss 4.4420(4.4621) | Error 0.4326(0.4437) Steps 976(970.69) | Grad Norm 0.5341(0.7794) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 82.2494(83.8138) | Bit/dim 3.8102(3.8372) | Xent 1.2218(1.2473) | Loss 4.4211(4.4608) | Error 0.4294(0.4433) Steps 988(971.21) | Grad Norm 0.5875(0.7736) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 32.1075, Epoch Time 547.7563(521.4393), Bit/dim 3.8223(best: 3.8247), Xent 1.2308, Loss 4.4376, Error 0.4420(best: 0.4426)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 82.3836(83.7709) | Bit/dim 3.8166(3.8366) | Xent 1.2358(1.2470) | Loss 4.4346(4.4600) | Error 0.4355(0.4431) Steps 952(970.63) | Grad Norm 0.5447(0.7668) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 82.2570(83.7255) | Bit/dim 3.8177(3.8360) | Xent 1.2143(1.2460) | Loss 4.4248(4.4590) | Error 0.4321(0.4427) Steps 952(970.07) | Grad Norm 0.4050(0.7559) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 83.9508(83.7322) | Bit/dim 3.8210(3.8356) | Xent 1.2375(1.2457) | Loss 4.4398(4.4584) | Error 0.4390(0.4426) Steps 970(970.07) | Grad Norm 0.5344(0.7493) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 84.2193(83.7469) | Bit/dim 3.8230(3.8352) | Xent 1.2183(1.2449) | Loss 4.4321(4.4576) | Error 0.4310(0.4423) Steps 964(969.89) | Grad Norm 0.4439(0.7401) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 82.1282(83.6983) | Bit/dim 3.8319(3.8351) | Xent 1.1999(1.2435) | Loss 4.4318(4.4569) | Error 0.4273(0.4418) Steps 970(969.89) | Grad Norm 0.3941(0.7297) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 82.5520(83.6639) | Bit/dim 3.8163(3.8345) | Xent 1.2286(1.2431) | Loss 4.4306(4.4561) | Error 0.4333(0.4416) Steps 934(968.81) | Grad Norm 0.4167(0.7203) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 32.2145, Epoch Time 545.7291(522.1680), Bit/dim 3.8221(best: 3.8223), Xent 1.2277, Loss 4.4359, Error 0.4406(best: 0.4420)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 82.4965(83.6289) | Bit/dim 3.8243(3.8342) | Xent 1.2212(1.2424) | Loss 4.4349(4.4554) | Error 0.4379(0.4415) Steps 982(969.21) | Grad Norm 0.3621(0.7096) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 84.0041(83.6401) | Bit/dim 3.8126(3.8336) | Xent 1.2150(1.2416) | Loss 4.4202(4.4544) | Error 0.4296(0.4411) Steps 946(968.51) | Grad Norm 0.4406(0.7015) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 80.6879(83.5516) | Bit/dim 3.8187(3.8331) | Xent 1.2284(1.2412) | Loss 4.4329(4.4537) | Error 0.4374(0.4410) Steps 940(967.66) | Grad Norm 0.3904(0.6922) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 83.2105(83.5413) | Bit/dim 3.8219(3.8328) | Xent 1.1985(1.2399) | Loss 4.4212(4.4527) | Error 0.4229(0.4405) Steps 958(967.37) | Grad Norm 0.4778(0.6858) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 86.4894(83.6298) | Bit/dim 3.8278(3.8326) | Xent 1.2121(1.2391) | Loss 4.4338(4.4522) | Error 0.4320(0.4402) Steps 964(967.27) | Grad Norm 0.4273(0.6780) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 80.9103(83.5482) | Bit/dim 3.8148(3.8321) | Xent 1.2346(1.2390) | Loss 4.4321(4.4516) | Error 0.4407(0.4402) Steps 982(967.71) | Grad Norm 0.3950(0.6695) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 32.0801, Epoch Time 546.1329(522.8870), Bit/dim 3.8211(best: 3.8221), Xent 1.2249, Loss 4.4335, Error 0.4430(best: 0.4406)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 83.2932(83.5406) | Bit/dim 3.8127(3.8315) | Xent 1.2194(1.2384) | Loss 4.4224(4.4507) | Error 0.4341(0.4400) Steps 958(967.42) | Grad Norm 0.7688(0.6725) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 86.6136(83.6327) | Bit/dim 3.8239(3.8313) | Xent 1.2140(1.2377) | Loss 4.4309(4.4501) | Error 0.4325(0.4398) Steps 976(967.68) | Grad Norm 0.5303(0.6682) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 81.9573(83.5825) | Bit/dim 3.8264(3.8311) | Xent 1.2117(1.2369) | Loss 4.4323(4.4496) | Error 0.4271(0.4394) Steps 982(968.10) | Grad Norm 0.6261(0.6670) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 82.2054(83.5412) | Bit/dim 3.8228(3.8309) | Xent 1.2103(1.2361) | Loss 4.4280(4.4489) | Error 0.4309(0.4392) Steps 946(967.44) | Grad Norm 0.4650(0.6609) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 83.3854(83.5365) | Bit/dim 3.8142(3.8304) | Xent 1.2092(1.2353) | Loss 4.4188(4.4480) | Error 0.4335(0.4390) Steps 946(966.80) | Grad Norm 0.4948(0.6559) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 81.0619(83.4623) | Bit/dim 3.8193(3.8301) | Xent 1.2277(1.2350) | Loss 4.4332(4.4476) | Error 0.4389(0.4390) Steps 940(965.99) | Grad Norm 0.4581(0.6500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 32.0796, Epoch Time 546.1259(523.5842), Bit/dim 3.8194(best: 3.8211), Xent 1.2234, Loss 4.4311, Error 0.4388(best: 0.4406)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 86.1247(83.5421) | Bit/dim 3.8128(3.8295) | Xent 1.2062(1.2342) | Loss 4.4159(4.4466) | Error 0.4300(0.4387) Steps 952(965.57) | Grad Norm 0.4305(0.6434) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 80.4683(83.4499) | Bit/dim 3.8263(3.8294) | Xent 1.2118(1.2335) | Loss 4.4322(4.4462) | Error 0.4344(0.4386) Steps 958(965.35) | Grad Norm 0.7974(0.6480) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 80.0870(83.3490) | Bit/dim 3.8195(3.8291) | Xent 1.2097(1.2328) | Loss 4.4243(4.4455) | Error 0.4221(0.4381) Steps 964(965.31) | Grad Norm 0.6480(0.6480) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 83.9765(83.3679) | Bit/dim 3.8184(3.8288) | Xent 1.2287(1.2327) | Loss 4.4328(4.4452) | Error 0.4395(0.4381) Steps 988(965.99) | Grad Norm 0.4345(0.6416) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 82.8180(83.3514) | Bit/dim 3.8113(3.8283) | Xent 1.2300(1.2326) | Loss 4.4263(4.4446) | Error 0.4385(0.4382) Steps 952(965.57) | Grad Norm 0.5942(0.6402) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 83.5988(83.3588) | Bit/dim 3.8167(3.8279) | Xent 1.2003(1.2316) | Loss 4.4169(4.4438) | Error 0.4255(0.4378) Steps 934(964.62) | Grad Norm 0.5172(0.6365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 31.9085, Epoch Time 544.9300(524.2245), Bit/dim 3.8175(best: 3.8194), Xent 1.2192, Loss 4.4271, Error 0.4372(best: 0.4388)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 80.2480(83.2655) | Bit/dim 3.8261(3.8279) | Xent 1.1983(1.2306) | Loss 4.4253(4.4432) | Error 0.4280(0.4375) Steps 940(963.88) | Grad Norm 0.5803(0.6348) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 81.2539(83.2051) | Bit/dim 3.8069(3.8273) | Xent 1.2101(1.2300) | Loss 4.4119(4.4423) | Error 0.4337(0.4374) Steps 946(963.35) | Grad Norm 0.5042(0.6309) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 83.7813(83.2224) | Bit/dim 3.8201(3.8270) | Xent 1.2186(1.2297) | Loss 4.4294(4.4419) | Error 0.4340(0.4373) Steps 940(962.65) | Grad Norm 0.4280(0.6248) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 82.1662(83.1907) | Bit/dim 3.8180(3.8268) | Xent 1.2096(1.2291) | Loss 4.4228(4.4413) | Error 0.4266(0.4369) Steps 946(962.15) | Grad Norm 0.5480(0.6225) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 82.3722(83.1662) | Bit/dim 3.8191(3.8265) | Xent 1.2162(1.2287) | Loss 4.4272(4.4409) | Error 0.4284(0.4367) Steps 940(961.48) | Grad Norm 0.4777(0.6182) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 80.7912(83.0949) | Bit/dim 3.8149(3.8262) | Xent 1.2072(1.2280) | Loss 4.4185(4.4402) | Error 0.4335(0.4366) Steps 952(961.20) | Grad Norm 0.4193(0.6122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 31.6630, Epoch Time 538.1848(524.6433), Bit/dim 3.8178(best: 3.8175), Xent 1.2167, Loss 4.4262, Error 0.4382(best: 0.4372)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 82.6883(83.0827) | Bit/dim 3.8206(3.8260) | Xent 1.1995(1.2272) | Loss 4.4203(4.4396) | Error 0.4277(0.4363) Steps 958(961.10) | Grad Norm 0.4035(0.6059) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 84.5163(83.1257) | Bit/dim 3.8115(3.8256) | Xent 1.1965(1.2263) | Loss 4.4098(4.4387) | Error 0.4169(0.4357) Steps 946(960.65) | Grad Norm 0.5391(0.6039) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 81.7004(83.0830) | Bit/dim 3.8119(3.8252) | Xent 1.2031(1.2256) | Loss 4.4134(4.4380) | Error 0.4270(0.4355) Steps 952(960.39) | Grad Norm 0.6740(0.6060) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 82.8433(83.0758) | Bit/dim 3.8183(3.8250) | Xent 1.2069(1.2250) | Loss 4.4218(4.4375) | Error 0.4273(0.4352) Steps 946(959.96) | Grad Norm 0.5181(0.6034) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 83.6929(83.0943) | Bit/dim 3.8244(3.8250) | Xent 1.1933(1.2240) | Loss 4.4210(4.4370) | Error 0.4273(0.4350) Steps 952(959.72) | Grad Norm 0.4051(0.5974) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 83.4324(83.1044) | Bit/dim 3.8126(3.8246) | Xent 1.2122(1.2237) | Loss 4.4187(4.4364) | Error 0.4324(0.4349) Steps 946(959.31) | Grad Norm 0.5176(0.5951) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 31.8345, Epoch Time 546.2712(525.2922), Bit/dim 3.8165(best: 3.8175), Xent 1.2140, Loss 4.4234, Error 0.4347(best: 0.4372)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 80.4175(83.0238) | Bit/dim 3.8237(3.8246) | Xent 1.2180(1.2235) | Loss 4.4327(4.4363) | Error 0.4395(0.4351) Steps 946(958.91) | Grad Norm 0.4901(0.5919) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 85.6782(83.1035) | Bit/dim 3.8129(3.8242) | Xent 1.1968(1.2227) | Loss 4.4113(4.4356) | Error 0.4236(0.4347) Steps 940(958.34) | Grad Norm 0.5207(0.5898) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 82.5542(83.0870) | Bit/dim 3.8137(3.8239) | Xent 1.1864(1.2216) | Loss 4.4069(4.4347) | Error 0.4194(0.4343) Steps 946(957.97) | Grad Norm 0.4420(0.5853) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 84.2902(83.1231) | Bit/dim 3.8184(3.8237) | Xent 1.1914(1.2207) | Loss 4.4141(4.4341) | Error 0.4263(0.4340) Steps 946(957.61) | Grad Norm 0.3801(0.5792) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 82.3890(83.1010) | Bit/dim 3.8119(3.8234) | Xent 1.2107(1.2204) | Loss 4.4173(4.4336) | Error 0.4304(0.4339) Steps 952(957.44) | Grad Norm 0.5614(0.5786) | Total Time 14.00(14.00)\n",
      "Iter 0726 | Time 82.7694(83.0911) | Bit/dim 3.8111(3.8230) | Xent 1.2031(1.2199) | Loss 4.4127(4.4330) | Error 0.4211(0.4335) Steps 946(957.10) | Grad Norm 0.5553(0.5779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 31.9363, Epoch Time 545.8459(525.9088), Bit/dim 3.8159(best: 3.8165), Xent 1.2095, Loss 4.4207, Error 0.4349(best: 0.4347)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 80.5498(83.0149) | Bit/dim 3.8177(3.8228) | Xent 1.2129(1.2197) | Loss 4.4241(4.4327) | Error 0.4353(0.4336) Steps 964(957.31) | Grad Norm 0.4129(0.5730) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 82.6471(83.0038) | Bit/dim 3.8198(3.8228) | Xent 1.1897(1.2188) | Loss 4.4147(4.4322) | Error 0.4265(0.4334) Steps 946(956.97) | Grad Norm 0.5400(0.5720) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 84.6629(83.0536) | Bit/dim 3.8084(3.8223) | Xent 1.1968(1.2181) | Loss 4.4068(4.4314) | Error 0.4230(0.4330) Steps 964(957.18) | Grad Norm 0.4680(0.5689) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 82.0218(83.0226) | Bit/dim 3.8114(3.8220) | Xent 1.2075(1.2178) | Loss 4.4152(4.4309) | Error 0.4224(0.4327) Steps 946(956.84) | Grad Norm 0.5362(0.5679) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 82.5239(83.0077) | Bit/dim 3.8146(3.8218) | Xent 1.1963(1.2172) | Loss 4.4127(4.4304) | Error 0.4263(0.4325) Steps 928(955.98) | Grad Norm 0.5087(0.5661) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 82.3552(82.9881) | Bit/dim 3.8136(3.8215) | Xent 1.1912(1.2164) | Loss 4.4092(4.4297) | Error 0.4251(0.4323) Steps 946(955.68) | Grad Norm 0.4209(0.5618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 32.0619, Epoch Time 542.8137(526.4159), Bit/dim 3.8148(best: 3.8159), Xent 1.2101, Loss 4.4198, Error 0.4345(best: 0.4347)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 82.1781(82.9638) | Bit/dim 3.8137(3.8213) | Xent 1.2148(1.2163) | Loss 4.4211(4.4295) | Error 0.4294(0.4322) Steps 934(955.03) | Grad Norm 0.4819(0.5594) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 81.1545(82.9095) | Bit/dim 3.8082(3.8209) | Xent 1.1931(1.2156) | Loss 4.4048(4.4287) | Error 0.4251(0.4320) Steps 946(954.76) | Grad Norm 0.4665(0.5566) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 83.3297(82.9221) | Bit/dim 3.8113(3.8206) | Xent 1.1829(1.2147) | Loss 4.4027(4.4279) | Error 0.4185(0.4316) Steps 946(954.49) | Grad Norm 0.6404(0.5591) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 82.6945(82.9153) | Bit/dim 3.8097(3.8203) | Xent 1.1877(1.2139) | Loss 4.4035(4.4272) | Error 0.4316(0.4316) Steps 940(954.06) | Grad Norm 0.5160(0.5578) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 84.4718(82.9620) | Bit/dim 3.8218(3.8203) | Xent 1.1942(1.2133) | Loss 4.4189(4.4270) | Error 0.4239(0.4314) Steps 946(953.82) | Grad Norm 0.6962(0.5620) | Total Time 14.00(14.00)\n",
      "Iter 0738 | Time 85.6520(83.0427) | Bit/dim 3.8151(3.8202) | Xent 1.2104(1.2132) | Loss 4.4203(4.4268) | Error 0.4271(0.4312) Steps 952(953.76) | Grad Norm 0.8713(0.5712) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 32.1069, Epoch Time 547.4155(527.0459), Bit/dim 3.8144(best: 3.8148), Xent 1.2056, Loss 4.4172, Error 0.4309(best: 0.4345)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 84.0249(83.0722) | Bit/dim 3.8144(3.8200) | Xent 1.1954(1.2126) | Loss 4.4121(4.4263) | Error 0.4181(0.4309) Steps 934(953.17) | Grad Norm 0.7060(0.5753) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 84.5688(83.1171) | Bit/dim 3.8156(3.8199) | Xent 1.1779(1.2116) | Loss 4.4046(4.4257) | Error 0.4170(0.4304) Steps 958(953.32) | Grad Norm 0.8487(0.5835) | Total Time 14.00(14.00)\n",
      "Iter 0741 | Time 82.1918(83.0893) | Bit/dim 3.8147(3.8197) | Xent 1.1939(1.2111) | Loss 4.4117(4.4253) | Error 0.4235(0.4302) Steps 946(953.10) | Grad Norm 1.2064(0.6022) | Total Time 14.00(14.00)\n",
      "Iter 0742 | Time 81.3872(83.0382) | Bit/dim 3.8130(3.8195) | Xent 1.2010(1.2108) | Loss 4.4135(4.4249) | Error 0.4255(0.4301) Steps 952(953.06) | Grad Norm 0.8342(0.6091) | Total Time 14.00(14.00)\n",
      "Iter 0743 | Time 81.6198(82.9957) | Bit/dim 3.8035(3.8190) | Xent 1.1844(1.2100) | Loss 4.3957(4.4240) | Error 0.4217(0.4298) Steps 946(952.85) | Grad Norm 1.0010(0.6209) | Total Time 14.00(14.00)\n",
      "Iter 0744 | Time 80.8786(82.9322) | Bit/dim 3.8124(3.8188) | Xent 1.1985(1.2096) | Loss 4.4117(4.4237) | Error 0.4294(0.4298) Steps 934(952.29) | Grad Norm 1.1304(0.6362) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 31.9345, Epoch Time 541.9801(527.4939), Bit/dim 3.8132(best: 3.8144), Xent 1.2033, Loss 4.4149, Error 0.4351(best: 0.4309)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 80.3252(82.8540) | Bit/dim 3.8068(3.8185) | Xent 1.1967(1.2092) | Loss 4.4051(4.4231) | Error 0.4227(0.4296) Steps 940(951.92) | Grad Norm 0.9048(0.6442) | Total Time 14.00(14.00)\n",
      "Iter 0746 | Time 81.0606(82.8002) | Bit/dim 3.8085(3.8182) | Xent 1.1813(1.2084) | Loss 4.3992(4.4224) | Error 0.4215(0.4294) Steps 934(951.38) | Grad Norm 0.6886(0.6456) | Total Time 14.00(14.00)\n",
      "Iter 0747 | Time 80.4061(82.7283) | Bit/dim 3.8193(3.8182) | Xent 1.1669(1.2072) | Loss 4.4028(4.4218) | Error 0.4117(0.4288) Steps 946(951.22) | Grad Norm 1.1732(0.6614) | Total Time 14.00(14.00)\n",
      "Iter 0748 | Time 81.7694(82.6996) | Bit/dim 3.7969(3.8176) | Xent 1.1912(1.2067) | Loss 4.3925(4.4209) | Error 0.4276(0.4288) Steps 958(951.42) | Grad Norm 1.1573(0.6763) | Total Time 14.00(14.00)\n",
      "Iter 0749 | Time 86.2600(82.8064) | Bit/dim 3.8188(3.8176) | Xent 1.1973(1.2064) | Loss 4.4175(4.4208) | Error 0.4241(0.4287) Steps 946(951.26) | Grad Norm 0.8434(0.6813) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 81.8551(82.7778) | Bit/dim 3.8181(3.8176) | Xent 1.1972(1.2061) | Loss 4.4167(4.4207) | Error 0.4250(0.4286) Steps 952(951.28) | Grad Norm 0.8876(0.6875) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 31.8193, Epoch Time 538.9684(527.8382), Bit/dim 3.8128(best: 3.8132), Xent 1.2005, Loss 4.4130, Error 0.4306(best: 0.4309)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 81.5332(82.7405) | Bit/dim 3.8163(3.8176) | Xent 1.2014(1.2060) | Loss 4.4170(4.4206) | Error 0.4230(0.4284) Steps 952(951.30) | Grad Norm 1.2805(0.7053) | Total Time 14.00(14.00)\n",
      "Iter 0752 | Time 84.1226(82.7820) | Bit/dim 3.8117(3.8174) | Xent 1.1968(1.2057) | Loss 4.4101(4.4203) | Error 0.4256(0.4283) Steps 940(950.96) | Grad Norm 1.0549(0.7158) | Total Time 14.00(14.00)\n",
      "Iter 0753 | Time 81.5017(82.7436) | Bit/dim 3.8125(3.8173) | Xent 1.1803(1.2050) | Loss 4.4026(4.4197) | Error 0.4260(0.4282) Steps 934(950.45) | Grad Norm 0.7726(0.7175) | Total Time 14.00(14.00)\n",
      "Iter 0754 | Time 81.8371(82.7164) | Bit/dim 3.7983(3.8167) | Xent 1.1849(1.2043) | Loss 4.3908(4.4189) | Error 0.4223(0.4281) Steps 934(949.96) | Grad Norm 1.0979(0.7289) | Total Time 14.00(14.00)\n",
      "Iter 0755 | Time 82.6882(82.7155) | Bit/dim 3.8131(3.8166) | Xent 1.1842(1.2037) | Loss 4.4052(4.4185) | Error 0.4185(0.4278) Steps 964(950.38) | Grad Norm 0.8292(0.7319) | Total Time 14.00(14.00)\n",
      "Iter 0756 | Time 80.4530(82.6476) | Bit/dim 3.8131(3.8165) | Xent 1.1782(1.2030) | Loss 4.4022(4.4180) | Error 0.4176(0.4275) Steps 952(950.43) | Grad Norm 0.8220(0.7346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 31.9576, Epoch Time 539.9311(528.2010), Bit/dim 3.8114(best: 3.8128), Xent 1.1972, Loss 4.4100, Error 0.4272(best: 0.4306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 83.0188(82.6588) | Bit/dim 3.8002(3.8160) | Xent 1.1929(1.2027) | Loss 4.3966(4.4173) | Error 0.4259(0.4274) Steps 934(949.94) | Grad Norm 0.5761(0.7298) | Total Time 14.00(14.00)\n",
      "Iter 0758 | Time 80.7083(82.6003) | Bit/dim 3.8166(3.8160) | Xent 1.1859(1.2022) | Loss 4.4096(4.4171) | Error 0.4251(0.4273) Steps 940(949.64) | Grad Norm 0.8938(0.7348) | Total Time 14.00(14.00)\n",
      "Iter 0759 | Time 86.4493(82.7157) | Bit/dim 3.8150(3.8160) | Xent 1.1949(1.2020) | Loss 4.4125(4.4170) | Error 0.4191(0.4271) Steps 952(949.71) | Grad Norm 1.3170(0.7522) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 84.2015(82.7603) | Bit/dim 3.8147(3.8159) | Xent 1.1738(1.2011) | Loss 4.4016(4.4165) | Error 0.4149(0.4267) Steps 964(950.14) | Grad Norm 0.6203(0.7483) | Total Time 14.00(14.00)\n",
      "Iter 0761 | Time 83.2354(82.7746) | Bit/dim 3.7925(3.8152) | Xent 1.1492(1.1996) | Loss 4.3672(4.4150) | Error 0.4097(0.4262) Steps 952(950.20) | Grad Norm 1.0531(0.7574) | Total Time 14.00(14.00)\n",
      "Iter 0762 | Time 84.8519(82.8369) | Bit/dim 3.8170(3.8153) | Xent 1.2024(1.1996) | Loss 4.4181(4.4151) | Error 0.4221(0.4261) Steps 946(950.07) | Grad Norm 1.0395(0.7659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 31.9425, Epoch Time 550.0975(528.8579), Bit/dim 3.8107(best: 3.8114), Xent 1.1961, Loss 4.4088, Error 0.4293(best: 0.4272)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 81.7634(82.8047) | Bit/dim 3.8146(3.8153) | Xent 1.1960(1.1995) | Loss 4.4126(4.4150) | Error 0.4199(0.4259) Steps 928(949.41) | Grad Norm 1.4551(0.7866) | Total Time 14.00(14.00)\n",
      "Iter 0764 | Time 84.8734(82.8667) | Bit/dim 3.8108(3.8151) | Xent 1.2006(1.1996) | Loss 4.4111(4.4149) | Error 0.4286(0.4260) Steps 934(948.94) | Grad Norm 0.5255(0.7787) | Total Time 14.00(14.00)\n",
      "Iter 0765 | Time 83.9192(82.8983) | Bit/dim 3.8024(3.8148) | Xent 1.1709(1.1987) | Loss 4.3879(4.4141) | Error 0.4137(0.4256) Steps 946(948.86) | Grad Norm 0.8320(0.7803) | Total Time 14.00(14.00)\n",
      "Iter 0766 | Time 85.9025(82.9884) | Bit/dim 3.8080(3.8145) | Xent 1.1599(1.1975) | Loss 4.3879(4.4133) | Error 0.4151(0.4253) Steps 940(948.59) | Grad Norm 0.9191(0.7845) | Total Time 14.00(14.00)\n",
      "Iter 0767 | Time 82.7985(82.9827) | Bit/dim 3.8049(3.8143) | Xent 1.1785(1.1970) | Loss 4.3941(4.4127) | Error 0.4184(0.4251) Steps 946(948.51) | Grad Norm 0.5700(0.7780) | Total Time 14.00(14.00)\n",
      "Iter 0768 | Time 83.5522(82.9998) | Bit/dim 3.8125(3.8142) | Xent 1.1786(1.1964) | Loss 4.4018(4.4124) | Error 0.4166(0.4249) Steps 928(947.90) | Grad Norm 0.8360(0.7798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 31.8883, Epoch Time 550.4051(529.5043), Bit/dim 3.8087(best: 3.8107), Xent 1.1923, Loss 4.4049, Error 0.4272(best: 0.4272)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 84.5803(83.0472) | Bit/dim 3.8066(3.8140) | Xent 1.1904(1.1962) | Loss 4.4018(4.4121) | Error 0.4291(0.4250) Steps 952(948.02) | Grad Norm 0.9716(0.7855) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 83.8140(83.0702) | Bit/dim 3.8082(3.8138) | Xent 1.1749(1.1956) | Loss 4.3956(4.4116) | Error 0.4197(0.4248) Steps 934(947.60) | Grad Norm 0.8928(0.7888) | Total Time 14.00(14.00)\n",
      "Iter 0771 | Time 83.6562(83.0878) | Bit/dim 3.8051(3.8135) | Xent 1.1662(1.1947) | Loss 4.3882(4.4109) | Error 0.4161(0.4246) Steps 928(947.01) | Grad Norm 0.8574(0.7908) | Total Time 14.00(14.00)\n",
      "Iter 0772 | Time 83.9595(83.1140) | Bit/dim 3.8125(3.8135) | Xent 1.1740(1.1941) | Loss 4.3994(4.4106) | Error 0.4136(0.4242) Steps 934(946.62) | Grad Norm 0.5306(0.7830) | Total Time 14.00(14.00)\n",
      "Iter 0773 | Time 83.2834(83.1191) | Bit/dim 3.8005(3.8131) | Xent 1.1707(1.1934) | Loss 4.3859(4.4098) | Error 0.4106(0.4238) Steps 934(946.24) | Grad Norm 1.5910(0.8072) | Total Time 14.00(14.00)\n",
      "Iter 0774 | Time 84.4916(83.1602) | Bit/dim 3.8090(3.8130) | Xent 1.1679(1.1926) | Loss 4.3929(4.4093) | Error 0.4200(0.4237) Steps 910(945.16) | Grad Norm 1.4372(0.8261) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 31.8493, Epoch Time 551.3471(530.1596), Bit/dim 3.8078(best: 3.8087), Xent 1.1902, Loss 4.4029, Error 0.4273(best: 0.4272)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 85.5181(83.2310) | Bit/dim 3.8134(3.8130) | Xent 1.1674(1.1919) | Loss 4.3971(4.4089) | Error 0.4183(0.4235) Steps 958(945.54) | Grad Norm 0.9354(0.8294) | Total Time 14.00(14.00)\n",
      "Iter 0776 | Time 84.0750(83.2563) | Bit/dim 3.7955(3.8125) | Xent 1.1745(1.1913) | Loss 4.3828(4.4082) | Error 0.4164(0.4233) Steps 928(945.01) | Grad Norm 0.6557(0.8242) | Total Time 14.00(14.00)\n",
      "Iter 0777 | Time 79.6211(83.1472) | Bit/dim 3.8186(3.8127) | Xent 1.1922(1.1914) | Loss 4.4147(4.4084) | Error 0.4233(0.4233) Steps 940(944.86) | Grad Norm 1.0726(0.8317) | Total Time 14.00(14.00)\n",
      "Iter 0778 | Time 80.0153(83.0533) | Bit/dim 3.8080(3.8125) | Xent 1.1632(1.1905) | Loss 4.3896(4.4078) | Error 0.4133(0.4230) Steps 910(943.82) | Grad Norm 1.2193(0.8433) | Total Time 14.00(14.00)\n",
      "Iter 0779 | Time 83.3170(83.0612) | Bit/dim 3.8028(3.8122) | Xent 1.1750(1.1901) | Loss 4.3903(4.4073) | Error 0.4204(0.4229) Steps 952(944.06) | Grad Norm 1.2645(0.8559) | Total Time 14.00(14.00)\n",
      "Iter 0780 | Time 80.8070(82.9936) | Bit/dim 3.8004(3.8119) | Xent 1.1755(1.1896) | Loss 4.3881(4.4067) | Error 0.4196(0.4228) Steps 940(943.94) | Grad Norm 0.8927(0.8570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 31.4860, Epoch Time 540.4814(530.4692), Bit/dim 3.8074(best: 3.8078), Xent 1.1867, Loss 4.4008, Error 0.4248(best: 0.4272)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 81.4476(82.9472) | Bit/dim 3.8058(3.8117) | Xent 1.1762(1.1892) | Loss 4.3939(4.4063) | Error 0.4145(0.4226) Steps 946(944.00) | Grad Norm 0.8966(0.8582) | Total Time 14.00(14.00)\n",
      "Iter 0782 | Time 82.1562(82.9234) | Bit/dim 3.7953(3.8112) | Xent 1.1631(1.1884) | Loss 4.3769(4.4054) | Error 0.4134(0.4223) Steps 952(944.24) | Grad Norm 0.7980(0.8564) | Total Time 14.00(14.00)\n",
      "Iter 0783 | Time 82.6324(82.9147) | Bit/dim 3.8147(3.8113) | Xent 1.1661(1.1878) | Loss 4.3978(4.4052) | Error 0.4062(0.4218) Steps 934(943.94) | Grad Norm 0.7420(0.8530) | Total Time 14.00(14.00)\n",
      "Iter 0784 | Time 83.2743(82.9255) | Bit/dim 3.8010(3.8110) | Xent 1.1774(1.1875) | Loss 4.3896(4.4047) | Error 0.4176(0.4217) Steps 922(943.28) | Grad Norm 1.2217(0.8640) | Total Time 14.00(14.00)\n",
      "Iter 0785 | Time 83.2039(82.9339) | Bit/dim 3.8045(3.8108) | Xent 1.1567(1.1865) | Loss 4.3828(4.4041) | Error 0.4055(0.4212) Steps 928(942.82) | Grad Norm 0.8125(0.8625) | Total Time 14.00(14.00)\n",
      "Iter 0786 | Time 84.9774(82.9952) | Bit/dim 3.8124(3.8109) | Xent 1.1778(1.1863) | Loss 4.4013(4.4040) | Error 0.4157(0.4211) Steps 910(941.84) | Grad Norm 0.5952(0.8545) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 31.6102, Epoch Time 544.8240(530.8999), Bit/dim 3.8062(best: 3.8074), Xent 1.1848, Loss 4.3986, Error 0.4261(best: 0.4248)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 81.6558(82.9550) | Bit/dim 3.7993(3.8105) | Xent 1.1648(1.1856) | Loss 4.3817(4.4033) | Error 0.4150(0.4209) Steps 952(942.14) | Grad Norm 0.9115(0.8562) | Total Time 14.00(14.00)\n",
      "Iter 0788 | Time 82.4614(82.9402) | Bit/dim 3.8018(3.8102) | Xent 1.1464(1.1845) | Loss 4.3750(4.4025) | Error 0.4121(0.4206) Steps 922(941.54) | Grad Norm 0.6018(0.8486) | Total Time 14.00(14.00)\n",
      "Iter 0789 | Time 81.4794(82.8963) | Bit/dim 3.8115(3.8103) | Xent 1.1683(1.1840) | Loss 4.3956(4.4023) | Error 0.4216(0.4206) Steps 904(940.41) | Grad Norm 0.8919(0.8499) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 83.2305(82.9064) | Bit/dim 3.8136(3.8104) | Xent 1.1476(1.1829) | Loss 4.3874(4.4018) | Error 0.4030(0.4201) Steps 964(941.12) | Grad Norm 0.6064(0.8426) | Total Time 14.00(14.00)\n",
      "Iter 0791 | Time 82.6097(82.8975) | Bit/dim 3.8065(3.8103) | Xent 1.1611(1.1822) | Loss 4.3871(4.4014) | Error 0.4099(0.4198) Steps 946(941.26) | Grad Norm 1.2341(0.8543) | Total Time 14.00(14.00)\n",
      "Iter 0792 | Time 81.9386(82.8687) | Bit/dim 3.7958(3.8098) | Xent 1.1544(1.1814) | Loss 4.3731(4.4005) | Error 0.4131(0.4196) Steps 928(940.87) | Grad Norm 0.4816(0.8431) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 31.7600, Epoch Time 540.9194(531.2005), Bit/dim 3.8055(best: 3.8062), Xent 1.1799, Loss 4.3955, Error 0.4255(best: 0.4248)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 82.9746(82.8719) | Bit/dim 3.8157(3.8100) | Xent 1.1598(1.1807) | Loss 4.3956(4.4004) | Error 0.4113(0.4194) Steps 928(940.48) | Grad Norm 1.2254(0.8546) | Total Time 14.00(14.00)\n",
      "Iter 0794 | Time 83.0042(82.8759) | Bit/dim 3.7963(3.8096) | Xent 1.1678(1.1804) | Loss 4.3803(4.3998) | Error 0.4126(0.4192) Steps 934(940.29) | Grad Norm 0.5523(0.8455) | Total Time 14.00(14.00)\n",
      "Iter 0795 | Time 81.8908(82.8463) | Bit/dim 3.7981(3.8093) | Xent 1.1579(1.1797) | Loss 4.3771(4.3991) | Error 0.4133(0.4190) Steps 916(939.56) | Grad Norm 0.8488(0.8456) | Total Time 14.00(14.00)\n",
      "Iter 0796 | Time 81.9950(82.8208) | Bit/dim 3.8095(3.8093) | Xent 1.1513(1.1788) | Loss 4.3851(4.3987) | Error 0.4097(0.4187) Steps 934(939.39) | Grad Norm 0.5593(0.8370) | Total Time 14.00(14.00)\n",
      "Iter 0797 | Time 84.4840(82.8707) | Bit/dim 3.8030(3.8091) | Xent 1.1806(1.1789) | Loss 4.3933(4.3985) | Error 0.4185(0.4187) Steps 898(938.15) | Grad Norm 1.3138(0.8513) | Total Time 14.00(14.00)\n",
      "Iter 0798 | Time 83.9341(82.9026) | Bit/dim 3.8018(3.8089) | Xent 1.1562(1.1782) | Loss 4.3799(4.3980) | Error 0.4110(0.4185) Steps 946(938.38) | Grad Norm 1.2413(0.8630) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 31.6327, Epoch Time 545.5874(531.6321), Bit/dim 3.8051(best: 3.8055), Xent 1.1745, Loss 4.3924, Error 0.4237(best: 0.4248)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 78.3173(82.7650) | Bit/dim 3.8084(3.8088) | Xent 1.1698(1.1779) | Loss 4.3934(4.3978) | Error 0.4170(0.4184) Steps 922(937.89) | Grad Norm 1.0406(0.8684) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 82.3773(82.7534) | Bit/dim 3.8012(3.8086) | Xent 1.1590(1.1774) | Loss 4.3806(4.3973) | Error 0.4073(0.4181) Steps 928(937.60) | Grad Norm 1.0429(0.8736) | Total Time 14.00(14.00)\n",
      "Iter 0801 | Time 82.1295(82.7347) | Bit/dim 3.8112(3.8087) | Xent 1.1430(1.1763) | Loss 4.3827(4.3969) | Error 0.4085(0.4178) Steps 928(937.31) | Grad Norm 1.0283(0.8782) | Total Time 14.00(14.00)\n",
      "Iter 0802 | Time 82.3587(82.7234) | Bit/dim 3.7914(3.8082) | Xent 1.1494(1.1755) | Loss 4.3661(4.3959) | Error 0.4067(0.4175) Steps 916(936.67) | Grad Norm 0.9721(0.8811) | Total Time 14.00(14.00)\n",
      "Iter 0803 | Time 80.8721(82.6678) | Bit/dim 3.8014(3.8080) | Xent 1.1754(1.1755) | Loss 4.3891(4.3957) | Error 0.4229(0.4176) Steps 910(935.87) | Grad Norm 2.3255(0.9244) | Total Time 14.00(14.00)\n",
      "Iter 0804 | Time 80.9487(82.6163) | Bit/dim 3.8030(3.8078) | Xent 1.1445(1.1746) | Loss 4.3753(4.3951) | Error 0.4061(0.4173) Steps 922(935.45) | Grad Norm 1.3531(0.9372) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 31.9436, Epoch Time 534.7477(531.7255), Bit/dim 3.8040(best: 3.8051), Xent 1.1761, Loss 4.3921, Error 0.4235(best: 0.4237)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 80.9485(82.5662) | Bit/dim 3.8054(3.8077) | Xent 1.1470(1.1738) | Loss 4.3789(4.3946) | Error 0.4084(0.4170) Steps 934(935.41) | Grad Norm 0.9146(0.9366) | Total Time 14.00(14.00)\n",
      "Iter 0806 | Time 79.4010(82.4713) | Bit/dim 3.8163(3.8080) | Xent 1.1615(1.1734) | Loss 4.3970(4.3947) | Error 0.4124(0.4169) Steps 904(934.47) | Grad Norm 1.2433(0.9458) | Total Time 14.00(14.00)\n",
      "Iter 0807 | Time 81.6720(82.4473) | Bit/dim 3.7950(3.8076) | Xent 1.1676(1.1732) | Loss 4.3788(4.3942) | Error 0.4129(0.4168) Steps 922(934.09) | Grad Norm 2.1659(0.9824) | Total Time 14.00(14.00)\n",
      "Iter 0808 | Time 82.8677(82.4599) | Bit/dim 3.7963(3.8073) | Xent 1.1696(1.1731) | Loss 4.3811(4.3938) | Error 0.4119(0.4166) Steps 916(933.55) | Grad Norm 1.5408(0.9991) | Total Time 14.00(14.00)\n",
      "Iter 0809 | Time 77.8586(82.3219) | Bit/dim 3.8028(3.8071) | Xent 1.1369(1.1720) | Loss 4.3712(4.3932) | Error 0.4051(0.4163) Steps 922(933.20) | Grad Norm 1.7231(1.0208) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 81.8902(82.3089) | Bit/dim 3.7979(3.8069) | Xent 1.1581(1.1716) | Loss 4.3769(4.3927) | Error 0.4061(0.4160) Steps 916(932.69) | Grad Norm 1.2908(1.0289) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 31.7510, Epoch Time 531.9919(531.7335), Bit/dim 3.8030(best: 3.8040), Xent 1.1779, Loss 4.3919, Error 0.4231(best: 0.4235)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 77.2221(82.1563) | Bit/dim 3.8022(3.8067) | Xent 1.1368(1.1706) | Loss 4.3706(4.3920) | Error 0.4077(0.4157) Steps 928(932.55) | Grad Norm 1.9229(1.0558) | Total Time 14.00(14.00)\n",
      "Iter 0812 | Time 82.3811(82.1631) | Bit/dim 3.7947(3.8064) | Xent 1.1476(1.1699) | Loss 4.3685(4.3913) | Error 0.4073(0.4155) Steps 916(932.05) | Grad Norm 1.2158(1.0606) | Total Time 14.00(14.00)\n",
      "Iter 0813 | Time 81.3474(82.1386) | Bit/dim 3.8076(3.8064) | Xent 1.1636(1.1697) | Loss 4.3894(4.3912) | Error 0.4175(0.4155) Steps 940(932.29) | Grad Norm 1.3750(1.0700) | Total Time 14.00(14.00)\n",
      "Iter 0814 | Time 79.8429(82.0697) | Bit/dim 3.7959(3.8061) | Xent 1.1717(1.1698) | Loss 4.3817(4.3910) | Error 0.4179(0.4156) Steps 910(931.62) | Grad Norm 0.8354(1.0630) | Total Time 14.00(14.00)\n",
      "Iter 0815 | Time 81.0598(82.0394) | Bit/dim 3.8078(3.8061) | Xent 1.1571(1.1694) | Loss 4.3863(4.3908) | Error 0.4114(0.4155) Steps 922(931.33) | Grad Norm 2.3867(1.1027) | Total Time 14.00(14.00)\n",
      "Iter 0816 | Time 81.9946(82.0381) | Bit/dim 3.8036(3.8061) | Xent 1.1491(1.1688) | Loss 4.3781(4.3904) | Error 0.4113(0.4153) Steps 934(931.41) | Grad Norm 2.3191(1.1392) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 31.3743, Epoch Time 531.1243(531.7152), Bit/dim 3.8026(best: 3.8030), Xent 1.1692, Loss 4.3872, Error 0.4197(best: 0.4231)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 79.5579(81.9637) | Bit/dim 3.8004(3.8059) | Xent 1.1538(1.1683) | Loss 4.3773(4.3900) | Error 0.4153(0.4153) Steps 922(931.13) | Grad Norm 1.4132(1.1474) | Total Time 14.00(14.00)\n",
      "Iter 0818 | Time 78.5834(81.8623) | Bit/dim 3.7959(3.8056) | Xent 1.1740(1.1685) | Loss 4.3829(4.3898) | Error 0.4194(0.4155) Steps 916(930.68) | Grad Norm 2.7216(1.1946) | Total Time 14.00(14.00)\n",
      "Iter 0819 | Time 82.6856(81.8870) | Bit/dim 3.7993(3.8054) | Xent 1.1713(1.1686) | Loss 4.3849(4.3897) | Error 0.4137(0.4154) Steps 940(930.96) | Grad Norm 0.6258(1.1775) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 82.0249(81.8911) | Bit/dim 3.7985(3.8052) | Xent 1.1470(1.1679) | Loss 4.3720(4.3892) | Error 0.4071(0.4152) Steps 898(929.97) | Grad Norm 3.2857(1.2408) | Total Time 14.00(14.00)\n",
      "Iter 0821 | Time 83.5789(81.9417) | Bit/dim 3.8063(3.8052) | Xent 1.1289(1.1668) | Loss 4.3708(4.3886) | Error 0.4004(0.4147) Steps 928(929.91) | Grad Norm 0.7966(1.2275) | Total Time 14.00(14.00)\n",
      "Iter 0822 | Time 83.7715(81.9966) | Bit/dim 3.8029(3.8052) | Xent 1.1388(1.1659) | Loss 4.3723(4.3881) | Error 0.4036(0.4144) Steps 904(929.13) | Grad Norm 2.8964(1.2775) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 31.6320, Epoch Time 537.8877(531.9004), Bit/dim 3.8005(best: 3.8026), Xent 1.1696, Loss 4.3853, Error 0.4215(best: 0.4197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 82.6139(82.0151) | Bit/dim 3.8020(3.8051) | Xent 1.1666(1.1659) | Loss 4.3853(4.3880) | Error 0.4175(0.4145) Steps 928(929.10) | Grad Norm 1.8417(1.2945) | Total Time 14.00(14.00)\n",
      "Iter 0824 | Time 83.4679(82.0587) | Bit/dim 3.8061(3.8051) | Xent 1.1281(1.1648) | Loss 4.3702(4.3875) | Error 0.4031(0.4141) Steps 934(929.24) | Grad Norm 1.5280(1.3015) | Total Time 14.00(14.00)\n",
      "Iter 0825 | Time 81.7544(82.0496) | Bit/dim 3.8045(3.8051) | Xent 1.1454(1.1642) | Loss 4.3772(4.3872) | Error 0.4036(0.4138) Steps 904(928.49) | Grad Norm 2.5603(1.3392) | Total Time 14.00(14.00)\n",
      "Iter 0826 | Time 83.5464(82.0945) | Bit/dim 3.7993(3.8049) | Xent 1.1618(1.1641) | Loss 4.3802(4.3870) | Error 0.4109(0.4137) Steps 922(928.29) | Grad Norm 1.5302(1.3450) | Total Time 14.00(14.00)\n",
      "Iter 0827 | Time 83.2397(82.1289) | Bit/dim 3.7946(3.8046) | Xent 1.1433(1.1635) | Loss 4.3663(4.3864) | Error 0.4071(0.4135) Steps 904(927.56) | Grad Norm 2.4779(1.3789) | Total Time 14.00(14.00)\n",
      "Iter 0828 | Time 80.0317(82.0659) | Bit/dim 3.7973(3.8044) | Xent 1.1562(1.1633) | Loss 4.3754(4.3860) | Error 0.4097(0.4134) Steps 910(927.04) | Grad Norm 1.8651(1.3935) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 31.7092, Epoch Time 542.3180(532.2129), Bit/dim 3.8000(best: 3.8005), Xent 1.1660, Loss 4.3831, Error 0.4211(best: 0.4197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 78.0208(81.9446) | Bit/dim 3.8023(3.8043) | Xent 1.1336(1.1624) | Loss 4.3691(4.3855) | Error 0.3962(0.4129) Steps 916(926.70) | Grad Norm 1.9710(1.4109) | Total Time 14.00(14.00)\n",
      "Iter 0830 | Time 82.5541(81.9629) | Bit/dim 3.8059(3.8044) | Xent 1.1397(1.1617) | Loss 4.3757(4.3852) | Error 0.4113(0.4129) Steps 910(926.20) | Grad Norm 2.4994(1.4435) | Total Time 14.00(14.00)\n",
      "Iter 0831 | Time 81.7592(81.9568) | Bit/dim 3.8016(3.8043) | Xent 1.1351(1.1609) | Loss 4.3692(4.3847) | Error 0.4076(0.4127) Steps 946(926.80) | Grad Norm 1.7171(1.4517) | Total Time 14.00(14.00)\n",
      "Iter 0832 | Time 79.4073(81.8803) | Bit/dim 3.7902(3.8039) | Xent 1.1392(1.1603) | Loss 4.3598(4.3840) | Error 0.3965(0.4122) Steps 904(926.11) | Grad Norm 3.0369(1.4993) | Total Time 14.00(14.00)\n",
      "Iter 0833 | Time 81.3227(81.8636) | Bit/dim 3.7964(3.8036) | Xent 1.1330(1.1595) | Loss 4.3629(4.3834) | Error 0.4038(0.4120) Steps 916(925.81) | Grad Norm 1.1559(1.4890) | Total Time 14.00(14.00)\n",
      "Iter 0834 | Time 83.1949(81.9035) | Bit/dim 3.8013(3.8036) | Xent 1.1648(1.1596) | Loss 4.3837(4.3834) | Error 0.4165(0.4121) Steps 910(925.34) | Grad Norm 2.3876(1.5159) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 31.3685, Epoch Time 533.2965(532.2455), Bit/dim 3.8005(best: 3.8000), Xent 1.1661, Loss 4.3836, Error 0.4197(best: 0.4197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 79.5584(81.8331) | Bit/dim 3.7954(3.8033) | Xent 1.1543(1.1595) | Loss 4.3726(4.3830) | Error 0.4091(0.4120) Steps 904(924.70) | Grad Norm 1.9577(1.5292) | Total Time 14.00(14.00)\n",
      "Iter 0836 | Time 83.5829(81.8856) | Bit/dim 3.8040(3.8033) | Xent 1.1471(1.1591) | Loss 4.3775(4.3829) | Error 0.4080(0.4119) Steps 904(924.08) | Grad Norm 2.5286(1.5592) | Total Time 14.00(14.00)\n",
      "Iter 0837 | Time 83.3744(81.9303) | Bit/dim 3.7997(3.8032) | Xent 1.1372(1.1584) | Loss 4.3684(4.3824) | Error 0.4093(0.4118) Steps 910(923.65) | Grad Norm 4.9352(1.6605) | Total Time 14.00(14.00)\n",
      "Iter 0838 | Time 81.4408(81.9156) | Bit/dim 3.7982(3.8031) | Xent 1.1289(1.1575) | Loss 4.3626(4.3818) | Error 0.4004(0.4115) Steps 934(923.96) | Grad Norm 0.8149(1.6351) | Total Time 14.00(14.00)\n",
      "Iter 0839 | Time 78.7332(81.8201) | Bit/dim 3.7954(3.8028) | Xent 1.1639(1.1577) | Loss 4.3773(4.3817) | Error 0.4086(0.4114) Steps 892(923.00) | Grad Norm 5.3870(1.7476) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 83.5061(81.8707) | Bit/dim 3.8030(3.8029) | Xent 1.1521(1.1576) | Loss 4.3791(4.3816) | Error 0.4096(0.4113) Steps 910(922.61) | Grad Norm 3.3220(1.7949) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 31.2781, Epoch Time 537.2749(532.3963), Bit/dim 3.7979(best: 3.8000), Xent 1.1680, Loss 4.3819, Error 0.4192(best: 0.4197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 80.0791(81.8170) | Bit/dim 3.7965(3.8027) | Xent 1.1578(1.1576) | Loss 4.3755(4.3814) | Error 0.4077(0.4112) Steps 922(922.60) | Grad Norm 2.3719(1.8122) | Total Time 14.00(14.00)\n",
      "Iter 0842 | Time 79.0216(81.7331) | Bit/dim 3.8028(3.8027) | Xent 1.1548(1.1575) | Loss 4.3802(4.3814) | Error 0.4073(0.4111) Steps 916(922.40) | Grad Norm 4.1932(1.8836) | Total Time 14.00(14.00)\n",
      "Iter 0843 | Time 84.5138(81.8165) | Bit/dim 3.7979(3.8025) | Xent 1.1233(1.1565) | Loss 4.3595(4.3808) | Error 0.3919(0.4105) Steps 916(922.21) | Grad Norm 0.7996(1.8511) | Total Time 14.00(14.00)\n",
      "Iter 0844 | Time 83.3159(81.8615) | Bit/dim 3.7990(3.8024) | Xent 1.1667(1.1568) | Loss 4.3823(4.3808) | Error 0.4116(0.4106) Steps 940(922.74) | Grad Norm 4.8446(1.9409) | Total Time 14.00(14.00)\n",
      "Iter 0845 | Time 80.0219(81.8063) | Bit/dim 3.7946(3.8022) | Xent 1.1316(1.1560) | Loss 4.3604(4.3802) | Error 0.4081(0.4105) Steps 922(922.72) | Grad Norm 1.2318(1.9196) | Total Time 14.00(14.00)\n",
      "Iter 0846 | Time 79.6063(81.7403) | Bit/dim 3.7925(3.8019) | Xent 1.1499(1.1558) | Loss 4.3674(4.3798) | Error 0.4149(0.4106) Steps 886(921.62) | Grad Norm 4.0967(1.9849) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 31.8948, Epoch Time 533.9750(532.4437), Bit/dim 3.7977(best: 3.7979), Xent 1.1597, Loss 4.3775, Error 0.4128(best: 0.4192)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 82.8658(81.7741) | Bit/dim 3.7998(3.8018) | Xent 1.1043(1.1543) | Loss 4.3520(4.3790) | Error 0.3971(0.4102) Steps 928(921.81) | Grad Norm 0.7833(1.9489) | Total Time 14.00(14.00)\n",
      "Iter 0848 | Time 83.6217(81.8295) | Bit/dim 3.7991(3.8017) | Xent 1.1452(1.1540) | Loss 4.3717(4.3788) | Error 0.4077(0.4101) Steps 922(921.81) | Grad Norm 4.1417(2.0147) | Total Time 14.00(14.00)\n",
      "Iter 0849 | Time 82.3699(81.8457) | Bit/dim 3.7897(3.8014) | Xent 1.1533(1.1540) | Loss 4.3663(4.3784) | Error 0.4177(0.4104) Steps 922(921.82) | Grad Norm 1.2363(1.9913) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 79.7251(81.7821) | Bit/dim 3.8094(3.8016) | Xent 1.1465(1.1538) | Loss 4.3827(4.3785) | Error 0.4051(0.4102) Steps 910(921.46) | Grad Norm 4.2713(2.0597) | Total Time 14.00(14.00)\n",
      "Iter 0851 | Time 79.6161(81.7171) | Bit/dim 3.7874(3.8012) | Xent 1.1355(1.1532) | Loss 4.3552(4.3778) | Error 0.4034(0.4100) Steps 922(921.48) | Grad Norm 2.4259(2.0707) | Total Time 14.00(14.00)\n",
      "Iter 0852 | Time 79.3746(81.6469) | Bit/dim 3.7945(3.8010) | Xent 1.1503(1.1531) | Loss 4.3696(4.3776) | Error 0.4085(0.4100) Steps 898(920.78) | Grad Norm 2.8623(2.0945) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 31.5150, Epoch Time 534.7525(532.5130), Bit/dim 3.7973(best: 3.7977), Xent 1.1632, Loss 4.3789, Error 0.4146(best: 0.4128)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 79.7617(81.5903) | Bit/dim 3.7987(3.8009) | Xent 1.1294(1.1524) | Loss 4.3634(4.3771) | Error 0.4036(0.4098) Steps 910(920.45) | Grad Norm 3.1864(2.1272) | Total Time 14.00(14.00)\n",
      "Iter 0854 | Time 80.1846(81.5481) | Bit/dim 3.7964(3.8008) | Xent 1.1406(1.1521) | Loss 4.3667(4.3768) | Error 0.4032(0.4096) Steps 910(920.14) | Grad Norm 2.1449(2.1277) | Total Time 14.00(14.00)\n",
      "Iter 0855 | Time 82.8602(81.5875) | Bit/dim 3.8027(3.8008) | Xent 1.1428(1.1518) | Loss 4.3741(4.3767) | Error 0.4061(0.4095) Steps 916(920.02) | Grad Norm 3.1156(2.1574) | Total Time 14.00(14.00)\n",
      "Iter 0856 | Time 82.7664(81.6229) | Bit/dim 3.7997(3.8008) | Xent 1.1436(1.1515) | Loss 4.3715(4.3766) | Error 0.4080(0.4094) Steps 904(919.53) | Grad Norm 2.5903(2.1704) | Total Time 14.00(14.00)\n",
      "Iter 0857 | Time 80.1919(81.5799) | Bit/dim 3.7907(3.8005) | Xent 1.1199(1.1506) | Loss 4.3506(4.3758) | Error 0.3975(0.4091) Steps 916(919.43) | Grad Norm 3.4184(2.2078) | Total Time 14.00(14.00)\n",
      "Iter 0858 | Time 79.2279(81.5094) | Bit/dim 3.7853(3.8001) | Xent 1.1248(1.1498) | Loss 4.3477(4.3750) | Error 0.4034(0.4089) Steps 880(918.25) | Grad Norm 1.5253(2.1873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 32.0216, Epoch Time 532.6466(532.5170), Bit/dim 3.7960(best: 3.7973), Xent 1.1532, Loss 4.3726, Error 0.4154(best: 0.4128)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 81.5641(81.5110) | Bit/dim 3.8088(3.8003) | Xent 1.1441(1.1496) | Loss 4.3808(4.3751) | Error 0.4058(0.4088) Steps 886(917.28) | Grad Norm 2.5430(2.1980) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 84.0447(81.5870) | Bit/dim 3.7897(3.8000) | Xent 1.1249(1.1489) | Loss 4.3521(4.3744) | Error 0.3952(0.4084) Steps 904(916.88) | Grad Norm 0.6111(2.1504) | Total Time 14.00(14.00)\n",
      "Iter 0861 | Time 80.9285(81.5673) | Bit/dim 3.7963(3.7999) | Xent 1.0997(1.1474) | Loss 4.3461(4.3736) | Error 0.3888(0.4078) Steps 904(916.49) | Grad Norm 1.5166(2.1314) | Total Time 14.00(14.00)\n",
      "Iter 0862 | Time 81.7946(81.5741) | Bit/dim 3.7876(3.7995) | Xent 1.1320(1.1470) | Loss 4.3536(4.3730) | Error 0.4034(0.4077) Steps 898(915.94) | Grad Norm 1.3536(2.1080) | Total Time 14.00(14.00)\n",
      "Iter 0863 | Time 81.6892(81.5775) | Bit/dim 3.7912(3.7993) | Xent 1.1246(1.1463) | Loss 4.3535(4.3724) | Error 0.3976(0.4074) Steps 916(915.94) | Grad Norm 1.6092(2.0931) | Total Time 14.00(14.00)\n",
      "Iter 0864 | Time 80.6726(81.5504) | Bit/dim 3.7979(3.7992) | Xent 1.1422(1.1462) | Loss 4.3690(4.3723) | Error 0.4084(0.4074) Steps 898(915.40) | Grad Norm 2.7101(2.1116) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 31.5822, Epoch Time 538.1728(532.6866), Bit/dim 3.7952(best: 3.7960), Xent 1.1467, Loss 4.3685, Error 0.4154(best: 0.4128)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 81.3908(81.5456) | Bit/dim 3.7908(3.7990) | Xent 1.1216(1.1454) | Loss 4.3516(4.3717) | Error 0.4026(0.4073) Steps 910(915.24) | Grad Norm 0.8865(2.0748) | Total Time 14.00(14.00)\n",
      "Iter 0866 | Time 79.9180(81.4968) | Bit/dim 3.7801(3.7984) | Xent 1.0945(1.1439) | Loss 4.3273(4.3704) | Error 0.3925(0.4068) Steps 904(914.90) | Grad Norm 1.6086(2.0609) | Total Time 14.00(14.00)\n",
      "Iter 0867 | Time 79.9130(81.4493) | Bit/dim 3.7999(3.7984) | Xent 1.1043(1.1427) | Loss 4.3520(4.3698) | Error 0.3951(0.4065) Steps 886(914.04) | Grad Norm 1.8577(2.0548) | Total Time 14.00(14.00)\n",
      "Iter 0868 | Time 82.0430(81.4671) | Bit/dim 3.8041(3.7986) | Xent 1.1276(1.1423) | Loss 4.3679(4.3698) | Error 0.4059(0.4064) Steps 940(914.81) | Grad Norm 0.6228(2.0118) | Total Time 14.00(14.00)\n",
      "Iter 0869 | Time 81.9705(81.4822) | Bit/dim 3.8022(3.7987) | Xent 1.1342(1.1420) | Loss 4.3693(4.3697) | Error 0.3998(0.4062) Steps 892(914.13) | Grad Norm 1.6388(2.0006) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 79.7288(81.4296) | Bit/dim 3.7932(3.7986) | Xent 1.1217(1.1414) | Loss 4.3541(4.3693) | Error 0.3991(0.4060) Steps 904(913.83) | Grad Norm 2.1964(2.0065) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 31.7538, Epoch Time 532.5037(532.6812), Bit/dim 3.7948(best: 3.7952), Xent 1.1418, Loss 4.3657, Error 0.4131(best: 0.4128)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 83.0072(81.4769) | Bit/dim 3.7988(3.7986) | Xent 1.1304(1.1411) | Loss 4.3640(4.3691) | Error 0.4097(0.4061) Steps 910(913.71) | Grad Norm 1.0738(1.9785) | Total Time 14.00(14.00)\n",
      "Iter 0872 | Time 77.0364(81.3437) | Bit/dim 3.7820(3.7981) | Xent 1.1018(1.1399) | Loss 4.3329(4.3680) | Error 0.3901(0.4057) Steps 892(913.06) | Grad Norm 1.4987(1.9641) | Total Time 14.00(14.00)\n",
      "Iter 0873 | Time 79.3385(81.2835) | Bit/dim 3.7935(3.7979) | Xent 1.1187(1.1393) | Loss 4.3528(4.3676) | Error 0.3978(0.4054) Steps 886(912.25) | Grad Norm 1.5217(1.9508) | Total Time 14.00(14.00)\n",
      "Iter 0874 | Time 82.4022(81.3171) | Bit/dim 3.7959(3.7979) | Xent 1.1120(1.1384) | Loss 4.3519(4.3671) | Error 0.4001(0.4053) Steps 904(912.00) | Grad Norm 2.3616(1.9632) | Total Time 14.00(14.00)\n",
      "Iter 0875 | Time 79.0326(81.2486) | Bit/dim 3.7975(3.7979) | Xent 1.1193(1.1379) | Loss 4.3572(4.3668) | Error 0.4005(0.4051) Steps 886(911.22) | Grad Norm 3.6754(2.0145) | Total Time 14.00(14.00)\n",
      "Iter 0876 | Time 79.8848(81.2076) | Bit/dim 3.7935(3.7977) | Xent 1.1189(1.1373) | Loss 4.3530(4.3664) | Error 0.3991(0.4049) Steps 904(911.00) | Grad Norm 2.7865(2.0377) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 31.7708, Epoch Time 527.9388(532.5389), Bit/dim 3.7936(best: 3.7948), Xent 1.1422, Loss 4.3647, Error 0.4120(best: 0.4128)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 83.5084(81.2767) | Bit/dim 3.7884(3.7974) | Xent 1.1011(1.1362) | Loss 4.3389(4.3656) | Error 0.3936(0.4046) Steps 898(910.61) | Grad Norm 0.8612(2.0024) | Total Time 14.00(14.00)\n",
      "Iter 0878 | Time 82.0420(81.2996) | Bit/dim 3.8003(3.7975) | Xent 1.1111(1.1355) | Loss 4.3559(4.3653) | Error 0.3952(0.4043) Steps 898(910.24) | Grad Norm 0.8177(1.9669) | Total Time 14.00(14.00)\n",
      "Iter 0879 | Time 81.8338(81.3157) | Bit/dim 3.7947(3.7974) | Xent 1.1358(1.1355) | Loss 4.3625(4.3652) | Error 0.4002(0.4042) Steps 892(909.69) | Grad Norm 1.2844(1.9464) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 79.2466(81.2536) | Bit/dim 3.7887(3.7972) | Xent 1.1141(1.1348) | Loss 4.3457(4.3646) | Error 0.3970(0.4040) Steps 898(909.34) | Grad Norm 1.2178(1.9245) | Total Time 14.00(14.00)\n",
      "Iter 0881 | Time 79.8261(81.2108) | Bit/dim 3.7888(3.7969) | Xent 1.1153(1.1342) | Loss 4.3464(4.3641) | Error 0.3938(0.4037) Steps 904(909.18) | Grad Norm 1.5797(1.9142) | Total Time 14.00(14.00)\n",
      "Iter 0882 | Time 83.6096(81.2827) | Bit/dim 3.7936(3.7968) | Xent 1.1036(1.1333) | Loss 4.3454(4.3635) | Error 0.4030(0.4037) Steps 880(908.30) | Grad Norm 2.4886(1.9314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 31.5492, Epoch Time 537.2726(532.6809), Bit/dim 3.7920(best: 3.7936), Xent 1.1383, Loss 4.3612, Error 0.4097(best: 0.4120)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 79.6955(81.2351) | Bit/dim 3.7860(3.7965) | Xent 1.0957(1.1322) | Loss 4.3338(4.3626) | Error 0.3928(0.4033) Steps 898(907.99) | Grad Norm 0.7150(1.8949) | Total Time 14.00(14.00)\n",
      "Iter 0884 | Time 80.6720(81.2182) | Bit/dim 3.7912(3.7963) | Xent 1.1242(1.1320) | Loss 4.3533(4.3623) | Error 0.3926(0.4030) Steps 886(907.33) | Grad Norm 2.8318(1.9230) | Total Time 14.00(14.00)\n",
      "Iter 0885 | Time 81.9590(81.2404) | Bit/dim 3.7880(3.7961) | Xent 1.1137(1.1314) | Loss 4.3448(4.3618) | Error 0.4014(0.4030) Steps 898(907.05) | Grad Norm 2.4066(1.9375) | Total Time 14.00(14.00)\n",
      "Iter 0886 | Time 83.1519(81.2978) | Bit/dim 3.7951(3.7961) | Xent 1.1056(1.1306) | Loss 4.3479(4.3614) | Error 0.3909(0.4026) Steps 898(906.78) | Grad Norm 2.0796(1.9418) | Total Time 14.00(14.00)\n",
      "Iter 0887 | Time 82.5386(81.3350) | Bit/dim 3.7984(3.7961) | Xent 1.1197(1.1303) | Loss 4.3582(4.3613) | Error 0.4000(0.4025) Steps 892(906.34) | Grad Norm 1.0629(1.9154) | Total Time 14.00(14.00)\n",
      "Iter 0888 | Time 80.0853(81.2975) | Bit/dim 3.7896(3.7959) | Xent 1.1073(1.1296) | Loss 4.3433(4.3607) | Error 0.3988(0.4024) Steps 886(905.73) | Grad Norm 1.9596(1.9168) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 32.0338, Epoch Time 535.8779(532.7768), Bit/dim 3.7911(best: 3.7920), Xent 1.1355, Loss 4.3589, Error 0.4088(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 78.9742(81.2278) | Bit/dim 3.7862(3.7956) | Xent 1.1045(1.1289) | Loss 4.3385(4.3601) | Error 0.3899(0.4020) Steps 868(904.60) | Grad Norm 1.3942(1.9011) | Total Time 14.00(14.00)\n",
      "Iter 0890 | Time 81.9379(81.2491) | Bit/dim 3.7983(3.7957) | Xent 1.1180(1.1285) | Loss 4.3572(4.3600) | Error 0.4016(0.4020) Steps 910(904.76) | Grad Norm 0.7548(1.8667) | Total Time 14.00(14.00)\n",
      "Iter 0891 | Time 83.1150(81.3051) | Bit/dim 3.7922(3.7956) | Xent 1.0997(1.1277) | Loss 4.3421(4.3595) | Error 0.3945(0.4018) Steps 904(904.74) | Grad Norm 1.1801(1.8461) | Total Time 14.00(14.00)\n",
      "Iter 0892 | Time 80.7180(81.2875) | Bit/dim 3.7884(3.7954) | Xent 1.0894(1.1265) | Loss 4.3330(4.3587) | Error 0.3865(0.4013) Steps 904(904.71) | Grad Norm 2.4374(1.8638) | Total Time 14.00(14.00)\n",
      "Iter 0893 | Time 80.3823(81.2603) | Bit/dim 3.7786(3.7949) | Xent 1.1073(1.1259) | Loss 4.3322(4.3579) | Error 0.3958(0.4012) Steps 886(904.15) | Grad Norm 3.3161(1.9074) | Total Time 14.00(14.00)\n",
      "Iter 0894 | Time 82.6283(81.3014) | Bit/dim 3.7979(3.7950) | Xent 1.1037(1.1253) | Loss 4.3497(4.3576) | Error 0.3930(0.4009) Steps 898(903.97) | Grad Norm 3.7527(1.9628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 31.3854, Epoch Time 534.5709(532.8306), Bit/dim 3.7915(best: 3.7911), Xent 1.1417, Loss 4.3624, Error 0.4110(best: 0.4088)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 82.9635(81.3512) | Bit/dim 3.7914(3.7949) | Xent 1.0881(1.1242) | Loss 4.3354(4.3570) | Error 0.3915(0.4006) Steps 904(903.97) | Grad Norm 3.4837(2.0084) | Total Time 14.00(14.00)\n",
      "Iter 0896 | Time 82.1009(81.3737) | Bit/dim 3.7860(3.7946) | Xent 1.1063(1.1236) | Loss 4.3392(4.3564) | Error 0.3871(0.4002) Steps 904(903.97) | Grad Norm 2.7488(2.0306) | Total Time 14.00(14.00)\n",
      "Iter 0897 | Time 82.5979(81.4104) | Bit/dim 3.7896(3.7945) | Xent 1.0983(1.1229) | Loss 4.3388(4.3559) | Error 0.3876(0.3999) Steps 898(903.79) | Grad Norm 2.4007(2.0417) | Total Time 14.00(14.00)\n",
      "Iter 0898 | Time 79.7933(81.3619) | Bit/dim 3.7990(3.7946) | Xent 1.1045(1.1223) | Loss 4.3512(4.3558) | Error 0.3932(0.3997) Steps 910(903.98) | Grad Norm 1.4803(2.0249) | Total Time 14.00(14.00)\n",
      "Iter 0899 | Time 80.1210(81.3247) | Bit/dim 3.7869(3.7944) | Xent 1.0998(1.1216) | Loss 4.3368(4.3552) | Error 0.3932(0.3995) Steps 904(903.98) | Grad Norm 1.7997(2.0181) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 77.6642(81.2149) | Bit/dim 3.7907(3.7943) | Xent 1.1024(1.1211) | Loss 4.3419(4.3548) | Error 0.3905(0.3992) Steps 880(903.26) | Grad Norm 0.6474(1.9770) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 31.2688, Epoch Time 532.1297(532.8096), Bit/dim 3.7912(best: 3.7911), Xent 1.1337, Loss 4.3581, Error 0.4072(best: 0.4088)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 80.4810(81.1929) | Bit/dim 3.7936(3.7942) | Xent 1.0824(1.1199) | Loss 4.3348(4.3542) | Error 0.3795(0.3986) Steps 886(902.74) | Grad Norm 1.6267(1.9665) | Total Time 14.00(14.00)\n",
      "Iter 0902 | Time 80.4660(81.1711) | Bit/dim 3.7866(3.7940) | Xent 1.1118(1.1197) | Loss 4.3425(4.3538) | Error 0.3922(0.3984) Steps 868(901.70) | Grad Norm 1.3804(1.9489) | Total Time 14.00(14.00)\n",
      "Iter 0903 | Time 82.4324(81.2089) | Bit/dim 3.7852(3.7937) | Xent 1.0975(1.1190) | Loss 4.3340(4.3532) | Error 0.3931(0.3983) Steps 922(902.31) | Grad Norm 1.6485(1.9399) | Total Time 14.00(14.00)\n",
      "Iter 0904 | Time 81.2132(81.2090) | Bit/dim 3.7941(3.7938) | Xent 1.0809(1.1179) | Loss 4.3346(4.3527) | Error 0.3849(0.3979) Steps 904(902.36) | Grad Norm 2.2254(1.9484) | Total Time 14.00(14.00)\n",
      "Iter 0905 | Time 80.4422(81.1860) | Bit/dim 3.8027(3.7940) | Xent 1.0978(1.1172) | Loss 4.3516(4.3527) | Error 0.3882(0.3976) Steps 904(902.41) | Grad Norm 2.3373(1.9601) | Total Time 14.00(14.00)\n",
      "Iter 0906 | Time 88.4838(81.4050) | Bit/dim 3.7718(3.7934) | Xent 1.0854(1.1163) | Loss 4.3145(4.3515) | Error 0.3871(0.3973) Steps 946(903.72) | Grad Norm 1.3787(1.9427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 31.9879, Epoch Time 541.1178(533.0588), Bit/dim 3.7889(best: 3.7911), Xent 1.1308, Loss 4.3542, Error 0.4059(best: 0.4072)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 81.2862(81.4014) | Bit/dim 3.7934(3.7934) | Xent 1.0734(1.1150) | Loss 4.3301(4.3509) | Error 0.3824(0.3968) Steps 886(903.18) | Grad Norm 1.7895(1.9381) | Total Time 14.00(14.00)\n",
      "Iter 0908 | Time 80.7077(81.3806) | Bit/dim 3.7863(3.7931) | Xent 1.0968(1.1145) | Loss 4.3347(4.3504) | Error 0.3955(0.3968) Steps 898(903.03) | Grad Norm 3.2469(1.9773) | Total Time 14.00(14.00)\n",
      "Iter 0909 | Time 80.5352(81.3552) | Bit/dim 3.7856(3.7929) | Xent 1.1226(1.1147) | Loss 4.3469(4.3503) | Error 0.3901(0.3966) Steps 892(902.70) | Grad Norm 6.1399(2.1022) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 79.9267(81.3124) | Bit/dim 3.7925(3.7929) | Xent 1.1412(1.1155) | Loss 4.3631(4.3507) | Error 0.4052(0.3968) Steps 898(902.56) | Grad Norm 9.0805(2.3116) | Total Time 14.00(14.00)\n",
      "Iter 0911 | Time 81.5229(81.3187) | Bit/dim 3.7854(3.7927) | Xent 1.2251(1.1188) | Loss 4.3979(4.3521) | Error 0.4394(0.3981) Steps 892(902.24) | Grad Norm 9.3497(2.5227) | Total Time 14.00(14.00)\n",
      "Iter 0912 | Time 80.5421(81.2954) | Bit/dim 3.7859(3.7925) | Xent 1.1253(1.1190) | Loss 4.3485(4.3520) | Error 0.4035(0.3983) Steps 922(902.83) | Grad Norm 3.2237(2.5437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 31.4069, Epoch Time 531.4559(533.0108), Bit/dim 3.7893(best: 3.7889), Xent 1.1909, Loss 4.3848, Error 0.4303(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 76.8145(81.1610) | Bit/dim 3.7938(3.7925) | Xent 1.1721(1.1206) | Loss 4.3798(4.3528) | Error 0.4177(0.3989) Steps 910(903.05) | Grad Norm 6.2655(2.6554) | Total Time 14.00(14.00)\n",
      "Iter 0914 | Time 80.2865(81.1347) | Bit/dim 3.7920(3.7925) | Xent 1.1161(1.1204) | Loss 4.3501(4.3527) | Error 0.3934(0.3987) Steps 910(903.26) | Grad Norm 4.0711(2.6979) | Total Time 14.00(14.00)\n",
      "Iter 0915 | Time 80.7355(81.1227) | Bit/dim 3.7925(3.7925) | Xent 1.1372(1.1209) | Loss 4.3611(4.3530) | Error 0.4021(0.3988) Steps 904(903.28) | Grad Norm 4.7581(2.7597) | Total Time 14.00(14.00)\n",
      "Iter 0916 | Time 84.9339(81.2371) | Bit/dim 3.7843(3.7923) | Xent 1.1155(1.1208) | Loss 4.3421(4.3526) | Error 0.3989(0.3988) Steps 916(903.66) | Grad Norm 3.9680(2.7959) | Total Time 14.00(14.00)\n",
      "Iter 0917 | Time 76.7128(81.1014) | Bit/dim 3.7889(3.7922) | Xent 1.1223(1.1208) | Loss 4.3500(4.3526) | Error 0.4009(0.3989) Steps 904(903.67) | Grad Norm 4.7116(2.8534) | Total Time 14.00(14.00)\n",
      "Iter 0918 | Time 80.0021(81.0684) | Bit/dim 3.7818(3.7918) | Xent 1.1145(1.1206) | Loss 4.3390(4.3522) | Error 0.3958(0.3988) Steps 892(903.32) | Grad Norm 2.8146(2.8522) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 31.4651, Epoch Time 526.6726(532.8206), Bit/dim 3.7900(best: 3.7889), Xent 1.1463, Loss 4.3631, Error 0.4164(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 84.7176(81.1778) | Bit/dim 3.7896(3.7918) | Xent 1.1246(1.1208) | Loss 4.3520(4.3522) | Error 0.3991(0.3988) Steps 904(903.34) | Grad Norm 3.8372(2.8818) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 80.9139(81.1699) | Bit/dim 3.8010(3.7921) | Xent 1.0993(1.1201) | Loss 4.3507(4.3521) | Error 0.3899(0.3985) Steps 916(903.72) | Grad Norm 2.3827(2.8668) | Total Time 14.00(14.00)\n",
      "Iter 0921 | Time 78.7881(81.0985) | Bit/dim 3.7827(3.7918) | Xent 1.1091(1.1198) | Loss 4.3373(4.3517) | Error 0.3924(0.3983) Steps 910(903.91) | Grad Norm 4.6872(2.9214) | Total Time 14.00(14.00)\n",
      "Iter 0922 | Time 82.9014(81.1526) | Bit/dim 3.7858(3.7916) | Xent 1.0960(1.1191) | Loss 4.3338(4.3511) | Error 0.3860(0.3980) Steps 958(905.53) | Grad Norm 2.1167(2.8973) | Total Time 14.00(14.00)\n",
      "Iter 0923 | Time 80.4000(81.1300) | Bit/dim 3.7934(3.7917) | Xent 1.1165(1.1190) | Loss 4.3517(4.3511) | Error 0.3976(0.3979) Steps 904(905.49) | Grad Norm 5.3961(2.9722) | Total Time 14.00(14.00)\n",
      "Iter 0924 | Time 81.6960(81.1470) | Bit/dim 3.7854(3.7915) | Xent 1.1028(1.1185) | Loss 4.3368(4.3507) | Error 0.3946(0.3978) Steps 904(905.44) | Grad Norm 2.4055(2.9552) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 30.9933, Epoch Time 535.6507(532.9055), Bit/dim 3.7920(best: 3.7889), Xent 1.1520, Loss 4.3680, Error 0.4118(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 77.1480(81.0270) | Bit/dim 3.7915(3.7915) | Xent 1.0896(1.1176) | Loss 4.3363(4.3503) | Error 0.3815(0.3974) Steps 892(905.04) | Grad Norm 5.5372(3.0327) | Total Time 14.00(14.00)\n",
      "Iter 0926 | Time 82.3926(81.0680) | Bit/dim 3.7863(3.7913) | Xent 1.0864(1.1167) | Loss 4.3295(4.3497) | Error 0.3845(0.3970) Steps 892(904.65) | Grad Norm 1.4116(2.9841) | Total Time 14.00(14.00)\n",
      "Iter 0927 | Time 83.3142(81.1354) | Bit/dim 3.7847(3.7911) | Xent 1.0886(1.1159) | Loss 4.3290(4.3490) | Error 0.3895(0.3967) Steps 898(904.45) | Grad Norm 4.6556(3.0342) | Total Time 14.00(14.00)\n",
      "Iter 0928 | Time 84.3419(81.2315) | Bit/dim 3.7871(3.7910) | Xent 1.0929(1.1152) | Loss 4.3335(4.3486) | Error 0.3904(0.3966) Steps 904(904.43) | Grad Norm 1.9118(3.0005) | Total Time 14.00(14.00)\n",
      "Iter 0929 | Time 83.3095(81.2939) | Bit/dim 3.7911(3.7910) | Xent 1.0915(1.1145) | Loss 4.3369(4.3482) | Error 0.3882(0.3963) Steps 922(904.96) | Grad Norm 2.0808(2.9729) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 80.8815(81.2815) | Bit/dim 3.7782(3.7906) | Xent 1.1011(1.1141) | Loss 4.3287(4.3476) | Error 0.3928(0.3962) Steps 904(904.93) | Grad Norm 2.0332(2.9448) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 31.3416, Epoch Time 538.4574(533.0721), Bit/dim 3.7867(best: 3.7889), Xent 1.1273, Loss 4.3503, Error 0.4066(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0931 | Time 78.9997(81.2131) | Bit/dim 3.7768(3.7902) | Xent 1.0777(1.1130) | Loss 4.3156(4.3467) | Error 0.3821(0.3958) Steps 904(904.90) | Grad Norm 0.9452(2.8848) | Total Time 14.00(14.00)\n",
      "Iter 0932 | Time 77.9635(81.1156) | Bit/dim 3.7943(3.7903) | Xent 1.0940(1.1124) | Loss 4.3413(4.3465) | Error 0.3872(0.3955) Steps 880(904.16) | Grad Norm 2.0674(2.8602) | Total Time 14.00(14.00)\n",
      "Iter 0933 | Time 77.9124(81.0195) | Bit/dim 3.7890(3.7903) | Xent 1.0885(1.1117) | Loss 4.3332(4.3461) | Error 0.3865(0.3952) Steps 910(904.33) | Grad Norm 2.7659(2.8574) | Total Time 14.00(14.00)\n",
      "Iter 0934 | Time 81.7722(81.0421) | Bit/dim 3.7860(3.7902) | Xent 1.0681(1.1104) | Loss 4.3200(4.3453) | Error 0.3831(0.3949) Steps 892(903.96) | Grad Norm 0.8746(2.7979) | Total Time 14.00(14.00)\n",
      "Iter 0935 | Time 82.2108(81.0771) | Bit/dim 3.7840(3.7900) | Xent 1.0914(1.1098) | Loss 4.3297(4.3449) | Error 0.3891(0.3947) Steps 880(903.24) | Grad Norm 2.3415(2.7842) | Total Time 14.00(14.00)\n",
      "Iter 0936 | Time 79.7778(81.0381) | Bit/dim 3.7797(3.7897) | Xent 1.0826(1.1090) | Loss 4.3211(4.3442) | Error 0.3810(0.3943) Steps 898(903.09) | Grad Norm 1.0772(2.7330) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 31.5310, Epoch Time 525.7232(532.8516), Bit/dim 3.7871(best: 3.7867), Xent 1.1184, Loss 4.3463, Error 0.4051(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0937 | Time 79.8181(81.0015) | Bit/dim 3.7837(3.7895) | Xent 1.0794(1.1081) | Loss 4.3235(4.3435) | Error 0.3811(0.3939) Steps 910(903.29) | Grad Norm 1.3955(2.6929) | Total Time 14.00(14.00)\n",
      "Iter 0938 | Time 79.4800(80.9559) | Bit/dim 3.7865(3.7894) | Xent 1.0599(1.1067) | Loss 4.3165(4.3427) | Error 0.3778(0.3934) Steps 874(902.41) | Grad Norm 0.8425(2.6374) | Total Time 14.00(14.00)\n",
      "Iter 0939 | Time 80.7598(80.9500) | Bit/dim 3.7857(3.7893) | Xent 1.0604(1.1053) | Loss 4.3159(4.3419) | Error 0.3811(0.3931) Steps 898(902.28) | Grad Norm 0.8783(2.5846) | Total Time 14.00(14.00)\n",
      "Iter 0940 | Time 83.7016(81.0326) | Bit/dim 3.7981(3.7895) | Xent 1.0837(1.1046) | Loss 4.3400(4.3419) | Error 0.3836(0.3928) Steps 886(901.79) | Grad Norm 2.7790(2.5904) | Total Time 14.00(14.00)\n",
      "Iter 0941 | Time 78.0469(80.9430) | Bit/dim 3.7848(3.7894) | Xent 1.0749(1.1037) | Loss 4.3222(4.3413) | Error 0.3834(0.3925) Steps 874(900.96) | Grad Norm 2.8623(2.5986) | Total Time 14.00(14.00)\n",
      "Iter 0942 | Time 77.6458(80.8441) | Bit/dim 3.7695(3.7888) | Xent 1.0956(1.1035) | Loss 4.3173(4.3406) | Error 0.3922(0.3925) Steps 874(900.15) | Grad Norm 3.5356(2.6267) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 31.1502, Epoch Time 526.2004(532.6521), Bit/dim 3.7857(best: 3.7867), Xent 1.1306, Loss 4.3510, Error 0.4090(best: 0.4051)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0943 | Time 81.7488(80.8712) | Bit/dim 3.7845(3.7887) | Xent 1.1160(1.1039) | Loss 4.3425(4.3406) | Error 0.3928(0.3925) Steps 874(899.37) | Grad Norm 4.8447(2.6933) | Total Time 14.00(14.00)\n",
      "Iter 0944 | Time 82.7162(80.9266) | Bit/dim 3.7814(3.7885) | Xent 1.0770(1.1031) | Loss 4.3199(4.3400) | Error 0.3849(0.3923) Steps 916(899.87) | Grad Norm 4.9938(2.7623) | Total Time 14.00(14.00)\n",
      "Iter 0945 | Time 81.8968(80.9557) | Bit/dim 3.7744(3.7880) | Xent 1.0774(1.1023) | Loss 4.3131(4.3392) | Error 0.3899(0.3922) Steps 934(900.89) | Grad Norm 2.8522(2.7650) | Total Time 14.00(14.00)\n",
      "Iter 0946 | Time 81.0407(80.9582) | Bit/dim 3.7893(3.7881) | Xent 1.0725(1.1014) | Loss 4.3256(4.3388) | Error 0.3782(0.3918) Steps 892(900.62) | Grad Norm 1.9922(2.7418) | Total Time 14.00(14.00)\n",
      "Iter 0947 | Time 79.7992(80.9235) | Bit/dim 3.7883(3.7881) | Xent 1.0760(1.1006) | Loss 4.3264(4.3384) | Error 0.3801(0.3914) Steps 880(900.00) | Grad Norm 1.7735(2.7127) | Total Time 14.00(14.00)\n",
      "Iter 0948 | Time 87.4862(81.1203) | Bit/dim 3.7803(3.7879) | Xent 1.0982(1.1006) | Loss 4.3294(4.3381) | Error 0.3879(0.3913) Steps 898(899.94) | Grad Norm 3.2081(2.7276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 31.3030, Epoch Time 541.5789(532.9199), Bit/dim 3.7848(best: 3.7857), Xent 1.1199, Loss 4.3447, Error 0.4047(best: 0.4051)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0949 | Time 79.5718(81.0739) | Bit/dim 3.7752(3.7875) | Xent 1.0698(1.0996) | Loss 4.3101(4.3373) | Error 0.3810(0.3910) Steps 898(899.89) | Grad Norm 2.9868(2.7354) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 79.9597(81.0405) | Bit/dim 3.7825(3.7873) | Xent 1.0616(1.0985) | Loss 4.3133(4.3366) | Error 0.3752(0.3905) Steps 898(899.83) | Grad Norm 0.9064(2.6805) | Total Time 14.00(14.00)\n",
      "Iter 0951 | Time 82.0134(81.0696) | Bit/dim 3.7840(3.7872) | Xent 1.0770(1.0978) | Loss 4.3225(4.3361) | Error 0.3866(0.3904) Steps 874(899.05) | Grad Norm 2.0410(2.6613) | Total Time 14.00(14.00)\n",
      "Iter 0952 | Time 81.9601(81.0964) | Bit/dim 3.7784(3.7870) | Xent 1.0644(1.0968) | Loss 4.3106(4.3354) | Error 0.3788(0.3901) Steps 916(899.56) | Grad Norm 2.1727(2.6467) | Total Time 14.00(14.00)\n",
      "Iter 0953 | Time 81.8376(81.1186) | Bit/dim 3.7905(3.7871) | Xent 1.0725(1.0961) | Loss 4.3268(4.3351) | Error 0.3860(0.3899) Steps 904(899.70) | Grad Norm 2.2868(2.6359) | Total Time 14.00(14.00)\n",
      "Iter 0954 | Time 85.4819(81.2495) | Bit/dim 3.7846(3.7870) | Xent 1.0618(1.0951) | Loss 4.3155(4.3345) | Error 0.3699(0.3893) Steps 934(900.72) | Grad Norm 3.9989(2.6768) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 31.3620, Epoch Time 538.0580(533.0740), Bit/dim 3.7850(best: 3.7848), Xent 1.1273, Loss 4.3486, Error 0.4058(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0955 | Time 85.1089(81.3653) | Bit/dim 3.7809(3.7868) | Xent 1.0756(1.0945) | Loss 4.3188(4.3341) | Error 0.3775(0.3890) Steps 922(901.36) | Grad Norm 4.9942(2.7463) | Total Time 14.00(14.00)\n",
      "Iter 0956 | Time 76.6488(81.2238) | Bit/dim 3.7794(3.7866) | Xent 1.0999(1.0947) | Loss 4.3293(4.3339) | Error 0.3936(0.3891) Steps 892(901.08) | Grad Norm 5.8401(2.8391) | Total Time 14.00(14.00)\n",
      "Iter 0957 | Time 81.0695(81.2191) | Bit/dim 3.7822(3.7865) | Xent 1.1217(1.0955) | Loss 4.3431(4.3342) | Error 0.4031(0.3895) Steps 892(900.81) | Grad Norm 6.2396(2.9411) | Total Time 14.00(14.00)\n",
      "Iter 0958 | Time 83.0714(81.2747) | Bit/dim 3.7904(3.7866) | Xent 1.0599(1.0944) | Loss 4.3204(4.3338) | Error 0.3759(0.3891) Steps 892(900.55) | Grad Norm 2.5673(2.9299) | Total Time 14.00(14.00)\n",
      "Iter 0959 | Time 81.9120(81.2938) | Bit/dim 3.7821(3.7864) | Xent 1.0755(1.0938) | Loss 4.3198(4.3334) | Error 0.3809(0.3889) Steps 892(900.29) | Grad Norm 4.9758(2.9913) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 78.3916(81.2068) | Bit/dim 3.7789(3.7862) | Xent 1.1006(1.0940) | Loss 4.3292(4.3332) | Error 0.3960(0.3891) Steps 898(900.22) | Grad Norm 7.0129(3.1119) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 31.6147, Epoch Time 533.0815(533.0742), Bit/dim 3.7806(best: 3.7848), Xent 1.1156, Loss 4.3384, Error 0.4026(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0961 | Time 78.0206(81.1112) | Bit/dim 3.7780(3.7860) | Xent 1.0694(1.0933) | Loss 4.3127(4.3326) | Error 0.3840(0.3889) Steps 904(900.33) | Grad Norm 1.1673(3.0536) | Total Time 14.00(14.00)\n",
      "Iter 0962 | Time 82.4652(81.1518) | Bit/dim 3.7827(3.7859) | Xent 1.1050(1.0937) | Loss 4.3352(4.3327) | Error 0.3985(0.3892) Steps 892(900.08) | Grad Norm 5.6658(3.1320) | Total Time 14.00(14.00)\n",
      "Iter 0963 | Time 83.2676(81.2153) | Bit/dim 3.7795(3.7857) | Xent 1.1010(1.0939) | Loss 4.3300(4.3326) | Error 0.3855(0.3891) Steps 856(898.76) | Grad Norm 4.2667(3.1660) | Total Time 14.00(14.00)\n",
      "Iter 0964 | Time 80.1160(81.1823) | Bit/dim 3.7829(3.7856) | Xent 1.0611(1.0929) | Loss 4.3134(4.3320) | Error 0.3766(0.3887) Steps 886(898.38) | Grad Norm 3.2424(3.1683) | Total Time 14.00(14.00)\n",
      "Iter 0965 | Time 80.7502(81.1693) | Bit/dim 3.7851(3.7856) | Xent 1.1186(1.0937) | Loss 4.3444(4.3324) | Error 0.4044(0.3892) Steps 892(898.19) | Grad Norm 6.8432(3.2785) | Total Time 14.00(14.00)\n",
      "Iter 0966 | Time 81.5438(81.1806) | Bit/dim 3.7814(3.7855) | Xent 1.0909(1.0936) | Loss 4.3269(4.3322) | Error 0.3845(0.3891) Steps 922(898.90) | Grad Norm 3.7797(3.2936) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 31.4463, Epoch Time 533.6545(533.0917), Bit/dim 3.7828(best: 3.7806), Xent 1.1500, Loss 4.3578, Error 0.4169(best: 0.4026)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0967 | Time 82.0015(81.2052) | Bit/dim 3.7881(3.7855) | Xent 1.1005(1.0938) | Loss 4.3383(4.3324) | Error 0.3875(0.3890) Steps 868(897.97) | Grad Norm 6.0605(3.3766) | Total Time 14.00(14.00)\n",
      "Iter 0968 | Time 80.0507(81.1706) | Bit/dim 3.7777(3.7853) | Xent 1.0854(1.0935) | Loss 4.3204(4.3321) | Error 0.3918(0.3891) Steps 874(897.26) | Grad Norm 3.6655(3.3852) | Total Time 14.00(14.00)\n",
      "Iter 0969 | Time 80.6308(81.1544) | Bit/dim 3.7785(3.7851) | Xent 1.0931(1.0935) | Loss 4.3251(4.3319) | Error 0.3852(0.3890) Steps 874(896.56) | Grad Norm 4.7297(3.4256) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 81.3103(81.1590) | Bit/dim 3.7770(3.7849) | Xent 1.0800(1.0931) | Loss 4.3170(4.3314) | Error 0.3854(0.3889) Steps 910(896.96) | Grad Norm 3.8728(3.4390) | Total Time 14.00(14.00)\n",
      "Iter 0971 | Time 84.0608(81.2461) | Bit/dim 3.7774(3.7846) | Xent 1.0839(1.0928) | Loss 4.3194(4.3310) | Error 0.3832(0.3887) Steps 904(897.17) | Grad Norm 4.5878(3.4735) | Total Time 14.00(14.00)\n",
      "Iter 0972 | Time 81.9487(81.2672) | Bit/dim 3.7892(3.7848) | Xent 1.0696(1.0921) | Loss 4.3240(4.3308) | Error 0.3766(0.3884) Steps 868(896.30) | Grad Norm 4.9999(3.5192) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 31.5595, Epoch Time 537.6773(533.2292), Bit/dim 3.7803(best: 3.7806), Xent 1.1175, Loss 4.3390, Error 0.4072(best: 0.4026)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0973 | Time 81.4157(81.2716) | Bit/dim 3.7773(3.7845) | Xent 1.0735(1.0916) | Loss 4.3140(4.3303) | Error 0.3810(0.3881) Steps 886(895.99) | Grad Norm 3.0157(3.5041) | Total Time 14.00(14.00)\n",
      "Iter 0974 | Time 78.1000(81.1765) | Bit/dim 3.7717(3.7842) | Xent 1.0665(1.0908) | Loss 4.3050(4.3296) | Error 0.3808(0.3879) Steps 880(895.51) | Grad Norm 4.6140(3.5374) | Total Time 14.00(14.00)\n",
      "Iter 0975 | Time 78.6606(81.1010) | Bit/dim 3.7811(3.7841) | Xent 1.0700(1.0902) | Loss 4.3161(4.3292) | Error 0.3781(0.3876) Steps 886(895.22) | Grad Norm 2.8681(3.5174) | Total Time 14.00(14.00)\n",
      "Iter 0976 | Time 82.7272(81.1498) | Bit/dim 3.7884(3.7842) | Xent 1.0655(1.0895) | Loss 4.3212(4.3289) | Error 0.3800(0.3874) Steps 856(894.05) | Grad Norm 3.7674(3.5249) | Total Time 14.00(14.00)\n",
      "Iter 0977 | Time 79.4606(81.0991) | Bit/dim 3.7824(3.7841) | Xent 1.0856(1.0893) | Loss 4.3252(4.3288) | Error 0.3846(0.3873) Steps 880(893.63) | Grad Norm 3.1246(3.5129) | Total Time 14.00(14.00)\n",
      "Iter 0978 | Time 81.0944(81.0990) | Bit/dim 3.7777(3.7839) | Xent 1.0621(1.0885) | Loss 4.3087(4.3282) | Error 0.3745(0.3869) Steps 880(893.22) | Grad Norm 3.4521(3.5110) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 31.2322, Epoch Time 528.1819(533.0778), Bit/dim 3.7811(best: 3.7803), Xent 1.1261, Loss 4.3441, Error 0.4066(best: 0.4026)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0979 | Time 80.0930(81.0688) | Bit/dim 3.7762(3.7837) | Xent 1.0642(1.0878) | Loss 4.3083(4.3276) | Error 0.3825(0.3868) Steps 904(893.54) | Grad Norm 4.1054(3.5289) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 80.4887(81.0514) | Bit/dim 3.7809(3.7836) | Xent 1.0509(1.0867) | Loss 4.3063(4.3270) | Error 0.3791(0.3866) Steps 904(893.85) | Grad Norm 2.0882(3.4856) | Total Time 14.00(14.00)\n",
      "Iter 0981 | Time 81.4124(81.0622) | Bit/dim 3.7789(3.7835) | Xent 1.0462(1.0855) | Loss 4.3019(4.3262) | Error 0.3665(0.3860) Steps 904(894.16) | Grad Norm 2.6407(3.4603) | Total Time 14.00(14.00)\n",
      "Iter 0982 | Time 80.4576(81.0441) | Bit/dim 3.7758(3.7833) | Xent 1.0825(1.0854) | Loss 4.3171(4.3260) | Error 0.3891(0.3861) Steps 880(893.73) | Grad Norm 3.7924(3.4703) | Total Time 14.00(14.00)\n",
      "Iter 0983 | Time 78.3298(80.9627) | Bit/dim 3.7827(3.7832) | Xent 1.0494(1.0843) | Loss 4.3074(4.3254) | Error 0.3740(0.3857) Steps 880(893.32) | Grad Norm 2.0156(3.4266) | Total Time 14.00(14.00)\n",
      "Iter 0984 | Time 79.3198(80.9134) | Bit/dim 3.7773(3.7831) | Xent 1.0478(1.0832) | Loss 4.3012(4.3247) | Error 0.3725(0.3853) Steps 898(893.46) | Grad Norm 1.6241(3.3725) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 30.8958, Epoch Time 526.9791(532.8948), Bit/dim 3.7799(best: 3.7803), Xent 1.1093, Loss 4.3346, Error 0.4002(best: 0.4026)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0985 | Time 79.9106(80.8833) | Bit/dim 3.7746(3.7828) | Xent 1.0502(1.0822) | Loss 4.2997(4.3239) | Error 0.3719(0.3849) Steps 910(893.96) | Grad Norm 2.4141(3.3438) | Total Time 14.00(14.00)\n",
      "Iter 0986 | Time 77.4487(80.7803) | Bit/dim 3.7841(3.7828) | Xent 1.0414(1.0810) | Loss 4.3048(4.3233) | Error 0.3691(0.3844) Steps 880(893.54) | Grad Norm 1.0662(3.2755) | Total Time 14.00(14.00)\n",
      "Iter 0987 | Time 78.4424(80.7101) | Bit/dim 3.7837(3.7829) | Xent 1.0299(1.0795) | Loss 4.2986(4.3226) | Error 0.3638(0.3838) Steps 904(893.85) | Grad Norm 1.3809(3.2186) | Total Time 14.00(14.00)\n",
      "Iter 0988 | Time 78.1428(80.6331) | Bit/dim 3.7776(3.7827) | Xent 1.0229(1.0778) | Loss 4.2891(4.3216) | Error 0.3660(0.3833) Steps 874(893.26) | Grad Norm 1.4121(3.1644) | Total Time 14.00(14.00)\n",
      "Iter 0989 | Time 75.1639(80.4690) | Bit/dim 3.7794(3.7826) | Xent 1.0426(1.0767) | Loss 4.3007(4.3210) | Error 0.3691(0.3828) Steps 892(893.22) | Grad Norm 2.1445(3.1338) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 80.2257(80.4617) | Bit/dim 3.7728(3.7823) | Xent 1.0561(1.0761) | Loss 4.3008(4.3204) | Error 0.3742(0.3826) Steps 874(892.64) | Grad Norm 3.2144(3.1362) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 31.1354, Epoch Time 516.2848(532.3965), Bit/dim 3.7800(best: 3.7799), Xent 1.1132, Loss 4.3366, Error 0.4044(best: 0.4002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0991 | Time 77.8657(80.3838) | Bit/dim 3.7776(3.7822) | Xent 1.0460(1.0752) | Loss 4.3006(4.3198) | Error 0.3694(0.3822) Steps 892(892.62) | Grad Norm 3.7530(3.1547) | Total Time 14.00(14.00)\n",
      "Iter 0992 | Time 79.3534(80.3529) | Bit/dim 3.7738(3.7819) | Xent 1.0323(1.0739) | Loss 4.2899(4.3189) | Error 0.3719(0.3819) Steps 886(892.42) | Grad Norm 4.5247(3.1958) | Total Time 14.00(14.00)\n",
      "Iter 0993 | Time 81.0926(80.3751) | Bit/dim 3.7790(3.7818) | Xent 1.0592(1.0735) | Loss 4.3085(4.3186) | Error 0.3825(0.3819) Steps 904(892.77) | Grad Norm 4.9978(3.2499) | Total Time 14.00(14.00)\n",
      "Iter 0994 | Time 80.8130(80.3883) | Bit/dim 3.7808(3.7818) | Xent 1.0641(1.0732) | Loss 4.3128(4.3184) | Error 0.3825(0.3819) Steps 868(892.03) | Grad Norm 4.0212(3.2730) | Total Time 14.00(14.00)\n",
      "Iter 0995 | Time 79.2755(80.3549) | Bit/dim 3.7777(3.7817) | Xent 1.0632(1.0729) | Loss 4.3093(4.3181) | Error 0.3721(0.3816) Steps 874(891.49) | Grad Norm 1.2611(3.2127) | Total Time 14.00(14.00)\n",
      "Iter 0996 | Time 77.9545(80.2829) | Bit/dim 3.7708(3.7814) | Xent 1.0462(1.0721) | Loss 4.2939(4.3174) | Error 0.3691(0.3812) Steps 868(890.78) | Grad Norm 3.4072(3.2185) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 31.5161, Epoch Time 523.7952(532.1385), Bit/dim 3.7774(best: 3.7799), Xent 1.0985, Loss 4.3267, Error 0.3983(best: 0.4002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0997 | Time 79.2922(80.2531) | Bit/dim 3.7784(3.7813) | Xent 1.0502(1.0714) | Loss 4.3035(4.3170) | Error 0.3714(0.3809) Steps 868(890.10) | Grad Norm 1.8668(3.1780) | Total Time 14.00(14.00)\n",
      "Iter 0998 | Time 82.7978(80.3295) | Bit/dim 3.7731(3.7810) | Xent 1.0338(1.0703) | Loss 4.2900(4.3162) | Error 0.3669(0.3805) Steps 886(889.98) | Grad Norm 2.6617(3.1625) | Total Time 14.00(14.00)\n",
      "Iter 0999 | Time 80.5919(80.3373) | Bit/dim 3.7718(3.7807) | Xent 1.0354(1.0692) | Loss 4.2895(4.3154) | Error 0.3785(0.3805) Steps 904(890.40) | Grad Norm 3.1739(3.1628) | Total Time 14.00(14.00)\n",
      "Iter 1000 | Time 79.8696(80.3233) | Bit/dim 3.7782(3.7807) | Xent 1.0494(1.0687) | Loss 4.3029(4.3150) | Error 0.3752(0.3803) Steps 886(890.27) | Grad Norm 5.4614(3.2318) | Total Time 14.00(14.00)\n",
      "Iter 1001 | Time 79.9799(80.3130) | Bit/dim 3.7762(3.7805) | Xent 1.1012(1.0696) | Loss 4.3268(4.3153) | Error 0.3975(0.3808) Steps 904(890.68) | Grad Norm 8.2314(3.3818) | Total Time 14.00(14.00)\n",
      "Iter 1002 | Time 83.7527(80.4162) | Bit/dim 3.7887(3.7808) | Xent 1.1282(1.0714) | Loss 4.3528(4.3165) | Error 0.4014(0.3814) Steps 886(890.54) | Grad Norm 9.1800(3.5557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 31.2980, Epoch Time 533.3776(532.1757), Bit/dim 3.7812(best: 3.7774), Xent 1.1242, Loss 4.3433, Error 0.4097(best: 0.3983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1003 | Time 82.2144(80.4701) | Bit/dim 3.7787(3.7807) | Xent 1.0573(1.0710) | Loss 4.3073(4.3162) | Error 0.3785(0.3814) Steps 880(890.22) | Grad Norm 6.4276(3.6419) | Total Time 14.00(14.00)\n",
      "Iter 1004 | Time 77.3246(80.3758) | Bit/dim 3.7891(3.7810) | Xent 1.0939(1.0716) | Loss 4.3360(4.3168) | Error 0.3884(0.3816) Steps 874(889.73) | Grad Norm 4.3669(3.6636) | Total Time 14.00(14.00)\n",
      "Iter 1005 | Time 76.4027(80.2566) | Bit/dim 3.7792(3.7809) | Xent 1.0609(1.0713) | Loss 4.3097(4.3166) | Error 0.3745(0.3814) Steps 856(888.72) | Grad Norm 5.7737(3.7269) | Total Time 14.00(14.00)\n",
      "Iter 1006 | Time 78.0995(80.1919) | Bit/dim 3.7772(3.7808) | Xent 1.0727(1.0714) | Loss 4.3135(4.3165) | Error 0.3812(0.3813) Steps 880(888.46) | Grad Norm 4.6896(3.7558) | Total Time 14.00(14.00)\n",
      "Iter 1007 | Time 78.2387(80.1333) | Bit/dim 3.7718(3.7805) | Xent 1.0849(1.0718) | Loss 4.3142(4.3164) | Error 0.3842(0.3814) Steps 868(887.85) | Grad Norm 7.4329(3.8661) | Total Time 14.00(14.00)\n",
      "Iter 1008 | Time 81.0609(80.1611) | Bit/dim 3.7699(3.7802) | Xent 1.0711(1.0718) | Loss 4.3054(4.3161) | Error 0.3775(0.3813) Steps 880(887.61) | Grad Norm 1.8618(3.8060) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 30.8042, Epoch Time 519.6346(531.7994), Bit/dim 3.7781(best: 3.7774), Xent 1.1311, Loss 4.3437, Error 0.4090(best: 0.3983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1009 | Time 82.7013(80.2373) | Bit/dim 3.7755(3.7801) | Xent 1.0858(1.0722) | Loss 4.3184(4.3162) | Error 0.3794(0.3813) Steps 892(887.74) | Grad Norm 5.3107(3.8511) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 81.8059(80.2844) | Bit/dim 3.7799(3.7801) | Xent 1.0394(1.0712) | Loss 4.2996(4.3157) | Error 0.3720(0.3810) Steps 880(887.51) | Grad Norm 2.3881(3.8072) | Total Time 14.00(14.00)\n",
      "Iter 1011 | Time 79.8252(80.2706) | Bit/dim 3.7715(3.7798) | Xent 1.0755(1.0713) | Loss 4.3093(4.3155) | Error 0.3784(0.3809) Steps 910(888.19) | Grad Norm 5.1736(3.8482) | Total Time 14.00(14.00)\n",
      "Iter 1012 | Time 78.7064(80.2237) | Bit/dim 3.7798(3.7798) | Xent 1.0413(1.0704) | Loss 4.3005(4.3150) | Error 0.3688(0.3805) Steps 886(888.12) | Grad Norm 2.4224(3.8055) | Total Time 14.00(14.00)\n",
      "Iter 1013 | Time 81.2138(80.2534) | Bit/dim 3.7753(3.7797) | Xent 1.0527(1.0699) | Loss 4.3017(4.3146) | Error 0.3741(0.3803) Steps 898(888.42) | Grad Norm 5.5038(3.8564) | Total Time 14.00(14.00)\n",
      "Iter 1014 | Time 79.7971(80.2397) | Bit/dim 3.7785(3.7796) | Xent 1.0328(1.0688) | Loss 4.2949(4.3140) | Error 0.3700(0.3800) Steps 892(888.52) | Grad Norm 2.4446(3.8141) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 31.2388, Epoch Time 531.0162(531.7759), Bit/dim 3.7810(best: 3.7774), Xent 1.1163, Loss 4.3391, Error 0.4014(best: 0.3983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1015 | Time 78.5379(80.1886) | Bit/dim 3.7715(3.7794) | Xent 1.0440(1.0680) | Loss 4.2935(4.3134) | Error 0.3730(0.3798) Steps 898(888.81) | Grad Norm 6.6006(3.8977) | Total Time 14.00(14.00)\n",
      "Iter 1016 | Time 80.3761(80.1943) | Bit/dim 3.7788(3.7794) | Xent 1.0456(1.0674) | Loss 4.3016(4.3131) | Error 0.3725(0.3796) Steps 892(888.90) | Grad Norm 4.5214(3.9164) | Total Time 14.00(14.00)\n",
      "Iter 1017 | Time 78.1981(80.1344) | Bit/dim 3.7878(3.7796) | Xent 1.1012(1.0684) | Loss 4.3384(4.3138) | Error 0.3930(0.3800) Steps 856(887.92) | Grad Norm 6.6954(3.9997) | Total Time 14.00(14.00)\n",
      "Iter 1018 | Time 78.2096(80.0766) | Bit/dim 3.7721(3.7794) | Xent 1.0457(1.0677) | Loss 4.2950(4.3133) | Error 0.3711(0.3797) Steps 868(887.32) | Grad Norm 3.1716(3.9749) | Total Time 14.00(14.00)\n",
      "Iter 1019 | Time 80.5627(80.0912) | Bit/dim 3.7829(3.7795) | Xent 1.0451(1.0670) | Loss 4.3054(4.3130) | Error 0.3781(0.3797) Steps 868(886.74) | Grad Norm 5.0713(4.0078) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 84.2292(80.2154) | Bit/dim 3.7692(3.7792) | Xent 1.0355(1.0661) | Loss 4.2870(4.3122) | Error 0.3705(0.3794) Steps 886(886.72) | Grad Norm 4.2413(4.0148) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 31.0087, Epoch Time 526.7145(531.6241), Bit/dim 3.7762(best: 3.7774), Xent 1.0972, Loss 4.3248, Error 0.3983(best: 0.3983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1021 | Time 77.7604(80.1417) | Bit/dim 3.7761(3.7791) | Xent 1.0522(1.0657) | Loss 4.3022(4.3119) | Error 0.3756(0.3793) Steps 886(886.70) | Grad Norm 3.5272(4.0002) | Total Time 14.00(14.00)\n",
      "Iter 1022 | Time 76.3176(80.0270) | Bit/dim 3.7821(3.7792) | Xent 1.0415(1.0649) | Loss 4.3028(4.3117) | Error 0.3754(0.3792) Steps 862(885.96) | Grad Norm 4.4098(4.0125) | Total Time 14.00(14.00)\n",
      "Iter 1023 | Time 80.7273(80.0480) | Bit/dim 3.7789(3.7792) | Xent 1.0485(1.0644) | Loss 4.3032(4.3114) | Error 0.3744(0.3790) Steps 856(885.06) | Grad Norm 4.5377(4.0282) | Total Time 14.00(14.00)\n",
      "Iter 1024 | Time 81.1258(80.0803) | Bit/dim 3.7711(3.7789) | Xent 1.0583(1.0643) | Loss 4.3003(4.3111) | Error 0.3729(0.3789) Steps 892(885.26) | Grad Norm 5.6552(4.0770) | Total Time 14.00(14.00)\n",
      "Iter 1025 | Time 82.1848(80.1435) | Bit/dim 3.7695(3.7787) | Xent 1.0278(1.0632) | Loss 4.2835(4.3102) | Error 0.3650(0.3784) Steps 898(885.65) | Grad Norm 1.7155(4.0062) | Total Time 14.00(14.00)\n",
      "Iter 1026 | Time 80.5502(80.1557) | Bit/dim 3.7735(3.7785) | Xent 1.0559(1.0629) | Loss 4.3014(4.3100) | Error 0.3801(0.3785) Steps 874(885.30) | Grad Norm 6.2674(4.0740) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 31.3597, Epoch Time 525.5148(531.4408), Bit/dim 3.7767(best: 3.7762), Xent 1.1025, Loss 4.3279, Error 0.3946(best: 0.3983)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1027 | Time 75.9514(80.0295) | Bit/dim 3.7823(3.7786) | Xent 1.0514(1.0626) | Loss 4.3080(4.3099) | Error 0.3746(0.3784) Steps 880(885.14) | Grad Norm 5.7114(4.1231) | Total Time 14.00(14.00)\n",
      "Iter 1028 | Time 80.3320(80.0386) | Bit/dim 3.7751(3.7785) | Xent 1.0539(1.0623) | Loss 4.3021(4.3097) | Error 0.3720(0.3782) Steps 880(884.98) | Grad Norm 3.6395(4.1086) | Total Time 14.00(14.00)\n",
      "Iter 1029 | Time 77.5483(79.9639) | Bit/dim 3.7781(3.7785) | Xent 1.0308(1.0614) | Loss 4.2935(4.3092) | Error 0.3610(0.3777) Steps 892(885.19) | Grad Norm 3.9384(4.1035) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 75.2388(79.8221) | Bit/dim 3.7619(3.7780) | Xent 1.0417(1.0608) | Loss 4.2827(4.3084) | Error 0.3665(0.3773) Steps 868(884.68) | Grad Norm 4.5393(4.1166) | Total Time 14.00(14.00)\n",
      "Iter 1031 | Time 79.5041(79.8126) | Bit/dim 3.7685(3.7777) | Xent 1.0538(1.0606) | Loss 4.2954(4.3080) | Error 0.3741(0.3772) Steps 880(884.54) | Grad Norm 6.0358(4.1742) | Total Time 14.00(14.00)\n",
      "Iter 1032 | Time 79.2150(79.7947) | Bit/dim 3.7787(3.7778) | Xent 1.0205(1.0594) | Loss 4.2890(4.3074) | Error 0.3658(0.3769) Steps 874(884.22) | Grad Norm 2.7893(4.1326) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 30.9184, Epoch Time 514.2727(530.9258), Bit/dim 3.7754(best: 3.7762), Xent 1.0999, Loss 4.3253, Error 0.3952(best: 0.3946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1033 | Time 79.5832(79.7883) | Bit/dim 3.7827(3.7779) | Xent 1.0273(1.0584) | Loss 4.2964(4.3071) | Error 0.3692(0.3767) Steps 886(884.28) | Grad Norm 4.2117(4.1350) | Total Time 14.00(14.00)\n",
      "Iter 1034 | Time 77.9838(79.7342) | Bit/dim 3.7766(3.7779) | Xent 1.0372(1.0578) | Loss 4.2951(4.3068) | Error 0.3624(0.3762) Steps 880(884.15) | Grad Norm 3.1623(4.1058) | Total Time 14.00(14.00)\n",
      "Iter 1035 | Time 79.5637(79.7291) | Bit/dim 3.7652(3.7775) | Xent 1.0218(1.0567) | Loss 4.2761(4.3058) | Error 0.3658(0.3759) Steps 892(884.38) | Grad Norm 5.0263(4.1334) | Total Time 14.00(14.00)\n",
      "Iter 1036 | Time 82.4226(79.8099) | Bit/dim 3.7649(3.7771) | Xent 1.0268(1.0558) | Loss 4.2783(4.3050) | Error 0.3640(0.3756) Steps 886(884.43) | Grad Norm 0.9056(4.0366) | Total Time 14.00(14.00)\n",
      "Iter 1037 | Time 80.8113(79.8399) | Bit/dim 3.7739(3.7770) | Xent 1.0314(1.0551) | Loss 4.2896(4.3046) | Error 0.3715(0.3754) Steps 880(884.30) | Grad Norm 5.4078(4.0777) | Total Time 14.00(14.00)\n",
      "Iter 1038 | Time 80.4629(79.8586) | Bit/dim 3.7716(3.7768) | Xent 1.0275(1.0543) | Loss 4.2854(4.3040) | Error 0.3611(0.3750) Steps 886(884.35) | Grad Norm 3.3867(4.0570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 31.0820, Epoch Time 527.6154(530.8265), Bit/dim 3.7737(best: 3.7754), Xent 1.0958, Loss 4.3216, Error 0.3965(best: 0.3946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1039 | Time 82.5912(79.9406) | Bit/dim 3.7823(3.7770) | Xent 1.0257(1.0534) | Loss 4.2952(4.3037) | Error 0.3692(0.3748) Steps 904(884.94) | Grad Norm 3.6489(4.0448) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 80.8681(79.9684) | Bit/dim 3.7727(3.7769) | Xent 1.0277(1.0526) | Loss 4.2865(4.3032) | Error 0.3689(0.3747) Steps 862(884.25) | Grad Norm 3.2576(4.0211) | Total Time 14.00(14.00)\n",
      "Iter 1041 | Time 78.5444(79.9257) | Bit/dim 3.7733(3.7768) | Xent 0.9878(1.0507) | Loss 4.2672(4.3021) | Error 0.3585(0.3742) Steps 868(883.76) | Grad Norm 2.1892(3.9662) | Total Time 14.00(14.00)\n",
      "Iter 1042 | Time 80.7636(79.9508) | Bit/dim 3.7629(3.7764) | Xent 1.0255(1.0499) | Loss 4.2757(4.3013) | Error 0.3616(0.3738) Steps 880(883.65) | Grad Norm 3.6054(3.9554) | Total Time 14.00(14.00)\n",
      "Iter 1043 | Time 80.6125(79.9707) | Bit/dim 3.7701(3.7762) | Xent 1.0180(1.0490) | Loss 4.2791(4.3007) | Error 0.3651(0.3735) Steps 886(883.72) | Grad Norm 3.2222(3.9334) | Total Time 14.00(14.00)\n",
      "Iter 1044 | Time 82.6585(80.0513) | Bit/dim 3.7737(3.7761) | Xent 1.0418(1.0488) | Loss 4.2947(4.3005) | Error 0.3769(0.3736) Steps 892(883.97) | Grad Norm 5.1423(3.9696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 31.2020, Epoch Time 532.7878(530.8853), Bit/dim 3.7710(best: 3.7737), Xent 1.0936, Loss 4.3178, Error 0.3913(best: 0.3946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1045 | Time 78.9900(80.0195) | Bit/dim 3.7735(3.7760) | Xent 1.0236(1.0480) | Loss 4.2853(4.3000) | Error 0.3612(0.3733) Steps 874(883.67) | Grad Norm 2.8658(3.9365) | Total Time 14.00(14.00)\n",
      "Iter 1046 | Time 81.3787(80.0603) | Bit/dim 3.7685(3.7758) | Xent 1.0127(1.0469) | Loss 4.2748(4.2993) | Error 0.3591(0.3728) Steps 868(883.20) | Grad Norm 1.9069(3.8756) | Total Time 14.00(14.00)\n",
      "Iter 1047 | Time 77.6773(79.9888) | Bit/dim 3.7670(3.7755) | Xent 1.0102(1.0458) | Loss 4.2721(4.2985) | Error 0.3600(0.3725) Steps 892(883.46) | Grad Norm 3.0936(3.8522) | Total Time 14.00(14.00)\n",
      "Iter 1048 | Time 81.7873(80.0427) | Bit/dim 3.7736(3.7755) | Xent 1.0010(1.0445) | Loss 4.2741(4.2977) | Error 0.3518(0.3718) Steps 904(884.08) | Grad Norm 5.0623(3.8885) | Total Time 14.00(14.00)\n",
      "Iter 1049 | Time 81.3837(80.0829) | Bit/dim 3.7811(3.7756) | Xent 1.1120(1.0465) | Loss 4.3371(4.2989) | Error 0.3948(0.3725) Steps 910(884.86) | Grad Norm 8.8697(4.0379) | Total Time 14.00(14.00)\n",
      "Iter 1050 | Time 78.7819(80.0439) | Bit/dim 3.7681(3.7754) | Xent 1.0354(1.0462) | Loss 4.2858(4.2985) | Error 0.3671(0.3724) Steps 880(884.71) | Grad Norm 6.7677(4.1198) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 30.8951, Epoch Time 526.2923(530.7475), Bit/dim 3.7702(best: 3.7710), Xent 1.0932, Loss 4.3168, Error 0.3925(best: 0.3913)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1051 | Time 79.0020(80.0127) | Bit/dim 3.7663(3.7751) | Xent 1.0202(1.0454) | Loss 4.2764(4.2978) | Error 0.3672(0.3722) Steps 868(884.21) | Grad Norm 2.8755(4.0825) | Total Time 14.00(14.00)\n",
      "Iter 1052 | Time 81.4187(80.0548) | Bit/dim 3.7718(3.7750) | Xent 1.0277(1.0449) | Loss 4.2857(4.2975) | Error 0.3646(0.3720) Steps 886(884.26) | Grad Norm 4.6195(4.0986) | Total Time 14.00(14.00)\n",
      "Iter 1053 | Time 78.9632(80.0221) | Bit/dim 3.7635(3.7747) | Xent 1.0118(1.0439) | Loss 4.2694(4.2966) | Error 0.3645(0.3718) Steps 874(883.96) | Grad Norm 1.8005(4.0296) | Total Time 14.00(14.00)\n",
      "Iter 1054 | Time 79.2902(80.0001) | Bit/dim 3.7739(3.7747) | Xent 1.0187(1.0431) | Loss 4.2832(4.2962) | Error 0.3642(0.3715) Steps 880(883.84) | Grad Norm 4.1915(4.0345) | Total Time 14.00(14.00)\n",
      "Iter 1055 | Time 77.8091(79.9344) | Bit/dim 3.7740(3.7747) | Xent 1.0006(1.0419) | Loss 4.2743(4.2956) | Error 0.3571(0.3711) Steps 856(883.00) | Grad Norm 1.9448(3.9718) | Total Time 14.00(14.00)\n",
      "Iter 1056 | Time 78.5125(79.8917) | Bit/dim 3.7780(3.7748) | Xent 1.0302(1.0415) | Loss 4.2931(4.2955) | Error 0.3650(0.3709) Steps 868(882.55) | Grad Norm 5.8902(4.0294) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 31.2838, Epoch Time 522.1000(530.4881), Bit/dim 3.7740(best: 3.7702), Xent 1.0873, Loss 4.3177, Error 0.3912(best: 0.3913)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1057 | Time 77.8135(79.8294) | Bit/dim 3.7701(3.7746) | Xent 0.9987(1.0402) | Loss 4.2694(4.2947) | Error 0.3576(0.3705) Steps 874(882.30) | Grad Norm 3.7348(4.0205) | Total Time 14.00(14.00)\n",
      "Iter 1058 | Time 77.9511(79.7731) | Bit/dim 3.7775(3.7747) | Xent 1.0126(1.0394) | Loss 4.2838(4.2944) | Error 0.3594(0.3702) Steps 862(881.69) | Grad Norm 3.5922(4.0077) | Total Time 14.00(14.00)\n",
      "Iter 1059 | Time 80.2006(79.7859) | Bit/dim 3.7669(3.7745) | Xent 0.9835(1.0377) | Loss 4.2586(4.2933) | Error 0.3478(0.3695) Steps 868(881.28) | Grad Norm 4.3774(4.0188) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 80.2008(79.7983) | Bit/dim 3.7618(3.7741) | Xent 1.0103(1.0369) | Loss 4.2669(4.2925) | Error 0.3562(0.3691) Steps 892(881.60) | Grad Norm 3.0701(3.9903) | Total Time 14.00(14.00)\n",
      "Iter 1061 | Time 78.4595(79.7582) | Bit/dim 3.7744(3.7741) | Xent 1.0337(1.0368) | Loss 4.2913(4.2925) | Error 0.3690(0.3691) Steps 892(881.91) | Grad Norm 4.7400(4.0128) | Total Time 14.00(14.00)\n",
      "Iter 1062 | Time 79.7187(79.7570) | Bit/dim 3.7710(3.7740) | Xent 1.0483(1.0371) | Loss 4.2952(4.2926) | Error 0.3728(0.3692) Steps 850(880.95) | Grad Norm 7.0396(4.1036) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 30.7386, Epoch Time 520.9517(530.2020), Bit/dim 3.7724(best: 3.7702), Xent 1.1545, Loss 4.3497, Error 0.4101(best: 0.3912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1063 | Time 80.5435(79.7806) | Bit/dim 3.7798(3.7742) | Xent 1.1018(1.0391) | Loss 4.3307(4.2937) | Error 0.3899(0.3698) Steps 868(880.56) | Grad Norm 10.3294(4.2904) | Total Time 14.00(14.00)\n",
      "Iter 1064 | Time 79.9244(79.7849) | Bit/dim 3.7736(3.7742) | Xent 1.0285(1.0388) | Loss 4.2879(4.2935) | Error 0.3671(0.3698) Steps 892(880.91) | Grad Norm 5.8507(4.3372) | Total Time 14.00(14.00)\n",
      "Iter 1065 | Time 79.1085(79.7646) | Bit/dim 3.7671(3.7739) | Xent 1.0280(1.0384) | Loss 4.2811(4.2932) | Error 0.3680(0.3697) Steps 856(880.16) | Grad Norm 3.6674(4.3171) | Total Time 14.00(14.00)\n",
      "Iter 1066 | Time 79.4851(79.7562) | Bit/dim 3.7652(3.7737) | Xent 1.0186(1.0378) | Loss 4.2745(4.2926) | Error 0.3601(0.3694) Steps 880(880.16) | Grad Norm 6.0133(4.3680) | Total Time 14.00(14.00)\n",
      "Iter 1067 | Time 78.1424(79.7078) | Bit/dim 3.7751(3.7737) | Xent 1.0403(1.0379) | Loss 4.2952(4.2927) | Error 0.3705(0.3694) Steps 862(879.61) | Grad Norm 3.4308(4.3399) | Total Time 14.00(14.00)\n",
      "Iter 1068 | Time 79.0671(79.6886) | Bit/dim 3.7564(3.7732) | Xent 1.0109(1.0371) | Loss 4.2619(4.2918) | Error 0.3592(0.3691) Steps 880(879.62) | Grad Norm 4.8195(4.3543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 31.4688, Epoch Time 523.7171(530.0075), Bit/dim 3.7723(best: 3.7702), Xent 1.0988, Loss 4.3217, Error 0.3945(best: 0.3912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1069 | Time 79.9761(79.6972) | Bit/dim 3.7740(3.7732) | Xent 1.0122(1.0364) | Loss 4.2801(4.2914) | Error 0.3554(0.3687) Steps 886(879.81) | Grad Norm 2.6866(4.3042) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 81.8588(79.7621) | Bit/dim 3.7609(3.7729) | Xent 1.0031(1.0354) | Loss 4.2624(4.2905) | Error 0.3610(0.3685) Steps 886(880.00) | Grad Norm 3.7189(4.2867) | Total Time 14.00(14.00)\n",
      "Iter 1071 | Time 80.2196(79.7758) | Bit/dim 3.7729(3.7729) | Xent 0.9985(1.0343) | Loss 4.2722(4.2900) | Error 0.3475(0.3679) Steps 874(879.82) | Grad Norm 2.6558(4.2377) | Total Time 14.00(14.00)\n",
      "Iter 1072 | Time 83.0618(79.8744) | Bit/dim 3.7716(3.7728) | Xent 1.0093(1.0335) | Loss 4.2762(4.2896) | Error 0.3558(0.3675) Steps 874(879.64) | Grad Norm 4.1346(4.2346) | Total Time 14.00(14.00)\n",
      "Iter 1073 | Time 77.8981(79.8151) | Bit/dim 3.7691(3.7727) | Xent 0.9801(1.0319) | Loss 4.2591(4.2887) | Error 0.3468(0.3669) Steps 886(879.84) | Grad Norm 1.7423(4.1599) | Total Time 14.00(14.00)\n",
      "Iter 1074 | Time 80.4191(79.8332) | Bit/dim 3.7726(3.7727) | Xent 1.0159(1.0314) | Loss 4.2806(4.2884) | Error 0.3644(0.3668) Steps 880(879.84) | Grad Norm 4.4780(4.1694) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 30.3603, Epoch Time 529.4112(529.9896), Bit/dim 3.7710(best: 3.7702), Xent 1.0950, Loss 4.3185, Error 0.3908(best: 0.3912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1075 | Time 80.0479(79.8396) | Bit/dim 3.7677(3.7726) | Xent 1.0105(1.0308) | Loss 4.2729(4.2880) | Error 0.3580(0.3665) Steps 862(879.31) | Grad Norm 3.7190(4.1559) | Total Time 14.00(14.00)\n",
      "Iter 1076 | Time 81.4643(79.8884) | Bit/dim 3.7777(3.7727) | Xent 0.9716(1.0290) | Loss 4.2635(4.2872) | Error 0.3479(0.3660) Steps 880(879.33) | Grad Norm 1.9115(4.0886) | Total Time 14.00(14.00)\n",
      "Iter 1077 | Time 79.9291(79.8896) | Bit/dim 3.7726(3.7727) | Xent 0.9983(1.0281) | Loss 4.2717(4.2868) | Error 0.3615(0.3658) Steps 868(878.99) | Grad Norm 4.2307(4.0928) | Total Time 14.00(14.00)\n",
      "Iter 1078 | Time 79.0109(79.8632) | Bit/dim 3.7658(3.7725) | Xent 0.9946(1.0271) | Loss 4.2631(4.2860) | Error 0.3539(0.3655) Steps 868(878.66) | Grad Norm 4.6735(4.1103) | Total Time 14.00(14.00)\n",
      "Iter 1079 | Time 80.8498(79.8928) | Bit/dim 3.7661(3.7723) | Xent 1.0287(1.0271) | Loss 4.2804(4.2859) | Error 0.3678(0.3656) Steps 874(878.52) | Grad Norm 6.0312(4.1679) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 78.8208(79.8607) | Bit/dim 3.7670(3.7721) | Xent 1.0823(1.0288) | Loss 4.3082(4.2865) | Error 0.3844(0.3661) Steps 886(878.74) | Grad Norm 8.5179(4.2984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 31.3919, Epoch Time 526.9369(529.8980), Bit/dim 3.7722(best: 3.7702), Xent 1.1426, Loss 4.3435, Error 0.4098(best: 0.3908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1081 | Time 82.1564(79.9295) | Bit/dim 3.7700(3.7721) | Xent 1.0649(1.0299) | Loss 4.3025(4.2870) | Error 0.3831(0.3666) Steps 868(878.42) | Grad Norm 7.8948(4.4063) | Total Time 14.00(14.00)\n",
      "Iter 1082 | Time 79.0532(79.9033) | Bit/dim 3.7648(3.7719) | Xent 1.0164(1.0295) | Loss 4.2730(4.2866) | Error 0.3616(0.3665) Steps 868(878.11) | Grad Norm 2.3633(4.3450) | Total Time 14.00(14.00)\n",
      "Iter 1083 | Time 77.5483(79.8326) | Bit/dim 3.7634(3.7716) | Xent 1.0327(1.0296) | Loss 4.2798(4.2864) | Error 0.3708(0.3666) Steps 862(877.62) | Grad Norm 5.1379(4.3688) | Total Time 14.00(14.00)\n",
      "Iter 1084 | Time 83.6364(79.9467) | Bit/dim 3.7717(3.7716) | Xent 1.0303(1.0296) | Loss 4.2869(4.2864) | Error 0.3658(0.3666) Steps 880(877.69) | Grad Norm 3.2706(4.3358) | Total Time 14.00(14.00)\n",
      "Iter 1085 | Time 80.9724(79.9775) | Bit/dim 3.7574(3.7712) | Xent 1.0212(1.0293) | Loss 4.2680(4.2859) | Error 0.3666(0.3666) Steps 880(877.76) | Grad Norm 4.2095(4.3320) | Total Time 14.00(14.00)\n",
      "Iter 1086 | Time 79.3173(79.9577) | Bit/dim 3.7712(3.7712) | Xent 1.0319(1.0294) | Loss 4.2872(4.2859) | Error 0.3625(0.3665) Steps 862(877.29) | Grad Norm 2.2856(4.2706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 30.6329, Epoch Time 529.1483(529.8755), Bit/dim 3.7691(best: 3.7702), Xent 1.0917, Loss 4.3149, Error 0.3940(best: 0.3908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1087 | Time 79.5470(79.9454) | Bit/dim 3.7750(3.7713) | Xent 1.0025(1.0286) | Loss 4.2763(4.2856) | Error 0.3530(0.3661) Steps 868(877.01) | Grad Norm 3.2600(4.2403) | Total Time 14.00(14.00)\n",
      "Iter 1088 | Time 82.2471(80.0144) | Bit/dim 3.7692(3.7712) | Xent 0.9820(1.0272) | Loss 4.2602(4.2848) | Error 0.3525(0.3657) Steps 904(877.82) | Grad Norm 2.3806(4.1845) | Total Time 14.00(14.00)\n",
      "Iter 1089 | Time 79.3131(79.9934) | Bit/dim 3.7755(3.7714) | Xent 1.0065(1.0266) | Loss 4.2787(4.2847) | Error 0.3599(0.3655) Steps 892(878.25) | Grad Norm 4.5539(4.1956) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 78.8962(79.9605) | Bit/dim 3.7625(3.7711) | Xent 1.0047(1.0259) | Loss 4.2648(4.2841) | Error 0.3534(0.3651) Steps 868(877.94) | Grad Norm 2.4799(4.1441) | Total Time 14.00(14.00)\n",
      "Iter 1091 | Time 81.0116(79.9920) | Bit/dim 3.7688(3.7710) | Xent 1.0013(1.0252) | Loss 4.2694(4.2836) | Error 0.3518(0.3647) Steps 868(877.64) | Grad Norm 4.5785(4.1572) | Total Time 14.00(14.00)\n",
      "Iter 1092 | Time 80.5507(80.0088) | Bit/dim 3.7606(3.7707) | Xent 0.9689(1.0235) | Loss 4.2451(4.2825) | Error 0.3425(0.3640) Steps 856(876.99) | Grad Norm 2.8487(4.1179) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 30.8362, Epoch Time 528.2975(529.8282), Bit/dim 3.7704(best: 3.7691), Xent 1.0950, Loss 4.3179, Error 0.3864(best: 0.3908)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1093 | Time 76.4000(79.9005) | Bit/dim 3.7589(3.7704) | Xent 1.0109(1.0231) | Loss 4.2643(4.2819) | Error 0.3615(0.3640) Steps 874(876.90) | Grad Norm 5.3823(4.1559) | Total Time 14.00(14.00)\n",
      "Iter 1094 | Time 79.1083(79.8767) | Bit/dim 3.7776(3.7706) | Xent 1.0146(1.0229) | Loss 4.2849(4.2820) | Error 0.3619(0.3639) Steps 886(877.18) | Grad Norm 6.4690(4.2252) | Total Time 14.00(14.00)\n",
      "Iter 1095 | Time 79.8609(79.8763) | Bit/dim 3.7774(3.7708) | Xent 1.0234(1.0229) | Loss 4.2890(4.2822) | Error 0.3646(0.3639) Steps 862(876.72) | Grad Norm 6.5694(4.2956) | Total Time 14.00(14.00)\n",
      "Iter 1096 | Time 82.0761(79.9422) | Bit/dim 3.7677(3.7707) | Xent 1.0030(1.0223) | Loss 4.2692(4.2818) | Error 0.3556(0.3637) Steps 868(876.46) | Grad Norm 5.9702(4.3458) | Total Time 14.00(14.00)\n",
      "Iter 1097 | Time 81.6719(79.9941) | Bit/dim 3.7688(3.7706) | Xent 0.9919(1.0214) | Loss 4.2648(4.2813) | Error 0.3502(0.3633) Steps 892(876.92) | Grad Norm 2.3724(4.2866) | Total Time 14.00(14.00)\n",
      "Iter 1098 | Time 80.1139(79.9977) | Bit/dim 3.7588(3.7703) | Xent 0.9962(1.0206) | Loss 4.2569(4.2806) | Error 0.3564(0.3631) Steps 892(877.38) | Grad Norm 4.1152(4.2815) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 30.7842, Epoch Time 526.1308(529.7172), Bit/dim 3.7679(best: 3.7691), Xent 1.1007, Loss 4.3182, Error 0.3912(best: 0.3864)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1099 | Time 78.9895(79.9675) | Bit/dim 3.7658(3.7701) | Xent 0.9835(1.0195) | Loss 4.2576(4.2799) | Error 0.3544(0.3628) Steps 892(877.82) | Grad Norm 3.7414(4.2653) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 83.1503(80.0630) | Bit/dim 3.7622(3.7699) | Xent 0.9743(1.0182) | Loss 4.2494(4.2790) | Error 0.3466(0.3623) Steps 898(878.42) | Grad Norm 3.7925(4.2511) | Total Time 14.00(14.00)\n",
      "Iter 1101 | Time 79.6786(80.0514) | Bit/dim 3.7584(3.7696) | Xent 1.0131(1.0180) | Loss 4.2650(4.2786) | Error 0.3559(0.3621) Steps 874(878.29) | Grad Norm 5.2059(4.2797) | Total Time 14.00(14.00)\n",
      "Iter 1102 | Time 80.9584(80.0786) | Bit/dim 3.7654(3.7694) | Xent 0.9840(1.0170) | Loss 4.2574(4.2779) | Error 0.3445(0.3616) Steps 892(878.70) | Grad Norm 1.9618(4.2102) | Total Time 14.00(14.00)\n",
      "Iter 1103 | Time 81.5496(80.1228) | Bit/dim 3.7706(3.7695) | Xent 0.9757(1.0157) | Loss 4.2585(4.2773) | Error 0.3466(0.3612) Steps 880(878.74) | Grad Norm 3.4685(4.1879) | Total Time 14.00(14.00)\n",
      "Iter 1104 | Time 80.7258(80.1409) | Bit/dim 3.7621(3.7693) | Xent 0.9652(1.0142) | Loss 4.2447(4.2764) | Error 0.3456(0.3607) Steps 874(878.60) | Grad Norm 3.3354(4.1624) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 30.8989, Epoch Time 531.5640(529.7726), Bit/dim 3.7665(best: 3.7679), Xent 1.0704, Loss 4.3017, Error 0.3838(best: 0.3864)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1105 | Time 77.6072(80.0649) | Bit/dim 3.7622(3.7690) | Xent 0.9670(1.0128) | Loss 4.2457(4.2754) | Error 0.3460(0.3602) Steps 874(878.46) | Grad Norm 1.9987(4.0974) | Total Time 14.00(14.00)\n",
      "Iter 1106 | Time 78.6134(80.0213) | Bit/dim 3.7654(3.7689) | Xent 0.9733(1.0116) | Loss 4.2520(4.2747) | Error 0.3464(0.3598) Steps 862(877.97) | Grad Norm 4.2109(4.1009) | Total Time 14.00(14.00)\n",
      "Iter 1107 | Time 81.6394(80.0699) | Bit/dim 3.7676(3.7689) | Xent 0.9717(1.0104) | Loss 4.2534(4.2741) | Error 0.3451(0.3594) Steps 874(877.85) | Grad Norm 2.7211(4.0595) | Total Time 14.00(14.00)\n",
      "Iter 1108 | Time 81.8140(80.1222) | Bit/dim 3.7622(3.7687) | Xent 0.9594(1.0089) | Loss 4.2419(4.2731) | Error 0.3450(0.3590) Steps 892(878.27) | Grad Norm 3.3904(4.0394) | Total Time 14.00(14.00)\n",
      "Iter 1109 | Time 80.1229(80.1222) | Bit/dim 3.7712(3.7688) | Xent 1.0088(1.0089) | Loss 4.2756(4.2732) | Error 0.3609(0.3590) Steps 868(877.96) | Grad Norm 5.1915(4.0740) | Total Time 14.00(14.00)\n",
      "Iter 1110 | Time 80.4904(80.1332) | Bit/dim 3.7595(3.7685) | Xent 0.9754(1.0079) | Loss 4.2473(4.2724) | Error 0.3485(0.3587) Steps 874(877.84) | Grad Norm 4.4876(4.0864) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 31.0781, Epoch Time 527.2783(529.6978), Bit/dim 3.7681(best: 3.7665), Xent 1.1149, Loss 4.3256, Error 0.3967(best: 0.3838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1111 | Time 81.6612(80.1791) | Bit/dim 3.7698(3.7685) | Xent 1.0037(1.0078) | Loss 4.2716(4.2724) | Error 0.3571(0.3587) Steps 874(877.73) | Grad Norm 6.8185(4.1683) | Total Time 14.00(14.00)\n",
      "Iter 1112 | Time 80.6951(80.1946) | Bit/dim 3.7616(3.7683) | Xent 1.0040(1.0077) | Loss 4.2636(4.2721) | Error 0.3556(0.3586) Steps 862(877.26) | Grad Norm 6.0125(4.2237) | Total Time 14.00(14.00)\n",
      "Iter 1113 | Time 77.9096(80.1260) | Bit/dim 3.7580(3.7680) | Xent 0.9597(1.0062) | Loss 4.2379(4.2711) | Error 0.3379(0.3579) Steps 874(877.16) | Grad Norm 3.0978(4.1899) | Total Time 14.00(14.00)\n",
      "Iter 1114 | Time 80.1998(80.1282) | Bit/dim 3.7551(3.7676) | Xent 0.9764(1.0053) | Loss 4.2432(4.2703) | Error 0.3502(0.3577) Steps 880(877.24) | Grad Norm 4.7646(4.2071) | Total Time 14.00(14.00)\n",
      "Iter 1115 | Time 77.9556(80.0630) | Bit/dim 3.7619(3.7674) | Xent 0.9732(1.0044) | Loss 4.2485(4.2696) | Error 0.3490(0.3574) Steps 886(877.51) | Grad Norm 2.5233(4.1566) | Total Time 14.00(14.00)\n",
      "Iter 1116 | Time 81.4791(80.1055) | Bit/dim 3.7743(3.7677) | Xent 0.9840(1.0038) | Loss 4.2663(4.2695) | Error 0.3492(0.3572) Steps 880(877.58) | Grad Norm 3.4813(4.1363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 31.4023, Epoch Time 527.0218(529.6175), Bit/dim 3.7635(best: 3.7665), Xent 1.0644, Loss 4.2957, Error 0.3830(best: 0.3838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1117 | Time 79.4264(80.0852) | Bit/dim 3.7590(3.7674) | Xent 0.9665(1.0026) | Loss 4.2422(4.2687) | Error 0.3416(0.3567) Steps 880(877.65) | Grad Norm 2.4782(4.0866) | Total Time 14.00(14.00)\n",
      "Iter 1118 | Time 83.3486(80.1831) | Bit/dim 3.7687(3.7674) | Xent 0.9483(1.0010) | Loss 4.2428(4.2679) | Error 0.3385(0.3562) Steps 886(877.90) | Grad Norm 2.1359(4.0281) | Total Time 14.00(14.00)\n",
      "Iter 1119 | Time 79.0937(80.1504) | Bit/dim 3.7675(3.7674) | Xent 0.9694(1.0001) | Loss 4.2522(4.2675) | Error 0.3419(0.3558) Steps 898(878.51) | Grad Norm 2.8014(3.9913) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 78.4641(80.0998) | Bit/dim 3.7457(3.7668) | Xent 0.9818(0.9995) | Loss 4.2366(4.2665) | Error 0.3526(0.3557) Steps 886(878.73) | Grad Norm 3.5137(3.9770) | Total Time 14.00(14.00)\n",
      "Iter 1121 | Time 81.1827(80.1323) | Bit/dim 3.7753(3.7670) | Xent 0.9873(0.9991) | Loss 4.2689(4.2666) | Error 0.3500(0.3555) Steps 880(878.77) | Grad Norm 5.7446(4.0300) | Total Time 14.00(14.00)\n",
      "Iter 1122 | Time 79.5185(80.1139) | Bit/dim 3.7667(3.7670) | Xent 1.0125(0.9995) | Loss 4.2729(4.2668) | Error 0.3608(0.3557) Steps 868(878.45) | Grad Norm 9.0662(4.1811) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 30.4307, Epoch Time 527.7069(529.5602), Bit/dim 3.7693(best: 3.7635), Xent 1.1469, Loss 4.3428, Error 0.4074(best: 0.3830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1123 | Time 77.3565(80.0311) | Bit/dim 3.7763(3.7673) | Xent 1.0333(1.0006) | Loss 4.2930(4.2676) | Error 0.3702(0.3561) Steps 868(878.13) | Grad Norm 10.0956(4.3585) | Total Time 14.00(14.00)\n",
      "Iter 1124 | Time 81.7084(80.0815) | Bit/dim 3.7526(3.7669) | Xent 0.9945(1.0004) | Loss 4.2499(4.2670) | Error 0.3551(0.3561) Steps 874(878.01) | Grad Norm 5.1661(4.3827) | Total Time 14.00(14.00)\n",
      "Iter 1125 | Time 78.8295(80.0439) | Bit/dim 3.7593(3.7666) | Xent 0.9710(0.9995) | Loss 4.2448(4.2664) | Error 0.3481(0.3558) Steps 892(878.43) | Grad Norm 3.3917(4.3530) | Total Time 14.00(14.00)\n",
      "Iter 1126 | Time 80.4428(80.0559) | Bit/dim 3.7680(3.7667) | Xent 1.0137(0.9999) | Loss 4.2748(4.2666) | Error 0.3598(0.3559) Steps 862(877.94) | Grad Norm 4.3487(4.3529) | Total Time 14.00(14.00)\n",
      "Iter 1127 | Time 79.5844(80.0417) | Bit/dim 3.7636(3.7666) | Xent 0.9768(0.9992) | Loss 4.2520(4.2662) | Error 0.3538(0.3559) Steps 880(878.00) | Grad Norm 3.2985(4.3212) | Total Time 14.00(14.00)\n",
      "Iter 1128 | Time 79.1360(80.0145) | Bit/dim 3.7667(3.7666) | Xent 1.0032(0.9993) | Loss 4.2683(4.2663) | Error 0.3568(0.3559) Steps 868(877.70) | Grad Norm 4.4388(4.3248) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 31.1333, Epoch Time 524.1894(529.3991), Bit/dim 3.7638(best: 3.7635), Xent 1.0800, Loss 4.3038, Error 0.3880(best: 0.3830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1129 | Time 79.8387(80.0093) | Bit/dim 3.7596(3.7664) | Xent 0.9906(0.9991) | Loss 4.2549(4.2659) | Error 0.3502(0.3557) Steps 880(877.77) | Grad Norm 2.6845(4.2756) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 80.2861(80.0176) | Bit/dim 3.7642(3.7663) | Xent 0.9691(0.9982) | Loss 4.2487(4.2654) | Error 0.3451(0.3554) Steps 904(878.55) | Grad Norm 4.0953(4.2701) | Total Time 14.00(14.00)\n",
      "Iter 1131 | Time 77.3727(79.9382) | Bit/dim 3.7722(3.7665) | Xent 0.9627(0.9971) | Loss 4.2536(4.2650) | Error 0.3468(0.3552) Steps 868(878.24) | Grad Norm 3.4641(4.2460) | Total Time 14.00(14.00)\n",
      "Iter 1132 | Time 78.8602(79.9059) | Bit/dim 3.7570(3.7662) | Xent 0.9698(0.9963) | Loss 4.2418(4.2644) | Error 0.3438(0.3548) Steps 880(878.29) | Grad Norm 2.4923(4.1934) | Total Time 14.00(14.00)\n",
      "Iter 1133 | Time 81.8243(79.9634) | Bit/dim 3.7650(3.7662) | Xent 0.9446(0.9947) | Loss 4.2373(4.2635) | Error 0.3377(0.3543) Steps 868(877.98) | Grad Norm 2.7627(4.1504) | Total Time 14.00(14.00)\n",
      "Iter 1134 | Time 81.3802(80.0059) | Bit/dim 3.7634(3.7661) | Xent 0.9853(0.9945) | Loss 4.2561(4.2633) | Error 0.3510(0.3542) Steps 874(877.86) | Grad Norm 3.0751(4.1182) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 30.5167, Epoch Time 526.0702(529.2992), Bit/dim 3.7651(best: 3.7635), Xent 1.0803, Loss 4.3053, Error 0.3835(best: 0.3830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1135 | Time 78.9321(79.9737) | Bit/dim 3.7580(3.7658) | Xent 0.9468(0.9930) | Loss 4.2314(4.2624) | Error 0.3429(0.3539) Steps 874(877.75) | Grad Norm 2.4867(4.0692) | Total Time 14.00(14.00)\n",
      "Iter 1136 | Time 80.2825(79.9830) | Bit/dim 3.7595(3.7657) | Xent 0.9578(0.9920) | Loss 4.2384(4.2616) | Error 0.3299(0.3531) Steps 874(877.63) | Grad Norm 1.4828(3.9916) | Total Time 14.00(14.00)\n",
      "Iter 1137 | Time 78.6660(79.9435) | Bit/dim 3.7665(3.7657) | Xent 0.9611(0.9910) | Loss 4.2470(4.2612) | Error 0.3417(0.3528) Steps 868(877.35) | Grad Norm 4.6041(4.0100) | Total Time 14.00(14.00)\n",
      "Iter 1138 | Time 78.8745(79.9114) | Bit/dim 3.7609(3.7655) | Xent 0.9781(0.9907) | Loss 4.2500(4.2609) | Error 0.3472(0.3526) Steps 862(876.88) | Grad Norm 6.5033(4.0848) | Total Time 14.00(14.00)\n",
      "Iter 1139 | Time 80.0392(79.9152) | Bit/dim 3.7621(3.7654) | Xent 0.9684(0.9900) | Loss 4.2463(4.2604) | Error 0.3442(0.3524) Steps 874(876.80) | Grad Norm 3.0388(4.0534) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 81.5223(79.9635) | Bit/dim 3.7674(3.7655) | Xent 0.9511(0.9888) | Loss 4.2430(4.2599) | Error 0.3425(0.3521) Steps 856(876.17) | Grad Norm 2.5993(4.0098) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 30.7716, Epoch Time 524.8131(529.1646), Bit/dim 3.7621(best: 3.7635), Xent 1.0839, Loss 4.3041, Error 0.3871(best: 0.3830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1141 | Time 81.1974(80.0005) | Bit/dim 3.7653(3.7655) | Xent 0.9527(0.9877) | Loss 4.2417(4.2594) | Error 0.3409(0.3517) Steps 892(876.65) | Grad Norm 4.1327(4.0135) | Total Time 14.00(14.00)\n",
      "Iter 1142 | Time 78.8267(79.9653) | Bit/dim 3.7569(3.7652) | Xent 0.9346(0.9861) | Loss 4.2242(4.2583) | Error 0.3364(0.3513) Steps 910(877.65) | Grad Norm 2.4564(3.9668) | Total Time 14.00(14.00)\n",
      "Iter 1143 | Time 81.0477(79.9977) | Bit/dim 3.7601(3.7651) | Xent 0.9474(0.9850) | Loss 4.2338(4.2576) | Error 0.3359(0.3508) Steps 874(877.54) | Grad Norm 2.6632(3.9277) | Total Time 14.00(14.00)\n",
      "Iter 1144 | Time 83.9367(80.1159) | Bit/dim 3.7677(3.7652) | Xent 0.9756(0.9847) | Loss 4.2555(4.2575) | Error 0.3514(0.3508) Steps 880(877.61) | Grad Norm 2.9965(3.8997) | Total Time 14.00(14.00)\n",
      "Iter 1145 | Time 78.8421(80.0777) | Bit/dim 3.7522(3.7648) | Xent 0.9585(0.9839) | Loss 4.2315(4.2567) | Error 0.3340(0.3503) Steps 886(877.87) | Grad Norm 1.8530(3.8383) | Total Time 14.00(14.00)\n",
      "Iter 1146 | Time 78.8972(80.0423) | Bit/dim 3.7573(3.7645) | Xent 0.9440(0.9827) | Loss 4.2293(4.2559) | Error 0.3337(0.3498) Steps 892(878.29) | Grad Norm 2.6680(3.8032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 30.9404, Epoch Time 529.4118(529.1720), Bit/dim 3.7639(best: 3.7621), Xent 1.0675, Loss 4.2976, Error 0.3827(best: 0.3830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1147 | Time 78.8209(80.0056) | Bit/dim 3.7600(3.7644) | Xent 0.9515(0.9818) | Loss 4.2358(4.2553) | Error 0.3397(0.3495) Steps 862(877.80) | Grad Norm 2.1528(3.7537) | Total Time 14.00(14.00)\n",
      "Iter 1148 | Time 80.2408(80.0127) | Bit/dim 3.7596(3.7643) | Xent 0.9526(0.9809) | Loss 4.2359(4.2547) | Error 0.3421(0.3493) Steps 868(877.51) | Grad Norm 2.4256(3.7139) | Total Time 14.00(14.00)\n",
      "Iter 1149 | Time 80.3130(80.0217) | Bit/dim 3.7547(3.7640) | Xent 0.9259(0.9793) | Loss 4.2176(4.2536) | Error 0.3307(0.3488) Steps 874(877.40) | Grad Norm 2.0921(3.6652) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 79.5208(80.0067) | Bit/dim 3.7620(3.7639) | Xent 0.9423(0.9781) | Loss 4.2332(4.2530) | Error 0.3381(0.3484) Steps 886(877.66) | Grad Norm 2.4052(3.6274) | Total Time 14.00(14.00)\n",
      "Iter 1151 | Time 84.2022(80.1325) | Bit/dim 3.7587(3.7638) | Xent 0.9549(0.9775) | Loss 4.2361(4.2525) | Error 0.3396(0.3482) Steps 898(878.27) | Grad Norm 3.1420(3.6129) | Total Time 14.00(14.00)\n",
      "Iter 1152 | Time 79.7316(80.1205) | Bit/dim 3.7674(3.7639) | Xent 0.9412(0.9764) | Loss 4.2380(4.2520) | Error 0.3351(0.3478) Steps 886(878.50) | Grad Norm 3.0210(3.5951) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 30.4836, Epoch Time 529.4848(529.1814), Bit/dim 3.7613(best: 3.7621), Xent 1.0714, Loss 4.2970, Error 0.3825(best: 0.3827)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1153 | Time 79.8960(80.1138) | Bit/dim 3.7651(3.7639) | Xent 0.9510(0.9756) | Loss 4.2406(4.2517) | Error 0.3346(0.3474) Steps 880(878.55) | Grad Norm 2.1156(3.5507) | Total Time 14.00(14.00)\n",
      "Iter 1154 | Time 78.7818(80.0738) | Bit/dim 3.7614(3.7638) | Xent 0.9426(0.9746) | Loss 4.2326(4.2511) | Error 0.3371(0.3471) Steps 868(878.23) | Grad Norm 2.7179(3.5257) | Total Time 14.00(14.00)\n",
      "Iter 1155 | Time 80.3584(80.0824) | Bit/dim 3.7601(3.7637) | Xent 0.9400(0.9736) | Loss 4.2301(4.2505) | Error 0.3327(0.3466) Steps 892(878.64) | Grad Norm 4.3632(3.5509) | Total Time 14.00(14.00)\n",
      "Iter 1156 | Time 78.0137(80.0203) | Bit/dim 3.7539(3.7634) | Xent 0.9405(0.9726) | Loss 4.2242(4.2497) | Error 0.3411(0.3465) Steps 862(878.14) | Grad Norm 2.1226(3.5080) | Total Time 14.00(14.00)\n",
      "Iter 1157 | Time 80.6638(80.0396) | Bit/dim 3.7645(3.7635) | Xent 0.9327(0.9714) | Loss 4.2309(4.2492) | Error 0.3294(0.3460) Steps 880(878.20) | Grad Norm 4.7638(3.5457) | Total Time 14.00(14.00)\n",
      "Iter 1158 | Time 81.3789(80.0798) | Bit/dim 3.7598(3.7633) | Xent 0.9546(0.9709) | Loss 4.2371(4.2488) | Error 0.3419(0.3458) Steps 886(878.43) | Grad Norm 5.9250(3.6171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 30.8211, Epoch Time 526.0103(529.0863), Bit/dim 3.7610(best: 3.7613), Xent 1.0973, Loss 4.3097, Error 0.3887(best: 0.3825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1159 | Time 78.0871(80.0200) | Bit/dim 3.7572(3.7632) | Xent 0.9752(0.9710) | Loss 4.2448(4.2487) | Error 0.3433(0.3458) Steps 880(878.48) | Grad Norm 5.6299(3.6775) | Total Time 14.00(14.00)\n",
      "Iter 1160 | Time 84.5394(80.1556) | Bit/dim 3.7593(3.7630) | Xent 0.9553(0.9705) | Loss 4.2370(4.2483) | Error 0.3435(0.3457) Steps 886(878.71) | Grad Norm 5.6699(3.7372) | Total Time 14.00(14.00)\n",
      "Iter 1161 | Time 80.0134(80.1513) | Bit/dim 3.7553(3.7628) | Xent 0.9654(0.9704) | Loss 4.2380(4.2480) | Error 0.3524(0.3459) Steps 868(878.39) | Grad Norm 5.5575(3.7918) | Total Time 14.00(14.00)\n",
      "Iter 1162 | Time 79.4859(80.1314) | Bit/dim 3.7636(3.7628) | Xent 0.9839(0.9708) | Loss 4.2556(4.2482) | Error 0.3541(0.3461) Steps 886(878.61) | Grad Norm 5.1192(3.8317) | Total Time 14.00(14.00)\n",
      "Iter 1163 | Time 81.5335(80.1734) | Bit/dim 3.7663(3.7629) | Xent 0.9519(0.9702) | Loss 4.2422(4.2481) | Error 0.3324(0.3457) Steps 868(878.30) | Grad Norm 5.5929(3.8845) | Total Time 14.00(14.00)\n",
      "Iter 1164 | Time 84.0476(80.2896) | Bit/dim 3.7517(3.7626) | Xent 0.9798(0.9705) | Loss 4.2416(4.2479) | Error 0.3600(0.3462) Steps 898(878.89) | Grad Norm 6.0208(3.9486) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 30.9560, Epoch Time 534.4675(529.2477), Bit/dim 3.7611(best: 3.7610), Xent 1.0752, Loss 4.2987, Error 0.3834(best: 0.3825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1165 | Time 80.3792(80.2923) | Bit/dim 3.7510(3.7623) | Xent 0.9473(0.9698) | Loss 4.2246(4.2472) | Error 0.3379(0.3459) Steps 886(879.10) | Grad Norm 4.6378(3.9693) | Total Time 14.00(14.00)\n",
      "Iter 1166 | Time 81.6990(80.3345) | Bit/dim 3.7647(3.7623) | Xent 0.9875(0.9703) | Loss 4.2585(4.2475) | Error 0.3502(0.3460) Steps 856(878.41) | Grad Norm 8.4082(4.1024) | Total Time 14.00(14.00)\n",
      "Iter 1167 | Time 79.4244(80.3072) | Bit/dim 3.7673(3.7625) | Xent 0.9926(0.9710) | Loss 4.2636(4.2480) | Error 0.3525(0.3462) Steps 880(878.45) | Grad Norm 9.7371(4.2715) | Total Time 14.00(14.00)\n",
      "Iter 1168 | Time 81.8620(80.3539) | Bit/dim 3.7572(3.7623) | Xent 0.9453(0.9702) | Loss 4.2298(4.2474) | Error 0.3404(0.3461) Steps 874(878.32) | Grad Norm 2.8231(4.2280) | Total Time 14.00(14.00)\n",
      "Iter 1169 | Time 77.2785(80.2616) | Bit/dim 3.7634(3.7624) | Xent 0.9778(0.9705) | Loss 4.2523(4.2476) | Error 0.3454(0.3460) Steps 880(878.37) | Grad Norm 5.5836(4.2687) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 80.9677(80.2828) | Bit/dim 3.7597(3.7623) | Xent 0.9429(0.9696) | Loss 4.2311(4.2471) | Error 0.3334(0.3457) Steps 886(878.60) | Grad Norm 3.7354(4.2527) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 30.3627, Epoch Time 527.7072(529.2015), Bit/dim 3.7613(best: 3.7610), Xent 1.0811, Loss 4.3018, Error 0.3859(best: 0.3825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1171 | Time 78.2784(80.2227) | Bit/dim 3.7649(3.7624) | Xent 0.9498(0.9690) | Loss 4.2398(4.2469) | Error 0.3401(0.3455) Steps 880(878.64) | Grad Norm 5.6821(4.2956) | Total Time 14.00(14.00)\n",
      "Iter 1172 | Time 77.4290(80.1388) | Bit/dim 3.7586(3.7622) | Xent 0.9690(0.9690) | Loss 4.2431(4.2468) | Error 0.3429(0.3454) Steps 874(878.50) | Grad Norm 4.4950(4.3016) | Total Time 14.00(14.00)\n",
      "Iter 1173 | Time 84.6629(80.2746) | Bit/dim 3.7514(3.7619) | Xent 0.9420(0.9682) | Loss 4.2224(4.2460) | Error 0.3375(0.3452) Steps 874(878.37) | Grad Norm 3.2999(4.2715) | Total Time 14.00(14.00)\n",
      "Iter 1174 | Time 80.9557(80.2950) | Bit/dim 3.7538(3.7617) | Xent 0.9164(0.9667) | Loss 4.2120(4.2450) | Error 0.3236(0.3445) Steps 868(878.06) | Grad Norm 3.3630(4.2442) | Total Time 14.00(14.00)\n",
      "Iter 1175 | Time 80.9638(80.3151) | Bit/dim 3.7627(3.7617) | Xent 0.9582(0.9664) | Loss 4.2418(4.2449) | Error 0.3444(0.3445) Steps 880(878.12) | Grad Norm 3.5683(4.2240) | Total Time 14.00(14.00)\n",
      "Iter 1176 | Time 79.3797(80.2870) | Bit/dim 3.7612(3.7617) | Xent 0.9550(0.9661) | Loss 4.2387(4.2447) | Error 0.3454(0.3446) Steps 886(878.35) | Grad Norm 4.8761(4.2435) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 30.8273, Epoch Time 528.2597(529.1733), Bit/dim 3.7614(best: 3.7610), Xent 1.0786, Loss 4.3007, Error 0.3825(best: 0.3825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1177 | Time 77.8353(80.2135) | Bit/dim 3.7730(3.7620) | Xent 0.9680(0.9661) | Loss 4.2570(4.2451) | Error 0.3416(0.3445) Steps 868(878.04) | Grad Norm 4.7608(4.2591) | Total Time 14.00(14.00)\n",
      "Iter 1178 | Time 79.4998(80.1920) | Bit/dim 3.7528(3.7618) | Xent 0.9133(0.9646) | Loss 4.2094(4.2440) | Error 0.3327(0.3441) Steps 892(878.46) | Grad Norm 2.4628(4.2052) | Total Time 14.00(14.00)\n",
      "Iter 1179 | Time 79.3511(80.1668) | Bit/dim 3.7419(3.7612) | Xent 0.9514(0.9642) | Loss 4.2176(4.2432) | Error 0.3407(0.3440) Steps 880(878.51) | Grad Norm 4.3895(4.2107) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 84.7420(80.3041) | Bit/dim 3.7642(3.7612) | Xent 0.9485(0.9637) | Loss 4.2384(4.2431) | Error 0.3405(0.3439) Steps 856(877.83) | Grad Norm 5.6698(4.2545) | Total Time 14.00(14.00)\n",
      "Iter 1181 | Time 80.6994(80.3159) | Bit/dim 3.7606(3.7612) | Xent 0.9578(0.9635) | Loss 4.2395(4.2430) | Error 0.3410(0.3438) Steps 886(878.08) | Grad Norm 4.8378(4.2720) | Total Time 14.00(14.00)\n",
      "Iter 1182 | Time 80.1797(80.3118) | Bit/dim 3.7603(3.7612) | Xent 0.9236(0.9623) | Loss 4.2221(4.2424) | Error 0.3257(0.3433) Steps 868(877.77) | Grad Norm 2.2824(4.2123) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 30.6292, Epoch Time 528.9822(529.1675), Bit/dim 3.7571(best: 3.7610), Xent 1.0589, Loss 4.2865, Error 0.3802(best: 0.3825)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1183 | Time 79.4008(80.2845) | Bit/dim 3.7591(3.7611) | Xent 0.9272(0.9613) | Loss 4.2227(4.2418) | Error 0.3333(0.3430) Steps 880(877.84) | Grad Norm 2.1483(4.1504) | Total Time 14.00(14.00)\n",
      "Iter 1184 | Time 79.1046(80.2491) | Bit/dim 3.7481(3.7607) | Xent 0.9524(0.9610) | Loss 4.2243(4.2412) | Error 0.3394(0.3429) Steps 874(877.73) | Grad Norm 4.3913(4.1576) | Total Time 14.00(14.00)\n",
      "Iter 1185 | Time 82.4734(80.3158) | Bit/dim 3.7546(3.7606) | Xent 0.9408(0.9604) | Loss 4.2250(4.2408) | Error 0.3337(0.3426) Steps 880(877.79) | Grad Norm 5.7161(4.2043) | Total Time 14.00(14.00)\n",
      "Iter 1186 | Time 79.2638(80.2843) | Bit/dim 3.7620(3.7606) | Xent 0.9162(0.9591) | Loss 4.2201(4.2401) | Error 0.3326(0.3423) Steps 856(877.14) | Grad Norm 4.6422(4.2175) | Total Time 14.00(14.00)\n",
      "Iter 1187 | Time 78.1960(80.2216) | Bit/dim 3.7472(3.7602) | Xent 0.9511(0.9588) | Loss 4.2228(4.2396) | Error 0.3320(0.3420) Steps 886(877.41) | Grad Norm 6.1279(4.2748) | Total Time 14.00(14.00)\n",
      "Iter 1188 | Time 79.1176(80.1885) | Bit/dim 3.7646(3.7603) | Xent 0.9873(0.9597) | Loss 4.2583(4.2402) | Error 0.3540(0.3423) Steps 862(876.94) | Grad Norm 7.8164(4.3810) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 30.0630, Epoch Time 523.6473(529.0019), Bit/dim 3.7586(best: 3.7571), Xent 1.0693, Loss 4.2932, Error 0.3813(best: 0.3802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1189 | Time 80.1567(80.1876) | Bit/dim 3.7609(3.7603) | Xent 0.9167(0.9584) | Loss 4.2192(4.2395) | Error 0.3239(0.3418) Steps 874(876.86) | Grad Norm 3.0656(4.3416) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 81.6622(80.2318) | Bit/dim 3.7544(3.7602) | Xent 0.9738(0.9588) | Loss 4.2413(4.2396) | Error 0.3431(0.3418) Steps 856(876.23) | Grad Norm 6.6996(4.4123) | Total Time 14.00(14.00)\n",
      "Iter 1191 | Time 78.4364(80.1779) | Bit/dim 3.7579(3.7601) | Xent 0.9596(0.9589) | Loss 4.2377(4.2395) | Error 0.3371(0.3417) Steps 862(875.80) | Grad Norm 6.4113(4.4723) | Total Time 14.00(14.00)\n",
      "Iter 1192 | Time 79.7507(80.1651) | Bit/dim 3.7683(3.7603) | Xent 0.9626(0.9590) | Loss 4.2496(4.2398) | Error 0.3436(0.3418) Steps 874(875.75) | Grad Norm 5.1932(4.4939) | Total Time 14.00(14.00)\n",
      "Iter 1193 | Time 80.2787(80.1685) | Bit/dim 3.7573(3.7603) | Xent 0.9461(0.9586) | Loss 4.2303(4.2396) | Error 0.3390(0.3417) Steps 844(874.80) | Grad Norm 4.4925(4.4939) | Total Time 14.00(14.00)\n",
      "Iter 1194 | Time 79.7079(80.1547) | Bit/dim 3.7494(3.7599) | Xent 0.9746(0.9591) | Loss 4.2367(4.2395) | Error 0.3484(0.3419) Steps 898(875.49) | Grad Norm 2.5603(4.4359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 30.6919, Epoch Time 526.5133(528.9273), Bit/dim 3.7574(best: 3.7571), Xent 1.0550, Loss 4.2850, Error 0.3790(best: 0.3802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1195 | Time 79.9096(80.1474) | Bit/dim 3.7515(3.7597) | Xent 0.9239(0.9580) | Loss 4.2135(4.2387) | Error 0.3333(0.3416) Steps 874(875.45) | Grad Norm 2.2160(4.3693) | Total Time 14.00(14.00)\n",
      "Iter 1196 | Time 82.5583(80.2197) | Bit/dim 3.7566(3.7596) | Xent 0.9538(0.9579) | Loss 4.2335(4.2385) | Error 0.3405(0.3416) Steps 898(876.12) | Grad Norm 3.0014(4.3282) | Total Time 14.00(14.00)\n",
      "Iter 1197 | Time 79.1704(80.1882) | Bit/dim 3.7514(3.7593) | Xent 0.9054(0.9563) | Loss 4.2041(4.2375) | Error 0.3244(0.3411) Steps 862(875.70) | Grad Norm 5.2169(4.3549) | Total Time 14.00(14.00)\n",
      "Iter 1198 | Time 79.2912(80.1613) | Bit/dim 3.7602(3.7594) | Xent 0.9676(0.9567) | Loss 4.2440(4.2377) | Error 0.3431(0.3411) Steps 868(875.47) | Grad Norm 7.4579(4.4480) | Total Time 14.00(14.00)\n",
      "Iter 1199 | Time 78.2370(80.1036) | Bit/dim 3.7512(3.7591) | Xent 0.9333(0.9560) | Loss 4.2178(4.2371) | Error 0.3303(0.3408) Steps 862(875.07) | Grad Norm 3.9246(4.4323) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 78.6626(80.0603) | Bit/dim 3.7681(3.7594) | Xent 0.9264(0.9551) | Loss 4.2313(4.2369) | Error 0.3296(0.3405) Steps 856(874.49) | Grad Norm 3.1744(4.3945) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 29.7921, Epoch Time 523.0648(528.7514), Bit/dim 3.7575(best: 3.7571), Xent 1.0709, Loss 4.2929, Error 0.3833(best: 0.3790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1201 | Time 79.6829(80.0490) | Bit/dim 3.7513(3.7591) | Xent 0.9378(0.9546) | Loss 4.2202(4.2364) | Error 0.3364(0.3403) Steps 868(874.30) | Grad Norm 4.6314(4.4017) | Total Time 14.00(14.00)\n",
      "Iter 1202 | Time 79.4783(80.0319) | Bit/dim 3.7546(3.7590) | Xent 0.9355(0.9540) | Loss 4.2223(4.2360) | Error 0.3370(0.3402) Steps 880(874.47) | Grad Norm 4.1371(4.3937) | Total Time 14.00(14.00)\n",
      "Iter 1203 | Time 80.9085(80.0582) | Bit/dim 3.7631(3.7591) | Xent 0.9172(0.9529) | Loss 4.2216(4.2356) | Error 0.3245(0.3398) Steps 880(874.64) | Grad Norm 2.6194(4.3405) | Total Time 14.00(14.00)\n",
      "Iter 1204 | Time 78.9721(80.0256) | Bit/dim 3.7545(3.7590) | Xent 0.9142(0.9517) | Loss 4.2116(4.2349) | Error 0.3280(0.3394) Steps 886(874.98) | Grad Norm 4.2333(4.3373) | Total Time 14.00(14.00)\n",
      "Iter 1205 | Time 78.0055(79.9650) | Bit/dim 3.7514(3.7588) | Xent 0.9606(0.9520) | Loss 4.2317(4.2348) | Error 0.3424(0.3395) Steps 862(874.59) | Grad Norm 8.4103(4.4595) | Total Time 14.00(14.00)\n",
      "Iter 1206 | Time 79.6449(79.9554) | Bit/dim 3.7571(3.7587) | Xent 0.9126(0.9508) | Loss 4.2134(4.2341) | Error 0.3249(0.3391) Steps 868(874.39) | Grad Norm 7.1738(4.5409) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 30.5740, Epoch Time 523.0434(528.5802), Bit/dim 3.7563(best: 3.7571), Xent 1.0786, Loss 4.2956, Error 0.3865(best: 0.3790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1207 | Time 80.6881(79.9774) | Bit/dim 3.7477(3.7584) | Xent 0.9386(0.9504) | Loss 4.2170(4.2336) | Error 0.3383(0.3390) Steps 898(875.10) | Grad Norm 3.8434(4.5200) | Total Time 14.00(14.00)\n",
      "Iter 1208 | Time 80.8754(80.0043) | Bit/dim 3.7600(3.7584) | Xent 0.9312(0.9499) | Loss 4.2256(4.2334) | Error 0.3350(0.3389) Steps 880(875.24) | Grad Norm 5.7759(4.5576) | Total Time 14.00(14.00)\n",
      "Iter 1209 | Time 82.5401(80.0804) | Bit/dim 3.7592(3.7585) | Xent 0.9120(0.9487) | Loss 4.2152(4.2328) | Error 0.3217(0.3384) Steps 886(875.57) | Grad Norm 2.9348(4.5090) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 81.0474(80.1094) | Bit/dim 3.7677(3.7587) | Xent 0.9409(0.9485) | Loss 4.2381(4.2330) | Error 0.3360(0.3383) Steps 868(875.34) | Grad Norm 7.1309(4.5876) | Total Time 14.00(14.00)\n",
      "Iter 1211 | Time 81.9883(80.1658) | Bit/dim 3.7586(3.7587) | Xent 0.9845(0.9496) | Loss 4.2508(4.2335) | Error 0.3485(0.3386) Steps 862(874.94) | Grad Norm 10.8769(4.7763) | Total Time 14.00(14.00)\n",
      "Iter 1212 | Time 81.7424(80.2131) | Bit/dim 3.7538(3.7586) | Xent 1.0005(0.9511) | Loss 4.2540(4.2341) | Error 0.3514(0.3390) Steps 868(874.73) | Grad Norm 7.2224(4.8497) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 30.6951, Epoch Time 535.4205(528.7854), Bit/dim 3.7625(best: 3.7563), Xent 1.0889, Loss 4.3070, Error 0.3897(best: 0.3790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1213 | Time 79.3600(80.1875) | Bit/dim 3.7597(3.7586) | Xent 0.9531(0.9511) | Loss 4.2362(4.2342) | Error 0.3361(0.3389) Steps 874(874.71) | Grad Norm 6.2260(4.8910) | Total Time 14.00(14.00)\n",
      "Iter 1214 | Time 80.4839(80.1964) | Bit/dim 3.7581(3.7586) | Xent 0.9672(0.9516) | Loss 4.2417(4.2344) | Error 0.3434(0.3391) Steps 886(875.05) | Grad Norm 6.2854(4.9328) | Total Time 14.00(14.00)\n",
      "Iter 1215 | Time 78.4505(80.1440) | Bit/dim 3.7574(3.7586) | Xent 0.9756(0.9523) | Loss 4.2452(4.2347) | Error 0.3525(0.3395) Steps 874(875.02) | Grad Norm 3.8937(4.9016) | Total Time 14.00(14.00)\n",
      "Iter 1216 | Time 80.9688(80.1687) | Bit/dim 3.7543(3.7584) | Xent 0.9612(0.9526) | Loss 4.2350(4.2348) | Error 0.3393(0.3395) Steps 874(874.99) | Grad Norm 3.7926(4.8684) | Total Time 14.00(14.00)\n",
      "Iter 1217 | Time 82.1300(80.2276) | Bit/dim 3.7589(3.7585) | Xent 0.9291(0.9519) | Loss 4.2235(4.2344) | Error 0.3286(0.3391) Steps 862(874.60) | Grad Norm 3.1819(4.8178) | Total Time 14.00(14.00)\n",
      "Iter 1218 | Time 82.4384(80.2939) | Bit/dim 3.7596(3.7585) | Xent 0.9365(0.9514) | Loss 4.2278(4.2342) | Error 0.3353(0.3390) Steps 850(873.86) | Grad Norm 2.8067(4.7574) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 30.3464, Epoch Time 530.0271(528.8226), Bit/dim 3.7594(best: 3.7563), Xent 1.0787, Loss 4.2987, Error 0.3819(best: 0.3790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1219 | Time 78.1902(80.2308) | Bit/dim 3.7488(3.7582) | Xent 0.9093(0.9502) | Loss 4.2034(4.2333) | Error 0.3245(0.3386) Steps 874(873.86) | Grad Norm 2.4629(4.6886) | Total Time 14.00(14.00)\n",
      "Iter 1220 | Time 78.7508(80.1864) | Bit/dim 3.7624(3.7583) | Xent 0.9228(0.9494) | Loss 4.2238(4.2330) | Error 0.3299(0.3383) Steps 850(873.15) | Grad Norm 2.9529(4.6365) | Total Time 14.00(14.00)\n",
      "Iter 1221 | Time 77.5423(80.1071) | Bit/dim 3.7577(3.7583) | Xent 0.8865(0.9475) | Loss 4.2009(4.2320) | Error 0.3191(0.3377) Steps 862(872.81) | Grad Norm 2.0959(4.5603) | Total Time 14.00(14.00)\n",
      "Iter 1222 | Time 82.0371(80.1650) | Bit/dim 3.7479(3.7580) | Xent 0.9421(0.9473) | Loss 4.2190(4.2317) | Error 0.3341(0.3376) Steps 874(872.85) | Grad Norm 2.7050(4.5046) | Total Time 14.00(14.00)\n",
      "Iter 1223 | Time 79.9991(80.1600) | Bit/dim 3.7555(3.7579) | Xent 0.9378(0.9470) | Loss 4.2243(4.2314) | Error 0.3286(0.3374) Steps 856(872.34) | Grad Norm 3.4588(4.4733) | Total Time 14.00(14.00)\n",
      "Iter 1224 | Time 80.6943(80.1760) | Bit/dim 3.7693(3.7583) | Xent 0.9171(0.9461) | Loss 4.2279(4.2313) | Error 0.3351(0.3373) Steps 880(872.57) | Grad Norm 2.9593(4.4279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 30.1660, Epoch Time 523.2026(528.6540), Bit/dim 3.7581(best: 3.7563), Xent 1.0704, Loss 4.2933, Error 0.3770(best: 0.3790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1225 | Time 79.6044(80.1589) | Bit/dim 3.7604(3.7583) | Xent 0.8718(0.9439) | Loss 4.1963(4.2303) | Error 0.3100(0.3365) Steps 850(871.90) | Grad Norm 2.7917(4.3788) | Total Time 14.00(14.00)\n",
      "Iter 1226 | Time 81.2581(80.1918) | Bit/dim 3.7495(3.7581) | Xent 0.9180(0.9431) | Loss 4.2085(4.2296) | Error 0.3297(0.3363) Steps 886(872.32) | Grad Norm 4.5424(4.3837) | Total Time 14.00(14.00)\n",
      "Iter 1227 | Time 83.4121(80.2885) | Bit/dim 3.7533(3.7579) | Xent 0.9056(0.9420) | Loss 4.2062(4.2289) | Error 0.3245(0.3359) Steps 898(873.09) | Grad Norm 4.0289(4.3730) | Total Time 14.00(14.00)\n",
      "Iter 1228 | Time 80.9197(80.3074) | Bit/dim 3.7609(3.7580) | Xent 0.9277(0.9416) | Loss 4.2248(4.2288) | Error 0.3329(0.3358) Steps 892(873.66) | Grad Norm 6.1823(4.4273) | Total Time 14.00(14.00)\n",
      "Iter 1229 | Time 79.7568(80.2909) | Bit/dim 3.7603(3.7581) | Xent 0.9487(0.9418) | Loss 4.2347(4.2290) | Error 0.3380(0.3359) Steps 874(873.67) | Grad Norm 5.7626(4.4674) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 81.1199(80.3157) | Bit/dim 3.7521(3.7579) | Xent 0.9422(0.9418) | Loss 4.2232(4.2288) | Error 0.3346(0.3359) Steps 874(873.68) | Grad Norm 4.7861(4.4769) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 30.2546, Epoch Time 532.5292(528.7703), Bit/dim 3.7549(best: 3.7563), Xent 1.0693, Loss 4.2895, Error 0.3798(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1231 | Time 78.7936(80.2701) | Bit/dim 3.7636(3.7581) | Xent 0.9134(0.9409) | Loss 4.2203(4.2285) | Error 0.3227(0.3355) Steps 886(874.05) | Grad Norm 4.3722(4.4738) | Total Time 14.00(14.00)\n",
      "Iter 1232 | Time 80.3016(80.2710) | Bit/dim 3.7554(3.7580) | Xent 0.9237(0.9404) | Loss 4.2173(4.2282) | Error 0.3219(0.3351) Steps 886(874.41) | Grad Norm 4.5609(4.4764) | Total Time 14.00(14.00)\n",
      "Iter 1233 | Time 80.4324(80.2759) | Bit/dim 3.7491(3.7577) | Xent 0.9069(0.9394) | Loss 4.2026(4.2274) | Error 0.3267(0.3348) Steps 892(874.93) | Grad Norm 2.9302(4.4300) | Total Time 14.00(14.00)\n",
      "Iter 1234 | Time 78.1809(80.2130) | Bit/dim 3.7531(3.7576) | Xent 0.9081(0.9385) | Loss 4.2071(4.2268) | Error 0.3199(0.3344) Steps 880(875.09) | Grad Norm 5.5976(4.4650) | Total Time 14.00(14.00)\n",
      "Iter 1235 | Time 78.3491(80.1571) | Bit/dim 3.7525(3.7574) | Xent 0.9068(0.9375) | Loss 4.2059(4.2262) | Error 0.3215(0.3340) Steps 874(875.05) | Grad Norm 4.5578(4.4678) | Total Time 14.00(14.00)\n",
      "Iter 1236 | Time 79.0514(80.1239) | Bit/dim 3.7546(3.7573) | Xent 0.9057(0.9366) | Loss 4.2075(4.2256) | Error 0.3269(0.3338) Steps 862(874.66) | Grad Norm 5.0668(4.4858) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 30.0191, Epoch Time 520.8995(528.5341), Bit/dim 3.7558(best: 3.7549), Xent 1.0646, Loss 4.2880, Error 0.3790(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1237 | Time 78.4857(80.0748) | Bit/dim 3.7581(3.7574) | Xent 0.8873(0.9351) | Loss 4.2018(4.2249) | Error 0.3151(0.3332) Steps 874(874.64) | Grad Norm 5.7366(4.5233) | Total Time 14.00(14.00)\n",
      "Iter 1238 | Time 81.4066(80.1147) | Bit/dim 3.7570(3.7574) | Xent 0.9313(0.9350) | Loss 4.2227(4.2249) | Error 0.3260(0.3330) Steps 886(874.98) | Grad Norm 7.9967(4.6275) | Total Time 14.00(14.00)\n",
      "Iter 1239 | Time 80.3321(80.1213) | Bit/dim 3.7559(3.7573) | Xent 0.9336(0.9349) | Loss 4.2227(4.2248) | Error 0.3326(0.3330) Steps 868(874.77) | Grad Norm 7.6456(4.7181) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 80.2884(80.1263) | Bit/dim 3.7559(3.7573) | Xent 0.9360(0.9350) | Loss 4.2239(4.2248) | Error 0.3326(0.3330) Steps 868(874.57) | Grad Norm 9.0525(4.8481) | Total Time 14.00(14.00)\n",
      "Iter 1241 | Time 79.4552(80.1061) | Bit/dim 3.7410(3.7568) | Xent 0.9230(0.9346) | Loss 4.2026(4.2241) | Error 0.3280(0.3328) Steps 856(874.01) | Grad Norm 5.9920(4.8824) | Total Time 14.00(14.00)\n",
      "Iter 1242 | Time 80.2349(80.1100) | Bit/dim 3.7628(3.7570) | Xent 0.9096(0.9339) | Loss 4.2176(4.2239) | Error 0.3274(0.3327) Steps 874(874.01) | Grad Norm 5.7641(4.9089) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 30.3056, Epoch Time 526.3278(528.4680), Bit/dim 3.7517(best: 3.7549), Xent 1.0668, Loss 4.2851, Error 0.3780(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1243 | Time 79.5713(80.0938) | Bit/dim 3.7476(3.7567) | Xent 0.9116(0.9332) | Loss 4.2035(4.2233) | Error 0.3294(0.3326) Steps 868(873.83) | Grad Norm 3.4700(4.8657) | Total Time 14.00(14.00)\n",
      "Iter 1244 | Time 79.1621(80.0659) | Bit/dim 3.7561(3.7567) | Xent 0.9113(0.9325) | Loss 4.2117(4.2229) | Error 0.3207(0.3322) Steps 862(873.48) | Grad Norm 3.8089(4.8340) | Total Time 14.00(14.00)\n",
      "Iter 1245 | Time 79.7631(80.0568) | Bit/dim 3.7518(3.7565) | Xent 0.9116(0.9319) | Loss 4.2076(4.2225) | Error 0.3225(0.3319) Steps 880(873.67) | Grad Norm 3.8494(4.8045) | Total Time 14.00(14.00)\n",
      "Iter 1246 | Time 78.5042(80.0102) | Bit/dim 3.7568(3.7565) | Xent 0.8639(0.9299) | Loss 4.1887(4.2215) | Error 0.3041(0.3311) Steps 892(874.22) | Grad Norm 2.4989(4.7353) | Total Time 14.00(14.00)\n",
      "Iter 1247 | Time 79.5638(79.9968) | Bit/dim 3.7470(3.7562) | Xent 0.8832(0.9285) | Loss 4.1887(4.2205) | Error 0.3159(0.3306) Steps 898(874.94) | Grad Norm 3.1207(4.6869) | Total Time 14.00(14.00)\n",
      "Iter 1248 | Time 78.8860(79.9635) | Bit/dim 3.7603(3.7564) | Xent 0.8913(0.9274) | Loss 4.2059(4.2200) | Error 0.3233(0.3304) Steps 886(875.27) | Grad Norm 3.9448(4.6646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 30.7249, Epoch Time 521.8439(528.2692), Bit/dim 3.7573(best: 3.7517), Xent 1.0664, Loss 4.2905, Error 0.3816(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1249 | Time 80.4153(79.9771) | Bit/dim 3.7662(3.7567) | Xent 0.9109(0.9269) | Loss 4.2217(4.2201) | Error 0.3271(0.3303) Steps 880(875.41) | Grad Norm 2.8881(4.6113) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 81.7922(80.0315) | Bit/dim 3.7522(3.7565) | Xent 0.8861(0.9256) | Loss 4.1952(4.2193) | Error 0.3174(0.3299) Steps 892(875.91) | Grad Norm 4.6307(4.6119) | Total Time 14.00(14.00)\n",
      "Iter 1251 | Time 80.1862(80.0362) | Bit/dim 3.7600(3.7566) | Xent 0.8792(0.9242) | Loss 4.1996(4.2188) | Error 0.3151(0.3295) Steps 868(875.67) | Grad Norm 5.8537(4.6491) | Total Time 14.00(14.00)\n",
      "Iter 1252 | Time 80.8351(80.0601) | Bit/dim 3.7478(3.7564) | Xent 0.8945(0.9234) | Loss 4.1950(4.2180) | Error 0.3166(0.3291) Steps 874(875.62) | Grad Norm 4.8389(4.6548) | Total Time 14.00(14.00)\n",
      "Iter 1253 | Time 82.1208(80.1220) | Bit/dim 3.7397(3.7559) | Xent 0.9194(0.9232) | Loss 4.1994(4.2175) | Error 0.3329(0.3292) Steps 898(876.29) | Grad Norm 5.9646(4.6941) | Total Time 14.00(14.00)\n",
      "Iter 1254 | Time 84.1361(80.2424) | Bit/dim 3.7618(3.7560) | Xent 0.9157(0.9230) | Loss 4.2196(4.2176) | Error 0.3190(0.3289) Steps 874(876.22) | Grad Norm 8.8789(4.8197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 30.2535, Epoch Time 535.2213(528.4778), Bit/dim 3.7526(best: 3.7517), Xent 1.0881, Loss 4.2967, Error 0.3821(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1255 | Time 77.4400(80.1583) | Bit/dim 3.7509(3.7559) | Xent 0.8965(0.9222) | Loss 4.1991(4.2170) | Error 0.3195(0.3286) Steps 874(876.16) | Grad Norm 5.5997(4.8431) | Total Time 14.00(14.00)\n",
      "Iter 1256 | Time 76.2497(80.0410) | Bit/dim 3.7549(3.7559) | Xent 0.9025(0.9216) | Loss 4.2062(4.2167) | Error 0.3190(0.3283) Steps 880(876.27) | Grad Norm 3.4565(4.8015) | Total Time 14.00(14.00)\n",
      "Iter 1257 | Time 78.6508(79.9993) | Bit/dim 3.7530(3.7558) | Xent 0.9021(0.9210) | Loss 4.2041(4.2163) | Error 0.3241(0.3282) Steps 880(876.38) | Grad Norm 4.1265(4.7812) | Total Time 14.00(14.00)\n",
      "Iter 1258 | Time 81.5164(80.0449) | Bit/dim 3.7521(3.7557) | Xent 0.8986(0.9204) | Loss 4.2014(4.2158) | Error 0.3230(0.3280) Steps 880(876.49) | Grad Norm 3.8492(4.7533) | Total Time 14.00(14.00)\n",
      "Iter 1259 | Time 79.2706(80.0216) | Bit/dim 3.7402(3.7552) | Xent 0.8767(0.9190) | Loss 4.1785(4.2147) | Error 0.3046(0.3273) Steps 868(876.24) | Grad Norm 2.5501(4.6872) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 80.0696(80.0231) | Bit/dim 3.7527(3.7551) | Xent 0.8957(0.9184) | Loss 4.2005(4.2143) | Error 0.3153(0.3270) Steps 868(875.99) | Grad Norm 3.4762(4.6508) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 30.9291, Epoch Time 520.1541(528.2281), Bit/dim 3.7535(best: 3.7517), Xent 1.0776, Loss 4.2923, Error 0.3797(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1261 | Time 79.3431(80.0027) | Bit/dim 3.7595(3.7553) | Xent 0.8860(0.9174) | Loss 4.2025(4.2139) | Error 0.3197(0.3268) Steps 898(876.65) | Grad Norm 2.9226(4.5990) | Total Time 14.00(14.00)\n",
      "Iter 1262 | Time 76.9983(79.9125) | Bit/dim 3.7526(3.7552) | Xent 0.8801(0.9163) | Loss 4.1927(4.2133) | Error 0.3100(0.3263) Steps 874(876.57) | Grad Norm 6.6546(4.6607) | Total Time 14.00(14.00)\n",
      "Iter 1263 | Time 85.4514(80.0787) | Bit/dim 3.7487(3.7550) | Xent 0.8819(0.9152) | Loss 4.1897(4.2126) | Error 0.3117(0.3258) Steps 874(876.49) | Grad Norm 5.5682(4.6879) | Total Time 14.00(14.00)\n",
      "Iter 1264 | Time 78.7071(80.0376) | Bit/dim 3.7418(3.7546) | Xent 0.8926(0.9146) | Loss 4.1881(4.2119) | Error 0.3206(0.3257) Steps 862(876.06) | Grad Norm 2.4853(4.6218) | Total Time 14.00(14.00)\n",
      "Iter 1265 | Time 78.7382(79.9986) | Bit/dim 3.7501(3.7545) | Xent 0.8717(0.9133) | Loss 4.1859(4.2111) | Error 0.3100(0.3252) Steps 868(875.82) | Grad Norm 2.1308(4.5471) | Total Time 14.00(14.00)\n",
      "Iter 1266 | Time 78.4609(79.9524) | Bit/dim 3.7539(3.7544) | Xent 0.8768(0.9122) | Loss 4.1923(4.2105) | Error 0.3133(0.3248) Steps 868(875.58) | Grad Norm 3.6458(4.5200) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 31.1296, Epoch Time 524.5966(528.1191), Bit/dim 3.7534(best: 3.7517), Xent 1.0526, Loss 4.2797, Error 0.3705(best: 0.3770)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1267 | Time 80.3145(79.9633) | Bit/dim 3.7504(3.7543) | Xent 0.8657(0.9108) | Loss 4.1833(4.2097) | Error 0.3080(0.3243) Steps 892(876.07) | Grad Norm 2.4300(4.4573) | Total Time 14.00(14.00)\n",
      "Iter 1268 | Time 78.0733(79.9066) | Bit/dim 3.7650(3.7546) | Xent 0.8581(0.9092) | Loss 4.1940(4.2092) | Error 0.3096(0.3239) Steps 892(876.55) | Grad Norm 2.2660(4.3916) | Total Time 14.00(14.00)\n",
      "Iter 1269 | Time 78.9296(79.8773) | Bit/dim 3.7505(3.7545) | Xent 0.8523(0.9075) | Loss 4.1767(4.2083) | Error 0.3124(0.3235) Steps 868(876.30) | Grad Norm 2.4150(4.3323) | Total Time 14.00(14.00)\n",
      "Iter 1270 | Time 80.7086(79.9022) | Bit/dim 3.7542(3.7545) | Xent 0.8666(0.9063) | Loss 4.1875(4.2076) | Error 0.3097(0.3231) Steps 880(876.41) | Grad Norm 2.7853(4.2859) | Total Time 14.00(14.00)\n",
      "Iter 1271 | Time 78.9268(79.8730) | Bit/dim 3.7378(3.7540) | Xent 0.8749(0.9053) | Loss 4.1752(4.2067) | Error 0.3060(0.3226) Steps 880(876.51) | Grad Norm 3.2802(4.2557) | Total Time 14.00(14.00)\n",
      "Iter 1272 | Time 80.9119(79.9041) | Bit/dim 3.7470(3.7538) | Xent 0.8748(0.9044) | Loss 4.1844(4.2060) | Error 0.3096(0.3222) Steps 874(876.44) | Grad Norm 4.5494(4.2645) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 29.8396, Epoch Time 523.6342(527.9846), Bit/dim 3.7528(best: 3.7517), Xent 1.0770, Loss 4.2913, Error 0.3781(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1273 | Time 79.2746(79.8853) | Bit/dim 3.7568(3.7539) | Xent 0.8754(0.9035) | Loss 4.1945(4.2057) | Error 0.3127(0.3219) Steps 880(876.55) | Grad Norm 6.0919(4.3193) | Total Time 14.00(14.00)\n",
      "Iter 1274 | Time 79.8555(79.8844) | Bit/dim 3.7435(3.7536) | Xent 0.8698(0.9025) | Loss 4.1784(4.2048) | Error 0.3149(0.3217) Steps 886(876.83) | Grad Norm 3.8607(4.3056) | Total Time 14.00(14.00)\n",
      "Iter 1275 | Time 80.7466(79.9102) | Bit/dim 3.7381(3.7531) | Xent 0.8473(0.9009) | Loss 4.1618(4.2035) | Error 0.3056(0.3212) Steps 886(877.10) | Grad Norm 2.4204(4.2490) | Total Time 14.00(14.00)\n",
      "Iter 1276 | Time 80.0284(79.9138) | Bit/dim 3.7516(3.7531) | Xent 0.8513(0.8994) | Loss 4.1772(4.2028) | Error 0.3045(0.3207) Steps 892(877.55) | Grad Norm 2.4452(4.1949) | Total Time 14.00(14.00)\n",
      "Iter 1277 | Time 80.4416(79.9296) | Bit/dim 3.7564(3.7532) | Xent 0.8913(0.8991) | Loss 4.2021(4.2027) | Error 0.3169(0.3206) Steps 886(877.81) | Grad Norm 5.7826(4.2425) | Total Time 14.00(14.00)\n",
      "Iter 1278 | Time 78.0817(79.8742) | Bit/dim 3.7508(3.7531) | Xent 0.8935(0.8990) | Loss 4.1975(4.2026) | Error 0.3223(0.3207) Steps 892(878.23) | Grad Norm 8.2999(4.3643) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 30.7679, Epoch Time 524.7202(527.8867), Bit/dim 3.7543(best: 3.7517), Xent 1.0840, Loss 4.2963, Error 0.3812(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1279 | Time 76.1844(79.7635) | Bit/dim 3.7541(3.7531) | Xent 0.8823(0.8985) | Loss 4.1952(4.2024) | Error 0.3164(0.3205) Steps 874(878.10) | Grad Norm 6.1938(4.4192) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 80.9597(79.7994) | Bit/dim 3.7579(3.7533) | Xent 0.8571(0.8972) | Loss 4.1865(4.2019) | Error 0.3090(0.3202) Steps 886(878.34) | Grad Norm 2.0945(4.3494) | Total Time 14.00(14.00)\n",
      "Iter 1281 | Time 83.3766(79.9067) | Bit/dim 3.7410(3.7529) | Xent 0.8527(0.8959) | Loss 4.1674(4.2008) | Error 0.2985(0.3196) Steps 874(878.21) | Grad Norm 2.7803(4.3023) | Total Time 14.00(14.00)\n",
      "Iter 1282 | Time 79.3048(79.8886) | Bit/dim 3.7476(3.7527) | Xent 0.8680(0.8951) | Loss 4.1816(4.2003) | Error 0.3091(0.3192) Steps 868(877.90) | Grad Norm 2.8313(4.2582) | Total Time 14.00(14.00)\n",
      "Iter 1283 | Time 79.7092(79.8832) | Bit/dim 3.7453(3.7525) | Xent 0.8740(0.8944) | Loss 4.1823(4.1997) | Error 0.3136(0.3191) Steps 886(878.15) | Grad Norm 3.3031(4.2296) | Total Time 14.00(14.00)\n",
      "Iter 1284 | Time 78.3551(79.8374) | Bit/dim 3.7518(3.7525) | Xent 0.8680(0.8936) | Loss 4.1858(4.1993) | Error 0.3095(0.3188) Steps 886(878.38) | Grad Norm 4.5659(4.2396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 29.9781, Epoch Time 523.6287(527.7589), Bit/dim 3.7510(best: 3.7517), Xent 1.0897, Loss 4.2958, Error 0.3782(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1285 | Time 78.6857(79.8028) | Bit/dim 3.7320(3.7519) | Xent 0.8831(0.8933) | Loss 4.1735(4.1985) | Error 0.3121(0.3186) Steps 874(878.25) | Grad Norm 6.6002(4.3105) | Total Time 14.00(14.00)\n",
      "Iter 1286 | Time 78.7835(79.7723) | Bit/dim 3.7566(3.7520) | Xent 0.8552(0.8922) | Loss 4.1842(4.1981) | Error 0.3070(0.3182) Steps 880(878.30) | Grad Norm 7.2620(4.3990) | Total Time 14.00(14.00)\n",
      "Iter 1287 | Time 82.1538(79.8437) | Bit/dim 3.7438(3.7518) | Xent 0.8747(0.8917) | Loss 4.1811(4.1976) | Error 0.3083(0.3179) Steps 880(878.35) | Grad Norm 6.4736(4.4612) | Total Time 14.00(14.00)\n",
      "Iter 1288 | Time 80.1837(79.8539) | Bit/dim 3.7556(3.7519) | Xent 0.8431(0.8902) | Loss 4.1772(4.1970) | Error 0.3036(0.3175) Steps 886(878.58) | Grad Norm 2.9981(4.4174) | Total Time 14.00(14.00)\n",
      "Iter 1289 | Time 79.8641(79.8542) | Bit/dim 3.7498(3.7518) | Xent 0.8722(0.8897) | Loss 4.1859(4.1967) | Error 0.3096(0.3173) Steps 868(878.27) | Grad Norm 7.1903(4.5005) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 79.1933(79.8344) | Bit/dim 3.7560(3.7519) | Xent 0.9437(0.8913) | Loss 4.2279(4.1976) | Error 0.3430(0.3180) Steps 874(878.14) | Grad Norm 12.5683(4.7426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 30.5166, Epoch Time 525.1785(527.6815), Bit/dim 3.7635(best: 3.7510), Xent 1.2782, Loss 4.4027, Error 0.4345(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1291 | Time 80.4196(79.8519) | Bit/dim 3.7650(3.7523) | Xent 1.0744(0.8968) | Loss 4.3022(4.2007) | Error 0.3716(0.3196) Steps 886(878.37) | Grad Norm 13.2966(4.9992) | Total Time 14.00(14.00)\n",
      "Iter 1292 | Time 80.2824(79.8649) | Bit/dim 3.7599(3.7526) | Xent 1.0482(0.9013) | Loss 4.2840(4.2032) | Error 0.3725(0.3212) Steps 904(879.14) | Grad Norm 13.4960(5.2541) | Total Time 14.00(14.00)\n",
      "Iter 1293 | Time 79.7939(79.8627) | Bit/dim 3.7649(3.7529) | Xent 1.1428(0.9086) | Loss 4.3363(4.2072) | Error 0.3965(0.3235) Steps 856(878.45) | Grad Norm 16.3842(5.5880) | Total Time 14.00(14.00)\n",
      "Iter 1294 | Time 79.3493(79.8473) | Bit/dim 3.7498(3.7528) | Xent 1.0322(0.9123) | Loss 4.2659(4.2090) | Error 0.3718(0.3249) Steps 898(879.04) | Grad Norm 4.6395(5.5595) | Total Time 14.00(14.00)\n",
      "Iter 1295 | Time 80.6157(79.8704) | Bit/dim 3.7774(3.7536) | Xent 1.0464(0.9163) | Loss 4.3005(4.2117) | Error 0.3680(0.3262) Steps 898(879.60) | Grad Norm 9.9157(5.6902) | Total Time 14.00(14.00)\n",
      "Iter 1296 | Time 81.8607(79.9301) | Bit/dim 3.7662(3.7540) | Xent 0.9798(0.9182) | Loss 4.2561(4.2131) | Error 0.3528(0.3270) Steps 892(879.98) | Grad Norm 5.2320(5.6765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 30.9366, Epoch Time 529.2169(527.7276), Bit/dim 3.7691(best: 3.7510), Xent 1.1279, Loss 4.3331, Error 0.3992(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1297 | Time 79.3589(79.9130) | Bit/dim 3.7636(3.7542) | Xent 0.9879(0.9203) | Loss 4.2575(4.2144) | Error 0.3495(0.3277) Steps 904(880.70) | Grad Norm 6.1006(5.6892) | Total Time 14.00(14.00)\n",
      "Iter 1298 | Time 80.0848(79.9181) | Bit/dim 3.7689(3.7547) | Xent 0.9799(0.9221) | Loss 4.2588(4.2157) | Error 0.3464(0.3283) Steps 892(881.04) | Grad Norm 6.9062(5.7257) | Total Time 14.00(14.00)\n",
      "Iter 1299 | Time 80.7388(79.9427) | Bit/dim 3.7579(3.7548) | Xent 0.9795(0.9238) | Loss 4.2477(4.2167) | Error 0.3498(0.3289) Steps 886(881.18) | Grad Norm 3.3080(5.6532) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 82.1596(80.0092) | Bit/dim 3.7705(3.7553) | Xent 0.9348(0.9241) | Loss 4.2379(4.2173) | Error 0.3285(0.3289) Steps 904(881.87) | Grad Norm 4.1701(5.6087) | Total Time 14.00(14.00)\n",
      "Iter 1301 | Time 89.0590(80.2807) | Bit/dim 3.7739(3.7558) | Xent 0.9854(0.9260) | Loss 4.2667(4.2188) | Error 0.3519(0.3296) Steps 898(882.35) | Grad Norm 6.0396(5.6216) | Total Time 14.00(14.00)\n",
      "Iter 1302 | Time 89.1032(80.5454) | Bit/dim 3.7536(3.7558) | Xent 0.9363(0.9263) | Loss 4.2218(4.2189) | Error 0.3390(0.3299) Steps 910(883.18) | Grad Norm 4.8458(5.5983) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 30.2944, Epoch Time 546.6132(528.2941), Bit/dim 3.7638(best: 3.7510), Xent 1.1028, Loss 4.3152, Error 0.3847(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1303 | Time 86.3885(80.7207) | Bit/dim 3.7678(3.7561) | Xent 0.9473(0.9269) | Loss 4.2415(4.2196) | Error 0.3335(0.3300) Steps 874(882.91) | Grad Norm 4.1627(5.5553) | Total Time 14.00(14.00)\n",
      "Iter 1304 | Time 89.9193(80.9967) | Bit/dim 3.7526(3.7560) | Xent 0.9320(0.9271) | Loss 4.2186(4.2195) | Error 0.3310(0.3300) Steps 880(882.82) | Grad Norm 4.4395(5.5218) | Total Time 14.00(14.00)\n",
      "Iter 1305 | Time 90.2316(81.2737) | Bit/dim 3.7549(3.7560) | Xent 0.8945(0.9261) | Loss 4.2022(4.2190) | Error 0.3185(0.3297) Steps 886(882.92) | Grad Norm 3.6790(5.4665) | Total Time 14.00(14.00)\n",
      "Iter 1306 | Time 92.7023(81.6166) | Bit/dim 3.7579(3.7560) | Xent 0.8940(0.9251) | Loss 4.2049(4.2186) | Error 0.3169(0.3293) Steps 874(882.65) | Grad Norm 3.1254(5.3963) | Total Time 14.00(14.00)\n",
      "Iter 1307 | Time 91.0104(81.8984) | Bit/dim 3.7628(3.7562) | Xent 0.9145(0.9248) | Loss 4.2200(4.2186) | Error 0.3260(0.3292) Steps 874(882.39) | Grad Norm 5.0513(5.3859) | Total Time 14.00(14.00)\n",
      "Iter 1308 | Time 89.2512(82.1190) | Bit/dim 3.7611(3.7564) | Xent 0.9085(0.9243) | Loss 4.2153(4.2185) | Error 0.3253(0.3291) Steps 874(882.14) | Grad Norm 4.5627(5.3612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 30.5279, Epoch Time 585.7118(530.0167), Bit/dim 3.7556(best: 3.7510), Xent 1.0831, Loss 4.2972, Error 0.3766(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1309 | Time 85.9011(82.2324) | Bit/dim 3.7570(3.7564) | Xent 0.8910(0.9233) | Loss 4.2025(4.2181) | Error 0.3154(0.3287) Steps 868(881.71) | Grad Norm 3.4112(5.3027) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 88.1607(82.4103) | Bit/dim 3.7542(3.7563) | Xent 0.9225(0.9233) | Loss 4.2155(4.2180) | Error 0.3255(0.3286) Steps 868(881.30) | Grad Norm 5.0847(5.2962) | Total Time 14.00(14.00)\n",
      "Iter 1311 | Time 91.0010(82.6680) | Bit/dim 3.7517(3.7562) | Xent 0.8932(0.9224) | Loss 4.1983(4.2174) | Error 0.3199(0.3283) Steps 886(881.44) | Grad Norm 3.4339(5.2403) | Total Time 14.00(14.00)\n",
      "Iter 1312 | Time 92.7521(82.9705) | Bit/dim 3.7535(3.7561) | Xent 0.8875(0.9213) | Loss 4.1972(4.2168) | Error 0.3220(0.3281) Steps 892(881.76) | Grad Norm 4.6334(5.2221) | Total Time 14.00(14.00)\n",
      "Iter 1313 | Time 92.8582(83.2671) | Bit/dim 3.7545(3.7561) | Xent 0.8514(0.9192) | Loss 4.1801(4.2157) | Error 0.3041(0.3274) Steps 874(881.53) | Grad Norm 3.4722(5.1696) | Total Time 14.00(14.00)\n",
      "Iter 1314 | Time 87.1925(83.3849) | Bit/dim 3.7527(3.7560) | Xent 0.8629(0.9176) | Loss 4.1841(4.2147) | Error 0.3146(0.3270) Steps 874(881.30) | Grad Norm 3.1034(5.1076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 30.7963, Epoch Time 584.1491(531.6406), Bit/dim 3.7541(best: 3.7510), Xent 1.0763, Loss 4.2923, Error 0.3770(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1315 | Time 95.2308(83.7403) | Bit/dim 3.7553(3.7559) | Xent 0.8634(0.9159) | Loss 4.1870(4.2139) | Error 0.3109(0.3265) Steps 898(881.80) | Grad Norm 2.9963(5.0443) | Total Time 14.00(14.00)\n",
      "Iter 1316 | Time 90.0763(83.9304) | Bit/dim 3.7441(3.7556) | Xent 0.8510(0.9140) | Loss 4.1696(4.2126) | Error 0.2987(0.3257) Steps 886(881.93) | Grad Norm 1.8384(4.9481) | Total Time 14.00(14.00)\n",
      "Iter 1317 | Time 92.0678(84.1745) | Bit/dim 3.7552(3.7556) | Xent 0.8566(0.9123) | Loss 4.1835(4.2117) | Error 0.3091(0.3252) Steps 898(882.41) | Grad Norm 2.4724(4.8739) | Total Time 14.00(14.00)\n",
      "Iter 1318 | Time 94.6034(84.4874) | Bit/dim 3.7522(3.7555) | Xent 0.8435(0.9102) | Loss 4.1740(4.2106) | Error 0.3004(0.3244) Steps 904(883.06) | Grad Norm 3.2985(4.8266) | Total Time 14.00(14.00)\n",
      "Iter 1319 | Time 91.0536(84.6843) | Bit/dim 3.7468(3.7552) | Xent 0.8438(0.9082) | Loss 4.1687(4.2093) | Error 0.2969(0.3236) Steps 892(883.33) | Grad Norm 2.9158(4.7693) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 93.2338(84.9408) | Bit/dim 3.7550(3.7552) | Xent 0.8756(0.9072) | Loss 4.1928(4.2088) | Error 0.3129(0.3233) Steps 886(883.41) | Grad Norm 5.3947(4.7880) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 31.1962, Epoch Time 603.0539(533.7830), Bit/dim 3.7549(best: 3.7510), Xent 1.0875, Loss 4.2986, Error 0.3791(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1321 | Time 91.4420(85.1359) | Bit/dim 3.7521(3.7551) | Xent 0.8626(0.9059) | Loss 4.1834(4.2081) | Error 0.3087(0.3229) Steps 898(883.84) | Grad Norm 4.0940(4.7672) | Total Time 14.00(14.00)\n",
      "Iter 1322 | Time 91.0556(85.3135) | Bit/dim 3.7570(3.7552) | Xent 0.8819(0.9052) | Loss 4.1979(4.2078) | Error 0.3140(0.3226) Steps 892(884.09) | Grad Norm 6.4954(4.8191) | Total Time 14.00(14.00)\n",
      "Iter 1323 | Time 90.6737(85.4743) | Bit/dim 3.7466(3.7549) | Xent 0.8755(0.9043) | Loss 4.1844(4.2071) | Error 0.3139(0.3223) Steps 880(883.97) | Grad Norm 5.4470(4.8379) | Total Time 14.00(14.00)\n",
      "Iter 1324 | Time 92.3034(85.6791) | Bit/dim 3.7397(3.7545) | Xent 0.8607(0.9030) | Loss 4.1701(4.2059) | Error 0.3104(0.3220) Steps 892(884.21) | Grad Norm 3.9705(4.8119) | Total Time 14.00(14.00)\n",
      "Iter 1325 | Time 96.6473(86.0082) | Bit/dim 3.7484(3.7543) | Xent 0.8711(0.9020) | Loss 4.1839(4.2053) | Error 0.3141(0.3217) Steps 892(884.44) | Grad Norm 6.6053(4.8657) | Total Time 14.00(14.00)\n",
      "Iter 1326 | Time 91.0212(86.1586) | Bit/dim 3.7498(3.7541) | Xent 0.8471(0.9004) | Loss 4.1734(4.2043) | Error 0.3034(0.3212) Steps 886(884.49) | Grad Norm 2.8541(4.8053) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 30.1389, Epoch Time 599.3051(535.7487), Bit/dim 3.7512(best: 3.7510), Xent 1.0872, Loss 4.2948, Error 0.3791(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1327 | Time 90.7459(86.2962) | Bit/dim 3.7540(3.7541) | Xent 0.8689(0.8994) | Loss 4.1884(4.2038) | Error 0.3153(0.3210) Steps 886(884.53) | Grad Norm 5.1946(4.8170) | Total Time 14.00(14.00)\n",
      "Iter 1328 | Time 93.3255(86.5071) | Bit/dim 3.7455(3.7539) | Xent 0.8674(0.8985) | Loss 4.1792(4.2031) | Error 0.3111(0.3207) Steps 874(884.22) | Grad Norm 4.9073(4.8197) | Total Time 14.00(14.00)\n",
      "Iter 1329 | Time 92.2638(86.6798) | Bit/dim 3.7468(3.7537) | Xent 0.8714(0.8977) | Loss 4.1825(4.2025) | Error 0.3091(0.3204) Steps 898(884.63) | Grad Norm 3.7950(4.7890) | Total Time 14.00(14.00)\n",
      "Iter 1330 | Time 90.2917(86.7881) | Bit/dim 3.7462(3.7534) | Xent 0.8666(0.8967) | Loss 4.1795(4.2018) | Error 0.3073(0.3200) Steps 880(884.49) | Grad Norm 4.8770(4.7916) | Total Time 14.00(14.00)\n",
      "Iter 1331 | Time 89.5290(86.8704) | Bit/dim 3.7452(3.7532) | Xent 0.8566(0.8955) | Loss 4.1736(4.2010) | Error 0.3061(0.3196) Steps 886(884.54) | Grad Norm 4.4416(4.7811) | Total Time 14.00(14.00)\n",
      "Iter 1332 | Time 88.9629(86.9331) | Bit/dim 3.7452(3.7530) | Xent 0.8210(0.8933) | Loss 4.1557(4.1996) | Error 0.2885(0.3186) Steps 898(884.94) | Grad Norm 5.7307(4.8096) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 30.4548, Epoch Time 591.1293(537.4101), Bit/dim 3.7516(best: 3.7510), Xent 1.0734, Loss 4.2883, Error 0.3756(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1333 | Time 91.5661(87.0721) | Bit/dim 3.7488(3.7528) | Xent 0.8411(0.8917) | Loss 4.1693(4.1987) | Error 0.2999(0.3181) Steps 904(885.51) | Grad Norm 4.0187(4.7859) | Total Time 14.00(14.00)\n",
      "Iter 1334 | Time 91.9131(87.2174) | Bit/dim 3.7588(3.7530) | Xent 0.8165(0.8895) | Loss 4.1670(4.1977) | Error 0.2929(0.3173) Steps 898(885.89) | Grad Norm 6.7675(4.8453) | Total Time 14.00(14.00)\n",
      "Iter 1335 | Time 92.1455(87.3652) | Bit/dim 3.7384(3.7526) | Xent 0.8531(0.8884) | Loss 4.1649(4.1968) | Error 0.3064(0.3170) Steps 874(885.53) | Grad Norm 5.1695(4.8550) | Total Time 14.00(14.00)\n",
      "Iter 1336 | Time 90.3064(87.4534) | Bit/dim 3.7477(3.7524) | Xent 0.8365(0.8868) | Loss 4.1659(4.1958) | Error 0.2947(0.3163) Steps 880(885.36) | Grad Norm 2.8368(4.7945) | Total Time 14.00(14.00)\n",
      "Iter 1337 | Time 90.6896(87.5505) | Bit/dim 3.7377(3.7520) | Xent 0.8475(0.8856) | Loss 4.1615(4.1948) | Error 0.3041(0.3159) Steps 868(884.84) | Grad Norm 3.1743(4.7459) | Total Time 14.00(14.00)\n",
      "Iter 1338 | Time 92.2380(87.6911) | Bit/dim 3.7553(3.7521) | Xent 0.8415(0.8843) | Loss 4.1760(4.1942) | Error 0.2923(0.3152) Steps 898(885.24) | Grad Norm 3.1300(4.6974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 30.2573, Epoch Time 594.9113(539.1352), Bit/dim 3.7482(best: 3.7510), Xent 1.0548, Loss 4.2757, Error 0.3720(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1339 | Time 92.3963(87.8323) | Bit/dim 3.7392(3.7517) | Xent 0.8247(0.8825) | Loss 4.1516(4.1930) | Error 0.2920(0.3145) Steps 886(885.26) | Grad Norm 2.9706(4.6456) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 91.1861(87.9329) | Bit/dim 3.7455(3.7515) | Xent 0.8445(0.8814) | Loss 4.1677(4.1922) | Error 0.2963(0.3140) Steps 862(884.56) | Grad Norm 2.7166(4.5877) | Total Time 14.00(14.00)\n",
      "Iter 1341 | Time 89.1321(87.9689) | Bit/dim 3.7452(3.7513) | Xent 0.8238(0.8797) | Loss 4.1571(4.1911) | Error 0.2953(0.3134) Steps 886(884.61) | Grad Norm 2.1819(4.5156) | Total Time 14.00(14.00)\n",
      "Iter 1342 | Time 96.4224(88.2225) | Bit/dim 3.7515(3.7513) | Xent 0.8399(0.8785) | Loss 4.1715(4.1906) | Error 0.3030(0.3131) Steps 904(885.19) | Grad Norm 3.3146(4.4795) | Total Time 14.00(14.00)\n",
      "Iter 1343 | Time 93.6185(88.3844) | Bit/dim 3.7462(3.7512) | Xent 0.8075(0.8763) | Loss 4.1499(4.1893) | Error 0.2905(0.3124) Steps 874(884.85) | Grad Norm 3.3214(4.4448) | Total Time 14.00(14.00)\n",
      "Iter 1344 | Time 90.1971(88.4388) | Bit/dim 3.7427(3.7509) | Xent 0.8153(0.8745) | Loss 4.1504(4.1882) | Error 0.2931(0.3119) Steps 886(884.89) | Grad Norm 3.1330(4.4054) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 30.0447, Epoch Time 598.5579(540.9178), Bit/dim 3.7478(best: 3.7482), Xent 1.0976, Loss 4.2966, Error 0.3793(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1345 | Time 91.0851(88.5181) | Bit/dim 3.7461(3.7508) | Xent 0.8424(0.8735) | Loss 4.1673(4.1875) | Error 0.2985(0.3115) Steps 880(884.74) | Grad Norm 3.7295(4.3852) | Total Time 14.00(14.00)\n",
      "Iter 1346 | Time 93.9443(88.6809) | Bit/dim 3.7387(3.7504) | Xent 0.8481(0.8728) | Loss 4.1627(4.1868) | Error 0.3036(0.3112) Steps 898(885.14) | Grad Norm 5.0353(4.4047) | Total Time 14.00(14.00)\n",
      "Iter 1347 | Time 87.4351(88.6436) | Bit/dim 3.7429(3.7502) | Xent 0.8287(0.8715) | Loss 4.1572(4.1859) | Error 0.2944(0.3107) Steps 874(884.80) | Grad Norm 4.6560(4.4122) | Total Time 14.00(14.00)\n",
      "Iter 1348 | Time 90.2518(88.6918) | Bit/dim 3.7468(3.7501) | Xent 0.8471(0.8707) | Loss 4.1704(4.1854) | Error 0.3011(0.3104) Steps 868(884.30) | Grad Norm 4.7492(4.4223) | Total Time 14.00(14.00)\n",
      "Iter 1349 | Time 92.4257(88.8038) | Bit/dim 3.7489(3.7500) | Xent 0.8396(0.8698) | Loss 4.1687(4.1849) | Error 0.3050(0.3103) Steps 898(884.71) | Grad Norm 2.9380(4.3778) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 94.0819(88.9622) | Bit/dim 3.7394(3.7497) | Xent 0.8273(0.8685) | Loss 4.1531(4.1840) | Error 0.2943(0.3098) Steps 874(884.39) | Grad Norm 2.4780(4.3208) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 30.6294, Epoch Time 595.7747(542.5635), Bit/dim 3.7466(best: 3.7478), Xent 1.0503, Loss 4.2717, Error 0.3657(best: 0.3705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1351 | Time 92.6491(89.0728) | Bit/dim 3.7546(3.7499) | Xent 0.8049(0.8666) | Loss 4.1571(4.1832) | Error 0.2906(0.3092) Steps 886(884.44) | Grad Norm 1.7436(4.2435) | Total Time 14.00(14.00)\n",
      "Iter 1352 | Time 92.3201(89.1702) | Bit/dim 3.7375(3.7495) | Xent 0.8381(0.8657) | Loss 4.1566(4.1824) | Error 0.3006(0.3090) Steps 868(883.94) | Grad Norm 2.4932(4.1910) | Total Time 14.00(14.00)\n",
      "Iter 1353 | Time 92.4403(89.2683) | Bit/dim 3.7377(3.7492) | Xent 0.8303(0.8647) | Loss 4.1529(4.1815) | Error 0.2957(0.3086) Steps 892(884.19) | Grad Norm 3.9199(4.1828) | Total Time 14.00(14.00)\n",
      "Iter 1354 | Time 95.4390(89.4534) | Bit/dim 3.7409(3.7489) | Xent 0.8100(0.8630) | Loss 4.1459(4.1804) | Error 0.2930(0.3081) Steps 886(884.24) | Grad Norm 3.5917(4.1651) | Total Time 14.00(14.00)\n",
      "Iter 1355 | Time 89.1315(89.4438) | Bit/dim 3.7591(3.7492) | Xent 0.8090(0.8614) | Loss 4.1636(4.1799) | Error 0.2871(0.3075) Steps 880(884.11) | Grad Norm 1.8536(4.0958) | Total Time 14.00(14.00)\n",
      "Iter 1356 | Time 95.6918(89.6312) | Bit/dim 3.7332(3.7487) | Xent 0.8269(0.8604) | Loss 4.1466(4.1789) | Error 0.2986(0.3072) Steps 886(884.17) | Grad Norm 6.0805(4.1553) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 30.0993, Epoch Time 603.5077(544.3919), Bit/dim 3.7509(best: 3.7466), Xent 1.1116, Loss 4.3067, Error 0.3848(best: 0.3657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1357 | Time 88.2265(89.5891) | Bit/dim 3.7444(3.7486) | Xent 0.8399(0.8598) | Loss 4.1643(4.1785) | Error 0.3006(0.3070) Steps 874(883.87) | Grad Norm 9.8856(4.3272) | Total Time 14.00(14.00)\n",
      "Iter 1358 | Time 89.7655(89.5943) | Bit/dim 3.7501(3.7486) | Xent 0.8735(0.8602) | Loss 4.1868(4.1787) | Error 0.3087(0.3071) Steps 886(883.93) | Grad Norm 9.2751(4.4756) | Total Time 14.00(14.00)\n",
      "Iter 1359 | Time 92.9698(89.6956) | Bit/dim 3.7584(3.7489) | Xent 0.8680(0.8604) | Loss 4.1924(4.1791) | Error 0.3129(0.3072) Steps 898(884.35) | Grad Norm 8.0972(4.5843) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 90.4770(89.7191) | Bit/dim 3.7390(3.7486) | Xent 0.8280(0.8594) | Loss 4.1529(4.1784) | Error 0.2943(0.3068) Steps 886(884.40) | Grad Norm 3.4630(4.5507) | Total Time 14.00(14.00)\n",
      "Iter 1361 | Time 91.7657(89.7805) | Bit/dim 3.7444(3.7485) | Xent 0.8595(0.8594) | Loss 4.1741(4.1782) | Error 0.3036(0.3067) Steps 886(884.45) | Grad Norm 4.0866(4.5367) | Total Time 14.00(14.00)\n",
      "Iter 1362 | Time 96.3887(89.9787) | Bit/dim 3.7434(3.7484) | Xent 0.8423(0.8589) | Loss 4.1645(4.1778) | Error 0.2971(0.3065) Steps 892(884.68) | Grad Norm 3.8222(4.5153) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 30.6097, Epoch Time 596.0358(545.9412), Bit/dim 3.7457(best: 3.7466), Xent 1.0861, Loss 4.2888, Error 0.3798(best: 0.3657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1363 | Time 91.4594(90.0231) | Bit/dim 3.7398(3.7481) | Xent 0.8707(0.8593) | Loss 4.1752(4.1777) | Error 0.3153(0.3067) Steps 892(884.89) | Grad Norm 3.7485(4.4923) | Total Time 14.00(14.00)\n",
      "Iter 1364 | Time 88.7544(89.9851) | Bit/dim 3.7413(3.7479) | Xent 0.8316(0.8585) | Loss 4.1571(4.1771) | Error 0.3013(0.3066) Steps 862(884.21) | Grad Norm 3.2773(4.4558) | Total Time 14.00(14.00)\n",
      "Iter 1365 | Time 92.7301(90.0674) | Bit/dim 3.7495(3.7479) | Xent 0.8182(0.8572) | Loss 4.1586(4.1766) | Error 0.2905(0.3061) Steps 874(883.90) | Grad Norm 2.3358(4.3922) | Total Time 14.00(14.00)\n",
      "Iter 1366 | Time 93.1484(90.1598) | Bit/dim 3.7388(3.7477) | Xent 0.8381(0.8567) | Loss 4.1579(4.1760) | Error 0.2984(0.3058) Steps 910(884.68) | Grad Norm 3.0991(4.3534) | Total Time 14.00(14.00)\n",
      "Iter 1367 | Time 90.3501(90.1655) | Bit/dim 3.7537(3.7479) | Xent 0.8059(0.8551) | Loss 4.1566(4.1754) | Error 0.2850(0.3052) Steps 898(885.08) | Grad Norm 2.8354(4.3079) | Total Time 14.00(14.00)\n",
      "Iter 1368 | Time 92.4221(90.2332) | Bit/dim 3.7425(3.7477) | Xent 0.8119(0.8538) | Loss 4.1485(4.1746) | Error 0.2913(0.3048) Steps 886(885.11) | Grad Norm 2.7212(4.2603) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 30.3215, Epoch Time 595.1968(547.4189), Bit/dim 3.7487(best: 3.7457), Xent 1.0647, Loss 4.2810, Error 0.3703(best: 0.3657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1369 | Time 89.8335(90.2213) | Bit/dim 3.7492(3.7477) | Xent 0.8258(0.8530) | Loss 4.1621(4.1742) | Error 0.3009(0.3047) Steps 880(884.96) | Grad Norm 4.7097(4.2738) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 92.4861(90.2892) | Bit/dim 3.7390(3.7475) | Xent 0.7973(0.8513) | Loss 4.1377(4.1731) | Error 0.2824(0.3040) Steps 868(884.45) | Grad Norm 4.0233(4.2663) | Total Time 14.00(14.00)\n",
      "Iter 1371 | Time 91.0813(90.3130) | Bit/dim 3.7442(3.7474) | Xent 0.8147(0.8502) | Loss 4.1516(4.1725) | Error 0.2905(0.3036) Steps 868(883.96) | Grad Norm 2.3581(4.2090) | Total Time 14.00(14.00)\n",
      "Iter 1372 | Time 93.8798(90.4200) | Bit/dim 3.7462(3.7473) | Xent 0.8082(0.8490) | Loss 4.1503(4.1718) | Error 0.2881(0.3031) Steps 886(884.02) | Grad Norm 2.1290(4.1466) | Total Time 14.00(14.00)\n",
      "Iter 1373 | Time 92.7776(90.4907) | Bit/dim 3.7357(3.7470) | Xent 0.8116(0.8479) | Loss 4.1415(4.1709) | Error 0.2881(0.3027) Steps 886(884.08) | Grad Norm 4.3443(4.1526) | Total Time 14.00(14.00)\n",
      "Iter 1374 | Time 95.8348(90.6510) | Bit/dim 3.7540(3.7472) | Xent 0.8075(0.8466) | Loss 4.1578(4.1705) | Error 0.2900(0.3023) Steps 898(884.49) | Grad Norm 6.0677(4.2100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 30.9461, Epoch Time 602.5307(549.0722), Bit/dim 3.7444(best: 3.7457), Xent 1.0917, Loss 4.2903, Error 0.3743(best: 0.3657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1375 | Time 91.5772(90.6788) | Bit/dim 3.7339(3.7468) | Xent 0.8011(0.8453) | Loss 4.1344(4.1694) | Error 0.2886(0.3019) Steps 898(884.90) | Grad Norm 5.5502(4.2502) | Total Time 14.00(14.00)\n",
      "Iter 1376 | Time 90.6913(90.6792) | Bit/dim 3.7398(3.7466) | Xent 0.8218(0.8446) | Loss 4.1507(4.1689) | Error 0.3001(0.3018) Steps 892(885.11) | Grad Norm 7.3819(4.3442) | Total Time 14.00(14.00)\n",
      "Iter 1377 | Time 89.6120(90.6472) | Bit/dim 3.7500(3.7467) | Xent 0.8316(0.8442) | Loss 4.1658(4.1688) | Error 0.2947(0.3016) Steps 868(884.60) | Grad Norm 4.7385(4.3560) | Total Time 14.00(14.00)\n",
      "Iter 1378 | Time 87.9746(90.5670) | Bit/dim 3.7439(3.7466) | Xent 0.8279(0.8437) | Loss 4.1579(4.1685) | Error 0.2957(0.3015) Steps 892(884.82) | Grad Norm 4.6083(4.3636) | Total Time 14.00(14.00)\n",
      "Iter 1379 | Time 94.4152(90.6824) | Bit/dim 3.7425(3.7465) | Xent 0.7882(0.8420) | Loss 4.1366(4.1675) | Error 0.2835(0.3009) Steps 880(884.68) | Grad Norm 4.0006(4.3527) | Total Time 14.00(14.00)\n",
      "Iter 1380 | Time 90.5902(90.6797) | Bit/dim 3.7410(3.7463) | Xent 0.8029(0.8409) | Loss 4.1425(4.1668) | Error 0.2827(0.3004) Steps 880(884.54) | Grad Norm 3.0852(4.3147) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 30.3418, Epoch Time 591.0384(550.3312), Bit/dim 3.7424(best: 3.7444), Xent 1.0525, Loss 4.2687, Error 0.3652(best: 0.3657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1381 | Time 92.5220(90.7349) | Bit/dim 3.7373(3.7461) | Xent 0.7721(0.8388) | Loss 4.1234(4.1655) | Error 0.2798(0.2998) Steps 868(884.04) | Grad Norm 2.9991(4.2752) | Total Time 14.00(14.00)\n",
      "Iter 1382 | Time 93.9972(90.8328) | Bit/dim 3.7511(3.7462) | Xent 0.7931(0.8374) | Loss 4.1477(4.1649) | Error 0.2826(0.2992) Steps 880(883.92) | Grad Norm 3.1554(4.2416) | Total Time 14.00(14.00)\n",
      "Iter 1383 | Time 91.4585(90.8516) | Bit/dim 3.7385(3.7460) | Xent 0.8209(0.8369) | Loss 4.1490(4.1644) | Error 0.2955(0.2991) Steps 874(883.62) | Grad Norm 5.0859(4.2669) | Total Time 14.00(14.00)\n",
      "Iter 1384 | Time 86.8144(90.7305) | Bit/dim 3.7404(3.7458) | Xent 0.7760(0.8351) | Loss 4.1284(4.1634) | Error 0.2804(0.2986) Steps 874(883.33) | Grad Norm 5.9280(4.3168) | Total Time 14.00(14.00)\n",
      "Iter 1385 | Time 94.3926(90.8403) | Bit/dim 3.7286(3.7453) | Xent 0.8220(0.8347) | Loss 4.1396(4.1626) | Error 0.2917(0.2984) Steps 880(883.23) | Grad Norm 4.0747(4.3095) | Total Time 14.00(14.00)\n",
      "Iter 1386 | Time 86.9142(90.7225) | Bit/dim 3.7428(3.7452) | Xent 0.7978(0.8336) | Loss 4.1417(4.1620) | Error 0.2808(0.2978) Steps 898(883.68) | Grad Norm 2.5343(4.2562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 30.8282, Epoch Time 593.0624(551.6131), Bit/dim 3.7418(best: 3.7424), Xent 1.0694, Loss 4.2765, Error 0.3704(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1387 | Time 87.1324(90.6148) | Bit/dim 3.7406(3.7451) | Xent 0.7887(0.8323) | Loss 4.1350(4.1612) | Error 0.2849(0.2974) Steps 886(883.75) | Grad Norm 3.5468(4.2350) | Total Time 14.00(14.00)\n",
      "Iter 1388 | Time 87.8776(90.5327) | Bit/dim 3.7290(3.7446) | Xent 0.7769(0.8306) | Loss 4.1175(4.1599) | Error 0.2785(0.2969) Steps 874(883.45) | Grad Norm 2.1003(4.1709) | Total Time 14.00(14.00)\n",
      "Iter 1389 | Time 92.9404(90.6050) | Bit/dim 3.7525(3.7448) | Xent 0.7759(0.8290) | Loss 4.1405(4.1593) | Error 0.2799(0.2964) Steps 874(883.17) | Grad Norm 4.4209(4.1784) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 91.0115(90.6172) | Bit/dim 3.7429(3.7448) | Xent 0.8101(0.8284) | Loss 4.1480(4.1590) | Error 0.2909(0.2962) Steps 892(883.43) | Grad Norm 5.1912(4.2088) | Total Time 14.00(14.00)\n",
      "Iter 1391 | Time 100.4409(90.9119) | Bit/dim 3.7400(3.7446) | Xent 0.7838(0.8271) | Loss 4.1319(4.1582) | Error 0.2798(0.2957) Steps 874(883.15) | Grad Norm 2.0291(4.1434) | Total Time 14.00(14.00)\n",
      "Iter 1392 | Time 95.1005(91.0375) | Bit/dim 3.7343(3.7443) | Xent 0.7721(0.8254) | Loss 4.1204(4.1570) | Error 0.2771(0.2951) Steps 874(882.88) | Grad Norm 3.2464(4.1165) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 30.0294, Epoch Time 609.2826(553.3432), Bit/dim 3.7444(best: 3.7418), Xent 1.0815, Loss 4.2852, Error 0.3721(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1393 | Time 88.1787(90.9518) | Bit/dim 3.7492(3.7445) | Xent 0.7802(0.8240) | Loss 4.1393(4.1565) | Error 0.2837(0.2948) Steps 898(883.33) | Grad Norm 4.6597(4.1328) | Total Time 14.00(14.00)\n",
      "Iter 1394 | Time 93.6980(91.0341) | Bit/dim 3.7429(3.7444) | Xent 0.7889(0.8230) | Loss 4.1374(4.1559) | Error 0.2835(0.2945) Steps 880(883.23) | Grad Norm 3.4057(4.1110) | Total Time 14.00(14.00)\n",
      "Iter 1395 | Time 89.9807(91.0025) | Bit/dim 3.7363(3.7442) | Xent 0.7908(0.8220) | Loss 4.1317(4.1552) | Error 0.2821(0.2941) Steps 880(883.13) | Grad Norm 5.3144(4.1471) | Total Time 14.00(14.00)\n",
      "Iter 1396 | Time 92.4955(91.0473) | Bit/dim 3.7333(3.7439) | Xent 0.8240(0.8221) | Loss 4.1453(4.1549) | Error 0.2911(0.2940) Steps 880(883.04) | Grad Norm 6.5203(4.2183) | Total Time 14.00(14.00)\n",
      "Iter 1397 | Time 91.7649(91.0689) | Bit/dim 3.7389(3.7437) | Xent 0.8847(0.8240) | Loss 4.1812(4.1557) | Error 0.3187(0.2948) Steps 886(883.13) | Grad Norm 10.6432(4.4110) | Total Time 14.00(14.00)\n",
      "Iter 1398 | Time 90.5183(91.0523) | Bit/dim 3.7542(3.7440) | Xent 0.9920(0.8290) | Loss 4.2502(4.1585) | Error 0.3481(0.2964) Steps 898(883.57) | Grad Norm 12.4883(4.6533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 30.3713, Epoch Time 592.6527(554.5225), Bit/dim 3.7489(best: 3.7418), Xent 1.2000, Loss 4.3489, Error 0.4092(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1399 | Time 90.3987(91.0327) | Bit/dim 3.7361(3.7438) | Xent 0.9563(0.8328) | Loss 4.2142(4.1602) | Error 0.3389(0.2976) Steps 892(883.83) | Grad Norm 13.8265(4.9285) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 89.8464(90.9971) | Bit/dim 3.7563(3.7442) | Xent 1.0042(0.8380) | Loss 4.2584(4.1631) | Error 0.3608(0.2995) Steps 898(884.25) | Grad Norm 11.8742(5.1369) | Total Time 14.00(14.00)\n",
      "Iter 1401 | Time 91.4249(91.0100) | Bit/dim 3.7603(3.7446) | Xent 0.9909(0.8426) | Loss 4.2558(4.1659) | Error 0.3498(0.3010) Steps 904(884.85) | Grad Norm 8.2773(5.2311) | Total Time 14.00(14.00)\n",
      "Iter 1402 | Time 92.8274(91.0645) | Bit/dim 3.7383(3.7445) | Xent 0.8641(0.8432) | Loss 4.1703(4.1661) | Error 0.3090(0.3013) Steps 922(885.96) | Grad Norm 4.9803(5.2236) | Total Time 14.00(14.00)\n",
      "Iter 1403 | Time 92.9239(91.1203) | Bit/dim 3.7586(3.7449) | Xent 0.9624(0.8468) | Loss 4.2398(4.1683) | Error 0.3481(0.3027) Steps 916(886.86) | Grad Norm 5.0065(5.2171) | Total Time 14.00(14.00)\n",
      "Iter 1404 | Time 95.0253(91.2374) | Bit/dim 3.7519(3.7451) | Xent 0.9401(0.8496) | Loss 4.2219(4.1699) | Error 0.3345(0.3036) Steps 916(887.74) | Grad Norm 4.7453(5.2029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 30.4157, Epoch Time 598.7178(555.8484), Bit/dim 3.7514(best: 3.7418), Xent 1.1097, Loss 4.3062, Error 0.3907(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1405 | Time 92.3853(91.2719) | Bit/dim 3.7486(3.7452) | Xent 0.8745(0.8503) | Loss 4.1859(4.1704) | Error 0.3117(0.3039) Steps 904(888.22) | Grad Norm 4.7291(5.1887) | Total Time 14.00(14.00)\n",
      "Iter 1406 | Time 93.0187(91.3243) | Bit/dim 3.7552(3.7455) | Xent 0.8739(0.8510) | Loss 4.1921(4.1710) | Error 0.3114(0.3041) Steps 886(888.16) | Grad Norm 3.4115(5.1354) | Total Time 14.00(14.00)\n",
      "Iter 1407 | Time 92.1174(91.3481) | Bit/dim 3.7494(3.7456) | Xent 0.9021(0.8526) | Loss 4.2004(4.1719) | Error 0.3209(0.3046) Steps 904(888.63) | Grad Norm 7.5183(5.2069) | Total Time 14.00(14.00)\n",
      "Iter 1408 | Time 97.2910(91.5264) | Bit/dim 3.7488(3.7457) | Xent 0.8617(0.8528) | Loss 4.1796(4.1721) | Error 0.3030(0.3046) Steps 874(888.19) | Grad Norm 2.5950(5.1285) | Total Time 14.00(14.00)\n",
      "Iter 1409 | Time 91.6754(91.5308) | Bit/dim 3.7523(3.7459) | Xent 0.8491(0.8527) | Loss 4.1769(4.1723) | Error 0.3029(0.3045) Steps 874(887.77) | Grad Norm 5.8150(5.1491) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 96.0770(91.6672) | Bit/dim 3.7452(3.7459) | Xent 0.8203(0.8518) | Loss 4.1553(4.1718) | Error 0.2897(0.3041) Steps 868(887.17) | Grad Norm 2.5998(5.0726) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 30.0701, Epoch Time 613.1518(557.5675), Bit/dim 3.7541(best: 3.7418), Xent 1.0820, Loss 4.2951, Error 0.3745(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1411 | Time 91.3710(91.6583) | Bit/dim 3.7491(3.7460) | Xent 0.8187(0.8508) | Loss 4.1584(4.1714) | Error 0.2947(0.3038) Steps 868(886.60) | Grad Norm 3.9148(5.0379) | Total Time 14.00(14.00)\n",
      "Iter 1412 | Time 89.6534(91.5982) | Bit/dim 3.7448(3.7459) | Xent 0.8161(0.8497) | Loss 4.1528(4.1708) | Error 0.2889(0.3033) Steps 862(885.86) | Grad Norm 3.6814(4.9972) | Total Time 14.00(14.00)\n",
      "Iter 1413 | Time 90.9192(91.5778) | Bit/dim 3.7515(3.7461) | Xent 0.8253(0.8490) | Loss 4.1641(4.1706) | Error 0.2975(0.3032) Steps 886(885.86) | Grad Norm 2.5118(4.9227) | Total Time 14.00(14.00)\n",
      "Iter 1414 | Time 90.1176(91.5340) | Bit/dim 3.7553(3.7464) | Xent 0.7971(0.8474) | Loss 4.1539(4.1701) | Error 0.2863(0.3027) Steps 880(885.69) | Grad Norm 4.0404(4.8962) | Total Time 14.00(14.00)\n",
      "Iter 1415 | Time 90.8341(91.5130) | Bit/dim 3.7451(3.7463) | Xent 0.8170(0.8465) | Loss 4.1537(4.1696) | Error 0.2879(0.3022) Steps 862(884.98) | Grad Norm 2.5258(4.8251) | Total Time 14.00(14.00)\n",
      "Iter 1416 | Time 87.4969(91.3925) | Bit/dim 3.7485(3.7464) | Xent 0.8035(0.8452) | Loss 4.1502(4.1690) | Error 0.2869(0.3017) Steps 880(884.83) | Grad Norm 3.0452(4.7717) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 30.7772, Epoch Time 587.4302(558.4633), Bit/dim 3.7470(best: 3.7418), Xent 1.0934, Loss 4.2937, Error 0.3741(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1417 | Time 89.8645(91.3467) | Bit/dim 3.7444(3.7463) | Xent 0.7975(0.8438) | Loss 4.1432(4.1682) | Error 0.2877(0.3013) Steps 874(884.50) | Grad Norm 2.8938(4.7153) | Total Time 14.00(14.00)\n",
      "Iter 1418 | Time 92.1132(91.3697) | Bit/dim 3.7420(3.7462) | Xent 0.8060(0.8427) | Loss 4.1451(4.1676) | Error 0.2887(0.3009) Steps 886(884.55) | Grad Norm 3.7127(4.6853) | Total Time 14.00(14.00)\n",
      "Iter 1419 | Time 95.0594(91.4804) | Bit/dim 3.7439(3.7462) | Xent 0.8028(0.8415) | Loss 4.1453(4.1669) | Error 0.2867(0.3005) Steps 880(884.41) | Grad Norm 3.3549(4.6454) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 92.8311(91.5209) | Bit/dim 3.7504(3.7463) | Xent 0.7933(0.8400) | Loss 4.1470(4.1663) | Error 0.2877(0.3001) Steps 892(884.64) | Grad Norm 3.1353(4.6001) | Total Time 14.00(14.00)\n",
      "Iter 1421 | Time 91.7161(91.5267) | Bit/dim 3.7425(3.7462) | Xent 0.8013(0.8389) | Loss 4.1432(4.1656) | Error 0.2876(0.2998) Steps 898(885.04) | Grad Norm 5.3689(4.6231) | Total Time 14.00(14.00)\n",
      "Iter 1422 | Time 92.2337(91.5480) | Bit/dim 3.7396(3.7460) | Xent 0.7985(0.8377) | Loss 4.1389(4.1648) | Error 0.2866(0.2994) Steps 886(885.07) | Grad Norm 7.1127(4.6978) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 31.2775, Epoch Time 601.0541(559.7411), Bit/dim 3.7457(best: 3.7418), Xent 1.0757, Loss 4.2836, Error 0.3685(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1423 | Time 93.3947(91.6034) | Bit/dim 3.7561(3.7463) | Xent 0.7853(0.8361) | Loss 4.1488(4.1643) | Error 0.2833(0.2989) Steps 886(885.10) | Grad Norm 4.2368(4.6840) | Total Time 14.00(14.00)\n",
      "Iter 1424 | Time 95.1210(91.7089) | Bit/dim 3.7435(3.7462) | Xent 0.7772(0.8343) | Loss 4.1321(4.1633) | Error 0.2758(0.2982) Steps 874(884.76) | Grad Norm 2.8512(4.6290) | Total Time 14.00(14.00)\n",
      "Iter 1425 | Time 88.3700(91.6087) | Bit/dim 3.7524(3.7464) | Xent 0.7615(0.8321) | Loss 4.1331(4.1624) | Error 0.2722(0.2974) Steps 898(885.16) | Grad Norm 5.0888(4.6428) | Total Time 14.00(14.00)\n",
      "Iter 1426 | Time 94.1415(91.6847) | Bit/dim 3.7277(3.7458) | Xent 0.7753(0.8304) | Loss 4.1154(4.1610) | Error 0.2806(0.2969) Steps 904(885.73) | Grad Norm 2.5032(4.5786) | Total Time 14.00(14.00)\n",
      "Iter 1427 | Time 88.7994(91.5981) | Bit/dim 3.7358(3.7455) | Xent 0.7577(0.8282) | Loss 4.1146(4.1596) | Error 0.2739(0.2962) Steps 898(886.10) | Grad Norm 2.5564(4.5179) | Total Time 14.00(14.00)\n",
      "Iter 1428 | Time 88.8927(91.5170) | Bit/dim 3.7258(3.7449) | Xent 0.7834(0.8269) | Loss 4.1176(4.1584) | Error 0.2769(0.2956) Steps 892(886.27) | Grad Norm 3.3125(4.4818) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 30.3475, Epoch Time 594.8055(560.7930), Bit/dim 3.7422(best: 3.7418), Xent 1.0921, Loss 4.2883, Error 0.3714(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1429 | Time 93.3551(91.5721) | Bit/dim 3.7324(3.7445) | Xent 0.7643(0.8250) | Loss 4.1145(4.1571) | Error 0.2755(0.2950) Steps 892(886.44) | Grad Norm 1.9726(4.4065) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 94.4750(91.6592) | Bit/dim 3.7414(3.7445) | Xent 0.7620(0.8231) | Loss 4.1223(4.1560) | Error 0.2718(0.2943) Steps 880(886.25) | Grad Norm 2.2195(4.3409) | Total Time 14.00(14.00)\n",
      "Iter 1431 | Time 94.6736(91.7496) | Bit/dim 3.7365(3.7442) | Xent 0.7593(0.8212) | Loss 4.1162(4.1548) | Error 0.2751(0.2938) Steps 898(886.60) | Grad Norm 3.3037(4.3098) | Total Time 14.00(14.00)\n",
      "Iter 1432 | Time 93.8508(91.8127) | Bit/dim 3.7375(3.7440) | Xent 0.7805(0.8200) | Loss 4.1277(4.1540) | Error 0.2771(0.2933) Steps 886(886.59) | Grad Norm 2.9990(4.2704) | Total Time 14.00(14.00)\n",
      "Iter 1433 | Time 90.5935(91.7761) | Bit/dim 3.7281(3.7435) | Xent 0.7501(0.8179) | Loss 4.1031(4.1525) | Error 0.2656(0.2924) Steps 892(886.75) | Grad Norm 1.9522(4.2009) | Total Time 14.00(14.00)\n",
      "Iter 1434 | Time 87.9262(91.6606) | Bit/dim 3.7529(3.7438) | Xent 0.7626(0.8162) | Loss 4.1342(4.1519) | Error 0.2784(0.2920) Steps 904(887.27) | Grad Norm 5.0433(4.2262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 30.9642, Epoch Time 601.6893(562.0199), Bit/dim 3.7413(best: 3.7418), Xent 1.0931, Loss 4.2878, Error 0.3739(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1435 | Time 90.3872(91.6224) | Bit/dim 3.7403(3.7437) | Xent 0.7727(0.8149) | Loss 4.1267(4.1512) | Error 0.2759(0.2915) Steps 910(887.95) | Grad Norm 5.2382(4.2565) | Total Time 14.00(14.00)\n",
      "Iter 1436 | Time 91.5801(91.6211) | Bit/dim 3.7358(3.7435) | Xent 0.7662(0.8135) | Loss 4.1189(4.1502) | Error 0.2750(0.2910) Steps 886(887.89) | Grad Norm 4.4760(4.2631) | Total Time 14.00(14.00)\n",
      "Iter 1437 | Time 97.4463(91.7959) | Bit/dim 3.7372(3.7433) | Xent 0.7799(0.8125) | Loss 4.1271(4.1495) | Error 0.2819(0.2908) Steps 910(888.55) | Grad Norm 6.5213(4.3309) | Total Time 14.00(14.00)\n",
      "Iter 1438 | Time 90.7722(91.7652) | Bit/dim 3.7333(3.7430) | Xent 0.7705(0.8112) | Loss 4.1185(4.1486) | Error 0.2715(0.2902) Steps 898(888.84) | Grad Norm 5.9185(4.3785) | Total Time 14.00(14.00)\n",
      "Iter 1439 | Time 93.2483(91.8097) | Bit/dim 3.7364(3.7428) | Xent 0.7551(0.8095) | Loss 4.1139(4.1475) | Error 0.2626(0.2894) Steps 898(889.11) | Grad Norm 2.5695(4.3242) | Total Time 14.00(14.00)\n",
      "Iter 1440 | Time 93.7351(91.8674) | Bit/dim 3.7522(3.7431) | Xent 0.7677(0.8083) | Loss 4.1361(4.1472) | Error 0.2751(0.2889) Steps 892(889.20) | Grad Norm 4.9100(4.3418) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 30.7190, Epoch Time 603.5184(563.2648), Bit/dim 3.7390(best: 3.7413), Xent 1.0835, Loss 4.2807, Error 0.3692(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1441 | Time 92.8958(91.8983) | Bit/dim 3.7412(3.7430) | Xent 0.7554(0.8067) | Loss 4.1189(4.1464) | Error 0.2685(0.2883) Steps 904(889.64) | Grad Norm 4.0583(4.3333) | Total Time 14.00(14.00)\n",
      "Iter 1442 | Time 91.9844(91.9009) | Bit/dim 3.7336(3.7427) | Xent 0.7554(0.8051) | Loss 4.1113(4.1453) | Error 0.2730(0.2879) Steps 904(890.07) | Grad Norm 3.5837(4.3108) | Total Time 14.00(14.00)\n",
      "Iter 1443 | Time 90.9477(91.8723) | Bit/dim 3.7346(3.7425) | Xent 0.7520(0.8035) | Loss 4.1106(4.1443) | Error 0.2742(0.2874) Steps 910(890.67) | Grad Norm 2.9044(4.2686) | Total Time 14.00(14.00)\n",
      "Iter 1444 | Time 91.5194(91.8617) | Bit/dim 3.7393(3.7424) | Xent 0.7608(0.8023) | Loss 4.1197(4.1435) | Error 0.2726(0.2870) Steps 886(890.53) | Grad Norm 2.2544(4.2082) | Total Time 14.00(14.00)\n",
      "Iter 1445 | Time 89.8111(91.8002) | Bit/dim 3.7427(3.7424) | Xent 0.7348(0.8002) | Loss 4.1101(4.1425) | Error 0.2585(0.2861) Steps 868(889.85) | Grad Norm 2.2304(4.1488) | Total Time 14.00(14.00)\n",
      "Iter 1446 | Time 91.0699(91.7783) | Bit/dim 3.7346(3.7422) | Xent 0.7481(0.7987) | Loss 4.1087(4.1415) | Error 0.2659(0.2855) Steps 898(890.10) | Grad Norm 2.4024(4.0965) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 31.3513, Epoch Time 597.1062(564.2801), Bit/dim 3.7398(best: 3.7390), Xent 1.0745, Loss 4.2771, Error 0.3667(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1447 | Time 90.4485(91.7384) | Bit/dim 3.7479(3.7423) | Xent 0.7471(0.7971) | Loss 4.1214(4.1409) | Error 0.2629(0.2849) Steps 898(890.34) | Grad Norm 3.1919(4.0693) | Total Time 14.00(14.00)\n",
      "Iter 1448 | Time 94.8815(91.8327) | Bit/dim 3.7265(3.7419) | Xent 0.7519(0.7958) | Loss 4.1024(4.1398) | Error 0.2712(0.2844) Steps 898(890.57) | Grad Norm 3.4774(4.0516) | Total Time 14.00(14.00)\n",
      "Iter 1449 | Time 94.3779(91.9090) | Bit/dim 3.7397(3.7418) | Xent 0.7301(0.7938) | Loss 4.1048(4.1387) | Error 0.2619(0.2838) Steps 892(890.61) | Grad Norm 2.3823(4.0015) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 93.3304(91.9517) | Bit/dim 3.7295(3.7414) | Xent 0.7539(0.7926) | Loss 4.1065(4.1377) | Error 0.2702(0.2834) Steps 892(890.65) | Grad Norm 4.2021(4.0075) | Total Time 14.00(14.00)\n",
      "Iter 1451 | Time 93.0148(91.9836) | Bit/dim 3.7418(3.7414) | Xent 0.7950(0.7927) | Loss 4.1393(4.1378) | Error 0.2867(0.2835) Steps 880(890.33) | Grad Norm 8.2415(4.1345) | Total Time 14.00(14.00)\n",
      "Iter 1452 | Time 89.6435(91.9134) | Bit/dim 3.7467(3.7416) | Xent 0.8488(0.7944) | Loss 4.1711(4.1388) | Error 0.3094(0.2842) Steps 892(890.38) | Grad Norm 14.1125(4.4339) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 31.1623, Epoch Time 602.1017(565.4147), Bit/dim 3.7497(best: 3.7390), Xent 1.2590, Loss 4.3792, Error 0.4147(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1453 | Time 90.2505(91.8635) | Bit/dim 3.7508(3.7419) | Xent 0.9505(0.7990) | Loss 4.2260(4.1414) | Error 0.3349(0.2858) Steps 898(890.61) | Grad Norm 12.2710(4.6690) | Total Time 14.00(14.00)\n",
      "Iter 1454 | Time 91.1264(91.8414) | Bit/dim 3.7391(3.7418) | Xent 0.7992(0.7990) | Loss 4.1387(4.1413) | Error 0.2820(0.2857) Steps 886(890.47) | Grad Norm 4.7379(4.6710) | Total Time 14.00(14.00)\n",
      "Iter 1455 | Time 92.4996(91.8611) | Bit/dim 3.7362(3.7416) | Xent 0.8650(0.8010) | Loss 4.1687(4.1421) | Error 0.3153(0.2865) Steps 886(890.34) | Grad Norm 8.3530(4.7815) | Total Time 14.00(14.00)\n",
      "Iter 1456 | Time 89.9884(91.8049) | Bit/dim 3.7436(3.7417) | Xent 0.8392(0.8022) | Loss 4.1632(4.1428) | Error 0.3009(0.2870) Steps 898(890.57) | Grad Norm 5.4252(4.8008) | Total Time 14.00(14.00)\n",
      "Iter 1457 | Time 94.5681(91.8878) | Bit/dim 3.7595(3.7422) | Xent 0.8315(0.8030) | Loss 4.1753(4.1437) | Error 0.2980(0.2873) Steps 898(890.79) | Grad Norm 4.8839(4.8033) | Total Time 14.00(14.00)\n",
      "Iter 1458 | Time 96.7455(92.0336) | Bit/dim 3.7525(3.7425) | Xent 0.8484(0.8044) | Loss 4.1767(4.1447) | Error 0.3053(0.2878) Steps 892(890.83) | Grad Norm 6.8866(4.8658) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 31.0703, Epoch Time 601.5434(566.4986), Bit/dim 3.7470(best: 3.7390), Xent 1.1060, Loss 4.3000, Error 0.3807(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1459 | Time 93.2449(92.0699) | Bit/dim 3.7501(3.7428) | Xent 0.8200(0.8049) | Loss 4.1601(4.1452) | Error 0.2924(0.2880) Steps 886(890.68) | Grad Norm 5.4298(4.8827) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 91.8564(92.0635) | Bit/dim 3.7476(3.7429) | Xent 0.7943(0.8046) | Loss 4.1447(4.1452) | Error 0.2837(0.2878) Steps 874(890.18) | Grad Norm 6.9700(4.9453) | Total Time 14.00(14.00)\n",
      "Iter 1461 | Time 91.2163(92.0381) | Bit/dim 3.7541(3.7432) | Xent 0.7932(0.8042) | Loss 4.1507(4.1453) | Error 0.2915(0.2880) Steps 898(890.42) | Grad Norm 4.2843(4.9255) | Total Time 14.00(14.00)\n",
      "Iter 1462 | Time 91.9925(92.0367) | Bit/dim 3.7447(3.7433) | Xent 0.7887(0.8038) | Loss 4.1391(4.1452) | Error 0.2827(0.2878) Steps 892(890.46) | Grad Norm 4.3378(4.9079) | Total Time 14.00(14.00)\n",
      "Iter 1463 | Time 88.2597(91.9234) | Bit/dim 3.7257(3.7428) | Xent 0.7898(0.8033) | Loss 4.1206(4.1444) | Error 0.2891(0.2878) Steps 874(889.97) | Grad Norm 5.2274(4.9175) | Total Time 14.00(14.00)\n",
      "Iter 1464 | Time 93.3127(91.9651) | Bit/dim 3.7410(3.7427) | Xent 0.7575(0.8020) | Loss 4.1198(4.1437) | Error 0.2732(0.2874) Steps 874(889.49) | Grad Norm 5.1203(4.9236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 29.8504, Epoch Time 595.2173(567.3602), Bit/dim 3.7488(best: 3.7390), Xent 1.1248, Loss 4.3112, Error 0.3819(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1465 | Time 93.8011(92.0202) | Bit/dim 3.7412(3.7427) | Xent 0.7991(0.8019) | Loss 4.1408(4.1436) | Error 0.2860(0.2874) Steps 880(889.21) | Grad Norm 3.5779(4.8832) | Total Time 14.00(14.00)\n",
      "Iter 1466 | Time 89.5450(91.9459) | Bit/dim 3.7248(3.7421) | Xent 0.7478(0.8003) | Loss 4.0987(4.1422) | Error 0.2676(0.2868) Steps 874(888.75) | Grad Norm 2.1372(4.8008) | Total Time 14.00(14.00)\n",
      "Iter 1467 | Time 91.2499(91.9250) | Bit/dim 3.7467(3.7423) | Xent 0.7534(0.7988) | Loss 4.1234(4.1417) | Error 0.2689(0.2862) Steps 874(888.31) | Grad Norm 3.4985(4.7617) | Total Time 14.00(14.00)\n",
      "Iter 1468 | Time 91.6717(91.9174) | Bit/dim 3.7545(3.7426) | Xent 0.7514(0.7974) | Loss 4.1302(4.1413) | Error 0.2712(0.2858) Steps 880(888.06) | Grad Norm 2.5926(4.6967) | Total Time 14.00(14.00)\n",
      "Iter 1469 | Time 92.3584(91.9307) | Bit/dim 3.7366(3.7424) | Xent 0.7550(0.7962) | Loss 4.1141(4.1405) | Error 0.2670(0.2852) Steps 892(888.18) | Grad Norm 3.3263(4.6556) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 90.0707(91.8749) | Bit/dim 3.7506(3.7427) | Xent 0.7510(0.7948) | Loss 4.1261(4.1401) | Error 0.2656(0.2846) Steps 892(888.29) | Grad Norm 3.8490(4.6314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 30.7266, Epoch Time 594.8585(568.1851), Bit/dim 3.7389(best: 3.7390), Xent 1.0994, Loss 4.2886, Error 0.3694(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1471 | Time 94.3606(91.9494) | Bit/dim 3.7310(3.7423) | Xent 0.7506(0.7935) | Loss 4.1063(4.1391) | Error 0.2676(0.2841) Steps 892(888.40) | Grad Norm 2.4326(4.5654) | Total Time 14.00(14.00)\n",
      "Iter 1472 | Time 91.7911(91.9447) | Bit/dim 3.7393(3.7422) | Xent 0.7688(0.7927) | Loss 4.1237(4.1386) | Error 0.2730(0.2838) Steps 892(888.51) | Grad Norm 4.8792(4.5748) | Total Time 14.00(14.00)\n",
      "Iter 1473 | Time 90.6465(91.9057) | Bit/dim 3.7340(3.7420) | Xent 0.7478(0.7914) | Loss 4.1079(4.1377) | Error 0.2649(0.2832) Steps 904(888.97) | Grad Norm 2.9201(4.5252) | Total Time 14.00(14.00)\n",
      "Iter 1474 | Time 91.4802(91.8930) | Bit/dim 3.7400(3.7419) | Xent 0.7517(0.7902) | Loss 4.1158(4.1370) | Error 0.2658(0.2827) Steps 898(889.25) | Grad Norm 2.9465(4.4778) | Total Time 14.00(14.00)\n",
      "Iter 1475 | Time 90.9733(91.8654) | Bit/dim 3.7430(3.7420) | Xent 0.7365(0.7886) | Loss 4.1112(4.1363) | Error 0.2611(0.2820) Steps 898(889.51) | Grad Norm 3.4714(4.4476) | Total Time 14.00(14.00)\n",
      "Iter 1476 | Time 97.4004(92.0314) | Bit/dim 3.7313(3.7416) | Xent 0.7255(0.7867) | Loss 4.0940(4.1350) | Error 0.2609(0.2814) Steps 904(889.94) | Grad Norm 2.2839(4.3827) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 30.4695, Epoch Time 602.7291(569.2214), Bit/dim 3.7401(best: 3.7389), Xent 1.0854, Loss 4.2828, Error 0.3679(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1477 | Time 92.2847(92.0390) | Bit/dim 3.7266(3.7412) | Xent 0.7332(0.7851) | Loss 4.0931(4.1337) | Error 0.2602(0.2808) Steps 886(889.82) | Grad Norm 3.3649(4.3522) | Total Time 14.00(14.00)\n",
      "Iter 1478 | Time 89.8264(91.9726) | Bit/dim 3.7398(3.7412) | Xent 0.7159(0.7830) | Loss 4.0977(4.1327) | Error 0.2542(0.2800) Steps 880(889.53) | Grad Norm 3.5972(4.3295) | Total Time 14.00(14.00)\n",
      "Iter 1479 | Time 93.6389(92.0226) | Bit/dim 3.7348(3.7410) | Xent 0.7086(0.7808) | Loss 4.0891(4.1314) | Error 0.2475(0.2790) Steps 880(889.24) | Grad Norm 2.7823(4.2831) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 91.1881(91.9976) | Bit/dim 3.7406(3.7410) | Xent 0.7357(0.7794) | Loss 4.1084(4.1307) | Error 0.2629(0.2785) Steps 898(889.51) | Grad Norm 1.8356(4.2097) | Total Time 14.00(14.00)\n",
      "Iter 1481 | Time 92.1164(92.0012) | Bit/dim 3.7326(3.7407) | Xent 0.7297(0.7779) | Loss 4.0974(4.1297) | Error 0.2586(0.2779) Steps 886(889.40) | Grad Norm 3.4812(4.1878) | Total Time 14.00(14.00)\n",
      "Iter 1482 | Time 90.9034(91.9682) | Bit/dim 3.7366(3.7406) | Xent 0.7475(0.7770) | Loss 4.1103(4.1291) | Error 0.2666(0.2776) Steps 880(889.12) | Grad Norm 6.5212(4.2578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 30.9802, Epoch Time 596.3984(570.0367), Bit/dim 3.7413(best: 3.7389), Xent 1.1286, Loss 4.3056, Error 0.3742(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1483 | Time 91.3088(91.9484) | Bit/dim 3.7402(3.7406) | Xent 0.7513(0.7762) | Loss 4.1158(4.1287) | Error 0.2666(0.2773) Steps 886(889.03) | Grad Norm 9.4460(4.4135) | Total Time 14.00(14.00)\n",
      "Iter 1484 | Time 90.3142(91.8994) | Bit/dim 3.7479(3.7408) | Xent 0.7665(0.7760) | Loss 4.1312(4.1288) | Error 0.2758(0.2772) Steps 892(889.11) | Grad Norm 10.2219(4.5877) | Total Time 14.00(14.00)\n",
      "Iter 1485 | Time 90.7071(91.8636) | Bit/dim 3.7383(3.7407) | Xent 0.7839(0.7762) | Loss 4.1302(4.1288) | Error 0.2738(0.2771) Steps 886(889.02) | Grad Norm 7.7456(4.6825) | Total Time 14.00(14.00)\n",
      "Iter 1486 | Time 94.8373(91.9529) | Bit/dim 3.7382(3.7406) | Xent 0.7650(0.7759) | Loss 4.1207(4.1286) | Error 0.2751(0.2771) Steps 898(889.29) | Grad Norm 4.4331(4.6750) | Total Time 14.00(14.00)\n",
      "Iter 1487 | Time 93.6364(92.0034) | Bit/dim 3.7241(3.7401) | Xent 0.7757(0.7758) | Loss 4.1119(4.1281) | Error 0.2764(0.2770) Steps 886(889.19) | Grad Norm 5.1717(4.6899) | Total Time 14.00(14.00)\n",
      "Iter 1488 | Time 92.0476(92.0047) | Bit/dim 3.7282(3.7398) | Xent 0.8106(0.7769) | Loss 4.1335(4.1282) | Error 0.2856(0.2773) Steps 892(889.28) | Grad Norm 6.9225(4.7569) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 30.5840, Epoch Time 598.8394(570.9008), Bit/dim 3.7397(best: 3.7389), Xent 1.1225, Loss 4.3010, Error 0.3752(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1489 | Time 95.5118(92.1099) | Bit/dim 3.7371(3.7397) | Xent 0.7644(0.7765) | Loss 4.1193(4.1280) | Error 0.2728(0.2772) Steps 904(889.72) | Grad Norm 3.6927(4.7249) | Total Time 14.00(14.00)\n",
      "Iter 1490 | Time 93.4680(92.1506) | Bit/dim 3.7348(3.7396) | Xent 0.7592(0.7760) | Loss 4.1144(4.1276) | Error 0.2672(0.2769) Steps 898(889.97) | Grad Norm 6.6998(4.7842) | Total Time 14.00(14.00)\n",
      "Iter 1491 | Time 90.6064(92.1043) | Bit/dim 3.7363(3.7395) | Xent 0.7719(0.7759) | Loss 4.1223(4.1274) | Error 0.2692(0.2766) Steps 886(889.85) | Grad Norm 6.5151(4.8361) | Total Time 14.00(14.00)\n",
      "Iter 1492 | Time 93.9311(92.1591) | Bit/dim 3.7354(3.7393) | Xent 0.7427(0.7749) | Loss 4.1067(4.1268) | Error 0.2655(0.2763) Steps 892(889.91) | Grad Norm 3.6070(4.7992) | Total Time 14.00(14.00)\n",
      "Iter 1493 | Time 92.2423(92.1616) | Bit/dim 3.7318(3.7391) | Xent 0.7210(0.7733) | Loss 4.0923(4.1257) | Error 0.2536(0.2756) Steps 898(890.15) | Grad Norm 2.9905(4.7450) | Total Time 14.00(14.00)\n",
      "Iter 1494 | Time 93.0206(92.1874) | Bit/dim 3.7375(3.7391) | Xent 0.7259(0.7718) | Loss 4.1005(4.1250) | Error 0.2578(0.2751) Steps 898(890.39) | Grad Norm 3.0698(4.6947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 31.0777, Epoch Time 605.5513(571.9403), Bit/dim 3.7366(best: 3.7389), Xent 1.0993, Loss 4.2862, Error 0.3693(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1495 | Time 95.5316(92.2877) | Bit/dim 3.7334(3.7389) | Xent 0.7530(0.7713) | Loss 4.1099(4.1245) | Error 0.2704(0.2749) Steps 904(890.80) | Grad Norm 4.3657(4.6849) | Total Time 14.00(14.00)\n",
      "Iter 1496 | Time 96.5222(92.4148) | Bit/dim 3.7447(3.7391) | Xent 0.7227(0.7698) | Loss 4.1060(4.1240) | Error 0.2609(0.2745) Steps 904(891.19) | Grad Norm 4.1808(4.6697) | Total Time 14.00(14.00)\n",
      "Iter 1497 | Time 90.0002(92.3423) | Bit/dim 3.7275(3.7387) | Xent 0.7024(0.7678) | Loss 4.0787(4.1226) | Error 0.2520(0.2738) Steps 904(891.58) | Grad Norm 1.8114(4.5840) | Total Time 14.00(14.00)\n",
      "Iter 1498 | Time 91.9834(92.3315) | Bit/dim 3.7293(3.7384) | Xent 0.7227(0.7664) | Loss 4.0907(4.1217) | Error 0.2566(0.2733) Steps 898(891.77) | Grad Norm 4.0177(4.5670) | Total Time 14.00(14.00)\n",
      "Iter 1499 | Time 88.9843(92.2311) | Bit/dim 3.7359(3.7384) | Xent 0.7190(0.7650) | Loss 4.0953(4.1209) | Error 0.2598(0.2729) Steps 904(892.14) | Grad Norm 6.6399(4.6292) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 90.7549(92.1868) | Bit/dim 3.7316(3.7382) | Xent 0.7416(0.7643) | Loss 4.1024(4.1203) | Error 0.2694(0.2728) Steps 898(892.31) | Grad Norm 5.9571(4.6690) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 31.1482, Epoch Time 600.5170(572.7976), Bit/dim 3.7406(best: 3.7366), Xent 1.1232, Loss 4.3022, Error 0.3749(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1501 | Time 89.4585(92.1050) | Bit/dim 3.7309(3.7379) | Xent 0.7209(0.7630) | Loss 4.0914(4.1194) | Error 0.2560(0.2723) Steps 910(892.84) | Grad Norm 4.9410(4.6772) | Total Time 14.00(14.00)\n",
      "Iter 1502 | Time 91.7437(92.0942) | Bit/dim 3.7440(3.7381) | Xent 0.7259(0.7619) | Loss 4.1069(4.1191) | Error 0.2550(0.2718) Steps 880(892.46) | Grad Norm 4.6676(4.6769) | Total Time 14.00(14.00)\n",
      "Iter 1503 | Time 88.7075(91.9926) | Bit/dim 3.7334(3.7380) | Xent 0.7375(0.7612) | Loss 4.1021(4.1186) | Error 0.2704(0.2717) Steps 904(892.81) | Grad Norm 4.9237(4.6843) | Total Time 14.00(14.00)\n",
      "Iter 1504 | Time 88.1834(91.8783) | Bit/dim 3.7352(3.7379) | Xent 0.7437(0.7606) | Loss 4.1070(4.1182) | Error 0.2700(0.2717) Steps 892(892.78) | Grad Norm 5.8391(4.7189) | Total Time 14.00(14.00)\n",
      "Iter 1505 | Time 92.5872(91.8995) | Bit/dim 3.7285(3.7376) | Xent 0.7294(0.7597) | Loss 4.0932(4.1175) | Error 0.2599(0.2713) Steps 904(893.12) | Grad Norm 5.1173(4.7309) | Total Time 14.00(14.00)\n",
      "Iter 1506 | Time 94.1678(91.9676) | Bit/dim 3.7358(3.7376) | Xent 0.7153(0.7584) | Loss 4.0935(4.1167) | Error 0.2584(0.2709) Steps 892(893.08) | Grad Norm 3.9521(4.7075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 30.0932, Epoch Time 590.6919(573.3345), Bit/dim 3.7423(best: 3.7366), Xent 1.1333, Loss 4.3090, Error 0.3764(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 90.0822(91.9110) | Bit/dim 3.7273(3.7373) | Xent 0.7240(0.7573) | Loss 4.0893(4.1159) | Error 0.2662(0.2708) Steps 886(892.87) | Grad Norm 7.0382(4.7774) | Total Time 14.00(14.00)\n",
      "Iter 1508 | Time 93.5579(91.9604) | Bit/dim 3.7389(3.7373) | Xent 0.7046(0.7558) | Loss 4.0912(4.1152) | Error 0.2510(0.2702) Steps 898(893.03) | Grad Norm 4.9408(4.7823) | Total Time 14.00(14.00)\n",
      "Iter 1509 | Time 92.7492(91.9841) | Bit/dim 3.7387(3.7373) | Xent 0.6955(0.7540) | Loss 4.0865(4.1143) | Error 0.2465(0.2695) Steps 886(892.81) | Grad Norm 5.4240(4.8016) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 93.1202(92.0182) | Bit/dim 3.7293(3.7371) | Xent 0.6993(0.7523) | Loss 4.0790(4.1133) | Error 0.2528(0.2690) Steps 892(892.79) | Grad Norm 4.6185(4.7961) | Total Time 14.00(14.00)\n",
      "Iter 1511 | Time 91.2321(91.9946) | Bit/dim 3.7449(3.7373) | Xent 0.7298(0.7516) | Loss 4.1098(4.1132) | Error 0.2579(0.2687) Steps 892(892.77) | Grad Norm 3.8879(4.7689) | Total Time 14.00(14.00)\n",
      "Iter 1512 | Time 91.2211(91.9714) | Bit/dim 3.7371(3.7373) | Xent 0.7129(0.7505) | Loss 4.0935(4.1126) | Error 0.2561(0.2683) Steps 886(892.56) | Grad Norm 2.1260(4.6896) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 29.6453, Epoch Time 599.2530(574.1120), Bit/dim 3.7383(best: 3.7366), Xent 1.1232, Loss 4.3000, Error 0.3699(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 91.1639(91.9472) | Bit/dim 3.7342(3.7372) | Xent 0.6990(0.7489) | Loss 4.0837(4.1117) | Error 0.2485(0.2677) Steps 886(892.37) | Grad Norm 4.9461(4.6973) | Total Time 14.00(14.00)\n",
      "Iter 1514 | Time 90.8418(91.9140) | Bit/dim 3.7360(3.7372) | Xent 0.7227(0.7481) | Loss 4.0973(4.1113) | Error 0.2648(0.2676) Steps 898(892.54) | Grad Norm 6.7199(4.7579) | Total Time 14.00(14.00)\n",
      "Iter 1515 | Time 92.0736(91.9188) | Bit/dim 3.7426(3.7374) | Xent 0.7068(0.7469) | Loss 4.0960(4.1108) | Error 0.2534(0.2672) Steps 892(892.52) | Grad Norm 4.1951(4.7411) | Total Time 14.00(14.00)\n",
      "Iter 1516 | Time 92.3925(91.9330) | Bit/dim 3.7272(3.7371) | Xent 0.7277(0.7463) | Loss 4.0910(4.1102) | Error 0.2625(0.2670) Steps 910(893.04) | Grad Norm 5.9901(4.7785) | Total Time 14.00(14.00)\n",
      "Iter 1517 | Time 93.3679(91.9761) | Bit/dim 3.7252(3.7367) | Xent 0.7656(0.7469) | Loss 4.1080(4.1102) | Error 0.2731(0.2672) Steps 898(893.19) | Grad Norm 8.3122(4.8845) | Total Time 14.00(14.00)\n",
      "Iter 1518 | Time 94.9193(92.0643) | Bit/dim 3.7311(3.7365) | Xent 0.7423(0.7468) | Loss 4.1023(4.1099) | Error 0.2620(0.2671) Steps 874(892.62) | Grad Norm 8.5635(4.9949) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 30.9986, Epoch Time 601.3107(574.9280), Bit/dim 3.7427(best: 3.7366), Xent 1.1489, Loss 4.3172, Error 0.3818(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 90.0911(92.0052) | Bit/dim 3.7282(3.7363) | Xent 0.7697(0.7475) | Loss 4.1131(4.1100) | Error 0.2686(0.2671) Steps 892(892.60) | Grad Norm 7.3968(5.0670) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 90.8658(91.9710) | Bit/dim 3.7467(3.7366) | Xent 0.7535(0.7476) | Loss 4.1234(4.1104) | Error 0.2802(0.2675) Steps 892(892.58) | Grad Norm 8.7845(5.1785) | Total Time 14.00(14.00)\n",
      "Iter 1521 | Time 92.9402(92.0001) | Bit/dim 3.7433(3.7368) | Xent 0.7782(0.7486) | Loss 4.1324(4.1111) | Error 0.2775(0.2678) Steps 892(892.56) | Grad Norm 8.6485(5.2826) | Total Time 14.00(14.00)\n",
      "Iter 1522 | Time 95.2210(92.0967) | Bit/dim 3.7251(3.7364) | Xent 0.7445(0.7484) | Loss 4.0973(4.1107) | Error 0.2670(0.2678) Steps 898(892.73) | Grad Norm 7.6464(5.3535) | Total Time 14.00(14.00)\n",
      "Iter 1523 | Time 89.5441(92.0201) | Bit/dim 3.7401(3.7366) | Xent 0.7164(0.7475) | Loss 4.0983(4.1103) | Error 0.2570(0.2675) Steps 898(892.88) | Grad Norm 4.8421(5.3382) | Total Time 14.00(14.00)\n",
      "Iter 1524 | Time 94.1382(92.0836) | Bit/dim 3.7274(3.7363) | Xent 0.7341(0.7471) | Loss 4.0944(4.1098) | Error 0.2631(0.2673) Steps 904(893.22) | Grad Norm 6.2625(5.3659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 30.5721, Epoch Time 599.3484(575.6606), Bit/dim 3.7383(best: 3.7366), Xent 1.1019, Loss 4.2892, Error 0.3753(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 97.2877(92.2398) | Bit/dim 3.7449(3.7365) | Xent 0.7228(0.7463) | Loss 4.1063(4.1097) | Error 0.2494(0.2668) Steps 916(893.90) | Grad Norm 4.8255(5.3497) | Total Time 14.00(14.00)\n",
      "Iter 1526 | Time 90.1034(92.1757) | Bit/dim 3.7335(3.7364) | Xent 0.7102(0.7453) | Loss 4.0886(4.1091) | Error 0.2525(0.2664) Steps 892(893.84) | Grad Norm 5.1517(5.3437) | Total Time 14.00(14.00)\n",
      "Iter 1527 | Time 93.8043(92.2245) | Bit/dim 3.7411(3.7366) | Xent 0.7095(0.7442) | Loss 4.0959(4.1087) | Error 0.2534(0.2660) Steps 904(894.15) | Grad Norm 6.8465(5.3888) | Total Time 14.00(14.00)\n",
      "Iter 1528 | Time 90.4489(92.1713) | Bit/dim 3.7296(3.7364) | Xent 0.6909(0.7426) | Loss 4.0750(4.1077) | Error 0.2498(0.2655) Steps 898(894.26) | Grad Norm 2.2826(5.2956) | Total Time 14.00(14.00)\n",
      "Iter 1529 | Time 89.0031(92.0762) | Bit/dim 3.7333(3.7363) | Xent 0.6917(0.7411) | Loss 4.0792(4.1068) | Error 0.2456(0.2649) Steps 892(894.20) | Grad Norm 4.9011(5.2838) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 91.1429(92.0482) | Bit/dim 3.7333(3.7362) | Xent 0.6941(0.7396) | Loss 4.0803(4.1060) | Error 0.2468(0.2643) Steps 886(893.95) | Grad Norm 3.0984(5.2182) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 30.8081, Epoch Time 598.1642(576.3357), Bit/dim 3.7426(best: 3.7366), Xent 1.1419, Loss 4.3135, Error 0.3698(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 90.3777(91.9981) | Bit/dim 3.7367(3.7362) | Xent 0.6858(0.7380) | Loss 4.0796(4.1052) | Error 0.2438(0.2637) Steps 886(893.71) | Grad Norm 7.5274(5.2875) | Total Time 14.00(14.00)\n",
      "Iter 1532 | Time 96.3039(92.1273) | Bit/dim 3.7326(3.7361) | Xent 0.7221(0.7376) | Loss 4.0937(4.1049) | Error 0.2501(0.2633) Steps 898(893.84) | Grad Norm 7.4743(5.3531) | Total Time 14.00(14.00)\n",
      "Iter 1533 | Time 96.5012(92.2585) | Bit/dim 3.7317(3.7360) | Xent 0.7092(0.7367) | Loss 4.0863(4.1043) | Error 0.2514(0.2630) Steps 898(893.97) | Grad Norm 5.4111(5.3549) | Total Time 14.00(14.00)\n",
      "Iter 1534 | Time 93.1429(92.2850) | Bit/dim 3.7403(3.7361) | Xent 0.7410(0.7368) | Loss 4.1108(4.1045) | Error 0.2668(0.2631) Steps 898(894.09) | Grad Norm 4.0117(5.3146) | Total Time 14.00(14.00)\n",
      "Iter 1535 | Time 89.7619(92.2093) | Bit/dim 3.7296(3.7359) | Xent 0.7015(0.7358) | Loss 4.0803(4.1038) | Error 0.2521(0.2627) Steps 910(894.56) | Grad Norm 2.3474(5.2256) | Total Time 14.00(14.00)\n",
      "Iter 1536 | Time 91.7754(92.1963) | Bit/dim 3.7247(3.7356) | Xent 0.7147(0.7351) | Loss 4.0820(4.1031) | Error 0.2538(0.2625) Steps 892(894.49) | Grad Norm 4.2856(5.1974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 31.0051, Epoch Time 604.4538(577.1792), Bit/dim 3.7368(best: 3.7366), Xent 1.1100, Loss 4.2919, Error 0.3728(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 93.6217(92.2391) | Bit/dim 3.7368(3.7356) | Xent 0.7143(0.7345) | Loss 4.0940(4.1029) | Error 0.2590(0.2624) Steps 892(894.41) | Grad Norm 5.7284(5.2133) | Total Time 14.00(14.00)\n",
      "Iter 1538 | Time 91.4622(92.2158) | Bit/dim 3.7232(3.7352) | Xent 0.6686(0.7325) | Loss 4.0575(4.1015) | Error 0.2370(0.2616) Steps 886(894.16) | Grad Norm 4.3311(5.1868) | Total Time 14.00(14.00)\n",
      "Iter 1539 | Time 90.3898(92.1610) | Bit/dim 3.7371(3.7353) | Xent 0.6859(0.7311) | Loss 4.0801(4.1009) | Error 0.2428(0.2610) Steps 880(893.74) | Grad Norm 4.0118(5.1516) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 91.7208(92.1478) | Bit/dim 3.7374(3.7354) | Xent 0.6779(0.7295) | Loss 4.0764(4.1001) | Error 0.2419(0.2605) Steps 910(894.22) | Grad Norm 3.2111(5.0934) | Total Time 14.00(14.00)\n",
      "Iter 1541 | Time 90.2967(92.0923) | Bit/dim 3.7419(3.7356) | Xent 0.6807(0.7281) | Loss 4.0823(4.0996) | Error 0.2355(0.2597) Steps 892(894.16) | Grad Norm 3.3510(5.0411) | Total Time 14.00(14.00)\n",
      "Iter 1542 | Time 94.9591(92.1783) | Bit/dim 3.7254(3.7352) | Xent 0.6986(0.7272) | Loss 4.0747(4.0988) | Error 0.2490(0.2594) Steps 904(894.45) | Grad Norm 5.4287(5.0527) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 30.5549, Epoch Time 598.5536(577.8205), Bit/dim 3.7358(best: 3.7366), Xent 1.1463, Loss 4.3090, Error 0.3697(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 94.2430(92.2402) | Bit/dim 3.7313(3.7351) | Xent 0.6726(0.7256) | Loss 4.0676(4.0979) | Error 0.2416(0.2589) Steps 880(894.02) | Grad Norm 5.4341(5.0642) | Total Time 14.00(14.00)\n",
      "Iter 1544 | Time 93.5185(92.2785) | Bit/dim 3.7327(3.7351) | Xent 0.6687(0.7238) | Loss 4.0671(4.0970) | Error 0.2422(0.2584) Steps 892(893.96) | Grad Norm 4.4773(5.0465) | Total Time 14.00(14.00)\n",
      "Iter 1545 | Time 96.8002(92.4142) | Bit/dim 3.7258(3.7348) | Xent 0.6759(0.7224) | Loss 4.0638(4.0960) | Error 0.2385(0.2578) Steps 904(894.26) | Grad Norm 2.1121(4.9585) | Total Time 14.00(14.00)\n",
      "Iter 1546 | Time 95.2966(92.5007) | Bit/dim 3.7397(3.7349) | Xent 0.6910(0.7215) | Loss 4.0853(4.0957) | Error 0.2466(0.2574) Steps 898(894.37) | Grad Norm 7.6223(5.0384) | Total Time 14.00(14.00)\n",
      "Iter 1547 | Time 93.1950(92.5215) | Bit/dim 3.7379(3.7350) | Xent 0.7361(0.7219) | Loss 4.1060(4.0960) | Error 0.2624(0.2576) Steps 892(894.30) | Grad Norm 10.0399(5.1885) | Total Time 14.00(14.00)\n",
      "Iter 1548 | Time 87.3024(92.3649) | Bit/dim 3.7331(3.7350) | Xent 0.7380(0.7224) | Loss 4.1021(4.0962) | Error 0.2672(0.2579) Steps 892(894.23) | Grad Norm 7.6851(5.2634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 30.7825, Epoch Time 607.6244(578.7146), Bit/dim 3.7357(best: 3.7358), Xent 1.1709, Loss 4.3211, Error 0.3893(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 90.5748(92.3112) | Bit/dim 3.7174(3.7344) | Xent 0.7535(0.7233) | Loss 4.0942(4.0961) | Error 0.2665(0.2581) Steps 892(894.16) | Grad Norm 10.0687(5.4075) | Total Time 14.00(14.00)\n",
      "Iter 1550 | Time 94.8754(92.3882) | Bit/dim 3.7431(3.7347) | Xent 0.9028(0.7287) | Loss 4.1946(4.0991) | Error 0.3204(0.2600) Steps 874(893.56) | Grad Norm 16.2065(5.7315) | Total Time 14.00(14.00)\n",
      "Iter 1551 | Time 89.8262(92.3113) | Bit/dim 3.7474(3.7351) | Xent 0.8342(0.7319) | Loss 4.1645(4.1010) | Error 0.3025(0.2613) Steps 910(894.05) | Grad Norm 8.4012(5.8116) | Total Time 14.00(14.00)\n",
      "Iter 1552 | Time 90.4178(92.2545) | Bit/dim 3.7456(3.7354) | Xent 0.8336(0.7349) | Loss 4.1624(4.1029) | Error 0.2969(0.2623) Steps 886(893.81) | Grad Norm 10.4417(5.9505) | Total Time 14.00(14.00)\n",
      "Iter 1553 | Time 98.7073(92.4481) | Bit/dim 3.7566(3.7360) | Xent 0.8063(0.7371) | Loss 4.1598(4.1046) | Error 0.2795(0.2629) Steps 880(893.40) | Grad Norm 10.2359(6.0791) | Total Time 14.00(14.00)\n",
      "Iter 1554 | Time 98.9632(92.6435) | Bit/dim 3.7301(3.7359) | Xent 0.7871(0.7386) | Loss 4.1237(4.1051) | Error 0.2821(0.2634) Steps 910(893.89) | Grad Norm 8.9218(6.1643) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 31.2206, Epoch Time 610.0526(579.6547), Bit/dim 3.7466(best: 3.7357), Xent 1.1137, Loss 4.3034, Error 0.3769(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 96.1265(92.7480) | Bit/dim 3.7456(3.7361) | Xent 0.7505(0.7389) | Loss 4.1208(4.1056) | Error 0.2641(0.2635) Steps 904(894.20) | Grad Norm 6.4479(6.1728) | Total Time 14.00(14.00)\n",
      "Iter 1556 | Time 89.2448(92.6429) | Bit/dim 3.7496(3.7365) | Xent 0.7783(0.7401) | Loss 4.1387(4.1066) | Error 0.2798(0.2639) Steps 904(894.49) | Grad Norm 7.6210(6.2163) | Total Time 14.00(14.00)\n",
      "Iter 1557 | Time 93.7820(92.6771) | Bit/dim 3.7372(3.7366) | Xent 0.6903(0.7386) | Loss 4.0823(4.1059) | Error 0.2425(0.2633) Steps 916(895.14) | Grad Norm 3.6220(6.1385) | Total Time 14.00(14.00)\n",
      "Iter 1558 | Time 92.1795(92.6622) | Bit/dim 3.7405(3.7367) | Xent 0.7511(0.7390) | Loss 4.1161(4.1062) | Error 0.2721(0.2636) Steps 904(895.40) | Grad Norm 7.8218(6.1890) | Total Time 14.00(14.00)\n",
      "Iter 1559 | Time 94.9827(92.7318) | Bit/dim 3.7464(3.7370) | Xent 0.7620(0.7397) | Loss 4.1274(4.1068) | Error 0.2692(0.2637) Steps 910(895.84) | Grad Norm 9.4295(6.2862) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 89.6667(92.6398) | Bit/dim 3.7333(3.7369) | Xent 0.7830(0.7410) | Loss 4.1248(4.1074) | Error 0.2825(0.2643) Steps 904(896.09) | Grad Norm 9.1603(6.3724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 31.4589, Epoch Time 602.9292(580.3530), Bit/dim 3.7419(best: 3.7357), Xent 1.1546, Loss 4.3192, Error 0.3829(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 92.9328(92.6486) | Bit/dim 3.7398(3.7370) | Xent 0.7156(0.7402) | Loss 4.0976(4.1071) | Error 0.2606(0.2642) Steps 904(896.32) | Grad Norm 4.0844(6.3038) | Total Time 14.00(14.00)\n",
      "Iter 1562 | Time 87.5414(92.4954) | Bit/dim 3.7315(3.7368) | Xent 0.7158(0.7395) | Loss 4.0894(4.1065) | Error 0.2520(0.2638) Steps 892(896.19) | Grad Norm 5.3589(6.2754) | Total Time 14.00(14.00)\n",
      "Iter 1563 | Time 92.3042(92.4897) | Bit/dim 3.7306(3.7366) | Xent 0.7462(0.7397) | Loss 4.1037(4.1064) | Error 0.2675(0.2639) Steps 904(896.43) | Grad Norm 5.8200(6.2618) | Total Time 14.00(14.00)\n",
      "Iter 1564 | Time 89.9151(92.4124) | Bit/dim 3.7412(3.7367) | Xent 0.6823(0.7380) | Loss 4.0824(4.1057) | Error 0.2426(0.2633) Steps 886(896.11) | Grad Norm 2.1496(6.1384) | Total Time 14.00(14.00)\n",
      "Iter 1565 | Time 90.8950(92.3669) | Bit/dim 3.7417(3.7369) | Xent 0.7421(0.7381) | Loss 4.1128(4.1059) | Error 0.2706(0.2635) Steps 898(896.17) | Grad Norm 6.5344(6.1503) | Total Time 14.00(14.00)\n",
      "Iter 1566 | Time 94.4228(92.4286) | Bit/dim 3.7355(3.7369) | Xent 0.6997(0.7369) | Loss 4.0854(4.1053) | Error 0.2588(0.2634) Steps 892(896.05) | Grad Norm 4.5343(6.1018) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 30.8144, Epoch Time 597.5447(580.8687), Bit/dim 3.7397(best: 3.7357), Xent 1.1816, Loss 4.3305, Error 0.3786(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 92.1874(92.4213) | Bit/dim 3.7425(3.7370) | Xent 0.7098(0.7361) | Loss 4.0974(4.1051) | Error 0.2566(0.2632) Steps 904(896.28) | Grad Norm 4.6249(6.0575) | Total Time 14.00(14.00)\n",
      "Iter 1568 | Time 95.0594(92.5005) | Bit/dim 3.7268(3.7367) | Xent 0.6785(0.7344) | Loss 4.0660(4.1039) | Error 0.2434(0.2626) Steps 886(895.98) | Grad Norm 3.8383(5.9909) | Total Time 14.00(14.00)\n",
      "Iter 1569 | Time 91.9633(92.4844) | Bit/dim 3.7366(3.7367) | Xent 0.6751(0.7326) | Loss 4.0742(4.1030) | Error 0.2394(0.2619) Steps 886(895.68) | Grad Norm 2.7039(5.8923) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 94.4258(92.5426) | Bit/dim 3.7366(3.7367) | Xent 0.6727(0.7308) | Loss 4.0730(4.1021) | Error 0.2340(0.2610) Steps 898(895.75) | Grad Norm 2.8148(5.8000) | Total Time 14.00(14.00)\n",
      "Iter 1571 | Time 90.9367(92.4944) | Bit/dim 3.7321(3.7366) | Xent 0.6644(0.7288) | Loss 4.0643(4.1010) | Error 0.2389(0.2604) Steps 898(895.81) | Grad Norm 2.7206(5.7076) | Total Time 14.00(14.00)\n",
      "Iter 1572 | Time 90.4899(92.4343) | Bit/dim 3.7333(3.7365) | Xent 0.6738(0.7272) | Loss 4.0702(4.1001) | Error 0.2414(0.2598) Steps 886(895.52) | Grad Norm 4.2449(5.6637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 30.6691, Epoch Time 601.0675(581.4747), Bit/dim 3.7361(best: 3.7357), Xent 1.1305, Loss 4.3013, Error 0.3719(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 95.0399(92.5125) | Bit/dim 3.7292(3.7363) | Xent 0.6532(0.7250) | Loss 4.0558(4.0987) | Error 0.2280(0.2589) Steps 904(895.77) | Grad Norm 3.1704(5.5889) | Total Time 14.00(14.00)\n",
      "Iter 1574 | Time 94.6563(92.5768) | Bit/dim 3.7327(3.7361) | Xent 0.6518(0.7228) | Loss 4.0586(4.0975) | Error 0.2336(0.2581) Steps 904(896.02) | Grad Norm 2.1157(5.4847) | Total Time 14.00(14.00)\n",
      "Iter 1575 | Time 90.1835(92.5050) | Bit/dim 3.7376(3.7362) | Xent 0.6652(0.7210) | Loss 4.0702(4.0967) | Error 0.2379(0.2575) Steps 886(895.72) | Grad Norm 4.1381(5.4443) | Total Time 14.00(14.00)\n",
      "Iter 1576 | Time 92.6005(92.5079) | Bit/dim 3.7239(3.7358) | Xent 0.6470(0.7188) | Loss 4.0475(4.0952) | Error 0.2274(0.2566) Steps 892(895.61) | Grad Norm 2.8790(5.3674) | Total Time 14.00(14.00)\n",
      "Iter 1577 | Time 93.1758(92.5279) | Bit/dim 3.7282(3.7356) | Xent 0.6087(0.7155) | Loss 4.0325(4.0933) | Error 0.2222(0.2556) Steps 904(895.86) | Grad Norm 3.3356(5.3064) | Total Time 14.00(14.00)\n",
      "Iter 1578 | Time 91.1552(92.4867) | Bit/dim 3.7391(3.7357) | Xent 0.6440(0.7134) | Loss 4.0611(4.0924) | Error 0.2258(0.2547) Steps 892(895.74) | Grad Norm 4.3940(5.2790) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 30.7835, Epoch Time 603.2895(582.1291), Bit/dim 3.7361(best: 3.7357), Xent 1.1563, Loss 4.3142, Error 0.3695(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 92.8383(92.4973) | Bit/dim 3.7307(3.7355) | Xent 0.6260(0.7107) | Loss 4.0436(4.0909) | Error 0.2212(0.2537) Steps 898(895.81) | Grad Norm 1.6902(5.1714) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 95.2899(92.5810) | Bit/dim 3.7348(3.7355) | Xent 0.6355(0.7085) | Loss 4.0526(4.0898) | Error 0.2246(0.2528) Steps 892(895.70) | Grad Norm 3.1276(5.1101) | Total Time 14.00(14.00)\n",
      "Iter 1581 | Time 92.8159(92.5881) | Bit/dim 3.7348(3.7355) | Xent 0.6514(0.7068) | Loss 4.0606(4.0889) | Error 0.2331(0.2522) Steps 898(895.77) | Grad Norm 3.7397(5.0689) | Total Time 14.00(14.00)\n",
      "Iter 1582 | Time 92.3525(92.5810) | Bit/dim 3.7266(3.7352) | Xent 0.6386(0.7047) | Loss 4.0459(4.0876) | Error 0.2242(0.2514) Steps 892(895.65) | Grad Norm 4.3788(5.0482) | Total Time 14.00(14.00)\n",
      "Iter 1583 | Time 93.0157(92.5941) | Bit/dim 3.7270(3.7350) | Xent 0.6282(0.7024) | Loss 4.0411(4.0862) | Error 0.2274(0.2506) Steps 892(895.54) | Grad Norm 1.7475(4.9492) | Total Time 14.00(14.00)\n",
      "Iter 1584 | Time 93.9482(92.6347) | Bit/dim 3.7327(3.7349) | Xent 0.6040(0.6995) | Loss 4.0347(4.0847) | Error 0.2140(0.2495) Steps 898(895.62) | Grad Norm 3.7307(4.9127) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 30.9876, Epoch Time 606.8648(582.8712), Bit/dim 3.7333(best: 3.7357), Xent 1.1723, Loss 4.3194, Error 0.3705(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 93.6176(92.6642) | Bit/dim 3.7318(3.7348) | Xent 0.6333(0.6975) | Loss 4.0484(4.0836) | Error 0.2231(0.2487) Steps 892(895.51) | Grad Norm 3.6741(4.8755) | Total Time 14.00(14.00)\n",
      "Iter 1586 | Time 93.5975(92.6922) | Bit/dim 3.7299(3.7347) | Xent 0.6353(0.6956) | Loss 4.0475(4.0825) | Error 0.2280(0.2481) Steps 904(895.76) | Grad Norm 3.6190(4.8378) | Total Time 14.00(14.00)\n",
      "Iter 1587 | Time 90.3848(92.6229) | Bit/dim 3.7289(3.7345) | Xent 0.6265(0.6936) | Loss 4.0421(4.0813) | Error 0.2212(0.2473) Steps 898(895.83) | Grad Norm 5.1907(4.8484) | Total Time 14.00(14.00)\n",
      "Iter 1588 | Time 89.0385(92.5154) | Bit/dim 3.7236(3.7342) | Xent 0.6481(0.6922) | Loss 4.0477(4.0803) | Error 0.2328(0.2469) Steps 898(895.90) | Grad Norm 5.2603(4.8608) | Total Time 14.00(14.00)\n",
      "Iter 1589 | Time 92.9005(92.5270) | Bit/dim 3.7290(3.7340) | Xent 0.6236(0.6901) | Loss 4.0408(4.0791) | Error 0.2205(0.2461) Steps 886(895.60) | Grad Norm 5.3098(4.8742) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 94.4850(92.5857) | Bit/dim 3.7334(3.7340) | Xent 0.6314(0.6884) | Loss 4.0491(4.0782) | Error 0.2269(0.2455) Steps 898(895.67) | Grad Norm 4.6905(4.8687) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 30.7083, Epoch Time 600.1719(583.3902), Bit/dim 3.7338(best: 3.7333), Xent 1.2100, Loss 4.3388, Error 0.3777(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 93.0404(92.5994) | Bit/dim 3.7316(3.7339) | Xent 0.6539(0.6873) | Loss 4.0586(4.0776) | Error 0.2324(0.2451) Steps 892(895.56) | Grad Norm 3.0242(4.8134) | Total Time 14.00(14.00)\n",
      "Iter 1592 | Time 94.9739(92.6706) | Bit/dim 3.7266(3.7337) | Xent 0.6098(0.6850) | Loss 4.0315(4.0762) | Error 0.2192(0.2443) Steps 904(895.81) | Grad Norm 1.7134(4.7204) | Total Time 14.00(14.00)\n",
      "Iter 1593 | Time 94.4546(92.7241) | Bit/dim 3.7275(3.7335) | Xent 0.6278(0.6833) | Loss 4.0414(4.0752) | Error 0.2229(0.2437) Steps 898(895.88) | Grad Norm 2.4692(4.6528) | Total Time 14.00(14.00)\n",
      "Iter 1594 | Time 92.4393(92.7156) | Bit/dim 3.7376(3.7336) | Xent 0.6139(0.6812) | Loss 4.0445(4.0743) | Error 0.2194(0.2430) Steps 898(895.94) | Grad Norm 2.4260(4.5860) | Total Time 14.00(14.00)\n",
      "Iter 1595 | Time 90.0313(92.6350) | Bit/dim 3.7239(3.7334) | Xent 0.6270(0.6796) | Loss 4.0374(4.0731) | Error 0.2236(0.2424) Steps 892(895.83) | Grad Norm 2.0660(4.5104) | Total Time 14.00(14.00)\n",
      "Iter 1596 | Time 95.0021(92.7060) | Bit/dim 3.7320(3.7333) | Xent 0.6220(0.6779) | Loss 4.0430(4.0722) | Error 0.2199(0.2417) Steps 904(896.07) | Grad Norm 3.9797(4.4945) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 30.9097, Epoch Time 606.1284(584.0724), Bit/dim 3.7346(best: 3.7333), Xent 1.1903, Loss 4.3297, Error 0.3742(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 97.1921(92.8406) | Bit/dim 3.7297(3.7332) | Xent 0.6330(0.6765) | Loss 4.0462(4.0715) | Error 0.2216(0.2411) Steps 892(895.95) | Grad Norm 4.9312(4.5076) | Total Time 14.00(14.00)\n",
      "Iter 1598 | Time 96.6952(92.9563) | Bit/dim 3.7262(3.7330) | Xent 0.6359(0.6753) | Loss 4.0441(4.0706) | Error 0.2260(0.2407) Steps 886(895.65) | Grad Norm 3.5627(4.4793) | Total Time 14.00(14.00)\n",
      "Iter 1599 | Time 93.3756(92.9688) | Bit/dim 3.7270(3.7328) | Xent 0.6302(0.6739) | Loss 4.0421(4.0698) | Error 0.2236(0.2401) Steps 898(895.72) | Grad Norm 2.4200(4.4175) | Total Time 14.00(14.00)\n",
      "Iter 1600 | Time 91.3209(92.9194) | Bit/dim 3.7262(3.7326) | Xent 0.6153(0.6722) | Loss 4.0338(4.0687) | Error 0.2174(0.2395) Steps 910(896.15) | Grad Norm 2.7907(4.3687) | Total Time 14.00(14.00)\n",
      "Iter 1601 | Time 98.4681(93.0859) | Bit/dim 3.7260(3.7324) | Xent 0.6457(0.6714) | Loss 4.0488(4.0681) | Error 0.2322(0.2392) Steps 904(896.38) | Grad Norm 4.9521(4.3862) | Total Time 14.00(14.00)\n",
      "Iter 1602 | Time 94.0004(93.1133) | Bit/dim 3.7245(3.7322) | Xent 0.6134(0.6697) | Loss 4.0312(4.0670) | Error 0.2189(0.2386) Steps 880(895.89) | Grad Norm 4.0254(4.3754) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 31.1703, Epoch Time 623.7735(585.2634), Bit/dim 3.7326(best: 3.7333), Xent 1.1749, Loss 4.3200, Error 0.3753(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 93.7905(93.1336) | Bit/dim 3.7217(3.7319) | Xent 0.5939(0.6674) | Loss 4.0186(4.0656) | Error 0.2079(0.2377) Steps 892(895.78) | Grad Norm 1.8922(4.3009) | Total Time 14.00(14.00)\n",
      "Iter 1604 | Time 88.2101(92.9859) | Bit/dim 3.7342(3.7319) | Xent 0.5984(0.6653) | Loss 4.0334(4.0646) | Error 0.2079(0.2368) Steps 886(895.48) | Grad Norm 4.5880(4.3095) | Total Time 14.00(14.00)\n",
      "Iter 1605 | Time 96.6793(93.0967) | Bit/dim 3.7347(3.7320) | Xent 0.6686(0.6654) | Loss 4.0690(4.0647) | Error 0.2439(0.2370) Steps 910(895.92) | Grad Norm 7.9682(4.4192) | Total Time 14.00(14.00)\n",
      "Iter 1606 | Time 95.6947(93.1747) | Bit/dim 3.7299(3.7320) | Xent 0.7276(0.6673) | Loss 4.0937(4.0656) | Error 0.2566(0.2376) Steps 898(895.98) | Grad Norm 10.9896(4.6164) | Total Time 14.00(14.00)\n",
      "Iter 1607 | Time 95.7668(93.2524) | Bit/dim 3.7238(3.7317) | Xent 0.8280(0.6721) | Loss 4.1378(4.0678) | Error 0.2964(0.2394) Steps 904(896.22) | Grad Norm 11.0827(4.8103) | Total Time 14.00(14.00)\n",
      "Iter 1608 | Time 91.2200(93.1914) | Bit/dim 3.7343(3.7318) | Xent 0.7527(0.6745) | Loss 4.1106(4.0690) | Error 0.2662(0.2402) Steps 904(896.45) | Grad Norm 6.3573(4.8568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 31.0070, Epoch Time 607.6956(585.9364), Bit/dim 3.7344(best: 3.7326), Xent 1.1999, Loss 4.3344, Error 0.3912(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 95.4144(93.2581) | Bit/dim 3.7284(3.7317) | Xent 0.7544(0.6769) | Loss 4.1056(4.0701) | Error 0.2676(0.2410) Steps 916(897.04) | Grad Norm 5.9532(4.8896) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 95.0744(93.3126) | Bit/dim 3.7365(3.7318) | Xent 0.6867(0.6772) | Loss 4.0799(4.0704) | Error 0.2518(0.2413) Steps 898(897.07) | Grad Norm 4.5866(4.8806) | Total Time 14.00(14.00)\n",
      "Iter 1611 | Time 93.0428(93.3045) | Bit/dim 3.7351(3.7319) | Xent 0.7052(0.6780) | Loss 4.0877(4.0710) | Error 0.2549(0.2417) Steps 892(896.92) | Grad Norm 4.3186(4.8637) | Total Time 14.00(14.00)\n",
      "Iter 1612 | Time 95.6225(93.3741) | Bit/dim 3.7368(3.7321) | Xent 0.6667(0.6777) | Loss 4.0701(4.0709) | Error 0.2356(0.2416) Steps 904(897.13) | Grad Norm 4.0369(4.8389) | Total Time 14.00(14.00)\n",
      "Iter 1613 | Time 92.0006(93.3329) | Bit/dim 3.7338(3.7321) | Xent 0.6911(0.6781) | Loss 4.0794(4.0712) | Error 0.2438(0.2416) Steps 904(897.34) | Grad Norm 5.1525(4.8483) | Total Time 14.00(14.00)\n",
      "Iter 1614 | Time 92.2654(93.3008) | Bit/dim 3.7189(3.7317) | Xent 0.6441(0.6771) | Loss 4.0409(4.0703) | Error 0.2290(0.2412) Steps 898(897.36) | Grad Norm 2.9942(4.7927) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 31.1143, Epoch Time 610.0844(586.6608), Bit/dim 3.7408(best: 3.7326), Xent 1.1991, Loss 4.3404, Error 0.3811(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 93.2864(93.3004) | Bit/dim 3.7389(3.7319) | Xent 0.6431(0.6761) | Loss 4.0604(4.0700) | Error 0.2318(0.2410) Steps 886(897.02) | Grad Norm 3.8571(4.7646) | Total Time 14.00(14.00)\n",
      "Iter 1616 | Time 94.9804(93.3508) | Bit/dim 3.7322(3.7320) | Xent 0.6397(0.6750) | Loss 4.0520(4.0694) | Error 0.2295(0.2406) Steps 898(897.05) | Grad Norm 4.5368(4.7578) | Total Time 14.00(14.00)\n",
      "Iter 1617 | Time 92.6142(93.3287) | Bit/dim 3.7321(3.7320) | Xent 0.6461(0.6741) | Loss 4.0551(4.0690) | Error 0.2272(0.2402) Steps 910(897.43) | Grad Norm 4.4311(4.7480) | Total Time 14.00(14.00)\n",
      "Iter 1618 | Time 92.6905(93.3096) | Bit/dim 3.7394(3.7322) | Xent 0.6193(0.6725) | Loss 4.0490(4.0684) | Error 0.2204(0.2396) Steps 886(897.09) | Grad Norm 3.3173(4.7051) | Total Time 14.00(14.00)\n",
      "Iter 1619 | Time 93.3518(93.3108) | Bit/dim 3.7325(3.7322) | Xent 0.5991(0.6703) | Loss 4.0321(4.0673) | Error 0.2154(0.2389) Steps 898(897.12) | Grad Norm 2.2758(4.6322) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 93.2066(93.3077) | Bit/dim 3.7281(3.7321) | Xent 0.6315(0.6691) | Loss 4.0439(4.0666) | Error 0.2289(0.2386) Steps 898(897.14) | Grad Norm 4.0840(4.6157) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 29.9289, Epoch Time 610.4483(587.3744), Bit/dim 3.7367(best: 3.7326), Xent 1.1898, Loss 4.3316, Error 0.3715(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 90.9020(93.2355) | Bit/dim 3.7328(3.7321) | Xent 0.6293(0.6679) | Loss 4.0474(4.0660) | Error 0.2260(0.2382) Steps 892(896.99) | Grad Norm 5.5192(4.6428) | Total Time 14.00(14.00)\n",
      "Iter 1622 | Time 93.6548(93.2481) | Bit/dim 3.7302(3.7320) | Xent 0.6132(0.6663) | Loss 4.0368(4.0652) | Error 0.2209(0.2377) Steps 880(896.48) | Grad Norm 3.3539(4.6042) | Total Time 14.00(14.00)\n",
      "Iter 1623 | Time 91.4305(93.1936) | Bit/dim 3.7315(3.7320) | Xent 0.6040(0.6644) | Loss 4.0335(4.0642) | Error 0.2176(0.2371) Steps 910(896.89) | Grad Norm 3.5570(4.5728) | Total Time 14.00(14.00)\n",
      "Iter 1624 | Time 94.3835(93.2293) | Bit/dim 3.7231(3.7317) | Xent 0.5938(0.6623) | Loss 4.0200(4.0629) | Error 0.2117(0.2363) Steps 904(897.10) | Grad Norm 3.1012(4.5286) | Total Time 14.00(14.00)\n",
      "Iter 1625 | Time 90.9858(93.1620) | Bit/dim 3.7251(3.7315) | Xent 0.6331(0.6614) | Loss 4.0417(4.0622) | Error 0.2206(0.2359) Steps 886(896.77) | Grad Norm 5.3991(4.5547) | Total Time 14.00(14.00)\n",
      "Iter 1626 | Time 93.3507(93.1676) | Bit/dim 3.7319(3.7316) | Xent 0.6435(0.6609) | Loss 4.0537(4.0620) | Error 0.2238(0.2355) Steps 904(896.98) | Grad Norm 6.6331(4.6171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 30.7592, Epoch Time 605.7689(587.9263), Bit/dim 3.7346(best: 3.7326), Xent 1.2116, Loss 4.3404, Error 0.3742(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 92.1704(93.1377) | Bit/dim 3.7267(3.7314) | Xent 0.6093(0.6593) | Loss 4.0313(4.0611) | Error 0.2194(0.2350) Steps 898(897.01) | Grad Norm 5.2358(4.6356) | Total Time 14.00(14.00)\n",
      "Iter 1628 | Time 90.9652(93.0725) | Bit/dim 3.7244(3.7312) | Xent 0.6175(0.6581) | Loss 4.0331(4.0602) | Error 0.2200(0.2346) Steps 910(897.40) | Grad Norm 3.7762(4.6099) | Total Time 14.00(14.00)\n",
      "Iter 1629 | Time 97.5404(93.2066) | Bit/dim 3.7260(3.7310) | Xent 0.6039(0.6564) | Loss 4.0279(4.0593) | Error 0.2127(0.2339) Steps 910(897.78) | Grad Norm 3.1838(4.5671) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 92.6719(93.1905) | Bit/dim 3.7192(3.7307) | Xent 0.6470(0.6562) | Loss 4.0428(4.0588) | Error 0.2290(0.2338) Steps 892(897.61) | Grad Norm 3.4087(4.5323) | Total Time 14.00(14.00)\n",
      "Iter 1631 | Time 92.7007(93.1758) | Bit/dim 3.7393(3.7309) | Xent 0.5974(0.6544) | Loss 4.0380(4.0581) | Error 0.2114(0.2331) Steps 904(897.80) | Grad Norm 2.7895(4.4800) | Total Time 14.00(14.00)\n",
      "Iter 1632 | Time 95.0257(93.2313) | Bit/dim 3.7294(3.7309) | Xent 0.6108(0.6531) | Loss 4.0348(4.0574) | Error 0.2164(0.2326) Steps 892(897.63) | Grad Norm 2.9760(4.4349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 30.7012, Epoch Time 607.3351(588.5085), Bit/dim 3.7316(best: 3.7326), Xent 1.1705, Loss 4.3169, Error 0.3740(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 96.2435(93.3217) | Bit/dim 3.7296(3.7309) | Xent 0.6002(0.6515) | Loss 4.0297(4.0566) | Error 0.2137(0.2320) Steps 904(897.82) | Grad Norm 3.8806(4.4183) | Total Time 14.00(14.00)\n",
      "Iter 1634 | Time 94.1385(93.3462) | Bit/dim 3.7318(3.7309) | Xent 0.6143(0.6504) | Loss 4.0389(4.0561) | Error 0.2225(0.2317) Steps 898(897.82) | Grad Norm 4.9751(4.4350) | Total Time 14.00(14.00)\n",
      "Iter 1635 | Time 90.0309(93.2468) | Bit/dim 3.7227(3.7306) | Xent 0.5978(0.6488) | Loss 4.0216(4.0550) | Error 0.2184(0.2313) Steps 904(898.01) | Grad Norm 4.6870(4.4425) | Total Time 14.00(14.00)\n",
      "Iter 1636 | Time 93.4399(93.2525) | Bit/dim 3.7285(3.7306) | Xent 0.6012(0.6474) | Loss 4.0291(4.0543) | Error 0.2136(0.2308) Steps 886(897.65) | Grad Norm 3.4738(4.4135) | Total Time 14.00(14.00)\n",
      "Iter 1637 | Time 94.1253(93.2787) | Bit/dim 3.7381(3.7308) | Xent 0.5910(0.6457) | Loss 4.0336(4.0536) | Error 0.2120(0.2302) Steps 916(898.20) | Grad Norm 4.6361(4.4202) | Total Time 14.00(14.00)\n",
      "Iter 1638 | Time 98.0652(93.4223) | Bit/dim 3.7168(3.7304) | Xent 0.6097(0.6446) | Loss 4.0217(4.0527) | Error 0.2192(0.2299) Steps 910(898.55) | Grad Norm 5.2036(4.4437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 30.3233, Epoch Time 616.6651(589.3532), Bit/dim 3.7321(best: 3.7316), Xent 1.1895, Loss 4.3269, Error 0.3792(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1639 | Time 98.1444(93.5640) | Bit/dim 3.7264(3.7303) | Xent 0.6146(0.6437) | Loss 4.0337(4.0521) | Error 0.2195(0.2296) Steps 892(898.36) | Grad Norm 6.2911(4.4991) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 93.1911(93.5528) | Bit/dim 3.7360(3.7304) | Xent 0.6172(0.6429) | Loss 4.0447(4.0519) | Error 0.2226(0.2294) Steps 916(898.88) | Grad Norm 7.6702(4.5942) | Total Time 14.00(14.00)\n",
      "Iter 1641 | Time 93.0440(93.5375) | Bit/dim 3.7273(3.7303) | Xent 0.6159(0.6421) | Loss 4.0352(4.0514) | Error 0.2250(0.2293) Steps 916(899.40) | Grad Norm 8.3950(4.7082) | Total Time 14.00(14.00)\n",
      "Iter 1642 | Time 92.5284(93.5073) | Bit/dim 3.7205(3.7300) | Xent 0.6311(0.6418) | Loss 4.0361(4.0509) | Error 0.2275(0.2292) Steps 898(899.36) | Grad Norm 7.2114(4.7833) | Total Time 14.00(14.00)\n",
      "Iter 1643 | Time 95.7460(93.5744) | Bit/dim 3.7209(3.7298) | Xent 0.6159(0.6410) | Loss 4.0288(4.0503) | Error 0.2201(0.2289) Steps 916(899.86) | Grad Norm 6.6069(4.8380) | Total Time 14.00(14.00)\n",
      "Iter 1644 | Time 93.1038(93.5603) | Bit/dim 3.7285(3.7297) | Xent 0.5749(0.6390) | Loss 4.0159(4.0492) | Error 0.2044(0.2282) Steps 916(900.34) | Grad Norm 4.0745(4.8151) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 31.0012, Epoch Time 612.2998(590.0416), Bit/dim 3.7309(best: 3.7316), Xent 1.2099, Loss 4.3358, Error 0.3789(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1645 | Time 93.0008(93.5435) | Bit/dim 3.7174(3.7294) | Xent 0.6034(0.6379) | Loss 4.0192(4.0483) | Error 0.2189(0.2279) Steps 898(900.27) | Grad Norm 5.0742(4.8229) | Total Time 14.00(14.00)\n",
      "Iter 1646 | Time 93.2643(93.5351) | Bit/dim 3.7342(3.7295) | Xent 0.6065(0.6370) | Loss 4.0374(4.0480) | Error 0.2181(0.2276) Steps 892(900.02) | Grad Norm 4.8149(4.8227) | Total Time 14.00(14.00)\n",
      "Iter 1647 | Time 96.5364(93.6252) | Bit/dim 3.7282(3.7295) | Xent 0.6012(0.6359) | Loss 4.0288(4.0474) | Error 0.2160(0.2273) Steps 928(900.86) | Grad Norm 4.2138(4.8044) | Total Time 14.00(14.00)\n",
      "Iter 1648 | Time 95.4263(93.6792) | Bit/dim 3.7237(3.7293) | Xent 0.5859(0.6344) | Loss 4.0167(4.0465) | Error 0.2073(0.2267) Steps 898(900.78) | Grad Norm 5.7030(4.8314) | Total Time 14.00(14.00)\n",
      "Iter 1649 | Time 97.1532(93.7834) | Bit/dim 3.7259(3.7292) | Xent 0.6154(0.6339) | Loss 4.0336(4.0461) | Error 0.2202(0.2265) Steps 898(900.69) | Grad Norm 5.7826(4.8599) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 97.0214(93.8806) | Bit/dim 3.7279(3.7292) | Xent 0.6673(0.6349) | Loss 4.0615(4.0466) | Error 0.2412(0.2269) Steps 904(900.79) | Grad Norm 7.8276(4.9489) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 30.5732, Epoch Time 626.4308(591.1333), Bit/dim 3.7340(best: 3.7309), Xent 1.3076, Loss 4.3878, Error 0.3924(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1651 | Time 94.7700(93.9072) | Bit/dim 3.7330(3.7293) | Xent 0.6547(0.6355) | Loss 4.0604(4.0470) | Error 0.2329(0.2271) Steps 904(900.89) | Grad Norm 11.2989(5.1394) | Total Time 14.00(14.00)\n",
      "Iter 1652 | Time 95.6748(93.9603) | Bit/dim 3.7361(3.7295) | Xent 0.8614(0.6422) | Loss 4.1668(4.0506) | Error 0.2935(0.2291) Steps 916(901.34) | Grad Norm 15.4376(5.4484) | Total Time 14.00(14.00)\n",
      "Iter 1653 | Time 93.5296(93.9474) | Bit/dim 3.7302(3.7295) | Xent 0.7069(0.6442) | Loss 4.0836(4.0516) | Error 0.2506(0.2297) Steps 892(901.06) | Grad Norm 9.5500(5.5714) | Total Time 14.00(14.00)\n",
      "Iter 1654 | Time 91.6946(93.8798) | Bit/dim 3.7468(3.7300) | Xent 0.7516(0.6474) | Loss 4.1226(4.0537) | Error 0.2721(0.2310) Steps 892(900.79) | Grad Norm 10.4169(5.7168) | Total Time 14.00(14.00)\n",
      "Iter 1655 | Time 97.8371(93.9985) | Bit/dim 3.7205(3.7297) | Xent 0.7207(0.6496) | Loss 4.0808(4.0545) | Error 0.2581(0.2318) Steps 910(901.07) | Grad Norm 6.5543(5.7419) | Total Time 14.00(14.00)\n",
      "Iter 1656 | Time 97.4640(94.1025) | Bit/dim 3.7573(3.7306) | Xent 0.7952(0.6540) | Loss 4.1549(4.0575) | Error 0.2893(0.2335) Steps 922(901.69) | Grad Norm 11.5229(5.9153) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 31.1099, Epoch Time 623.8373(592.1144), Bit/dim 3.7383(best: 3.7309), Xent 1.2026, Loss 4.3396, Error 0.3923(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1657 | Time 93.4724(94.0836) | Bit/dim 3.7266(3.7304) | Xent 0.7098(0.6556) | Loss 4.0815(4.0583) | Error 0.2544(0.2342) Steps 922(902.30) | Grad Norm 6.2056(5.9241) | Total Time 14.00(14.00)\n",
      "Iter 1658 | Time 93.1358(94.0551) | Bit/dim 3.7496(3.7310) | Xent 0.6976(0.6569) | Loss 4.0984(4.0595) | Error 0.2486(0.2346) Steps 886(901.81) | Grad Norm 4.8617(5.8922) | Total Time 14.00(14.00)\n",
      "Iter 1659 | Time 91.1452(93.9678) | Bit/dim 3.7326(3.7311) | Xent 0.7004(0.6582) | Loss 4.0828(4.0602) | Error 0.2465(0.2350) Steps 904(901.88) | Grad Norm 4.7341(5.8574) | Total Time 14.00(14.00)\n",
      "Iter 1660 | Time 98.7606(94.1116) | Bit/dim 3.7372(3.7312) | Xent 0.6828(0.6589) | Loss 4.0786(4.0607) | Error 0.2478(0.2353) Steps 916(902.30) | Grad Norm 3.8711(5.7978) | Total Time 14.00(14.00)\n",
      "Iter 1661 | Time 94.9374(94.1364) | Bit/dim 3.7391(3.7315) | Xent 0.6808(0.6596) | Loss 4.0795(4.0613) | Error 0.2474(0.2357) Steps 886(901.81) | Grad Norm 4.1145(5.7473) | Total Time 14.00(14.00)\n",
      "Iter 1662 | Time 96.7128(94.2137) | Bit/dim 3.7484(3.7320) | Xent 0.6457(0.6592) | Loss 4.0712(4.0616) | Error 0.2302(0.2355) Steps 904(901.88) | Grad Norm 3.6698(5.6850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 30.4659, Epoch Time 613.8290(592.7659), Bit/dim 3.7458(best: 3.7309), Xent 1.2395, Loss 4.3655, Error 0.3842(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1663 | Time 93.6559(94.1969) | Bit/dim 3.7451(3.7324) | Xent 0.6504(0.6589) | Loss 4.0703(4.0618) | Error 0.2302(0.2354) Steps 916(902.30) | Grad Norm 6.1070(5.6977) | Total Time 14.00(14.00)\n",
      "Iter 1664 | Time 88.5781(94.0284) | Bit/dim 3.7411(3.7326) | Xent 0.6322(0.6581) | Loss 4.0571(4.0617) | Error 0.2240(0.2350) Steps 898(902.17) | Grad Norm 5.4296(5.6896) | Total Time 14.00(14.00)\n",
      "Iter 1665 | Time 93.3754(94.0088) | Bit/dim 3.7374(3.7328) | Xent 0.6203(0.6570) | Loss 4.0476(4.0613) | Error 0.2186(0.2346) Steps 874(901.33) | Grad Norm 3.5233(5.6247) | Total Time 14.00(14.00)\n",
      "Iter 1666 | Time 95.4003(94.0505) | Bit/dim 3.7338(3.7328) | Xent 0.6100(0.6556) | Loss 4.0388(4.0606) | Error 0.2160(0.2340) Steps 904(901.41) | Grad Norm 3.5444(5.5622) | Total Time 14.00(14.00)\n",
      "Iter 1667 | Time 98.3381(94.1792) | Bit/dim 3.7280(3.7327) | Xent 0.5921(0.6537) | Loss 4.0241(4.0595) | Error 0.2093(0.2333) Steps 916(901.85) | Grad Norm 2.5566(5.4721) | Total Time 14.00(14.00)\n",
      "Iter 1668 | Time 96.4216(94.2464) | Bit/dim 3.7267(3.7325) | Xent 0.5935(0.6519) | Loss 4.0234(4.0584) | Error 0.2136(0.2327) Steps 910(902.09) | Grad Norm 3.7416(5.4202) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 30.6931, Epoch Time 611.7174(593.3344), Bit/dim 3.7374(best: 3.7309), Xent 1.2670, Loss 4.3709, Error 0.3839(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1669 | Time 95.8999(94.2960) | Bit/dim 3.7328(3.7325) | Xent 0.5656(0.6493) | Loss 4.0156(4.0571) | Error 0.1997(0.2317) Steps 898(901.97) | Grad Norm 2.3982(5.3295) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 97.2736(94.3854) | Bit/dim 3.7392(3.7327) | Xent 0.5838(0.6473) | Loss 4.0311(4.0564) | Error 0.2090(0.2310) Steps 898(901.85) | Grad Norm 3.5214(5.2753) | Total Time 14.00(14.00)\n",
      "Iter 1671 | Time 92.5058(94.3290) | Bit/dim 3.7252(3.7325) | Xent 0.5636(0.6448) | Loss 4.0070(4.0549) | Error 0.1996(0.2301) Steps 910(902.09) | Grad Norm 3.5748(5.2242) | Total Time 14.00(14.00)\n",
      "Iter 1672 | Time 93.8316(94.3141) | Bit/dim 3.7244(3.7322) | Xent 0.5722(0.6426) | Loss 4.0105(4.0535) | Error 0.2000(0.2292) Steps 904(902.15) | Grad Norm 3.8146(5.1820) | Total Time 14.00(14.00)\n",
      "Iter 1673 | Time 97.1818(94.4001) | Bit/dim 3.7285(3.7321) | Xent 0.5746(0.6406) | Loss 4.0158(4.0524) | Error 0.2041(0.2284) Steps 898(902.03) | Grad Norm 2.1761(5.0918) | Total Time 14.00(14.00)\n",
      "Iter 1674 | Time 94.3367(94.3982) | Bit/dim 3.7350(3.7322) | Xent 0.5818(0.6388) | Loss 4.0259(4.0516) | Error 0.2029(0.2276) Steps 910(902.27) | Grad Norm 3.2890(5.0377) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 31.1856, Epoch Time 621.7756(594.1876), Bit/dim 3.7338(best: 3.7309), Xent 1.2310, Loss 4.3493, Error 0.3742(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1675 | Time 93.1023(94.3593) | Bit/dim 3.7348(3.7323) | Xent 0.5704(0.6368) | Loss 4.0200(4.0507) | Error 0.2013(0.2268) Steps 892(901.96) | Grad Norm 2.8645(4.9725) | Total Time 14.00(14.00)\n",
      "Iter 1676 | Time 93.3591(94.3293) | Bit/dim 3.7095(3.7316) | Xent 0.5740(0.6349) | Loss 3.9966(4.0490) | Error 0.2056(0.2262) Steps 904(902.02) | Grad Norm 2.9847(4.9129) | Total Time 14.00(14.00)\n",
      "Iter 1677 | Time 94.1048(94.3226) | Bit/dim 3.7381(3.7318) | Xent 0.5678(0.6329) | Loss 4.0220(4.0482) | Error 0.1991(0.2254) Steps 886(901.54) | Grad Norm 2.9662(4.8545) | Total Time 14.00(14.00)\n",
      "Iter 1678 | Time 94.9996(94.3429) | Bit/dim 3.7216(3.7315) | Xent 0.5499(0.6304) | Loss 3.9965(4.0467) | Error 0.1900(0.2243) Steps 898(901.43) | Grad Norm 2.2490(4.7763) | Total Time 14.00(14.00)\n",
      "Iter 1679 | Time 92.8885(94.2992) | Bit/dim 3.7196(3.7311) | Xent 0.5309(0.6274) | Loss 3.9851(4.0448) | Error 0.1905(0.2233) Steps 910(901.69) | Grad Norm 2.4414(4.7063) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 98.1072(94.4135) | Bit/dim 3.7379(3.7313) | Xent 0.5520(0.6251) | Loss 4.0139(4.0439) | Error 0.2013(0.2227) Steps 922(902.30) | Grad Norm 2.3135(4.6345) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 31.0725, Epoch Time 614.4378(594.7951), Bit/dim 3.7345(best: 3.7309), Xent 1.2531, Loss 4.3610, Error 0.3747(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1681 | Time 95.4027(94.4432) | Bit/dim 3.7274(3.7312) | Xent 0.5550(0.6230) | Loss 4.0049(4.0427) | Error 0.1966(0.2219) Steps 916(902.71) | Grad Norm 3.4148(4.5979) | Total Time 14.00(14.00)\n",
      "Iter 1682 | Time 91.8297(94.3648) | Bit/dim 3.7229(3.7310) | Xent 0.5280(0.6202) | Loss 3.9869(4.0411) | Error 0.1870(0.2208) Steps 898(902.57) | Grad Norm 2.2289(4.5268) | Total Time 14.00(14.00)\n",
      "Iter 1683 | Time 95.5087(94.3991) | Bit/dim 3.7231(3.7307) | Xent 0.5462(0.6180) | Loss 3.9962(4.0397) | Error 0.1933(0.2200) Steps 892(902.25) | Grad Norm 3.9916(4.5108) | Total Time 14.00(14.00)\n",
      "Iter 1684 | Time 94.6981(94.4080) | Bit/dim 3.7280(3.7307) | Xent 0.5524(0.6160) | Loss 4.0042(4.0387) | Error 0.1924(0.2192) Steps 904(902.30) | Grad Norm 4.2046(4.5016) | Total Time 14.00(14.00)\n",
      "Iter 1685 | Time 93.4577(94.3795) | Bit/dim 3.7258(3.7305) | Xent 0.5546(0.6141) | Loss 4.0031(4.0376) | Error 0.1970(0.2185) Steps 904(902.35) | Grad Norm 3.4573(4.4702) | Total Time 14.00(14.00)\n",
      "Iter 1686 | Time 95.4095(94.4104) | Bit/dim 3.7227(3.7303) | Xent 0.5572(0.6124) | Loss 4.0013(4.0365) | Error 0.1966(0.2179) Steps 910(902.58) | Grad Norm 3.7422(4.4484) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 30.9936, Epoch Time 615.1085(595.4045), Bit/dim 3.7321(best: 3.7309), Xent 1.2855, Loss 4.3749, Error 0.3785(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1687 | Time 95.3004(94.4371) | Bit/dim 3.7271(3.7302) | Xent 0.5448(0.6104) | Loss 3.9995(4.0354) | Error 0.1951(0.2172) Steps 916(902.99) | Grad Norm 3.3974(4.4169) | Total Time 14.00(14.00)\n",
      "Iter 1688 | Time 95.9095(94.4813) | Bit/dim 3.7211(3.7299) | Xent 0.5713(0.6092) | Loss 4.0068(4.0345) | Error 0.2036(0.2168) Steps 892(902.66) | Grad Norm 5.0644(4.4363) | Total Time 14.00(14.00)\n",
      "Iter 1689 | Time 96.0409(94.5281) | Bit/dim 3.7271(3.7298) | Xent 0.6197(0.6096) | Loss 4.0369(4.0346) | Error 0.2228(0.2169) Steps 892(902.34) | Grad Norm 12.3993(4.6752) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 93.7854(94.5058) | Bit/dim 3.7436(3.7302) | Xent 0.7950(0.6151) | Loss 4.1411(4.0378) | Error 0.2770(0.2187) Steps 910(902.57) | Grad Norm 15.5954(5.0028) | Total Time 14.00(14.00)\n",
      "Iter 1691 | Time 95.1282(94.5245) | Bit/dim 3.7291(3.7302) | Xent 0.7942(0.6205) | Loss 4.1262(4.0405) | Error 0.2788(0.2205) Steps 904(902.61) | Grad Norm 9.5790(5.1401) | Total Time 14.00(14.00)\n",
      "Iter 1692 | Time 94.1387(94.5129) | Bit/dim 3.7374(3.7304) | Xent 0.8886(0.6285) | Loss 4.1817(4.0447) | Error 0.3101(0.2232) Steps 886(902.11) | Grad Norm 10.7330(5.3079) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 30.6821, Epoch Time 616.4709(596.0365), Bit/dim 3.7535(best: 3.7309), Xent 1.3382, Loss 4.4226, Error 0.4325(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1693 | Time 91.0373(94.4086) | Bit/dim 3.7508(3.7310) | Xent 0.9368(0.6378) | Loss 4.2192(4.0499) | Error 0.3275(0.2264) Steps 910(902.35) | Grad Norm 15.0731(5.6008) | Total Time 14.00(14.00)\n",
      "Iter 1694 | Time 93.9452(94.3947) | Bit/dim 3.7524(3.7317) | Xent 0.8494(0.6441) | Loss 4.1771(4.0537) | Error 0.3020(0.2286) Steps 916(902.76) | Grad Norm 6.2128(5.6192) | Total Time 14.00(14.00)\n",
      "Iter 1695 | Time 89.6131(94.2513) | Bit/dim 3.7602(3.7325) | Xent 0.8773(0.6511) | Loss 4.1989(4.0581) | Error 0.3124(0.2311) Steps 874(901.90) | Grad Norm 11.5521(5.7972) | Total Time 14.00(14.00)\n",
      "Iter 1696 | Time 92.3528(94.1943) | Bit/dim 3.7675(3.7336) | Xent 0.7806(0.6550) | Loss 4.1578(4.0611) | Error 0.2802(0.2326) Steps 904(901.96) | Grad Norm 9.0214(5.8939) | Total Time 14.00(14.00)\n",
      "Iter 1697 | Time 95.1697(94.2236) | Bit/dim 3.7591(3.7343) | Xent 0.8553(0.6610) | Loss 4.1868(4.0649) | Error 0.3070(0.2348) Steps 928(902.74) | Grad Norm 7.4763(5.9414) | Total Time 14.00(14.00)\n",
      "Iter 1698 | Time 95.9829(94.2764) | Bit/dim 3.7584(3.7351) | Xent 0.7579(0.6639) | Loss 4.1373(4.0670) | Error 0.2752(0.2361) Steps 934(903.68) | Grad Norm 6.3541(5.9538) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 30.9765, Epoch Time 604.4633(596.2893), Bit/dim 3.7568(best: 3.7309), Xent 1.1709, Loss 4.3422, Error 0.3947(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1699 | Time 95.5447(94.3144) | Bit/dim 3.7641(3.7359) | Xent 0.7993(0.6680) | Loss 4.1637(4.0699) | Error 0.2824(0.2374) Steps 916(904.05) | Grad Norm 4.4518(5.9087) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 97.4195(94.4076) | Bit/dim 3.7552(3.7365) | Xent 0.7452(0.6703) | Loss 4.1278(4.0717) | Error 0.2686(0.2384) Steps 916(904.41) | Grad Norm 5.6601(5.9012) | Total Time 14.00(14.00)\n",
      "Iter 1701 | Time 91.1823(94.3108) | Bit/dim 3.7450(3.7368) | Xent 0.6887(0.6709) | Loss 4.0894(4.0722) | Error 0.2404(0.2384) Steps 916(904.75) | Grad Norm 4.1557(5.8489) | Total Time 14.00(14.00)\n",
      "Iter 1702 | Time 92.2693(94.2496) | Bit/dim 3.7437(3.7370) | Xent 0.7075(0.6720) | Loss 4.0975(4.0730) | Error 0.2529(0.2389) Steps 910(904.91) | Grad Norm 4.8868(5.8200) | Total Time 14.00(14.00)\n",
      "Iter 1703 | Time 91.4760(94.1664) | Bit/dim 3.7466(3.7373) | Xent 0.7083(0.6730) | Loss 4.1007(4.0738) | Error 0.2508(0.2392) Steps 898(904.70) | Grad Norm 6.5019(5.8405) | Total Time 14.00(14.00)\n",
      "Iter 1704 | Time 93.0499(94.1329) | Bit/dim 3.7416(3.7374) | Xent 0.6648(0.6728) | Loss 4.0740(4.0738) | Error 0.2439(0.2394) Steps 916(905.04) | Grad Norm 4.0674(5.7873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 30.8288, Epoch Time 607.3495(596.6211), Bit/dim 3.7448(best: 3.7309), Xent 1.1594, Loss 4.3245, Error 0.3755(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1705 | Time 91.4121(94.0512) | Bit/dim 3.7402(3.7375) | Xent 0.6786(0.6730) | Loss 4.0795(4.0740) | Error 0.2425(0.2395) Steps 910(905.19) | Grad Norm 5.3964(5.7755) | Total Time 14.00(14.00)\n",
      "Iter 1706 | Time 92.7167(94.0112) | Bit/dim 3.7424(3.7376) | Xent 0.6718(0.6729) | Loss 4.0783(4.0741) | Error 0.2331(0.2393) Steps 916(905.52) | Grad Norm 5.9587(5.7810) | Total Time 14.00(14.00)\n",
      "Iter 1707 | Time 93.8737(94.0071) | Bit/dim 3.7451(3.7378) | Xent 0.6331(0.6717) | Loss 4.0616(4.0737) | Error 0.2235(0.2388) Steps 898(905.29) | Grad Norm 4.7597(5.7504) | Total Time 14.00(14.00)\n",
      "Iter 1708 | Time 90.7687(93.9099) | Bit/dim 3.7432(3.7380) | Xent 0.6322(0.6706) | Loss 4.0593(4.0733) | Error 0.2275(0.2385) Steps 904(905.25) | Grad Norm 3.6421(5.6872) | Total Time 14.00(14.00)\n",
      "Iter 1709 | Time 96.9715(94.0018) | Bit/dim 3.7373(3.7380) | Xent 0.6022(0.6685) | Loss 4.0384(4.0722) | Error 0.2103(0.2376) Steps 892(904.85) | Grad Norm 3.4870(5.6211) | Total Time 14.00(14.00)\n",
      "Iter 1710 | Time 93.3428(93.9820) | Bit/dim 3.7249(3.7376) | Xent 0.6208(0.6671) | Loss 4.0353(4.0711) | Error 0.2245(0.2372) Steps 886(904.29) | Grad Norm 3.4743(5.5567) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 30.9612, Epoch Time 605.5158(596.8880), Bit/dim 3.7392(best: 3.7309), Xent 1.2408, Loss 4.3596, Error 0.3803(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1711 | Time 95.1108(94.0159) | Bit/dim 3.7347(3.7375) | Xent 0.6064(0.6653) | Loss 4.0379(4.0701) | Error 0.2155(0.2366) Steps 910(904.46) | Grad Norm 4.3955(5.5219) | Total Time 14.00(14.00)\n",
      "Iter 1712 | Time 96.8677(94.1014) | Bit/dim 3.7289(3.7372) | Xent 0.5660(0.6623) | Loss 4.0119(4.0684) | Error 0.2027(0.2356) Steps 898(904.27) | Grad Norm 3.7049(5.4674) | Total Time 14.00(14.00)\n",
      "Iter 1713 | Time 95.2879(94.1370) | Bit/dim 3.7347(3.7372) | Xent 0.5834(0.6599) | Loss 4.0264(4.0671) | Error 0.2101(0.2348) Steps 898(904.08) | Grad Norm 2.7478(5.3858) | Total Time 14.00(14.00)\n",
      "Iter 1714 | Time 96.7604(94.2157) | Bit/dim 3.7365(3.7372) | Xent 0.5619(0.6570) | Loss 4.0175(4.0656) | Error 0.1979(0.2337) Steps 916(904.44) | Grad Norm 2.8590(5.3100) | Total Time 14.00(14.00)\n",
      "Iter 1715 | Time 94.7859(94.2328) | Bit/dim 3.7317(3.7370) | Xent 0.5884(0.6549) | Loss 4.0258(4.0644) | Error 0.2095(0.2330) Steps 886(903.88) | Grad Norm 3.3013(5.2497) | Total Time 14.00(14.00)\n",
      "Iter 1716 | Time 96.7657(94.3088) | Bit/dim 3.7297(3.7368) | Xent 0.5697(0.6524) | Loss 4.0146(4.0629) | Error 0.2026(0.2321) Steps 898(903.71) | Grad Norm 2.6832(5.1727) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 30.7170, Epoch Time 621.9233(597.6390), Bit/dim 3.7338(best: 3.7309), Xent 1.1985, Loss 4.3330, Error 0.3707(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1717 | Time 92.0672(94.2416) | Bit/dim 3.7195(3.7363) | Xent 0.5582(0.6495) | Loss 3.9986(4.0610) | Error 0.2005(0.2311) Steps 910(903.89) | Grad Norm 2.7132(5.0990) | Total Time 14.00(14.00)\n",
      "Iter 1718 | Time 93.8764(94.2306) | Bit/dim 3.7347(3.7362) | Xent 0.5605(0.6469) | Loss 4.0150(4.0596) | Error 0.2041(0.2303) Steps 898(903.72) | Grad Norm 4.8128(5.0904) | Total Time 14.00(14.00)\n",
      "Iter 1719 | Time 95.8452(94.2791) | Bit/dim 3.7301(3.7360) | Xent 0.5828(0.6449) | Loss 4.0215(4.0585) | Error 0.2043(0.2295) Steps 910(903.91) | Grad Norm 4.5943(5.0755) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 96.8028(94.3548) | Bit/dim 3.7307(3.7359) | Xent 0.5670(0.6426) | Loss 4.0141(4.0572) | Error 0.2001(0.2286) Steps 874(903.01) | Grad Norm 3.5774(5.0305) | Total Time 14.00(14.00)\n",
      "Iter 1721 | Time 96.0341(94.4051) | Bit/dim 3.7268(3.7356) | Xent 0.5668(0.6403) | Loss 4.0102(4.0558) | Error 0.2000(0.2278) Steps 922(903.58) | Grad Norm 5.6968(5.0505) | Total Time 14.00(14.00)\n",
      "Iter 1722 | Time 96.6678(94.4730) | Bit/dim 3.7303(3.7354) | Xent 0.5420(0.6374) | Loss 4.0013(4.0541) | Error 0.1956(0.2268) Steps 904(903.59) | Grad Norm 6.5551(5.0957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 30.8088, Epoch Time 617.7892(598.2435), Bit/dim 3.7299(best: 3.7309), Xent 1.2590, Loss 4.3594, Error 0.3740(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1723 | Time 93.3901(94.4405) | Bit/dim 3.7189(3.7349) | Xent 0.5404(0.6345) | Loss 3.9891(4.0522) | Error 0.1931(0.2258) Steps 892(903.24) | Grad Norm 3.1000(5.0358) | Total Time 14.00(14.00)\n",
      "Iter 1724 | Time 95.6415(94.4766) | Bit/dim 3.7127(3.7343) | Xent 0.5316(0.6314) | Loss 3.9786(4.0500) | Error 0.1884(0.2247) Steps 904(903.27) | Grad Norm 2.8326(4.9697) | Total Time 14.00(14.00)\n",
      "Iter 1725 | Time 97.1821(94.5577) | Bit/dim 3.7268(3.7340) | Xent 0.5454(0.6288) | Loss 3.9995(4.0484) | Error 0.1929(0.2237) Steps 898(903.11) | Grad Norm 3.5188(4.9262) | Total Time 14.00(14.00)\n",
      "Iter 1726 | Time 91.9007(94.4780) | Bit/dim 3.7327(3.7340) | Xent 0.5148(0.6254) | Loss 3.9901(4.0467) | Error 0.1807(0.2224) Steps 910(903.32) | Grad Norm 2.3519(4.8490) | Total Time 14.00(14.00)\n",
      "Iter 1727 | Time 93.8475(94.4591) | Bit/dim 3.7268(3.7338) | Xent 0.5386(0.6228) | Loss 3.9961(4.0452) | Error 0.1910(0.2215) Steps 898(903.16) | Grad Norm 2.7290(4.7854) | Total Time 14.00(14.00)\n",
      "Iter 1728 | Time 91.4543(94.3690) | Bit/dim 3.7297(3.7337) | Xent 0.5428(0.6204) | Loss 4.0011(4.0439) | Error 0.1956(0.2207) Steps 922(903.72) | Grad Norm 5.1654(4.7968) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 30.3831, Epoch Time 609.4626(598.5801), Bit/dim 3.7316(best: 3.7299), Xent 1.3001, Loss 4.3816, Error 0.3771(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1729 | Time 92.5066(94.3131) | Bit/dim 3.7186(3.7332) | Xent 0.5559(0.6184) | Loss 3.9965(4.0424) | Error 0.1956(0.2200) Steps 904(903.73) | Grad Norm 7.6318(4.8818) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 98.5051(94.4389) | Bit/dim 3.7269(3.7330) | Xent 0.5254(0.6157) | Loss 3.9896(4.0409) | Error 0.1899(0.2191) Steps 916(904.10) | Grad Norm 6.3007(4.9244) | Total Time 14.00(14.00)\n",
      "Iter 1731 | Time 92.3769(94.3770) | Bit/dim 3.7231(3.7327) | Xent 0.5575(0.6139) | Loss 4.0019(4.0397) | Error 0.1993(0.2185) Steps 880(903.37) | Grad Norm 3.3204(4.8763) | Total Time 14.00(14.00)\n",
      "Iter 1732 | Time 95.9099(94.4230) | Bit/dim 3.7270(3.7326) | Xent 0.5202(0.6111) | Loss 3.9871(4.0381) | Error 0.1834(0.2174) Steps 916(903.75) | Grad Norm 4.8169(4.8745) | Total Time 14.00(14.00)\n",
      "Iter 1733 | Time 95.9318(94.4682) | Bit/dim 3.7096(3.7319) | Xent 0.5208(0.6084) | Loss 3.9700(4.0361) | Error 0.1823(0.2164) Steps 910(903.94) | Grad Norm 4.1773(4.8536) | Total Time 14.00(14.00)\n",
      "Iter 1734 | Time 92.4899(94.4089) | Bit/dim 3.7366(3.7320) | Xent 0.5233(0.6058) | Loss 3.9983(4.0349) | Error 0.1883(0.2155) Steps 898(903.76) | Grad Norm 4.0924(4.8307) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 31.1658, Epoch Time 618.6688(599.1828), Bit/dim 3.7283(best: 3.7299), Xent 1.2660, Loss 4.3613, Error 0.3763(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 95.6766(94.4469) | Bit/dim 3.7204(3.7317) | Xent 0.5171(0.6032) | Loss 3.9789(4.0332) | Error 0.1816(0.2145) Steps 928(904.49) | Grad Norm 4.1298(4.8097) | Total Time 14.00(14.00)\n",
      "Iter 1736 | Time 95.1183(94.4671) | Bit/dim 3.7156(3.7312) | Xent 0.5165(0.6006) | Loss 3.9738(4.0315) | Error 0.1813(0.2135) Steps 904(904.47) | Grad Norm 5.7029(4.8365) | Total Time 14.00(14.00)\n",
      "Iter 1737 | Time 94.2451(94.4604) | Bit/dim 3.7278(3.7311) | Xent 0.5070(0.5978) | Loss 3.9813(4.0300) | Error 0.1815(0.2125) Steps 928(905.18) | Grad Norm 3.6094(4.7997) | Total Time 14.00(14.00)\n",
      "Iter 1738 | Time 95.2042(94.4827) | Bit/dim 3.7353(3.7312) | Xent 0.4976(0.5948) | Loss 3.9841(4.0286) | Error 0.1756(0.2114) Steps 910(905.33) | Grad Norm 2.9047(4.7428) | Total Time 14.00(14.00)\n",
      "Iter 1739 | Time 96.1374(94.5324) | Bit/dim 3.7262(3.7311) | Xent 0.5087(0.5922) | Loss 3.9806(4.0271) | Error 0.1784(0.2104) Steps 928(906.01) | Grad Norm 4.6624(4.7404) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 96.4204(94.5890) | Bit/dim 3.7211(3.7308) | Xent 0.5103(0.5897) | Loss 3.9763(4.0256) | Error 0.1810(0.2096) Steps 922(906.49) | Grad Norm 3.3821(4.6997) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 30.8301, Epoch Time 619.3568(599.7880), Bit/dim 3.7305(best: 3.7283), Xent 1.2881, Loss 4.3746, Error 0.3742(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 94.7494(94.5938) | Bit/dim 3.7310(3.7308) | Xent 0.5242(0.5878) | Loss 3.9931(4.0246) | Error 0.1871(0.2089) Steps 904(906.41) | Grad Norm 3.6662(4.6687) | Total Time 14.00(14.00)\n",
      "Iter 1742 | Time 95.1112(94.6093) | Bit/dim 3.7160(3.7303) | Xent 0.5302(0.5860) | Loss 3.9811(4.0233) | Error 0.1873(0.2082) Steps 916(906.70) | Grad Norm 5.6122(4.6970) | Total Time 14.00(14.00)\n",
      "Iter 1743 | Time 94.4473(94.6045) | Bit/dim 3.7105(3.7297) | Xent 0.5161(0.5839) | Loss 3.9686(4.0217) | Error 0.1815(0.2074) Steps 898(906.44) | Grad Norm 6.7859(4.7596) | Total Time 14.00(14.00)\n",
      "Iter 1744 | Time 94.7458(94.6087) | Bit/dim 3.7232(3.7295) | Xent 0.4835(0.5809) | Loss 3.9650(4.0200) | Error 0.1715(0.2064) Steps 904(906.36) | Grad Norm 3.6549(4.7265) | Total Time 14.00(14.00)\n",
      "Iter 1745 | Time 98.3866(94.7220) | Bit/dim 3.7281(3.7295) | Xent 0.4892(0.5782) | Loss 3.9727(4.0186) | Error 0.1731(0.2054) Steps 928(907.01) | Grad Norm 3.1625(4.6796) | Total Time 14.00(14.00)\n",
      "Iter 1746 | Time 98.1173(94.8239) | Bit/dim 3.7175(3.7291) | Xent 0.5115(0.5762) | Loss 3.9733(4.0172) | Error 0.1821(0.2047) Steps 916(907.28) | Grad Norm 5.2674(4.6972) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 30.7882, Epoch Time 621.7746(600.4476), Bit/dim 3.7279(best: 3.7283), Xent 1.3154, Loss 4.3856, Error 0.3828(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 94.0049(94.7993) | Bit/dim 3.7258(3.7290) | Xent 0.5264(0.5747) | Loss 3.9890(4.0164) | Error 0.1839(0.2040) Steps 916(907.54) | Grad Norm 6.6359(4.7554) | Total Time 14.00(14.00)\n",
      "Iter 1748 | Time 96.2668(94.8434) | Bit/dim 3.7237(3.7289) | Xent 0.5187(0.5730) | Loss 3.9831(4.0154) | Error 0.1814(0.2034) Steps 910(907.62) | Grad Norm 6.4731(4.8069) | Total Time 14.00(14.00)\n",
      "Iter 1749 | Time 96.1985(94.8840) | Bit/dim 3.7218(3.7287) | Xent 0.5115(0.5711) | Loss 3.9776(4.0142) | Error 0.1809(0.2027) Steps 904(907.51) | Grad Norm 6.3150(4.8521) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 97.8661(94.9735) | Bit/dim 3.7282(3.7286) | Xent 0.5234(0.5697) | Loss 3.9899(4.0135) | Error 0.1874(0.2022) Steps 922(907.94) | Grad Norm 5.0416(4.8578) | Total Time 14.00(14.00)\n",
      "Iter 1751 | Time 93.1730(94.9195) | Bit/dim 3.6996(3.7278) | Xent 0.4854(0.5672) | Loss 3.9423(4.0114) | Error 0.1750(0.2014) Steps 910(908.01) | Grad Norm 2.2515(4.7796) | Total Time 14.00(14.00)\n",
      "Iter 1752 | Time 95.5888(94.9395) | Bit/dim 3.7227(3.7276) | Xent 0.4976(0.5651) | Loss 3.9715(4.0102) | Error 0.1744(0.2006) Steps 916(908.25) | Grad Norm 2.5626(4.7131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 30.6290, Epoch Time 619.1084(601.0074), Bit/dim 3.7236(best: 3.7279), Xent 1.2915, Loss 4.3694, Error 0.3747(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 92.9397(94.8795) | Bit/dim 3.7139(3.7272) | Xent 0.5010(0.5632) | Loss 3.9644(4.0088) | Error 0.1789(0.1999) Steps 904(908.12) | Grad Norm 3.7319(4.6837) | Total Time 14.00(14.00)\n",
      "Iter 1754 | Time 94.0625(94.8550) | Bit/dim 3.7209(3.7270) | Xent 0.5197(0.5619) | Loss 3.9808(4.0080) | Error 0.1861(0.1995) Steps 916(908.35) | Grad Norm 4.9069(4.6904) | Total Time 14.00(14.00)\n",
      "Iter 1755 | Time 94.6756(94.8497) | Bit/dim 3.7208(3.7268) | Xent 0.4960(0.5599) | Loss 3.9688(4.0068) | Error 0.1703(0.1987) Steps 910(908.40) | Grad Norm 5.4152(4.7121) | Total Time 14.00(14.00)\n",
      "Iter 1756 | Time 99.0829(94.9766) | Bit/dim 3.7093(3.7263) | Xent 0.5059(0.5583) | Loss 3.9623(4.0054) | Error 0.1813(0.1981) Steps 916(908.63) | Grad Norm 3.4824(4.6752) | Total Time 14.00(14.00)\n",
      "Iter 1757 | Time 98.3640(95.0783) | Bit/dim 3.7272(3.7263) | Xent 0.4687(0.5556) | Loss 3.9616(4.0041) | Error 0.1645(0.1971) Steps 916(908.85) | Grad Norm 2.1074(4.5982) | Total Time 14.00(14.00)\n",
      "Iter 1758 | Time 96.8041(95.1300) | Bit/dim 3.7242(3.7263) | Xent 0.4991(0.5539) | Loss 3.9738(4.0032) | Error 0.1746(0.1964) Steps 922(909.25) | Grad Norm 4.6051(4.5984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 31.0327, Epoch Time 622.2113(601.6435), Bit/dim 3.7297(best: 3.7236), Xent 1.3559, Loss 4.4077, Error 0.3789(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 99.3073(95.2554) | Bit/dim 3.7162(3.7260) | Xent 0.5041(0.5524) | Loss 3.9683(4.0022) | Error 0.1820(0.1960) Steps 904(909.09) | Grad Norm 6.6829(4.6609) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 98.0888(95.3404) | Bit/dim 3.7276(3.7260) | Xent 0.5748(0.5531) | Loss 4.0150(4.0026) | Error 0.2034(0.1962) Steps 910(909.12) | Grad Norm 7.5478(4.7475) | Total Time 14.00(14.00)\n",
      "Iter 1761 | Time 98.4788(95.4345) | Bit/dim 3.7170(3.7258) | Xent 0.5478(0.5529) | Loss 3.9910(4.0022) | Error 0.1950(0.1962) Steps 904(908.96) | Grad Norm 7.4057(4.8273) | Total Time 14.00(14.00)\n",
      "Iter 1762 | Time 97.1151(95.4849) | Bit/dim 3.7343(3.7260) | Xent 0.5145(0.5518) | Loss 3.9916(4.0019) | Error 0.1880(0.1960) Steps 898(908.64) | Grad Norm 5.5921(4.8502) | Total Time 14.00(14.00)\n",
      "Iter 1763 | Time 95.5580(95.4871) | Bit/dim 3.7117(3.7256) | Xent 0.5205(0.5508) | Loss 3.9719(4.0010) | Error 0.1901(0.1958) Steps 928(909.22) | Grad Norm 5.0851(4.8573) | Total Time 14.00(14.00)\n",
      "Iter 1764 | Time 95.9643(95.5014) | Bit/dim 3.7204(3.7254) | Xent 0.5319(0.5503) | Loss 3.9863(4.0006) | Error 0.1881(0.1955) Steps 904(909.06) | Grad Norm 6.0505(4.8931) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 30.5800, Epoch Time 630.9319(602.5222), Bit/dim 3.7263(best: 3.7236), Xent 1.3150, Loss 4.3838, Error 0.3751(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 94.8372(95.4815) | Bit/dim 3.7300(3.7256) | Xent 0.4706(0.5479) | Loss 3.9652(3.9995) | Error 0.1656(0.1946) Steps 916(909.27) | Grad Norm 4.9055(4.8934) | Total Time 14.00(14.00)\n",
      "Iter 1766 | Time 94.7511(95.4596) | Bit/dim 3.7229(3.7255) | Xent 0.5343(0.5475) | Loss 3.9901(3.9992) | Error 0.1857(0.1944) Steps 910(909.29) | Grad Norm 5.0978(4.8996) | Total Time 14.00(14.00)\n",
      "Iter 1767 | Time 97.0589(95.5076) | Bit/dim 3.7242(3.7254) | Xent 0.4885(0.5457) | Loss 3.9685(3.9983) | Error 0.1741(0.1938) Steps 916(909.49) | Grad Norm 3.2573(4.8503) | Total Time 14.00(14.00)\n",
      "Iter 1768 | Time 97.9407(95.5806) | Bit/dim 3.7208(3.7253) | Xent 0.4818(0.5438) | Loss 3.9617(3.9972) | Error 0.1720(0.1931) Steps 910(909.51) | Grad Norm 3.3228(4.8045) | Total Time 14.00(14.00)\n",
      "Iter 1769 | Time 94.7173(95.5547) | Bit/dim 3.7168(3.7251) | Xent 0.5063(0.5426) | Loss 3.9700(3.9964) | Error 0.1803(0.1927) Steps 934(910.24) | Grad Norm 3.6742(4.7706) | Total Time 14.00(14.00)\n",
      "Iter 1770 | Time 93.9521(95.5066) | Bit/dim 3.7081(3.7245) | Xent 0.4987(0.5413) | Loss 3.9574(3.9952) | Error 0.1754(0.1922) Steps 904(910.05) | Grad Norm 4.0831(4.7500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 31.3166, Epoch Time 620.1668(603.0515), Bit/dim 3.7284(best: 3.7236), Xent 1.3717, Loss 4.4143, Error 0.3815(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 101.9352(95.6995) | Bit/dim 3.7228(3.7245) | Xent 0.4863(0.5397) | Loss 3.9659(3.9943) | Error 0.1727(0.1916) Steps 916(910.23) | Grad Norm 5.1638(4.7624) | Total Time 14.00(14.00)\n",
      "Iter 1772 | Time 96.4164(95.7210) | Bit/dim 3.7239(3.7245) | Xent 0.5098(0.5388) | Loss 3.9788(3.9939) | Error 0.1841(0.1914) Steps 916(910.41) | Grad Norm 5.2076(4.7757) | Total Time 14.00(14.00)\n",
      "Iter 1773 | Time 98.7773(95.8127) | Bit/dim 3.7254(3.7245) | Xent 0.5146(0.5381) | Loss 3.9827(3.9935) | Error 0.1821(0.1911) Steps 904(910.21) | Grad Norm 8.2570(4.8802) | Total Time 14.00(14.00)\n",
      "Iter 1774 | Time 96.1566(95.8230) | Bit/dim 3.7163(3.7243) | Xent 0.5306(0.5378) | Loss 3.9816(3.9932) | Error 0.1895(0.1911) Steps 910(910.21) | Grad Norm 9.3123(5.0131) | Total Time 14.00(14.00)\n",
      "Iter 1775 | Time 93.8900(95.7650) | Bit/dim 3.7312(3.7245) | Xent 0.4770(0.5360) | Loss 3.9697(3.9925) | Error 0.1661(0.1903) Steps 910(910.20) | Grad Norm 6.9746(5.0720) | Total Time 14.00(14.00)\n",
      "Iter 1776 | Time 95.9460(95.7704) | Bit/dim 3.7081(3.7240) | Xent 0.5363(0.5360) | Loss 3.9763(3.9920) | Error 0.1863(0.1902) Steps 916(910.37) | Grad Norm 5.5068(5.0850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 31.1893, Epoch Time 629.5720(603.8471), Bit/dim 3.7289(best: 3.7236), Xent 1.3317, Loss 4.3947, Error 0.3823(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 95.6524(95.7669) | Bit/dim 3.7224(3.7239) | Xent 0.5137(0.5353) | Loss 3.9793(3.9916) | Error 0.1823(0.1900) Steps 910(910.36) | Grad Norm 3.5309(5.0384) | Total Time 14.00(14.00)\n",
      "Iter 1778 | Time 95.1207(95.7475) | Bit/dim 3.7120(3.7236) | Xent 0.5121(0.5346) | Loss 3.9680(3.9909) | Error 0.1834(0.1898) Steps 916(910.53) | Grad Norm 6.1770(5.0725) | Total Time 14.00(14.00)\n",
      "Iter 1779 | Time 93.0405(95.6663) | Bit/dim 3.7266(3.7237) | Xent 0.5288(0.5345) | Loss 3.9910(3.9909) | Error 0.1830(0.1896) Steps 904(910.34) | Grad Norm 6.5956(5.1182) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 95.2278(95.6531) | Bit/dim 3.7184(3.7235) | Xent 0.4845(0.5330) | Loss 3.9607(3.9900) | Error 0.1723(0.1890) Steps 904(910.15) | Grad Norm 4.2799(5.0931) | Total Time 14.00(14.00)\n",
      "Iter 1781 | Time 95.5332(95.6495) | Bit/dim 3.7200(3.7234) | Xent 0.5129(0.5324) | Loss 3.9764(3.9896) | Error 0.1805(0.1888) Steps 934(910.86) | Grad Norm 6.4070(5.1325) | Total Time 14.00(14.00)\n",
      "Iter 1782 | Time 99.2735(95.7582) | Bit/dim 3.7276(3.7235) | Xent 0.4740(0.5306) | Loss 3.9646(3.9888) | Error 0.1680(0.1882) Steps 928(911.38) | Grad Norm 4.1227(5.1022) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 30.8900, Epoch Time 620.2850(604.3403), Bit/dim 3.7297(best: 3.7236), Xent 1.3867, Loss 4.4230, Error 0.3831(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 94.0043(95.7056) | Bit/dim 3.7253(3.7236) | Xent 0.5091(0.5300) | Loss 3.9798(3.9886) | Error 0.1817(0.1880) Steps 922(911.69) | Grad Norm 9.9928(5.2489) | Total Time 14.00(14.00)\n",
      "Iter 1784 | Time 95.8121(95.7088) | Bit/dim 3.7142(3.7233) | Xent 0.5245(0.5298) | Loss 3.9764(3.9882) | Error 0.1880(0.1880) Steps 922(912.00) | Grad Norm 9.9843(5.3910) | Total Time 14.00(14.00)\n",
      "Iter 1785 | Time 97.2631(95.7555) | Bit/dim 3.7198(3.7232) | Xent 0.5488(0.5304) | Loss 3.9942(3.9884) | Error 0.1986(0.1883) Steps 898(911.58) | Grad Norm 10.2072(5.5355) | Total Time 14.00(14.00)\n",
      "Iter 1786 | Time 97.8772(95.8191) | Bit/dim 3.7213(3.7231) | Xent 0.5277(0.5303) | Loss 3.9852(3.9883) | Error 0.1884(0.1883) Steps 904(911.36) | Grad Norm 6.1450(5.5538) | Total Time 14.00(14.00)\n",
      "Iter 1787 | Time 97.7882(95.8782) | Bit/dim 3.7261(3.7232) | Xent 0.5052(0.5295) | Loss 3.9787(3.9880) | Error 0.1821(0.1881) Steps 916(911.50) | Grad Norm 4.2714(5.5153) | Total Time 14.00(14.00)\n",
      "Iter 1788 | Time 94.8575(95.8476) | Bit/dim 3.7198(3.7231) | Xent 0.5466(0.5301) | Loss 3.9931(3.9881) | Error 0.2023(0.1885) Steps 922(911.81) | Grad Norm 7.8627(5.5857) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 31.1980, Epoch Time 624.1782(604.9354), Bit/dim 3.7318(best: 3.7236), Xent 1.3157, Loss 4.3897, Error 0.3831(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 93.5221(95.7778) | Bit/dim 3.7373(3.7235) | Xent 0.5214(0.5298) | Loss 3.9980(3.9884) | Error 0.1847(0.1884) Steps 922(912.12) | Grad Norm 8.4509(5.6717) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 98.4618(95.8583) | Bit/dim 3.7174(3.7234) | Xent 0.5079(0.5291) | Loss 3.9714(3.9879) | Error 0.1810(0.1882) Steps 904(911.87) | Grad Norm 6.0981(5.6845) | Total Time 14.00(14.00)\n",
      "Iter 1791 | Time 97.8429(95.9178) | Bit/dim 3.7266(3.7235) | Xent 0.5012(0.5283) | Loss 3.9772(3.9876) | Error 0.1791(0.1879) Steps 898(911.46) | Grad Norm 7.7962(5.7478) | Total Time 14.00(14.00)\n",
      "Iter 1792 | Time 95.6086(95.9086) | Bit/dim 3.7241(3.7235) | Xent 0.5623(0.5293) | Loss 4.0053(3.9881) | Error 0.2009(0.1883) Steps 910(911.41) | Grad Norm 8.3929(5.8272) | Total Time 14.00(14.00)\n",
      "Iter 1793 | Time 98.0914(95.9741) | Bit/dim 3.7156(3.7232) | Xent 0.4801(0.5278) | Loss 3.9556(3.9872) | Error 0.1646(0.1876) Steps 922(911.73) | Grad Norm 3.9100(5.7697) | Total Time 14.00(14.00)\n",
      "Iter 1794 | Time 96.0096(95.9751) | Bit/dim 3.7187(3.7231) | Xent 0.5685(0.5291) | Loss 4.0029(3.9876) | Error 0.2046(0.1881) Steps 904(911.50) | Grad Norm 9.4368(5.8797) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 30.8649, Epoch Time 625.7732(605.5605), Bit/dim 3.7320(best: 3.7236), Xent 1.3442, Loss 4.4041, Error 0.3829(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 95.9493(95.9743) | Bit/dim 3.7257(3.7232) | Xent 0.5220(0.5289) | Loss 3.9867(3.9876) | Error 0.1835(0.1880) Steps 916(911.63) | Grad Norm 9.3445(5.9836) | Total Time 14.00(14.00)\n",
      "Iter 1796 | Time 95.1870(95.9507) | Bit/dim 3.7167(3.7230) | Xent 0.5098(0.5283) | Loss 3.9716(3.9871) | Error 0.1821(0.1878) Steps 928(912.12) | Grad Norm 4.5350(5.9402) | Total Time 14.00(14.00)\n",
      "Iter 1797 | Time 94.9364(95.9203) | Bit/dim 3.7214(3.7229) | Xent 0.5307(0.5284) | Loss 3.9867(3.9871) | Error 0.1896(0.1879) Steps 910(912.06) | Grad Norm 7.7382(5.9941) | Total Time 14.00(14.00)\n",
      "Iter 1798 | Time 95.8236(95.9174) | Bit/dim 3.7318(3.7232) | Xent 0.6140(0.5309) | Loss 4.0388(3.9887) | Error 0.2185(0.1888) Steps 892(911.46) | Grad Norm 10.7517(6.1368) | Total Time 14.00(14.00)\n",
      "Iter 1799 | Time 97.6455(95.9692) | Bit/dim 3.7285(3.7234) | Xent 0.5108(0.5303) | Loss 3.9839(3.9885) | Error 0.1799(0.1885) Steps 922(911.78) | Grad Norm 5.9625(6.1316) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 92.9207(95.8778) | Bit/dim 3.7253(3.7234) | Xent 0.5812(0.5318) | Loss 4.0159(3.9893) | Error 0.2069(0.1891) Steps 916(911.90) | Grad Norm 10.8308(6.2726) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 31.1537, Epoch Time 619.1768(605.9690), Bit/dim 3.7283(best: 3.7236), Xent 1.2830, Loss 4.3698, Error 0.3762(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 95.8614(95.8773) | Bit/dim 3.7224(3.7234) | Xent 0.4889(0.5306) | Loss 3.9669(3.9887) | Error 0.1716(0.1885) Steps 910(911.85) | Grad Norm 5.9579(6.2631) | Total Time 14.00(14.00)\n",
      "Iter 1802 | Time 97.2752(95.9192) | Bit/dim 3.7200(3.7233) | Xent 0.6497(0.5341) | Loss 4.0449(3.9904) | Error 0.2249(0.1896) Steps 916(911.97) | Grad Norm 12.7013(6.4563) | Total Time 14.00(14.00)\n",
      "Iter 1803 | Time 93.3389(95.8418) | Bit/dim 3.7381(3.7237) | Xent 0.7099(0.5394) | Loss 4.0930(3.9934) | Error 0.2400(0.1911) Steps 910(911.91) | Grad Norm 15.6716(6.7327) | Total Time 14.00(14.00)\n",
      "Iter 1804 | Time 95.8890(95.8432) | Bit/dim 3.7493(3.7245) | Xent 0.7256(0.5450) | Loss 4.1120(3.9970) | Error 0.2556(0.1931) Steps 910(911.85) | Grad Norm 16.9092(7.0380) | Total Time 14.00(14.00)\n",
      "Iter 1805 | Time 97.6625(95.8978) | Bit/dim 3.7348(3.7248) | Xent 0.6510(0.5482) | Loss 4.0603(3.9989) | Error 0.2271(0.1941) Steps 922(912.16) | Grad Norm 9.7208(7.1185) | Total Time 14.00(14.00)\n",
      "Iter 1806 | Time 88.4073(95.6731) | Bit/dim 3.7390(3.7252) | Xent 0.6597(0.5515) | Loss 4.0689(4.0010) | Error 0.2289(0.1951) Steps 910(912.09) | Grad Norm 10.1911(7.2107) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 30.6117, Epoch Time 614.5403(606.2262), Bit/dim 3.7411(best: 3.7236), Xent 1.2574, Loss 4.3698, Error 0.3848(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 94.9537(95.6515) | Bit/dim 3.7376(3.7256) | Xent 0.5661(0.5520) | Loss 4.0207(4.0016) | Error 0.1984(0.1952) Steps 892(911.49) | Grad Norm 6.5542(7.1910) | Total Time 14.00(14.00)\n",
      "Iter 1808 | Time 93.5253(95.5877) | Bit/dim 3.7354(3.7259) | Xent 0.6430(0.5547) | Loss 4.0569(4.0032) | Error 0.2305(0.1963) Steps 946(912.53) | Grad Norm 8.8526(7.2408) | Total Time 14.00(14.00)\n",
      "Iter 1809 | Time 96.1048(95.6032) | Bit/dim 3.7307(3.7260) | Xent 0.6247(0.5568) | Loss 4.0431(4.0044) | Error 0.2234(0.1971) Steps 922(912.81) | Grad Norm 9.5880(7.3113) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 98.7738(95.6984) | Bit/dim 3.7251(3.7260) | Xent 0.5738(0.5573) | Loss 4.0120(4.0047) | Error 0.2026(0.1973) Steps 922(913.09) | Grad Norm 7.6316(7.3209) | Total Time 14.00(14.00)\n",
      "Iter 1811 | Time 99.3930(95.8092) | Bit/dim 3.7371(3.7264) | Xent 0.5644(0.5575) | Loss 4.0193(4.0051) | Error 0.2031(0.1974) Steps 910(912.99) | Grad Norm 7.8324(7.3362) | Total Time 14.00(14.00)\n",
      "Iter 1812 | Time 97.7603(95.8677) | Bit/dim 3.7392(3.7267) | Xent 0.5670(0.5578) | Loss 4.0227(4.0056) | Error 0.2007(0.1975) Steps 910(912.90) | Grad Norm 6.7747(7.3194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 31.1459, Epoch Time 627.4321(606.8623), Bit/dim 3.7417(best: 3.7236), Xent 1.2949, Loss 4.3892, Error 0.3837(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 94.0940(95.8145) | Bit/dim 3.7352(3.7270) | Xent 0.5443(0.5574) | Loss 4.0074(4.0057) | Error 0.1936(0.1974) Steps 904(912.64) | Grad Norm 7.1560(7.3145) | Total Time 14.00(14.00)\n",
      "Iter 1814 | Time 95.2157(95.7966) | Bit/dim 3.7322(3.7271) | Xent 0.5316(0.5566) | Loss 3.9980(4.0055) | Error 0.1910(0.1972) Steps 898(912.20) | Grad Norm 4.1571(7.2197) | Total Time 14.00(14.00)\n",
      "Iter 1815 | Time 98.3813(95.8741) | Bit/dim 3.7288(3.7272) | Xent 0.5509(0.5564) | Loss 4.0042(4.0054) | Error 0.1959(0.1972) Steps 928(912.67) | Grad Norm 6.6374(7.2023) | Total Time 14.00(14.00)\n",
      "Iter 1816 | Time 97.1021(95.9109) | Bit/dim 3.7255(3.7271) | Xent 0.5176(0.5553) | Loss 3.9843(4.0048) | Error 0.1855(0.1968) Steps 904(912.41) | Grad Norm 4.2416(7.1135) | Total Time 14.00(14.00)\n",
      "Iter 1817 | Time 96.3059(95.9228) | Bit/dim 3.7303(3.7272) | Xent 0.5150(0.5541) | Loss 3.9878(4.0043) | Error 0.1814(0.1964) Steps 898(911.98) | Grad Norm 6.0661(7.0820) | Total Time 14.00(14.00)\n",
      "Iter 1818 | Time 98.4660(95.9991) | Bit/dim 3.7288(3.7273) | Xent 0.4842(0.5520) | Loss 3.9710(4.0033) | Error 0.1747(0.1957) Steps 916(912.10) | Grad Norm 3.4883(6.9742) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 31.2143, Epoch Time 626.4226(607.4492), Bit/dim 3.7313(best: 3.7236), Xent 1.3780, Loss 4.4203, Error 0.3833(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 91.0175(95.8496) | Bit/dim 3.7184(3.7270) | Xent 0.4836(0.5499) | Loss 3.9602(4.0020) | Error 0.1687(0.1949) Steps 904(911.86) | Grad Norm 4.1091(6.8883) | Total Time 14.00(14.00)\n",
      "Iter 1820 | Time 94.5626(95.8110) | Bit/dim 3.7362(3.7273) | Xent 0.4963(0.5483) | Loss 3.9843(4.0015) | Error 0.1739(0.1943) Steps 886(911.08) | Grad Norm 4.6090(6.8199) | Total Time 14.00(14.00)\n",
      "Iter 1821 | Time 100.9981(95.9666) | Bit/dim 3.7200(3.7271) | Xent 0.5007(0.5469) | Loss 3.9704(4.0005) | Error 0.1740(0.1937) Steps 916(911.23) | Grad Norm 5.0091(6.7656) | Total Time 14.00(14.00)\n",
      "Iter 1822 | Time 94.9443(95.9360) | Bit/dim 3.7330(3.7273) | Xent 0.4941(0.5453) | Loss 3.9800(3.9999) | Error 0.1704(0.1930) Steps 904(911.01) | Grad Norm 4.0110(6.6829) | Total Time 14.00(14.00)\n",
      "Iter 1823 | Time 96.6895(95.9586) | Bit/dim 3.7190(3.7270) | Xent 0.4794(0.5433) | Loss 3.9587(3.9987) | Error 0.1723(0.1924) Steps 928(911.52) | Grad Norm 4.6329(6.6214) | Total Time 14.00(14.00)\n",
      "Iter 1824 | Time 94.4188(95.9124) | Bit/dim 3.7265(3.7270) | Xent 0.4596(0.5408) | Loss 3.9563(3.9974) | Error 0.1679(0.1916) Steps 910(911.48) | Grad Norm 5.4139(6.5852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 31.0751, Epoch Time 619.2140(607.8021), Bit/dim 3.7320(best: 3.7236), Xent 1.3322, Loss 4.3981, Error 0.3785(best: 0.3652)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 91.5546(95.7816) | Bit/dim 3.7307(3.7271) | Xent 0.4414(0.5378) | Loss 3.9513(3.9960) | Error 0.1580(0.1906) Steps 904(911.25) | Grad Norm 2.7469(6.4701) | Total Time 14.00(14.00)\n",
      "Iter 1826 | Time 97.8408(95.8434) | Bit/dim 3.7241(3.7270) | Xent 0.4616(0.5355) | Loss 3.9549(3.9948) | Error 0.1634(0.1898) Steps 916(911.39) | Grad Norm 6.0861(6.4585) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_lr_0_01_run1_post --load_dir ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_lr_0_01_run1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-5  --rtol 1e-5 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
