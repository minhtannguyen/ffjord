{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run3/current_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run3', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 1687 | Time 117.6911(62.9369) | Bit/dim 3.6734(3.6713) | Xent 0.9414(0.9455) | Loss 11.1046(9.6587) | Error 0.3363(0.3380) Steps 640(624.79) | Grad Norm 8.2685(6.7023) | Total Time 0.00(0.00)\n",
      "Iter 1688 | Time 57.4954(62.7737) | Bit/dim 3.6717(3.6713) | Xent 0.9777(0.9465) | Loss 9.1910(9.6446) | Error 0.3498(0.3383) Steps 640(625.24) | Grad Norm 13.4140(6.9036) | Total Time 0.00(0.00)\n",
      "Iter 1689 | Time 61.2900(62.7292) | Bit/dim 3.6589(3.6710) | Xent 0.9487(0.9465) | Loss 9.2016(9.6313) | Error 0.3403(0.3384) Steps 640(625.68) | Grad Norm 4.7659(6.8395) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 61.2568(62.6850) | Bit/dim 3.6664(3.6708) | Xent 0.9770(0.9474) | Loss 9.1353(9.6165) | Error 0.3484(0.3387) Steps 664(626.83) | Grad Norm 10.5837(6.9518) | Total Time 0.00(0.00)\n",
      "Iter 1691 | Time 58.0339(62.5455) | Bit/dim 3.6777(3.6710) | Xent 0.9463(0.9474) | Loss 9.2103(9.6043) | Error 0.3367(0.3386) Steps 628(626.87) | Grad Norm 6.3200(6.9328) | Total Time 0.00(0.00)\n",
      "Iter 1692 | Time 55.5348(62.3352) | Bit/dim 3.6741(3.6711) | Xent 0.9428(0.9473) | Loss 9.0388(9.5873) | Error 0.3323(0.3384) Steps 610(626.36) | Grad Norm 8.8785(6.9912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 36.7810, Epoch Time 464.4575(406.8349), Bit/dim 3.6748(best: inf), Xent 0.9635, Loss 4.1565, Error 0.3405(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1693 | Time 71.5713(62.6122) | Bit/dim 3.6651(3.6709) | Xent 0.9723(0.9480) | Loss 12.1278(9.6635) | Error 0.3484(0.3387) Steps 610(625.87) | Grad Norm 8.2060(7.0277) | Total Time 0.00(0.00)\n",
      "Iter 1694 | Time 64.8416(62.6791) | Bit/dim 3.6673(3.6708) | Xent 0.9387(0.9477) | Loss 9.0549(9.6453) | Error 0.3370(0.3387) Steps 610(625.40) | Grad Norm 5.2968(6.9757) | Total Time 0.00(0.00)\n",
      "Iter 1695 | Time 58.8746(62.5650) | Bit/dim 3.6737(3.6709) | Xent 0.9301(0.9472) | Loss 9.1122(9.6293) | Error 0.3333(0.3385) Steps 634(625.65) | Grad Norm 7.4581(6.9902) | Total Time 0.00(0.00)\n",
      "Iter 1696 | Time 62.3518(62.5586) | Bit/dim 3.6733(3.6710) | Xent 0.9142(0.9462) | Loss 9.2324(9.6174) | Error 0.3317(0.3383) Steps 604(625.00) | Grad Norm 3.8975(6.8974) | Total Time 0.00(0.00)\n",
      "Iter 1697 | Time 63.9030(62.5989) | Bit/dim 3.6805(3.6713) | Xent 0.9365(0.9459) | Loss 8.6685(9.5889) | Error 0.3350(0.3382) Steps 604(624.37) | Grad Norm 6.6787(6.8909) | Total Time 0.00(0.00)\n",
      "Iter 1698 | Time 57.1486(62.4354) | Bit/dim 3.6739(3.6714) | Xent 0.9491(0.9460) | Loss 8.9355(9.5693) | Error 0.3417(0.3383) Steps 604(623.76) | Grad Norm 5.0541(6.8358) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 22.7227, Epoch Time 418.0645(407.1718), Bit/dim 3.6762(best: 3.6748), Xent 0.9273, Loss 4.1398, Error 0.3334(best: 0.3405)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1699 | Time 67.7504(62.5949) | Bit/dim 3.6656(3.6712) | Xent 0.9158(0.9451) | Loss 12.5112(9.6576) | Error 0.3316(0.3381) Steps 646(624.43) | Grad Norm 6.8815(6.8371) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 54.9716(62.3662) | Bit/dim 3.6721(3.6712) | Xent 0.9208(0.9444) | Loss 9.1060(9.6410) | Error 0.3264(0.3378) Steps 610(624.00) | Grad Norm 4.3850(6.7636) | Total Time 0.00(0.00)\n",
      "Iter 1701 | Time 62.8204(62.3798) | Bit/dim 3.6671(3.6711) | Xent 0.9258(0.9438) | Loss 9.0105(9.6221) | Error 0.3296(0.3375) Steps 634(624.30) | Grad Norm 5.0081(6.7109) | Total Time 0.00(0.00)\n",
      "Iter 1702 | Time 58.1043(62.2515) | Bit/dim 3.6701(3.6711) | Xent 0.9532(0.9441) | Loss 8.7971(9.5973) | Error 0.3525(0.3380) Steps 640(624.77) | Grad Norm 3.9364(6.6277) | Total Time 0.00(0.00)\n",
      "Iter 1703 | Time 61.2581(62.2217) | Bit/dim 3.6659(3.6709) | Xent 0.9305(0.9437) | Loss 9.1060(9.5826) | Error 0.3333(0.3378) Steps 634(625.05) | Grad Norm 5.5741(6.5961) | Total Time 0.00(0.00)\n",
      "Iter 1704 | Time 64.5511(62.2916) | Bit/dim 3.6743(3.6710) | Xent 0.9141(0.9428) | Loss 9.2975(9.5741) | Error 0.3239(0.3374) Steps 616(624.77) | Grad Norm 3.5659(6.5052) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 23.0405, Epoch Time 408.7658(407.2196), Bit/dim 3.6712(best: 3.6748), Xent 0.9334, Loss 4.1380, Error 0.3325(best: 0.3334)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1705 | Time 61.2300(62.2598) | Bit/dim 3.6776(3.6712) | Xent 0.9145(0.9420) | Loss 12.5134(9.6622) | Error 0.3244(0.3370) Steps 652(625.59) | Grad Norm 4.8585(6.4558) | Total Time 0.00(0.00)\n",
      "Iter 1706 | Time 57.1855(62.1075) | Bit/dim 3.6592(3.6708) | Xent 0.9101(0.9410) | Loss 8.7177(9.6339) | Error 0.3184(0.3364) Steps 628(625.66) | Grad Norm 2.5231(6.3378) | Total Time 0.00(0.00)\n",
      "Iter 1707 | Time 63.5571(62.1510) | Bit/dim 3.6733(3.6709) | Xent 0.9470(0.9412) | Loss 9.1596(9.6197) | Error 0.3377(0.3365) Steps 658(626.63) | Grad Norm 5.6211(6.3163) | Total Time 0.00(0.00)\n",
      "Iter 1708 | Time 65.0690(62.2386) | Bit/dim 3.6600(3.6706) | Xent 0.9280(0.9408) | Loss 9.2300(9.6080) | Error 0.3317(0.3363) Steps 616(626.31) | Grad Norm 6.0451(6.3081) | Total Time 0.00(0.00)\n",
      "Iter 1709 | Time 58.2047(62.1175) | Bit/dim 3.6729(3.6707) | Xent 0.9495(0.9411) | Loss 9.1066(9.5929) | Error 0.3406(0.3365) Steps 622(626.18) | Grad Norm 10.8072(6.4431) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 63.6010(62.1620) | Bit/dim 3.6721(3.6707) | Xent 0.9361(0.9409) | Loss 9.2982(9.5841) | Error 0.3327(0.3364) Steps 652(626.96) | Grad Norm 8.6007(6.5078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 23.4033, Epoch Time 408.5504(407.2596), Bit/dim 3.6745(best: 3.6712), Xent 0.9430, Loss 4.1460, Error 0.3374(best: 0.3325)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1711 | Time 56.7601(62.0000) | Bit/dim 3.6604(3.6704) | Xent 0.9293(0.9406) | Loss 12.3515(9.6671) | Error 0.3335(0.3363) Steps 622(626.81) | Grad Norm 6.1295(6.4965) | Total Time 0.00(0.00)\n",
      "Iter 1712 | Time 56.0641(61.8219) | Bit/dim 3.6766(3.6706) | Xent 0.9207(0.9400) | Loss 9.0794(9.6495) | Error 0.3295(0.3361) Steps 628(626.85) | Grad Norm 6.5963(6.4995) | Total Time 0.00(0.00)\n",
      "Iter 1713 | Time 59.3501(61.7478) | Bit/dim 3.6737(3.6707) | Xent 0.9329(0.9398) | Loss 9.0404(9.6312) | Error 0.3323(0.3360) Steps 622(626.70) | Grad Norm 2.9358(6.3926) | Total Time 0.00(0.00)\n",
      "Iter 1714 | Time 57.5420(61.6216) | Bit/dim 3.6649(3.6705) | Xent 0.9313(0.9395) | Loss 9.0463(9.6137) | Error 0.3344(0.3359) Steps 634(626.92) | Grad Norm 4.7485(6.3433) | Total Time 0.00(0.00)\n",
      "Iter 1715 | Time 61.6536(61.6225) | Bit/dim 3.6642(3.6703) | Xent 0.9032(0.9384) | Loss 9.1875(9.6009) | Error 0.3265(0.3356) Steps 652(627.67) | Grad Norm 3.9625(6.2718) | Total Time 0.00(0.00)\n",
      "Iter 1716 | Time 61.5875(61.6215) | Bit/dim 3.6706(3.6703) | Xent 0.9280(0.9381) | Loss 8.9082(9.5801) | Error 0.3311(0.3355) Steps 616(627.32) | Grad Norm 4.7810(6.2271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 22.9675, Epoch Time 392.6895(406.8225), Bit/dim 3.6748(best: 3.6712), Xent 0.9217, Loss 4.1356, Error 0.3298(best: 0.3325)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1717 | Time 58.8925(61.5396) | Bit/dim 3.6661(3.6702) | Xent 0.9256(0.9377) | Loss 12.0932(9.6555) | Error 0.3290(0.3353) Steps 640(627.70) | Grad Norm 4.8589(6.1861) | Total Time 0.00(0.00)\n",
      "Iter 1718 | Time 61.0123(61.5238) | Bit/dim 3.6766(3.6704) | Xent 0.9227(0.9373) | Loss 8.9862(9.6354) | Error 0.3325(0.3352) Steps 628(627.71) | Grad Norm 5.6401(6.1697) | Total Time 0.00(0.00)\n",
      "Iter 1719 | Time 56.7576(61.3808) | Bit/dim 3.6733(3.6705) | Xent 0.9013(0.9362) | Loss 9.2112(9.6227) | Error 0.3240(0.3349) Steps 658(628.62) | Grad Norm 3.8037(6.0987) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 60.2415(61.3466) | Bit/dim 3.6728(3.6705) | Xent 0.9070(0.9353) | Loss 9.0561(9.6057) | Error 0.3249(0.3346) Steps 640(628.96) | Grad Norm 4.2543(6.0434) | Total Time 0.00(0.00)\n",
      "Iter 1721 | Time 60.3583(61.3170) | Bit/dim 3.6585(3.6702) | Xent 0.9081(0.9345) | Loss 9.0304(9.5884) | Error 0.3199(0.3341) Steps 634(629.11) | Grad Norm 3.0091(5.9523) | Total Time 0.00(0.00)\n",
      "Iter 1722 | Time 59.5677(61.2645) | Bit/dim 3.6710(3.6702) | Xent 0.9170(0.9340) | Loss 9.1079(9.5740) | Error 0.3243(0.3338) Steps 634(629.26) | Grad Norm 4.1016(5.8968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 23.0633, Epoch Time 396.1292(406.5017), Bit/dim 3.6727(best: 3.6712), Xent 0.9204, Loss 4.1329, Error 0.3251(best: 0.3298)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1723 | Time 59.9295(61.2245) | Bit/dim 3.6673(3.6701) | Xent 0.9007(0.9330) | Loss 11.9797(9.6462) | Error 0.3204(0.3334) Steps 592(628.14) | Grad Norm 3.8187(5.8345) | Total Time 0.00(0.00)\n",
      "Iter 1724 | Time 58.3706(61.1388) | Bit/dim 3.6668(3.6700) | Xent 0.9234(0.9327) | Loss 9.0595(9.6286) | Error 0.3276(0.3333) Steps 598(627.24) | Grad Norm 6.6509(5.8590) | Total Time 0.00(0.00)\n",
      "Iter 1725 | Time 63.7119(61.2160) | Bit/dim 3.6737(3.6701) | Xent 0.9412(0.9329) | Loss 9.1868(9.6153) | Error 0.3379(0.3334) Steps 622(627.08) | Grad Norm 9.3457(5.9636) | Total Time 0.00(0.00)\n",
      "Iter 1726 | Time 60.9127(61.2069) | Bit/dim 3.6661(3.6700) | Xent 0.9516(0.9335) | Loss 9.2666(9.6049) | Error 0.3384(0.3336) Steps 640(627.47) | Grad Norm 7.6842(6.0152) | Total Time 0.00(0.00)\n",
      "Iter 1727 | Time 60.7045(61.1919) | Bit/dim 3.6583(3.6697) | Xent 0.9481(0.9339) | Loss 8.9432(9.5850) | Error 0.3386(0.3337) Steps 634(627.66) | Grad Norm 11.8547(6.1904) | Total Time 0.00(0.00)\n",
      "Iter 1728 | Time 58.9592(61.1249) | Bit/dim 3.6796(3.6700) | Xent 0.9240(0.9336) | Loss 9.0010(9.5675) | Error 0.3337(0.3337) Steps 646(628.21) | Grad Norm 11.9172(6.3622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 22.8435, Epoch Time 401.6099(406.3549), Bit/dim 3.6694(best: 3.6712), Xent 0.9375, Loss 4.1382, Error 0.3336(best: 0.3251)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1729 | Time 54.6027(60.9292) | Bit/dim 3.6681(3.6699) | Xent 0.9316(0.9336) | Loss 12.0079(9.6407) | Error 0.3370(0.3338) Steps 604(627.49) | Grad Norm 5.6141(6.3397) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 58.2167(60.8478) | Bit/dim 3.6690(3.6699) | Xent 0.9211(0.9332) | Loss 9.2195(9.6281) | Error 0.3276(0.3336) Steps 598(626.60) | Grad Norm 5.6461(6.3189) | Total Time 0.00(0.00)\n",
      "Iter 1731 | Time 64.7915(60.9662) | Bit/dim 3.6612(3.6696) | Xent 0.9069(0.9324) | Loss 9.1421(9.6135) | Error 0.3221(0.3333) Steps 616(626.28) | Grad Norm 3.5614(6.2362) | Total Time 0.00(0.00)\n",
      "Iter 1732 | Time 58.5699(60.8943) | Bit/dim 3.6755(3.6698) | Xent 0.8964(0.9313) | Loss 9.1080(9.5983) | Error 0.3185(0.3328) Steps 598(625.44) | Grad Norm 3.3251(6.1489) | Total Time 0.00(0.00)\n",
      "Iter 1733 | Time 60.8369(60.8925) | Bit/dim 3.6676(3.6697) | Xent 0.9303(0.9313) | Loss 9.1977(9.5863) | Error 0.3304(0.3328) Steps 586(624.25) | Grad Norm 4.8839(6.1109) | Total Time 0.00(0.00)\n",
      "Iter 1734 | Time 56.2986(60.7547) | Bit/dim 3.6622(3.6695) | Xent 0.9192(0.9309) | Loss 9.1639(9.5736) | Error 0.3279(0.3326) Steps 616(624.01) | Grad Norm 4.8417(6.0728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 23.2277, Epoch Time 392.5004(405.9393), Bit/dim 3.6676(best: 3.6694), Xent 0.9278, Loss 4.1315, Error 0.3343(best: 0.3251)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 60.1354(60.7362) | Bit/dim 3.6709(3.6695) | Xent 0.9211(0.9306) | Loss 12.1884(9.6521) | Error 0.3336(0.3326) Steps 628(624.13) | Grad Norm 6.0399(6.0719) | Total Time 0.00(0.00)\n",
      "Iter 1736 | Time 64.0004(60.8341) | Bit/dim 3.6736(3.6697) | Xent 0.9152(0.9302) | Loss 9.1401(9.6367) | Error 0.3251(0.3324) Steps 652(624.96) | Grad Norm 5.3375(6.0498) | Total Time 0.00(0.00)\n",
      "Iter 1737 | Time 61.1838(60.8446) | Bit/dim 3.6582(3.6693) | Xent 0.8922(0.9290) | Loss 9.0869(9.6202) | Error 0.3205(0.3321) Steps 610(624.51) | Grad Norm 4.3061(5.9975) | Total Time 0.00(0.00)\n",
      "Iter 1738 | Time 57.0318(60.7302) | Bit/dim 3.6663(3.6692) | Xent 0.9092(0.9284) | Loss 9.1229(9.6053) | Error 0.3250(0.3318) Steps 628(624.62) | Grad Norm 3.8952(5.9344) | Total Time 0.00(0.00)\n",
      "Iter 1739 | Time 60.6814(60.7287) | Bit/dim 3.6638(3.6691) | Xent 0.9243(0.9283) | Loss 9.2672(9.5952) | Error 0.3235(0.3316) Steps 646(625.26) | Grad Norm 6.9543(5.9650) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 58.5647(60.6638) | Bit/dim 3.6761(3.6693) | Xent 0.9177(0.9280) | Loss 9.0559(9.5790) | Error 0.3300(0.3315) Steps 616(624.98) | Grad Norm 6.1666(5.9711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 23.1430, Epoch Time 400.9793(405.7905), Bit/dim 3.6679(best: 3.6676), Xent 0.9128, Loss 4.1243, Error 0.3242(best: 0.3251)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 62.7010(60.7249) | Bit/dim 3.6726(3.6694) | Xent 0.9062(0.9274) | Loss 12.2823(9.6601) | Error 0.3266(0.3314) Steps 646(625.61) | Grad Norm 3.5631(5.8988) | Total Time 0.00(0.00)\n",
      "Iter 1742 | Time 60.7749(60.7264) | Bit/dim 3.6726(3.6695) | Xent 0.9210(0.9272) | Loss 9.2301(9.6472) | Error 0.3319(0.3314) Steps 652(626.40) | Grad Norm 5.5633(5.8888) | Total Time 0.00(0.00)\n",
      "Iter 1743 | Time 58.4020(60.6567) | Bit/dim 3.6653(3.6694) | Xent 0.9177(0.9269) | Loss 9.1848(9.6333) | Error 0.3317(0.3314) Steps 622(626.27) | Grad Norm 4.5255(5.8479) | Total Time 0.00(0.00)\n",
      "Iter 1744 | Time 56.8385(60.5421) | Bit/dim 3.6619(3.6691) | Xent 0.8992(0.9260) | Loss 8.9162(9.6118) | Error 0.3217(0.3311) Steps 634(626.50) | Grad Norm 3.8699(5.7885) | Total Time 0.00(0.00)\n",
      "Iter 1745 | Time 59.1208(60.4995) | Bit/dim 3.6625(3.6689) | Xent 0.9025(0.9253) | Loss 9.1050(9.5966) | Error 0.3245(0.3309) Steps 628(626.55) | Grad Norm 4.3835(5.7464) | Total Time 0.00(0.00)\n",
      "Iter 1746 | Time 58.0395(60.4257) | Bit/dim 3.6677(3.6689) | Xent 0.9125(0.9250) | Loss 9.2393(9.5859) | Error 0.3234(0.3307) Steps 610(626.05) | Grad Norm 7.0683(5.7860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 23.4132, Epoch Time 395.4918(405.4815), Bit/dim 3.6717(best: 3.6676), Xent 0.9232, Loss 4.1333, Error 0.3262(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 61.8116(60.4673) | Bit/dim 3.6666(3.6688) | Xent 0.9177(0.9247) | Loss 12.3671(9.6693) | Error 0.3287(0.3306) Steps 592(625.03) | Grad Norm 8.0934(5.8553) | Total Time 0.00(0.00)\n",
      "Iter 1748 | Time 61.8929(60.5100) | Bit/dim 3.6708(3.6689) | Xent 0.8859(0.9236) | Loss 9.0594(9.6510) | Error 0.3151(0.3302) Steps 670(626.38) | Grad Norm 4.6237(5.8183) | Total Time 0.00(0.00)\n",
      "Iter 1749 | Time 64.5530(60.6313) | Bit/dim 3.6706(3.6689) | Xent 0.8986(0.9228) | Loss 9.2594(9.6393) | Error 0.3194(0.3299) Steps 640(626.79) | Grad Norm 6.2473(5.8312) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 58.4157(60.5649) | Bit/dim 3.6595(3.6686) | Xent 0.9134(0.9225) | Loss 9.1639(9.6250) | Error 0.3233(0.3297) Steps 628(626.82) | Grad Norm 8.8267(5.9211) | Total Time 0.00(0.00)\n",
      "Iter 1751 | Time 64.0290(60.6688) | Bit/dim 3.6589(3.6684) | Xent 0.9283(0.9227) | Loss 9.0797(9.6087) | Error 0.3295(0.3297) Steps 646(627.40) | Grad Norm 6.5343(5.9394) | Total Time 0.00(0.00)\n",
      "Iter 1752 | Time 58.9450(60.6171) | Bit/dim 3.6708(3.6684) | Xent 0.9023(0.9221) | Loss 9.2347(9.5974) | Error 0.3267(0.3296) Steps 634(627.60) | Grad Norm 2.6048(5.8394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 23.5424, Epoch Time 409.4124(405.5994), Bit/dim 3.6688(best: 3.6676), Xent 0.9176, Loss 4.1276, Error 0.3280(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 61.7006(60.6496) | Bit/dim 3.6713(3.6685) | Xent 0.9244(0.9222) | Loss 12.3624(9.6804) | Error 0.3314(0.3296) Steps 634(627.79) | Grad Norm 6.5797(5.8616) | Total Time 0.00(0.00)\n",
      "Iter 1754 | Time 57.8576(60.5658) | Bit/dim 3.6625(3.6683) | Xent 0.8901(0.9212) | Loss 9.0216(9.6606) | Error 0.3213(0.3294) Steps 628(627.80) | Grad Norm 7.3608(5.9066) | Total Time 0.00(0.00)\n",
      "Iter 1755 | Time 60.4295(60.5617) | Bit/dim 3.6732(3.6685) | Xent 0.9046(0.9207) | Loss 9.0934(9.6436) | Error 0.3201(0.3291) Steps 646(628.34) | Grad Norm 4.6574(5.8691) | Total Time 0.00(0.00)\n",
      "Iter 1756 | Time 58.6301(60.5038) | Bit/dim 3.6700(3.6685) | Xent 0.9189(0.9207) | Loss 9.1807(9.6297) | Error 0.3266(0.3290) Steps 646(628.87) | Grad Norm 8.8151(5.9575) | Total Time 0.00(0.00)\n",
      "Iter 1757 | Time 58.8396(60.4539) | Bit/dim 3.6650(3.6684) | Xent 0.9704(0.9221) | Loss 9.0410(9.6121) | Error 0.3452(0.3295) Steps 604(628.13) | Grad Norm 13.1955(6.1746) | Total Time 0.00(0.00)\n",
      "Iter 1758 | Time 62.9766(60.5295) | Bit/dim 3.6669(3.6684) | Xent 1.0234(0.9252) | Loss 9.1657(9.5987) | Error 0.3644(0.3306) Steps 616(627.76) | Grad Norm 13.7613(6.4022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 23.7382, Epoch Time 400.2406(405.4387), Bit/dim 3.6710(best: 3.6676), Xent 0.9977, Loss 4.1698, Error 0.3532(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 59.2293(60.4905) | Bit/dim 3.6830(3.6688) | Xent 0.9931(0.9272) | Loss 12.2032(9.6768) | Error 0.3535(0.3312) Steps 610(627.23) | Grad Norm 12.0527(6.5718) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 60.0257(60.4766) | Bit/dim 3.6607(3.6686) | Xent 1.0091(0.9297) | Loss 9.2507(9.6640) | Error 0.3621(0.3322) Steps 628(627.25) | Grad Norm 11.4284(6.7175) | Total Time 0.00(0.00)\n",
      "Iter 1761 | Time 61.8362(60.5174) | Bit/dim 3.6627(3.6684) | Xent 0.9426(0.9301) | Loss 9.1393(9.6483) | Error 0.3385(0.3324) Steps 598(626.37) | Grad Norm 9.2128(6.7923) | Total Time 0.00(0.00)\n",
      "Iter 1762 | Time 57.6214(60.4305) | Bit/dim 3.6632(3.6682) | Xent 0.9477(0.9306) | Loss 9.1737(9.6340) | Error 0.3393(0.3326) Steps 622(626.24) | Grad Norm 7.0140(6.7990) | Total Time 0.00(0.00)\n",
      "Iter 1763 | Time 63.4301(60.5205) | Bit/dim 3.6717(3.6683) | Xent 0.9874(0.9323) | Loss 9.1828(9.6205) | Error 0.3574(0.3333) Steps 592(625.22) | Grad Norm 16.4958(7.0899) | Total Time 0.00(0.00)\n",
      "Iter 1764 | Time 58.9679(60.4739) | Bit/dim 3.6719(3.6684) | Xent 0.9885(0.9340) | Loss 9.0815(9.6043) | Error 0.3566(0.3340) Steps 664(626.38) | Grad Norm 14.1811(7.3026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 23.2265, Epoch Time 400.4520(405.2891), Bit/dim 3.6661(best: 3.6676), Xent 0.9674, Loss 4.1498, Error 0.3391(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 56.9312(60.3676) | Bit/dim 3.6693(3.6685) | Xent 0.9722(0.9351) | Loss 12.0327(9.6772) | Error 0.3442(0.3343) Steps 628(626.43) | Grad Norm 8.2679(7.3316) | Total Time 0.00(0.00)\n",
      "Iter 1766 | Time 55.2584(60.2143) | Bit/dim 3.6855(3.6690) | Xent 0.9420(0.9353) | Loss 9.0133(9.6573) | Error 0.3381(0.3344) Steps 622(626.30) | Grad Norm 8.3470(7.3620) | Total Time 0.00(0.00)\n",
      "Iter 1767 | Time 60.0307(60.2088) | Bit/dim 3.6697(3.6690) | Xent 0.9299(0.9352) | Loss 9.2471(9.6450) | Error 0.3319(0.3344) Steps 622(626.17) | Grad Norm 7.0936(7.3540) | Total Time 0.00(0.00)\n",
      "Iter 1768 | Time 62.3389(60.2727) | Bit/dim 3.6597(3.6687) | Xent 0.9486(0.9356) | Loss 9.1392(9.6298) | Error 0.3357(0.3344) Steps 646(626.76) | Grad Norm 6.0592(7.3151) | Total Time 0.00(0.00)\n",
      "Iter 1769 | Time 59.3847(60.2461) | Bit/dim 3.6636(3.6686) | Xent 0.9244(0.9352) | Loss 9.1910(9.6166) | Error 0.3390(0.3345) Steps 622(626.62) | Grad Norm 6.4974(7.2906) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 63.0761(60.3310) | Bit/dim 3.6791(3.6689) | Xent 0.9352(0.9352) | Loss 9.1974(9.6041) | Error 0.3346(0.3345) Steps 610(626.12) | Grad Norm 7.6064(7.3001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 23.2427, Epoch Time 396.0631(405.0123), Bit/dim 3.6734(best: 3.6661), Xent 0.9210, Loss 4.1339, Error 0.3306(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 57.4221(60.2437) | Bit/dim 3.6743(3.6691) | Xent 0.9051(0.9343) | Loss 12.3436(9.6862) | Error 0.3303(0.3344) Steps 634(626.36) | Grad Norm 6.7870(7.2847) | Total Time 0.00(0.00)\n",
      "Iter 1772 | Time 58.6578(60.1962) | Bit/dim 3.6715(3.6691) | Xent 0.9291(0.9342) | Loss 8.9691(9.6647) | Error 0.3281(0.3342) Steps 646(626.95) | Grad Norm 6.6196(7.2647) | Total Time 0.00(0.00)\n",
      "Iter 1773 | Time 58.3575(60.1410) | Bit/dim 3.6709(3.6692) | Xent 0.9044(0.9333) | Loss 8.9728(9.6440) | Error 0.3187(0.3338) Steps 622(626.80) | Grad Norm 5.7202(7.2184) | Total Time 0.00(0.00)\n",
      "Iter 1774 | Time 59.3690(60.1178) | Bit/dim 3.6700(3.6692) | Xent 0.9261(0.9331) | Loss 8.9179(9.6222) | Error 0.3329(0.3337) Steps 616(626.47) | Grad Norm 5.9674(7.1809) | Total Time 0.00(0.00)\n",
      "Iter 1775 | Time 58.6405(60.0735) | Bit/dim 3.6738(3.6693) | Xent 0.9115(0.9324) | Loss 9.0471(9.6049) | Error 0.3271(0.3335) Steps 646(627.06) | Grad Norm 4.6342(7.1045) | Total Time 0.00(0.00)\n",
      "Iter 1776 | Time 63.5530(60.1779) | Bit/dim 3.6686(3.6693) | Xent 0.9173(0.9320) | Loss 9.1816(9.5922) | Error 0.3291(0.3334) Steps 646(627.63) | Grad Norm 6.6628(7.0912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 23.1393, Epoch Time 394.9729(404.7111), Bit/dim 3.6697(best: 3.6661), Xent 0.9118, Loss 4.1255, Error 0.3268(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 54.5392(60.0087) | Bit/dim 3.6819(3.6697) | Xent 0.9152(0.9315) | Loss 12.1134(9.6679) | Error 0.3310(0.3333) Steps 604(626.92) | Grad Norm 3.3036(6.9776) | Total Time 0.00(0.00)\n",
      "Iter 1778 | Time 56.8745(59.9147) | Bit/dim 3.6664(3.6696) | Xent 0.9218(0.9312) | Loss 9.1931(9.6536) | Error 0.3310(0.3333) Steps 628(626.95) | Grad Norm 7.6586(6.9980) | Total Time 0.00(0.00)\n",
      "Iter 1779 | Time 58.7449(59.8796) | Bit/dim 3.6639(3.6694) | Xent 0.9174(0.9308) | Loss 9.0846(9.6366) | Error 0.3323(0.3332) Steps 628(626.98) | Grad Norm 6.1279(6.9719) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 59.8813(59.8797) | Bit/dim 3.6665(3.6693) | Xent 0.9073(0.9301) | Loss 9.2051(9.6236) | Error 0.3199(0.3328) Steps 640(627.37) | Grad Norm 6.6639(6.9627) | Total Time 0.00(0.00)\n",
      "Iter 1781 | Time 60.8440(59.9086) | Bit/dim 3.6771(3.6696) | Xent 0.9180(0.9297) | Loss 9.1062(9.6081) | Error 0.3234(0.3325) Steps 622(627.21) | Grad Norm 8.7857(7.0174) | Total Time 0.00(0.00)\n",
      "Iter 1782 | Time 63.9382(60.0295) | Bit/dim 3.6600(3.6693) | Xent 0.8825(0.9283) | Loss 9.1511(9.5944) | Error 0.3116(0.3319) Steps 634(627.42) | Grad Norm 3.3731(6.9080) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 23.1606, Epoch Time 394.2529(404.3974), Bit/dim 3.6657(best: 3.6661), Xent 0.9081, Loss 4.1198, Error 0.3225(best: 0.3242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 63.3011(60.1276) | Bit/dim 3.6588(3.6690) | Xent 0.8938(0.9272) | Loss 11.8978(9.6635) | Error 0.3126(0.3313) Steps 670(628.69) | Grad Norm 5.9298(6.8787) | Total Time 0.00(0.00)\n",
      "Iter 1784 | Time 62.1490(60.1883) | Bit/dim 3.6618(3.6688) | Xent 0.9302(0.9273) | Loss 8.9767(9.6429) | Error 0.3325(0.3314) Steps 622(628.49) | Grad Norm 4.2192(6.7989) | Total Time 0.00(0.00)\n",
      "Iter 1785 | Time 58.1044(60.1258) | Bit/dim 3.6785(3.6691) | Xent 0.8968(0.9264) | Loss 9.2885(9.6323) | Error 0.3217(0.3311) Steps 610(627.94) | Grad Norm 5.6578(6.7647) | Total Time 0.00(0.00)\n",
      "Iter 1786 | Time 59.0890(60.0947) | Bit/dim 3.6620(3.6688) | Xent 0.9013(0.9257) | Loss 9.0906(9.6160) | Error 0.3165(0.3306) Steps 610(627.40) | Grad Norm 2.9816(6.6512) | Total Time 0.00(0.00)\n",
      "Iter 1787 | Time 62.6495(60.1713) | Bit/dim 3.6626(3.6687) | Xent 0.8871(0.9245) | Loss 9.0818(9.6000) | Error 0.3227(0.3304) Steps 640(627.78) | Grad Norm 5.0382(6.6028) | Total Time 0.00(0.00)\n",
      "Iter 1788 | Time 58.7211(60.1278) | Bit/dim 3.6750(3.6688) | Xent 0.8850(0.9233) | Loss 9.0795(9.5844) | Error 0.3093(0.3298) Steps 640(628.14) | Grad Norm 3.1692(6.4998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 23.4635, Epoch Time 403.5760(404.3727), Bit/dim 3.6679(best: 3.6657), Xent 0.9054, Loss 4.1206, Error 0.3210(best: 0.3225)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 65.1212(60.2776) | Bit/dim 3.6608(3.6686) | Xent 0.9142(0.9231) | Loss 12.2488(9.6643) | Error 0.3281(0.3297) Steps 622(627.96) | Grad Norm 3.7385(6.4169) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 57.9691(60.2083) | Bit/dim 3.6688(3.6686) | Xent 0.9070(0.9226) | Loss 8.9231(9.6421) | Error 0.3251(0.3296) Steps 604(627.24) | Grad Norm 4.0484(6.3459) | Total Time 0.00(0.00)\n",
      "Iter 1791 | Time 64.1724(60.3273) | Bit/dim 3.6860(3.6691) | Xent 0.8720(0.9211) | Loss 9.1782(9.6281) | Error 0.3134(0.3291) Steps 616(626.90) | Grad Norm 4.1114(6.2789) | Total Time 0.00(0.00)\n",
      "Iter 1792 | Time 59.0862(60.2900) | Bit/dim 3.6626(3.6689) | Xent 0.9023(0.9205) | Loss 9.1452(9.6137) | Error 0.3270(0.3290) Steps 622(626.76) | Grad Norm 2.5756(6.1678) | Total Time 0.00(0.00)\n",
      "Iter 1793 | Time 63.0395(60.3725) | Bit/dim 3.6619(3.6687) | Xent 0.8996(0.9199) | Loss 9.0252(9.5960) | Error 0.3213(0.3288) Steps 616(626.43) | Grad Norm 4.9499(6.1312) | Total Time 0.00(0.00)\n",
      "Iter 1794 | Time 58.8443(60.3267) | Bit/dim 3.6591(3.6684) | Xent 0.8822(0.9187) | Loss 9.0411(9.5794) | Error 0.3121(0.3283) Steps 640(626.84) | Grad Norm 6.1009(6.1303) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 22.9063, Epoch Time 407.2802(404.4600), Bit/dim 3.6648(best: 3.6657), Xent 0.9003, Loss 4.1150, Error 0.3223(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 61.4262(60.3597) | Bit/dim 3.6599(3.6682) | Xent 0.8830(0.9177) | Loss 12.4451(9.6653) | Error 0.3150(0.3279) Steps 598(625.98) | Grad Norm 2.8355(6.0315) | Total Time 0.00(0.00)\n",
      "Iter 1796 | Time 60.6377(60.3680) | Bit/dim 3.6562(3.6678) | Xent 0.9010(0.9172) | Loss 9.1559(9.6500) | Error 0.3181(0.3276) Steps 628(626.04) | Grad Norm 6.3511(6.0411) | Total Time 0.00(0.00)\n",
      "Iter 1797 | Time 57.9255(60.2947) | Bit/dim 3.6665(3.6678) | Xent 0.9110(0.9170) | Loss 9.1040(9.6337) | Error 0.3275(0.3276) Steps 628(626.10) | Grad Norm 6.4356(6.0529) | Total Time 0.00(0.00)\n",
      "Iter 1798 | Time 60.4633(60.2998) | Bit/dim 3.6716(3.6679) | Xent 0.9090(0.9167) | Loss 8.7316(9.6066) | Error 0.3319(0.3277) Steps 652(626.87) | Grad Norm 4.5618(6.0082) | Total Time 0.00(0.00)\n",
      "Iter 1799 | Time 57.8770(60.2271) | Bit/dim 3.6656(3.6678) | Xent 0.9035(0.9163) | Loss 8.9921(9.5882) | Error 0.3284(0.3278) Steps 646(627.45) | Grad Norm 7.1715(6.0431) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 58.5168(60.1758) | Bit/dim 3.6684(3.6678) | Xent 0.8909(0.9156) | Loss 8.9146(9.5680) | Error 0.3137(0.3273) Steps 628(627.46) | Grad Norm 5.8714(6.0379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 23.2212, Epoch Time 396.1988(404.2121), Bit/dim 3.6609(best: 3.6648), Xent 0.9069, Loss 4.1143, Error 0.3232(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 59.4962(60.1554) | Bit/dim 3.6579(3.6675) | Xent 0.8940(0.9149) | Loss 12.1121(9.6443) | Error 0.3214(0.3272) Steps 634(627.66) | Grad Norm 4.7678(5.9998) | Total Time 0.00(0.00)\n",
      "Iter 1802 | Time 60.7025(60.1718) | Bit/dim 3.6630(3.6674) | Xent 0.9296(0.9154) | Loss 9.2162(9.6314) | Error 0.3381(0.3275) Steps 628(627.67) | Grad Norm 8.1793(6.0652) | Total Time 0.00(0.00)\n",
      "Iter 1803 | Time 63.3584(60.2674) | Bit/dim 3.6726(3.6676) | Xent 0.9374(0.9160) | Loss 8.7869(9.6061) | Error 0.3344(0.3277) Steps 622(627.50) | Grad Norm 9.4221(6.1659) | Total Time 0.00(0.00)\n",
      "Iter 1804 | Time 64.2956(60.3883) | Bit/dim 3.6614(3.6674) | Xent 0.8908(0.9153) | Loss 9.0966(9.5908) | Error 0.3121(0.3272) Steps 646(628.05) | Grad Norm 4.7821(6.1244) | Total Time 0.00(0.00)\n",
      "Iter 1805 | Time 62.8941(60.4634) | Bit/dim 3.6624(3.6672) | Xent 0.9064(0.9150) | Loss 8.9919(9.5729) | Error 0.3254(0.3272) Steps 604(627.33) | Grad Norm 7.3232(6.1603) | Total Time 0.00(0.00)\n",
      "Iter 1806 | Time 55.1899(60.3052) | Bit/dim 3.6657(3.6672) | Xent 0.9005(0.9146) | Loss 9.0473(9.5571) | Error 0.3253(0.3271) Steps 604(626.63) | Grad Norm 8.0868(6.2181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 23.4715, Epoch Time 405.7350(404.2578), Bit/dim 3.6611(best: 3.6609), Xent 0.9114, Loss 4.1169, Error 0.3283(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 57.3827(60.2176) | Bit/dim 3.6718(3.6673) | Xent 0.8966(0.9140) | Loss 12.3310(9.6403) | Error 0.3203(0.3269) Steps 628(626.67) | Grad Norm 4.2937(6.1604) | Total Time 0.00(0.00)\n",
      "Iter 1808 | Time 60.0578(60.2128) | Bit/dim 3.6700(3.6674) | Xent 0.9139(0.9140) | Loss 9.1682(9.6261) | Error 0.3274(0.3269) Steps 628(626.71) | Grad Norm 4.6936(6.1164) | Total Time 0.00(0.00)\n",
      "Iter 1809 | Time 58.8921(60.1731) | Bit/dim 3.6581(3.6671) | Xent 0.9043(0.9137) | Loss 8.8163(9.6018) | Error 0.3273(0.3269) Steps 622(626.57) | Grad Norm 4.3964(6.0648) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 58.5370(60.1241) | Bit/dim 3.6547(3.6667) | Xent 0.8728(0.9125) | Loss 9.0095(9.5841) | Error 0.3166(0.3266) Steps 610(626.07) | Grad Norm 5.8873(6.0595) | Total Time 0.00(0.00)\n",
      "Iter 1811 | Time 57.9841(60.0599) | Bit/dim 3.6798(3.6671) | Xent 0.9145(0.9126) | Loss 9.2461(9.5739) | Error 0.3230(0.3265) Steps 604(625.41) | Grad Norm 7.8694(6.1138) | Total Time 0.00(0.00)\n",
      "Iter 1812 | Time 59.0427(60.0293) | Bit/dim 3.6560(3.6668) | Xent 0.9018(0.9122) | Loss 8.9228(9.5544) | Error 0.3203(0.3263) Steps 616(625.13) | Grad Norm 5.0052(6.0805) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 23.0547, Epoch Time 391.1099(403.8634), Bit/dim 3.6626(best: 3.6609), Xent 0.9090, Loss 4.1171, Error 0.3221(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 56.5917(59.9262) | Bit/dim 3.6615(3.6666) | Xent 0.9121(0.9122) | Loss 12.0167(9.6283) | Error 0.3214(0.3262) Steps 616(624.86) | Grad Norm 4.7570(6.0408) | Total Time 0.00(0.00)\n",
      "Iter 1814 | Time 58.4743(59.8827) | Bit/dim 3.6659(3.6666) | Xent 0.9207(0.9125) | Loss 9.1099(9.6127) | Error 0.3224(0.3261) Steps 634(625.13) | Grad Norm 6.6182(6.0581) | Total Time 0.00(0.00)\n",
      "Iter 1815 | Time 62.9492(59.9747) | Bit/dim 3.6784(3.6670) | Xent 0.8671(0.9111) | Loss 9.2102(9.6006) | Error 0.3154(0.3257) Steps 658(626.12) | Grad Norm 5.9040(6.0535) | Total Time 0.00(0.00)\n",
      "Iter 1816 | Time 61.7538(60.0280) | Bit/dim 3.6578(3.6667) | Xent 0.8939(0.9106) | Loss 9.0776(9.5849) | Error 0.3209(0.3256) Steps 610(625.63) | Grad Norm 5.9916(6.0517) | Total Time 0.00(0.00)\n",
      "Iter 1817 | Time 55.3081(59.8864) | Bit/dim 3.6572(3.6664) | Xent 0.8918(0.9101) | Loss 9.0050(9.5676) | Error 0.3284(0.3257) Steps 610(625.16) | Grad Norm 6.1187(6.0537) | Total Time 0.00(0.00)\n",
      "Iter 1818 | Time 55.8194(59.7644) | Bit/dim 3.6732(3.6666) | Xent 0.8909(0.9095) | Loss 9.2262(9.5573) | Error 0.3167(0.3254) Steps 634(625.43) | Grad Norm 7.5815(6.0995) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 23.1422, Epoch Time 389.8375(403.4426), Bit/dim 3.6653(best: 3.6609), Xent 0.9577, Loss 4.1442, Error 0.3431(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 62.2118(59.8378) | Bit/dim 3.6506(3.6661) | Xent 0.9332(0.9102) | Loss 12.3594(9.6414) | Error 0.3360(0.3257) Steps 634(625.69) | Grad Norm 7.7617(6.1494) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 61.2318(59.8797) | Bit/dim 3.6724(3.6663) | Xent 0.9080(0.9101) | Loss 9.2328(9.6291) | Error 0.3224(0.3256) Steps 610(625.22) | Grad Norm 5.7015(6.1359) | Total Time 0.00(0.00)\n",
      "Iter 1821 | Time 63.9248(60.0010) | Bit/dim 3.6689(3.6664) | Xent 0.8919(0.9096) | Loss 9.0379(9.6114) | Error 0.3243(0.3256) Steps 640(625.66) | Grad Norm 3.7958(6.0657) | Total Time 0.00(0.00)\n",
      "Iter 1822 | Time 57.1777(59.9163) | Bit/dim 3.6598(3.6662) | Xent 0.8921(0.9090) | Loss 8.9973(9.5930) | Error 0.3216(0.3255) Steps 616(625.37) | Grad Norm 6.0551(6.0654) | Total Time 0.00(0.00)\n",
      "Iter 1823 | Time 60.7398(59.9410) | Bit/dim 3.6619(3.6661) | Xent 0.9008(0.9088) | Loss 9.0076(9.5754) | Error 0.3180(0.3252) Steps 640(625.81) | Grad Norm 5.7725(6.0566) | Total Time 0.00(0.00)\n",
      "Iter 1824 | Time 55.9406(59.8210) | Bit/dim 3.6672(3.6661) | Xent 0.9010(0.9086) | Loss 9.2112(9.5645) | Error 0.3276(0.3253) Steps 610(625.33) | Grad Norm 6.7057(6.0761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 22.7677, Epoch Time 400.2621(403.3472), Bit/dim 3.6669(best: 3.6609), Xent 0.9146, Loss 4.1242, Error 0.3240(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 59.1333(59.8004) | Bit/dim 3.6680(3.6662) | Xent 0.9222(0.9090) | Loss 12.2102(9.6438) | Error 0.3293(0.3254) Steps 634(625.59) | Grad Norm 7.9610(6.1326) | Total Time 0.00(0.00)\n",
      "Iter 1826 | Time 65.8389(59.9815) | Bit/dim 3.6583(3.6659) | Xent 0.8928(0.9085) | Loss 9.0261(9.6253) | Error 0.3184(0.3252) Steps 628(625.67) | Grad Norm 5.7878(6.1223) | Total Time 0.00(0.00)\n",
      "Iter 1827 | Time 60.3685(59.9931) | Bit/dim 3.6596(3.6657) | Xent 0.8931(0.9080) | Loss 9.0636(9.6085) | Error 0.3215(0.3251) Steps 616(625.38) | Grad Norm 4.9097(6.0859) | Total Time 0.00(0.00)\n",
      "Iter 1828 | Time 56.7292(59.8952) | Bit/dim 3.6602(3.6656) | Xent 0.8980(0.9077) | Loss 9.1255(9.5940) | Error 0.3189(0.3249) Steps 640(625.82) | Grad Norm 7.6483(6.1328) | Total Time 0.00(0.00)\n",
      "Iter 1829 | Time 57.5315(59.8243) | Bit/dim 3.6627(3.6655) | Xent 0.8868(0.9071) | Loss 9.0288(9.5770) | Error 0.3201(0.3248) Steps 634(626.06) | Grad Norm 6.4493(6.1423) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 54.5170(59.6651) | Bit/dim 3.6641(3.6654) | Xent 0.8884(0.9065) | Loss 9.1598(9.5645) | Error 0.3206(0.3247) Steps 616(625.76) | Grad Norm 3.3490(6.0585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 23.0835, Epoch Time 393.4243(403.0495), Bit/dim 3.6628(best: 3.6609), Xent 0.9134, Loss 4.1195, Error 0.3269(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1831 | Time 63.4549(59.7788) | Bit/dim 3.6644(3.6654) | Xent 0.8668(0.9053) | Loss 12.2626(9.6454) | Error 0.3061(0.3241) Steps 628(625.83) | Grad Norm 6.2100(6.0630) | Total Time 0.00(0.00)\n",
      "Iter 1832 | Time 57.3924(59.7072) | Bit/dim 3.6642(3.6654) | Xent 0.9032(0.9053) | Loss 9.0272(9.6269) | Error 0.3276(0.3242) Steps 634(626.07) | Grad Norm 5.8084(6.0554) | Total Time 0.00(0.00)\n",
      "Iter 1833 | Time 59.8485(59.7114) | Bit/dim 3.6683(3.6655) | Xent 0.8719(0.9043) | Loss 8.8611(9.6039) | Error 0.3113(0.3238) Steps 646(626.67) | Grad Norm 5.7652(6.0467) | Total Time 0.00(0.00)\n",
      "Iter 1834 | Time 64.3603(59.8509) | Bit/dim 3.6548(3.6652) | Xent 0.8734(0.9034) | Loss 9.1796(9.5912) | Error 0.3109(0.3234) Steps 634(626.89) | Grad Norm 6.3378(6.0554) | Total Time 0.00(0.00)\n",
      "Iter 1835 | Time 55.8207(59.7300) | Bit/dim 3.6589(3.6650) | Xent 0.9053(0.9034) | Loss 8.9949(9.5733) | Error 0.3250(0.3235) Steps 616(626.56) | Grad Norm 5.5881(6.0414) | Total Time 0.00(0.00)\n",
      "Iter 1836 | Time 59.4133(59.7205) | Bit/dim 3.6663(3.6650) | Xent 0.8899(0.9030) | Loss 9.0934(9.5589) | Error 0.3223(0.3234) Steps 622(626.43) | Grad Norm 3.1540(5.9548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 23.1768, Epoch Time 399.4309(402.9409), Bit/dim 3.6606(best: 3.6609), Xent 0.8941, Loss 4.1076, Error 0.3209(best: 0.3210)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1837 | Time 60.6547(59.7485) | Bit/dim 3.6554(3.6647) | Xent 0.8913(0.9027) | Loss 12.1437(9.6364) | Error 0.3207(0.3234) Steps 628(626.47) | Grad Norm 5.5050(5.9413) | Total Time 0.00(0.00)\n",
      "Iter 1838 | Time 63.2015(59.8521) | Bit/dim 3.6698(3.6649) | Xent 0.8857(0.9021) | Loss 9.0847(9.6199) | Error 0.3157(0.3231) Steps 634(626.70) | Grad Norm 3.6755(5.8733) | Total Time 0.00(0.00)\n",
      "Iter 1839 | Time 64.5935(59.9944) | Bit/dim 3.6635(3.6648) | Xent 0.8770(0.9014) | Loss 9.1094(9.6046) | Error 0.3080(0.3227) Steps 604(626.02) | Grad Norm 4.9373(5.8452) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 61.1436(60.0288) | Bit/dim 3.6573(3.6646) | Xent 0.8901(0.9011) | Loss 9.1831(9.5919) | Error 0.3246(0.3227) Steps 586(624.82) | Grad Norm 7.0939(5.8827) | Total Time 0.00(0.00)\n",
      "Iter 1841 | Time 55.1107(59.8813) | Bit/dim 3.6543(3.6643) | Xent 0.9064(0.9012) | Loss 9.1363(9.5783) | Error 0.3225(0.3227) Steps 628(624.91) | Grad Norm 7.9195(5.9438) | Total Time 0.00(0.00)\n",
      "Iter 1842 | Time 58.0147(59.8253) | Bit/dim 3.6576(3.6641) | Xent 0.8898(0.9009) | Loss 9.1289(9.5648) | Error 0.3181(0.3226) Steps 598(624.11) | Grad Norm 6.9183(5.9730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 22.3043, Epoch Time 400.8661(402.8787), Bit/dim 3.6679(best: 3.6606), Xent 0.9004, Loss 4.1180, Error 0.3200(best: 0.3209)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1843 | Time 57.7740(59.7638) | Bit/dim 3.6602(3.6640) | Xent 0.8722(0.9000) | Loss 12.0934(9.6406) | Error 0.3127(0.3223) Steps 616(623.86) | Grad Norm 5.8679(5.9699) | Total Time 0.00(0.00)\n",
      "Iter 1844 | Time 62.4500(59.8443) | Bit/dim 3.6522(3.6636) | Xent 0.9566(0.9017) | Loss 9.1903(9.6271) | Error 0.3455(0.3230) Steps 640(624.35) | Grad Norm 9.6513(6.0803) | Total Time 0.00(0.00)\n",
      "Iter 1845 | Time 62.6486(59.9285) | Bit/dim 3.6809(3.6641) | Xent 1.0236(0.9054) | Loss 9.3338(9.6183) | Error 0.3576(0.3240) Steps 610(623.92) | Grad Norm 13.3885(6.2996) | Total Time 0.00(0.00)\n",
      "Iter 1846 | Time 56.9660(59.8396) | Bit/dim 3.6703(3.6643) | Xent 1.0620(0.9101) | Loss 9.2932(9.6086) | Error 0.3789(0.3257) Steps 634(624.22) | Grad Norm 16.7583(6.6133) | Total Time 0.00(0.00)\n",
      "Iter 1847 | Time 58.1158(59.7879) | Bit/dim 3.6676(3.6644) | Xent 0.9766(0.9121) | Loss 9.2522(9.5979) | Error 0.3435(0.3262) Steps 604(623.61) | Grad Norm 19.0419(6.9862) | Total Time 0.00(0.00)\n",
      "Iter 1848 | Time 55.8604(59.6701) | Bit/dim 3.6747(3.6647) | Xent 1.0370(0.9158) | Loss 9.1213(9.5836) | Error 0.3690(0.3275) Steps 622(623.56) | Grad Norm 14.1920(7.2024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 23.3660, Epoch Time 393.4150(402.5948), Bit/dim 3.6695(best: 3.6606), Xent 1.0107, Loss 4.1749, Error 0.3534(best: 0.3200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1849 | Time 54.0798(59.5023) | Bit/dim 3.6709(3.6649) | Xent 0.9922(0.9181) | Loss 12.4960(9.6710) | Error 0.3544(0.3283) Steps 616(623.34) | Grad Norm 15.8628(7.4622) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 57.1610(59.4321) | Bit/dim 3.6633(3.6649) | Xent 1.0054(0.9207) | Loss 9.1727(9.6560) | Error 0.3572(0.3292) Steps 634(623.66) | Grad Norm 11.7683(7.5914) | Total Time 0.00(0.00)\n",
      "Iter 1851 | Time 55.7507(59.3217) | Bit/dim 3.6740(3.6651) | Xent 1.0003(0.9231) | Loss 9.3037(9.6454) | Error 0.3551(0.3299) Steps 598(622.89) | Grad Norm 10.3303(7.6735) | Total Time 0.00(0.00)\n",
      "Iter 1852 | Time 57.5686(59.2691) | Bit/dim 3.6709(3.6653) | Xent 0.9953(0.9253) | Loss 9.0435(9.6274) | Error 0.3486(0.3305) Steps 604(622.32) | Grad Norm 9.9300(7.7412) | Total Time 0.00(0.00)\n",
      "Iter 1853 | Time 63.0841(59.3835) | Bit/dim 3.6665(3.6654) | Xent 0.9320(0.9255) | Loss 9.0065(9.6088) | Error 0.3307(0.3305) Steps 610(621.95) | Grad Norm 6.6939(7.7098) | Total Time 0.00(0.00)\n",
      "Iter 1854 | Time 59.0888(59.3747) | Bit/dim 3.6613(3.6652) | Xent 0.9719(0.9269) | Loss 9.1947(9.5963) | Error 0.3436(0.3309) Steps 628(622.13) | Grad Norm 6.1165(7.6620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 22.9665, Epoch Time 385.5991(402.0849), Bit/dim 3.6718(best: 3.6606), Xent 0.9518, Loss 4.1477, Error 0.3369(best: 0.3200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1855 | Time 56.3120(59.2828) | Bit/dim 3.6774(3.6656) | Xent 0.9660(0.9280) | Loss 12.5049(9.6836) | Error 0.3492(0.3315) Steps 646(622.85) | Grad Norm 7.5385(7.6583) | Total Time 0.00(0.00)\n",
      "Iter 1856 | Time 55.3013(59.1634) | Bit/dim 3.6679(3.6657) | Xent 0.8929(0.9270) | Loss 9.1000(9.6661) | Error 0.3289(0.3314) Steps 604(622.28) | Grad Norm 3.6779(7.5389) | Total Time 0.00(0.00)\n",
      "Iter 1857 | Time 66.0785(59.3708) | Bit/dim 3.6663(3.6657) | Xent 0.9171(0.9267) | Loss 9.2332(9.6531) | Error 0.3270(0.3312) Steps 610(621.91) | Grad Norm 6.2885(7.5014) | Total Time 0.00(0.00)\n",
      "Iter 1858 | Time 64.1186(59.5132) | Bit/dim 3.6650(3.6657) | Xent 0.9133(0.9263) | Loss 9.0938(9.6363) | Error 0.3306(0.3312) Steps 622(621.92) | Grad Norm 5.1050(7.4295) | Total Time 0.00(0.00)\n",
      "Iter 1859 | Time 57.1339(59.4419) | Bit/dim 3.6687(3.6658) | Xent 0.9329(0.9265) | Loss 9.1507(9.6218) | Error 0.3406(0.3315) Steps 616(621.74) | Grad Norm 5.1729(7.3618) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 58.1040(59.4017) | Bit/dim 3.6746(3.6660) | Xent 0.9290(0.9266) | Loss 9.2662(9.6111) | Error 0.3321(0.3315) Steps 634(622.11) | Grad Norm 6.0325(7.3219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 22.5186, Epoch Time 395.6205(401.8910), Bit/dim 3.6669(best: 3.6606), Xent 0.8995, Loss 4.1167, Error 0.3199(best: 0.3200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1861 | Time 62.2561(59.4874) | Bit/dim 3.6525(3.6656) | Xent 0.8777(0.9251) | Loss 11.8589(9.6785) | Error 0.3133(0.3310) Steps 604(621.56) | Grad Norm 3.0832(7.1947) | Total Time 0.00(0.00)\n",
      "Iter 1862 | Time 58.1996(59.4487) | Bit/dim 3.6654(3.6656) | Xent 0.9113(0.9247) | Loss 9.1951(9.6640) | Error 0.3264(0.3308) Steps 628(621.76) | Grad Norm 6.5811(7.1763) | Total Time 0.00(0.00)\n",
      "Iter 1863 | Time 60.3568(59.4760) | Bit/dim 3.6666(3.6656) | Xent 0.9245(0.9247) | Loss 9.1299(9.6480) | Error 0.3277(0.3308) Steps 604(621.22) | Grad Norm 4.3830(7.0925) | Total Time 0.00(0.00)\n",
      "Iter 1864 | Time 61.4478(59.5351) | Bit/dim 3.6806(3.6661) | Xent 0.9227(0.9246) | Loss 9.1834(9.6341) | Error 0.3361(0.3309) Steps 664(622.51) | Grad Norm 5.0274(7.0306) | Total Time 0.00(0.00)\n",
      "Iter 1865 | Time 64.0330(59.6701) | Bit/dim 3.6565(3.6658) | Xent 0.9049(0.9240) | Loss 9.1676(9.6201) | Error 0.3233(0.3307) Steps 622(622.49) | Grad Norm 3.7160(6.9311) | Total Time 0.00(0.00)\n",
      "Iter 1866 | Time 62.2955(59.7488) | Bit/dim 3.6540(3.6654) | Xent 0.9051(0.9235) | Loss 9.2328(9.6084) | Error 0.3234(0.3305) Steps 640(623.02) | Grad Norm 4.4382(6.8564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 23.4533, Epoch Time 407.8233(402.0689), Bit/dim 3.6679(best: 3.6606), Xent 0.9010, Loss 4.1184, Error 0.3220(best: 0.3199)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1867 | Time 60.8110(59.7807) | Bit/dim 3.6675(3.6655) | Xent 0.8700(0.9219) | Loss 12.1948(9.6860) | Error 0.3049(0.3297) Steps 628(623.17) | Grad Norm 3.0571(6.7424) | Total Time 0.00(0.00)\n",
      "Iter 1868 | Time 58.1682(59.7323) | Bit/dim 3.6728(3.6657) | Xent 0.9083(0.9215) | Loss 8.9009(9.6625) | Error 0.3264(0.3296) Steps 628(623.31) | Grad Norm 6.0155(6.7206) | Total Time 0.00(0.00)\n",
      "Iter 1869 | Time 62.2828(59.8088) | Bit/dim 3.6504(3.6653) | Xent 0.8846(0.9203) | Loss 9.0097(9.6429) | Error 0.3190(0.3293) Steps 622(623.27) | Grad Norm 3.7868(6.6326) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 61.4793(59.8589) | Bit/dim 3.6681(3.6653) | Xent 0.8951(0.9196) | Loss 8.8949(9.6205) | Error 0.3189(0.3290) Steps 652(624.13) | Grad Norm 7.2830(6.6521) | Total Time 0.00(0.00)\n",
      "Iter 1871 | Time 58.6400(59.8224) | Bit/dim 3.6572(3.6651) | Xent 0.8921(0.9188) | Loss 8.9532(9.6004) | Error 0.3184(0.3286) Steps 640(624.61) | Grad Norm 2.4008(6.5245) | Total Time 0.00(0.00)\n",
      "Iter 1872 | Time 52.8274(59.6125) | Bit/dim 3.6635(3.6651) | Xent 0.8849(0.9177) | Loss 9.0764(9.5847) | Error 0.3187(0.3284) Steps 622(624.53) | Grad Norm 5.6220(6.4975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 23.2176, Epoch Time 393.9518(401.8254), Bit/dim 3.6630(best: 3.6606), Xent 0.8955, Loss 4.1107, Error 0.3188(best: 0.3199)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1873 | Time 66.5978(59.8221) | Bit/dim 3.6512(3.6646) | Xent 0.8913(0.9170) | Loss 12.6313(9.6761) | Error 0.3161(0.3280) Steps 640(625.00) | Grad Norm 3.0581(6.3943) | Total Time 0.00(0.00)\n",
      "Iter 1874 | Time 61.5305(59.8733) | Bit/dim 3.6595(3.6645) | Xent 0.8823(0.9159) | Loss 9.1900(9.6615) | Error 0.3117(0.3275) Steps 646(625.63) | Grad Norm 4.8499(6.3479) | Total Time 0.00(0.00)\n",
      "Iter 1875 | Time 56.9942(59.7870) | Bit/dim 3.6663(3.6645) | Xent 0.8872(0.9151) | Loss 9.1108(9.6450) | Error 0.3173(0.3272) Steps 622(625.52) | Grad Norm 3.8979(6.2744) | Total Time 0.00(0.00)\n",
      "Iter 1876 | Time 62.6957(59.8742) | Bit/dim 3.6638(3.6645) | Xent 0.9081(0.9148) | Loss 9.2810(9.6341) | Error 0.3263(0.3272) Steps 670(626.85) | Grad Norm 3.3860(6.1878) | Total Time 0.00(0.00)\n",
      "Iter 1877 | Time 57.5163(59.8035) | Bit/dim 3.6705(3.6647) | Xent 0.8615(0.9132) | Loss 9.0273(9.6159) | Error 0.3103(0.3267) Steps 598(625.99) | Grad Norm 3.1286(6.0960) | Total Time 0.00(0.00)\n",
      "Iter 1878 | Time 57.0031(59.7195) | Bit/dim 3.6564(3.6644) | Xent 0.8642(0.9118) | Loss 9.0074(9.5976) | Error 0.3070(0.3261) Steps 622(625.87) | Grad Norm 3.2387(6.0103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 23.0335, Epoch Time 401.5028(401.8157), Bit/dim 3.6626(best: 3.6606), Xent 0.9020, Loss 4.1136, Error 0.3182(best: 0.3188)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1879 | Time 63.3741(59.8291) | Bit/dim 3.6576(3.6642) | Xent 0.8932(0.9112) | Loss 12.1248(9.6734) | Error 0.3214(0.3259) Steps 658(626.83) | Grad Norm 5.2444(5.9873) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 53.9746(59.6535) | Bit/dim 3.6593(3.6641) | Xent 0.8928(0.9107) | Loss 9.0505(9.6548) | Error 0.3239(0.3259) Steps 610(626.33) | Grad Norm 7.9571(6.0464) | Total Time 0.00(0.00)\n",
      "Iter 1881 | Time 56.5173(59.5594) | Bit/dim 3.6686(3.6642) | Xent 0.8895(0.9100) | Loss 9.0327(9.6361) | Error 0.3155(0.3256) Steps 628(626.38) | Grad Norm 8.4057(6.1172) | Total Time 0.00(0.00)\n",
      "Iter 1882 | Time 62.5120(59.6480) | Bit/dim 3.6638(3.6642) | Xent 0.8883(0.9094) | Loss 8.8891(9.6137) | Error 0.3165(0.3253) Steps 628(626.42) | Grad Norm 3.5578(6.0404) | Total Time 0.00(0.00)\n",
      "Iter 1883 | Time 58.7262(59.6203) | Bit/dim 3.6598(3.6641) | Xent 0.9093(0.9094) | Loss 9.1979(9.6012) | Error 0.3230(0.3252) Steps 622(626.29) | Grad Norm 9.5517(6.1457) | Total Time 0.00(0.00)\n",
      "Iter 1884 | Time 55.9438(59.5100) | Bit/dim 3.6666(3.6642) | Xent 0.8900(0.9088) | Loss 9.0011(9.5832) | Error 0.3225(0.3251) Steps 622(626.16) | Grad Norm 5.9809(6.1408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 23.3626, Epoch Time 390.4300(401.4742), Bit/dim 3.6632(best: 3.6606), Xent 0.9024, Loss 4.1144, Error 0.3178(best: 0.3182)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1885 | Time 57.8774(59.4610) | Bit/dim 3.6654(3.6642) | Xent 0.9088(0.9088) | Loss 12.1223(9.6594) | Error 0.3230(0.3251) Steps 640(626.58) | Grad Norm 5.6790(6.1269) | Total Time 0.00(0.00)\n",
      "Iter 1886 | Time 57.3639(59.3981) | Bit/dim 3.6609(3.6641) | Xent 0.8672(0.9076) | Loss 8.9131(9.6370) | Error 0.3110(0.3246) Steps 628(626.62) | Grad Norm 6.6596(6.1429) | Total Time 0.00(0.00)\n",
      "Iter 1887 | Time 64.4078(59.5484) | Bit/dim 3.6586(3.6639) | Xent 0.8909(0.9070) | Loss 9.2432(9.6252) | Error 0.3140(0.3243) Steps 586(625.40) | Grad Norm 4.4411(6.0919) | Total Time 0.00(0.00)\n",
      "Iter 1888 | Time 59.9522(59.5605) | Bit/dim 3.6598(3.6638) | Xent 0.8741(0.9061) | Loss 9.1684(9.6115) | Error 0.3093(0.3239) Steps 646(626.02) | Grad Norm 2.9710(5.9982) | Total Time 0.00(0.00)\n",
      "Iter 1889 | Time 60.2963(59.5826) | Bit/dim 3.6531(3.6635) | Xent 0.8819(0.9053) | Loss 9.0076(9.5934) | Error 0.3135(0.3236) Steps 634(626.26) | Grad Norm 3.1034(5.9114) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 57.1226(59.5088) | Bit/dim 3.6468(3.6630) | Xent 0.8813(0.9046) | Loss 8.9091(9.5728) | Error 0.3200(0.3235) Steps 634(626.49) | Grad Norm 4.3837(5.8656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 22.9228, Epoch Time 396.0774(401.3123), Bit/dim 3.6579(best: 3.6606), Xent 0.8941, Loss 4.1049, Error 0.3175(best: 0.3178)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1891 | Time 56.7508(59.4261) | Bit/dim 3.6540(3.6627) | Xent 0.8691(0.9036) | Loss 11.5990(9.6336) | Error 0.3064(0.3229) Steps 616(626.18) | Grad Norm 3.9744(5.8088) | Total Time 0.00(0.00)\n",
      "Iter 1892 | Time 57.5678(59.3703) | Bit/dim 3.6567(3.6625) | Xent 0.8839(0.9030) | Loss 9.1656(9.6196) | Error 0.3067(0.3225) Steps 640(626.59) | Grad Norm 3.5119(5.7399) | Total Time 0.00(0.00)\n",
      "Iter 1893 | Time 57.4212(59.3118) | Bit/dim 3.6687(3.6627) | Xent 0.8949(0.9027) | Loss 9.1139(9.6044) | Error 0.3221(0.3224) Steps 634(626.81) | Grad Norm 6.6115(5.7661) | Total Time 0.00(0.00)\n",
      "Iter 1894 | Time 56.0034(59.2126) | Bit/dim 3.6595(3.6626) | Xent 0.8678(0.9017) | Loss 8.9667(9.5853) | Error 0.3160(0.3223) Steps 586(625.59) | Grad Norm 5.9588(5.7719) | Total Time 0.00(0.00)\n",
      "Iter 1895 | Time 61.1412(59.2705) | Bit/dim 3.6521(3.6623) | Xent 0.8558(0.9003) | Loss 9.1058(9.5709) | Error 0.3083(0.3218) Steps 604(624.94) | Grad Norm 2.6534(5.6783) | Total Time 0.00(0.00)\n",
      "Iter 1896 | Time 63.0445(59.3837) | Bit/dim 3.6639(3.6624) | Xent 0.8836(0.8998) | Loss 9.1950(9.5596) | Error 0.3121(0.3215) Steps 622(624.85) | Grad Norm 8.3871(5.7596) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 23.5970, Epoch Time 391.8172(401.0274), Bit/dim 3.6616(best: 3.6579), Xent 0.9022, Loss 4.1126, Error 0.3198(best: 0.3175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1897 | Time 60.8313(59.4271) | Bit/dim 3.6495(3.6620) | Xent 0.8875(0.8994) | Loss 12.0238(9.6335) | Error 0.3163(0.3214) Steps 592(623.87) | Grad Norm 7.3012(5.8058) | Total Time 0.00(0.00)\n",
      "Iter 1898 | Time 61.7546(59.4969) | Bit/dim 3.6716(3.6623) | Xent 0.8633(0.8983) | Loss 9.0835(9.6170) | Error 0.3103(0.3210) Steps 622(623.81) | Grad Norm 3.4496(5.7351) | Total Time 0.00(0.00)\n",
      "Iter 1899 | Time 54.0972(59.3349) | Bit/dim 3.6607(3.6622) | Xent 0.8530(0.8970) | Loss 8.9154(9.5960) | Error 0.3023(0.3205) Steps 616(623.58) | Grad Norm 7.5497(5.7896) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 53.9909(59.1746) | Bit/dim 3.6662(3.6623) | Xent 0.8912(0.8968) | Loss 9.1377(9.5822) | Error 0.3220(0.3205) Steps 604(622.99) | Grad Norm 8.5636(5.8728) | Total Time 0.00(0.00)\n",
      "Iter 1901 | Time 64.0503(59.3209) | Bit/dim 3.6667(3.6625) | Xent 0.8654(0.8959) | Loss 9.1407(9.5690) | Error 0.3110(0.3202) Steps 610(622.60) | Grad Norm 5.4967(5.8615) | Total Time 0.00(0.00)\n",
      "Iter 1902 | Time 64.3761(59.4725) | Bit/dim 3.6475(3.6620) | Xent 0.8793(0.8954) | Loss 9.0698(9.5540) | Error 0.3173(0.3202) Steps 658(623.66) | Grad Norm 4.7803(5.8291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 23.2166, Epoch Time 398.4860(400.9512), Bit/dim 3.6654(best: 3.6579), Xent 0.9056, Loss 4.1182, Error 0.3231(best: 0.3175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1903 | Time 58.9838(59.4579) | Bit/dim 3.6738(3.6624) | Xent 0.8788(0.8949) | Loss 12.3078(9.6366) | Error 0.3173(0.3201) Steps 634(623.97) | Grad Norm 6.7905(5.8579) | Total Time 0.00(0.00)\n",
      "Iter 1904 | Time 61.9493(59.5326) | Bit/dim 3.6493(3.6620) | Xent 0.8646(0.8940) | Loss 9.0287(9.6184) | Error 0.3113(0.3198) Steps 616(623.73) | Grad Norm 5.1617(5.8370) | Total Time 0.00(0.00)\n",
      "Iter 1905 | Time 61.2564(59.5843) | Bit/dim 3.6584(3.6619) | Xent 0.8723(0.8933) | Loss 8.9636(9.5987) | Error 0.3107(0.3195) Steps 634(624.04) | Grad Norm 6.3510(5.8524) | Total Time 0.00(0.00)\n",
      "Iter 1906 | Time 63.1559(59.6915) | Bit/dim 3.6781(3.6624) | Xent 0.8972(0.8934) | Loss 8.9184(9.5783) | Error 0.3205(0.3196) Steps 622(623.98) | Grad Norm 6.6386(5.8760) | Total Time 0.00(0.00)\n",
      "Iter 1907 | Time 55.3622(59.5616) | Bit/dim 3.6488(3.6619) | Xent 0.8935(0.8934) | Loss 9.0569(9.5627) | Error 0.3213(0.3196) Steps 604(623.38) | Grad Norm 7.9771(5.9391) | Total Time 0.00(0.00)\n",
      "Iter 1908 | Time 67.2673(59.7928) | Bit/dim 3.6497(3.6616) | Xent 0.8826(0.8931) | Loss 9.1896(9.5515) | Error 0.3205(0.3196) Steps 640(623.88) | Grad Norm 5.2175(5.9174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 23.1718, Epoch Time 407.5413(401.1489), Bit/dim 3.6621(best: 3.6579), Xent 0.9074, Loss 4.1159, Error 0.3230(best: 0.3175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1909 | Time 62.2185(59.8655) | Bit/dim 3.6620(3.6616) | Xent 0.8710(0.8924) | Loss 12.2231(9.6316) | Error 0.3091(0.3193) Steps 610(623.46) | Grad Norm 7.4728(5.9641) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 58.6863(59.8302) | Bit/dim 3.6654(3.6617) | Xent 0.9054(0.8928) | Loss 9.0722(9.6149) | Error 0.3223(0.3194) Steps 586(622.34) | Grad Norm 11.5049(6.1303) | Total Time 0.00(0.00)\n",
      "Iter 1911 | Time 57.7688(59.7683) | Bit/dim 3.6506(3.6614) | Xent 0.8982(0.8930) | Loss 9.0680(9.5985) | Error 0.3184(0.3194) Steps 610(621.97) | Grad Norm 11.4993(6.2914) | Total Time 0.00(0.00)\n",
      "Iter 1912 | Time 54.6391(59.6144) | Bit/dim 3.6647(3.6615) | Xent 0.9257(0.8940) | Loss 9.0818(9.5830) | Error 0.3221(0.3195) Steps 604(621.43) | Grad Norm 8.8440(6.3679) | Total Time 0.00(0.00)\n",
      "Iter 1913 | Time 62.8710(59.7121) | Bit/dim 3.6573(3.6613) | Xent 0.8765(0.8934) | Loss 8.9856(9.5650) | Error 0.3129(0.3193) Steps 610(621.09) | Grad Norm 7.7674(6.4099) | Total Time 0.00(0.00)\n",
      "Iter 1914 | Time 62.8898(59.8075) | Bit/dim 3.6668(3.6615) | Xent 0.8789(0.8930) | Loss 8.9676(9.5471) | Error 0.3189(0.3193) Steps 610(620.75) | Grad Norm 7.0326(6.4286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 23.3129, Epoch Time 398.5439(401.0707), Bit/dim 3.6554(best: 3.6579), Xent 0.8903, Loss 4.1006, Error 0.3146(best: 0.3175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1915 | Time 65.8847(59.9898) | Bit/dim 3.6576(3.6614) | Xent 0.8744(0.8925) | Loss 12.2457(9.6281) | Error 0.3024(0.3187) Steps 658(621.87) | Grad Norm 4.1832(6.3612) | Total Time 0.00(0.00)\n",
      "Iter 1916 | Time 56.8827(59.8966) | Bit/dim 3.6522(3.6611) | Xent 0.8889(0.8923) | Loss 8.8367(9.6043) | Error 0.3154(0.3186) Steps 598(621.16) | Grad Norm 6.8360(6.3755) | Total Time 0.00(0.00)\n",
      "Iter 1917 | Time 53.9375(59.7178) | Bit/dim 3.6641(3.6612) | Xent 0.8750(0.8918) | Loss 9.0970(9.5891) | Error 0.3111(0.3184) Steps 598(620.46) | Grad Norm 3.8867(6.3008) | Total Time 0.00(0.00)\n",
      "Iter 1918 | Time 59.8211(59.7209) | Bit/dim 3.6694(3.6615) | Xent 0.8651(0.8910) | Loss 9.1340(9.5755) | Error 0.3184(0.3184) Steps 634(620.87) | Grad Norm 8.6275(6.3706) | Total Time 0.00(0.00)\n",
      "Iter 1919 | Time 57.8456(59.6646) | Bit/dim 3.6478(3.6610) | Xent 0.8714(0.8904) | Loss 9.0427(9.5595) | Error 0.3137(0.3183) Steps 604(620.36) | Grad Norm 6.1833(6.3650) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 57.8512(59.6102) | Bit/dim 3.6635(3.6611) | Xent 0.8947(0.8906) | Loss 9.0149(9.5431) | Error 0.3207(0.3184) Steps 610(620.05) | Grad Norm 10.4624(6.4879) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 22.9459, Epoch Time 391.0854(400.7712), Bit/dim 3.6614(best: 3.6554), Xent 0.9132, Loss 4.1180, Error 0.3227(best: 0.3146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1921 | Time 58.5805(59.5794) | Bit/dim 3.6594(3.6611) | Xent 0.8829(0.8903) | Loss 12.3968(9.6287) | Error 0.3191(0.3184) Steps 634(620.47) | Grad Norm 8.1500(6.5378) | Total Time 0.00(0.00)\n",
      "Iter 1922 | Time 57.7416(59.5242) | Bit/dim 3.6656(3.6612) | Xent 0.9094(0.8909) | Loss 9.1967(9.6158) | Error 0.3277(0.3187) Steps 598(619.79) | Grad Norm 9.9078(6.6389) | Total Time 0.00(0.00)\n",
      "Iter 1923 | Time 63.1018(59.6315) | Bit/dim 3.6646(3.6613) | Xent 0.8786(0.8905) | Loss 8.8854(9.5939) | Error 0.3161(0.3186) Steps 598(619.14) | Grad Norm 8.5896(6.6974) | Total Time 0.00(0.00)\n",
      "Iter 1924 | Time 57.6117(59.5709) | Bit/dim 3.6597(3.6613) | Xent 0.8598(0.8896) | Loss 8.9183(9.5736) | Error 0.3096(0.3183) Steps 634(619.59) | Grad Norm 4.8903(6.6432) | Total Time 0.00(0.00)\n",
      "Iter 1925 | Time 60.5135(59.5992) | Bit/dim 3.6551(3.6611) | Xent 0.9021(0.8900) | Loss 9.0318(9.5573) | Error 0.3193(0.3183) Steps 652(620.56) | Grad Norm 6.8679(6.6499) | Total Time 0.00(0.00)\n",
      "Iter 1926 | Time 58.2038(59.5574) | Bit/dim 3.6493(3.6607) | Xent 0.8462(0.8887) | Loss 8.7207(9.5322) | Error 0.3013(0.3178) Steps 628(620.78) | Grad Norm 3.4480(6.5539) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 23.4172, Epoch Time 395.0587(400.5998), Bit/dim 3.6632(best: 3.6554), Xent 0.8869, Loss 4.1066, Error 0.3163(best: 0.3146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1927 | Time 55.9357(59.4487) | Bit/dim 3.6630(3.6608) | Xent 0.8580(0.8878) | Loss 11.9764(9.6056) | Error 0.3080(0.3175) Steps 592(619.92) | Grad Norm 5.3189(6.5168) | Total Time 0.00(0.00)\n",
      "Iter 1928 | Time 59.2831(59.4437) | Bit/dim 3.6484(3.6604) | Xent 0.8829(0.8876) | Loss 9.0187(9.5880) | Error 0.3149(0.3175) Steps 610(619.62) | Grad Norm 3.6275(6.4302) | Total Time 0.00(0.00)\n",
      "Iter 1929 | Time 57.7081(59.3917) | Bit/dim 3.6550(3.6602) | Xent 0.8702(0.8871) | Loss 8.9233(9.5680) | Error 0.3120(0.3173) Steps 634(620.05) | Grad Norm 5.4401(6.4005) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 61.1213(59.4436) | Bit/dim 3.6615(3.6603) | Xent 0.8584(0.8862) | Loss 8.9017(9.5480) | Error 0.3055(0.3169) Steps 616(619.93) | Grad Norm 2.8176(6.2930) | Total Time 0.00(0.00)\n",
      "Iter 1931 | Time 61.0563(59.4919) | Bit/dim 3.6657(3.6604) | Xent 0.8693(0.8857) | Loss 8.9859(9.5312) | Error 0.3070(0.3166) Steps 640(620.53) | Grad Norm 6.1828(6.2897) | Total Time 0.00(0.00)\n",
      "Iter 1932 | Time 58.5387(59.4633) | Bit/dim 3.6537(3.6602) | Xent 0.8565(0.8848) | Loss 9.1097(9.5185) | Error 0.3084(0.3164) Steps 634(620.94) | Grad Norm 3.2595(6.1988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 22.9770, Epoch Time 392.7928(400.3656), Bit/dim 3.6580(best: 3.6554), Xent 0.8801, Loss 4.0981, Error 0.3083(best: 0.3146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1933 | Time 57.7040(59.4106) | Bit/dim 3.6507(3.6600) | Xent 0.8424(0.8836) | Loss 12.1011(9.5960) | Error 0.2995(0.3159) Steps 616(620.79) | Grad Norm 3.9166(6.1303) | Total Time 0.00(0.00)\n",
      "Iter 1934 | Time 57.9844(59.3678) | Bit/dim 3.6712(3.6603) | Xent 0.8700(0.8832) | Loss 9.0833(9.5806) | Error 0.3131(0.3158) Steps 640(621.37) | Grad Norm 3.1241(6.0401) | Total Time 0.00(0.00)\n",
      "Iter 1935 | Time 53.8156(59.2012) | Bit/dim 3.6493(3.6600) | Xent 0.8594(0.8825) | Loss 8.8850(9.5598) | Error 0.3087(0.3156) Steps 598(620.66) | Grad Norm 4.0545(5.9805) | Total Time 0.00(0.00)\n",
      "Iter 1936 | Time 60.2433(59.2325) | Bit/dim 3.6678(3.6602) | Xent 0.8566(0.8817) | Loss 9.0716(9.5451) | Error 0.3080(0.3154) Steps 604(620.16) | Grad Norm 4.1676(5.9261) | Total Time 0.00(0.00)\n",
      "Iter 1937 | Time 58.1707(59.2006) | Bit/dim 3.6592(3.6602) | Xent 0.8766(0.8815) | Loss 8.8476(9.5242) | Error 0.3156(0.3154) Steps 622(620.22) | Grad Norm 4.9994(5.8983) | Total Time 0.00(0.00)\n",
      "Iter 1938 | Time 60.9746(59.2538) | Bit/dim 3.6511(3.6599) | Xent 0.8837(0.8816) | Loss 9.2159(9.5149) | Error 0.3181(0.3155) Steps 610(619.91) | Grad Norm 5.0201(5.8720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 23.0865, Epoch Time 388.2112(400.0010), Bit/dim 3.6580(best: 3.6554), Xent 0.8900, Loss 4.1031, Error 0.3155(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1939 | Time 64.9572(59.4249) | Bit/dim 3.6646(3.6600) | Xent 0.8645(0.8811) | Loss 12.3419(9.5997) | Error 0.3126(0.3154) Steps 592(619.08) | Grad Norm 4.2600(5.8236) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 56.8881(59.3488) | Bit/dim 3.6596(3.6600) | Xent 0.8707(0.8808) | Loss 8.8953(9.5786) | Error 0.3074(0.3151) Steps 616(618.98) | Grad Norm 5.3932(5.8107) | Total Time 0.00(0.00)\n",
      "Iter 1941 | Time 64.2187(59.4949) | Bit/dim 3.6561(3.6599) | Xent 0.8418(0.8796) | Loss 9.0160(9.5617) | Error 0.2995(0.3147) Steps 604(618.53) | Grad Norm 3.2754(5.7347) | Total Time 0.00(0.00)\n",
      "Iter 1942 | Time 57.1068(59.4233) | Bit/dim 3.6575(3.6598) | Xent 0.8687(0.8793) | Loss 8.9477(9.5433) | Error 0.3083(0.3145) Steps 616(618.46) | Grad Norm 4.1618(5.6875) | Total Time 0.00(0.00)\n",
      "Iter 1943 | Time 60.9243(59.4683) | Bit/dim 3.6505(3.6596) | Xent 0.8790(0.8793) | Loss 9.0694(9.5291) | Error 0.3167(0.3145) Steps 646(619.28) | Grad Norm 8.4111(5.7692) | Total Time 0.00(0.00)\n",
      "Iter 1944 | Time 61.8253(59.5390) | Bit/dim 3.6445(3.6591) | Xent 0.8894(0.8796) | Loss 8.8017(9.5073) | Error 0.3144(0.3145) Steps 616(619.19) | Grad Norm 9.9034(5.8932) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 23.2441, Epoch Time 405.4921(400.1657), Bit/dim 3.6617(best: 3.6554), Xent 0.9381, Loss 4.1307, Error 0.3354(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1945 | Time 63.4670(59.6569) | Bit/dim 3.6628(3.6592) | Xent 0.9089(0.8804) | Loss 12.4150(9.5945) | Error 0.3229(0.3148) Steps 640(619.81) | Grad Norm 9.1305(5.9903) | Total Time 0.00(0.00)\n",
      "Iter 1946 | Time 64.4229(59.7998) | Bit/dim 3.6670(3.6594) | Xent 0.8829(0.8805) | Loss 9.0611(9.5785) | Error 0.3206(0.3150) Steps 592(618.98) | Grad Norm 7.4769(6.0349) | Total Time 0.00(0.00)\n",
      "Iter 1947 | Time 57.1754(59.7211) | Bit/dim 3.6471(3.6591) | Xent 0.9017(0.8812) | Loss 9.0859(9.5637) | Error 0.3284(0.3154) Steps 622(619.07) | Grad Norm 5.6906(6.0246) | Total Time 0.00(0.00)\n",
      "Iter 1948 | Time 58.5508(59.6860) | Bit/dim 3.6541(3.6589) | Xent 0.8808(0.8811) | Loss 8.9724(9.5460) | Error 0.3210(0.3155) Steps 616(618.97) | Grad Norm 3.4325(5.9468) | Total Time 0.00(0.00)\n",
      "Iter 1949 | Time 55.0687(59.5475) | Bit/dim 3.6530(3.6588) | Xent 0.8859(0.8813) | Loss 8.9388(9.5278) | Error 0.3195(0.3156) Steps 610(618.71) | Grad Norm 6.6876(5.9691) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 58.6801(59.5215) | Bit/dim 3.6512(3.6585) | Xent 0.8681(0.8809) | Loss 9.0140(9.5124) | Error 0.3033(0.3153) Steps 640(619.34) | Grad Norm 5.9329(5.9680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 22.5167, Epoch Time 396.1766(400.0460), Bit/dim 3.6568(best: 3.6554), Xent 0.8772, Loss 4.0954, Error 0.3116(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1951 | Time 57.6022(59.4639) | Bit/dim 3.6489(3.6582) | Xent 0.8609(0.8803) | Loss 12.3669(9.5980) | Error 0.3065(0.3150) Steps 616(619.24) | Grad Norm 2.0763(5.8512) | Total Time 0.00(0.00)\n",
      "Iter 1952 | Time 59.5519(59.4665) | Bit/dim 3.6669(3.6585) | Xent 0.8557(0.8796) | Loss 9.0344(9.5811) | Error 0.3070(0.3148) Steps 610(618.97) | Grad Norm 6.7080(5.8769) | Total Time 0.00(0.00)\n",
      "Iter 1953 | Time 53.2516(59.2801) | Bit/dim 3.6523(3.6583) | Xent 0.8659(0.8791) | Loss 8.9750(9.5629) | Error 0.3125(0.3147) Steps 610(618.70) | Grad Norm 5.5241(5.8663) | Total Time 0.00(0.00)\n",
      "Iter 1954 | Time 59.7210(59.2933) | Bit/dim 3.6544(3.6582) | Xent 0.8707(0.8789) | Loss 9.2533(9.5536) | Error 0.3074(0.3145) Steps 634(619.16) | Grad Norm 7.9492(5.9288) | Total Time 0.00(0.00)\n",
      "Iter 1955 | Time 52.5454(59.0909) | Bit/dim 3.6689(3.6585) | Xent 0.8638(0.8784) | Loss 8.7671(9.5300) | Error 0.3080(0.3143) Steps 598(618.52) | Grad Norm 8.5586(6.0077) | Total Time 0.00(0.00)\n",
      "Iter 1956 | Time 56.4203(59.0107) | Bit/dim 3.6466(3.6582) | Xent 0.8806(0.8785) | Loss 8.9542(9.5127) | Error 0.3166(0.3144) Steps 604(618.09) | Grad Norm 8.0050(6.0676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 22.8012, Epoch Time 378.3155(399.3941), Bit/dim 3.6613(best: 3.6554), Xent 0.8953, Loss 4.1090, Error 0.3193(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1957 | Time 58.1378(58.9846) | Bit/dim 3.6498(3.6579) | Xent 0.8764(0.8784) | Loss 11.5423(9.5736) | Error 0.3126(0.3143) Steps 610(617.84) | Grad Norm 6.2158(6.0721) | Total Time 0.00(0.00)\n",
      "Iter 1958 | Time 59.7348(59.0071) | Bit/dim 3.6524(3.6577) | Xent 0.8791(0.8785) | Loss 9.1437(9.5607) | Error 0.3144(0.3143) Steps 622(617.97) | Grad Norm 4.2815(6.0184) | Total Time 0.00(0.00)\n",
      "Iter 1959 | Time 64.5416(59.1731) | Bit/dim 3.6598(3.6578) | Xent 0.8389(0.8773) | Loss 9.0696(9.5460) | Error 0.3025(0.3140) Steps 610(617.73) | Grad Norm 4.2667(5.9658) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 60.4166(59.2104) | Bit/dim 3.6550(3.6577) | Xent 0.8594(0.8767) | Loss 8.8700(9.5257) | Error 0.3063(0.3137) Steps 622(617.86) | Grad Norm 5.1507(5.9414) | Total Time 0.00(0.00)\n",
      "Iter 1961 | Time 58.2644(59.1820) | Bit/dim 3.6616(3.6578) | Xent 0.8849(0.8770) | Loss 9.0115(9.5103) | Error 0.3190(0.3139) Steps 604(617.44) | Grad Norm 9.8946(6.0600) | Total Time 0.00(0.00)\n",
      "Iter 1962 | Time 57.3857(59.1281) | Bit/dim 3.6632(3.6580) | Xent 0.8726(0.8769) | Loss 9.0582(9.4967) | Error 0.3117(0.3138) Steps 616(617.40) | Grad Norm 8.5936(6.1360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 22.9585, Epoch Time 397.9883(399.3519), Bit/dim 3.6543(best: 3.6554), Xent 0.8910, Loss 4.0998, Error 0.3158(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1963 | Time 59.9317(59.1522) | Bit/dim 3.6486(3.6577) | Xent 0.8896(0.8772) | Loss 11.8246(9.5666) | Error 0.3215(0.3140) Steps 604(617.00) | Grad Norm 5.8500(6.1274) | Total Time 0.00(0.00)\n",
      "Iter 1964 | Time 55.9744(59.0569) | Bit/dim 3.6595(3.6578) | Xent 0.8656(0.8769) | Loss 9.0253(9.5503) | Error 0.3101(0.3139) Steps 640(617.69) | Grad Norm 10.1384(6.2477) | Total Time 0.00(0.00)\n",
      "Iter 1965 | Time 57.0916(58.9980) | Bit/dim 3.6506(3.6576) | Xent 0.8838(0.8771) | Loss 9.0793(9.5362) | Error 0.3171(0.3140) Steps 622(617.82) | Grad Norm 9.0286(6.3312) | Total Time 0.00(0.00)\n",
      "Iter 1966 | Time 62.4420(59.1013) | Bit/dim 3.6742(3.6581) | Xent 0.8800(0.8772) | Loss 9.1259(9.5239) | Error 0.3146(0.3140) Steps 622(617.94) | Grad Norm 4.7012(6.2823) | Total Time 0.00(0.00)\n",
      "Iter 1967 | Time 54.8784(58.9746) | Bit/dim 3.6434(3.6576) | Xent 0.8748(0.8771) | Loss 8.9549(9.5068) | Error 0.3137(0.3140) Steps 598(617.34) | Grad Norm 5.8046(6.2679) | Total Time 0.00(0.00)\n",
      "Iter 1968 | Time 61.6250(59.0541) | Bit/dim 3.6608(3.6577) | Xent 0.8577(0.8765) | Loss 9.0996(9.4946) | Error 0.3034(0.3137) Steps 616(617.30) | Grad Norm 6.8048(6.2840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 22.7029, Epoch Time 390.5929(399.0892), Bit/dim 3.6564(best: 3.6543), Xent 0.8969, Loss 4.1049, Error 0.3217(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1969 | Time 65.0833(59.2350) | Bit/dim 3.6611(3.6578) | Xent 0.8692(0.8763) | Loss 12.2241(9.5765) | Error 0.3127(0.3137) Steps 628(617.62) | Grad Norm 7.3509(6.3160) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 58.6867(59.2185) | Bit/dim 3.6573(3.6578) | Xent 0.8590(0.8758) | Loss 9.0417(9.5604) | Error 0.3019(0.3133) Steps 640(618.30) | Grad Norm 1.9030(6.1836) | Total Time 0.00(0.00)\n",
      "Iter 1971 | Time 58.5839(59.1995) | Bit/dim 3.6554(3.6577) | Xent 0.8613(0.8753) | Loss 8.9891(9.5433) | Error 0.3063(0.3131) Steps 616(618.23) | Grad Norm 7.0167(6.2086) | Total Time 0.00(0.00)\n",
      "Iter 1972 | Time 60.3483(59.2340) | Bit/dim 3.6515(3.6575) | Xent 0.8392(0.8743) | Loss 9.0659(9.5290) | Error 0.3025(0.3128) Steps 616(618.16) | Grad Norm 6.0150(6.2028) | Total Time 0.00(0.00)\n",
      "Iter 1973 | Time 62.3045(59.3261) | Bit/dim 3.6434(3.6571) | Xent 0.8690(0.8741) | Loss 9.1804(9.5185) | Error 0.3063(0.3126) Steps 628(618.45) | Grad Norm 5.2554(6.1744) | Total Time 0.00(0.00)\n",
      "Iter 1974 | Time 60.5525(59.3629) | Bit/dim 3.6470(3.6568) | Xent 0.8857(0.8745) | Loss 8.9009(9.5000) | Error 0.3237(0.3129) Steps 634(618.92) | Grad Norm 6.5945(6.1870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 22.8639, Epoch Time 404.5405(399.2527), Bit/dim 3.6549(best: 3.6543), Xent 0.8798, Loss 4.0947, Error 0.3138(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1975 | Time 58.4514(59.3355) | Bit/dim 3.6586(3.6569) | Xent 0.8652(0.8742) | Loss 12.0687(9.5771) | Error 0.3069(0.3128) Steps 598(618.29) | Grad Norm 4.9742(6.1506) | Total Time 0.00(0.00)\n",
      "Iter 1976 | Time 60.6665(59.3754) | Bit/dim 3.6532(3.6568) | Xent 0.8652(0.8739) | Loss 9.1543(9.5644) | Error 0.3087(0.3126) Steps 616(618.22) | Grad Norm 5.3510(6.1266) | Total Time 0.00(0.00)\n",
      "Iter 1977 | Time 59.9411(59.3924) | Bit/dim 3.6552(3.6567) | Xent 0.8690(0.8738) | Loss 8.8437(9.5428) | Error 0.3130(0.3126) Steps 610(617.98) | Grad Norm 6.7556(6.1455) | Total Time 0.00(0.00)\n",
      "Iter 1978 | Time 57.2344(59.3277) | Bit/dim 3.6566(3.6567) | Xent 0.8862(0.8741) | Loss 8.7560(9.5192) | Error 0.3159(0.3127) Steps 616(617.92) | Grad Norm 8.5272(6.2170) | Total Time 0.00(0.00)\n",
      "Iter 1979 | Time 58.6834(59.3083) | Bit/dim 3.6668(3.6570) | Xent 0.8507(0.8734) | Loss 9.0498(9.5051) | Error 0.3043(0.3125) Steps 622(618.04) | Grad Norm 6.0698(6.2125) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 59.8020(59.3232) | Bit/dim 3.6640(3.6572) | Xent 0.8620(0.8731) | Loss 8.9032(9.4870) | Error 0.3036(0.3122) Steps 640(618.70) | Grad Norm 10.5822(6.3436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 22.9116, Epoch Time 393.6368(399.0842), Bit/dim 3.6588(best: 3.6543), Xent 0.8811, Loss 4.0993, Error 0.3158(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1981 | Time 61.9571(59.4022) | Bit/dim 3.6612(3.6573) | Xent 0.8614(0.8727) | Loss 12.3506(9.5729) | Error 0.3100(0.3122) Steps 598(618.08) | Grad Norm 7.4730(6.3775) | Total Time 0.00(0.00)\n",
      "Iter 1982 | Time 57.0887(59.3328) | Bit/dim 3.6630(3.6575) | Xent 0.8512(0.8721) | Loss 9.0696(9.5578) | Error 0.3041(0.3119) Steps 622(618.20) | Grad Norm 5.1649(6.3411) | Total Time 0.00(0.00)\n",
      "Iter 1983 | Time 56.7337(59.2548) | Bit/dim 3.6539(3.6574) | Xent 0.8762(0.8722) | Loss 9.0804(9.5435) | Error 0.3164(0.3120) Steps 598(617.59) | Grad Norm 10.4216(6.4635) | Total Time 0.00(0.00)\n",
      "Iter 1984 | Time 58.0476(59.2186) | Bit/dim 3.6474(3.6571) | Xent 0.8771(0.8724) | Loss 8.9504(9.5257) | Error 0.3116(0.3120) Steps 622(617.72) | Grad Norm 8.9363(6.5377) | Total Time 0.00(0.00)\n",
      "Iter 1985 | Time 54.8242(59.0868) | Bit/dim 3.6530(3.6570) | Xent 0.8832(0.8727) | Loss 8.9327(9.5079) | Error 0.3161(0.3122) Steps 640(618.39) | Grad Norm 6.9767(6.5509) | Total Time 0.00(0.00)\n",
      "Iter 1986 | Time 64.6504(59.2537) | Bit/dim 3.6554(3.6569) | Xent 0.8555(0.8722) | Loss 9.0772(9.4950) | Error 0.3079(0.3120) Steps 628(618.68) | Grad Norm 3.8642(6.4703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 23.6771, Epoch Time 393.1989(398.9077), Bit/dim 3.6575(best: 3.6543), Xent 0.8826, Loss 4.0987, Error 0.3160(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 66.4398(59.4692) | Bit/dim 3.6551(3.6569) | Xent 0.8514(0.8715) | Loss 12.2599(9.5779) | Error 0.3079(0.3119) Steps 598(618.06) | Grad Norm 4.0114(6.3965) | Total Time 0.00(0.00)\n",
      "Iter 1988 | Time 59.7517(59.4777) | Bit/dim 3.6607(3.6570) | Xent 0.8724(0.8716) | Loss 8.9073(9.5578) | Error 0.3113(0.3119) Steps 616(618.00) | Grad Norm 2.5351(6.2807) | Total Time 0.00(0.00)\n",
      "Iter 1989 | Time 61.9005(59.5504) | Bit/dim 3.6516(3.6568) | Xent 0.8516(0.8710) | Loss 9.0156(9.5416) | Error 0.3014(0.3116) Steps 616(617.94) | Grad Norm 4.6447(6.2316) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 61.1778(59.5992) | Bit/dim 3.6518(3.6567) | Xent 0.8427(0.8701) | Loss 9.1814(9.5308) | Error 0.3029(0.3113) Steps 646(618.78) | Grad Norm 3.1392(6.1388) | Total Time 0.00(0.00)\n",
      "Iter 1991 | Time 59.7494(59.6037) | Bit/dim 3.6536(3.6566) | Xent 0.8359(0.8691) | Loss 8.9140(9.5122) | Error 0.3026(0.3111) Steps 640(619.42) | Grad Norm 4.2319(6.0816) | Total Time 0.00(0.00)\n",
      "Iter 1992 | Time 58.6985(59.5766) | Bit/dim 3.6580(3.6566) | Xent 0.8586(0.8688) | Loss 8.8165(9.4914) | Error 0.3059(0.3109) Steps 628(619.67) | Grad Norm 2.8079(5.9834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 23.1833, Epoch Time 406.6237(399.1391), Bit/dim 3.6545(best: 3.6543), Xent 0.8752, Loss 4.0921, Error 0.3116(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 55.2213(59.4459) | Bit/dim 3.6569(3.6566) | Xent 0.8616(0.8686) | Loss 11.8066(9.5608) | Error 0.3063(0.3108) Steps 598(619.02) | Grad Norm 3.2365(5.9010) | Total Time 0.00(0.00)\n",
      "Iter 1994 | Time 55.4856(59.3271) | Bit/dim 3.6670(3.6569) | Xent 0.8437(0.8678) | Loss 9.1541(9.5486) | Error 0.3014(0.3105) Steps 604(618.57) | Grad Norm 3.2656(5.8219) | Total Time 0.00(0.00)\n",
      "Iter 1995 | Time 60.3032(59.3564) | Bit/dim 3.6435(3.6565) | Xent 0.8338(0.8668) | Loss 8.8290(9.5270) | Error 0.2957(0.3100) Steps 610(618.32) | Grad Norm 4.1254(5.7710) | Total Time 0.00(0.00)\n",
      "Iter 1996 | Time 55.7928(59.2495) | Bit/dim 3.6506(3.6564) | Xent 0.8653(0.8668) | Loss 9.1076(9.5145) | Error 0.3073(0.3099) Steps 622(618.43) | Grad Norm 4.1641(5.7228) | Total Time 0.00(0.00)\n",
      "Iter 1997 | Time 62.4229(59.3447) | Bit/dim 3.6572(3.6564) | Xent 0.8644(0.8667) | Loss 8.9659(9.4980) | Error 0.3107(0.3100) Steps 604(617.99) | Grad Norm 5.8882(5.7278) | Total Time 0.00(0.00)\n",
      "Iter 1998 | Time 56.6205(59.2630) | Bit/dim 3.6589(3.6565) | Xent 0.8618(0.8665) | Loss 8.9871(9.4827) | Error 0.3109(0.3100) Steps 628(618.29) | Grad Norm 5.4216(5.7186) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 23.0508, Epoch Time 384.7609(398.7078), Bit/dim 3.6559(best: 3.6543), Xent 0.8730, Loss 4.0923, Error 0.3104(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 59.3812(59.2665) | Bit/dim 3.6464(3.6562) | Xent 0.8330(0.8655) | Loss 12.2160(9.5647) | Error 0.2966(0.3096) Steps 646(619.12) | Grad Norm 4.3625(5.6779) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 62.5686(59.3656) | Bit/dim 3.6528(3.6561) | Xent 0.8529(0.8651) | Loss 9.0307(9.5487) | Error 0.2984(0.3093) Steps 646(619.93) | Grad Norm 4.7600(5.6504) | Total Time 0.00(0.00)\n",
      "Iter 2001 | Time 58.6956(59.3455) | Bit/dim 3.6749(3.6566) | Xent 0.8638(0.8651) | Loss 9.1418(9.5364) | Error 0.3126(0.3094) Steps 598(619.27) | Grad Norm 6.5143(5.6763) | Total Time 0.00(0.00)\n",
      "Iter 2002 | Time 59.9467(59.3635) | Bit/dim 3.6555(3.6566) | Xent 0.8915(0.8659) | Loss 8.9063(9.5175) | Error 0.3169(0.3096) Steps 652(620.25) | Grad Norm 9.2347(5.7831) | Total Time 0.00(0.00)\n",
      "Iter 2003 | Time 64.3053(59.5118) | Bit/dim 3.6487(3.6564) | Xent 0.8835(0.8664) | Loss 9.0801(9.5044) | Error 0.3134(0.3097) Steps 658(621.39) | Grad Norm 9.4445(5.8929) | Total Time 0.00(0.00)\n",
      "Iter 2004 | Time 61.4488(59.5699) | Bit/dim 3.6485(3.6561) | Xent 0.8797(0.8668) | Loss 8.8202(9.4839) | Error 0.3136(0.3098) Steps 646(622.13) | Grad Norm 8.1800(5.9615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 23.2072, Epoch Time 405.6417(398.9158), Bit/dim 3.6582(best: 3.6543), Xent 0.9304, Loss 4.1234, Error 0.3308(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 68.0628(59.8247) | Bit/dim 3.6637(3.6563) | Xent 0.9126(0.8682) | Loss 12.2621(9.5672) | Error 0.3276(0.3104) Steps 622(622.12) | Grad Norm 8.3686(6.0337) | Total Time 0.00(0.00)\n",
      "Iter 2006 | Time 57.3195(59.7495) | Bit/dim 3.6475(3.6561) | Xent 0.8762(0.8684) | Loss 8.9690(9.5493) | Error 0.3117(0.3104) Steps 628(622.30) | Grad Norm 5.6010(6.0208) | Total Time 0.00(0.00)\n",
      "Iter 2007 | Time 58.3455(59.7074) | Bit/dim 3.6494(3.6559) | Xent 0.8578(0.8681) | Loss 8.9946(9.5326) | Error 0.3067(0.3103) Steps 634(622.65) | Grad Norm 2.2394(5.9073) | Total Time 0.00(0.00)\n",
      "Iter 2008 | Time 53.6389(59.5253) | Bit/dim 3.6539(3.6558) | Xent 0.8471(0.8675) | Loss 8.8557(9.5123) | Error 0.2999(0.3100) Steps 616(622.45) | Grad Norm 4.6374(5.8692) | Total Time 0.00(0.00)\n",
      "Iter 2009 | Time 61.5834(59.5871) | Bit/dim 3.6563(3.6558) | Xent 0.8744(0.8677) | Loss 8.8713(9.4931) | Error 0.3094(0.3100) Steps 616(622.26) | Grad Norm 5.2829(5.8516) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 53.8314(59.4144) | Bit/dim 3.6474(3.6556) | Xent 0.8452(0.8670) | Loss 8.9875(9.4779) | Error 0.3031(0.3098) Steps 592(621.35) | Grad Norm 4.1021(5.7991) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 22.9920, Epoch Time 391.5681(398.6954), Bit/dim 3.6574(best: 3.6543), Xent 0.8870, Loss 4.1009, Error 0.3139(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 62.9944(59.5218) | Bit/dim 3.6460(3.6553) | Xent 0.8776(0.8673) | Loss 12.1437(9.5579) | Error 0.3166(0.3100) Steps 610(621.01) | Grad Norm 5.7219(5.7968) | Total Time 0.00(0.00)\n",
      "Iter 2012 | Time 60.2245(59.5429) | Bit/dim 3.6538(3.6552) | Xent 0.8505(0.8668) | Loss 9.0983(9.5441) | Error 0.3054(0.3098) Steps 616(620.86) | Grad Norm 6.8429(5.8282) | Total Time 0.00(0.00)\n",
      "Iter 2013 | Time 60.4295(59.5695) | Bit/dim 3.6466(3.6550) | Xent 0.8654(0.8668) | Loss 8.8836(9.5243) | Error 0.3097(0.3098) Steps 652(621.79) | Grad Norm 6.6497(5.8529) | Total Time 0.00(0.00)\n",
      "Iter 2014 | Time 60.6451(59.6018) | Bit/dim 3.6560(3.6550) | Xent 0.8675(0.8668) | Loss 9.1430(9.5129) | Error 0.3091(0.3098) Steps 622(621.80) | Grad Norm 5.6728(5.8475) | Total Time 0.00(0.00)\n",
      "Iter 2015 | Time 59.4396(59.5969) | Bit/dim 3.6591(3.6551) | Xent 0.8500(0.8663) | Loss 8.9860(9.4971) | Error 0.3050(0.3097) Steps 634(622.16) | Grad Norm 4.0920(5.7948) | Total Time 0.00(0.00)\n",
      "Iter 2016 | Time 61.5028(59.6541) | Bit/dim 3.6653(3.6554) | Xent 0.8495(0.8658) | Loss 9.0349(9.4832) | Error 0.3027(0.3094) Steps 616(621.98) | Grad Norm 4.4075(5.7532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 23.5697, Epoch Time 405.1130(398.8879), Bit/dim 3.6537(best: 3.6543), Xent 0.8974, Loss 4.1024, Error 0.3186(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 59.2716(59.6426) | Bit/dim 3.6422(3.6550) | Xent 0.8888(0.8665) | Loss 12.0482(9.5601) | Error 0.3104(0.3095) Steps 610(621.62) | Grad Norm 9.6162(5.8691) | Total Time 0.00(0.00)\n",
      "Iter 2018 | Time 55.7922(59.5271) | Bit/dim 3.6556(3.6551) | Xent 0.8670(0.8665) | Loss 9.0232(9.5440) | Error 0.3127(0.3096) Steps 604(621.09) | Grad Norm 9.1456(5.9674) | Total Time 0.00(0.00)\n",
      "Iter 2019 | Time 61.3001(59.5803) | Bit/dim 3.6621(3.6553) | Xent 0.8341(0.8655) | Loss 9.0111(9.5280) | Error 0.2990(0.3093) Steps 634(621.48) | Grad Norm 3.8557(5.9040) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 63.7908(59.7066) | Bit/dim 3.6532(3.6552) | Xent 0.8465(0.8650) | Loss 9.0513(9.5137) | Error 0.3037(0.3091) Steps 622(621.49) | Grad Norm 7.2666(5.9449) | Total Time 0.00(0.00)\n",
      "Iter 2021 | Time 55.0599(59.5672) | Bit/dim 3.6640(3.6555) | Xent 0.8557(0.8647) | Loss 8.9824(9.4978) | Error 0.2990(0.3088) Steps 622(621.51) | Grad Norm 8.8604(6.0323) | Total Time 0.00(0.00)\n",
      "Iter 2022 | Time 57.4820(59.5046) | Bit/dim 3.6584(3.6556) | Xent 0.8739(0.8650) | Loss 8.9707(9.4820) | Error 0.3113(0.3089) Steps 610(621.16) | Grad Norm 6.0437(6.0327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 23.3242, Epoch Time 392.2302(398.6882), Bit/dim 3.6562(best: 3.6537), Xent 0.8729, Loss 4.0927, Error 0.3124(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 55.4677(59.3835) | Bit/dim 3.6638(3.6558) | Xent 0.8460(0.8644) | Loss 12.2192(9.5641) | Error 0.3105(0.3089) Steps 628(621.37) | Grad Norm 3.5197(5.9573) | Total Time 0.00(0.00)\n",
      "Iter 2024 | Time 57.7127(59.3334) | Bit/dim 3.6497(3.6556) | Xent 0.8376(0.8636) | Loss 8.9389(9.5454) | Error 0.3014(0.3087) Steps 592(620.49) | Grad Norm 4.4426(5.9119) | Total Time 0.00(0.00)\n",
      "Iter 2025 | Time 57.6437(59.2827) | Bit/dim 3.6411(3.6552) | Xent 0.8703(0.8638) | Loss 8.9569(9.5277) | Error 0.3121(0.3088) Steps 604(619.99) | Grad Norm 6.4852(5.9291) | Total Time 0.00(0.00)\n",
      "Iter 2026 | Time 59.8626(59.3001) | Bit/dim 3.6555(3.6552) | Xent 0.8547(0.8635) | Loss 8.8808(9.5083) | Error 0.3025(0.3086) Steps 622(620.05) | Grad Norm 7.7140(5.9826) | Total Time 0.00(0.00)\n",
      "Iter 2027 | Time 58.1997(59.2671) | Bit/dim 3.6798(3.6559) | Xent 0.8856(0.8642) | Loss 8.9948(9.4929) | Error 0.3169(0.3088) Steps 610(619.75) | Grad Norm 7.0877(6.0158) | Total Time 0.00(0.00)\n",
      "Iter 2028 | Time 60.2197(59.2957) | Bit/dim 3.6562(3.6559) | Xent 0.8846(0.8648) | Loss 9.0918(9.4809) | Error 0.3190(0.3092) Steps 598(619.10) | Grad Norm 13.0396(6.2265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 23.8277, Epoch Time 389.2885(398.4062), Bit/dim 3.6696(best: 3.6537), Xent 0.9230, Loss 4.1311, Error 0.3270(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 67.0717(59.5290) | Bit/dim 3.6789(3.6566) | Xent 0.8948(0.8657) | Loss 12.0168(9.5569) | Error 0.3239(0.3096) Steps 640(619.73) | Grad Norm 14.7965(6.4836) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 59.1907(59.5188) | Bit/dim 3.6547(3.6566) | Xent 0.8972(0.8666) | Loss 9.1090(9.5435) | Error 0.3241(0.3100) Steps 622(619.79) | Grad Norm 8.9747(6.5583) | Total Time 0.00(0.00)\n",
      "Iter 2031 | Time 59.4977(59.5182) | Bit/dim 3.6507(3.6564) | Xent 0.8730(0.8668) | Loss 9.0406(9.5284) | Error 0.3139(0.3101) Steps 604(619.32) | Grad Norm 4.8195(6.5061) | Total Time 0.00(0.00)\n",
      "Iter 2032 | Time 60.6568(59.5523) | Bit/dim 3.6530(3.6563) | Xent 0.8979(0.8678) | Loss 9.1104(9.5159) | Error 0.3194(0.3104) Steps 646(620.12) | Grad Norm 7.4305(6.5339) | Total Time 0.00(0.00)\n",
      "Iter 2033 | Time 58.8373(59.5309) | Bit/dim 3.6490(3.6561) | Xent 0.8837(0.8682) | Loss 9.0300(9.5013) | Error 0.3199(0.3107) Steps 598(619.46) | Grad Norm 10.6959(6.6587) | Total Time 0.00(0.00)\n",
      "Iter 2034 | Time 56.0845(59.4275) | Bit/dim 3.6683(3.6564) | Xent 0.8490(0.8677) | Loss 8.9330(9.4842) | Error 0.3015(0.3104) Steps 622(619.53) | Grad Norm 6.7330(6.6610) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 22.6741, Epoch Time 400.0107(398.4543), Bit/dim 3.6581(best: 3.6537), Xent 0.8956, Loss 4.1059, Error 0.3167(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 57.1259(59.3584) | Bit/dim 3.6551(3.6564) | Xent 0.8712(0.8678) | Loss 12.0104(9.5600) | Error 0.3120(0.3105) Steps 616(619.43) | Grad Norm 6.8027(6.6652) | Total Time 0.00(0.00)\n",
      "Iter 2036 | Time 56.4524(59.2713) | Bit/dim 3.6607(3.6565) | Xent 0.8881(0.8684) | Loss 9.0576(9.5450) | Error 0.3220(0.3108) Steps 640(620.05) | Grad Norm 10.2136(6.7717) | Total Time 0.00(0.00)\n",
      "Iter 2037 | Time 61.4552(59.3368) | Bit/dim 3.6573(3.6566) | Xent 0.8530(0.8679) | Loss 8.9884(9.5283) | Error 0.3014(0.3105) Steps 634(620.46) | Grad Norm 4.1466(6.6929) | Total Time 0.00(0.00)\n",
      "Iter 2038 | Time 53.6548(59.1663) | Bit/dim 3.6563(3.6566) | Xent 0.8507(0.8674) | Loss 8.7472(9.5048) | Error 0.3066(0.3104) Steps 592(619.61) | Grad Norm 9.5875(6.7798) | Total Time 0.00(0.00)\n",
      "Iter 2039 | Time 60.0622(59.1932) | Bit/dim 3.6641(3.6568) | Xent 0.8763(0.8677) | Loss 9.0987(9.4926) | Error 0.3140(0.3105) Steps 640(620.22) | Grad Norm 5.3591(6.7371) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 57.6991(59.1484) | Bit/dim 3.6552(3.6567) | Xent 0.8658(0.8676) | Loss 8.9357(9.4759) | Error 0.3096(0.3105) Steps 616(620.09) | Grad Norm 5.1689(6.6901) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 22.1852, Epoch Time 385.1071(398.0539), Bit/dim 3.6597(best: 3.6537), Xent 0.8909, Loss 4.1052, Error 0.3196(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 58.0711(59.1161) | Bit/dim 3.6667(3.6570) | Xent 0.8791(0.8680) | Loss 12.0501(9.5532) | Error 0.3199(0.3108) Steps 598(619.43) | Grad Norm 6.2160(6.6759) | Total Time 0.00(0.00)\n",
      "Iter 2042 | Time 64.4839(59.2771) | Bit/dim 3.6531(3.6569) | Xent 0.8354(0.8670) | Loss 9.1211(9.5402) | Error 0.2984(0.3104) Steps 670(620.95) | Grad Norm 3.5193(6.5812) | Total Time 0.00(0.00)\n",
      "Iter 2043 | Time 57.3585(59.2195) | Bit/dim 3.6463(3.6566) | Xent 0.8437(0.8663) | Loss 8.8946(9.5208) | Error 0.3057(0.3103) Steps 598(620.26) | Grad Norm 4.7923(6.5275) | Total Time 0.00(0.00)\n",
      "Iter 2044 | Time 58.1783(59.1883) | Bit/dim 3.6566(3.6566) | Xent 0.8880(0.8669) | Loss 9.0453(9.5066) | Error 0.3200(0.3106) Steps 586(619.23) | Grad Norm 8.5135(6.5871) | Total Time 0.00(0.00)\n",
      "Iter 2045 | Time 55.8841(59.0892) | Bit/dim 3.6485(3.6564) | Xent 0.8674(0.8670) | Loss 9.0149(9.4918) | Error 0.3059(0.3104) Steps 628(619.50) | Grad Norm 7.9791(6.6288) | Total Time 0.00(0.00)\n",
      "Iter 2046 | Time 58.5854(59.0741) | Bit/dim 3.6618(3.6565) | Xent 0.8628(0.8668) | Loss 8.8701(9.4732) | Error 0.3090(0.3104) Steps 634(619.93) | Grad Norm 10.5042(6.7451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 22.9502, Epoch Time 391.7382(397.8644), Bit/dim 3.6710(best: 3.6537), Xent 0.9192, Loss 4.1306, Error 0.3292(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 56.7733(59.0050) | Bit/dim 3.6607(3.6566) | Xent 0.8827(0.8673) | Loss 12.2794(9.5573) | Error 0.3143(0.3105) Steps 610(619.63) | Grad Norm 15.1013(6.9958) | Total Time 0.00(0.00)\n",
      "Iter 2048 | Time 62.0728(59.0971) | Bit/dim 3.6599(3.6567) | Xent 0.8479(0.8667) | Loss 8.8212(9.5353) | Error 0.3011(0.3102) Steps 634(620.06) | Grad Norm 7.1472(7.0003) | Total Time 0.00(0.00)\n",
      "Iter 2049 | Time 59.4709(59.1083) | Bit/dim 3.6668(3.6570) | Xent 0.8577(0.8665) | Loss 8.9795(9.5186) | Error 0.3163(0.3104) Steps 616(619.94) | Grad Norm 7.7494(7.0228) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 58.9353(59.1031) | Bit/dim 3.6459(3.6567) | Xent 0.8686(0.8665) | Loss 8.7272(9.4948) | Error 0.3157(0.3106) Steps 622(620.00) | Grad Norm 5.1562(6.9668) | Total Time 0.00(0.00)\n",
      "Iter 2051 | Time 61.8427(59.1853) | Bit/dim 3.6573(3.6567) | Xent 0.8562(0.8662) | Loss 9.1970(9.4859) | Error 0.3091(0.3105) Steps 640(620.60) | Grad Norm 6.1156(6.9413) | Total Time 0.00(0.00)\n",
      "Iter 2052 | Time 59.7402(59.2019) | Bit/dim 3.6493(3.6565) | Xent 0.8585(0.8660) | Loss 9.0872(9.4739) | Error 0.3071(0.3104) Steps 622(620.65) | Grad Norm 5.2222(6.8897) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 22.5776, Epoch Time 397.5721(397.8557), Bit/dim 3.6532(best: 3.6537), Xent 0.8838, Loss 4.0952, Error 0.3170(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 61.4781(59.2702) | Bit/dim 3.6555(3.6565) | Xent 0.8601(0.8658) | Loss 12.1459(9.5541) | Error 0.3113(0.3104) Steps 604(620.15) | Grad Norm 5.1676(6.8380) | Total Time 0.00(0.00)\n",
      "Iter 2054 | Time 57.0196(59.2027) | Bit/dim 3.6432(3.6561) | Xent 0.8446(0.8652) | Loss 9.0810(9.5399) | Error 0.2940(0.3099) Steps 640(620.74) | Grad Norm 4.1837(6.7584) | Total Time 0.00(0.00)\n",
      "Iter 2055 | Time 55.0822(59.0791) | Bit/dim 3.6537(3.6560) | Xent 0.8468(0.8646) | Loss 9.1565(9.5284) | Error 0.3087(0.3099) Steps 634(621.14) | Grad Norm 4.1318(6.6796) | Total Time 0.00(0.00)\n",
      "Iter 2056 | Time 60.1169(59.1102) | Bit/dim 3.6584(3.6561) | Xent 0.8560(0.8644) | Loss 9.0761(9.5148) | Error 0.3069(0.3098) Steps 604(620.63) | Grad Norm 4.8571(6.6249) | Total Time 0.00(0.00)\n",
      "Iter 2057 | Time 54.7491(58.9794) | Bit/dim 3.6456(3.6558) | Xent 0.8445(0.8638) | Loss 8.9460(9.4978) | Error 0.2973(0.3094) Steps 610(620.31) | Grad Norm 5.5615(6.5930) | Total Time 0.00(0.00)\n",
      "Iter 2058 | Time 56.6970(58.9109) | Bit/dim 3.6664(3.6561) | Xent 0.8698(0.8639) | Loss 9.1081(9.4861) | Error 0.3075(0.3094) Steps 610(620.00) | Grad Norm 5.5849(6.5628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 22.7428, Epoch Time 384.1187(397.4436), Bit/dim 3.6537(best: 3.6532), Xent 0.8623, Loss 4.0848, Error 0.3055(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 60.5702(58.9607) | Bit/dim 3.6397(3.6556) | Xent 0.8465(0.8634) | Loss 12.3516(9.5721) | Error 0.2996(0.3091) Steps 634(620.42) | Grad Norm 3.6892(6.4766) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 55.8286(58.8667) | Bit/dim 3.6548(3.6556) | Xent 0.8392(0.8627) | Loss 8.9898(9.5546) | Error 0.3017(0.3089) Steps 616(620.28) | Grad Norm 5.0586(6.4340) | Total Time 0.00(0.00)\n",
      "Iter 2061 | Time 59.4932(58.8855) | Bit/dim 3.6509(3.6554) | Xent 0.8500(0.8623) | Loss 9.0453(9.5393) | Error 0.3071(0.3088) Steps 616(620.16) | Grad Norm 4.9278(6.3889) | Total Time 0.00(0.00)\n",
      "Iter 2062 | Time 62.5016(58.9940) | Bit/dim 3.6537(3.6554) | Xent 0.8033(0.8605) | Loss 8.9577(9.5219) | Error 0.2853(0.3081) Steps 670(621.65) | Grad Norm 4.3405(6.3274) | Total Time 0.00(0.00)\n",
      "Iter 2063 | Time 56.3839(58.9157) | Bit/dim 3.6546(3.6553) | Xent 0.8574(0.8604) | Loss 8.9358(9.5043) | Error 0.3101(0.3082) Steps 622(621.66) | Grad Norm 5.0851(6.2901) | Total Time 0.00(0.00)\n",
      "Iter 2064 | Time 53.3991(58.7502) | Bit/dim 3.6570(3.6554) | Xent 0.8365(0.8597) | Loss 8.9393(9.4873) | Error 0.3045(0.3081) Steps 604(621.13) | Grad Norm 5.4294(6.2643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 23.2159, Epoch Time 387.6011(397.1483), Bit/dim 3.6543(best: 3.6532), Xent 0.8612, Loss 4.0850, Error 0.3054(best: 0.3055)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 55.5149(58.6531) | Bit/dim 3.6621(3.6556) | Xent 0.8396(0.8591) | Loss 11.8452(9.5581) | Error 0.3031(0.3079) Steps 598(620.44) | Grad Norm 2.7579(6.1591) | Total Time 0.00(0.00)\n",
      "Iter 2066 | Time 55.2936(58.5524) | Bit/dim 3.6512(3.6555) | Xent 0.8429(0.8586) | Loss 9.0255(9.5421) | Error 0.2955(0.3075) Steps 604(619.95) | Grad Norm 5.9061(6.1515) | Total Time 0.00(0.00)\n",
      "Iter 2067 | Time 54.8422(58.4411) | Bit/dim 3.6513(3.6553) | Xent 0.8447(0.8582) | Loss 8.8032(9.5199) | Error 0.2989(0.3073) Steps 610(619.65) | Grad Norm 9.3361(6.2471) | Total Time 0.00(0.00)\n",
      "Iter 2068 | Time 58.4123(58.4402) | Bit/dim 3.6377(3.6548) | Xent 0.8433(0.8578) | Loss 9.0423(9.5056) | Error 0.3006(0.3071) Steps 610(619.36) | Grad Norm 5.2812(6.2181) | Total Time 0.00(0.00)\n",
      "Iter 2069 | Time 61.4239(58.5297) | Bit/dim 3.6535(3.6548) | Xent 0.8278(0.8569) | Loss 9.0626(9.4923) | Error 0.2920(0.3066) Steps 622(619.44) | Grad Norm 3.5968(6.1395) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 55.9433(58.4521) | Bit/dim 3.6631(3.6550) | Xent 0.8602(0.8570) | Loss 8.6913(9.4683) | Error 0.3097(0.3067) Steps 616(619.33) | Grad Norm 5.9903(6.1350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 23.5292, Epoch Time 380.9328(396.6618), Bit/dim 3.6559(best: 3.6532), Xent 0.8790, Loss 4.0954, Error 0.3131(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 60.5430(58.5148) | Bit/dim 3.6555(3.6550) | Xent 0.8525(0.8568) | Loss 11.9615(9.5431) | Error 0.3059(0.3067) Steps 598(618.69) | Grad Norm 5.5128(6.1163) | Total Time 0.00(0.00)\n",
      "Iter 2072 | Time 58.2569(58.5071) | Bit/dim 3.6399(3.6546) | Xent 0.8354(0.8562) | Loss 8.9931(9.5266) | Error 0.2985(0.3064) Steps 634(619.15) | Grad Norm 4.5059(6.0680) | Total Time 0.00(0.00)\n",
      "Iter 2073 | Time 58.0110(58.4922) | Bit/dim 3.6602(3.6547) | Xent 0.8609(0.8563) | Loss 8.9213(9.5084) | Error 0.3039(0.3064) Steps 640(619.78) | Grad Norm 11.1717(6.2211) | Total Time 0.00(0.00)\n",
      "Iter 2074 | Time 57.5417(58.4637) | Bit/dim 3.6555(3.6548) | Xent 0.8869(0.8572) | Loss 9.0400(9.4944) | Error 0.3205(0.3068) Steps 616(619.66) | Grad Norm 13.1421(6.4287) | Total Time 0.00(0.00)\n",
      "Iter 2075 | Time 51.2240(58.2465) | Bit/dim 3.6570(3.6548) | Xent 0.8833(0.8580) | Loss 8.7474(9.4720) | Error 0.3133(0.3070) Steps 604(619.19) | Grad Norm 9.6597(6.5257) | Total Time 0.00(0.00)\n",
      "Iter 2076 | Time 56.4217(58.1918) | Bit/dim 3.6625(3.6551) | Xent 0.9569(0.8610) | Loss 8.8523(9.4534) | Error 0.3375(0.3079) Steps 616(619.10) | Grad Norm 16.2669(6.8179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 23.0447, Epoch Time 381.1577(396.1967), Bit/dim 3.6743(best: 3.6532), Xent 0.9487, Loss 4.1487, Error 0.3342(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 60.1186(58.2496) | Bit/dim 3.6740(3.6556) | Xent 0.9287(0.8630) | Loss 12.3222(9.5394) | Error 0.3346(0.3087) Steps 616(619.01) | Grad Norm 14.6538(7.0530) | Total Time 0.00(0.00)\n",
      "Iter 2078 | Time 58.8097(58.2664) | Bit/dim 3.6636(3.6559) | Xent 0.9249(0.8649) | Loss 9.1311(9.5272) | Error 0.3363(0.3095) Steps 640(619.64) | Grad Norm 11.2665(7.1794) | Total Time 0.00(0.00)\n",
      "Iter 2079 | Time 58.3721(58.2696) | Bit/dim 3.6654(3.6562) | Xent 0.9385(0.8671) | Loss 9.1504(9.5159) | Error 0.3335(0.3103) Steps 598(618.99) | Grad Norm 18.8432(7.5293) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 56.0655(58.2034) | Bit/dim 3.6741(3.6567) | Xent 1.0410(0.8723) | Loss 9.1399(9.5046) | Error 0.3622(0.3118) Steps 616(618.90) | Grad Norm 21.0056(7.9336) | Total Time 0.00(0.00)\n",
      "Iter 2081 | Time 57.4594(58.1811) | Bit/dim 3.6632(3.6569) | Xent 1.0375(0.8773) | Loss 9.1285(9.4933) | Error 0.3714(0.3136) Steps 622(618.99) | Grad Norm 16.6406(8.1948) | Total Time 0.00(0.00)\n",
      "Iter 2082 | Time 56.8458(58.1410) | Bit/dim 3.6675(3.6572) | Xent 1.0305(0.8819) | Loss 9.1443(9.4828) | Error 0.3649(0.3151) Steps 622(619.08) | Grad Norm 10.2717(8.2571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 22.5296, Epoch Time 386.5382(395.9069), Bit/dim 3.6671(best: 3.6532), Xent 1.0081, Loss 4.1712, Error 0.3603(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 57.4575(58.1205) | Bit/dim 3.6617(3.6573) | Xent 0.9969(0.8853) | Loss 12.2014(9.5644) | Error 0.3611(0.3165) Steps 628(619.35) | Grad Norm 9.4968(8.2943) | Total Time 0.00(0.00)\n",
      "Iter 2084 | Time 60.1450(58.1813) | Bit/dim 3.6796(3.6580) | Xent 0.9627(0.8876) | Loss 9.3719(9.5586) | Error 0.3385(0.3172) Steps 604(618.89) | Grad Norm 11.3505(8.3860) | Total Time 0.00(0.00)\n",
      "Iter 2085 | Time 55.7922(58.1096) | Bit/dim 3.6746(3.6585) | Xent 0.9623(0.8899) | Loss 9.2616(9.5497) | Error 0.3451(0.3180) Steps 628(619.16) | Grad Norm 6.5891(8.3321) | Total Time 0.00(0.00)\n",
      "Iter 2086 | Time 54.9496(58.0148) | Bit/dim 3.6738(3.6590) | Xent 0.9045(0.8903) | Loss 9.1255(9.5370) | Error 0.3209(0.3181) Steps 616(619.07) | Grad Norm 11.5871(8.4297) | Total Time 0.00(0.00)\n",
      "Iter 2087 | Time 53.1503(57.8689) | Bit/dim 3.6683(3.6593) | Xent 0.9201(0.8912) | Loss 8.8977(9.5178) | Error 0.3341(0.3186) Steps 616(618.97) | Grad Norm 6.6799(8.3772) | Total Time 0.00(0.00)\n",
      "Iter 2088 | Time 55.6367(57.8019) | Bit/dim 3.6791(3.6598) | Xent 0.9295(0.8924) | Loss 9.1694(9.5074) | Error 0.3353(0.3191) Steps 622(619.07) | Grad Norm 8.0923(8.3687) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 23.1149, Epoch Time 376.3644(395.3207), Bit/dim 3.6654(best: 3.6532), Xent 0.9166, Loss 4.1238, Error 0.3259(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 55.0559(57.7195) | Bit/dim 3.6562(3.6597) | Xent 0.9076(0.8928) | Loss 12.3890(9.5938) | Error 0.3277(0.3193) Steps 604(618.61) | Grad Norm 4.8065(8.2618) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 61.1816(57.8234) | Bit/dim 3.6722(3.6601) | Xent 0.9217(0.8937) | Loss 9.2350(9.5830) | Error 0.3316(0.3197) Steps 610(618.35) | Grad Norm 6.0550(8.1956) | Total Time 0.00(0.00)\n",
      "Iter 2091 | Time 62.3626(57.9596) | Bit/dim 3.6710(3.6604) | Xent 0.8963(0.8938) | Loss 9.2146(9.5720) | Error 0.3143(0.3195) Steps 634(618.82) | Grad Norm 4.8764(8.0960) | Total Time 0.00(0.00)\n",
      "Iter 2092 | Time 59.5412(58.0070) | Bit/dim 3.6718(3.6608) | Xent 0.8822(0.8934) | Loss 8.8855(9.5514) | Error 0.3120(0.3193) Steps 622(618.92) | Grad Norm 5.7465(8.0256) | Total Time 0.00(0.00)\n",
      "Iter 2093 | Time 58.8430(58.0321) | Bit/dim 3.6645(3.6609) | Xent 0.8985(0.8936) | Loss 9.0968(9.5378) | Error 0.3243(0.3195) Steps 628(619.19) | Grad Norm 5.5308(7.9507) | Total Time 0.00(0.00)\n",
      "Iter 2094 | Time 56.6643(57.9910) | Bit/dim 3.6612(3.6609) | Xent 0.8526(0.8923) | Loss 9.0048(9.5218) | Error 0.3034(0.3190) Steps 628(619.46) | Grad Norm 5.5663(7.8792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 23.3607, Epoch Time 393.5946(395.2689), Bit/dim 3.6663(best: 3.6532), Xent 0.8879, Loss 4.1103, Error 0.3181(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 58.8479(58.0168) | Bit/dim 3.6606(3.6609) | Xent 0.8639(0.8915) | Loss 11.9646(9.5951) | Error 0.3073(0.3186) Steps 616(619.35) | Grad Norm 4.9250(7.7905) | Total Time 0.00(0.00)\n",
      "Iter 2096 | Time 56.5518(57.9728) | Bit/dim 3.6550(3.6607) | Xent 0.8746(0.8910) | Loss 9.0766(9.5795) | Error 0.3104(0.3184) Steps 610(619.07) | Grad Norm 4.2062(7.6830) | Total Time 0.00(0.00)\n",
      "Iter 2097 | Time 59.8634(58.0295) | Bit/dim 3.6708(3.6610) | Xent 0.8640(0.8902) | Loss 9.1054(9.5653) | Error 0.3063(0.3180) Steps 610(618.80) | Grad Norm 6.5484(7.6490) | Total Time 0.00(0.00)\n",
      "Iter 2098 | Time 60.7135(58.1100) | Bit/dim 3.6738(3.6614) | Xent 0.8495(0.8889) | Loss 9.0783(9.5507) | Error 0.3066(0.3177) Steps 616(618.72) | Grad Norm 3.2530(7.5171) | Total Time 0.00(0.00)\n",
      "Iter 2099 | Time 55.0008(58.0168) | Bit/dim 3.6687(3.6616) | Xent 0.8609(0.8881) | Loss 8.7716(9.5273) | Error 0.3074(0.3174) Steps 604(618.27) | Grad Norm 3.9301(7.4095) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 66.0774(58.2586) | Bit/dim 3.6494(3.6613) | Xent 0.8601(0.8873) | Loss 9.1982(9.5174) | Error 0.3039(0.3170) Steps 634(618.75) | Grad Norm 3.6374(7.2963) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 23.7768, Epoch Time 397.4732(395.3350), Bit/dim 3.6577(best: 3.6532), Xent 0.8740, Loss 4.0947, Error 0.3132(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 53.7287(58.1227) | Bit/dim 3.6687(3.6615) | Xent 0.8447(0.8860) | Loss 12.0879(9.5945) | Error 0.3060(0.3166) Steps 616(618.66) | Grad Norm 4.3457(7.2078) | Total Time 0.00(0.00)\n",
      "Iter 2102 | Time 59.9855(58.1786) | Bit/dim 3.6518(3.6612) | Xent 0.8721(0.8856) | Loss 9.1501(9.5812) | Error 0.3081(0.3164) Steps 604(618.22) | Grad Norm 6.4190(7.1841) | Total Time 0.00(0.00)\n",
      "Iter 2103 | Time 61.9977(58.2931) | Bit/dim 3.6607(3.6612) | Xent 0.8586(0.8848) | Loss 9.1053(9.5669) | Error 0.3064(0.3161) Steps 622(618.34) | Grad Norm 4.3144(7.0981) | Total Time 0.00(0.00)\n",
      "Iter 2104 | Time 58.6865(58.3049) | Bit/dim 3.6566(3.6610) | Xent 0.8550(0.8839) | Loss 9.0886(9.5526) | Error 0.3031(0.3157) Steps 616(618.27) | Grad Norm 4.5477(7.0215) | Total Time 0.00(0.00)\n",
      "Iter 2105 | Time 56.9520(58.2644) | Bit/dim 3.6539(3.6608) | Xent 0.8520(0.8829) | Loss 9.1885(9.5417) | Error 0.3095(0.3155) Steps 640(618.92) | Grad Norm 4.4919(6.9457) | Total Time 0.00(0.00)\n",
      "Iter 2106 | Time 56.5918(58.2142) | Bit/dim 3.6463(3.6604) | Xent 0.8506(0.8819) | Loss 9.1868(9.5310) | Error 0.3069(0.3152) Steps 646(619.73) | Grad Norm 3.7982(6.8512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 23.3940, Epoch Time 387.8159(395.1094), Bit/dim 3.6479(best: 3.6532), Xent 0.8688, Loss 4.0823, Error 0.3078(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 61.5204(58.3134) | Bit/dim 3.6607(3.6604) | Xent 0.8609(0.8813) | Loss 12.2276(9.6119) | Error 0.3127(0.3152) Steps 604(619.26) | Grad Norm 3.2993(6.7447) | Total Time 0.00(0.00)\n",
      "Iter 2108 | Time 56.7933(58.2678) | Bit/dim 3.6591(3.6604) | Xent 0.8635(0.8808) | Loss 8.9358(9.5916) | Error 0.3099(0.3150) Steps 646(620.06) | Grad Norm 4.3547(6.6730) | Total Time 0.00(0.00)\n",
      "Iter 2109 | Time 59.9324(58.3177) | Bit/dim 3.6459(3.6599) | Xent 0.8226(0.8790) | Loss 9.0178(9.5744) | Error 0.2929(0.3143) Steps 658(621.20) | Grad Norm 3.4584(6.5765) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 54.7645(58.2111) | Bit/dim 3.6519(3.6597) | Xent 0.8344(0.8777) | Loss 8.9328(9.5552) | Error 0.3046(0.3141) Steps 634(621.58) | Grad Norm 2.5876(6.4569) | Total Time 0.00(0.00)\n",
      "Iter 2111 | Time 58.8883(58.2314) | Bit/dim 3.6458(3.6593) | Xent 0.8293(0.8762) | Loss 9.0221(9.5392) | Error 0.2981(0.3136) Steps 634(621.96) | Grad Norm 3.8804(6.3796) | Total Time 0.00(0.00)\n",
      "Iter 2112 | Time 62.6333(58.3635) | Bit/dim 3.6480(3.6589) | Xent 0.8275(0.8748) | Loss 8.9674(9.5220) | Error 0.2959(0.3130) Steps 622(621.96) | Grad Norm 3.1772(6.2835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 23.0522, Epoch Time 393.7027(395.0672), Bit/dim 3.6589(best: 3.6479), Xent 0.8570, Loss 4.0874, Error 0.3035(best: 0.3054)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 57.6481(58.3420) | Bit/dim 3.6660(3.6591) | Xent 0.8284(0.8734) | Loss 12.0292(9.5972) | Error 0.2995(0.3126) Steps 586(620.88) | Grad Norm 1.9672(6.1540) | Total Time 0.00(0.00)\n",
      "Iter 2114 | Time 60.4230(58.4044) | Bit/dim 3.6575(3.6591) | Xent 0.8469(0.8726) | Loss 8.8959(9.5762) | Error 0.3007(0.3123) Steps 628(621.09) | Grad Norm 3.7961(6.0833) | Total Time 0.00(0.00)\n",
      "Iter 2115 | Time 59.2947(58.4312) | Bit/dim 3.6558(3.6590) | Xent 0.8513(0.8720) | Loss 9.1660(9.5639) | Error 0.3011(0.3120) Steps 604(620.58) | Grad Norm 5.3762(6.0621) | Total Time 0.00(0.00)\n",
      "Iter 2116 | Time 56.9983(58.3882) | Bit/dim 3.6520(3.6588) | Xent 0.8417(0.8711) | Loss 8.9875(9.5466) | Error 0.3020(0.3117) Steps 616(620.44) | Grad Norm 3.6002(5.9882) | Total Time 0.00(0.00)\n",
      "Iter 2117 | Time 61.7488(58.4890) | Bit/dim 3.6418(3.6583) | Xent 0.8115(0.8693) | Loss 8.8173(9.5247) | Error 0.2913(0.3110) Steps 640(621.03) | Grad Norm 3.0910(5.9013) | Total Time 0.00(0.00)\n",
      "Iter 2118 | Time 58.6969(58.4952) | Bit/dim 3.6380(3.6577) | Xent 0.8542(0.8688) | Loss 8.9908(9.5087) | Error 0.3059(0.3109) Steps 610(620.70) | Grad Norm 4.4724(5.8584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 22.6715, Epoch Time 393.3384(395.0154), Bit/dim 3.6541(best: 3.6479), Xent 0.8648, Loss 4.0865, Error 0.3083(best: 0.3035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 61.8351(58.5954) | Bit/dim 3.6553(3.6576) | Xent 0.8268(0.8676) | Loss 12.1441(9.5878) | Error 0.2950(0.3104) Steps 634(621.10) | Grad Norm 5.6153(5.8511) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 57.4058(58.5597) | Bit/dim 3.6439(3.6572) | Xent 0.8201(0.8661) | Loss 9.0316(9.5711) | Error 0.2921(0.3099) Steps 634(621.48) | Grad Norm 5.2230(5.8323) | Total Time 0.00(0.00)\n",
      "Iter 2121 | Time 56.4882(58.4976) | Bit/dim 3.6562(3.6572) | Xent 0.8268(0.8649) | Loss 9.0469(9.5553) | Error 0.2954(0.3094) Steps 610(621.14) | Grad Norm 3.0828(5.7498) | Total Time 0.00(0.00)\n",
      "Iter 2122 | Time 61.8665(58.5987) | Bit/dim 3.6489(3.6569) | Xent 0.8363(0.8641) | Loss 9.0503(9.5402) | Error 0.2995(0.3091) Steps 610(620.81) | Grad Norm 3.8371(5.6924) | Total Time 0.00(0.00)\n",
      "Iter 2123 | Time 61.3213(58.6803) | Bit/dim 3.6509(3.6567) | Xent 0.8268(0.8630) | Loss 8.9429(9.5223) | Error 0.2959(0.3087) Steps 622(620.84) | Grad Norm 4.0917(5.6444) | Total Time 0.00(0.00)\n",
      "Iter 2124 | Time 58.4323(58.6729) | Bit/dim 3.6480(3.6565) | Xent 0.8385(0.8622) | Loss 8.9912(9.5063) | Error 0.2995(0.3085) Steps 616(620.70) | Grad Norm 4.1002(5.5981) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 22.9001, Epoch Time 396.9910(395.0746), Bit/dim 3.6509(best: 3.6479), Xent 0.8657, Loss 4.0838, Error 0.3088(best: 0.3035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 55.6000(58.5807) | Bit/dim 3.6474(3.6562) | Xent 0.8342(0.8614) | Loss 11.9751(9.5804) | Error 0.2961(0.3081) Steps 604(620.20) | Grad Norm 5.3505(5.5906) | Total Time 0.00(0.00)\n",
      "Iter 2126 | Time 58.2677(58.5713) | Bit/dim 3.6545(3.6561) | Xent 0.8375(0.8607) | Loss 9.0740(9.5652) | Error 0.3014(0.3079) Steps 616(620.07) | Grad Norm 3.4693(5.5270) | Total Time 0.00(0.00)\n",
      "Iter 2127 | Time 56.9109(58.5215) | Bit/dim 3.6540(3.6561) | Xent 0.8325(0.8598) | Loss 8.9463(9.5466) | Error 0.2960(0.3075) Steps 640(620.67) | Grad Norm 4.2168(5.4877) | Total Time 0.00(0.00)\n",
      "Iter 2128 | Time 60.0778(58.5682) | Bit/dim 3.6442(3.6557) | Xent 0.8253(0.8588) | Loss 8.9760(9.5295) | Error 0.2979(0.3072) Steps 628(620.89) | Grad Norm 3.2646(5.4210) | Total Time 0.00(0.00)\n",
      "Iter 2129 | Time 58.5246(58.5669) | Bit/dim 3.6478(3.6555) | Xent 0.8099(0.8573) | Loss 8.8205(9.5083) | Error 0.2924(0.3068) Steps 604(620.38) | Grad Norm 3.5843(5.3659) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 62.5319(58.6858) | Bit/dim 3.6419(3.6551) | Xent 0.8233(0.8563) | Loss 8.9958(9.4929) | Error 0.2964(0.3065) Steps 628(620.61) | Grad Norm 3.9221(5.3226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 23.0566, Epoch Time 391.0239(394.9531), Bit/dim 3.6505(best: 3.6479), Xent 0.8615, Loss 4.0813, Error 0.3055(best: 0.3035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 56.7427(58.6275) | Bit/dim 3.6486(3.6549) | Xent 0.8331(0.8556) | Loss 11.7381(9.5602) | Error 0.2967(0.3062) Steps 634(621.01) | Grad Norm 5.3172(5.3224) | Total Time 0.00(0.00)\n",
      "Iter 2132 | Time 55.9909(58.5484) | Bit/dim 3.6502(3.6547) | Xent 0.8272(0.8548) | Loss 9.0169(9.5439) | Error 0.2955(0.3059) Steps 604(620.50) | Grad Norm 9.9514(5.4613) | Total Time 0.00(0.00)\n",
      "Iter 2133 | Time 59.2819(58.5704) | Bit/dim 3.6540(3.6547) | Xent 0.8477(0.8546) | Loss 8.9244(9.5254) | Error 0.3020(0.3057) Steps 634(620.91) | Grad Norm 8.4103(5.5498) | Total Time 0.00(0.00)\n",
      "Iter 2134 | Time 56.0278(58.4942) | Bit/dim 3.6445(3.6544) | Xent 0.8370(0.8540) | Loss 9.0359(9.5107) | Error 0.2963(0.3055) Steps 646(621.66) | Grad Norm 3.1812(5.4787) | Total Time 0.00(0.00)\n",
      "Iter 2135 | Time 59.5673(58.5264) | Bit/dim 3.6463(3.6542) | Xent 0.8336(0.8534) | Loss 8.7784(9.4887) | Error 0.2930(0.3051) Steps 616(621.49) | Grad Norm 6.5472(5.5108) | Total Time 0.00(0.00)\n",
      "Iter 2136 | Time 59.0628(58.5425) | Bit/dim 3.6560(3.6542) | Xent 0.8451(0.8532) | Loss 8.9025(9.4711) | Error 0.3014(0.3050) Steps 634(621.86) | Grad Norm 7.0427(5.5567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 23.4615, Epoch Time 386.3022(394.6936), Bit/dim 3.6539(best: 3.6479), Xent 0.8569, Loss 4.0823, Error 0.3046(best: 0.3035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 57.7331(58.5182) | Bit/dim 3.6558(3.6543) | Xent 0.8183(0.8521) | Loss 12.1733(9.5522) | Error 0.2950(0.3047) Steps 640(622.41) | Grad Norm 5.1196(5.5436) | Total Time 0.00(0.00)\n",
      "Iter 2138 | Time 56.6221(58.4613) | Bit/dim 3.6574(3.6544) | Xent 0.8344(0.8516) | Loss 8.9918(9.5354) | Error 0.2930(0.3043) Steps 634(622.76) | Grad Norm 3.9046(5.4944) | Total Time 0.00(0.00)\n",
      "Iter 2139 | Time 62.1211(58.5711) | Bit/dim 3.6467(3.6541) | Xent 0.8480(0.8515) | Loss 9.1066(9.5225) | Error 0.2987(0.3042) Steps 640(623.27) | Grad Norm 6.1868(5.5152) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 60.6939(58.6348) | Bit/dim 3.6512(3.6540) | Xent 0.8194(0.8505) | Loss 8.9115(9.5042) | Error 0.2979(0.3040) Steps 604(622.70) | Grad Norm 4.0911(5.4725) | Total Time 0.00(0.00)\n",
      "Iter 2141 | Time 61.6198(58.7243) | Bit/dim 3.6532(3.6540) | Xent 0.8227(0.8497) | Loss 8.9543(9.4877) | Error 0.2941(0.3037) Steps 610(622.31) | Grad Norm 3.1302(5.4022) | Total Time 0.00(0.00)\n",
      "Iter 2142 | Time 56.8207(58.6672) | Bit/dim 3.6493(3.6539) | Xent 0.8381(0.8493) | Loss 9.0693(9.4751) | Error 0.2924(0.3033) Steps 604(621.76) | Grad Norm 5.5271(5.4060) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 22.6493, Epoch Time 394.7821(394.6962), Bit/dim 3.6559(best: 3.6479), Xent 0.8521, Loss 4.0820, Error 0.3028(best: 0.3035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 55.1246(58.5609) | Bit/dim 3.6589(3.6540) | Xent 0.8265(0.8486) | Loss 12.0707(9.5530) | Error 0.2947(0.3031) Steps 640(622.31) | Grad Norm 4.8354(5.3888) | Total Time 0.00(0.00)\n",
      "Iter 2144 | Time 53.1024(58.3972) | Bit/dim 3.6610(3.6542) | Xent 0.8178(0.8477) | Loss 9.0368(9.5375) | Error 0.2865(0.3026) Steps 598(621.58) | Grad Norm 3.0016(5.3172) | Total Time 0.00(0.00)\n",
      "Iter 2145 | Time 60.8712(58.4714) | Bit/dim 3.6399(3.6538) | Xent 0.8385(0.8474) | Loss 8.9677(9.5204) | Error 0.3019(0.3026) Steps 592(620.69) | Grad Norm 3.2129(5.2541) | Total Time 0.00(0.00)\n",
      "Iter 2146 | Time 61.2423(58.5545) | Bit/dim 3.6549(3.6538) | Xent 0.8175(0.8465) | Loss 8.9847(9.5043) | Error 0.2927(0.3023) Steps 604(620.19) | Grad Norm 4.1359(5.2206) | Total Time 0.00(0.00)\n",
      "Iter 2147 | Time 59.3006(58.5769) | Bit/dim 3.6421(3.6535) | Xent 0.8358(0.8462) | Loss 8.9503(9.4877) | Error 0.2991(0.3022) Steps 634(620.61) | Grad Norm 7.1414(5.2782) | Total Time 0.00(0.00)\n",
      "Iter 2148 | Time 53.1652(58.4146) | Bit/dim 3.6439(3.6532) | Xent 0.8258(0.8456) | Loss 9.0405(9.4743) | Error 0.3000(0.3021) Steps 616(620.47) | Grad Norm 7.6974(5.3508) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 23.3315, Epoch Time 382.2058(394.3215), Bit/dim 3.6491(best: 3.6479), Xent 0.8588, Loss 4.0786, Error 0.3048(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 53.9814(58.2816) | Bit/dim 3.6528(3.6532) | Xent 0.8102(0.8446) | Loss 12.1955(9.5559) | Error 0.2901(0.3017) Steps 616(620.34) | Grad Norm 3.6174(5.2988) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 58.4840(58.2876) | Bit/dim 3.6384(3.6527) | Xent 0.8466(0.8446) | Loss 8.9816(9.5387) | Error 0.3036(0.3018) Steps 616(620.21) | Grad Norm 4.8672(5.2858) | Total Time 0.00(0.00)\n",
      "Iter 2151 | Time 61.2283(58.3759) | Bit/dim 3.6557(3.6528) | Xent 0.8211(0.8439) | Loss 8.7362(9.5146) | Error 0.2934(0.3016) Steps 628(620.44) | Grad Norm 4.7800(5.2706) | Total Time 0.00(0.00)\n",
      "Iter 2152 | Time 56.8477(58.3300) | Bit/dim 3.6510(3.6528) | Xent 0.8311(0.8435) | Loss 9.0925(9.5020) | Error 0.2994(0.3015) Steps 652(621.39) | Grad Norm 5.9436(5.2908) | Total Time 0.00(0.00)\n",
      "Iter 2153 | Time 59.2440(58.3574) | Bit/dim 3.6508(3.6527) | Xent 0.8309(0.8431) | Loss 8.9203(9.4845) | Error 0.2955(0.3013) Steps 610(621.04) | Grad Norm 4.5416(5.2683) | Total Time 0.00(0.00)\n",
      "Iter 2154 | Time 58.9349(58.3748) | Bit/dim 3.6439(3.6525) | Xent 0.8322(0.8428) | Loss 8.9501(9.4685) | Error 0.2949(0.3011) Steps 628(621.25) | Grad Norm 4.5139(5.2457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 23.6158, Epoch Time 388.2799(394.1403), Bit/dim 3.6495(best: 3.6479), Xent 0.8735, Loss 4.0863, Error 0.3117(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 57.9916(58.3633) | Bit/dim 3.6530(3.6525) | Xent 0.8279(0.8424) | Loss 12.3228(9.5541) | Error 0.2993(0.3011) Steps 640(621.82) | Grad Norm 8.6675(5.3484) | Total Time 0.00(0.00)\n",
      "Iter 2156 | Time 61.5705(58.4595) | Bit/dim 3.6435(3.6522) | Xent 0.8452(0.8425) | Loss 9.1719(9.5427) | Error 0.3057(0.3012) Steps 616(621.64) | Grad Norm 8.7169(5.4494) | Total Time 0.00(0.00)\n",
      "Iter 2157 | Time 56.3905(58.3974) | Bit/dim 3.6517(3.6522) | Xent 0.8479(0.8426) | Loss 8.9083(9.5236) | Error 0.3015(0.3012) Steps 622(621.65) | Grad Norm 7.2134(5.5023) | Total Time 0.00(0.00)\n",
      "Iter 2158 | Time 53.4402(58.2487) | Bit/dim 3.6526(3.6522) | Xent 0.8388(0.8425) | Loss 8.9745(9.5072) | Error 0.3021(0.3012) Steps 592(620.76) | Grad Norm 6.5906(5.5350) | Total Time 0.00(0.00)\n",
      "Iter 2159 | Time 57.9384(58.2394) | Bit/dim 3.6562(3.6523) | Xent 0.8604(0.8430) | Loss 9.1021(9.4950) | Error 0.3101(0.3015) Steps 628(620.98) | Grad Norm 10.7057(5.6901) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 56.0525(58.1738) | Bit/dim 3.6381(3.6519) | Xent 0.9002(0.8448) | Loss 8.7843(9.4737) | Error 0.3237(0.3022) Steps 616(620.83) | Grad Norm 13.4036(5.9215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 23.5900, Epoch Time 383.0619(393.8079), Bit/dim 3.6506(best: 3.6479), Xent 0.8714, Loss 4.0862, Error 0.3090(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 57.7875(58.1622) | Bit/dim 3.6561(3.6520) | Xent 0.8331(0.8444) | Loss 11.7448(9.5418) | Error 0.2961(0.3020) Steps 628(621.05) | Grad Norm 7.2418(5.9611) | Total Time 0.00(0.00)\n",
      "Iter 2162 | Time 57.9617(58.1562) | Bit/dim 3.6364(3.6515) | Xent 0.8494(0.8446) | Loss 9.0212(9.5262) | Error 0.2966(0.3018) Steps 652(621.97) | Grad Norm 10.6878(6.1029) | Total Time 0.00(0.00)\n",
      "Iter 2163 | Time 56.0335(58.0925) | Bit/dim 3.6514(3.6515) | Xent 0.8495(0.8447) | Loss 8.7808(9.5038) | Error 0.3004(0.3018) Steps 622(621.97) | Grad Norm 11.7086(6.2711) | Total Time 0.00(0.00)\n",
      "Iter 2164 | Time 60.1877(58.1553) | Bit/dim 3.6429(3.6513) | Xent 0.8479(0.8448) | Loss 8.9338(9.4867) | Error 0.3017(0.3018) Steps 634(622.34) | Grad Norm 4.2783(6.2113) | Total Time 0.00(0.00)\n",
      "Iter 2165 | Time 59.0280(58.1815) | Bit/dim 3.6622(3.6516) | Xent 0.8579(0.8452) | Loss 9.1448(9.4765) | Error 0.3077(0.3020) Steps 610(621.97) | Grad Norm 7.6378(6.2541) | Total Time 0.00(0.00)\n",
      "Iter 2166 | Time 54.6588(58.0758) | Bit/dim 3.6617(3.6519) | Xent 0.8689(0.8459) | Loss 8.9945(9.4620) | Error 0.3157(0.3024) Steps 610(621.61) | Grad Norm 6.7298(6.2684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 23.8012, Epoch Time 385.3434(393.5540), Bit/dim 3.6540(best: 3.6479), Xent 0.8848, Loss 4.0964, Error 0.3168(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 55.8220(58.0082) | Bit/dim 3.6561(3.6520) | Xent 0.8481(0.8460) | Loss 12.1506(9.5427) | Error 0.2996(0.3023) Steps 634(621.98) | Grad Norm 6.7590(6.2831) | Total Time 0.00(0.00)\n",
      "Iter 2168 | Time 60.9764(58.0973) | Bit/dim 3.6471(3.6519) | Xent 0.8665(0.8466) | Loss 9.1441(9.5307) | Error 0.3161(0.3027) Steps 622(621.98) | Grad Norm 9.8300(6.3895) | Total Time 0.00(0.00)\n",
      "Iter 2169 | Time 58.9526(58.1229) | Bit/dim 3.6659(3.6523) | Xent 0.8507(0.8467) | Loss 8.9418(9.5131) | Error 0.3050(0.3028) Steps 616(621.80) | Grad Norm 12.0771(6.5601) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 61.2622(58.2171) | Bit/dim 3.6557(3.6524) | Xent 0.8662(0.8473) | Loss 8.9513(9.4962) | Error 0.3149(0.3031) Steps 634(622.17) | Grad Norm 9.6696(6.6534) | Total Time 0.00(0.00)\n",
      "Iter 2171 | Time 59.1588(58.2454) | Bit/dim 3.6615(3.6527) | Xent 0.8642(0.8478) | Loss 8.9409(9.4795) | Error 0.3046(0.3032) Steps 598(621.44) | Grad Norm 10.9397(6.7820) | Total Time 0.00(0.00)\n",
      "Iter 2172 | Time 55.3841(58.1595) | Bit/dim 3.6416(3.6524) | Xent 0.8319(0.8473) | Loss 9.0312(9.4661) | Error 0.2983(0.3030) Steps 640(622.00) | Grad Norm 6.3128(6.7679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 23.0418, Epoch Time 390.6811(393.4678), Bit/dim 3.6510(best: 3.6479), Xent 0.8746, Loss 4.0883, Error 0.3111(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 60.5618(58.2316) | Bit/dim 3.6499(3.6523) | Xent 0.8419(0.8472) | Loss 12.1095(9.5454) | Error 0.3009(0.3030) Steps 622(622.00) | Grad Norm 7.5397(6.7911) | Total Time 0.00(0.00)\n",
      "Iter 2174 | Time 57.2015(58.2007) | Bit/dim 3.6569(3.6524) | Xent 0.8367(0.8468) | Loss 9.2151(9.5355) | Error 0.3001(0.3029) Steps 622(622.00) | Grad Norm 5.8476(6.7628) | Total Time 0.00(0.00)\n",
      "Iter 2175 | Time 57.4917(58.1794) | Bit/dim 3.6581(3.6526) | Xent 0.8426(0.8467) | Loss 8.9015(9.5165) | Error 0.3019(0.3029) Steps 646(622.72) | Grad Norm 6.2345(6.7469) | Total Time 0.00(0.00)\n",
      "Iter 2176 | Time 57.7998(58.1680) | Bit/dim 3.6436(3.6523) | Xent 0.8428(0.8466) | Loss 8.8410(9.4962) | Error 0.3057(0.3029) Steps 646(623.42) | Grad Norm 6.9950(6.7544) | Total Time 0.00(0.00)\n",
      "Iter 2177 | Time 57.3958(58.1449) | Bit/dim 3.6528(3.6523) | Xent 0.8038(0.8453) | Loss 8.9392(9.4795) | Error 0.2867(0.3025) Steps 604(622.83) | Grad Norm 4.3615(6.6826) | Total Time 0.00(0.00)\n",
      "Iter 2178 | Time 58.8109(58.1648) | Bit/dim 3.6442(3.6521) | Xent 0.8557(0.8456) | Loss 9.0528(9.4667) | Error 0.3079(0.3026) Steps 622(622.81) | Grad Norm 5.0833(6.6346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 23.2880, Epoch Time 388.7769(393.3271), Bit/dim 3.6560(best: 3.6479), Xent 0.8576, Loss 4.0848, Error 0.3047(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 61.1074(58.2531) | Bit/dim 3.6443(3.6519) | Xent 0.8154(0.8447) | Loss 12.3582(9.5534) | Error 0.2944(0.3024) Steps 622(622.78) | Grad Norm 6.3299(6.6255) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 60.2785(58.3139) | Bit/dim 3.6605(3.6521) | Xent 0.8583(0.8451) | Loss 9.0858(9.5394) | Error 0.3064(0.3025) Steps 604(622.22) | Grad Norm 9.0085(6.6970) | Total Time 0.00(0.00)\n",
      "Iter 2181 | Time 57.9483(58.3029) | Bit/dim 3.6556(3.6522) | Xent 0.8365(0.8449) | Loss 8.9212(9.5209) | Error 0.2985(0.3024) Steps 628(622.39) | Grad Norm 5.8628(6.6719) | Total Time 0.00(0.00)\n",
      "Iter 2182 | Time 57.5356(58.2799) | Bit/dim 3.6367(3.6518) | Xent 0.8221(0.8442) | Loss 8.8839(9.5018) | Error 0.2915(0.3020) Steps 592(621.48) | Grad Norm 6.5250(6.6675) | Total Time 0.00(0.00)\n",
      "Iter 2183 | Time 57.6527(58.2611) | Bit/dim 3.6549(3.6519) | Xent 0.8630(0.8447) | Loss 9.0694(9.4888) | Error 0.3107(0.3023) Steps 616(621.32) | Grad Norm 10.2156(6.7740) | Total Time 0.00(0.00)\n",
      "Iter 2184 | Time 60.5234(58.3289) | Bit/dim 3.6570(3.6520) | Xent 0.8564(0.8451) | Loss 8.7579(9.4669) | Error 0.3056(0.3024) Steps 616(621.16) | Grad Norm 6.3341(6.7608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 23.0869, Epoch Time 394.5562(393.3640), Bit/dim 3.6562(best: 3.6479), Xent 0.8569, Loss 4.0847, Error 0.3060(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 53.1759(58.1744) | Bit/dim 3.6475(3.6519) | Xent 0.8289(0.8446) | Loss 11.0925(9.5156) | Error 0.2933(0.3021) Steps 622(621.18) | Grad Norm 5.2101(6.7143) | Total Time 0.00(0.00)\n",
      "Iter 2186 | Time 57.0912(58.1419) | Bit/dim 3.6283(3.6512) | Xent 0.8231(0.8440) | Loss 8.7625(9.4930) | Error 0.2965(0.3020) Steps 634(621.57) | Grad Norm 5.2811(6.6713) | Total Time 0.00(0.00)\n",
      "Iter 2187 | Time 55.6901(58.0683) | Bit/dim 3.6422(3.6509) | Xent 0.8576(0.8444) | Loss 8.9673(9.4773) | Error 0.3055(0.3021) Steps 604(621.04) | Grad Norm 7.9591(6.7099) | Total Time 0.00(0.00)\n",
      "Iter 2188 | Time 61.1678(58.1613) | Bit/dim 3.6477(3.6508) | Xent 0.8592(0.8448) | Loss 9.1045(9.4661) | Error 0.3096(0.3023) Steps 646(621.79) | Grad Norm 6.4450(6.7019) | Total Time 0.00(0.00)\n",
      "Iter 2189 | Time 56.8977(58.1234) | Bit/dim 3.6548(3.6509) | Xent 0.8643(0.8454) | Loss 9.1658(9.4571) | Error 0.3051(0.3024) Steps 616(621.62) | Grad Norm 7.7850(6.7344) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 56.6862(58.0803) | Bit/dim 3.6667(3.6514) | Xent 0.8483(0.8455) | Loss 9.0173(9.4439) | Error 0.3053(0.3025) Steps 592(620.73) | Grad Norm 7.9592(6.7712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 22.9331, Epoch Time 379.8008(392.9571), Bit/dim 3.6552(best: 3.6479), Xent 0.8683, Loss 4.0894, Error 0.3067(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 55.7914(58.0116) | Bit/dim 3.6478(3.6513) | Xent 0.8383(0.8453) | Loss 12.2552(9.5282) | Error 0.3005(0.3024) Steps 622(620.77) | Grad Norm 10.8326(6.8930) | Total Time 0.00(0.00)\n",
      "Iter 2192 | Time 53.3452(57.8716) | Bit/dim 3.6544(3.6514) | Xent 0.8839(0.8464) | Loss 9.0505(9.5139) | Error 0.3117(0.3027) Steps 598(620.08) | Grad Norm 11.1489(7.0207) | Total Time 0.00(0.00)\n",
      "Iter 2193 | Time 56.0471(57.8169) | Bit/dim 3.6501(3.6513) | Xent 0.8304(0.8460) | Loss 8.9843(9.4980) | Error 0.2947(0.3025) Steps 640(620.68) | Grad Norm 4.5529(6.9467) | Total Time 0.00(0.00)\n",
      "Iter 2194 | Time 59.8443(57.8777) | Bit/dim 3.6590(3.6516) | Xent 0.8497(0.8461) | Loss 8.9935(9.4829) | Error 0.3049(0.3025) Steps 622(620.72) | Grad Norm 11.1572(7.0730) | Total Time 0.00(0.00)\n",
      "Iter 2195 | Time 55.7690(57.8144) | Bit/dim 3.6426(3.6513) | Xent 0.8419(0.8459) | Loss 9.0792(9.4708) | Error 0.3083(0.3027) Steps 586(619.68) | Grad Norm 6.8617(7.0666) | Total Time 0.00(0.00)\n",
      "Iter 2196 | Time 62.6251(57.9588) | Bit/dim 3.6488(3.6512) | Xent 0.8198(0.8452) | Loss 9.1217(9.4603) | Error 0.2903(0.3023) Steps 616(619.57) | Grad Norm 8.2350(7.1017) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 23.0744, Epoch Time 382.5824(392.6458), Bit/dim 3.6517(best: 3.6479), Xent 0.8653, Loss 4.0843, Error 0.3084(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 57.4924(57.9448) | Bit/dim 3.6489(3.6512) | Xent 0.8379(0.8449) | Loss 12.2642(9.5444) | Error 0.2994(0.3022) Steps 598(618.92) | Grad Norm 8.7129(7.1500) | Total Time 0.00(0.00)\n",
      "Iter 2198 | Time 61.0762(58.0387) | Bit/dim 3.6626(3.6515) | Xent 0.8384(0.8447) | Loss 9.0997(9.5311) | Error 0.3049(0.3023) Steps 658(620.09) | Grad Norm 8.1904(7.1812) | Total Time 0.00(0.00)\n",
      "Iter 2199 | Time 61.3171(58.1371) | Bit/dim 3.6605(3.6518) | Xent 0.8052(0.8436) | Loss 9.0974(9.5180) | Error 0.2893(0.3019) Steps 598(619.43) | Grad Norm 4.1040(7.0889) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 56.6888(58.0936) | Bit/dim 3.6400(3.6514) | Xent 0.8426(0.8435) | Loss 9.0154(9.5030) | Error 0.3033(0.3020) Steps 628(619.69) | Grad Norm 7.1657(7.0912) | Total Time 0.00(0.00)\n",
      "Iter 2201 | Time 56.8941(58.0576) | Bit/dim 3.6422(3.6511) | Xent 0.8264(0.8430) | Loss 8.9512(9.4864) | Error 0.3000(0.3019) Steps 580(618.50) | Grad Norm 4.9115(7.0258) | Total Time 0.00(0.00)\n",
      "Iter 2202 | Time 57.6804(58.0463) | Bit/dim 3.6395(3.6508) | Xent 0.8346(0.8428) | Loss 9.1408(9.4760) | Error 0.3031(0.3019) Steps 592(617.70) | Grad Norm 5.1898(6.9708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 23.3608, Epoch Time 390.5870(392.5841), Bit/dim 3.6480(best: 3.6479), Xent 0.8644, Loss 4.0802, Error 0.3059(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 60.5232(58.1206) | Bit/dim 3.6562(3.6510) | Xent 0.8207(0.8421) | Loss 12.2055(9.5579) | Error 0.2960(0.3018) Steps 616(617.65) | Grad Norm 6.3336(6.9516) | Total Time 0.00(0.00)\n",
      "Iter 2204 | Time 60.1366(58.1811) | Bit/dim 3.6490(3.6509) | Xent 0.8291(0.8417) | Loss 8.9487(9.5397) | Error 0.2960(0.3016) Steps 586(616.70) | Grad Norm 7.5587(6.9699) | Total Time 0.00(0.00)\n",
      "Iter 2205 | Time 57.8626(58.1715) | Bit/dim 3.6503(3.6509) | Xent 0.8241(0.8412) | Loss 8.9998(9.5235) | Error 0.2913(0.3013) Steps 658(617.94) | Grad Norm 4.5944(6.8986) | Total Time 0.00(0.00)\n",
      "Iter 2206 | Time 56.0235(58.1071) | Bit/dim 3.6593(3.6511) | Xent 0.8117(0.8403) | Loss 9.0529(9.5093) | Error 0.2886(0.3009) Steps 652(618.96) | Grad Norm 5.4629(6.8555) | Total Time 0.00(0.00)\n",
      "Iter 2207 | Time 58.7836(58.1274) | Bit/dim 3.6464(3.6510) | Xent 0.8339(0.8401) | Loss 8.9978(9.4940) | Error 0.3007(0.3009) Steps 634(619.41) | Grad Norm 5.3889(6.8115) | Total Time 0.00(0.00)\n",
      "Iter 2208 | Time 56.8059(58.0877) | Bit/dim 3.6398(3.6507) | Xent 0.8112(0.8392) | Loss 8.5371(9.4653) | Error 0.2924(0.3006) Steps 598(618.77) | Grad Norm 3.9273(6.7250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 22.5249, Epoch Time 388.5582(392.4633), Bit/dim 3.6546(best: 3.6479), Xent 0.8487, Loss 4.0790, Error 0.3017(best: 0.3028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 57.6131(58.0735) | Bit/dim 3.6547(3.6508) | Xent 0.8169(0.8386) | Loss 12.1535(9.5459) | Error 0.2923(0.3004) Steps 634(619.23) | Grad Norm 4.4632(6.6571) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 55.7969(58.0052) | Bit/dim 3.6471(3.6507) | Xent 0.8228(0.8381) | Loss 8.8861(9.5261) | Error 0.2956(0.3002) Steps 628(619.49) | Grad Norm 5.3656(6.6184) | Total Time 0.00(0.00)\n",
      "Iter 2211 | Time 55.9342(57.9431) | Bit/dim 3.6349(3.6502) | Xent 0.8253(0.8377) | Loss 8.9087(9.5076) | Error 0.2929(0.3000) Steps 622(619.57) | Grad Norm 6.4917(6.6146) | Total Time 0.00(0.00)\n",
      "Iter 2212 | Time 63.0308(58.0957) | Bit/dim 3.6526(3.6503) | Xent 0.8287(0.8374) | Loss 8.9716(9.4915) | Error 0.3050(0.3002) Steps 610(619.28) | Grad Norm 5.5991(6.5841) | Total Time 0.00(0.00)\n",
      "Iter 2213 | Time 55.5345(58.0189) | Bit/dim 3.6537(3.6504) | Xent 0.7911(0.8361) | Loss 8.8355(9.4719) | Error 0.2815(0.2996) Steps 610(619.00) | Grad Norm 4.0958(6.5095) | Total Time 0.00(0.00)\n",
      "Iter 2214 | Time 57.5280(58.0041) | Bit/dim 3.6426(3.6501) | Xent 0.8293(0.8359) | Loss 9.0145(9.4581) | Error 0.2966(0.2995) Steps 634(619.45) | Grad Norm 4.0899(6.4369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 23.4508, Epoch Time 385.3711(392.2505), Bit/dim 3.6470(best: 3.6479), Xent 0.8612, Loss 4.0776, Error 0.3026(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 57.9651(58.0030) | Bit/dim 3.6470(3.6500) | Xent 0.8196(0.8354) | Loss 11.9246(9.5321) | Error 0.2964(0.2994) Steps 628(619.71) | Grad Norm 4.2183(6.3703) | Total Time 0.00(0.00)\n",
      "Iter 2216 | Time 59.9878(58.0625) | Bit/dim 3.6345(3.6496) | Xent 0.8168(0.8348) | Loss 8.9928(9.5160) | Error 0.2909(0.2992) Steps 628(619.96) | Grad Norm 4.2370(6.3063) | Total Time 0.00(0.00)\n",
      "Iter 2217 | Time 60.4203(58.1333) | Bit/dim 3.6521(3.6496) | Xent 0.8468(0.8352) | Loss 9.0515(9.5020) | Error 0.2993(0.2992) Steps 640(620.56) | Grad Norm 8.4890(6.3718) | Total Time 0.00(0.00)\n",
      "Iter 2218 | Time 53.4663(57.9932) | Bit/dim 3.6597(3.6499) | Xent 0.8969(0.8370) | Loss 9.1598(9.4918) | Error 0.3219(0.2999) Steps 616(620.42) | Grad Norm 11.6131(6.5291) | Total Time 0.00(0.00)\n",
      "Iter 2219 | Time 61.6405(58.1027) | Bit/dim 3.6436(3.6498) | Xent 0.9294(0.8398) | Loss 9.2737(9.4852) | Error 0.3269(0.3007) Steps 616(620.29) | Grad Norm 16.2192(6.8198) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 55.0439(58.0109) | Bit/dim 3.6599(3.6501) | Xent 0.9746(0.8438) | Loss 8.9385(9.4688) | Error 0.3426(0.3019) Steps 610(619.98) | Grad Norm 24.4406(7.3484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 22.9840, Epoch Time 387.8835(392.1195), Bit/dim 3.6564(best: 3.6470), Xent 0.9241, Loss 4.1184, Error 0.3338(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 58.1777(58.0159) | Bit/dim 3.6469(3.6500) | Xent 0.8961(0.8454) | Loss 12.0872(9.5474) | Error 0.3254(0.3026) Steps 604(619.50) | Grad Norm 15.7149(7.5994) | Total Time 0.00(0.00)\n",
      "Iter 2222 | Time 55.2749(57.9337) | Bit/dim 3.6551(3.6501) | Xent 0.9334(0.8480) | Loss 9.0183(9.5315) | Error 0.3401(0.3038) Steps 616(619.40) | Grad Norm 10.5447(7.6877) | Total Time 0.00(0.00)\n",
      "Iter 2223 | Time 57.5021(57.9207) | Bit/dim 3.6505(3.6501) | Xent 0.8998(0.8496) | Loss 9.1875(9.5212) | Error 0.3253(0.3044) Steps 598(618.75) | Grad Norm 12.9543(7.8457) | Total Time 0.00(0.00)\n",
      "Iter 2224 | Time 55.1231(57.8368) | Bit/dim 3.6493(3.6501) | Xent 0.8849(0.8507) | Loss 9.0214(9.5062) | Error 0.3144(0.3047) Steps 598(618.13) | Grad Norm 9.7102(7.9017) | Total Time 0.00(0.00)\n",
      "Iter 2225 | Time 61.0443(57.9330) | Bit/dim 3.6488(3.6501) | Xent 0.8825(0.8516) | Loss 8.9887(9.4907) | Error 0.3151(0.3050) Steps 628(618.43) | Grad Norm 6.0846(7.8472) | Total Time 0.00(0.00)\n",
      "Iter 2226 | Time 58.4191(57.9476) | Bit/dim 3.6491(3.6500) | Xent 0.8536(0.8517) | Loss 9.1055(9.4791) | Error 0.3109(0.3052) Steps 586(617.45) | Grad Norm 5.9549(7.7904) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 22.3925, Epoch Time 383.9920(391.8757), Bit/dim 3.6546(best: 3.6470), Xent 0.8746, Loss 4.0919, Error 0.3097(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 62.1622(58.0740) | Bit/dim 3.6481(3.6500) | Xent 0.8332(0.8511) | Loss 12.3700(9.5658) | Error 0.2983(0.3050) Steps 634(617.95) | Grad Norm 5.1147(7.7101) | Total Time 0.00(0.00)\n",
      "Iter 2228 | Time 58.8573(58.0975) | Bit/dim 3.6643(3.6504) | Xent 0.8540(0.8512) | Loss 8.9759(9.5481) | Error 0.3010(0.3049) Steps 658(619.15) | Grad Norm 6.8971(7.6857) | Total Time 0.00(0.00)\n",
      "Iter 2229 | Time 58.6914(58.1154) | Bit/dim 3.6547(3.6505) | Xent 0.8315(0.8506) | Loss 8.9314(9.5296) | Error 0.2976(0.3046) Steps 604(618.70) | Grad Norm 3.7306(7.5671) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 54.6492(58.0114) | Bit/dim 3.6549(3.6507) | Xent 0.8356(0.8502) | Loss 9.1016(9.5168) | Error 0.2959(0.3044) Steps 616(618.62) | Grad Norm 5.8108(7.5144) | Total Time 0.00(0.00)\n",
      "Iter 2231 | Time 58.5859(58.0286) | Bit/dim 3.6462(3.6505) | Xent 0.8175(0.8492) | Loss 8.9318(9.4992) | Error 0.2935(0.3041) Steps 646(619.44) | Grad Norm 3.2605(7.3868) | Total Time 0.00(0.00)\n",
      "Iter 2232 | Time 58.9114(58.0551) | Bit/dim 3.6620(3.6509) | Xent 0.8270(0.8485) | Loss 8.9318(9.4822) | Error 0.2945(0.3038) Steps 658(620.59) | Grad Norm 6.7316(7.3671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 23.7031, Epoch Time 391.5200(391.8650), Bit/dim 3.6523(best: 3.6470), Xent 0.8500, Loss 4.0773, Error 0.3032(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2233 | Time 57.9524(58.0520) | Bit/dim 3.6546(3.6510) | Xent 0.8098(0.8474) | Loss 11.9180(9.5553) | Error 0.2887(0.3033) Steps 658(621.72) | Grad Norm 4.2814(7.2745) | Total Time 0.00(0.00)\n",
      "Iter 2234 | Time 53.4397(57.9136) | Bit/dim 3.6489(3.6509) | Xent 0.8421(0.8472) | Loss 9.0872(9.5412) | Error 0.3070(0.3034) Steps 604(621.19) | Grad Norm 5.7221(7.2280) | Total Time 0.00(0.00)\n",
      "Iter 2235 | Time 58.5377(57.9324) | Bit/dim 3.6573(3.6511) | Xent 0.8421(0.8470) | Loss 8.8231(9.5197) | Error 0.3010(0.3034) Steps 622(621.21) | Grad Norm 2.6383(7.0903) | Total Time 0.00(0.00)\n",
      "Iter 2236 | Time 57.2980(57.9133) | Bit/dim 3.6428(3.6509) | Xent 0.8363(0.8467) | Loss 9.0820(9.5066) | Error 0.3029(0.3033) Steps 610(620.87) | Grad Norm 6.3970(7.0695) | Total Time 0.00(0.00)\n",
      "Iter 2237 | Time 55.9589(57.8547) | Bit/dim 3.6462(3.6507) | Xent 0.8085(0.8456) | Loss 8.6920(9.4821) | Error 0.2890(0.3029) Steps 616(620.73) | Grad Norm 3.2214(6.9540) | Total Time 0.00(0.00)\n",
      "Iter 2238 | Time 59.0154(57.8895) | Bit/dim 3.6419(3.6505) | Xent 0.8248(0.8450) | Loss 9.0155(9.4681) | Error 0.2923(0.3026) Steps 628(620.95) | Grad Norm 5.8771(6.9217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 22.7696, Epoch Time 381.2126(391.5454), Bit/dim 3.6442(best: 3.6470), Xent 0.8525, Loss 4.0705, Error 0.3046(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2239 | Time 58.3160(57.9023) | Bit/dim 3.6427(3.6502) | Xent 0.8230(0.8443) | Loss 12.2603(9.5519) | Error 0.2877(0.3021) Steps 622(620.98) | Grad Norm 4.7266(6.8559) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 55.0071(57.8155) | Bit/dim 3.6515(3.6503) | Xent 0.8562(0.8447) | Loss 9.1520(9.5399) | Error 0.3116(0.3024) Steps 628(621.19) | Grad Norm 4.9772(6.7995) | Total Time 0.00(0.00)\n",
      "Iter 2241 | Time 56.7998(57.7850) | Bit/dim 3.6520(3.6503) | Xent 0.8197(0.8439) | Loss 8.9164(9.5212) | Error 0.2940(0.3022) Steps 634(621.57) | Grad Norm 2.4684(6.6696) | Total Time 0.00(0.00)\n",
      "Iter 2242 | Time 59.1566(57.8261) | Bit/dim 3.6516(3.6504) | Xent 0.8052(0.8427) | Loss 8.9413(9.5038) | Error 0.2869(0.3017) Steps 634(621.94) | Grad Norm 5.3384(6.6297) | Total Time 0.00(0.00)\n",
      "Iter 2243 | Time 61.1765(57.9267) | Bit/dim 3.6223(3.6495) | Xent 0.8309(0.8424) | Loss 9.0134(9.4891) | Error 0.2981(0.3016) Steps 622(621.95) | Grad Norm 4.1890(6.5564) | Total Time 0.00(0.00)\n",
      "Iter 2244 | Time 56.7212(57.8905) | Bit/dim 3.6576(3.6498) | Xent 0.8057(0.8413) | Loss 8.8290(9.4693) | Error 0.2900(0.3013) Steps 616(621.77) | Grad Norm 5.6054(6.5279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 23.3247, Epoch Time 386.8552(391.4047), Bit/dim 3.6460(best: 3.6442), Xent 0.8488, Loss 4.0704, Error 0.2996(best: 0.3017)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2245 | Time 55.2052(57.8099) | Bit/dim 3.6500(3.6498) | Xent 0.8074(0.8403) | Loss 11.9755(9.5445) | Error 0.2886(0.3009) Steps 616(621.60) | Grad Norm 2.6940(6.4129) | Total Time 0.00(0.00)\n",
      "Iter 2246 | Time 58.0021(57.8157) | Bit/dim 3.6552(3.6499) | Xent 0.8212(0.8397) | Loss 9.1600(9.5329) | Error 0.2963(0.3007) Steps 664(622.87) | Grad Norm 6.2407(6.4077) | Total Time 0.00(0.00)\n",
      "Iter 2247 | Time 57.7198(57.8128) | Bit/dim 3.6432(3.6497) | Xent 0.8069(0.8387) | Loss 8.9664(9.5159) | Error 0.2924(0.3005) Steps 622(622.84) | Grad Norm 5.7481(6.3879) | Total Time 0.00(0.00)\n",
      "Iter 2248 | Time 57.8908(57.8152) | Bit/dim 3.6549(3.6499) | Xent 0.7927(0.8373) | Loss 8.9772(9.4998) | Error 0.2853(0.3000) Steps 592(621.92) | Grad Norm 3.2024(6.2924) | Total Time 0.00(0.00)\n",
      "Iter 2249 | Time 59.4195(57.8633) | Bit/dim 3.6524(3.6500) | Xent 0.8200(0.8368) | Loss 8.7394(9.4770) | Error 0.2983(0.3000) Steps 646(622.64) | Grad Norm 5.3614(6.2644) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 53.7716(57.7405) | Bit/dim 3.6503(3.6500) | Xent 0.8028(0.8358) | Loss 9.0092(9.4629) | Error 0.2853(0.2995) Steps 604(622.08) | Grad Norm 3.8828(6.1930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 23.4040, Epoch Time 381.3264(391.1024), Bit/dim 3.6476(best: 3.6442), Xent 0.8466, Loss 4.0709, Error 0.3003(best: 0.2996)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2251 | Time 56.4489(57.7018) | Bit/dim 3.6495(3.6500) | Xent 0.8321(0.8357) | Loss 11.7132(9.5304) | Error 0.2924(0.2993) Steps 634(622.44) | Grad Norm 4.0024(6.1273) | Total Time 0.00(0.00)\n",
      "Iter 2252 | Time 58.1545(57.7154) | Bit/dim 3.6438(3.6498) | Xent 0.8063(0.8348) | Loss 8.7935(9.5083) | Error 0.2964(0.2992) Steps 592(621.52) | Grad Norm 4.2727(6.0716) | Total Time 0.00(0.00)\n",
      "Iter 2253 | Time 61.2844(57.8224) | Bit/dim 3.6430(3.6496) | Xent 0.8136(0.8342) | Loss 8.9543(9.4917) | Error 0.2947(0.2991) Steps 616(621.36) | Grad Norm 4.5779(6.0268) | Total Time 0.00(0.00)\n",
      "Iter 2254 | Time 60.7455(57.9101) | Bit/dim 3.6428(3.6494) | Xent 0.8043(0.8333) | Loss 8.9272(9.4748) | Error 0.2903(0.2988) Steps 616(621.20) | Grad Norm 4.7659(5.9890) | Total Time 0.00(0.00)\n",
      "Iter 2255 | Time 54.7907(57.8166) | Bit/dim 3.6508(3.6494) | Xent 0.8181(0.8328) | Loss 8.6458(9.4499) | Error 0.2955(0.2987) Steps 592(620.32) | Grad Norm 3.3967(5.9112) | Total Time 0.00(0.00)\n",
      "Iter 2256 | Time 58.7526(57.8446) | Bit/dim 3.6516(3.6495) | Xent 0.7906(0.8316) | Loss 9.0135(9.4368) | Error 0.2866(0.2984) Steps 604(619.83) | Grad Norm 5.7968(5.9078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 23.1371, Epoch Time 389.4366(391.0524), Bit/dim 3.6443(best: 3.6442), Xent 0.8599, Loss 4.0743, Error 0.3049(best: 0.2996)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2257 | Time 54.5691(57.7464) | Bit/dim 3.6417(3.6492) | Xent 0.8182(0.8311) | Loss 11.9603(9.5125) | Error 0.2909(0.2981) Steps 634(620.26) | Grad Norm 7.3193(5.9501) | Total Time 0.00(0.00)\n",
      "Iter 2258 | Time 54.5673(57.6510) | Bit/dim 3.6343(3.6488) | Xent 0.8107(0.8305) | Loss 8.9694(9.4962) | Error 0.2916(0.2980) Steps 586(619.23) | Grad Norm 3.5366(5.8777) | Total Time 0.00(0.00)\n",
      "Iter 2259 | Time 57.1518(57.6360) | Bit/dim 3.6505(3.6488) | Xent 0.8124(0.8300) | Loss 8.9324(9.4793) | Error 0.2904(0.2977) Steps 592(618.41) | Grad Norm 3.6795(5.8118) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 61.2131(57.7433) | Bit/dim 3.6402(3.6486) | Xent 0.8028(0.8292) | Loss 8.8857(9.4615) | Error 0.2874(0.2974) Steps 628(618.70) | Grad Norm 4.6221(5.7761) | Total Time 0.00(0.00)\n",
      "Iter 2261 | Time 55.9737(57.6902) | Bit/dim 3.6464(3.6485) | Xent 0.7968(0.8282) | Loss 8.7425(9.4399) | Error 0.2840(0.2970) Steps 616(618.62) | Grad Norm 5.3664(5.7638) | Total Time 0.00(0.00)\n",
      "Iter 2262 | Time 59.5361(57.7456) | Bit/dim 3.6613(3.6489) | Xent 0.8130(0.8277) | Loss 8.8357(9.4218) | Error 0.2913(0.2968) Steps 646(619.44) | Grad Norm 4.6371(5.7300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 22.7653, Epoch Time 381.8068(390.7750), Bit/dim 3.6453(best: 3.6442), Xent 0.8500, Loss 4.0703, Error 0.3014(best: 0.2996)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2263 | Time 58.5658(57.7702) | Bit/dim 3.6431(3.6487) | Xent 0.8123(0.8273) | Loss 12.1124(9.5025) | Error 0.2876(0.2966) Steps 640(620.06) | Grad Norm 3.8622(5.6740) | Total Time 0.00(0.00)\n",
      "Iter 2264 | Time 58.0429(57.7784) | Bit/dim 3.6433(3.6486) | Xent 0.8082(0.8267) | Loss 8.9359(9.4855) | Error 0.2880(0.2963) Steps 604(619.58) | Grad Norm 4.1813(5.6292) | Total Time 0.00(0.00)\n",
      "Iter 2265 | Time 56.9482(57.7535) | Bit/dim 3.6450(3.6485) | Xent 0.8054(0.8261) | Loss 8.9853(9.4705) | Error 0.2943(0.2962) Steps 622(619.65) | Grad Norm 3.4329(5.5633) | Total Time 0.00(0.00)\n",
      "Iter 2266 | Time 61.8742(57.8771) | Bit/dim 3.6430(3.6483) | Xent 0.8165(0.8258) | Loss 8.9877(9.4560) | Error 0.2966(0.2963) Steps 592(618.82) | Grad Norm 3.2076(5.4926) | Total Time 0.00(0.00)\n",
      "Iter 2267 | Time 61.2958(57.9797) | Bit/dim 3.6417(3.6481) | Xent 0.8004(0.8250) | Loss 8.7229(9.4340) | Error 0.2860(0.2959) Steps 652(619.81) | Grad Norm 3.7537(5.4405) | Total Time 0.00(0.00)\n",
      "Iter 2268 | Time 58.4999(57.9953) | Bit/dim 3.6505(3.6482) | Xent 0.8055(0.8244) | Loss 8.9163(9.4185) | Error 0.2879(0.2957) Steps 622(619.88) | Grad Norm 3.1807(5.3727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 23.3442, Epoch Time 394.4931(390.8866), Bit/dim 3.6463(best: 3.6442), Xent 0.8413, Loss 4.0669, Error 0.2962(best: 0.2996)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2269 | Time 54.4470(57.8888) | Bit/dim 3.6417(3.6480) | Xent 0.7972(0.8236) | Loss 11.7461(9.4883) | Error 0.2812(0.2953) Steps 604(619.40) | Grad Norm 2.4513(5.2850) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 55.2415(57.8094) | Bit/dim 3.6338(3.6475) | Xent 0.7828(0.8224) | Loss 8.8435(9.4690) | Error 0.2794(0.2948) Steps 586(618.40) | Grad Norm 3.6404(5.2357) | Total Time 0.00(0.00)\n",
      "Iter 2271 | Time 62.5711(57.9523) | Bit/dim 3.6389(3.6473) | Xent 0.7966(0.8216) | Loss 9.1064(9.4581) | Error 0.2870(0.2946) Steps 628(618.69) | Grad Norm 6.2229(5.2653) | Total Time 0.00(0.00)\n",
      "Iter 2272 | Time 57.8181(57.9482) | Bit/dim 3.6612(3.6477) | Xent 0.8145(0.8214) | Loss 8.9046(9.4415) | Error 0.2881(0.2944) Steps 604(618.25) | Grad Norm 7.6614(5.3372) | Total Time 0.00(0.00)\n",
      "Iter 2273 | Time 58.3274(57.9596) | Bit/dim 3.6501(3.6478) | Xent 0.8303(0.8217) | Loss 8.8924(9.4250) | Error 0.2975(0.2945) Steps 616(618.18) | Grad Norm 8.5948(5.4349) | Total Time 0.00(0.00)\n",
      "Iter 2274 | Time 57.1857(57.9364) | Bit/dim 3.6500(3.6478) | Xent 0.8631(0.8229) | Loss 8.8804(9.4087) | Error 0.3064(0.2948) Steps 604(617.76) | Grad Norm 11.3797(5.6133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 23.1364, Epoch Time 385.2038(390.7161), Bit/dim 3.6456(best: 3.6442), Xent 0.8733, Loss 4.0823, Error 0.3098(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2275 | Time 53.5842(57.8058) | Bit/dim 3.6460(3.6478) | Xent 0.8276(0.8231) | Loss 11.7210(9.4781) | Error 0.2977(0.2949) Steps 604(617.34) | Grad Norm 11.1633(5.7798) | Total Time 0.00(0.00)\n",
      "Iter 2276 | Time 57.2516(57.7892) | Bit/dim 3.6548(3.6480) | Xent 0.8181(0.8229) | Loss 8.8377(9.4589) | Error 0.2906(0.2948) Steps 598(616.76) | Grad Norm 6.5073(5.8016) | Total Time 0.00(0.00)\n",
      "Iter 2277 | Time 59.1693(57.8306) | Bit/dim 3.6547(3.6482) | Xent 0.8375(0.8234) | Loss 9.1218(9.4487) | Error 0.3025(0.2950) Steps 670(618.36) | Grad Norm 7.9905(5.8673) | Total Time 0.00(0.00)\n",
      "Iter 2278 | Time 57.0813(57.8081) | Bit/dim 3.6529(3.6483) | Xent 0.8992(0.8256) | Loss 8.8934(9.4321) | Error 0.3217(0.2958) Steps 628(618.65) | Grad Norm 14.0822(6.1137) | Total Time 0.00(0.00)\n",
      "Iter 2279 | Time 57.9745(57.8131) | Bit/dim 3.6442(3.6482) | Xent 0.8511(0.8264) | Loss 8.7894(9.4128) | Error 0.2980(0.2959) Steps 640(619.29) | Grad Norm 12.3300(6.3002) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 62.8095(57.9630) | Bit/dim 3.6379(3.6479) | Xent 0.8500(0.8271) | Loss 9.1408(9.4046) | Error 0.3057(0.2962) Steps 622(619.37) | Grad Norm 7.2448(6.3285) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 23.0178, Epoch Time 387.2516(390.6122), Bit/dim 3.6479(best: 3.6442), Xent 0.9274, Loss 4.1116, Error 0.3272(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2281 | Time 59.7585(58.0169) | Bit/dim 3.6332(3.6475) | Xent 0.8942(0.8291) | Loss 11.8944(9.4793) | Error 0.3201(0.2969) Steps 604(618.91) | Grad Norm 14.5575(6.5754) | Total Time 0.00(0.00)\n",
      "Iter 2282 | Time 61.5123(58.1217) | Bit/dim 3.6499(3.6475) | Xent 0.8068(0.8284) | Loss 8.9342(9.4630) | Error 0.2950(0.2968) Steps 622(619.00) | Grad Norm 10.3530(6.6887) | Total Time 0.00(0.00)\n",
      "Iter 2283 | Time 62.8561(58.2638) | Bit/dim 3.6494(3.6476) | Xent 0.8848(0.8301) | Loss 9.1349(9.4531) | Error 0.3177(0.2975) Steps 628(619.27) | Grad Norm 9.4416(6.7713) | Total Time 0.00(0.00)\n",
      "Iter 2284 | Time 61.9361(58.3739) | Bit/dim 3.6489(3.6476) | Xent 0.8615(0.8311) | Loss 8.9781(9.4389) | Error 0.3085(0.2978) Steps 616(619.17) | Grad Norm 9.1926(6.8439) | Total Time 0.00(0.00)\n",
      "Iter 2285 | Time 59.8338(58.4177) | Bit/dim 3.6509(3.6477) | Xent 0.8494(0.8316) | Loss 8.9921(9.4255) | Error 0.3043(0.2980) Steps 616(619.08) | Grad Norm 6.3095(6.8279) | Total Time 0.00(0.00)\n",
      "Iter 2286 | Time 62.0955(58.5281) | Bit/dim 3.6649(3.6482) | Xent 0.8623(0.8325) | Loss 9.0300(9.4136) | Error 0.3063(0.2982) Steps 622(619.17) | Grad Norm 9.3892(6.9048) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 22.8275, Epoch Time 406.5812(391.0912), Bit/dim 3.6535(best: 3.6442), Xent 0.8837, Loss 4.0954, Error 0.3154(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2287 | Time 53.9387(58.3904) | Bit/dim 3.6633(3.6487) | Xent 0.8361(0.8327) | Loss 12.2436(9.4985) | Error 0.2964(0.2982) Steps 604(618.71) | Grad Norm 12.9293(7.0855) | Total Time 0.00(0.00)\n",
      "Iter 2288 | Time 56.2512(58.3262) | Bit/dim 3.6440(3.6486) | Xent 0.8373(0.8328) | Loss 9.0804(9.4860) | Error 0.2991(0.2982) Steps 616(618.63) | Grad Norm 7.1909(7.0887) | Total Time 0.00(0.00)\n",
      "Iter 2289 | Time 61.1760(58.4117) | Bit/dim 3.6551(3.6488) | Xent 0.8384(0.8330) | Loss 9.0857(9.4740) | Error 0.3039(0.2984) Steps 634(619.09) | Grad Norm 10.1913(7.1817) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 61.7000(58.5104) | Bit/dim 3.6552(3.6490) | Xent 0.8596(0.8338) | Loss 9.1869(9.4654) | Error 0.3077(0.2987) Steps 640(619.72) | Grad Norm 8.8527(7.2319) | Total Time 0.00(0.00)\n",
      "Iter 2291 | Time 58.3660(58.5060) | Bit/dim 3.6402(3.6487) | Xent 0.8556(0.8344) | Loss 8.7308(9.4433) | Error 0.3109(0.2990) Steps 616(619.61) | Grad Norm 9.0571(7.2866) | Total Time 0.00(0.00)\n",
      "Iter 2292 | Time 59.5313(58.5368) | Bit/dim 3.6520(3.6488) | Xent 0.8202(0.8340) | Loss 8.9495(9.4285) | Error 0.2907(0.2988) Steps 628(619.86) | Grad Norm 5.1291(7.2219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 23.0572, Epoch Time 389.9618(391.0574), Bit/dim 3.6473(best: 3.6442), Xent 0.8671, Loss 4.0808, Error 0.3085(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2293 | Time 59.4366(58.5638) | Bit/dim 3.6514(3.6489) | Xent 0.8228(0.8337) | Loss 12.1936(9.5115) | Error 0.2920(0.2986) Steps 616(619.74) | Grad Norm 6.1131(7.1886) | Total Time 0.00(0.00)\n",
      "Iter 2294 | Time 62.1193(58.6704) | Bit/dim 3.6495(3.6489) | Xent 0.8120(0.8330) | Loss 8.9748(9.4954) | Error 0.2947(0.2985) Steps 652(620.71) | Grad Norm 4.9189(7.1205) | Total Time 0.00(0.00)\n",
      "Iter 2295 | Time 56.2508(58.5979) | Bit/dim 3.6398(3.6486) | Xent 0.8095(0.8323) | Loss 9.0329(9.4815) | Error 0.2865(0.2981) Steps 598(620.03) | Grad Norm 6.1688(7.0920) | Total Time 0.00(0.00)\n",
      "Iter 2296 | Time 60.5354(58.6560) | Bit/dim 3.6475(3.6486) | Xent 0.8043(0.8315) | Loss 8.9295(9.4649) | Error 0.2945(0.2980) Steps 622(620.09) | Grad Norm 6.0390(7.0604) | Total Time 0.00(0.00)\n",
      "Iter 2297 | Time 60.9641(58.7252) | Bit/dim 3.6589(3.6489) | Xent 0.8562(0.8322) | Loss 8.9680(9.4500) | Error 0.3087(0.2983) Steps 622(620.15) | Grad Norm 9.0106(7.1189) | Total Time 0.00(0.00)\n",
      "Iter 2298 | Time 63.4328(58.8665) | Bit/dim 3.6354(3.6485) | Xent 0.8227(0.8319) | Loss 8.9141(9.4339) | Error 0.2959(0.2982) Steps 610(619.84) | Grad Norm 5.2751(7.0636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 22.6739, Epoch Time 401.2875(391.3643), Bit/dim 3.6503(best: 3.6442), Xent 0.8483, Loss 4.0744, Error 0.3034(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2299 | Time 61.5228(58.9461) | Bit/dim 3.6442(3.6484) | Xent 0.7978(0.8309) | Loss 12.3199(9.5205) | Error 0.2778(0.2976) Steps 652(620.81) | Grad Norm 4.5918(6.9894) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 58.3600(58.9286) | Bit/dim 3.6498(3.6484) | Xent 0.8269(0.8308) | Loss 9.0025(9.5050) | Error 0.2973(0.2976) Steps 664(622.10) | Grad Norm 4.9154(6.9272) | Total Time 0.00(0.00)\n",
      "Iter 2301 | Time 59.1864(58.9363) | Bit/dim 3.6497(3.6484) | Xent 0.8344(0.8309) | Loss 9.0583(9.4916) | Error 0.2965(0.2976) Steps 646(622.82) | Grad Norm 6.6198(6.9180) | Total Time 0.00(0.00)\n",
      "Iter 2302 | Time 60.1876(58.9738) | Bit/dim 3.6406(3.6482) | Xent 0.8292(0.8308) | Loss 8.8880(9.4735) | Error 0.2927(0.2974) Steps 622(622.79) | Grad Norm 4.5578(6.8472) | Total Time 0.00(0.00)\n",
      "Iter 2303 | Time 57.9616(58.9435) | Bit/dim 3.6584(3.6485) | Xent 0.7990(0.8299) | Loss 9.0458(9.4606) | Error 0.2843(0.2970) Steps 628(622.95) | Grad Norm 6.8327(6.8467) | Total Time 0.00(0.00)\n",
      "Iter 2304 | Time 59.9573(58.9739) | Bit/dim 3.6470(3.6485) | Xent 0.8109(0.8293) | Loss 8.9325(9.4448) | Error 0.2917(0.2969) Steps 610(622.56) | Grad Norm 8.2816(6.8898) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 23.8971, Epoch Time 397.0327(391.5343), Bit/dim 3.6496(best: 3.6442), Xent 0.8521, Loss 4.0756, Error 0.2975(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2305 | Time 63.7579(59.1174) | Bit/dim 3.6403(3.6482) | Xent 0.8180(0.8290) | Loss 12.3616(9.5323) | Error 0.2961(0.2969) Steps 616(622.37) | Grad Norm 8.5882(6.9407) | Total Time 0.00(0.00)\n",
      "Iter 2306 | Time 58.5771(59.1012) | Bit/dim 3.6625(3.6486) | Xent 0.7827(0.8276) | Loss 8.7566(9.5090) | Error 0.2796(0.2963) Steps 652(623.25) | Grad Norm 2.9457(6.8209) | Total Time 0.00(0.00)\n",
      "Iter 2307 | Time 59.9915(59.1279) | Bit/dim 3.6425(3.6485) | Xent 0.8260(0.8275) | Loss 9.0238(9.4945) | Error 0.3024(0.2965) Steps 622(623.22) | Grad Norm 7.1527(6.8308) | Total Time 0.00(0.00)\n",
      "Iter 2308 | Time 59.6604(59.1439) | Bit/dim 3.6320(3.6480) | Xent 0.8210(0.8273) | Loss 8.8221(9.4743) | Error 0.2913(0.2964) Steps 616(623.00) | Grad Norm 7.1993(6.8419) | Total Time 0.00(0.00)\n",
      "Iter 2309 | Time 54.4879(59.0042) | Bit/dim 3.6449(3.6479) | Xent 0.8138(0.8269) | Loss 8.8715(9.4562) | Error 0.2909(0.2962) Steps 628(623.15) | Grad Norm 7.2023(6.8527) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 56.0811(58.9165) | Bit/dim 3.6444(3.6478) | Xent 0.8004(0.8261) | Loss 8.8818(9.4390) | Error 0.2849(0.2959) Steps 622(623.12) | Grad Norm 3.8511(6.7627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 23.4450, Epoch Time 392.2594(391.5561), Bit/dim 3.6508(best: 3.6442), Xent 0.8585, Loss 4.0801, Error 0.3062(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2311 | Time 62.8015(59.0331) | Bit/dim 3.6432(3.6476) | Xent 0.8255(0.8261) | Loss 12.2191(9.5224) | Error 0.2956(0.2959) Steps 664(624.34) | Grad Norm 6.7623(6.7627) | Total Time 0.00(0.00)\n",
      "Iter 2312 | Time 56.0707(58.9442) | Bit/dim 3.6624(3.6481) | Xent 0.8162(0.8258) | Loss 9.0093(9.5070) | Error 0.2923(0.2957) Steps 616(624.09) | Grad Norm 6.2215(6.7464) | Total Time 0.00(0.00)\n",
      "Iter 2313 | Time 62.6907(59.0566) | Bit/dim 3.6565(3.6483) | Xent 0.7901(0.8248) | Loss 8.9245(9.4895) | Error 0.2809(0.2953) Steps 604(623.49) | Grad Norm 5.1633(6.6989) | Total Time 0.00(0.00)\n",
      "Iter 2314 | Time 62.5608(59.1617) | Bit/dim 3.6459(3.6483) | Xent 0.8174(0.8245) | Loss 8.9368(9.4729) | Error 0.2915(0.2952) Steps 652(624.34) | Grad Norm 6.2447(6.6853) | Total Time 0.00(0.00)\n",
      "Iter 2315 | Time 57.5433(59.1132) | Bit/dim 3.6448(3.6482) | Xent 0.8057(0.8240) | Loss 8.7725(9.4519) | Error 0.2879(0.2950) Steps 634(624.63) | Grad Norm 6.0836(6.6672) | Total Time 0.00(0.00)\n",
      "Iter 2316 | Time 57.9138(59.0772) | Bit/dim 3.6320(3.6477) | Xent 0.8289(0.8241) | Loss 9.0521(9.4399) | Error 0.2994(0.2951) Steps 604(624.02) | Grad Norm 9.2398(6.7444) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 23.2462, Epoch Time 398.7613(391.7722), Bit/dim 3.6487(best: 3.6442), Xent 0.8767, Loss 4.0871, Error 0.3074(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2317 | Time 56.0706(58.9870) | Bit/dim 3.6468(3.6476) | Xent 0.8442(0.8247) | Loss 12.0329(9.5177) | Error 0.3010(0.2953) Steps 616(623.77) | Grad Norm 11.3232(6.8818) | Total Time 0.00(0.00)\n",
      "Iter 2318 | Time 60.4885(59.0320) | Bit/dim 3.6540(3.6478) | Xent 0.8353(0.8250) | Loss 8.5740(9.4894) | Error 0.2943(0.2952) Steps 646(624.44) | Grad Norm 10.0089(6.9756) | Total Time 0.00(0.00)\n",
      "Iter 2319 | Time 61.5608(59.1079) | Bit/dim 3.6437(3.6477) | Xent 0.8222(0.8250) | Loss 8.8633(9.4706) | Error 0.2914(0.2951) Steps 604(623.83) | Grad Norm 4.8202(6.9109) | Total Time 0.00(0.00)\n",
      "Iter 2320 | Time 56.5304(59.0306) | Bit/dim 3.6464(3.6477) | Xent 0.8049(0.8244) | Loss 8.7755(9.4498) | Error 0.2863(0.2949) Steps 634(624.13) | Grad Norm 4.2389(6.8308) | Total Time 0.00(0.00)\n",
      "Iter 2321 | Time 58.1108(59.0030) | Bit/dim 3.6526(3.6478) | Xent 0.8396(0.8248) | Loss 8.9597(9.4351) | Error 0.2963(0.2949) Steps 598(623.35) | Grad Norm 9.2265(6.9027) | Total Time 0.00(0.00)\n",
      "Iter 2322 | Time 57.6816(58.9633) | Bit/dim 3.6325(3.6474) | Xent 0.8266(0.8249) | Loss 8.8861(9.4186) | Error 0.2934(0.2949) Steps 592(622.41) | Grad Norm 10.6769(7.0159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 24.0700, Epoch Time 390.7411(391.7413), Bit/dim 3.6433(best: 3.6442), Xent 0.8553, Loss 4.0709, Error 0.3048(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2323 | Time 64.0752(59.1167) | Bit/dim 3.6460(3.6473) | Xent 0.8148(0.8246) | Loss 12.0972(9.4990) | Error 0.2911(0.2947) Steps 592(621.50) | Grad Norm 5.4820(6.9699) | Total Time 0.00(0.00)\n",
      "Iter 2324 | Time 55.7769(59.0165) | Bit/dim 3.6314(3.6468) | Xent 0.8201(0.8244) | Loss 8.8390(9.4792) | Error 0.2920(0.2947) Steps 646(622.23) | Grad Norm 7.6634(6.9907) | Total Time 0.00(0.00)\n",
      "Iter 2325 | Time 62.7463(59.1284) | Bit/dim 3.6437(3.6467) | Xent 0.8243(0.8244) | Loss 8.9742(9.4640) | Error 0.2976(0.2948) Steps 658(623.30) | Grad Norm 9.0507(7.0525) | Total Time 0.00(0.00)\n",
      "Iter 2326 | Time 58.3345(59.1046) | Bit/dim 3.6310(3.6463) | Xent 0.8300(0.8246) | Loss 8.8849(9.4466) | Error 0.3014(0.2950) Steps 634(623.63) | Grad Norm 5.9816(7.0203) | Total Time 0.00(0.00)\n",
      "Iter 2327 | Time 61.3004(59.1704) | Bit/dim 3.6509(3.6464) | Xent 0.8017(0.8239) | Loss 8.7311(9.4252) | Error 0.2894(0.2948) Steps 628(623.76) | Grad Norm 5.9690(6.9888) | Total Time 0.00(0.00)\n",
      "Iter 2328 | Time 61.0297(59.2262) | Bit/dim 3.6524(3.6466) | Xent 0.8513(0.8247) | Loss 9.1398(9.4166) | Error 0.3069(0.2951) Steps 622(623.70) | Grad Norm 9.4161(7.0616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 22.4615, Epoch Time 401.6327(392.0380), Bit/dim 3.6421(best: 3.6433), Xent 0.8722, Loss 4.0782, Error 0.3095(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2329 | Time 52.5083(59.0247) | Bit/dim 3.6508(3.6467) | Xent 0.8211(0.8246) | Loss 12.0155(9.4946) | Error 0.2943(0.2951) Steps 610(623.29) | Grad Norm 6.4671(7.0438) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 58.3118(59.0033) | Bit/dim 3.6490(3.6468) | Xent 0.8309(0.8248) | Loss 9.0122(9.4801) | Error 0.2920(0.2950) Steps 634(623.61) | Grad Norm 8.6207(7.0911) | Total Time 0.00(0.00)\n",
      "Iter 2331 | Time 58.0400(58.9744) | Bit/dim 3.6394(3.6466) | Xent 0.8843(0.8266) | Loss 9.0792(9.4681) | Error 0.3184(0.2957) Steps 598(622.85) | Grad Norm 14.6534(7.3180) | Total Time 0.00(0.00)\n",
      "Iter 2332 | Time 55.3240(58.8649) | Bit/dim 3.6371(3.6463) | Xent 0.8393(0.8270) | Loss 9.1103(9.4573) | Error 0.3031(0.2959) Steps 628(623.00) | Grad Norm 7.2311(7.3154) | Total Time 0.00(0.00)\n",
      "Iter 2333 | Time 56.9679(58.8080) | Bit/dim 3.6477(3.6463) | Xent 0.8410(0.8274) | Loss 9.0876(9.4463) | Error 0.3066(0.2963) Steps 616(622.79) | Grad Norm 12.4020(7.4680) | Total Time 0.00(0.00)\n",
      "Iter 2334 | Time 61.8513(58.8993) | Bit/dim 3.6590(3.6467) | Xent 0.8697(0.8287) | Loss 9.1323(9.4368) | Error 0.3123(0.2967) Steps 646(623.49) | Grad Norm 13.9014(7.6610) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 23.8231, Epoch Time 383.1433(391.7712), Bit/dim 3.6439(best: 3.6421), Xent 0.8463, Loss 4.0671, Error 0.2966(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2335 | Time 68.5656(59.1893) | Bit/dim 3.6499(3.6468) | Xent 0.7867(0.8274) | Loss 12.0543(9.5154) | Error 0.2871(0.2965) Steps 652(624.34) | Grad Norm 3.9479(7.5496) | Total Time 0.00(0.00)\n",
      "Iter 2336 | Time 68.7610(59.4764) | Bit/dim 3.6339(3.6464) | Xent 0.8550(0.8282) | Loss 9.0647(9.5018) | Error 0.3026(0.2966) Steps 604(623.73) | Grad Norm 12.3932(7.6949) | Total Time 0.00(0.00)\n",
      "Iter 2337 | Time 64.9526(59.6407) | Bit/dim 3.6373(3.6461) | Xent 0.8184(0.8279) | Loss 8.8496(9.4823) | Error 0.2877(0.2964) Steps 640(624.22) | Grad Norm 6.6899(7.6647) | Total Time 0.00(0.00)\n",
      "Iter 2338 | Time 70.8887(59.9781) | Bit/dim 3.6492(3.6462) | Xent 0.8111(0.8274) | Loss 8.9836(9.4673) | Error 0.2871(0.2961) Steps 640(624.69) | Grad Norm 7.0584(7.6465) | Total Time 0.00(0.00)\n",
      "Iter 2339 | Time 67.4111(60.2011) | Bit/dim 3.6439(3.6462) | Xent 0.8298(0.8275) | Loss 9.0008(9.4533) | Error 0.2951(0.2961) Steps 652(625.51) | Grad Norm 4.2112(7.5435) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 73.2700(60.5932) | Bit/dim 3.6398(3.6460) | Xent 0.8297(0.8276) | Loss 8.9837(9.4392) | Error 0.3000(0.2962) Steps 646(626.13) | Grad Norm 7.3535(7.5378) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 23.6585, Epoch Time 453.1860(393.6136), Bit/dim 3.6447(best: 3.6421), Xent 0.8485, Loss 4.0690, Error 0.3014(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2341 | Time 64.2009(60.7014) | Bit/dim 3.6422(3.6459) | Xent 0.8285(0.8276) | Loss 11.8692(9.5121) | Error 0.3006(0.2963) Steps 646(626.72) | Grad Norm 4.7052(7.4528) | Total Time 0.00(0.00)\n",
      "Iter 2342 | Time 67.9393(60.9186) | Bit/dim 3.6448(3.6458) | Xent 0.8171(0.8273) | Loss 8.9215(9.4944) | Error 0.2910(0.2962) Steps 652(627.48) | Grad Norm 5.7191(7.4008) | Total Time 0.00(0.00)\n",
      "Iter 2343 | Time 63.1382(60.9852) | Bit/dim 3.6473(3.6459) | Xent 0.7890(0.8261) | Loss 8.9764(9.4789) | Error 0.2814(0.2957) Steps 610(626.96) | Grad Norm 2.6466(7.2582) | Total Time 0.00(0.00)\n",
      "Iter 2344 | Time 64.2735(61.0838) | Bit/dim 3.6457(3.6459) | Xent 0.8101(0.8257) | Loss 8.9841(9.4640) | Error 0.2934(0.2956) Steps 580(625.55) | Grad Norm 4.5027(7.1755) | Total Time 0.00(0.00)\n",
      "Iter 2345 | Time 71.6980(61.4022) | Bit/dim 3.6385(3.6456) | Xent 0.8113(0.8252) | Loss 9.0889(9.4528) | Error 0.2916(0.2955) Steps 616(625.26) | Grad Norm 7.2741(7.1785) | Total Time 0.00(0.00)\n",
      "Iter 2346 | Time 68.4327(61.6131) | Bit/dim 3.6491(3.6457) | Xent 0.8118(0.8248) | Loss 9.0676(9.4412) | Error 0.2915(0.2954) Steps 598(624.44) | Grad Norm 9.2446(7.2404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 22.9968, Epoch Time 438.3394(394.9554), Bit/dim 3.6446(best: 3.6421), Xent 0.8467, Loss 4.0680, Error 0.3041(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2347 | Time 65.1836(61.7203) | Bit/dim 3.6380(3.6455) | Xent 0.8130(0.8245) | Loss 11.9468(9.5164) | Error 0.2911(0.2953) Steps 652(625.27) | Grad Norm 5.1449(7.1776) | Total Time 0.00(0.00)\n",
      "Iter 2348 | Time 65.1388(61.8228) | Bit/dim 3.6389(3.6453) | Xent 0.8040(0.8239) | Loss 8.9933(9.5007) | Error 0.2870(0.2950) Steps 628(625.35) | Grad Norm 6.5418(7.1585) | Total Time 0.00(0.00)\n",
      "Iter 2349 | Time 64.8356(61.9132) | Bit/dim 3.6478(3.6454) | Xent 0.8303(0.8240) | Loss 9.0484(9.4871) | Error 0.2975(0.2951) Steps 628(625.43) | Grad Norm 8.7214(7.2054) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 70.9635(62.1847) | Bit/dim 3.6450(3.6454) | Xent 0.7811(0.8228) | Loss 9.1130(9.4759) | Error 0.2752(0.2945) Steps 616(625.15) | Grad Norm 3.5629(7.0961) | Total Time 0.00(0.00)\n",
      "Iter 2351 | Time 62.7814(62.2026) | Bit/dim 3.6470(3.6454) | Xent 0.8102(0.8224) | Loss 8.8368(9.4567) | Error 0.2875(0.2943) Steps 646(625.77) | Grad Norm 5.3680(7.0443) | Total Time 0.00(0.00)\n",
      "Iter 2352 | Time 69.6113(62.4249) | Bit/dim 3.6336(3.6451) | Xent 0.8039(0.8218) | Loss 8.8655(9.4390) | Error 0.2849(0.2940) Steps 628(625.84) | Grad Norm 4.6764(6.9732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 22.7895, Epoch Time 437.3455(396.2271), Bit/dim 3.6401(best: 3.6421), Xent 0.8427, Loss 4.0614, Error 0.2991(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2353 | Time 65.9373(62.5302) | Bit/dim 3.6337(3.6447) | Xent 0.8059(0.8213) | Loss 12.0711(9.5180) | Error 0.2895(0.2939) Steps 628(625.91) | Grad Norm 3.9431(6.8823) | Total Time 0.00(0.00)\n",
      "Iter 2354 | Time 64.0757(62.5766) | Bit/dim 3.6453(3.6447) | Xent 0.7887(0.8204) | Loss 8.7747(9.4957) | Error 0.2798(0.2935) Steps 604(625.25) | Grad Norm 2.7834(6.7594) | Total Time 0.00(0.00)\n",
      "Iter 2355 | Time 69.4553(62.7830) | Bit/dim 3.6388(3.6446) | Xent 0.8359(0.8208) | Loss 8.9663(9.4798) | Error 0.2970(0.2936) Steps 634(625.51) | Grad Norm 7.9581(6.7953) | Total Time 0.00(0.00)\n",
      "Iter 2356 | Time 69.0350(62.9705) | Bit/dim 3.6576(3.6450) | Xent 0.8065(0.8204) | Loss 8.8659(9.4614) | Error 0.2907(0.2935) Steps 622(625.41) | Grad Norm 6.9247(6.7992) | Total Time 0.00(0.00)\n",
      "Iter 2357 | Time 68.8481(63.1469) | Bit/dim 3.6370(3.6447) | Xent 0.8038(0.8199) | Loss 8.8514(9.4431) | Error 0.2926(0.2935) Steps 598(624.58) | Grad Norm 6.4072(6.7874) | Total Time 0.00(0.00)\n",
      "Iter 2358 | Time 63.7444(63.1648) | Bit/dim 3.6378(3.6445) | Xent 0.8222(0.8200) | Loss 9.0387(9.4309) | Error 0.2956(0.2935) Steps 622(624.51) | Grad Norm 9.8061(6.8780) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 22.7272, Epoch Time 440.0617(397.5421), Bit/dim 3.6486(best: 3.6401), Xent 0.8711, Loss 4.0842, Error 0.3078(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2359 | Time 68.6610(63.3297) | Bit/dim 3.6358(3.6443) | Xent 0.8307(0.8203) | Loss 12.2907(9.5167) | Error 0.2959(0.2936) Steps 640(624.97) | Grad Norm 9.0451(6.9430) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 63.8528(63.3454) | Bit/dim 3.6386(3.6441) | Xent 0.8082(0.8199) | Loss 8.8168(9.4957) | Error 0.2910(0.2935) Steps 610(624.52) | Grad Norm 7.3857(6.9563) | Total Time 0.00(0.00)\n",
      "Iter 2361 | Time 63.1058(63.3382) | Bit/dim 3.6336(3.6438) | Xent 0.8094(0.8196) | Loss 8.9487(9.4793) | Error 0.2900(0.2934) Steps 610(624.09) | Grad Norm 6.2318(6.9346) | Total Time 0.00(0.00)\n",
      "Iter 2362 | Time 65.0650(63.3900) | Bit/dim 3.6476(3.6439) | Xent 0.8436(0.8203) | Loss 9.0094(9.4652) | Error 0.3039(0.2937) Steps 622(624.02) | Grad Norm 12.3748(7.0978) | Total Time 0.00(0.00)\n",
      "Iter 2363 | Time 66.9201(63.4959) | Bit/dim 3.6535(3.6442) | Xent 0.8197(0.8203) | Loss 9.1629(9.4561) | Error 0.2975(0.2938) Steps 610(623.60) | Grad Norm 12.4951(7.2597) | Total Time 0.00(0.00)\n",
      "Iter 2364 | Time 66.6575(63.5907) | Bit/dim 3.6374(3.6440) | Xent 0.8143(0.8201) | Loss 9.1156(9.4459) | Error 0.2837(0.2935) Steps 604(623.02) | Grad Norm 3.1164(7.1354) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 23.3170, Epoch Time 433.5085(398.6211), Bit/dim 3.6487(best: 3.6401), Xent 0.8559, Loss 4.0767, Error 0.2992(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2365 | Time 70.1750(63.7883) | Bit/dim 3.6450(3.6440) | Xent 0.8236(0.8202) | Loss 11.8214(9.5172) | Error 0.2930(0.2935) Steps 592(622.08) | Grad Norm 10.3379(7.2315) | Total Time 0.00(0.00)\n",
      "Iter 2366 | Time 66.1640(63.8595) | Bit/dim 3.6295(3.6436) | Xent 0.8071(0.8198) | Loss 8.7201(9.4933) | Error 0.2909(0.2934) Steps 670(623.52) | Grad Norm 4.3318(7.1445) | Total Time 0.00(0.00)\n",
      "Iter 2367 | Time 74.0710(64.1659) | Bit/dim 3.6487(3.6437) | Xent 0.7818(0.8187) | Loss 9.1078(9.4817) | Error 0.2784(0.2930) Steps 652(624.38) | Grad Norm 6.9792(7.1395) | Total Time 0.00(0.00)\n",
      "Iter 2368 | Time 69.6701(64.3310) | Bit/dim 3.6363(3.6435) | Xent 0.8078(0.8184) | Loss 8.7303(9.4592) | Error 0.2933(0.2930) Steps 622(624.31) | Grad Norm 5.1731(7.0805) | Total Time 0.00(0.00)\n",
      "Iter 2369 | Time 70.7447(64.5234) | Bit/dim 3.6509(3.6437) | Xent 0.7843(0.8174) | Loss 8.9231(9.4431) | Error 0.2821(0.2927) Steps 646(624.96) | Grad Norm 5.3035(7.0272) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 66.6064(64.5859) | Bit/dim 3.6419(3.6437) | Xent 0.8156(0.8173) | Loss 8.9791(9.4292) | Error 0.2891(0.2926) Steps 646(625.59) | Grad Norm 7.2413(7.0336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 23.4044, Epoch Time 456.9933(400.3723), Bit/dim 3.6510(best: 3.6401), Xent 0.8343, Loss 4.0681, Error 0.2925(best: 0.2962)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2371 | Time 69.5068(64.7335) | Bit/dim 3.6349(3.6434) | Xent 0.7803(0.8162) | Loss 12.3343(9.5163) | Error 0.2781(0.2921) Steps 640(626.02) | Grad Norm 3.7401(6.9348) | Total Time 0.00(0.00)\n",
      "Iter 2372 | Time 65.6544(64.7612) | Bit/dim 3.6539(3.6437) | Xent 0.7771(0.8150) | Loss 8.9290(9.4987) | Error 0.2744(0.2916) Steps 628(626.08) | Grad Norm 4.6374(6.8659) | Total Time 0.00(0.00)\n",
      "Iter 2373 | Time 66.2548(64.8060) | Bit/dim 3.6501(3.6439) | Xent 0.8239(0.8153) | Loss 8.7405(9.4760) | Error 0.2914(0.2916) Steps 616(625.78) | Grad Norm 7.7656(6.8929) | Total Time 0.00(0.00)\n",
      "Iter 2374 | Time 70.7658(64.9848) | Bit/dim 3.6307(3.6435) | Xent 0.8305(0.8157) | Loss 9.1914(9.4674) | Error 0.2963(0.2917) Steps 634(626.02) | Grad Norm 8.0983(6.9291) | Total Time 0.00(0.00)\n",
      "Iter 2375 | Time 63.4322(64.9382) | Bit/dim 3.6418(3.6435) | Xent 0.8036(0.8154) | Loss 8.7915(9.4471) | Error 0.2895(0.2917) Steps 640(626.44) | Grad Norm 7.0586(6.9329) | Total Time 0.00(0.00)\n",
      "Iter 2376 | Time 66.7440(64.9924) | Bit/dim 3.6452(3.6435) | Xent 0.8006(0.8149) | Loss 8.9135(9.4311) | Error 0.2911(0.2916) Steps 628(626.49) | Grad Norm 5.5121(6.8903) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 22.7667, Epoch Time 441.2399(401.5983), Bit/dim 3.6456(best: 3.6401), Xent 0.8387, Loss 4.0649, Error 0.2941(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2377 | Time 73.3586(65.2433) | Bit/dim 3.6341(3.6432) | Xent 0.8040(0.8146) | Loss 11.9809(9.5076) | Error 0.2896(0.2916) Steps 610(625.99) | Grad Norm 4.1894(6.8093) | Total Time 0.00(0.00)\n",
      "Iter 2378 | Time 68.0413(65.3273) | Bit/dim 3.6419(3.6432) | Xent 0.8083(0.8144) | Loss 8.9859(9.4920) | Error 0.2934(0.2916) Steps 598(625.15) | Grad Norm 3.7945(6.7188) | Total Time 0.00(0.00)\n",
      "Iter 2379 | Time 73.3690(65.5685) | Bit/dim 3.6409(3.6431) | Xent 0.8065(0.8142) | Loss 9.0991(9.4802) | Error 0.2906(0.2916) Steps 652(625.96) | Grad Norm 5.6560(6.6870) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 70.7591(65.7243) | Bit/dim 3.6521(3.6434) | Xent 0.7949(0.8136) | Loss 8.9860(9.4654) | Error 0.2835(0.2914) Steps 622(625.84) | Grad Norm 6.6814(6.6868) | Total Time 0.00(0.00)\n",
      "Iter 2381 | Time 65.2808(65.7109) | Bit/dim 3.6447(3.6434) | Xent 0.8014(0.8132) | Loss 8.8363(9.4465) | Error 0.2859(0.2912) Steps 610(625.37) | Grad Norm 7.1297(6.7001) | Total Time 0.00(0.00)\n",
      "Iter 2382 | Time 70.3386(65.8498) | Bit/dim 3.6353(3.6432) | Xent 0.7893(0.8125) | Loss 9.0470(9.4345) | Error 0.2771(0.2908) Steps 598(624.55) | Grad Norm 4.6730(6.6393) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 23.4237, Epoch Time 460.6027(403.3685), Bit/dim 3.6417(best: 3.6401), Xent 0.8442, Loss 4.0637, Error 0.2991(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2383 | Time 73.3369(66.0744) | Bit/dim 3.6362(3.6430) | Xent 0.7988(0.8121) | Loss 12.2948(9.5203) | Error 0.2814(0.2905) Steps 628(624.65) | Grad Norm 5.1904(6.5958) | Total Time 0.00(0.00)\n",
      "Iter 2384 | Time 67.5342(66.1182) | Bit/dim 3.6423(3.6430) | Xent 0.7938(0.8116) | Loss 8.7942(9.4985) | Error 0.2840(0.2903) Steps 604(624.03) | Grad Norm 8.4659(6.6519) | Total Time 0.00(0.00)\n",
      "Iter 2385 | Time 68.5470(66.1910) | Bit/dim 3.6269(3.6425) | Xent 0.7939(0.8110) | Loss 8.9864(9.4832) | Error 0.2836(0.2901) Steps 634(624.33) | Grad Norm 3.5954(6.5602) | Total Time 0.00(0.00)\n",
      "Iter 2386 | Time 65.6304(66.1742) | Bit/dim 3.6369(3.6423) | Xent 0.8024(0.8108) | Loss 9.0814(9.4711) | Error 0.2814(0.2898) Steps 622(624.26) | Grad Norm 6.6923(6.5642) | Total Time 0.00(0.00)\n",
      "Iter 2387 | Time 69.4491(66.2725) | Bit/dim 3.6463(3.6424) | Xent 0.8030(0.8105) | Loss 8.9777(9.4563) | Error 0.2881(0.2898) Steps 628(624.37) | Grad Norm 7.5468(6.5937) | Total Time 0.00(0.00)\n",
      "Iter 2388 | Time 60.8346(66.1093) | Bit/dim 3.6482(3.6426) | Xent 0.7961(0.8101) | Loss 8.8233(9.4373) | Error 0.2839(0.2896) Steps 610(623.94) | Grad Norm 5.1559(6.5505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 23.4021, Epoch Time 444.5615(404.6043), Bit/dim 3.6431(best: 3.6401), Xent 0.8336, Loss 4.0599, Error 0.2993(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2389 | Time 68.6612(66.1859) | Bit/dim 3.6521(3.6429) | Xent 0.7910(0.8095) | Loss 11.8724(9.5104) | Error 0.2824(0.2894) Steps 646(624.60) | Grad Norm 3.7312(6.4659) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 70.2372(66.3074) | Bit/dim 3.6229(3.6423) | Xent 0.7806(0.8087) | Loss 8.9254(9.4928) | Error 0.2749(0.2890) Steps 604(623.98) | Grad Norm 2.4087(6.3442) | Total Time 0.00(0.00)\n",
      "Iter 2391 | Time 66.5101(66.3135) | Bit/dim 3.6450(3.6424) | Xent 0.7831(0.8079) | Loss 9.0856(9.4806) | Error 0.2814(0.2887) Steps 622(623.92) | Grad Norm 4.4622(6.2878) | Total Time 0.00(0.00)\n",
      "Iter 2392 | Time 70.4685(66.4382) | Bit/dim 3.6467(3.6425) | Xent 0.7978(0.8076) | Loss 8.9046(9.4633) | Error 0.2861(0.2887) Steps 616(623.69) | Grad Norm 4.3754(6.2304) | Total Time 0.00(0.00)\n",
      "Iter 2393 | Time 68.1345(66.4891) | Bit/dim 3.6348(3.6423) | Xent 0.7801(0.8068) | Loss 8.7958(9.4433) | Error 0.2798(0.2884) Steps 658(624.72) | Grad Norm 5.3634(6.2044) | Total Time 0.00(0.00)\n",
      "Iter 2394 | Time 67.4078(66.5166) | Bit/dim 3.6455(3.6424) | Xent 0.7925(0.8063) | Loss 8.9435(9.4283) | Error 0.2806(0.2882) Steps 646(625.35) | Grad Norm 2.9214(6.1059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 23.7608, Epoch Time 451.1218(405.9998), Bit/dim 3.6396(best: 3.6401), Xent 0.8258, Loss 4.0525, Error 0.2933(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2395 | Time 63.2602(66.4189) | Bit/dim 3.6409(3.6423) | Xent 0.7855(0.8057) | Loss 11.8050(9.4996) | Error 0.2788(0.2879) Steps 622(625.25) | Grad Norm 3.9267(6.0405) | Total Time 0.00(0.00)\n",
      "Iter 2396 | Time 66.3939(66.4182) | Bit/dim 3.6457(3.6424) | Xent 0.7847(0.8051) | Loss 8.7910(9.4783) | Error 0.2854(0.2878) Steps 640(625.70) | Grad Norm 7.1532(6.0739) | Total Time 0.00(0.00)\n",
      "Iter 2397 | Time 63.4710(66.3298) | Bit/dim 3.6280(3.6420) | Xent 0.8261(0.8057) | Loss 8.9665(9.4630) | Error 0.3019(0.2882) Steps 622(625.59) | Grad Norm 7.1496(6.1062) | Total Time 0.00(0.00)\n",
      "Iter 2398 | Time 68.4325(66.3928) | Bit/dim 3.6419(3.6420) | Xent 0.7983(0.8055) | Loss 8.8656(9.4451) | Error 0.2871(0.2882) Steps 640(626.02) | Grad Norm 6.2585(6.1107) | Total Time 0.00(0.00)\n",
      "Iter 2399 | Time 72.7585(66.5838) | Bit/dim 3.6354(3.6418) | Xent 0.7779(0.8047) | Loss 8.7958(9.4256) | Error 0.2815(0.2880) Steps 670(627.34) | Grad Norm 2.9159(6.0149) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 70.4305(66.6992) | Bit/dim 3.6579(3.6423) | Xent 0.8198(0.8051) | Loss 9.0283(9.4137) | Error 0.2941(0.2882) Steps 616(627.00) | Grad Norm 9.0944(6.1073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 22.9066, Epoch Time 443.4171(407.1223), Bit/dim 3.6422(best: 3.6396), Xent 0.8989, Loss 4.0917, Error 0.3186(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2401 | Time 67.1773(66.7136) | Bit/dim 3.6344(3.6420) | Xent 0.8439(0.8063) | Loss 12.1057(9.4944) | Error 0.3031(0.2886) Steps 634(627.21) | Grad Norm 10.3543(6.2347) | Total Time 0.00(0.00)\n",
      "Iter 2402 | Time 66.3937(66.7040) | Bit/dim 3.6508(3.6423) | Xent 0.8161(0.8066) | Loss 8.9613(9.4784) | Error 0.2896(0.2886) Steps 616(626.87) | Grad Norm 9.7054(6.3388) | Total Time 0.00(0.00)\n",
      "Iter 2403 | Time 75.0144(66.9533) | Bit/dim 3.6630(3.6429) | Xent 0.8174(0.8069) | Loss 9.1038(9.4672) | Error 0.2951(0.2888) Steps 634(627.08) | Grad Norm 10.9101(6.4760) | Total Time 0.00(0.00)\n",
      "Iter 2404 | Time 66.3381(66.9348) | Bit/dim 3.6433(3.6429) | Xent 0.8452(0.8080) | Loss 9.0361(9.4543) | Error 0.3087(0.2894) Steps 628(627.11) | Grad Norm 13.5953(6.6895) | Total Time 0.00(0.00)\n",
      "Iter 2405 | Time 69.5610(67.0136) | Bit/dim 3.6309(3.6426) | Xent 0.8814(0.8102) | Loss 9.0840(9.4432) | Error 0.3099(0.2901) Steps 616(626.78) | Grad Norm 15.0587(6.9406) | Total Time 0.00(0.00)\n",
      "Iter 2406 | Time 60.3563(66.8139) | Bit/dim 3.6332(3.6423) | Xent 0.7806(0.8094) | Loss 8.5677(9.4169) | Error 0.2796(0.2897) Steps 634(627.00) | Grad Norm 4.9117(6.8797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 23.4160, Epoch Time 444.3152(408.2381), Bit/dim 3.6457(best: 3.6396), Xent 0.8873, Loss 4.0893, Error 0.3121(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 70.0092(66.9097) | Bit/dim 3.6497(3.6425) | Xent 0.8426(0.8104) | Loss 12.0608(9.4962) | Error 0.3017(0.2901) Steps 622(626.85) | Grad Norm 10.8592(6.9991) | Total Time 0.00(0.00)\n",
      "Iter 2408 | Time 67.8039(66.9366) | Bit/dim 3.6436(3.6425) | Xent 0.8066(0.8102) | Loss 8.9258(9.4791) | Error 0.2923(0.2902) Steps 616(626.52) | Grad Norm 5.8955(6.9660) | Total Time 0.00(0.00)\n",
      "Iter 2409 | Time 66.0679(66.9105) | Bit/dim 3.6375(3.6424) | Xent 0.8228(0.8106) | Loss 9.0143(9.4652) | Error 0.2971(0.2904) Steps 634(626.74) | Grad Norm 7.1363(6.9711) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 63.9718(66.8223) | Bit/dim 3.6358(3.6422) | Xent 0.8032(0.8104) | Loss 8.8565(9.4469) | Error 0.2896(0.2904) Steps 622(626.60) | Grad Norm 5.9291(6.9399) | Total Time 0.00(0.00)\n",
      "Iter 2411 | Time 65.8619(66.7935) | Bit/dim 3.6493(3.6424) | Xent 0.8241(0.8108) | Loss 9.0366(9.4346) | Error 0.2951(0.2905) Steps 640(627.00) | Grad Norm 9.9296(7.0296) | Total Time 0.00(0.00)\n",
      "Iter 2412 | Time 62.4495(66.6632) | Bit/dim 3.6444(3.6425) | Xent 0.8165(0.8110) | Loss 9.0546(9.4232) | Error 0.2890(0.2904) Steps 634(627.21) | Grad Norm 8.0747(7.0609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 23.1225, Epoch Time 435.2277(409.0478), Bit/dim 3.6400(best: 3.6396), Xent 0.8419, Loss 4.0609, Error 0.2939(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 65.6515(66.6329) | Bit/dim 3.6386(3.6424) | Xent 0.8025(0.8107) | Loss 12.0593(9.5023) | Error 0.2877(0.2904) Steps 598(626.34) | Grad Norm 6.6209(7.0477) | Total Time 0.00(0.00)\n",
      "Iter 2414 | Time 66.8426(66.6392) | Bit/dim 3.6413(3.6423) | Xent 0.8172(0.8109) | Loss 9.0356(9.4883) | Error 0.2941(0.2905) Steps 634(626.57) | Grad Norm 12.4096(7.2086) | Total Time 0.00(0.00)\n",
      "Iter 2415 | Time 64.3876(66.5716) | Bit/dim 3.6486(3.6425) | Xent 0.8164(0.8111) | Loss 8.9980(9.4736) | Error 0.2943(0.2906) Steps 628(626.61) | Grad Norm 11.2043(7.3284) | Total Time 0.00(0.00)\n",
      "Iter 2416 | Time 63.9429(66.4927) | Bit/dim 3.6412(3.6425) | Xent 0.8204(0.8114) | Loss 9.1295(9.4632) | Error 0.2955(0.2907) Steps 610(626.11) | Grad Norm 7.6084(7.3368) | Total Time 0.00(0.00)\n",
      "Iter 2417 | Time 67.4294(66.5208) | Bit/dim 3.6379(3.6423) | Xent 0.8243(0.8117) | Loss 8.9644(9.4483) | Error 0.2955(0.2909) Steps 580(624.73) | Grad Norm 9.0021(7.3868) | Total Time 0.00(0.00)\n",
      "Iter 2418 | Time 67.5693(66.5523) | Bit/dim 3.6486(3.6425) | Xent 0.7887(0.8111) | Loss 9.1128(9.4382) | Error 0.2853(0.2907) Steps 616(624.47) | Grad Norm 6.6492(7.3647) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 23.0450, Epoch Time 434.9063(409.8235), Bit/dim 3.6451(best: 3.6396), Xent 0.8560, Loss 4.0731, Error 0.3042(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 63.5818(66.4632) | Bit/dim 3.6282(3.6421) | Xent 0.8185(0.8113) | Loss 12.2196(9.5217) | Error 0.2921(0.2908) Steps 622(624.39) | Grad Norm 6.3749(7.3350) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 70.6667(66.5893) | Bit/dim 3.6445(3.6422) | Xent 0.8151(0.8114) | Loss 8.8325(9.5010) | Error 0.2921(0.2908) Steps 652(625.22) | Grad Norm 6.6158(7.3134) | Total Time 0.00(0.00)\n",
      "Iter 2421 | Time 64.9798(66.5410) | Bit/dim 3.6540(3.6425) | Xent 0.8189(0.8116) | Loss 9.0050(9.4861) | Error 0.2957(0.2909) Steps 604(624.58) | Grad Norm 7.0459(7.3054) | Total Time 0.00(0.00)\n",
      "Iter 2422 | Time 69.4939(66.6296) | Bit/dim 3.6416(3.6425) | Xent 0.8310(0.8122) | Loss 8.9951(9.4714) | Error 0.2969(0.2911) Steps 604(623.97) | Grad Norm 7.4500(7.3097) | Total Time 0.00(0.00)\n",
      "Iter 2423 | Time 67.9052(66.6679) | Bit/dim 3.6468(3.6426) | Xent 0.7962(0.8117) | Loss 8.9225(9.4549) | Error 0.2874(0.2910) Steps 640(624.45) | Grad Norm 7.7782(7.3238) | Total Time 0.00(0.00)\n",
      "Iter 2424 | Time 61.8590(66.5236) | Bit/dim 3.6489(3.6428) | Xent 0.7728(0.8106) | Loss 8.7397(9.4334) | Error 0.2796(0.2907) Steps 622(624.37) | Grad Norm 6.8109(7.3084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 23.1483, Epoch Time 437.4312(410.6518), Bit/dim 3.6451(best: 3.6396), Xent 0.8330, Loss 4.0616, Error 0.2984(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 65.3120(66.4872) | Bit/dim 3.6326(3.6425) | Xent 0.8002(0.8102) | Loss 12.1936(9.5163) | Error 0.2887(0.2906) Steps 634(624.66) | Grad Norm 6.2103(7.2754) | Total Time 0.00(0.00)\n",
      "Iter 2426 | Time 73.5455(66.6990) | Bit/dim 3.6467(3.6426) | Xent 0.7936(0.8097) | Loss 8.9297(9.4987) | Error 0.2805(0.2903) Steps 646(625.30) | Grad Norm 8.4233(7.3099) | Total Time 0.00(0.00)\n",
      "Iter 2427 | Time 68.2872(66.7466) | Bit/dim 3.6352(3.6424) | Xent 0.7976(0.8094) | Loss 8.6662(9.4737) | Error 0.2856(0.2902) Steps 640(625.74) | Grad Norm 3.6062(7.1988) | Total Time 0.00(0.00)\n",
      "Iter 2428 | Time 63.6513(66.6538) | Bit/dim 3.6519(3.6427) | Xent 0.7863(0.8087) | Loss 8.9017(9.4565) | Error 0.2827(0.2899) Steps 628(625.81) | Grad Norm 7.5835(7.2103) | Total Time 0.00(0.00)\n",
      "Iter 2429 | Time 63.5073(66.5594) | Bit/dim 3.6376(3.6425) | Xent 0.8080(0.8087) | Loss 8.8858(9.4394) | Error 0.2917(0.2900) Steps 640(626.24) | Grad Norm 4.7188(7.1356) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 64.6813(66.5030) | Bit/dim 3.6395(3.6424) | Xent 0.7877(0.8080) | Loss 8.9685(9.4253) | Error 0.2817(0.2898) Steps 598(625.39) | Grad Norm 5.1159(7.0750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 23.1625, Epoch Time 438.0352(411.4733), Bit/dim 3.6483(best: 3.6396), Xent 0.8285, Loss 4.0625, Error 0.2945(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 67.6903(66.5387) | Bit/dim 3.6470(3.6426) | Xent 0.7852(0.8074) | Loss 12.0121(9.5029) | Error 0.2805(0.2895) Steps 604(624.75) | Grad Norm 4.1222(6.9864) | Total Time 0.00(0.00)\n",
      "Iter 2432 | Time 62.6979(66.4234) | Bit/dim 3.6428(3.6426) | Xent 0.8004(0.8071) | Loss 8.8227(9.4825) | Error 0.2830(0.2893) Steps 610(624.31) | Grad Norm 3.8441(6.8921) | Total Time 0.00(0.00)\n",
      "Iter 2433 | Time 67.8918(66.4675) | Bit/dim 3.6345(3.6423) | Xent 0.7871(0.8065) | Loss 8.7859(9.4616) | Error 0.2911(0.2893) Steps 658(625.32) | Grad Norm 6.0148(6.8658) | Total Time 0.00(0.00)\n",
      "Iter 2434 | Time 69.3201(66.5531) | Bit/dim 3.6445(3.6424) | Xent 0.7784(0.8057) | Loss 8.8165(9.4422) | Error 0.2788(0.2890) Steps 622(625.22) | Grad Norm 5.3121(6.8192) | Total Time 0.00(0.00)\n",
      "Iter 2435 | Time 65.6081(66.5247) | Bit/dim 3.6379(3.6423) | Xent 0.7700(0.8046) | Loss 8.9506(9.4275) | Error 0.2771(0.2887) Steps 628(625.30) | Grad Norm 3.4078(6.7169) | Total Time 0.00(0.00)\n",
      "Iter 2436 | Time 67.0385(66.5401) | Bit/dim 3.6329(3.6420) | Xent 0.7754(0.8038) | Loss 9.0320(9.4156) | Error 0.2824(0.2885) Steps 604(624.66) | Grad Norm 4.6927(6.6561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 23.3977, Epoch Time 439.5983(412.3170), Bit/dim 3.6430(best: 3.6396), Xent 0.8334, Loss 4.0597, Error 0.2912(best: 0.2925)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 71.0064(66.6741) | Bit/dim 3.6284(3.6416) | Xent 0.7870(0.8032) | Loss 12.0071(9.4934) | Error 0.2846(0.2884) Steps 634(624.94) | Grad Norm 6.7226(6.6581) | Total Time 0.00(0.00)\n",
      "Iter 2438 | Time 73.4938(66.8787) | Bit/dim 3.6327(3.6413) | Xent 0.7703(0.8023) | Loss 8.8926(9.4753) | Error 0.2769(0.2880) Steps 652(625.75) | Grad Norm 5.3089(6.6176) | Total Time 0.00(0.00)\n",
      "Iter 2439 | Time 68.8091(66.9366) | Bit/dim 3.6457(3.6415) | Xent 0.7859(0.8018) | Loss 9.0120(9.4614) | Error 0.2854(0.2879) Steps 610(625.28) | Grad Norm 3.2821(6.5176) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 65.8788(66.9049) | Bit/dim 3.6438(3.6415) | Xent 0.7965(0.8016) | Loss 8.9327(9.4456) | Error 0.2843(0.2878) Steps 610(624.82) | Grad Norm 6.4309(6.5150) | Total Time 0.00(0.00)\n",
      "Iter 2441 | Time 64.6520(66.8373) | Bit/dim 3.6533(3.6419) | Xent 0.7809(0.8010) | Loss 8.9249(9.4299) | Error 0.2805(0.2876) Steps 640(625.28) | Grad Norm 6.8828(6.5260) | Total Time 0.00(0.00)\n",
      "Iter 2442 | Time 62.6947(66.7130) | Bit/dim 3.6348(3.6417) | Xent 0.7980(0.8009) | Loss 8.8070(9.4113) | Error 0.2837(0.2875) Steps 634(625.54) | Grad Norm 8.6326(6.5892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 23.2508, Epoch Time 445.7783(413.3209), Bit/dim 3.6374(best: 3.6396), Xent 0.8400, Loss 4.0574, Error 0.3022(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 71.4381(66.8548) | Bit/dim 3.6336(3.6414) | Xent 0.8164(0.8014) | Loss 11.9815(9.4884) | Error 0.2921(0.2876) Steps 634(625.79) | Grad Norm 6.4729(6.5857) | Total Time 0.00(0.00)\n",
      "Iter 2444 | Time 64.5551(66.7858) | Bit/dim 3.6352(3.6412) | Xent 0.7688(0.8004) | Loss 9.0391(9.4749) | Error 0.2791(0.2874) Steps 634(626.04) | Grad Norm 4.5277(6.5240) | Total Time 0.00(0.00)\n",
      "Iter 2445 | Time 67.3232(66.8019) | Bit/dim 3.6472(3.6414) | Xent 0.7835(0.7999) | Loss 8.8146(9.4551) | Error 0.2861(0.2873) Steps 622(625.92) | Grad Norm 3.4210(6.4309) | Total Time 0.00(0.00)\n",
      "Iter 2446 | Time 64.1260(66.7216) | Bit/dim 3.6421(3.6414) | Xent 0.7983(0.7998) | Loss 8.6054(9.4296) | Error 0.2890(0.2874) Steps 622(625.80) | Grad Norm 5.0287(6.3888) | Total Time 0.00(0.00)\n",
      "Iter 2447 | Time 67.2372(66.7371) | Bit/dim 3.6426(3.6415) | Xent 0.7938(0.7997) | Loss 8.9927(9.4165) | Error 0.2865(0.2874) Steps 610(625.33) | Grad Norm 5.8672(6.3732) | Total Time 0.00(0.00)\n",
      "Iter 2448 | Time 66.2673(66.7230) | Bit/dim 3.6330(3.6412) | Xent 0.7831(0.7992) | Loss 8.9763(9.4033) | Error 0.2812(0.2872) Steps 622(625.23) | Grad Norm 7.8642(6.4179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 23.0584, Epoch Time 439.9845(414.1208), Bit/dim 3.6372(best: 3.6374), Xent 0.8414, Loss 4.0579, Error 0.2987(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 65.8107(66.6956) | Bit/dim 3.6458(3.6414) | Xent 0.7646(0.7981) | Loss 12.0055(9.4813) | Error 0.2744(0.2868) Steps 646(625.85) | Grad Norm 7.7355(6.4574) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 63.2325(66.5917) | Bit/dim 3.6436(3.6414) | Xent 0.7866(0.7978) | Loss 8.9839(9.4664) | Error 0.2846(0.2867) Steps 628(625.91) | Grad Norm 2.9449(6.3521) | Total Time 0.00(0.00)\n",
      "Iter 2451 | Time 74.8279(66.8388) | Bit/dim 3.6409(3.6414) | Xent 0.7956(0.7977) | Loss 9.0011(9.4525) | Error 0.2831(0.2866) Steps 610(625.44) | Grad Norm 4.8854(6.3081) | Total Time 0.00(0.00)\n",
      "Iter 2452 | Time 68.1837(66.8792) | Bit/dim 3.6374(3.6413) | Xent 0.7981(0.7977) | Loss 8.8153(9.4333) | Error 0.2855(0.2866) Steps 658(626.41) | Grad Norm 6.1879(6.3045) | Total Time 0.00(0.00)\n",
      "Iter 2453 | Time 65.8460(66.8482) | Bit/dim 3.6390(3.6412) | Xent 0.7889(0.7975) | Loss 8.7844(9.4139) | Error 0.2785(0.2863) Steps 628(626.46) | Grad Norm 4.7209(6.2569) | Total Time 0.00(0.00)\n",
      "Iter 2454 | Time 65.6575(66.8125) | Bit/dim 3.6257(3.6408) | Xent 0.7928(0.7973) | Loss 8.9666(9.4005) | Error 0.2897(0.2864) Steps 604(625.79) | Grad Norm 4.6508(6.2088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 22.4894, Epoch Time 442.5828(414.9746), Bit/dim 3.6398(best: 3.6372), Xent 0.8418, Loss 4.0607, Error 0.2972(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 67.6923(66.8389) | Bit/dim 3.6390(3.6407) | Xent 0.7986(0.7974) | Loss 12.0384(9.4796) | Error 0.2893(0.2865) Steps 658(626.75) | Grad Norm 9.0699(6.2946) | Total Time 0.00(0.00)\n",
      "Iter 2456 | Time 62.8857(66.7203) | Bit/dim 3.6392(3.6407) | Xent 0.7847(0.7970) | Loss 9.0034(9.4653) | Error 0.2801(0.2863) Steps 568(624.99) | Grad Norm 11.2540(6.4434) | Total Time 0.00(0.00)\n",
      "Iter 2457 | Time 64.6321(66.6576) | Bit/dim 3.6442(3.6408) | Xent 0.8178(0.7976) | Loss 9.0223(9.4520) | Error 0.2867(0.2863) Steps 616(624.72) | Grad Norm 10.2208(6.5567) | Total Time 0.00(0.00)\n",
      "Iter 2458 | Time 65.6689(66.6279) | Bit/dim 3.6387(3.6407) | Xent 0.7884(0.7973) | Loss 8.8726(9.4346) | Error 0.2829(0.2862) Steps 586(623.56) | Grad Norm 6.9338(6.5680) | Total Time 0.00(0.00)\n",
      "Iter 2459 | Time 64.3722(66.5603) | Bit/dim 3.6343(3.6405) | Xent 0.7973(0.7973) | Loss 8.7760(9.4149) | Error 0.2877(0.2863) Steps 640(624.05) | Grad Norm 9.1873(6.6466) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 68.2453(66.6108) | Bit/dim 3.6429(3.6406) | Xent 0.7945(0.7972) | Loss 8.7862(9.3960) | Error 0.2804(0.2861) Steps 646(624.71) | Grad Norm 7.2709(6.6653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 23.3711, Epoch Time 432.6915(415.5061), Bit/dim 3.6410(best: 3.6372), Xent 0.8594, Loss 4.0707, Error 0.3057(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 62.8330(66.4975) | Bit/dim 3.6284(3.6402) | Xent 0.8106(0.7976) | Loss 12.0746(9.4764) | Error 0.2926(0.2863) Steps 604(624.09) | Grad Norm 9.5488(6.7518) | Total Time 0.00(0.00)\n",
      "Iter 2462 | Time 62.4624(66.3764) | Bit/dim 3.6351(3.6401) | Xent 0.8071(0.7979) | Loss 8.7940(9.4559) | Error 0.2857(0.2863) Steps 628(624.21) | Grad Norm 9.6681(6.8393) | Total Time 0.00(0.00)\n",
      "Iter 2463 | Time 70.4144(66.4976) | Bit/dim 3.6369(3.6400) | Xent 0.8046(0.7981) | Loss 9.1137(9.4456) | Error 0.2870(0.2863) Steps 640(624.68) | Grad Norm 4.9639(6.7831) | Total Time 0.00(0.00)\n",
      "Iter 2464 | Time 70.6930(66.6234) | Bit/dim 3.6454(3.6401) | Xent 0.7995(0.7982) | Loss 8.9941(9.4321) | Error 0.2863(0.2863) Steps 598(623.88) | Grad Norm 8.8516(6.8451) | Total Time 0.00(0.00)\n",
      "Iter 2465 | Time 66.4061(66.6169) | Bit/dim 3.6494(3.6404) | Xent 0.7904(0.7979) | Loss 9.0081(9.4194) | Error 0.2841(0.2862) Steps 622(623.82) | Grad Norm 6.8806(6.8462) | Total Time 0.00(0.00)\n",
      "Iter 2466 | Time 65.8756(66.5947) | Bit/dim 3.6449(3.6405) | Xent 0.7900(0.7977) | Loss 8.8905(9.4035) | Error 0.2841(0.2862) Steps 640(624.31) | Grad Norm 3.9352(6.7588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 23.5008, Epoch Time 438.1211(416.1846), Bit/dim 3.6398(best: 3.6372), Xent 0.8543, Loss 4.0669, Error 0.3012(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 66.9404(66.6051) | Bit/dim 3.6234(3.6400) | Xent 0.7999(0.7978) | Loss 12.0310(9.4823) | Error 0.2869(0.2862) Steps 652(625.14) | Grad Norm 7.8576(6.7918) | Total Time 0.00(0.00)\n",
      "Iter 2468 | Time 62.8186(66.4915) | Bit/dim 3.6406(3.6400) | Xent 0.8032(0.7979) | Loss 9.0565(9.4696) | Error 0.2884(0.2863) Steps 616(624.87) | Grad Norm 7.7732(6.8212) | Total Time 0.00(0.00)\n",
      "Iter 2469 | Time 67.2806(66.5151) | Bit/dim 3.6433(3.6401) | Xent 0.8060(0.7982) | Loss 9.0155(9.4559) | Error 0.2963(0.2866) Steps 616(624.60) | Grad Norm 6.4168(6.8091) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 71.1716(66.6548) | Bit/dim 3.6408(3.6402) | Xent 0.7802(0.7976) | Loss 8.8818(9.4387) | Error 0.2829(0.2865) Steps 640(625.06) | Grad Norm 6.9707(6.8140) | Total Time 0.00(0.00)\n",
      "Iter 2471 | Time 68.1225(66.6989) | Bit/dim 3.6358(3.6400) | Xent 0.7942(0.7975) | Loss 9.0379(9.4267) | Error 0.2814(0.2863) Steps 640(625.51) | Grad Norm 5.9946(6.7894) | Total Time 0.00(0.00)\n",
      "Iter 2472 | Time 60.7158(66.5194) | Bit/dim 3.6503(3.6403) | Xent 0.7986(0.7976) | Loss 8.8439(9.4092) | Error 0.2829(0.2862) Steps 628(625.59) | Grad Norm 7.8703(6.8218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 23.1705, Epoch Time 436.3128(416.7884), Bit/dim 3.6446(best: 3.6372), Xent 0.8471, Loss 4.0682, Error 0.3003(best: 0.2912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 64.6442(66.4631) | Bit/dim 3.6317(3.6401) | Xent 0.8092(0.7979) | Loss 11.8849(9.4835) | Error 0.2899(0.2863) Steps 634(625.84) | Grad Norm 7.5981(6.8451) | Total Time 0.00(0.00)\n",
      "Iter 2474 | Time 61.4063(66.3114) | Bit/dim 3.6371(3.6400) | Xent 0.7751(0.7972) | Loss 8.8558(9.4646) | Error 0.2778(0.2861) Steps 634(626.08) | Grad Norm 3.6827(6.7502) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run3/epoch_400_checkpt.pth --seed 3 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
