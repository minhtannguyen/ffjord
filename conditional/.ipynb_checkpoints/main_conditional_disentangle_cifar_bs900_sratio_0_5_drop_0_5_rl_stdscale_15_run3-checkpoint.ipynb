{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3/epoch_156_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 8590 | Time 19.5357(20.9857) | Bit/dim 3.6210(3.6117) | Xent 0.3616(0.3862) | Loss 8.6622(9.4604) | Error 0.1278(0.1370) Steps 790(808.19) | Grad Norm 7.6065(8.1178) | Total Time 0.00(0.00)\n",
      "Iter 8600 | Time 20.9636(20.8794) | Bit/dim 3.6331(3.6105) | Xent 0.4150(0.3836) | Loss 8.8664(9.2612) | Error 0.1389(0.1361) Steps 832(811.87) | Grad Norm 8.4586(7.9900) | Total Time 0.00(0.00)\n",
      "Iter 8610 | Time 19.6005(20.7784) | Bit/dim 3.6007(3.6085) | Xent 0.4296(0.3853) | Loss 8.7181(9.1145) | Error 0.1478(0.1359) Steps 784(810.99) | Grad Norm 10.0450(7.9648) | Total Time 0.00(0.00)\n",
      "Iter 8620 | Time 19.9488(20.7316) | Bit/dim 3.6037(3.6070) | Xent 0.3456(0.3932) | Loss 8.7663(9.0259) | Error 0.1300(0.1384) Steps 784(810.86) | Grad Norm 7.6509(8.2769) | Total Time 0.00(0.00)\n",
      "Iter 8630 | Time 20.7651(20.6742) | Bit/dim 3.6013(3.6089) | Xent 0.3488(0.3995) | Loss 8.7716(8.9672) | Error 0.1211(0.1406) Steps 826(812.30) | Grad Norm 6.6687(8.9466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 101.7785, Epoch Time 1273.4317(1153.9201), Bit/dim 3.6101(best: inf), Xent 0.6726, Loss 3.9464, Error 0.2137(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 21.3549(20.6130) | Bit/dim 3.5939(3.6081) | Xent 0.3579(0.3917) | Loss 8.7527(9.5746) | Error 0.1156(0.1375) Steps 802(811.14) | Grad Norm 5.1193(8.4315) | Total Time 0.00(0.00)\n",
      "Iter 8650 | Time 19.6303(20.5546) | Bit/dim 3.5880(3.6075) | Xent 0.3427(0.3866) | Loss 8.6092(9.3611) | Error 0.1289(0.1365) Steps 814(812.05) | Grad Norm 5.9522(8.0475) | Total Time 0.00(0.00)\n",
      "Iter 8660 | Time 21.1713(20.5423) | Bit/dim 3.6257(3.6069) | Xent 0.3839(0.3795) | Loss 8.9212(9.2044) | Error 0.1333(0.1340) Steps 868(815.28) | Grad Norm 8.1740(7.7824) | Total Time 0.00(0.00)\n",
      "Iter 8670 | Time 20.4985(20.4973) | Bit/dim 3.5836(3.6035) | Xent 0.3463(0.3716) | Loss 8.6481(9.0751) | Error 0.1122(0.1309) Steps 790(813.72) | Grad Norm 5.6687(7.1935) | Total Time 0.00(0.00)\n",
      "Iter 8680 | Time 20.3114(20.4313) | Bit/dim 3.5872(3.6042) | Xent 0.3504(0.3754) | Loss 8.7455(8.9932) | Error 0.1289(0.1321) Steps 814(815.67) | Grad Norm 8.9727(8.0489) | Total Time 0.00(0.00)\n",
      "Iter 8690 | Time 20.8514(20.4387) | Bit/dim 3.6297(3.6076) | Xent 0.3395(0.3852) | Loss 8.7215(8.9405) | Error 0.1144(0.1359) Steps 790(812.36) | Grad Norm 5.6646(8.2811) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 97.7840, Epoch Time 1238.9691(1156.4715), Bit/dim 3.6179(best: 3.6101), Xent 0.6694, Loss 3.9526, Error 0.2121(best: 0.2137)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 21.0329(20.5667) | Bit/dim 3.6053(3.6088) | Xent 0.4156(0.3838) | Loss 8.6984(9.4840) | Error 0.1444(0.1351) Steps 808(814.16) | Grad Norm 11.2733(8.3829) | Total Time 0.00(0.00)\n",
      "Iter 8710 | Time 20.9723(20.5532) | Bit/dim 3.6211(3.6087) | Xent 0.3749(0.3791) | Loss 8.6963(9.2843) | Error 0.1344(0.1330) Steps 856(814.68) | Grad Norm 6.6007(7.9293) | Total Time 0.00(0.00)\n",
      "Iter 8720 | Time 20.2626(20.5724) | Bit/dim 3.5978(3.6074) | Xent 0.3764(0.3794) | Loss 8.7605(9.1485) | Error 0.1322(0.1333) Steps 820(814.28) | Grad Norm 8.3757(7.6622) | Total Time 0.00(0.00)\n",
      "Iter 8730 | Time 20.0313(20.3829) | Bit/dim 3.6170(3.6064) | Xent 0.3653(0.3754) | Loss 8.6560(9.0311) | Error 0.1278(0.1321) Steps 832(812.55) | Grad Norm 5.6367(7.5997) | Total Time 0.00(0.00)\n",
      "Iter 8740 | Time 19.3585(20.3668) | Bit/dim 3.5792(3.6053) | Xent 0.4104(0.3775) | Loss 8.7363(8.9625) | Error 0.1467(0.1330) Steps 796(812.74) | Grad Norm 5.9813(7.3435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 94.3348, Epoch Time 1232.8846(1158.7639), Bit/dim 3.6096(best: 3.6101), Xent 0.6686, Loss 3.9439, Error 0.2088(best: 0.2121)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 19.2245(20.2128) | Bit/dim 3.6178(3.6055) | Xent 0.3406(0.3688) | Loss 8.6302(9.5780) | Error 0.1178(0.1293) Steps 796(810.50) | Grad Norm 7.4521(6.9931) | Total Time 0.00(0.00)\n",
      "Iter 8760 | Time 19.9785(20.1585) | Bit/dim 3.5636(3.6003) | Xent 0.4042(0.3660) | Loss 8.7439(9.3456) | Error 0.1411(0.1287) Steps 820(810.55) | Grad Norm 9.7701(7.1086) | Total Time 0.00(0.00)\n",
      "Iter 8770 | Time 19.5188(20.0678) | Bit/dim 3.5963(3.6026) | Xent 0.3184(0.3601) | Loss 8.6026(9.1715) | Error 0.1133(0.1268) Steps 814(812.64) | Grad Norm 4.3894(7.0561) | Total Time 0.00(0.00)\n",
      "Iter 8780 | Time 20.6201(20.1507) | Bit/dim 3.6271(3.6062) | Xent 0.4032(0.3606) | Loss 8.9088(9.0727) | Error 0.1378(0.1265) Steps 832(819.35) | Grad Norm 9.1587(7.0060) | Total Time 0.00(0.00)\n",
      "Iter 8790 | Time 19.5021(20.0883) | Bit/dim 3.5729(3.6060) | Xent 0.3637(0.3647) | Loss 8.8308(8.9943) | Error 0.1344(0.1284) Steps 850(821.25) | Grad Norm 5.7014(7.3402) | Total Time 0.00(0.00)\n",
      "Iter 8800 | Time 20.0512(20.2342) | Bit/dim 3.6394(3.6048) | Xent 0.3304(0.3682) | Loss 8.7831(8.9392) | Error 0.1256(0.1298) Steps 862(823.53) | Grad Norm 4.1514(7.3797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 95.1690, Epoch Time 1217.8012(1160.5350), Bit/dim 3.6059(best: 3.6096), Xent 0.7356, Loss 3.9737, Error 0.2257(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 20.4114(20.1256) | Bit/dim 3.6208(3.6030) | Xent 0.3582(0.3660) | Loss 8.8065(9.4703) | Error 0.1178(0.1288) Steps 844(824.88) | Grad Norm 11.0625(7.7855) | Total Time 0.00(0.00)\n",
      "Iter 8820 | Time 20.3659(20.1195) | Bit/dim 3.6241(3.6040) | Xent 0.3799(0.3725) | Loss 8.7640(9.2866) | Error 0.1311(0.1310) Steps 820(821.59) | Grad Norm 8.5927(8.5221) | Total Time 0.00(0.00)\n",
      "Iter 8830 | Time 19.4728(20.1231) | Bit/dim 3.6069(3.6064) | Xent 0.5787(0.3883) | Loss 8.8723(9.1562) | Error 0.1922(0.1363) Steps 820(821.91) | Grad Norm 11.5689(8.8518) | Total Time 0.00(0.00)\n",
      "Iter 8840 | Time 20.0148(20.0877) | Bit/dim 3.6121(3.6072) | Xent 0.4673(0.3965) | Loss 8.8375(9.0606) | Error 0.1556(0.1396) Steps 820(820.10) | Grad Norm 13.7334(9.2206) | Total Time 0.00(0.00)\n",
      "Iter 8850 | Time 20.0416(20.1087) | Bit/dim 3.6050(3.6070) | Xent 0.3595(0.3924) | Loss 8.6601(8.9776) | Error 0.1367(0.1392) Steps 802(823.74) | Grad Norm 6.0149(9.0555) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 94.2046, Epoch Time 1212.3765(1162.0903), Bit/dim 3.6134(best: 3.6059), Xent 0.7069, Loss 3.9668, Error 0.2192(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 20.0760(20.0828) | Bit/dim 3.6379(3.6081) | Xent 0.3991(0.3893) | Loss 8.9276(9.6179) | Error 0.1378(0.1380) Steps 832(823.07) | Grad Norm 6.7356(8.9076) | Total Time 0.00(0.00)\n",
      "Iter 8870 | Time 19.8166(20.2230) | Bit/dim 3.5893(3.6103) | Xent 0.3605(0.3854) | Loss 8.7404(9.4051) | Error 0.1222(0.1364) Steps 856(829.90) | Grad Norm 5.4548(8.3725) | Total Time 0.00(0.00)\n",
      "Iter 8880 | Time 20.0380(20.1909) | Bit/dim 3.6200(3.6092) | Xent 0.4597(0.3860) | Loss 8.7995(9.2387) | Error 0.1600(0.1362) Steps 838(829.82) | Grad Norm 19.3034(8.7583) | Total Time 0.00(0.00)\n",
      "Iter 8890 | Time 20.1691(20.2424) | Bit/dim 3.6272(3.6122) | Xent 0.4457(0.3941) | Loss 8.9661(9.1363) | Error 0.1656(0.1387) Steps 826(828.46) | Grad Norm 10.3014(9.4791) | Total Time 0.00(0.00)\n",
      "Iter 8900 | Time 20.6802(20.1949) | Bit/dim 3.6077(3.6122) | Xent 0.3474(0.3832) | Loss 8.7182(9.0283) | Error 0.1289(0.1350) Steps 802(826.00) | Grad Norm 5.2493(8.8472) | Total Time 0.00(0.00)\n",
      "Iter 8910 | Time 20.0047(20.2870) | Bit/dim 3.6017(3.6110) | Xent 0.3923(0.3897) | Loss 8.8253(8.9706) | Error 0.1589(0.1376) Steps 832(830.70) | Grad Norm 8.6039(8.4700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 94.2087, Epoch Time 1227.4848(1164.0521), Bit/dim 3.6136(best: 3.6059), Xent 0.6785, Loss 3.9529, Error 0.2098(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 19.4566(20.1934) | Bit/dim 3.6123(3.6063) | Xent 0.3617(0.3828) | Loss 8.8245(9.5143) | Error 0.1289(0.1356) Steps 826(831.48) | Grad Norm 7.4924(8.1167) | Total Time 0.00(0.00)\n",
      "Iter 8930 | Time 18.9844(20.2030) | Bit/dim 3.5638(3.6065) | Xent 0.3128(0.3747) | Loss 8.4917(9.2969) | Error 0.1211(0.1324) Steps 808(830.97) | Grad Norm 6.7950(7.9763) | Total Time 0.00(0.00)\n",
      "Iter 8940 | Time 18.6721(20.3006) | Bit/dim 3.6528(3.6080) | Xent 0.4314(0.3755) | Loss 8.8183(9.1629) | Error 0.1489(0.1324) Steps 802(829.83) | Grad Norm 11.0519(7.9882) | Total Time 0.00(0.00)\n",
      "Iter 8950 | Time 21.2744(20.3399) | Bit/dim 3.6069(3.6081) | Xent 0.3869(0.3836) | Loss 8.8289(9.0677) | Error 0.1433(0.1361) Steps 820(828.77) | Grad Norm 10.4569(8.6201) | Total Time 0.00(0.00)\n",
      "Iter 8960 | Time 21.0987(20.3736) | Bit/dim 3.6039(3.6060) | Xent 0.3780(0.3813) | Loss 8.8458(8.9862) | Error 0.1278(0.1347) Steps 844(826.68) | Grad Norm 4.8217(8.5225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 94.1067, Epoch Time 1227.6479(1165.9600), Bit/dim 3.6118(best: 3.6059), Xent 0.7071, Loss 3.9653, Error 0.2265(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 19.7849(20.3243) | Bit/dim 3.6090(3.6066) | Xent 0.3596(0.3794) | Loss 8.8659(9.6340) | Error 0.1178(0.1343) Steps 850(825.31) | Grad Norm 6.1819(8.4317) | Total Time 0.00(0.00)\n",
      "Iter 8980 | Time 20.2112(20.3510) | Bit/dim 3.6037(3.6073) | Xent 0.3883(0.3778) | Loss 8.6611(9.4121) | Error 0.1367(0.1341) Steps 814(824.83) | Grad Norm 11.9012(8.2700) | Total Time 0.00(0.00)\n",
      "Iter 8990 | Time 21.6421(20.3198) | Bit/dim 3.5818(3.6063) | Xent 0.3084(0.3750) | Loss 8.7128(9.2314) | Error 0.1067(0.1309) Steps 862(823.77) | Grad Norm 6.1620(8.3435) | Total Time 0.00(0.00)\n",
      "Iter 9000 | Time 20.6982(20.2888) | Bit/dim 3.6088(3.6044) | Xent 0.3776(0.3692) | Loss 8.7835(9.0968) | Error 0.1311(0.1289) Steps 838(826.18) | Grad Norm 6.1206(7.9411) | Total Time 0.00(0.00)\n",
      "Iter 9010 | Time 19.4996(20.3233) | Bit/dim 3.5837(3.6030) | Xent 0.3217(0.3632) | Loss 8.6596(9.0042) | Error 0.1167(0.1262) Steps 838(826.81) | Grad Norm 4.4611(7.4229) | Total Time 0.00(0.00)\n",
      "Iter 9020 | Time 21.0110(20.3890) | Bit/dim 3.6225(3.6027) | Xent 0.3800(0.3646) | Loss 8.8512(8.9401) | Error 0.1333(0.1270) Steps 784(826.38) | Grad Norm 6.8078(7.4222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 92.9538, Epoch Time 1228.7443(1167.8435), Bit/dim 3.6041(best: 3.6059), Xent 0.6635, Loss 3.9358, Error 0.2159(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 20.0239(20.3744) | Bit/dim 3.5922(3.6036) | Xent 0.3214(0.3624) | Loss 8.7328(9.4940) | Error 0.1144(0.1263) Steps 796(825.15) | Grad Norm 6.1283(7.6896) | Total Time 0.00(0.00)\n",
      "Iter 9040 | Time 20.0616(20.3610) | Bit/dim 3.5977(3.6070) | Xent 0.2874(0.3605) | Loss 8.6715(9.3024) | Error 0.1022(0.1255) Steps 838(828.21) | Grad Norm 8.7096(7.8099) | Total Time 0.00(0.00)\n",
      "Iter 9050 | Time 20.4505(20.3911) | Bit/dim 3.6290(3.6053) | Xent 0.4177(0.3665) | Loss 8.8848(9.1604) | Error 0.1456(0.1285) Steps 820(826.35) | Grad Norm 10.0158(8.1161) | Total Time 0.00(0.00)\n",
      "Iter 9060 | Time 20.3804(20.3734) | Bit/dim 3.6161(3.6056) | Xent 0.3872(0.3722) | Loss 8.7475(9.0560) | Error 0.1378(0.1316) Steps 814(825.32) | Grad Norm 6.7207(8.4009) | Total Time 0.00(0.00)\n",
      "Iter 9070 | Time 21.0982(20.4617) | Bit/dim 3.6037(3.6070) | Xent 0.3477(0.3676) | Loss 8.6151(8.9750) | Error 0.1300(0.1309) Steps 838(826.48) | Grad Norm 4.9870(7.8399) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 92.9094, Epoch Time 1230.5794(1169.7256), Bit/dim 3.6034(best: 3.6041), Xent 0.6814, Loss 3.9441, Error 0.2157(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 20.3757(20.3552) | Bit/dim 3.6069(3.6069) | Xent 0.3067(0.3631) | Loss 8.7981(9.6135) | Error 0.1089(0.1283) Steps 838(824.82) | Grad Norm 5.2594(7.4835) | Total Time 0.00(0.00)\n",
      "Iter 9090 | Time 19.9591(20.3770) | Bit/dim 3.5915(3.6056) | Xent 0.3702(0.3635) | Loss 8.6763(9.3791) | Error 0.1289(0.1290) Steps 814(826.16) | Grad Norm 9.9937(7.9968) | Total Time 0.00(0.00)\n",
      "Iter 9100 | Time 20.3392(20.3536) | Bit/dim 3.5807(3.6059) | Xent 0.3199(0.3589) | Loss 8.6393(9.2038) | Error 0.1178(0.1257) Steps 784(827.72) | Grad Norm 7.2812(7.7989) | Total Time 0.00(0.00)\n",
      "Iter 9110 | Time 19.8775(20.3795) | Bit/dim 3.6205(3.6078) | Xent 0.3571(0.3635) | Loss 8.8048(9.0949) | Error 0.1267(0.1275) Steps 802(827.53) | Grad Norm 8.8956(8.2682) | Total Time 0.00(0.00)\n",
      "Iter 9120 | Time 20.6041(20.3779) | Bit/dim 3.6317(3.6077) | Xent 0.4652(0.3719) | Loss 8.8301(8.9922) | Error 0.1544(0.1300) Steps 868(827.53) | Grad Norm 10.1636(8.6871) | Total Time 0.00(0.00)\n",
      "Iter 9130 | Time 19.8708(20.3529) | Bit/dim 3.6288(3.6092) | Xent 0.3744(0.3704) | Loss 8.7979(8.9350) | Error 0.1289(0.1299) Steps 844(830.60) | Grad Norm 6.1266(8.3556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 95.0061, Epoch Time 1230.6816(1171.5543), Bit/dim 3.6103(best: 3.6034), Xent 0.7018, Loss 3.9612, Error 0.2209(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 21.1190(20.5008) | Bit/dim 3.6052(3.6090) | Xent 0.3466(0.3622) | Loss 8.7330(9.4856) | Error 0.1200(0.1267) Steps 802(832.58) | Grad Norm 6.0007(7.9884) | Total Time 0.00(0.00)\n",
      "Iter 9150 | Time 20.8657(20.5406) | Bit/dim 3.6149(3.6076) | Xent 0.3629(0.3653) | Loss 8.8340(9.2989) | Error 0.1300(0.1285) Steps 820(835.27) | Grad Norm 7.4341(8.1565) | Total Time 0.00(0.00)\n",
      "Iter 9160 | Time 19.7851(20.4591) | Bit/dim 3.6021(3.6061) | Xent 0.3360(0.3690) | Loss 8.7173(9.1552) | Error 0.1122(0.1291) Steps 838(836.12) | Grad Norm 6.9977(7.9362) | Total Time 0.00(0.00)\n",
      "Iter 9170 | Time 20.7172(20.4976) | Bit/dim 3.6478(3.6067) | Xent 0.3913(0.3726) | Loss 8.7689(9.0513) | Error 0.1433(0.1306) Steps 832(838.84) | Grad Norm 11.2751(8.2123) | Total Time 0.00(0.00)\n",
      "Iter 9180 | Time 20.9889(20.4771) | Bit/dim 3.5855(3.6117) | Xent 0.3590(0.3780) | Loss 8.7296(9.0003) | Error 0.1189(0.1329) Steps 832(839.45) | Grad Norm 6.7938(8.7085) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 94.5521, Epoch Time 1241.9672(1173.6667), Bit/dim 3.6179(best: 3.6034), Xent 0.6649, Loss 3.9503, Error 0.2114(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 20.3073(20.4490) | Bit/dim 3.5938(3.6085) | Xent 0.2953(0.3714) | Loss 8.6469(9.6363) | Error 0.0989(0.1312) Steps 832(840.06) | Grad Norm 5.5773(8.2619) | Total Time 0.00(0.00)\n",
      "Iter 9200 | Time 20.9421(20.5175) | Bit/dim 3.6239(3.6090) | Xent 0.3168(0.3680) | Loss 8.8677(9.4043) | Error 0.1089(0.1298) Steps 808(837.92) | Grad Norm 8.1215(8.3938) | Total Time 0.00(0.00)\n",
      "Iter 9210 | Time 20.5178(20.5291) | Bit/dim 3.6063(3.6094) | Xent 0.3325(0.3708) | Loss 8.7415(9.2495) | Error 0.1144(0.1304) Steps 874(840.30) | Grad Norm 10.3611(8.9607) | Total Time 0.00(0.00)\n",
      "Iter 9220 | Time 21.0796(20.5401) | Bit/dim 3.5896(3.6110) | Xent 0.3658(0.3713) | Loss 8.7904(9.1270) | Error 0.1356(0.1312) Steps 844(837.93) | Grad Norm 12.2440(9.3011) | Total Time 0.00(0.00)\n",
      "Iter 9230 | Time 20.4883(20.5075) | Bit/dim 3.5733(3.6080) | Xent 0.3381(0.3749) | Loss 8.6665(9.0361) | Error 0.1233(0.1330) Steps 838(838.09) | Grad Norm 10.0929(9.2065) | Total Time 0.00(0.00)\n",
      "Iter 9240 | Time 20.4382(20.5463) | Bit/dim 3.6761(3.6121) | Xent 0.3673(0.3690) | Loss 8.8979(8.9680) | Error 0.1300(0.1311) Steps 820(838.63) | Grad Norm 7.5125(8.8936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 94.5920, Epoch Time 1241.4765(1175.7010), Bit/dim 3.6148(best: 3.6034), Xent 0.6943, Loss 3.9619, Error 0.2164(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 20.2982(20.5366) | Bit/dim 3.5995(3.6105) | Xent 0.3418(0.3601) | Loss 8.6472(9.5144) | Error 0.1089(0.1278) Steps 838(839.65) | Grad Norm 9.3604(8.4671) | Total Time 0.00(0.00)\n",
      "Iter 9260 | Time 21.4888(20.6272) | Bit/dim 3.5842(3.6066) | Xent 0.3167(0.3567) | Loss 8.8350(9.3019) | Error 0.1122(0.1266) Steps 910(842.30) | Grad Norm 5.9120(8.4178) | Total Time 0.00(0.00)\n",
      "Iter 9270 | Time 21.3968(20.7033) | Bit/dim 3.6157(3.6049) | Xent 0.3263(0.3528) | Loss 8.6687(9.1465) | Error 0.1233(0.1251) Steps 838(846.72) | Grad Norm 6.1549(8.0770) | Total Time 0.00(0.00)\n",
      "Iter 9280 | Time 20.1145(20.6820) | Bit/dim 3.6001(3.6068) | Xent 0.4121(0.3610) | Loss 8.8139(9.0425) | Error 0.1500(0.1276) Steps 844(844.83) | Grad Norm 9.2258(8.3319) | Total Time 0.00(0.00)\n",
      "Iter 9290 | Time 20.4128(20.6818) | Bit/dim 3.6116(3.6076) | Xent 0.3594(0.3592) | Loss 8.7603(8.9596) | Error 0.1189(0.1263) Steps 820(844.17) | Grad Norm 6.8179(8.0005) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 94.8569, Epoch Time 1250.6150(1177.9484), Bit/dim 3.6160(best: 3.6034), Xent 0.6743, Loss 3.9532, Error 0.2091(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 21.7176(20.8041) | Bit/dim 3.6193(3.6043) | Xent 0.3177(0.3552) | Loss 8.7346(9.6082) | Error 0.1111(0.1247) Steps 850(843.15) | Grad Norm 10.2627(7.7574) | Total Time 0.00(0.00)\n",
      "Iter 9310 | Time 20.9520(20.8151) | Bit/dim 3.6403(3.6066) | Xent 0.3535(0.3596) | Loss 8.7172(9.3779) | Error 0.1344(0.1265) Steps 898(847.72) | Grad Norm 7.2226(7.8095) | Total Time 0.00(0.00)\n",
      "Iter 9320 | Time 19.8867(20.7935) | Bit/dim 3.6002(3.6090) | Xent 0.3231(0.3502) | Loss 8.6421(9.2078) | Error 0.1100(0.1233) Steps 832(847.52) | Grad Norm 6.3711(7.6486) | Total Time 0.00(0.00)\n",
      "Iter 9330 | Time 20.0189(20.7925) | Bit/dim 3.5753(3.6087) | Xent 0.3446(0.3478) | Loss 8.7138(9.0829) | Error 0.1200(0.1223) Steps 820(846.53) | Grad Norm 7.9317(7.3904) | Total Time 0.00(0.00)\n",
      "Iter 9340 | Time 20.2058(20.8375) | Bit/dim 3.6025(3.6075) | Xent 0.3601(0.3466) | Loss 8.7376(8.9951) | Error 0.1422(0.1222) Steps 856(848.23) | Grad Norm 11.8713(7.3544) | Total Time 0.00(0.00)\n",
      "Iter 9350 | Time 20.1167(20.7605) | Bit/dim 3.6327(3.6072) | Xent 0.3430(0.3475) | Loss 8.7005(8.9210) | Error 0.1178(0.1220) Steps 802(845.58) | Grad Norm 6.2349(7.4131) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 95.6014, Epoch Time 1258.1913(1180.3557), Bit/dim 3.6089(best: 3.6034), Xent 0.7006, Loss 3.9592, Error 0.2207(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 20.9012(20.8051) | Bit/dim 3.5754(3.6067) | Xent 0.3779(0.3439) | Loss 8.7396(9.4675) | Error 0.1389(0.1210) Steps 892(848.09) | Grad Norm 7.0990(7.6937) | Total Time 0.00(0.00)\n",
      "Iter 9370 | Time 21.0598(20.8471) | Bit/dim 3.6079(3.6102) | Xent 0.3398(0.3410) | Loss 8.7387(9.2840) | Error 0.1200(0.1207) Steps 844(849.58) | Grad Norm 4.7890(7.4835) | Total Time 0.00(0.00)\n",
      "Iter 9380 | Time 21.5170(20.8907) | Bit/dim 3.6023(3.6108) | Xent 0.3026(0.3393) | Loss 8.6926(9.1369) | Error 0.1167(0.1204) Steps 898(851.13) | Grad Norm 4.6724(7.5690) | Total Time 0.00(0.00)\n",
      "Iter 9390 | Time 21.9382(20.9815) | Bit/dim 3.6261(3.6143) | Xent 0.3213(0.3446) | Loss 8.8217(9.0462) | Error 0.1056(0.1214) Steps 868(848.81) | Grad Norm 6.8805(7.6373) | Total Time 0.00(0.00)\n",
      "Iter 9400 | Time 21.0589(20.9785) | Bit/dim 3.5863(3.6137) | Xent 0.3754(0.3488) | Loss 8.6488(8.9657) | Error 0.1433(0.1235) Steps 808(850.09) | Grad Norm 9.9196(7.6989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 94.4023, Epoch Time 1266.7870(1182.9486), Bit/dim 3.6407(best: 3.6034), Xent 0.6967, Loss 3.9891, Error 0.2167(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 20.9424(20.9738) | Bit/dim 3.6526(3.6170) | Xent 0.2944(0.3505) | Loss 8.8166(9.6182) | Error 0.1089(0.1244) Steps 838(848.02) | Grad Norm 8.0907(8.5222) | Total Time 0.00(0.00)\n",
      "Iter 9420 | Time 21.2334(21.0388) | Bit/dim 3.6414(3.6191) | Xent 0.3645(0.3525) | Loss 8.8644(9.4030) | Error 0.1233(0.1244) Steps 898(851.55) | Grad Norm 11.2002(8.9648) | Total Time 0.00(0.00)\n",
      "Iter 9430 | Time 20.6124(21.0968) | Bit/dim 3.6437(3.6275) | Xent 0.3567(0.3485) | Loss 8.8337(9.2526) | Error 0.1267(0.1230) Steps 844(855.09) | Grad Norm 6.1498(8.4542) | Total Time 0.00(0.00)\n",
      "Iter 9440 | Time 21.2201(21.1606) | Bit/dim 3.6324(3.6286) | Xent 0.3285(0.3518) | Loss 8.7377(9.1345) | Error 0.1156(0.1237) Steps 856(855.53) | Grad Norm 5.6564(8.2159) | Total Time 0.00(0.00)\n",
      "Iter 9450 | Time 20.9515(21.2140) | Bit/dim 3.6314(3.6259) | Xent 0.3323(0.3586) | Loss 8.5168(9.0330) | Error 0.1211(0.1267) Steps 808(854.41) | Grad Norm 4.1459(8.1689) | Total Time 0.00(0.00)\n",
      "Iter 9460 | Time 20.7427(21.2399) | Bit/dim 3.6158(3.6241) | Xent 0.3753(0.3576) | Loss 8.8918(8.9756) | Error 0.1289(0.1266) Steps 862(855.57) | Grad Norm 7.4107(8.0251) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 95.1212, Epoch Time 1280.5663(1185.8771), Bit/dim 3.6252(best: 3.6034), Xent 0.6911, Loss 3.9707, Error 0.2162(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 21.2126(21.2321) | Bit/dim 3.6044(3.6236) | Xent 0.3673(0.3542) | Loss 8.7773(9.5322) | Error 0.1267(0.1245) Steps 874(854.74) | Grad Norm 11.9222(8.0429) | Total Time 0.00(0.00)\n",
      "Iter 9480 | Time 21.1660(21.2779) | Bit/dim 3.5977(3.6238) | Xent 0.3011(0.3466) | Loss 8.7450(9.3377) | Error 0.1000(0.1216) Steps 844(857.50) | Grad Norm 5.3352(7.7096) | Total Time 0.00(0.00)\n",
      "Iter 9490 | Time 21.8987(21.2838) | Bit/dim 3.6382(3.6253) | Xent 0.3614(0.3465) | Loss 8.7473(9.1891) | Error 0.1289(0.1222) Steps 844(858.39) | Grad Norm 9.3318(7.8284) | Total Time 0.00(0.00)\n",
      "Iter 9500 | Time 21.1067(21.1485) | Bit/dim 3.6843(3.6376) | Xent 0.4029(0.3752) | Loss 9.0310(9.1285) | Error 0.1367(0.1308) Steps 880(856.05) | Grad Norm 12.0352(9.5593) | Total Time 0.00(0.00)\n",
      "Iter 9510 | Time 21.9236(21.2336) | Bit/dim 3.6294(3.6417) | Xent 0.3638(0.3889) | Loss 8.8408(9.0780) | Error 0.1289(0.1360) Steps 856(859.97) | Grad Norm 6.9229(9.4969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 96.4573, Epoch Time 1282.3453(1188.7712), Bit/dim 3.6511(best: 3.6034), Xent 0.7122, Loss 4.0072, Error 0.2227(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 21.9335(21.3469) | Bit/dim 3.6443(3.6420) | Xent 0.3906(0.3849) | Loss 8.9033(9.7197) | Error 0.1367(0.1357) Steps 850(862.92) | Grad Norm 12.1094(9.4299) | Total Time 0.00(0.00)\n",
      "Iter 9530 | Time 21.6085(21.3261) | Bit/dim 3.6441(3.6367) | Xent 0.3333(0.3752) | Loss 8.7604(9.4811) | Error 0.1233(0.1327) Steps 862(864.24) | Grad Norm 9.0103(9.2578) | Total Time 0.00(0.00)\n",
      "Iter 9540 | Time 21.0144(21.2324) | Bit/dim 3.6086(3.6321) | Xent 0.3317(0.3647) | Loss 8.6930(9.2916) | Error 0.1200(0.1290) Steps 838(857.24) | Grad Norm 7.0810(8.6906) | Total Time 0.00(0.00)\n",
      "Iter 9550 | Time 20.3362(21.1571) | Bit/dim 3.6370(3.6288) | Xent 0.3633(0.3643) | Loss 8.7966(9.1643) | Error 0.1378(0.1287) Steps 868(857.23) | Grad Norm 10.3465(8.6294) | Total Time 0.00(0.00)\n",
      "Iter 9560 | Time 21.3960(21.1233) | Bit/dim 3.6376(3.6240) | Xent 0.3789(0.3661) | Loss 8.8797(9.0497) | Error 0.1356(0.1299) Steps 868(853.60) | Grad Norm 8.5087(8.5990) | Total Time 0.00(0.00)\n",
      "Iter 9570 | Time 20.9942(21.1717) | Bit/dim 3.6511(3.6235) | Xent 0.3482(0.3663) | Loss 8.8783(8.9756) | Error 0.1156(0.1295) Steps 856(852.23) | Grad Norm 6.3713(8.5463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 96.0828, Epoch Time 1275.2184(1191.3646), Bit/dim 3.6481(best: 3.6034), Xent 0.7086, Loss 4.0024, Error 0.2223(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9580 | Time 20.9140(21.2239) | Bit/dim 3.6475(3.6249) | Xent 0.4356(0.3599) | Loss 8.8152(9.5433) | Error 0.1556(0.1281) Steps 844(854.73) | Grad Norm 11.6003(8.5610) | Total Time 0.00(0.00)\n",
      "Iter 9590 | Time 21.3037(21.3406) | Bit/dim 3.6393(3.6249) | Xent 0.3742(0.3543) | Loss 8.7719(9.3413) | Error 0.1300(0.1250) Steps 844(853.89) | Grad Norm 5.4646(8.2840) | Total Time 0.00(0.00)\n",
      "Iter 9600 | Time 21.8413(21.5695) | Bit/dim 3.9794(3.6707) | Xent 1.4556(0.4596) | Loss 10.7304(9.4045) | Error 0.3856(0.1514) Steps 862(858.58) | Grad Norm 19.8439(13.5116) | Total Time 0.00(0.00)\n",
      "Iter 9610 | Time 21.3735(21.5029) | Bit/dim 3.9060(3.7484) | Xent 1.1163(0.6575) | Loss 10.2233(9.6585) | Error 0.3911(0.2176) Steps 874(861.24) | Grad Norm 19.8379(14.7419) | Total Time 0.00(0.00)\n",
      "Iter 9620 | Time 22.4486(21.5828) | Bit/dim 3.8075(3.7678) | Xent 0.7555(0.7172) | Loss 9.5378(9.6756) | Error 0.2444(0.2408) Steps 910(868.66) | Grad Norm 7.6088(13.1678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 96.9834, Epoch Time 1303.5667(1194.7307), Bit/dim 3.7273(best: 3.6034), Xent 0.7640, Loss 4.1093, Error 0.2665(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9630 | Time 21.0633(21.5715) | Bit/dim 3.6783(3.7586) | Xent 0.5766(0.7037) | Loss 9.1457(10.2882) | Error 0.2011(0.2388) Steps 892(870.02) | Grad Norm 5.5212(11.4049) | Total Time 0.00(0.00)\n",
      "Iter 9640 | Time 22.3288(21.6207) | Bit/dim 3.6818(3.7380) | Xent 0.5179(0.6672) | Loss 9.0885(9.9778) | Error 0.1889(0.2284) Steps 874(867.74) | Grad Norm 3.6803(9.7747) | Total Time 0.00(0.00)\n",
      "Iter 9650 | Time 20.5018(21.4895) | Bit/dim 3.6847(3.7178) | Xent 0.5258(0.6256) | Loss 9.0634(9.7255) | Error 0.1811(0.2152) Steps 856(865.11) | Grad Norm 4.5676(8.4502) | Total Time 0.00(0.00)\n",
      "Iter 9660 | Time 21.2645(21.4210) | Bit/dim 3.6220(3.6969) | Xent 0.4511(0.5836) | Loss 8.7401(9.5152) | Error 0.1678(0.2019) Steps 862(863.78) | Grad Norm 3.1689(7.3678) | Total Time 0.00(0.00)\n",
      "Iter 9670 | Time 21.8418(21.3557) | Bit/dim 3.6116(3.6777) | Xent 0.3954(0.5473) | Loss 8.8289(9.3426) | Error 0.1456(0.1900) Steps 850(862.18) | Grad Norm 8.2430(6.8941) | Total Time 0.00(0.00)\n",
      "Iter 9680 | Time 20.8474(21.2626) | Bit/dim 3.6688(3.6671) | Xent 0.3562(0.5189) | Loss 8.9290(9.2256) | Error 0.1378(0.1821) Steps 856(860.37) | Grad Norm 3.9818(6.5787) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 94.2287, Epoch Time 1281.8444(1197.3441), Bit/dim 3.6289(best: 3.6034), Xent 0.6820, Loss 3.9699, Error 0.2232(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9690 | Time 21.1208(21.2039) | Bit/dim 3.6172(3.6550) | Xent 0.4299(0.4867) | Loss 8.7083(9.7177) | Error 0.1600(0.1714) Steps 856(862.77) | Grad Norm 9.1929(6.3117) | Total Time 0.00(0.00)\n",
      "Iter 9700 | Time 20.2271(21.2248) | Bit/dim 3.6047(3.6456) | Xent 0.4072(0.4681) | Loss 8.7922(9.4825) | Error 0.1389(0.1649) Steps 826(862.02) | Grad Norm 6.5437(6.4637) | Total Time 0.00(0.00)\n",
      "Iter 9710 | Time 20.6078(21.1853) | Bit/dim 3.6217(3.6400) | Xent 0.4167(0.4456) | Loss 8.8548(9.3098) | Error 0.1611(0.1571) Steps 838(862.59) | Grad Norm 4.5185(6.3776) | Total Time 0.00(0.00)\n",
      "Iter 9720 | Time 20.8292(21.1536) | Bit/dim 3.6106(3.6330) | Xent 0.3500(0.4250) | Loss 8.7384(9.1637) | Error 0.1311(0.1499) Steps 856(859.68) | Grad Norm 5.4339(6.1552) | Total Time 0.00(0.00)\n",
      "Iter 9730 | Time 21.5784(21.2964) | Bit/dim 3.6051(3.6293) | Xent 0.3543(0.4120) | Loss 8.8011(9.0681) | Error 0.1356(0.1452) Steps 892(863.77) | Grad Norm 4.8040(6.0130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 95.4292, Epoch Time 1281.2281(1199.8606), Bit/dim 3.6333(best: 3.6034), Xent 0.6711, Loss 3.9688, Error 0.2172(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9740 | Time 22.3179(21.3925) | Bit/dim 3.6319(3.6306) | Xent 0.3446(0.4013) | Loss 8.9451(9.7332) | Error 0.1267(0.1422) Steps 868(862.48) | Grad Norm 6.3617(6.2128) | Total Time 0.00(0.00)\n",
      "Iter 9750 | Time 20.5456(21.5220) | Bit/dim 3.6443(3.6314) | Xent 0.3616(0.3942) | Loss 8.6506(9.4895) | Error 0.1189(0.1390) Steps 814(860.08) | Grad Norm 8.7496(6.5209) | Total Time 0.00(0.00)\n",
      "Iter 9760 | Time 22.6344(21.5784) | Bit/dim 3.6016(3.6309) | Xent 0.3816(0.3908) | Loss 8.8520(9.3169) | Error 0.1289(0.1375) Steps 874(860.09) | Grad Norm 7.9992(6.7111) | Total Time 0.00(0.00)\n",
      "Iter 9770 | Time 22.5529(21.6652) | Bit/dim 3.6404(3.6337) | Xent 0.3521(0.3864) | Loss 8.7408(9.1911) | Error 0.1244(0.1364) Steps 856(862.73) | Grad Norm 8.4186(7.1409) | Total Time 0.00(0.00)\n",
      "Iter 9780 | Time 22.1291(21.7453) | Bit/dim 3.6443(3.6418) | Xent 0.4942(0.3949) | Loss 8.9593(9.1218) | Error 0.1767(0.1388) Steps 928(864.61) | Grad Norm 11.1506(7.3859) | Total Time 0.00(0.00)\n",
      "Iter 9790 | Time 21.7573(21.6798) | Bit/dim 3.6656(3.6415) | Xent 0.3652(0.3922) | Loss 8.9169(9.0563) | Error 0.1267(0.1385) Steps 874(865.49) | Grad Norm 5.2245(6.9969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 96.0099, Epoch Time 1311.1208(1203.1984), Bit/dim 3.6507(best: 3.6034), Xent 0.6704, Loss 3.9859, Error 0.2127(best: 0.2088)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9800 | Time 21.6835(21.7249) | Bit/dim 3.6738(3.6400) | Xent 0.3512(0.3824) | Loss 8.8265(9.6058) | Error 0.1256(0.1337) Steps 874(866.76) | Grad Norm 14.4242(7.4320) | Total Time 0.00(0.00)\n",
      "Iter 9810 | Time 20.4180(21.6766) | Bit/dim 4.2460(3.6884) | Xent 1.9963(0.5129) | Loss 11.6807(9.6436) | Error 0.4800(0.1636) Steps 826(868.45) | Grad Norm 39.1028(13.0893) | Total Time 0.00(0.00)\n",
      "Iter 9820 | Time 22.3054(21.4784) | Bit/dim 4.1637(3.8258) | Xent 1.4372(0.8599) | Loss 11.0678(10.1286) | Error 0.5033(0.2689) Steps 922(876.91) | Grad Norm 14.2395(18.1968) | Total Time 0.00(0.00)\n",
      "Iter 9830 | Time 21.5776(21.5584) | Bit/dim 3.9580(3.8820) | Xent 1.0588(0.9375) | Loss 10.2873(10.2363) | Error 0.3767(0.3064) Steps 886(888.55) | Grad Norm 4.6872(15.6054) | Total Time 0.00(0.00)\n",
      "Iter 9840 | Time 23.0310(21.6102) | Bit/dim 3.8694(3.8847) | Xent 0.9264(0.9372) | Loss 9.9271(10.1553) | Error 0.3211(0.3139) Steps 922(891.28) | Grad Norm 219.7939(19.5329) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 103.4956, Epoch Time 1318.6837(1206.6630), Bit/dim 4.0132(best: 3.6034), Xent 1.2902, Loss 4.6583, Error 0.4562(best: 0.2088)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3/epoch_150_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
