{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1/epoch_390_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 2341 | Time 117.8678(59.3645) | Bit/dim 3.6451(3.6473) | Xent 0.7126(0.7541) | Loss 11.7291(10.0860) | Error 0.2549(0.2683) Steps 610(630.50) | Grad Norm 3.7950(6.1403) | Total Time 0.00(0.00)\n",
      "Iter 2342 | Time 62.6381(59.4627) | Bit/dim 3.6529(3.6475) | Xent 0.7349(0.7536) | Loss 9.5368(10.0695) | Error 0.2606(0.2681) Steps 670(631.68) | Grad Norm 5.8581(6.1319) | Total Time 0.00(0.00)\n",
      "Iter 2343 | Time 58.6814(59.4392) | Bit/dim 3.6327(3.6470) | Xent 0.7647(0.7539) | Loss 9.5924(10.0552) | Error 0.2695(0.2681) Steps 622(631.39) | Grad Norm 6.5319(6.1439) | Total Time 0.00(0.00)\n",
      "Iter 2344 | Time 59.0509(59.4276) | Bit/dim 3.6558(3.6473) | Xent 0.7406(0.7535) | Loss 9.3582(10.0343) | Error 0.2655(0.2681) Steps 652(632.01) | Grad Norm 4.3016(6.0886) | Total Time 0.00(0.00)\n",
      "Iter 2345 | Time 55.8044(59.3189) | Bit/dim 3.6365(3.6470) | Xent 0.7255(0.7527) | Loss 9.4236(10.0160) | Error 0.2630(0.2679) Steps 640(632.25) | Grad Norm 2.5353(5.9820) | Total Time 0.00(0.00)\n",
      "Iter 2346 | Time 56.6513(59.2389) | Bit/dim 3.6484(3.6470) | Xent 0.7723(0.7533) | Loss 9.4597(9.9993) | Error 0.2729(0.2681) Steps 640(632.48) | Grad Norm 6.5766(5.9998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 40.5852, Epoch Time 467.4690(385.8045), Bit/dim 3.6480(best: inf), Xent 0.8051, Loss 4.0506, Error 0.2822(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2347 | Time 71.4941(59.6065) | Bit/dim 3.6559(3.6473) | Xent 0.7322(0.7526) | Loss 13.1961(10.0952) | Error 0.2626(0.2679) Steps 634(632.53) | Grad Norm 6.3278(6.0097) | Total Time 0.00(0.00)\n",
      "Iter 2348 | Time 60.9871(59.6479) | Bit/dim 3.6467(3.6473) | Xent 0.7412(0.7523) | Loss 9.5299(10.0782) | Error 0.2652(0.2678) Steps 634(632.57) | Grad Norm 4.9947(5.9792) | Total Time 0.00(0.00)\n",
      "Iter 2349 | Time 57.0115(59.5688) | Bit/dim 3.6443(3.6472) | Xent 0.7562(0.7524) | Loss 9.3240(10.0556) | Error 0.2689(0.2678) Steps 628(632.44) | Grad Norm 6.7938(6.0037) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 57.1630(59.4967) | Bit/dim 3.6476(3.6472) | Xent 0.7470(0.7522) | Loss 9.4579(10.0377) | Error 0.2685(0.2679) Steps 646(632.84) | Grad Norm 6.3546(6.0142) | Total Time 0.00(0.00)\n",
      "Iter 2351 | Time 57.1389(59.4259) | Bit/dim 3.6465(3.6472) | Xent 0.7452(0.7520) | Loss 9.4359(10.0196) | Error 0.2655(0.2678) Steps 628(632.70) | Grad Norm 6.8005(6.0378) | Total Time 0.00(0.00)\n",
      "Iter 2352 | Time 58.3089(59.3924) | Bit/dim 3.6340(3.6468) | Xent 0.7527(0.7520) | Loss 9.5160(10.0045) | Error 0.2678(0.2678) Steps 640(632.92) | Grad Norm 5.3890(6.0183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 23.8991, Epoch Time 401.8315(386.2853), Bit/dim 3.6474(best: 3.6480), Xent 0.8086, Loss 4.0517, Error 0.2831(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2353 | Time 60.0819(59.4131) | Bit/dim 3.6345(3.6464) | Xent 0.7347(0.7515) | Loss 12.9089(10.0917) | Error 0.2614(0.2676) Steps 670(634.03) | Grad Norm 5.2304(5.9947) | Total Time 0.00(0.00)\n",
      "Iter 2354 | Time 54.4610(59.2645) | Bit/dim 3.6413(3.6463) | Xent 0.7646(0.7519) | Loss 9.3987(10.0709) | Error 0.2761(0.2679) Steps 628(633.85) | Grad Norm 9.7379(6.1070) | Total Time 0.00(0.00)\n",
      "Iter 2355 | Time 61.1590(59.3214) | Bit/dim 3.6523(3.6464) | Xent 0.7774(0.7527) | Loss 9.2450(10.0461) | Error 0.2771(0.2681) Steps 658(634.57) | Grad Norm 10.4248(6.2365) | Total Time 0.00(0.00)\n",
      "Iter 2356 | Time 58.8283(59.3066) | Bit/dim 3.6440(3.6464) | Xent 0.7297(0.7520) | Loss 9.5156(10.0302) | Error 0.2589(0.2679) Steps 634(634.56) | Grad Norm 4.1797(6.1748) | Total Time 0.00(0.00)\n",
      "Iter 2357 | Time 57.8346(59.2624) | Bit/dim 3.6433(3.6463) | Xent 0.7422(0.7517) | Loss 9.3963(10.0112) | Error 0.2651(0.2678) Steps 634(634.54) | Grad Norm 6.5429(6.1858) | Total Time 0.00(0.00)\n",
      "Iter 2358 | Time 59.7898(59.2782) | Bit/dim 3.6450(3.6462) | Xent 0.7644(0.7521) | Loss 9.6427(10.0001) | Error 0.2721(0.2679) Steps 646(634.88) | Grad Norm 9.0858(6.2728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 23.3890, Epoch Time 391.5553(386.4434), Bit/dim 3.6418(best: 3.6474), Xent 0.8025, Loss 4.0430, Error 0.2834(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2359 | Time 59.6351(59.2889) | Bit/dim 3.6391(3.6460) | Xent 0.7485(0.7520) | Loss 13.1960(10.0960) | Error 0.2660(0.2678) Steps 664(635.76) | Grad Norm 6.2903(6.2734) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 61.5721(59.3574) | Bit/dim 3.6435(3.6459) | Xent 0.7819(0.7529) | Loss 9.5769(10.0804) | Error 0.2790(0.2682) Steps 646(636.06) | Grad Norm 9.5313(6.3711) | Total Time 0.00(0.00)\n",
      "Iter 2361 | Time 58.3805(59.3281) | Bit/dim 3.6463(3.6460) | Xent 0.7699(0.7534) | Loss 9.2932(10.0568) | Error 0.2696(0.2682) Steps 628(635.82) | Grad Norm 10.6003(6.4980) | Total Time 0.00(0.00)\n",
      "Iter 2362 | Time 60.3764(59.3596) | Bit/dim 3.6474(3.6460) | Xent 0.7688(0.7538) | Loss 9.5593(10.0419) | Error 0.2724(0.2684) Steps 628(635.59) | Grad Norm 7.5458(6.5294) | Total Time 0.00(0.00)\n",
      "Iter 2363 | Time 53.1262(59.1726) | Bit/dim 3.6510(3.6462) | Xent 0.7791(0.7546) | Loss 9.2750(10.0189) | Error 0.2748(0.2685) Steps 616(635.00) | Grad Norm 5.6333(6.5025) | Total Time 0.00(0.00)\n",
      "Iter 2364 | Time 55.4993(59.0624) | Bit/dim 3.6390(3.6459) | Xent 0.7989(0.7559) | Loss 9.5947(10.0061) | Error 0.2801(0.2689) Steps 616(634.43) | Grad Norm 8.4299(6.5604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 22.8753, Epoch Time 387.1001(386.4631), Bit/dim 3.6452(best: 3.6418), Xent 0.9130, Loss 4.1017, Error 0.3220(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2365 | Time 57.7512(59.0230) | Bit/dim 3.6370(3.6457) | Xent 0.8462(0.7586) | Loss 13.3749(10.1072) | Error 0.3015(0.2699) Steps 640(634.60) | Grad Norm 11.8802(6.7199) | Total Time 0.00(0.00)\n",
      "Iter 2366 | Time 54.1074(58.8756) | Bit/dim 3.6459(3.6457) | Xent 0.8429(0.7612) | Loss 9.6809(10.0944) | Error 0.3050(0.2709) Steps 628(634.40) | Grad Norm 10.3026(6.8274) | Total Time 0.00(0.00)\n",
      "Iter 2367 | Time 55.9056(58.7865) | Bit/dim 3.6519(3.6459) | Xent 0.8549(0.7640) | Loss 9.7127(10.0830) | Error 0.3005(0.2718) Steps 646(634.75) | Grad Norm 14.1297(7.0465) | Total Time 0.00(0.00)\n",
      "Iter 2368 | Time 58.4262(58.7757) | Bit/dim 3.6527(3.6461) | Xent 0.8126(0.7654) | Loss 9.6145(10.0689) | Error 0.2913(0.2724) Steps 646(635.08) | Grad Norm 8.3269(7.0849) | Total Time 0.00(0.00)\n",
      "Iter 2369 | Time 58.2320(58.7594) | Bit/dim 3.6601(3.6465) | Xent 0.7810(0.7659) | Loss 9.7150(10.0583) | Error 0.2782(0.2726) Steps 658(635.77) | Grad Norm 9.4896(7.1570) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 59.1955(58.7724) | Bit/dim 3.6388(3.6463) | Xent 0.8098(0.7672) | Loss 9.5557(10.0432) | Error 0.2909(0.2731) Steps 652(636.26) | Grad Norm 7.6423(7.1716) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 22.8821, Epoch Time 382.1601(386.3340), Bit/dim 3.6564(best: 3.6418), Xent 0.8276, Loss 4.0701, Error 0.2913(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2371 | Time 56.4685(58.7033) | Bit/dim 3.6464(3.6463) | Xent 0.7817(0.7677) | Loss 13.0715(10.1341) | Error 0.2782(0.2733) Steps 640(636.37) | Grad Norm 9.7488(7.2489) | Total Time 0.00(0.00)\n",
      "Iter 2372 | Time 56.4884(58.6369) | Bit/dim 3.6634(3.6468) | Xent 0.7448(0.7670) | Loss 9.5042(10.1152) | Error 0.2654(0.2730) Steps 634(636.30) | Grad Norm 9.9084(7.3287) | Total Time 0.00(0.00)\n",
      "Iter 2373 | Time 60.0643(58.6797) | Bit/dim 3.6554(3.6470) | Xent 0.7810(0.7674) | Loss 9.4089(10.0940) | Error 0.2825(0.2733) Steps 640(636.41) | Grad Norm 8.6163(7.3673) | Total Time 0.00(0.00)\n",
      "Iter 2374 | Time 52.2560(58.4870) | Bit/dim 3.6381(3.6468) | Xent 0.7838(0.7679) | Loss 9.4796(10.0755) | Error 0.2802(0.2735) Steps 616(635.80) | Grad Norm 10.0442(7.4476) | Total Time 0.00(0.00)\n",
      "Iter 2375 | Time 56.4378(58.4255) | Bit/dim 3.6584(3.6471) | Xent 0.7736(0.7681) | Loss 9.6168(10.0618) | Error 0.2764(0.2736) Steps 640(635.92) | Grad Norm 7.7052(7.4554) | Total Time 0.00(0.00)\n",
      "Iter 2376 | Time 59.2734(58.4510) | Bit/dim 3.6603(3.6475) | Xent 0.7777(0.7683) | Loss 9.5406(10.0461) | Error 0.2771(0.2737) Steps 634(635.87) | Grad Norm 11.2601(7.5695) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 22.7181, Epoch Time 379.7816(386.1374), Bit/dim 3.6715(best: 3.6418), Xent 0.8265, Loss 4.0848, Error 0.2881(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2377 | Time 56.9400(58.4056) | Bit/dim 3.6611(3.6479) | Xent 0.7640(0.7682) | Loss 12.5419(10.1210) | Error 0.2710(0.2736) Steps 628(635.63) | Grad Norm 12.0493(7.7039) | Total Time 0.00(0.00)\n",
      "Iter 2378 | Time 58.9445(58.4218) | Bit/dim 3.6388(3.6476) | Xent 0.7378(0.7673) | Loss 9.4155(10.0999) | Error 0.2684(0.2735) Steps 646(635.94) | Grad Norm 6.9086(7.6800) | Total Time 0.00(0.00)\n",
      "Iter 2379 | Time 56.1567(58.3538) | Bit/dim 3.6701(3.6483) | Xent 0.7643(0.7672) | Loss 9.3057(10.0760) | Error 0.2772(0.2736) Steps 640(636.06) | Grad Norm 10.1827(7.7551) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 57.5898(58.3309) | Bit/dim 3.6519(3.6484) | Xent 0.7736(0.7674) | Loss 9.5051(10.0589) | Error 0.2704(0.2735) Steps 634(636.00) | Grad Norm 5.9716(7.7016) | Total Time 0.00(0.00)\n",
      "Iter 2381 | Time 60.5871(58.3986) | Bit/dim 3.6640(3.6489) | Xent 0.7601(0.7672) | Loss 9.6591(10.0469) | Error 0.2638(0.2732) Steps 640(636.12) | Grad Norm 7.3566(7.6913) | Total Time 0.00(0.00)\n",
      "Iter 2382 | Time 55.6959(58.3175) | Bit/dim 3.6512(3.6490) | Xent 0.7670(0.7672) | Loss 9.3997(10.0275) | Error 0.2720(0.2732) Steps 652(636.60) | Grad Norm 6.7246(7.6623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 22.5210, Epoch Time 384.1917(386.0790), Bit/dim 3.6526(best: 3.6418), Xent 0.8010, Loss 4.0531, Error 0.2797(best: 0.2822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2383 | Time 58.6426(58.3273) | Bit/dim 3.6601(3.6493) | Xent 0.7394(0.7663) | Loss 13.0372(10.1178) | Error 0.2586(0.2727) Steps 628(636.34) | Grad Norm 5.9198(7.6100) | Total Time 0.00(0.00)\n",
      "Iter 2384 | Time 57.0091(58.2877) | Bit/dim 3.6496(3.6493) | Xent 0.7539(0.7660) | Loss 9.4739(10.0985) | Error 0.2675(0.2726) Steps 634(636.27) | Grad Norm 6.3187(7.5713) | Total Time 0.00(0.00)\n",
      "Iter 2385 | Time 58.5102(58.2944) | Bit/dim 3.6486(3.6493) | Xent 0.7466(0.7654) | Loss 9.6414(10.0848) | Error 0.2671(0.2724) Steps 628(636.02) | Grad Norm 4.7725(7.4873) | Total Time 0.00(0.00)\n",
      "Iter 2386 | Time 56.8945(58.2524) | Bit/dim 3.6635(3.6497) | Xent 0.7601(0.7652) | Loss 9.4288(10.0651) | Error 0.2774(0.2726) Steps 652(636.50) | Grad Norm 6.7778(7.4660) | Total Time 0.00(0.00)\n",
      "Iter 2387 | Time 59.9250(58.3026) | Bit/dim 3.6565(3.6499) | Xent 0.7365(0.7644) | Loss 9.5025(10.0482) | Error 0.2622(0.2722) Steps 652(636.97) | Grad Norm 6.6410(7.4413) | Total Time 0.00(0.00)\n",
      "Iter 2388 | Time 56.1870(58.2391) | Bit/dim 3.6506(3.6499) | Xent 0.7727(0.7646) | Loss 9.5976(10.0347) | Error 0.2729(0.2723) Steps 628(636.70) | Grad Norm 10.8196(7.5426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 22.9243, Epoch Time 385.7849(386.0702), Bit/dim 3.6485(best: 3.6418), Xent 0.8051, Loss 4.0510, Error 0.2821(best: 0.2797)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2389 | Time 56.3109(58.1813) | Bit/dim 3.6494(3.6499) | Xent 0.7551(0.7643) | Loss 12.9369(10.1217) | Error 0.2692(0.2722) Steps 622(636.26) | Grad Norm 5.2125(7.4727) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 58.2726(58.1840) | Bit/dim 3.6438(3.6497) | Xent 0.7607(0.7642) | Loss 9.5541(10.1047) | Error 0.2655(0.2720) Steps 616(635.65) | Grad Norm 5.5233(7.4142) | Total Time 0.00(0.00)\n",
      "Iter 2391 | Time 54.5390(58.0747) | Bit/dim 3.6501(3.6497) | Xent 0.7356(0.7634) | Loss 9.4425(10.0848) | Error 0.2602(0.2716) Steps 622(635.24) | Grad Norm 3.7928(7.3056) | Total Time 0.00(0.00)\n",
      "Iter 2392 | Time 56.6936(58.0332) | Bit/dim 3.6526(3.6498) | Xent 0.7346(0.7625) | Loss 9.0212(10.0529) | Error 0.2608(0.2713) Steps 616(634.66) | Grad Norm 4.6180(7.2250) | Total Time 0.00(0.00)\n",
      "Iter 2393 | Time 61.1593(58.1270) | Bit/dim 3.6475(3.6498) | Xent 0.7383(0.7618) | Loss 9.4787(10.0357) | Error 0.2620(0.2710) Steps 646(635.00) | Grad Norm 5.3995(7.1702) | Total Time 0.00(0.00)\n",
      "Iter 2394 | Time 57.5781(58.1105) | Bit/dim 3.6474(3.6497) | Xent 0.7447(0.7613) | Loss 9.4009(10.0167) | Error 0.2691(0.2710) Steps 640(635.15) | Grad Norm 3.5805(7.0625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 22.4986, Epoch Time 382.7382(385.9703), Bit/dim 3.6552(best: 3.6418), Xent 0.7976, Loss 4.0540, Error 0.2790(best: 0.2797)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2395 | Time 58.8267(58.1320) | Bit/dim 3.6558(3.6499) | Xent 0.7254(0.7602) | Loss 13.0684(10.1082) | Error 0.2582(0.2706) Steps 652(635.66) | Grad Norm 6.9469(7.0590) | Total Time 0.00(0.00)\n",
      "Iter 2396 | Time 61.2594(58.2259) | Bit/dim 3.6508(3.6499) | Xent 0.7393(0.7596) | Loss 9.5721(10.0921) | Error 0.2695(0.2705) Steps 652(636.15) | Grad Norm 5.1679(7.0023) | Total Time 0.00(0.00)\n",
      "Iter 2397 | Time 55.7421(58.1513) | Bit/dim 3.6481(3.6498) | Xent 0.7442(0.7591) | Loss 9.2879(10.0680) | Error 0.2625(0.2703) Steps 634(636.08) | Grad Norm 5.3491(6.9527) | Total Time 0.00(0.00)\n",
      "Iter 2398 | Time 58.5784(58.1641) | Bit/dim 3.6510(3.6499) | Xent 0.7414(0.7586) | Loss 9.4642(10.0499) | Error 0.2608(0.2700) Steps 628(635.84) | Grad Norm 5.3517(6.9047) | Total Time 0.00(0.00)\n",
      "Iter 2399 | Time 56.7677(58.1223) | Bit/dim 3.6320(3.6493) | Xent 0.7483(0.7583) | Loss 9.4626(10.0323) | Error 0.2691(0.2700) Steps 616(635.25) | Grad Norm 5.3401(6.8577) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 56.5740(58.0758) | Bit/dim 3.6377(3.6490) | Xent 0.7491(0.7580) | Loss 9.4436(10.0146) | Error 0.2648(0.2698) Steps 640(635.39) | Grad Norm 5.1855(6.8076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 23.8562, Epoch Time 387.4344(386.0142), Bit/dim 3.6455(best: 3.6418), Xent 0.7981, Loss 4.0445, Error 0.2802(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2401 | Time 57.9788(58.0729) | Bit/dim 3.6424(3.6488) | Xent 0.7412(0.7575) | Loss 13.1075(10.1074) | Error 0.2594(0.2695) Steps 628(635.17) | Grad Norm 4.5198(6.7389) | Total Time 0.00(0.00)\n",
      "Iter 2402 | Time 62.2237(58.1974) | Bit/dim 3.6370(3.6484) | Xent 0.7389(0.7569) | Loss 9.3072(10.0834) | Error 0.2598(0.2692) Steps 622(634.77) | Grad Norm 3.9208(6.6544) | Total Time 0.00(0.00)\n",
      "Iter 2403 | Time 56.9107(58.1588) | Bit/dim 3.6391(3.6482) | Xent 0.7431(0.7565) | Loss 9.1651(10.0558) | Error 0.2682(0.2692) Steps 640(634.93) | Grad Norm 6.1554(6.6394) | Total Time 0.00(0.00)\n",
      "Iter 2404 | Time 63.0216(58.3047) | Bit/dim 3.6445(3.6481) | Xent 0.7499(0.7563) | Loss 9.5732(10.0414) | Error 0.2680(0.2692) Steps 634(634.90) | Grad Norm 5.0874(6.5929) | Total Time 0.00(0.00)\n",
      "Iter 2405 | Time 55.8514(58.2311) | Bit/dim 3.6447(3.6480) | Xent 0.7241(0.7553) | Loss 9.4760(10.0244) | Error 0.2530(0.2687) Steps 646(635.23) | Grad Norm 4.6803(6.5355) | Total Time 0.00(0.00)\n",
      "Iter 2406 | Time 56.7511(58.1867) | Bit/dim 3.6558(3.6482) | Xent 0.7231(0.7544) | Loss 9.5522(10.0102) | Error 0.2584(0.2684) Steps 646(635.56) | Grad Norm 4.5410(6.4756) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 23.0896, Epoch Time 391.8461(386.1891), Bit/dim 3.6444(best: 3.6418), Xent 0.8021, Loss 4.0455, Error 0.2823(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 59.2600(58.2189) | Bit/dim 3.6471(3.6482) | Xent 0.7474(0.7542) | Loss 12.8981(10.0969) | Error 0.2668(0.2683) Steps 658(636.23) | Grad Norm 4.6516(6.4209) | Total Time 0.00(0.00)\n",
      "Iter 2408 | Time 57.0309(58.1833) | Bit/dim 3.6378(3.6478) | Xent 0.7403(0.7538) | Loss 9.6101(10.0823) | Error 0.2650(0.2682) Steps 664(637.06) | Grad Norm 4.8106(6.3726) | Total Time 0.00(0.00)\n",
      "Iter 2409 | Time 57.0981(58.1507) | Bit/dim 3.6393(3.6476) | Xent 0.7481(0.7536) | Loss 9.5404(10.0660) | Error 0.2671(0.2682) Steps 622(636.61) | Grad Norm 7.8240(6.4162) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 55.3625(58.0671) | Bit/dim 3.6466(3.6476) | Xent 0.7607(0.7538) | Loss 9.6421(10.0533) | Error 0.2780(0.2685) Steps 646(636.89) | Grad Norm 9.6341(6.5127) | Total Time 0.00(0.00)\n",
      "Iter 2411 | Time 62.1826(58.1905) | Bit/dim 3.6508(3.6477) | Xent 0.7416(0.7534) | Loss 9.5033(10.0368) | Error 0.2599(0.2682) Steps 664(637.71) | Grad Norm 6.9337(6.5253) | Total Time 0.00(0.00)\n",
      "Iter 2412 | Time 56.5855(58.1424) | Bit/dim 3.6544(3.6479) | Xent 0.7305(0.7527) | Loss 9.5119(10.0211) | Error 0.2590(0.2679) Steps 628(637.41) | Grad Norm 6.1223(6.5132) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 22.8368, Epoch Time 386.0380(386.1846), Bit/dim 3.6452(best: 3.6418), Xent 0.8127, Loss 4.0516, Error 0.2863(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 60.1224(58.2018) | Bit/dim 3.6447(3.6478) | Xent 0.7500(0.7527) | Loss 12.5324(10.0964) | Error 0.2641(0.2678) Steps 646(637.67) | Grad Norm 7.5403(6.5440) | Total Time 0.00(0.00)\n",
      "Iter 2414 | Time 60.3840(58.2672) | Bit/dim 3.6390(3.6475) | Xent 0.7368(0.7522) | Loss 9.5394(10.0797) | Error 0.2642(0.2677) Steps 634(637.56) | Grad Norm 5.4122(6.5101) | Total Time 0.00(0.00)\n",
      "Iter 2415 | Time 59.5454(58.3056) | Bit/dim 3.6295(3.6470) | Xent 0.7122(0.7510) | Loss 9.1339(10.0513) | Error 0.2586(0.2675) Steps 616(636.92) | Grad Norm 3.7378(6.4269) | Total Time 0.00(0.00)\n",
      "Iter 2416 | Time 54.7071(58.1976) | Bit/dim 3.6481(3.6470) | Xent 0.7523(0.7510) | Loss 9.5528(10.0364) | Error 0.2668(0.2674) Steps 622(636.47) | Grad Norm 7.2031(6.4502) | Total Time 0.00(0.00)\n",
      "Iter 2417 | Time 54.9250(58.0995) | Bit/dim 3.6475(3.6470) | Xent 0.7450(0.7508) | Loss 9.4215(10.0179) | Error 0.2668(0.2674) Steps 646(636.75) | Grad Norm 6.0910(6.4394) | Total Time 0.00(0.00)\n",
      "Iter 2418 | Time 55.8448(58.0318) | Bit/dim 3.6324(3.6466) | Xent 0.7384(0.7505) | Loss 9.5124(10.0027) | Error 0.2679(0.2674) Steps 610(635.95) | Grad Norm 5.4652(6.4102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 22.6640, Epoch Time 383.9809(386.1185), Bit/dim 3.6433(best: 3.6418), Xent 0.8073, Loss 4.0469, Error 0.2812(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 55.9351(57.9689) | Bit/dim 3.6467(3.6466) | Xent 0.7233(0.7497) | Loss 13.1939(10.0985) | Error 0.2590(0.2672) Steps 646(636.25) | Grad Norm 7.6226(6.4466) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 58.8948(57.9967) | Bit/dim 3.6550(3.6468) | Xent 0.7448(0.7495) | Loss 9.6378(10.0847) | Error 0.2612(0.2670) Steps 616(635.64) | Grad Norm 6.8355(6.4582) | Total Time 0.00(0.00)\n",
      "Iter 2421 | Time 57.6197(57.9854) | Bit/dim 3.6386(3.6466) | Xent 0.7064(0.7482) | Loss 9.4594(10.0659) | Error 0.2569(0.2667) Steps 640(635.78) | Grad Norm 4.8464(6.4099) | Total Time 0.00(0.00)\n",
      "Iter 2422 | Time 58.6319(58.0048) | Bit/dim 3.6512(3.6467) | Xent 0.7479(0.7482) | Loss 9.4119(10.0463) | Error 0.2702(0.2668) Steps 628(635.54) | Grad Norm 9.6426(6.5069) | Total Time 0.00(0.00)\n",
      "Iter 2423 | Time 54.7321(57.9066) | Bit/dim 3.6401(3.6465) | Xent 0.7535(0.7484) | Loss 9.4146(10.0273) | Error 0.2691(0.2669) Steps 622(635.14) | Grad Norm 10.3364(6.6218) | Total Time 0.00(0.00)\n",
      "Iter 2424 | Time 55.2536(57.8270) | Bit/dim 3.6338(3.6461) | Xent 0.7498(0.7484) | Loss 9.4021(10.0086) | Error 0.2685(0.2669) Steps 634(635.10) | Grad Norm 5.6534(6.5927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 23.3225, Epoch Time 380.2540(385.9426), Bit/dim 3.6470(best: 3.6418), Xent 0.8014, Loss 4.0477, Error 0.2797(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 56.3452(57.7826) | Bit/dim 3.6398(3.6459) | Xent 0.7203(0.7476) | Loss 12.9774(10.0976) | Error 0.2564(0.2666) Steps 652(635.61) | Grad Norm 7.2306(6.6118) | Total Time 0.00(0.00)\n",
      "Iter 2426 | Time 57.8108(57.7834) | Bit/dim 3.6448(3.6459) | Xent 0.7430(0.7474) | Loss 9.4697(10.0788) | Error 0.2669(0.2666) Steps 658(636.28) | Grad Norm 7.4782(6.6378) | Total Time 0.00(0.00)\n",
      "Iter 2427 | Time 58.4120(57.8023) | Bit/dim 3.6527(3.6461) | Xent 0.7270(0.7468) | Loss 9.6133(10.0648) | Error 0.2572(0.2663) Steps 646(636.57) | Grad Norm 3.4124(6.5411) | Total Time 0.00(0.00)\n",
      "Iter 2428 | Time 58.8000(57.8322) | Bit/dim 3.6352(3.6458) | Xent 0.7504(0.7469) | Loss 9.4341(10.0459) | Error 0.2704(0.2664) Steps 628(636.32) | Grad Norm 6.7693(6.5479) | Total Time 0.00(0.00)\n",
      "Iter 2429 | Time 57.7079(57.8285) | Bit/dim 3.6435(3.6457) | Xent 0.7779(0.7478) | Loss 9.4065(10.0267) | Error 0.2735(0.2667) Steps 664(637.15) | Grad Norm 7.0046(6.5616) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 55.7348(57.7657) | Bit/dim 3.6459(3.6457) | Xent 0.7316(0.7474) | Loss 9.4527(10.0095) | Error 0.2609(0.2665) Steps 640(637.23) | Grad Norm 6.1553(6.5494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 24.2780, Epoch Time 384.9826(385.9138), Bit/dim 3.6419(best: 3.6418), Xent 0.8257, Loss 4.0548, Error 0.2902(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 58.4749(57.7869) | Bit/dim 3.6440(3.6457) | Xent 0.7676(0.7480) | Loss 12.6168(10.0877) | Error 0.2701(0.2666) Steps 652(637.67) | Grad Norm 8.5354(6.6090) | Total Time 0.00(0.00)\n",
      "Iter 2432 | Time 58.2521(57.8009) | Bit/dim 3.6374(3.6454) | Xent 0.7467(0.7479) | Loss 9.2645(10.0630) | Error 0.2640(0.2665) Steps 658(638.28) | Grad Norm 6.6827(6.6112) | Total Time 0.00(0.00)\n",
      "Iter 2433 | Time 58.7294(57.8287) | Bit/dim 3.6512(3.6456) | Xent 0.7423(0.7478) | Loss 9.5362(10.0472) | Error 0.2664(0.2665) Steps 634(638.16) | Grad Norm 5.4098(6.5752) | Total Time 0.00(0.00)\n",
      "Iter 2434 | Time 60.9923(57.9236) | Bit/dim 3.6433(3.6455) | Xent 0.7629(0.7482) | Loss 9.5146(10.0312) | Error 0.2699(0.2666) Steps 640(638.21) | Grad Norm 10.3208(6.6875) | Total Time 0.00(0.00)\n",
      "Iter 2435 | Time 57.4484(57.9094) | Bit/dim 3.6507(3.6457) | Xent 0.7515(0.7483) | Loss 9.4899(10.0150) | Error 0.2681(0.2667) Steps 646(638.44) | Grad Norm 9.1288(6.7608) | Total Time 0.00(0.00)\n",
      "Iter 2436 | Time 56.8856(57.8787) | Bit/dim 3.6254(3.6451) | Xent 0.7151(0.7473) | Loss 9.3966(9.9965) | Error 0.2566(0.2664) Steps 634(638.31) | Grad Norm 4.5343(6.6940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 23.9536, Epoch Time 390.7750(386.0596), Bit/dim 3.6539(best: 3.6418), Xent 0.8099, Loss 4.0589, Error 0.2806(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 61.8699(57.9984) | Bit/dim 3.6589(3.6455) | Xent 0.7329(0.7469) | Loss 13.2835(10.0951) | Error 0.2628(0.2663) Steps 640(638.36) | Grad Norm 9.3845(6.7747) | Total Time 0.00(0.00)\n",
      "Iter 2438 | Time 58.6514(58.0180) | Bit/dim 3.6400(3.6453) | Xent 0.7093(0.7458) | Loss 9.2789(10.0706) | Error 0.2511(0.2658) Steps 610(637.51) | Grad Norm 5.7613(6.7443) | Total Time 0.00(0.00)\n",
      "Iter 2439 | Time 61.1487(58.1119) | Bit/dim 3.6506(3.6455) | Xent 0.7276(0.7452) | Loss 9.4751(10.0527) | Error 0.2625(0.2657) Steps 628(637.23) | Grad Norm 8.6621(6.8018) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 58.2474(58.1160) | Bit/dim 3.6397(3.6453) | Xent 0.7660(0.7458) | Loss 9.4912(10.0359) | Error 0.2690(0.2658) Steps 646(637.49) | Grad Norm 10.9832(6.9273) | Total Time 0.00(0.00)\n",
      "Iter 2441 | Time 57.5933(58.1003) | Bit/dim 3.6370(3.6451) | Xent 0.7296(0.7454) | Loss 9.5534(10.0214) | Error 0.2625(0.2657) Steps 634(637.38) | Grad Norm 4.3434(6.8498) | Total Time 0.00(0.00)\n",
      "Iter 2442 | Time 60.2430(58.1646) | Bit/dim 3.6533(3.6453) | Xent 0.7929(0.7468) | Loss 9.3510(10.0013) | Error 0.2826(0.2662) Steps 646(637.64) | Grad Norm 12.0264(7.0051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 23.2981, Epoch Time 396.9038(386.3849), Bit/dim 3.6450(best: 3.6418), Xent 0.8627, Loss 4.0763, Error 0.3015(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 59.1427(58.1939) | Bit/dim 3.6541(3.6456) | Xent 0.8074(0.7486) | Loss 12.9960(10.0911) | Error 0.2776(0.2665) Steps 628(637.35) | Grad Norm 13.0519(7.1865) | Total Time 0.00(0.00)\n",
      "Iter 2444 | Time 58.1920(58.1939) | Bit/dim 3.6474(3.6456) | Xent 0.7730(0.7493) | Loss 9.4076(10.0706) | Error 0.2806(0.2670) Steps 634(637.25) | Grad Norm 12.6338(7.3499) | Total Time 0.00(0.00)\n",
      "Iter 2445 | Time 58.3588(58.1988) | Bit/dim 3.6444(3.6456) | Xent 0.7900(0.7505) | Loss 9.5086(10.0538) | Error 0.2830(0.2675) Steps 640(637.34) | Grad Norm 9.6283(7.4182) | Total Time 0.00(0.00)\n",
      "Iter 2446 | Time 58.7695(58.2159) | Bit/dim 3.6402(3.6454) | Xent 0.7559(0.7507) | Loss 9.4437(10.0355) | Error 0.2672(0.2674) Steps 658(637.96) | Grad Norm 7.8293(7.4306) | Total Time 0.00(0.00)\n",
      "Iter 2447 | Time 57.5353(58.1955) | Bit/dim 3.6424(3.6453) | Xent 0.7646(0.7511) | Loss 9.5605(10.0212) | Error 0.2750(0.2677) Steps 628(637.66) | Grad Norm 9.6380(7.4968) | Total Time 0.00(0.00)\n",
      "Iter 2448 | Time 60.9349(58.2777) | Bit/dim 3.6515(3.6455) | Xent 0.8022(0.7527) | Loss 9.6696(10.0107) | Error 0.2863(0.2682) Steps 652(638.09) | Grad Norm 10.3889(7.5836) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 22.9294, Epoch Time 391.5251(386.5391), Bit/dim 3.6454(best: 3.6418), Xent 0.8295, Loss 4.0602, Error 0.2887(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 62.0473(58.3908) | Bit/dim 3.6437(3.6455) | Xent 0.7803(0.7535) | Loss 13.4267(10.1131) | Error 0.2827(0.2687) Steps 634(637.96) | Grad Norm 6.4110(7.5484) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 57.8097(58.3734) | Bit/dim 3.6493(3.6456) | Xent 0.7813(0.7543) | Loss 9.2344(10.0868) | Error 0.2800(0.2690) Steps 646(638.21) | Grad Norm 9.4714(7.6061) | Total Time 0.00(0.00)\n",
      "Iter 2451 | Time 61.4056(58.4643) | Bit/dim 3.6530(3.6458) | Xent 0.7479(0.7541) | Loss 9.5974(10.0721) | Error 0.2624(0.2688) Steps 646(638.44) | Grad Norm 10.0481(7.6793) | Total Time 0.00(0.00)\n",
      "Iter 2452 | Time 56.0660(58.3924) | Bit/dim 3.6437(3.6457) | Xent 0.7641(0.7544) | Loss 9.6367(10.0590) | Error 0.2765(0.2690) Steps 634(638.31) | Grad Norm 6.6242(7.6477) | Total Time 0.00(0.00)\n",
      "Iter 2453 | Time 58.4300(58.3935) | Bit/dim 3.6573(3.6461) | Xent 0.7684(0.7548) | Loss 9.4771(10.0416) | Error 0.2731(0.2692) Steps 658(638.90) | Grad Norm 9.5767(7.7055) | Total Time 0.00(0.00)\n",
      "Iter 2454 | Time 56.6605(58.3415) | Bit/dim 3.6385(3.6459) | Xent 0.7442(0.7545) | Loss 9.5053(10.0255) | Error 0.2642(0.2690) Steps 610(638.03) | Grad Norm 7.2431(7.6917) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 22.8993, Epoch Time 391.1886(386.6786), Bit/dim 3.6544(best: 3.6418), Xent 0.8069, Loss 4.0579, Error 0.2813(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 60.1560(58.3960) | Bit/dim 3.6525(3.6461) | Xent 0.7338(0.7539) | Loss 13.2420(10.1220) | Error 0.2660(0.2689) Steps 622(637.55) | Grad Norm 8.4233(7.7136) | Total Time 0.00(0.00)\n",
      "Iter 2456 | Time 59.6118(58.4324) | Bit/dim 3.6474(3.6461) | Xent 0.7234(0.7530) | Loss 9.3606(10.0991) | Error 0.2540(0.2685) Steps 658(638.16) | Grad Norm 4.2320(7.6092) | Total Time 0.00(0.00)\n",
      "Iter 2457 | Time 60.0427(58.4807) | Bit/dim 3.6425(3.6460) | Xent 0.7499(0.7529) | Loss 9.4315(10.0791) | Error 0.2682(0.2685) Steps 658(638.76) | Grad Norm 7.5993(7.6089) | Total Time 0.00(0.00)\n",
      "Iter 2458 | Time 60.1486(58.5308) | Bit/dim 3.6503(3.6461) | Xent 0.7283(0.7522) | Loss 9.3869(10.0583) | Error 0.2586(0.2682) Steps 664(639.51) | Grad Norm 6.0118(7.5610) | Total Time 0.00(0.00)\n",
      "Iter 2459 | Time 59.5098(58.5601) | Bit/dim 3.6525(3.6463) | Xent 0.7320(0.7516) | Loss 9.5631(10.0435) | Error 0.2624(0.2680) Steps 640(639.53) | Grad Norm 5.8642(7.5101) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 56.2441(58.4907) | Bit/dim 3.6503(3.6464) | Xent 0.7399(0.7512) | Loss 9.5754(10.0294) | Error 0.2660(0.2679) Steps 628(639.18) | Grad Norm 4.5748(7.4220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 23.6714, Epoch Time 395.1311(386.9322), Bit/dim 3.6500(best: 3.6418), Xent 0.7910, Loss 4.0455, Error 0.2750(best: 0.2790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 60.2137(58.5424) | Bit/dim 3.6446(3.6464) | Xent 0.7244(0.7504) | Loss 13.2405(10.1258) | Error 0.2594(0.2677) Steps 634(639.03) | Grad Norm 4.5264(7.3351) | Total Time 0.00(0.00)\n",
      "Iter 2462 | Time 62.6800(58.6665) | Bit/dim 3.6527(3.6466) | Xent 0.7401(0.7501) | Loss 9.6531(10.1116) | Error 0.2632(0.2675) Steps 652(639.42) | Grad Norm 5.4804(7.2795) | Total Time 0.00(0.00)\n",
      "Iter 2463 | Time 60.1702(58.7116) | Bit/dim 3.6446(3.6465) | Xent 0.7266(0.7494) | Loss 9.5466(10.0946) | Error 0.2598(0.2673) Steps 652(639.79) | Grad Norm 3.2226(7.1578) | Total Time 0.00(0.00)\n",
      "Iter 2464 | Time 59.4489(58.7337) | Bit/dim 3.6439(3.6464) | Xent 0.7305(0.7488) | Loss 9.4954(10.0767) | Error 0.2626(0.2672) Steps 628(639.44) | Grad Norm 5.0965(7.0959) | Total Time 0.00(0.00)\n",
      "Iter 2465 | Time 58.0863(58.7143) | Bit/dim 3.6378(3.6462) | Xent 0.7341(0.7484) | Loss 9.1154(10.0478) | Error 0.2584(0.2669) Steps 664(640.18) | Grad Norm 3.0858(6.9756) | Total Time 0.00(0.00)\n",
      "Iter 2466 | Time 59.5873(58.7405) | Bit/dim 3.6422(3.6461) | Xent 0.7049(0.7471) | Loss 9.4294(10.0293) | Error 0.2504(0.2664) Steps 634(639.99) | Grad Norm 5.9759(6.9457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 23.1593, Epoch Time 399.0267(387.2950), Bit/dim 3.6415(best: 3.6418), Xent 0.7962, Loss 4.0396, Error 0.2772(best: 0.2750)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 59.9605(58.7771) | Bit/dim 3.6417(3.6459) | Xent 0.7447(0.7470) | Loss 13.3119(10.1278) | Error 0.2664(0.2664) Steps 622(639.45) | Grad Norm 5.0829(6.8898) | Total Time 0.00(0.00)\n",
      "Iter 2468 | Time 59.7314(58.8057) | Bit/dim 3.6426(3.6458) | Xent 0.7152(0.7460) | Loss 9.4940(10.1087) | Error 0.2575(0.2661) Steps 640(639.47) | Grad Norm 5.4560(6.8468) | Total Time 0.00(0.00)\n",
      "Iter 2469 | Time 58.7338(58.8036) | Bit/dim 3.6579(3.6462) | Xent 0.7555(0.7463) | Loss 9.5308(10.0914) | Error 0.2679(0.2662) Steps 646(639.66) | Grad Norm 7.2432(6.8586) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 56.9881(58.7491) | Bit/dim 3.6430(3.6461) | Xent 0.7498(0.7464) | Loss 9.3400(10.0689) | Error 0.2665(0.2662) Steps 622(639.13) | Grad Norm 7.5035(6.8780) | Total Time 0.00(0.00)\n",
      "Iter 2471 | Time 55.7554(58.6593) | Bit/dim 3.6339(3.6457) | Xent 0.7366(0.7461) | Loss 9.5930(10.0546) | Error 0.2634(0.2661) Steps 640(639.16) | Grad Norm 7.5022(6.8967) | Total Time 0.00(0.00)\n",
      "Iter 2472 | Time 62.4827(58.7740) | Bit/dim 3.6360(3.6454) | Xent 0.7377(0.7459) | Loss 9.5705(10.0401) | Error 0.2609(0.2660) Steps 628(638.83) | Grad Norm 7.6818(6.9203) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 23.0650, Epoch Time 392.2968(387.4451), Bit/dim 3.6477(best: 3.6415), Xent 0.7997, Loss 4.0475, Error 0.2776(best: 0.2750)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 59.4499(58.7943) | Bit/dim 3.6447(3.6454) | Xent 0.7216(0.7452) | Loss 13.0710(10.1310) | Error 0.2514(0.2655) Steps 640(638.86) | Grad Norm 4.6114(6.8510) | Total Time 0.00(0.00)\n",
      "Iter 2474 | Time 56.8232(58.7351) | Bit/dim 3.6459(3.6454) | Xent 0.7712(0.7459) | Loss 9.5442(10.1134) | Error 0.2748(0.2658) Steps 646(639.08) | Grad Norm 11.3876(6.9871) | Total Time 0.00(0.00)\n",
      "Iter 2475 | Time 60.7270(58.7949) | Bit/dim 3.6409(3.6453) | Xent 0.7566(0.7463) | Loss 9.5034(10.0951) | Error 0.2684(0.2659) Steps 652(639.46) | Grad Norm 8.9863(7.0471) | Total Time 0.00(0.00)\n",
      "Iter 2476 | Time 56.0198(58.7116) | Bit/dim 3.6371(3.6450) | Xent 0.7204(0.7455) | Loss 9.4807(10.0767) | Error 0.2542(0.2655) Steps 652(639.84) | Grad Norm 5.7201(7.0073) | Total Time 0.00(0.00)\n",
      "Iter 2477 | Time 59.5730(58.7375) | Bit/dim 3.6478(3.6451) | Xent 0.7625(0.7460) | Loss 9.4559(10.0580) | Error 0.2696(0.2657) Steps 640(639.84) | Grad Norm 8.9893(7.0667) | Total Time 0.00(0.00)\n",
      "Iter 2478 | Time 61.8476(58.8308) | Bit/dim 3.6348(3.6448) | Xent 0.7590(0.7464) | Loss 9.5130(10.0417) | Error 0.2715(0.2658) Steps 628(639.49) | Grad Norm 5.9963(7.0346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 23.7149, Epoch Time 394.1237(387.6454), Bit/dim 3.6443(best: 3.6415), Xent 0.8377, Loss 4.0632, Error 0.2939(best: 0.2750)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2479 | Time 58.7740(58.8291) | Bit/dim 3.6462(3.6449) | Xent 0.7670(0.7470) | Loss 13.2675(10.1385) | Error 0.2779(0.2662) Steps 652(639.86) | Grad Norm 10.0651(7.1255) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 60.3044(58.8733) | Bit/dim 3.6438(3.6448) | Xent 0.7571(0.7473) | Loss 9.6774(10.1246) | Error 0.2691(0.2663) Steps 634(639.69) | Grad Norm 9.4911(7.1965) | Total Time 0.00(0.00)\n",
      "Iter 2481 | Time 57.9440(58.8455) | Bit/dim 3.6404(3.6447) | Xent 0.7502(0.7474) | Loss 9.3518(10.1014) | Error 0.2644(0.2662) Steps 628(639.34) | Grad Norm 6.6329(7.1796) | Total Time 0.00(0.00)\n",
      "Iter 2482 | Time 59.8717(58.8762) | Bit/dim 3.6500(3.6448) | Xent 0.7695(0.7481) | Loss 9.1913(10.0741) | Error 0.2709(0.2664) Steps 646(639.54) | Grad Norm 8.2584(7.2120) | Total Time 0.00(0.00)\n",
      "Iter 2483 | Time 61.6888(58.9606) | Bit/dim 3.6327(3.6445) | Xent 0.7263(0.7474) | Loss 9.6434(10.0612) | Error 0.2579(0.2661) Steps 640(639.55) | Grad Norm 3.9865(7.1152) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1/epoch_400_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
