{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run2/epoch_330_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run2', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 1981 | Time 109.5646(59.5948) | Bit/dim 3.6582(3.6653) | Xent 0.8712(0.9220) | Loss 11.7012(10.1373) | Error 0.3121(0.3300) Steps 604(589.38) | Grad Norm 2.1154(6.8891) | Total Time 0.00(0.00)\n",
      "Iter 1982 | Time 59.5126(59.5924) | Bit/dim 3.6620(3.6652) | Xent 0.9153(0.9218) | Loss 9.4635(10.1171) | Error 0.3279(0.3299) Steps 610(590.00) | Grad Norm 7.0370(6.8935) | Total Time 0.00(0.00)\n",
      "Iter 1983 | Time 55.8807(59.4810) | Bit/dim 3.6690(3.6653) | Xent 0.9241(0.9219) | Loss 9.4334(10.0966) | Error 0.3324(0.3300) Steps 604(590.42) | Grad Norm 8.4713(6.9408) | Total Time 0.00(0.00)\n",
      "Iter 1984 | Time 56.3993(59.3886) | Bit/dim 3.6671(3.6654) | Xent 0.8906(0.9209) | Loss 9.4544(10.0773) | Error 0.3243(0.3298) Steps 568(589.74) | Grad Norm 6.4451(6.9260) | Total Time 0.00(0.00)\n",
      "Iter 1985 | Time 57.2377(59.3240) | Bit/dim 3.6613(3.6652) | Xent 0.9156(0.9208) | Loss 9.6948(10.0658) | Error 0.3216(0.3296) Steps 580(589.45) | Grad Norm 5.8948(6.8950) | Total Time 0.00(0.00)\n",
      "Iter 1986 | Time 56.4158(59.2368) | Bit/dim 3.6574(3.6650) | Xent 0.9070(0.9204) | Loss 9.4691(10.0479) | Error 0.3260(0.3295) Steps 556(588.45) | Grad Norm 4.3130(6.8176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 38.5328, Epoch Time 449.4980(387.7495), Bit/dim 3.6602(best: inf), Xent 0.8997, Loss 4.1101, Error 0.3197(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 69.4425(59.5430) | Bit/dim 3.6676(3.6651) | Xent 0.9054(0.9199) | Loss 12.7224(10.1282) | Error 0.3209(0.3292) Steps 574(588.01) | Grad Norm 5.2554(6.7707) | Total Time 0.00(0.00)\n",
      "Iter 1988 | Time 55.8541(59.4323) | Bit/dim 3.6641(3.6651) | Xent 0.9086(0.9196) | Loss 9.7075(10.1155) | Error 0.3215(0.3290) Steps 580(587.77) | Grad Norm 6.5457(6.7640) | Total Time 0.00(0.00)\n",
      "Iter 1989 | Time 58.7316(59.4113) | Bit/dim 3.6624(3.6650) | Xent 0.8866(0.9186) | Loss 9.4312(10.0950) | Error 0.3154(0.3286) Steps 598(588.08) | Grad Norm 4.7363(6.7031) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 60.8676(59.4550) | Bit/dim 3.6454(3.6644) | Xent 0.8929(0.9178) | Loss 9.5406(10.0784) | Error 0.3185(0.3283) Steps 598(588.38) | Grad Norm 4.3253(6.6318) | Total Time 0.00(0.00)\n",
      "Iter 1991 | Time 55.0365(59.3224) | Bit/dim 3.6593(3.6642) | Xent 0.8988(0.9172) | Loss 9.5171(10.0615) | Error 0.3207(0.3280) Steps 568(587.77) | Grad Norm 6.7860(6.6364) | Total Time 0.00(0.00)\n",
      "Iter 1992 | Time 59.4102(59.3250) | Bit/dim 3.6642(3.6642) | Xent 0.8824(0.9162) | Loss 9.6783(10.0500) | Error 0.3210(0.3278) Steps 580(587.53) | Grad Norm 6.2913(6.6261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 23.2224, Epoch Time 398.3596(388.0678), Bit/dim 3.6605(best: 3.6602), Xent 0.8980, Loss 4.1095, Error 0.3204(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 58.4335(59.2983) | Bit/dim 3.6611(3.6641) | Xent 0.8935(0.9155) | Loss 12.8156(10.1330) | Error 0.3213(0.3276) Steps 580(587.31) | Grad Norm 3.7989(6.5413) | Total Time 0.00(0.00)\n",
      "Iter 1994 | Time 56.2384(59.2065) | Bit/dim 3.6615(3.6641) | Xent 0.8690(0.9141) | Loss 9.2861(10.1076) | Error 0.3055(0.3270) Steps 580(587.09) | Grad Norm 6.3703(6.5361) | Total Time 0.00(0.00)\n",
      "Iter 1995 | Time 58.2534(59.1779) | Bit/dim 3.6633(3.6640) | Xent 0.9251(0.9145) | Loss 9.5697(10.0915) | Error 0.3255(0.3269) Steps 586(587.06) | Grad Norm 7.7890(6.5737) | Total Time 0.00(0.00)\n",
      "Iter 1996 | Time 56.0361(59.0837) | Bit/dim 3.6598(3.6639) | Xent 0.9147(0.9145) | Loss 9.4980(10.0737) | Error 0.3266(0.3269) Steps 586(587.02) | Grad Norm 3.8350(6.4915) | Total Time 0.00(0.00)\n",
      "Iter 1997 | Time 61.4517(59.1547) | Bit/dim 3.6612(3.6638) | Xent 0.8881(0.9137) | Loss 9.3520(10.0520) | Error 0.3176(0.3266) Steps 586(586.99) | Grad Norm 8.1748(6.5420) | Total Time 0.00(0.00)\n",
      "Iter 1998 | Time 57.9836(59.1196) | Bit/dim 3.6597(3.6637) | Xent 0.8993(0.9132) | Loss 9.4906(10.0352) | Error 0.3231(0.3265) Steps 604(587.50) | Grad Norm 6.9401(6.5540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 22.5929, Epoch Time 387.0140(388.0362), Bit/dim 3.6620(best: 3.6602), Xent 0.9064, Loss 4.1152, Error 0.3266(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 56.3680(59.0370) | Bit/dim 3.6570(3.6635) | Xent 0.8976(0.9128) | Loss 13.0344(10.1251) | Error 0.3209(0.3264) Steps 592(587.64) | Grad Norm 5.3348(6.5174) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 60.5341(59.0819) | Bit/dim 3.6643(3.6635) | Xent 0.9381(0.9135) | Loss 9.7044(10.1125) | Error 0.3345(0.3266) Steps 592(587.77) | Grad Norm 8.2725(6.5701) | Total Time 0.00(0.00)\n",
      "Iter 2001 | Time 56.1123(58.9928) | Bit/dim 3.6750(3.6639) | Xent 0.9094(0.9134) | Loss 9.5771(10.0965) | Error 0.3267(0.3266) Steps 598(588.08) | Grad Norm 10.5061(6.6881) | Total Time 0.00(0.00)\n",
      "Iter 2002 | Time 61.6512(59.0726) | Bit/dim 3.6517(3.6635) | Xent 0.9342(0.9140) | Loss 9.6008(10.0816) | Error 0.3326(0.3268) Steps 598(588.37) | Grad Norm 12.6446(6.8668) | Total Time 0.00(0.00)\n",
      "Iter 2003 | Time 53.1353(58.8945) | Bit/dim 3.6667(3.6636) | Xent 0.9671(0.9156) | Loss 9.4466(10.0625) | Error 0.3442(0.3273) Steps 568(587.76) | Grad Norm 16.5414(7.1571) | Total Time 0.00(0.00)\n",
      "Iter 2004 | Time 65.6127(59.0960) | Bit/dim 3.6668(3.6637) | Xent 1.0283(0.9190) | Loss 9.8241(10.0554) | Error 0.3592(0.3283) Steps 574(587.35) | Grad Norm 17.1099(7.4557) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 23.4130, Epoch Time 392.9692(388.1842), Bit/dim 3.6638(best: 3.6602), Xent 0.9315, Loss 4.1296, Error 0.3316(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 60.7184(59.1447) | Bit/dim 3.6678(3.6638) | Xent 0.9311(0.9194) | Loss 12.8657(10.1397) | Error 0.3319(0.3284) Steps 586(587.31) | Grad Norm 9.9645(7.5309) | Total Time 0.00(0.00)\n",
      "Iter 2006 | Time 55.6612(59.0402) | Bit/dim 3.6577(3.6636) | Xent 0.9459(0.9202) | Loss 9.4887(10.1202) | Error 0.3390(0.3287) Steps 586(587.27) | Grad Norm 9.0536(7.5766) | Total Time 0.00(0.00)\n",
      "Iter 2007 | Time 64.5610(59.2058) | Bit/dim 3.6603(3.6635) | Xent 0.9483(0.9210) | Loss 9.4938(10.1014) | Error 0.3427(0.3291) Steps 598(587.59) | Grad Norm 13.3409(7.7495) | Total Time 0.00(0.00)\n",
      "Iter 2008 | Time 59.0052(59.1998) | Bit/dim 3.6753(3.6639) | Xent 0.9951(0.9232) | Loss 9.6899(10.0890) | Error 0.3548(0.3299) Steps 610(588.26) | Grad Norm 20.2926(8.1258) | Total Time 0.00(0.00)\n",
      "Iter 2009 | Time 55.8576(59.0995) | Bit/dim 3.6698(3.6641) | Xent 0.9404(0.9237) | Loss 9.6203(10.0750) | Error 0.3329(0.3300) Steps 580(588.02) | Grad Norm 7.3074(8.1013) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 58.2902(59.0752) | Bit/dim 3.6587(3.6639) | Xent 0.9668(0.9250) | Loss 9.6068(10.0609) | Error 0.3509(0.3306) Steps 580(587.78) | Grad Norm 11.1064(8.1914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 22.6820, Epoch Time 392.8222(388.3233), Bit/dim 3.6629(best: 3.6602), Xent 0.9256, Loss 4.1257, Error 0.3258(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 55.9762(58.9823) | Bit/dim 3.6718(3.6642) | Xent 0.9343(0.9253) | Loss 12.8942(10.1459) | Error 0.3341(0.3307) Steps 580(587.54) | Grad Norm 8.1207(8.1893) | Total Time 0.00(0.00)\n",
      "Iter 2012 | Time 61.7409(59.0650) | Bit/dim 3.6699(3.6643) | Xent 0.9247(0.9253) | Loss 9.5227(10.1272) | Error 0.3385(0.3309) Steps 604(588.04) | Grad Norm 8.1209(8.1873) | Total Time 0.00(0.00)\n",
      "Iter 2013 | Time 59.9208(59.0907) | Bit/dim 3.6666(3.6644) | Xent 0.9239(0.9253) | Loss 9.6414(10.1127) | Error 0.3261(0.3308) Steps 598(588.34) | Grad Norm 6.9219(8.1493) | Total Time 0.00(0.00)\n",
      "Iter 2014 | Time 54.9943(58.9678) | Bit/dim 3.6705(3.6646) | Xent 0.9535(0.9261) | Loss 9.6807(10.0997) | Error 0.3431(0.3312) Steps 574(587.91) | Grad Norm 12.7293(8.2867) | Total Time 0.00(0.00)\n",
      "Iter 2015 | Time 61.8398(59.0540) | Bit/dim 3.6602(3.6644) | Xent 0.9392(0.9265) | Loss 9.5978(10.0846) | Error 0.3309(0.3312) Steps 616(588.75) | Grad Norm 11.1326(8.3721) | Total Time 0.00(0.00)\n",
      "Iter 2016 | Time 56.7231(58.9840) | Bit/dim 3.6750(3.6648) | Xent 0.9545(0.9273) | Loss 9.5188(10.0677) | Error 0.3326(0.3312) Steps 562(587.95) | Grad Norm 14.2052(8.5471) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 22.7717, Epoch Time 390.0391(388.3748), Bit/dim 3.6722(best: 3.6602), Xent 1.0090, Loss 4.1767, Error 0.3544(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 58.7200(58.9761) | Bit/dim 3.6660(3.6648) | Xent 1.0167(0.9300) | Loss 13.0391(10.1568) | Error 0.3591(0.3320) Steps 616(588.79) | Grad Norm 21.0517(8.9222) | Total Time 0.00(0.00)\n",
      "Iter 2018 | Time 55.4868(58.8714) | Bit/dim 3.6594(3.6646) | Xent 0.9282(0.9300) | Loss 9.3483(10.1325) | Error 0.3270(0.3319) Steps 592(588.88) | Grad Norm 9.2541(8.9322) | Total Time 0.00(0.00)\n",
      "Iter 2019 | Time 58.5424(58.8616) | Bit/dim 3.6635(3.6646) | Xent 0.9570(0.9308) | Loss 9.6369(10.1177) | Error 0.3380(0.3321) Steps 604(589.34) | Grad Norm 14.9339(9.1122) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 60.9720(58.9249) | Bit/dim 3.6739(3.6649) | Xent 1.0290(0.9337) | Loss 9.7268(10.1060) | Error 0.3664(0.3331) Steps 580(589.06) | Grad Norm 15.6547(9.3085) | Total Time 0.00(0.00)\n",
      "Iter 2021 | Time 55.4789(58.8215) | Bit/dim 3.6672(3.6650) | Xent 0.9567(0.9344) | Loss 9.4533(10.0864) | Error 0.3395(0.3333) Steps 604(589.51) | Grad Norm 7.8294(9.2641) | Total Time 0.00(0.00)\n",
      "Iter 2022 | Time 57.1484(58.7713) | Bit/dim 3.6785(3.6654) | Xent 0.9910(0.9361) | Loss 9.7096(10.0751) | Error 0.3612(0.3341) Steps 562(588.68) | Grad Norm 11.8518(9.3417) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 22.2612, Epoch Time 384.6100(388.2618), Bit/dim 3.6741(best: 3.6602), Xent 0.9457, Loss 4.1470, Error 0.3364(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 58.8044(58.7723) | Bit/dim 3.6793(3.6658) | Xent 0.9398(0.9362) | Loss 13.0641(10.1647) | Error 0.3373(0.3342) Steps 592(588.78) | Grad Norm 9.3344(9.3415) | Total Time 0.00(0.00)\n",
      "Iter 2024 | Time 58.0009(58.7492) | Bit/dim 3.6537(3.6654) | Xent 0.9601(0.9369) | Loss 9.5640(10.1467) | Error 0.3403(0.3344) Steps 598(589.06) | Grad Norm 9.4756(9.3455) | Total Time 0.00(0.00)\n",
      "Iter 2025 | Time 54.8649(58.6326) | Bit/dim 3.6860(3.6660) | Xent 0.9389(0.9370) | Loss 9.6480(10.1318) | Error 0.3350(0.3344) Steps 586(588.96) | Grad Norm 7.6037(9.2933) | Total Time 0.00(0.00)\n",
      "Iter 2026 | Time 59.4758(58.6579) | Bit/dim 3.6635(3.6660) | Xent 0.9457(0.9373) | Loss 9.7636(10.1207) | Error 0.3416(0.3346) Steps 610(589.60) | Grad Norm 8.8824(9.2810) | Total Time 0.00(0.00)\n",
      "Iter 2027 | Time 59.2309(58.6751) | Bit/dim 3.6577(3.6657) | Xent 0.9215(0.9368) | Loss 9.5408(10.1033) | Error 0.3326(0.3346) Steps 562(588.77) | Grad Norm 4.1475(9.1270) | Total Time 0.00(0.00)\n",
      "Iter 2028 | Time 57.5702(58.6420) | Bit/dim 3.6712(3.6659) | Xent 0.9386(0.9368) | Loss 9.5583(10.0870) | Error 0.3299(0.3344) Steps 574(588.32) | Grad Norm 7.6895(9.0838) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 22.8611, Epoch Time 386.4914(388.2087), Bit/dim 3.6661(best: 3.6602), Xent 0.9133, Loss 4.1228, Error 0.3287(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 57.9503(58.6212) | Bit/dim 3.6783(3.6662) | Xent 0.8987(0.9357) | Loss 12.4253(10.1571) | Error 0.3227(0.3341) Steps 610(588.98) | Grad Norm 5.8083(8.9856) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 61.1299(58.6965) | Bit/dim 3.6746(3.6665) | Xent 0.9271(0.9354) | Loss 9.4282(10.1352) | Error 0.3363(0.3342) Steps 616(589.79) | Grad Norm 6.7768(8.9193) | Total Time 0.00(0.00)\n",
      "Iter 2031 | Time 56.8402(58.6408) | Bit/dim 3.6596(3.6663) | Xent 0.8960(0.9342) | Loss 9.6225(10.1199) | Error 0.3214(0.3338) Steps 574(589.31) | Grad Norm 6.7037(8.8528) | Total Time 0.00(0.00)\n",
      "Iter 2032 | Time 56.0139(58.5620) | Bit/dim 3.6603(3.6661) | Xent 0.9131(0.9336) | Loss 9.4272(10.0991) | Error 0.3244(0.3335) Steps 586(589.21) | Grad Norm 7.3250(8.8070) | Total Time 0.00(0.00)\n",
      "Iter 2033 | Time 56.8661(58.5111) | Bit/dim 3.6757(3.6664) | Xent 0.9302(0.9335) | Loss 9.4565(10.0798) | Error 0.3360(0.3336) Steps 586(589.12) | Grad Norm 6.2906(8.7315) | Total Time 0.00(0.00)\n",
      "Iter 2034 | Time 58.9625(58.5247) | Bit/dim 3.6627(3.6663) | Xent 0.9154(0.9330) | Loss 9.3809(10.0588) | Error 0.3286(0.3334) Steps 604(589.56) | Grad Norm 3.6510(8.5791) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 23.2437, Epoch Time 387.3379(388.1826), Bit/dim 3.6709(best: 3.6602), Xent 0.9120, Loss 4.1268, Error 0.3259(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 55.6376(58.4380) | Bit/dim 3.6623(3.6662) | Xent 0.8905(0.9317) | Loss 12.7288(10.1389) | Error 0.3209(0.3330) Steps 580(589.28) | Grad Norm 5.2064(8.4779) | Total Time 0.00(0.00)\n",
      "Iter 2036 | Time 57.7177(58.4164) | Bit/dim 3.6570(3.6659) | Xent 0.9067(0.9309) | Loss 9.2419(10.1120) | Error 0.3269(0.3329) Steps 586(589.18) | Grad Norm 5.5056(8.3887) | Total Time 0.00(0.00)\n",
      "Iter 2037 | Time 55.2502(58.3214) | Bit/dim 3.6647(3.6659) | Xent 0.8948(0.9299) | Loss 9.5407(10.0949) | Error 0.3225(0.3325) Steps 574(588.72) | Grad Norm 5.0861(8.2897) | Total Time 0.00(0.00)\n",
      "Iter 2038 | Time 56.2992(58.2608) | Bit/dim 3.6594(3.6657) | Xent 0.8804(0.9284) | Loss 9.4557(10.0757) | Error 0.3064(0.3318) Steps 586(588.64) | Grad Norm 6.2837(8.2295) | Total Time 0.00(0.00)\n",
      "Iter 2039 | Time 58.3726(58.2641) | Bit/dim 3.6664(3.6657) | Xent 0.9029(0.9276) | Loss 9.4611(10.0573) | Error 0.3220(0.3315) Steps 616(589.46) | Grad Norm 7.9760(8.2219) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 57.4126(58.2386) | Bit/dim 3.6623(3.6656) | Xent 0.9084(0.9270) | Loss 9.6095(10.0438) | Error 0.3270(0.3313) Steps 580(589.18) | Grad Norm 4.2423(8.1025) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 22.5299, Epoch Time 379.3636(387.9180), Bit/dim 3.6629(best: 3.6602), Xent 0.8909, Loss 4.1083, Error 0.3192(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 57.0692(58.2035) | Bit/dim 3.6526(3.6652) | Xent 0.9066(0.9264) | Loss 12.8044(10.1267) | Error 0.3181(0.3309) Steps 574(588.72) | Grad Norm 2.9689(7.9485) | Total Time 0.00(0.00)\n",
      "Iter 2042 | Time 57.2633(58.1753) | Bit/dim 3.6635(3.6651) | Xent 0.8687(0.9247) | Loss 9.6306(10.1118) | Error 0.3116(0.3304) Steps 598(589.00) | Grad Norm 3.8799(7.8264) | Total Time 0.00(0.00)\n",
      "Iter 2043 | Time 56.6755(58.1303) | Bit/dim 3.6697(3.6653) | Xent 0.8940(0.9238) | Loss 9.4604(10.0922) | Error 0.3156(0.3299) Steps 592(589.09) | Grad Norm 3.3757(7.6929) | Total Time 0.00(0.00)\n",
      "Iter 2044 | Time 61.1693(58.2215) | Bit/dim 3.6711(3.6655) | Xent 0.8984(0.9230) | Loss 9.5260(10.0752) | Error 0.3247(0.3298) Steps 622(590.08) | Grad Norm 6.3156(7.6516) | Total Time 0.00(0.00)\n",
      "Iter 2045 | Time 58.4892(58.2295) | Bit/dim 3.6551(3.6651) | Xent 0.8745(0.9216) | Loss 9.5080(10.0582) | Error 0.3199(0.3295) Steps 592(590.14) | Grad Norm 2.7917(7.5058) | Total Time 0.00(0.00)\n",
      "Iter 2046 | Time 59.0362(58.2537) | Bit/dim 3.6690(3.6653) | Xent 0.8891(0.9206) | Loss 9.4242(10.0392) | Error 0.3199(0.3292) Steps 604(590.55) | Grad Norm 3.8557(7.3963) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 22.8433, Epoch Time 388.4468(387.9339), Bit/dim 3.6618(best: 3.6602), Xent 0.8859, Loss 4.1047, Error 0.3172(best: 0.3192)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 59.1542(58.2807) | Bit/dim 3.6608(3.6651) | Xent 0.8857(0.9195) | Loss 12.9688(10.1271) | Error 0.3214(0.3289) Steps 574(590.06) | Grad Norm 1.6982(7.2253) | Total Time 0.00(0.00)\n",
      "Iter 2048 | Time 63.0885(58.4250) | Bit/dim 3.6594(3.6650) | Xent 0.8824(0.9184) | Loss 9.6555(10.1130) | Error 0.3183(0.3286) Steps 598(590.29) | Grad Norm 3.0600(7.1004) | Total Time 0.00(0.00)\n",
      "Iter 2049 | Time 58.2222(58.4189) | Bit/dim 3.6715(3.6652) | Xent 0.8834(0.9174) | Loss 9.5953(10.0974) | Error 0.3147(0.3282) Steps 586(590.16) | Grad Norm 4.9289(7.0352) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 56.0782(58.3487) | Bit/dim 3.6488(3.6647) | Xent 0.8969(0.9168) | Loss 9.4782(10.0788) | Error 0.3214(0.3280) Steps 610(590.76) | Grad Norm 9.9915(7.1239) | Total Time 0.00(0.00)\n",
      "Iter 2051 | Time 57.6410(58.3274) | Bit/dim 3.6584(3.6645) | Xent 0.8987(0.9162) | Loss 9.6615(10.0663) | Error 0.3166(0.3277) Steps 610(591.34) | Grad Norm 13.4789(7.3146) | Total Time 0.00(0.00)\n",
      "Iter 2052 | Time 54.6702(58.2177) | Bit/dim 3.6771(3.6649) | Xent 0.9557(0.9174) | Loss 9.7507(10.0569) | Error 0.3370(0.3279) Steps 574(590.82) | Grad Norm 14.6994(7.5361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 22.8296, Epoch Time 387.7746(387.9291), Bit/dim 3.6677(best: 3.6602), Xent 0.9276, Loss 4.1315, Error 0.3352(best: 0.3172)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 54.8879(58.1178) | Bit/dim 3.6700(3.6650) | Xent 0.9398(0.9181) | Loss 13.2138(10.1516) | Error 0.3394(0.3283) Steps 592(590.85) | Grad Norm 11.6154(7.6585) | Total Time 0.00(0.00)\n",
      "Iter 2054 | Time 61.2431(58.2116) | Bit/dim 3.6638(3.6650) | Xent 0.9023(0.9176) | Loss 9.4503(10.1305) | Error 0.3245(0.3282) Steps 616(591.61) | Grad Norm 2.4408(7.5020) | Total Time 0.00(0.00)\n",
      "Iter 2055 | Time 51.4993(58.0102) | Bit/dim 3.6546(3.6647) | Xent 0.9167(0.9176) | Loss 9.3538(10.1072) | Error 0.3231(0.3280) Steps 556(590.54) | Grad Norm 8.1103(7.5202) | Total Time 0.00(0.00)\n",
      "Iter 2056 | Time 60.8803(58.0963) | Bit/dim 3.6487(3.6642) | Xent 0.8991(0.9170) | Loss 9.5404(10.0902) | Error 0.3239(0.3279) Steps 616(591.30) | Grad Norm 5.7273(7.4664) | Total Time 0.00(0.00)\n",
      "Iter 2057 | Time 58.8329(58.1184) | Bit/dim 3.6542(3.6639) | Xent 0.8807(0.9159) | Loss 9.6509(10.0770) | Error 0.3151(0.3275) Steps 604(591.68) | Grad Norm 5.2831(7.4009) | Total Time 0.00(0.00)\n",
      "Iter 2058 | Time 53.7882(57.9885) | Bit/dim 3.6574(3.6637) | Xent 0.8846(0.9150) | Loss 9.4486(10.0582) | Error 0.3219(0.3273) Steps 544(590.25) | Grad Norm 6.1414(7.3631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 22.7636, Epoch Time 379.8462(387.6866), Bit/dim 3.6650(best: 3.6602), Xent 0.8915, Loss 4.1108, Error 0.3180(best: 0.3172)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 56.1474(57.9333) | Bit/dim 3.6523(3.6633) | Xent 0.8953(0.9144) | Loss 12.9671(10.1455) | Error 0.3243(0.3272) Steps 568(589.59) | Grad Norm 5.9360(7.3203) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 58.0459(57.9366) | Bit/dim 3.6715(3.6636) | Xent 0.8852(0.9135) | Loss 9.5433(10.1274) | Error 0.3083(0.3267) Steps 598(589.84) | Grad Norm 8.0951(7.3436) | Total Time 0.00(0.00)\n",
      "Iter 2061 | Time 62.0176(58.0591) | Bit/dim 3.6680(3.6637) | Xent 0.8957(0.9130) | Loss 9.5418(10.1098) | Error 0.3211(0.3265) Steps 610(590.44) | Grad Norm 6.6475(7.3227) | Total Time 0.00(0.00)\n",
      "Iter 2062 | Time 58.9818(58.0868) | Bit/dim 3.6554(3.6635) | Xent 0.8974(0.9125) | Loss 9.6447(10.0959) | Error 0.3193(0.3263) Steps 610(591.03) | Grad Norm 3.3055(7.2022) | Total Time 0.00(0.00)\n",
      "Iter 2063 | Time 59.0474(58.1156) | Bit/dim 3.6567(3.6633) | Xent 0.8916(0.9119) | Loss 9.4082(10.0752) | Error 0.3239(0.3262) Steps 616(591.78) | Grad Norm 6.7502(7.1886) | Total Time 0.00(0.00)\n",
      "Iter 2064 | Time 57.1751(58.0874) | Bit/dim 3.6649(3.6633) | Xent 0.8901(0.9112) | Loss 9.4777(10.0573) | Error 0.3234(0.3261) Steps 592(591.78) | Grad Norm 9.9908(7.2727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 22.8578, Epoch Time 389.7367(387.7481), Bit/dim 3.6599(best: 3.6602), Xent 0.9014, Loss 4.1106, Error 0.3200(best: 0.3172)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 60.6901(58.1654) | Bit/dim 3.6569(3.6631) | Xent 0.9009(0.9109) | Loss 12.9654(10.1446) | Error 0.3221(0.3260) Steps 580(591.43) | Grad Norm 8.6903(7.3152) | Total Time 0.00(0.00)\n",
      "Iter 2066 | Time 59.9570(58.2192) | Bit/dim 3.6619(3.6631) | Xent 0.8949(0.9104) | Loss 9.5482(10.1267) | Error 0.3165(0.3257) Steps 604(591.81) | Grad Norm 7.0190(7.3063) | Total Time 0.00(0.00)\n",
      "Iter 2067 | Time 62.2386(58.3398) | Bit/dim 3.6661(3.6632) | Xent 0.8855(0.9097) | Loss 9.6171(10.1114) | Error 0.3174(0.3255) Steps 616(592.53) | Grad Norm 6.1844(7.2727) | Total Time 0.00(0.00)\n",
      "Iter 2068 | Time 59.9188(58.3871) | Bit/dim 3.6539(3.6629) | Xent 0.8687(0.9085) | Loss 9.5996(10.0960) | Error 0.3081(0.3250) Steps 604(592.88) | Grad Norm 3.9974(7.1744) | Total Time 0.00(0.00)\n",
      "Iter 2069 | Time 59.9857(58.4351) | Bit/dim 3.6581(3.6628) | Xent 0.8844(0.9078) | Loss 9.4385(10.0763) | Error 0.3149(0.3247) Steps 592(592.85) | Grad Norm 4.8617(7.1050) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 57.0708(58.3942) | Bit/dim 3.6645(3.6628) | Xent 0.8844(0.9071) | Loss 9.3060(10.0532) | Error 0.3210(0.3245) Steps 586(592.65) | Grad Norm 5.6216(7.0605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 22.2011, Epoch Time 398.1354(388.0597), Bit/dim 3.6588(best: 3.6599), Xent 0.8937, Loss 4.1056, Error 0.3166(best: 0.3172)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 55.7760(58.3156) | Bit/dim 3.6490(3.6624) | Xent 0.9057(0.9070) | Loss 12.8239(10.1363) | Error 0.3266(0.3246) Steps 604(592.99) | Grad Norm 5.1515(7.0033) | Total Time 0.00(0.00)\n",
      "Iter 2072 | Time 59.8997(58.3631) | Bit/dim 3.6626(3.6624) | Xent 0.8668(0.9058) | Loss 9.5902(10.1199) | Error 0.3106(0.3242) Steps 604(593.32) | Grad Norm 2.9414(6.8814) | Total Time 0.00(0.00)\n",
      "Iter 2073 | Time 57.0199(58.3228) | Bit/dim 3.6592(3.6623) | Xent 0.8963(0.9055) | Loss 9.5877(10.1040) | Error 0.3270(0.3243) Steps 592(593.28) | Grad Norm 4.7540(6.8176) | Total Time 0.00(0.00)\n",
      "Iter 2074 | Time 60.5746(58.3904) | Bit/dim 3.6537(3.6620) | Xent 0.8797(0.9047) | Loss 9.6218(10.0895) | Error 0.3131(0.3239) Steps 616(593.96) | Grad Norm 5.8745(6.7893) | Total Time 0.00(0.00)\n",
      "Iter 2075 | Time 59.2419(58.4159) | Bit/dim 3.6626(3.6621) | Xent 0.8767(0.9039) | Loss 9.2200(10.0634) | Error 0.3103(0.3235) Steps 598(594.08) | Grad Norm 5.3139(6.7450) | Total Time 0.00(0.00)\n",
      "Iter 2076 | Time 57.2395(58.3807) | Bit/dim 3.6575(3.6619) | Xent 0.8836(0.9033) | Loss 9.7221(10.0532) | Error 0.3114(0.3232) Steps 592(594.02) | Grad Norm 6.4801(6.7371) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 23.1627, Epoch Time 388.3873(388.0696), Bit/dim 3.6615(best: 3.6588), Xent 0.9089, Loss 4.1160, Error 0.3239(best: 0.3166)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 57.1194(58.3428) | Bit/dim 3.6713(3.6622) | Xent 0.8915(0.9029) | Loss 12.7250(10.1333) | Error 0.3179(0.3230) Steps 610(594.50) | Grad Norm 7.2404(6.7522) | Total Time 0.00(0.00)\n",
      "Iter 2078 | Time 61.9253(58.4503) | Bit/dim 3.6539(3.6620) | Xent 0.8920(0.9026) | Loss 9.6741(10.1196) | Error 0.3213(0.3230) Steps 616(595.14) | Grad Norm 8.3548(6.8003) | Total Time 0.00(0.00)\n",
      "Iter 2079 | Time 56.7659(58.3998) | Bit/dim 3.6615(3.6619) | Xent 0.9072(0.9027) | Loss 9.5078(10.1012) | Error 0.3244(0.3230) Steps 592(595.05) | Grad Norm 8.4269(6.8491) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 56.8348(58.3528) | Bit/dim 3.6603(3.6619) | Xent 0.9029(0.9028) | Loss 9.5005(10.0832) | Error 0.3249(0.3231) Steps 592(594.96) | Grad Norm 7.9626(6.8825) | Total Time 0.00(0.00)\n",
      "Iter 2081 | Time 58.1183(58.3458) | Bit/dim 3.6628(3.6619) | Xent 0.8916(0.9024) | Loss 9.5448(10.0670) | Error 0.3201(0.3230) Steps 574(594.33) | Grad Norm 11.7770(7.0293) | Total Time 0.00(0.00)\n",
      "Iter 2082 | Time 58.2868(58.3440) | Bit/dim 3.6588(3.6618) | Xent 0.8676(0.9014) | Loss 9.6120(10.0534) | Error 0.3090(0.3225) Steps 592(594.26) | Grad Norm 8.7575(7.0811) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 23.7021, Epoch Time 389.3246(388.1072), Bit/dim 3.6623(best: 3.6588), Xent 0.8844, Loss 4.1045, Error 0.3158(best: 0.3166)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 56.5829(58.2912) | Bit/dim 3.6539(3.6616) | Xent 0.8887(0.9010) | Loss 12.6985(10.1327) | Error 0.3144(0.3223) Steps 586(594.01) | Grad Norm 5.6904(7.0394) | Total Time 0.00(0.00)\n",
      "Iter 2084 | Time 53.4935(58.1472) | Bit/dim 3.6584(3.6615) | Xent 0.9131(0.9014) | Loss 9.4854(10.1133) | Error 0.3286(0.3225) Steps 586(593.77) | Grad Norm 9.5280(7.1141) | Total Time 0.00(0.00)\n",
      "Iter 2085 | Time 56.9058(58.1100) | Bit/dim 3.6678(3.6617) | Xent 0.9073(0.9015) | Loss 9.5724(10.0971) | Error 0.3206(0.3224) Steps 574(593.18) | Grad Norm 6.1447(7.0850) | Total Time 0.00(0.00)\n",
      "Iter 2086 | Time 57.0329(58.0777) | Bit/dim 3.6635(3.6617) | Xent 0.9034(0.9016) | Loss 9.2975(10.0731) | Error 0.3224(0.3224) Steps 586(592.96) | Grad Norm 6.3419(7.0627) | Total Time 0.00(0.00)\n",
      "Iter 2087 | Time 56.0552(58.0170) | Bit/dim 3.6601(3.6617) | Xent 0.8773(0.9009) | Loss 9.5579(10.0576) | Error 0.3180(0.3223) Steps 580(592.57) | Grad Norm 7.9319(7.0888) | Total Time 0.00(0.00)\n",
      "Iter 2088 | Time 60.1382(58.0806) | Bit/dim 3.6567(3.6615) | Xent 0.8776(0.9002) | Loss 9.4796(10.0403) | Error 0.3199(0.3222) Steps 598(592.74) | Grad Norm 7.2022(7.0922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 23.0016, Epoch Time 379.4767(387.8483), Bit/dim 3.6627(best: 3.6588), Xent 0.8946, Loss 4.1099, Error 0.3195(best: 0.3158)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 55.6787(58.0086) | Bit/dim 3.6653(3.6616) | Xent 0.8931(0.9000) | Loss 13.1381(10.1332) | Error 0.3225(0.3222) Steps 586(592.53) | Grad Norm 5.3288(7.0393) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 57.4166(57.9908) | Bit/dim 3.6469(3.6612) | Xent 0.8875(0.8996) | Loss 9.4423(10.1125) | Error 0.3241(0.3223) Steps 580(592.16) | Grad Norm 4.6100(6.9664) | Total Time 0.00(0.00)\n",
      "Iter 2091 | Time 57.7159(57.9826) | Bit/dim 3.6569(3.6611) | Xent 0.8834(0.8991) | Loss 9.4851(10.0937) | Error 0.3163(0.3221) Steps 580(591.79) | Grad Norm 2.5001(6.8324) | Total Time 0.00(0.00)\n",
      "Iter 2092 | Time 56.9861(57.9527) | Bit/dim 3.6555(3.6609) | Xent 0.8713(0.8983) | Loss 9.6336(10.0799) | Error 0.3126(0.3218) Steps 592(591.80) | Grad Norm 5.3733(6.7886) | Total Time 0.00(0.00)\n",
      "Iter 2093 | Time 56.3417(57.9044) | Bit/dim 3.6543(3.6607) | Xent 0.8787(0.8977) | Loss 9.6483(10.0669) | Error 0.3133(0.3216) Steps 580(591.45) | Grad Norm 3.8104(6.6993) | Total Time 0.00(0.00)\n",
      "Iter 2094 | Time 58.5726(57.9244) | Bit/dim 3.6624(3.6608) | Xent 0.8668(0.8967) | Loss 9.3517(10.0455) | Error 0.3057(0.3211) Steps 598(591.64) | Grad Norm 3.8964(6.6152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 22.7713, Epoch Time 381.5863(387.6604), Bit/dim 3.6587(best: 3.6588), Xent 0.8853, Loss 4.1013, Error 0.3186(best: 0.3158)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 58.2686(57.9347) | Bit/dim 3.6533(3.6605) | Xent 0.8859(0.8964) | Loss 13.2188(10.1407) | Error 0.3201(0.3211) Steps 604(592.01) | Grad Norm 7.6778(6.6471) | Total Time 0.00(0.00)\n",
      "Iter 2096 | Time 56.3950(57.8885) | Bit/dim 3.6693(3.6608) | Xent 0.8853(0.8961) | Loss 9.3817(10.1179) | Error 0.3177(0.3210) Steps 610(592.55) | Grad Norm 9.6881(6.7383) | Total Time 0.00(0.00)\n",
      "Iter 2097 | Time 58.7402(57.9141) | Bit/dim 3.6501(3.6605) | Xent 0.9137(0.8966) | Loss 9.6947(10.1052) | Error 0.3281(0.3212) Steps 592(592.54) | Grad Norm 6.9311(6.7441) | Total Time 0.00(0.00)\n",
      "Iter 2098 | Time 55.7813(57.8501) | Bit/dim 3.6496(3.6602) | Xent 0.8761(0.8960) | Loss 9.4375(10.0852) | Error 0.3186(0.3211) Steps 574(591.98) | Grad Norm 6.3753(6.7330) | Total Time 0.00(0.00)\n",
      "Iter 2099 | Time 61.5882(57.9622) | Bit/dim 3.6598(3.6601) | Xent 0.8907(0.8958) | Loss 9.6353(10.0717) | Error 0.3139(0.3209) Steps 598(592.16) | Grad Norm 6.4702(6.7251) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 58.8422(57.9886) | Bit/dim 3.6635(3.6602) | Xent 0.8762(0.8953) | Loss 9.4679(10.0536) | Error 0.3140(0.3207) Steps 580(591.80) | Grad Norm 5.6860(6.6940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 23.2399, Epoch Time 388.8866(387.6972), Bit/dim 3.6549(best: 3.6587), Xent 0.8814, Loss 4.0956, Error 0.3151(best: 0.3158)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 54.3533(57.8796) | Bit/dim 3.6693(3.6605) | Xent 0.8727(0.8946) | Loss 12.8609(10.1378) | Error 0.3125(0.3204) Steps 586(591.62) | Grad Norm 3.5847(6.6007) | Total Time 0.00(0.00)\n",
      "Iter 2102 | Time 57.6764(57.8735) | Bit/dim 3.6652(3.6607) | Xent 0.8863(0.8943) | Loss 9.3314(10.1136) | Error 0.3144(0.3203) Steps 610(592.17) | Grad Norm 5.4969(6.5676) | Total Time 0.00(0.00)\n",
      "Iter 2103 | Time 62.0866(57.9999) | Bit/dim 3.6599(3.6606) | Xent 0.8587(0.8933) | Loss 9.3860(10.0918) | Error 0.3051(0.3198) Steps 622(593.07) | Grad Norm 3.4506(6.4741) | Total Time 0.00(0.00)\n",
      "Iter 2104 | Time 58.0220(58.0005) | Bit/dim 3.6592(3.6606) | Xent 0.8979(0.8934) | Loss 9.3682(10.0701) | Error 0.3213(0.3198) Steps 616(593.76) | Grad Norm 9.4340(6.5629) | Total Time 0.00(0.00)\n",
      "Iter 2105 | Time 65.3084(58.2198) | Bit/dim 3.6701(3.6609) | Xent 0.8972(0.8935) | Loss 9.6226(10.0566) | Error 0.3201(0.3199) Steps 604(594.06) | Grad Norm 12.9952(6.7558) | Total Time 0.00(0.00)\n",
      "Iter 2106 | Time 55.4879(58.1378) | Bit/dim 3.6556(3.6607) | Xent 0.9927(0.8965) | Loss 9.4821(10.0394) | Error 0.3456(0.3206) Steps 580(593.64) | Grad Norm 19.7731(7.1464) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 23.3135, Epoch Time 392.0022(387.8264), Bit/dim 3.6654(best: 3.6549), Xent 1.0349, Loss 4.1828, Error 0.3710(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 58.9920(58.1634) | Bit/dim 3.6578(3.6606) | Xent 1.0348(0.9006) | Loss 13.0366(10.1293) | Error 0.3649(0.3220) Steps 574(593.05) | Grad Norm 21.6961(7.5828) | Total Time 0.00(0.00)\n",
      "Iter 2108 | Time 57.2254(58.1353) | Bit/dim 3.6666(3.6608) | Xent 0.9343(0.9017) | Loss 9.7673(10.1185) | Error 0.3340(0.3223) Steps 604(593.38) | Grad Norm 6.6421(7.5546) | Total Time 0.00(0.00)\n",
      "Iter 2109 | Time 55.9575(58.0700) | Bit/dim 3.6484(3.6604) | Xent 0.9390(0.9028) | Loss 9.6145(10.1033) | Error 0.3304(0.3226) Steps 604(593.70) | Grad Norm 10.8215(7.6526) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 57.4776(58.0522) | Bit/dim 3.6693(3.6607) | Xent 0.9201(0.9033) | Loss 9.6338(10.0893) | Error 0.3276(0.3227) Steps 568(592.93) | Grad Norm 10.8006(7.7471) | Total Time 0.00(0.00)\n",
      "Iter 2111 | Time 60.5109(58.1260) | Bit/dim 3.6742(3.6611) | Xent 0.9387(0.9044) | Loss 9.7876(10.0802) | Error 0.3386(0.3232) Steps 574(592.36) | Grad Norm 15.5274(7.9805) | Total Time 0.00(0.00)\n",
      "Iter 2112 | Time 54.8763(58.0285) | Bit/dim 3.6650(3.6612) | Xent 0.9333(0.9052) | Loss 9.7021(10.0689) | Error 0.3350(0.3235) Steps 592(592.35) | Grad Norm 10.0537(8.0427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 22.8541, Epoch Time 383.9394(387.7098), Bit/dim 3.6696(best: 3.6549), Xent 0.9023, Loss 4.1208, Error 0.3238(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 59.6090(58.0759) | Bit/dim 3.6702(3.6615) | Xent 0.8737(0.9043) | Loss 12.8611(10.1526) | Error 0.3136(0.3232) Steps 604(592.70) | Grad Norm 8.6519(8.0610) | Total Time 0.00(0.00)\n",
      "Iter 2114 | Time 58.5924(58.0914) | Bit/dim 3.6670(3.6617) | Xent 0.9219(0.9048) | Loss 9.5510(10.1346) | Error 0.3285(0.3234) Steps 604(593.04) | Grad Norm 7.0757(8.0314) | Total Time 0.00(0.00)\n",
      "Iter 2115 | Time 57.7321(58.0806) | Bit/dim 3.6638(3.6617) | Xent 0.8840(0.9042) | Loss 9.6822(10.1210) | Error 0.3139(0.3231) Steps 592(593.01) | Grad Norm 6.6371(7.9896) | Total Time 0.00(0.00)\n",
      "Iter 2116 | Time 60.2429(58.1455) | Bit/dim 3.6802(3.6623) | Xent 0.9620(0.9059) | Loss 9.5349(10.1034) | Error 0.3446(0.3238) Steps 562(592.08) | Grad Norm 7.9674(7.9889) | Total Time 0.00(0.00)\n",
      "Iter 2117 | Time 65.0823(58.3536) | Bit/dim 3.6516(3.6620) | Xent 0.9522(0.9073) | Loss 9.7569(10.0930) | Error 0.3445(0.3244) Steps 580(591.71) | Grad Norm 11.6229(8.0979) | Total Time 0.00(0.00)\n",
      "Iter 2118 | Time 54.8720(58.2491) | Bit/dim 3.6727(3.6623) | Xent 0.9320(0.9080) | Loss 9.7530(10.0828) | Error 0.3299(0.3245) Steps 568(591.00) | Grad Norm 13.4154(8.2574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 23.3474, Epoch Time 395.3775(387.9398), Bit/dim 3.6634(best: 3.6549), Xent 0.9655, Loss 4.1461, Error 0.3416(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 57.6900(58.2324) | Bit/dim 3.6585(3.6622) | Xent 0.9398(0.9090) | Loss 13.0837(10.1729) | Error 0.3311(0.3247) Steps 598(591.21) | Grad Norm 12.6573(8.3894) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 59.3644(58.2663) | Bit/dim 3.6696(3.6624) | Xent 0.9242(0.9094) | Loss 9.6072(10.1559) | Error 0.3289(0.3249) Steps 604(591.60) | Grad Norm 10.4915(8.4525) | Total Time 0.00(0.00)\n",
      "Iter 2121 | Time 60.1917(58.3241) | Bit/dim 3.6562(3.6622) | Xent 0.8890(0.9088) | Loss 9.7212(10.1428) | Error 0.3183(0.3247) Steps 616(592.33) | Grad Norm 5.9832(8.3784) | Total Time 0.00(0.00)\n",
      "Iter 2122 | Time 56.5579(58.2711) | Bit/dim 3.6675(3.6624) | Xent 0.9316(0.9095) | Loss 9.6589(10.1283) | Error 0.3285(0.3248) Steps 592(592.32) | Grad Norm 9.7407(8.4193) | Total Time 0.00(0.00)\n",
      "Iter 2123 | Time 56.3567(58.2137) | Bit/dim 3.6583(3.6622) | Xent 0.9149(0.9097) | Loss 9.5480(10.1109) | Error 0.3289(0.3249) Steps 592(592.31) | Grad Norm 7.2626(8.3846) | Total Time 0.00(0.00)\n",
      "Iter 2124 | Time 57.3080(58.1865) | Bit/dim 3.6637(3.6623) | Xent 0.9023(0.9095) | Loss 9.5081(10.0928) | Error 0.3254(0.3249) Steps 592(592.30) | Grad Norm 6.9612(8.3419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 23.4265, Epoch Time 387.0235(387.9123), Bit/dim 3.6599(best: 3.6549), Xent 0.8980, Loss 4.1089, Error 0.3246(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 59.1177(58.2144) | Bit/dim 3.6743(3.6626) | Xent 0.8855(0.9087) | Loss 12.7654(10.1730) | Error 0.3129(0.3246) Steps 592(592.29) | Grad Norm 6.2471(8.2790) | Total Time 0.00(0.00)\n",
      "Iter 2126 | Time 54.3568(58.0987) | Bit/dim 3.6566(3.6625) | Xent 0.9055(0.9086) | Loss 9.6060(10.1560) | Error 0.3267(0.3246) Steps 592(592.28) | Grad Norm 7.1703(8.2458) | Total Time 0.00(0.00)\n",
      "Iter 2127 | Time 61.1598(58.1905) | Bit/dim 3.6591(3.6624) | Xent 0.8783(0.9077) | Loss 9.6380(10.1405) | Error 0.3173(0.3244) Steps 610(592.81) | Grad Norm 4.2496(8.1259) | Total Time 0.00(0.00)\n",
      "Iter 2128 | Time 55.6554(58.1145) | Bit/dim 3.6543(3.6621) | Xent 0.8697(0.9066) | Loss 9.4044(10.1184) | Error 0.3116(0.3240) Steps 598(592.97) | Grad Norm 6.3029(8.0712) | Total Time 0.00(0.00)\n",
      "Iter 2129 | Time 56.4010(58.0631) | Bit/dim 3.6695(3.6623) | Xent 0.8821(0.9059) | Loss 9.5408(10.1010) | Error 0.3111(0.3236) Steps 598(593.12) | Grad Norm 4.4267(7.9619) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 62.1687(58.1862) | Bit/dim 3.6537(3.6621) | Xent 0.8957(0.9056) | Loss 9.4727(10.0822) | Error 0.3221(0.3236) Steps 586(592.91) | Grad Norm 8.1674(7.9680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 23.4701, Epoch Time 388.0099(387.9152), Bit/dim 3.6623(best: 3.6549), Xent 0.8845, Loss 4.1046, Error 0.3153(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 55.3471(58.1011) | Bit/dim 3.6651(3.6622) | Xent 0.8801(0.9048) | Loss 12.7506(10.1622) | Error 0.3144(0.3233) Steps 592(592.88) | Grad Norm 5.4992(7.8940) | Total Time 0.00(0.00)\n",
      "Iter 2132 | Time 55.8712(58.0342) | Bit/dim 3.6613(3.6621) | Xent 0.8786(0.9040) | Loss 9.5663(10.1444) | Error 0.3129(0.3230) Steps 598(593.03) | Grad Norm 6.2681(7.8452) | Total Time 0.00(0.00)\n",
      "Iter 2133 | Time 54.9795(57.9425) | Bit/dim 3.6608(3.6621) | Xent 0.8778(0.9032) | Loss 9.4762(10.1243) | Error 0.3169(0.3228) Steps 568(592.28) | Grad Norm 5.1425(7.7641) | Total Time 0.00(0.00)\n",
      "Iter 2134 | Time 55.6149(57.8727) | Bit/dim 3.6646(3.6622) | Xent 0.8636(0.9020) | Loss 9.6540(10.1102) | Error 0.3136(0.3225) Steps 598(592.45) | Grad Norm 4.8136(7.6756) | Total Time 0.00(0.00)\n",
      "Iter 2135 | Time 57.2709(57.8547) | Bit/dim 3.6567(3.6620) | Xent 0.8793(0.9013) | Loss 9.5133(10.0923) | Error 0.3156(0.3223) Steps 586(592.26) | Grad Norm 2.8504(7.5308) | Total Time 0.00(0.00)\n",
      "Iter 2136 | Time 61.4096(57.9613) | Bit/dim 3.6587(3.6619) | Xent 0.8636(0.9002) | Loss 9.5504(10.0761) | Error 0.3095(0.3219) Steps 586(592.07) | Grad Norm 3.1501(7.3994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 23.2700, Epoch Time 379.8007(387.6718), Bit/dim 3.6575(best: 3.6549), Xent 0.8806, Loss 4.0978, Error 0.3130(best: 0.3151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 57.9506(57.9610) | Bit/dim 3.6589(3.6618) | Xent 0.8794(0.8996) | Loss 12.7928(10.1576) | Error 0.3117(0.3216) Steps 610(592.61) | Grad Norm 2.6524(7.2570) | Total Time 0.00(0.00)\n",
      "Iter 2138 | Time 60.3839(58.0337) | Bit/dim 3.6489(3.6614) | Xent 0.8776(0.8989) | Loss 9.6671(10.1428) | Error 0.3063(0.3212) Steps 610(593.13) | Grad Norm 4.3202(7.1689) | Total Time 0.00(0.00)\n",
      "Iter 2139 | Time 60.7607(58.1155) | Bit/dim 3.6635(3.6615) | Xent 0.8856(0.8985) | Loss 9.5525(10.1251) | Error 0.3215(0.3212) Steps 604(593.46) | Grad Norm 5.2241(7.1106) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 58.4144(58.1244) | Bit/dim 3.6595(3.6614) | Xent 0.8669(0.8976) | Loss 9.5741(10.1086) | Error 0.3104(0.3209) Steps 616(594.13) | Grad Norm 4.0280(7.0181) | Total Time 0.00(0.00)\n",
      "Iter 2141 | Time 56.6979(58.0817) | Bit/dim 3.6610(3.6614) | Xent 0.8611(0.8965) | Loss 9.5726(10.0925) | Error 0.3097(0.3205) Steps 568(593.35) | Grad Norm 4.4962(6.9424) | Total Time 0.00(0.00)\n",
      "Iter 2142 | Time 61.5243(58.1849) | Bit/dim 3.6549(3.6612) | Xent 0.8537(0.8952) | Loss 9.5979(10.0777) | Error 0.3045(0.3201) Steps 598(593.49) | Grad Norm 5.3680(6.8952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 22.8968, Epoch Time 394.5882(387.8793), Bit/dim 3.6654(best: 3.6549), Xent 0.9054, Loss 4.1181, Error 0.3239(best: 0.3130)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 56.4532(58.1330) | Bit/dim 3.6637(3.6613) | Xent 0.8992(0.8953) | Loss 13.1030(10.1684) | Error 0.3276(0.3203) Steps 568(592.72) | Grad Norm 10.6332(7.0073) | Total Time 0.00(0.00)\n",
      "Iter 2144 | Time 59.3057(58.1682) | Bit/dim 3.6499(3.6610) | Xent 0.9024(0.8955) | Loss 9.5353(10.1495) | Error 0.3226(0.3203) Steps 598(592.88) | Grad Norm 14.2840(7.2256) | Total Time 0.00(0.00)\n",
      "Iter 2145 | Time 58.4304(58.1760) | Bit/dim 3.6672(3.6611) | Xent 0.9079(0.8959) | Loss 9.3847(10.1265) | Error 0.3219(0.3204) Steps 598(593.04) | Grad Norm 10.4844(7.3234) | Total Time 0.00(0.00)\n",
      "Iter 2146 | Time 64.2097(58.3570) | Bit/dim 3.6558(3.6610) | Xent 0.8866(0.8956) | Loss 9.4150(10.1052) | Error 0.3165(0.3203) Steps 622(593.91) | Grad Norm 5.7808(7.2771) | Total Time 0.00(0.00)\n",
      "Iter 2147 | Time 59.3856(58.3879) | Bit/dim 3.6521(3.6607) | Xent 0.8660(0.8947) | Loss 9.4666(10.0860) | Error 0.3121(0.3200) Steps 592(593.85) | Grad Norm 6.3610(7.2496) | Total Time 0.00(0.00)\n",
      "Iter 2148 | Time 60.0632(58.4382) | Bit/dim 3.6540(3.6605) | Xent 0.8977(0.8948) | Loss 9.3712(10.0646) | Error 0.3226(0.3201) Steps 580(593.43) | Grad Norm 6.5131(7.2275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 22.8016, Epoch Time 396.8667(388.1489), Bit/dim 3.6608(best: 3.6549), Xent 0.8756, Loss 4.0986, Error 0.3135(best: 0.3130)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 58.0834(58.4275) | Bit/dim 3.6711(3.6608) | Xent 0.8594(0.8938) | Loss 12.8117(10.1470) | Error 0.3071(0.3197) Steps 598(593.57) | Grad Norm 3.7561(7.1234) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 60.3409(58.4849) | Bit/dim 3.6520(3.6606) | Xent 0.8503(0.8925) | Loss 9.4372(10.1257) | Error 0.3050(0.3193) Steps 592(593.52) | Grad Norm 5.5497(7.0762) | Total Time 0.00(0.00)\n",
      "Iter 2151 | Time 61.2351(58.5674) | Bit/dim 3.6645(3.6607) | Xent 0.8702(0.8918) | Loss 9.6767(10.1122) | Error 0.3121(0.3191) Steps 610(594.02) | Grad Norm 8.0280(7.1047) | Total Time 0.00(0.00)\n",
      "Iter 2152 | Time 63.4056(58.7126) | Bit/dim 3.6558(3.6605) | Xent 0.8935(0.8918) | Loss 9.5994(10.0968) | Error 0.3170(0.3190) Steps 598(594.14) | Grad Norm 7.1751(7.1069) | Total Time 0.00(0.00)\n",
      "Iter 2153 | Time 57.2023(58.6673) | Bit/dim 3.6460(3.6601) | Xent 0.8650(0.8910) | Loss 9.5764(10.0812) | Error 0.3074(0.3187) Steps 604(594.43) | Grad Norm 6.6207(7.0923) | Total Time 0.00(0.00)\n",
      "Iter 2154 | Time 55.1189(58.5608) | Bit/dim 3.6521(3.6599) | Xent 0.8691(0.8904) | Loss 9.4893(10.0635) | Error 0.3126(0.3185) Steps 580(594.00) | Grad Norm 3.9133(6.9969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 23.3769, Epoch Time 397.5693(388.4315), Bit/dim 3.6534(best: 3.6549), Xent 0.8745, Loss 4.0907, Error 0.3116(best: 0.3130)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 58.4084(58.5562) | Bit/dim 3.6651(3.6600) | Xent 0.8675(0.8897) | Loss 12.7628(10.1444) | Error 0.3136(0.3183) Steps 586(593.76) | Grad Norm 2.4889(6.8617) | Total Time 0.00(0.00)\n",
      "Iter 2156 | Time 61.3121(58.6389) | Bit/dim 3.6509(3.6597) | Xent 0.8807(0.8894) | Loss 9.5459(10.1265) | Error 0.3130(0.3182) Steps 562(592.81) | Grad Norm 6.3924(6.8476) | Total Time 0.00(0.00)\n",
      "Iter 2157 | Time 61.2083(58.7160) | Bit/dim 3.6624(3.6598) | Xent 0.8644(0.8887) | Loss 9.4305(10.1056) | Error 0.3106(0.3179) Steps 586(592.60) | Grad Norm 6.6851(6.8427) | Total Time 0.00(0.00)\n",
      "Iter 2158 | Time 59.3773(58.7358) | Bit/dim 3.6573(3.6598) | Xent 0.8620(0.8879) | Loss 9.2623(10.0803) | Error 0.3100(0.3177) Steps 592(592.58) | Grad Norm 5.6717(6.8076) | Total Time 0.00(0.00)\n",
      "Iter 2159 | Time 59.9415(58.7720) | Bit/dim 3.6500(3.6595) | Xent 0.8511(0.8868) | Loss 9.5585(10.0646) | Error 0.3050(0.3173) Steps 598(592.75) | Grad Norm 6.2314(6.7903) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 56.8263(58.7136) | Bit/dim 3.6505(3.6592) | Xent 0.8660(0.8861) | Loss 9.5004(10.0477) | Error 0.3090(0.3171) Steps 592(592.72) | Grad Norm 7.6286(6.8154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 22.1306, Epoch Time 394.8679(388.6246), Bit/dim 3.6634(best: 3.6534), Xent 0.9034, Loss 4.1151, Error 0.3250(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 57.5959(58.6801) | Bit/dim 3.6527(3.6590) | Xent 0.8933(0.8864) | Loss 12.8682(10.1323) | Error 0.3190(0.3171) Steps 592(592.70) | Grad Norm 9.6250(6.8997) | Total Time 0.00(0.00)\n",
      "Iter 2162 | Time 58.3202(58.6693) | Bit/dim 3.6555(3.6589) | Xent 0.8905(0.8865) | Loss 9.5338(10.1144) | Error 0.3185(0.3172) Steps 574(592.14) | Grad Norm 11.0631(7.0246) | Total Time 0.00(0.00)\n",
      "Iter 2163 | Time 54.8177(58.5538) | Bit/dim 3.6571(3.6588) | Xent 0.8745(0.8861) | Loss 9.5264(10.0967) | Error 0.3191(0.3172) Steps 592(592.14) | Grad Norm 11.2691(7.1520) | Total Time 0.00(0.00)\n",
      "Iter 2164 | Time 56.7970(58.5011) | Bit/dim 3.6512(3.6586) | Xent 0.8881(0.8862) | Loss 9.3585(10.0746) | Error 0.3160(0.3172) Steps 604(592.49) | Grad Norm 7.3741(7.1586) | Total Time 0.00(0.00)\n",
      "Iter 2165 | Time 58.2976(58.4949) | Bit/dim 3.6630(3.6587) | Xent 0.8669(0.8856) | Loss 9.5609(10.0592) | Error 0.3021(0.3167) Steps 610(593.02) | Grad Norm 5.5613(7.1107) | Total Time 0.00(0.00)\n",
      "Iter 2166 | Time 59.3411(58.5203) | Bit/dim 3.6602(3.6588) | Xent 0.8658(0.8850) | Loss 9.6391(10.0466) | Error 0.3083(0.3165) Steps 598(593.17) | Grad Norm 8.4815(7.1518) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 22.4339, Epoch Time 383.1513(388.4604), Bit/dim 3.6580(best: 3.6534), Xent 0.8778, Loss 4.0970, Error 0.3149(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 56.6715(58.4649) | Bit/dim 3.6554(3.6587) | Xent 0.8883(0.8851) | Loss 13.1916(10.1409) | Error 0.3215(0.3166) Steps 574(592.59) | Grad Norm 5.3817(7.0987) | Total Time 0.00(0.00)\n",
      "Iter 2168 | Time 58.5670(58.4679) | Bit/dim 3.6493(3.6584) | Xent 0.8486(0.8840) | Loss 9.4943(10.1215) | Error 0.3053(0.3163) Steps 592(592.58) | Grad Norm 3.9034(7.0029) | Total Time 0.00(0.00)\n",
      "Iter 2169 | Time 59.9949(58.5137) | Bit/dim 3.6548(3.6583) | Xent 0.8645(0.8834) | Loss 9.3717(10.0990) | Error 0.3090(0.3161) Steps 604(592.92) | Grad Norm 7.2415(7.0100) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 62.2699(58.6264) | Bit/dim 3.6544(3.6582) | Xent 0.8626(0.8828) | Loss 9.2752(10.0743) | Error 0.3077(0.3158) Steps 616(593.61) | Grad Norm 6.9100(7.0070) | Total Time 0.00(0.00)\n",
      "Iter 2171 | Time 61.9196(58.7252) | Bit/dim 3.6479(3.6579) | Xent 0.8585(0.8821) | Loss 9.5039(10.0572) | Error 0.3060(0.3155) Steps 634(594.82) | Grad Norm 3.0099(6.8871) | Total Time 0.00(0.00)\n",
      "Iter 2172 | Time 58.2161(58.7099) | Bit/dim 3.6558(3.6578) | Xent 0.8689(0.8817) | Loss 9.6338(10.0445) | Error 0.3104(0.3154) Steps 604(595.10) | Grad Norm 6.3513(6.8710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 23.5344, Epoch Time 397.1069(388.7198), Bit/dim 3.6541(best: 3.6534), Xent 0.9094, Loss 4.1088, Error 0.3305(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 58.9986(58.7186) | Bit/dim 3.6517(3.6576) | Xent 0.8870(0.8818) | Loss 13.1215(10.1368) | Error 0.3167(0.3154) Steps 586(594.82) | Grad Norm 12.9393(7.0531) | Total Time 0.00(0.00)\n",
      "Iter 2174 | Time 58.3883(58.7087) | Bit/dim 3.6596(3.6577) | Xent 0.9415(0.8836) | Loss 9.6259(10.1215) | Error 0.3387(0.3161) Steps 562(593.84) | Grad Norm 17.0673(7.3535) | Total Time 0.00(0.00)\n",
      "Iter 2175 | Time 60.9409(58.7757) | Bit/dim 3.6717(3.6581) | Xent 0.9677(0.8862) | Loss 9.5906(10.1056) | Error 0.3421(0.3169) Steps 592(593.78) | Grad Norm 17.6215(7.6616) | Total Time 0.00(0.00)\n",
      "Iter 2176 | Time 61.6222(58.8611) | Bit/dim 3.6579(3.6581) | Xent 0.9345(0.8876) | Loss 9.4086(10.0847) | Error 0.3334(0.3174) Steps 616(594.45) | Grad Norm 13.9926(7.8515) | Total Time 0.00(0.00)\n",
      "Iter 2177 | Time 58.6030(58.8533) | Bit/dim 3.6615(3.6582) | Xent 0.9080(0.8882) | Loss 9.5507(10.0686) | Error 0.3215(0.3175) Steps 580(594.02) | Grad Norm 11.6705(7.9661) | Total Time 0.00(0.00)\n",
      "Iter 2178 | Time 57.8987(58.8247) | Bit/dim 3.6527(3.6580) | Xent 0.9385(0.8897) | Loss 9.5033(10.0517) | Error 0.3357(0.3181) Steps 592(593.96) | Grad Norm 9.2311(8.0040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 23.3970, Epoch Time 395.6905(388.9289), Bit/dim 3.6623(best: 3.6534), Xent 0.9122, Loss 4.1184, Error 0.3292(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 57.3327(58.7799) | Bit/dim 3.6604(3.6581) | Xent 0.9074(0.8903) | Loss 12.4719(10.1243) | Error 0.3264(0.3183) Steps 598(594.08) | Grad Norm 8.9179(8.0314) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 59.9676(58.8156) | Bit/dim 3.6561(3.6580) | Xent 0.9052(0.8907) | Loss 9.5482(10.1070) | Error 0.3264(0.3186) Steps 598(594.20) | Grad Norm 8.7354(8.0525) | Total Time 0.00(0.00)\n",
      "Iter 2181 | Time 61.4490(58.8946) | Bit/dim 3.6599(3.6581) | Xent 0.8837(0.8905) | Loss 9.4185(10.0863) | Error 0.3101(0.3183) Steps 634(595.39) | Grad Norm 7.9741(8.0502) | Total Time 0.00(0.00)\n",
      "Iter 2182 | Time 58.7299(58.8896) | Bit/dim 3.6544(3.6580) | Xent 0.8834(0.8903) | Loss 9.5479(10.0702) | Error 0.3123(0.3181) Steps 580(594.93) | Grad Norm 9.4053(8.0908) | Total Time 0.00(0.00)\n",
      "Iter 2183 | Time 62.1270(58.9867) | Bit/dim 3.6727(3.6584) | Xent 0.9264(0.8914) | Loss 9.3992(10.0501) | Error 0.3244(0.3183) Steps 586(594.66) | Grad Norm 14.1969(8.2740) | Total Time 0.00(0.00)\n",
      "Iter 2184 | Time 56.9255(58.9249) | Bit/dim 3.6542(3.6583) | Xent 0.8985(0.8916) | Loss 9.5722(10.0357) | Error 0.3226(0.3184) Steps 586(594.40) | Grad Norm 10.9739(8.3550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 22.9719, Epoch Time 395.4144(389.1235), Bit/dim 3.6697(best: 3.6534), Xent 0.9004, Loss 4.1199, Error 0.3232(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 60.2055(58.9633) | Bit/dim 3.6633(3.6585) | Xent 0.8756(0.8911) | Loss 12.6716(10.1148) | Error 0.3154(0.3183) Steps 586(594.15) | Grad Norm 6.4848(8.2989) | Total Time 0.00(0.00)\n",
      "Iter 2186 | Time 58.5844(58.9519) | Bit/dim 3.6570(3.6584) | Xent 0.8960(0.8912) | Loss 9.6119(10.0997) | Error 0.3220(0.3185) Steps 616(594.80) | Grad Norm 10.6891(8.3706) | Total Time 0.00(0.00)\n",
      "Iter 2187 | Time 62.4166(59.0559) | Bit/dim 3.6621(3.6585) | Xent 0.8863(0.8911) | Loss 9.4207(10.0793) | Error 0.3134(0.3183) Steps 610(595.26) | Grad Norm 5.2643(8.2774) | Total Time 0.00(0.00)\n",
      "Iter 2188 | Time 60.1707(59.0893) | Bit/dim 3.6649(3.6587) | Xent 0.8948(0.8912) | Loss 9.3830(10.0585) | Error 0.3226(0.3184) Steps 622(596.06) | Grad Norm 8.0165(8.2696) | Total Time 0.00(0.00)\n",
      "Iter 2189 | Time 56.8121(59.0210) | Bit/dim 3.6547(3.6586) | Xent 0.8779(0.8908) | Loss 9.3801(10.0381) | Error 0.3176(0.3184) Steps 586(595.76) | Grad Norm 7.1804(8.2369) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 60.8094(59.0747) | Bit/dim 3.6593(3.6586) | Xent 0.8952(0.8909) | Loss 9.4767(10.0213) | Error 0.3169(0.3184) Steps 616(596.37) | Grad Norm 12.1377(8.3539) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 22.8362, Epoch Time 397.8108(389.3841), Bit/dim 3.6595(best: 3.6534), Xent 0.8909, Loss 4.1049, Error 0.3207(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 60.1883(59.1081) | Bit/dim 3.6650(3.6588) | Xent 0.8797(0.8906) | Loss 13.1005(10.1136) | Error 0.3156(0.3183) Steps 586(596.06) | Grad Norm 8.6152(8.3618) | Total Time 0.00(0.00)\n",
      "Iter 2192 | Time 60.9519(59.1634) | Bit/dim 3.6515(3.6586) | Xent 0.8751(0.8901) | Loss 9.6299(10.0991) | Error 0.3089(0.3180) Steps 598(596.11) | Grad Norm 4.8520(8.2565) | Total Time 0.00(0.00)\n",
      "Iter 2193 | Time 63.6108(59.2968) | Bit/dim 3.6568(3.6585) | Xent 0.8496(0.8889) | Loss 9.4590(10.0799) | Error 0.3003(0.3175) Steps 598(596.17) | Grad Norm 6.5325(8.2048) | Total Time 0.00(0.00)\n",
      "Iter 2194 | Time 53.7776(59.1312) | Bit/dim 3.6596(3.6586) | Xent 0.8661(0.8882) | Loss 9.6893(10.0682) | Error 0.3153(0.3174) Steps 568(595.33) | Grad Norm 3.4030(8.0607) | Total Time 0.00(0.00)\n",
      "Iter 2195 | Time 62.1243(59.2210) | Bit/dim 3.6602(3.6586) | Xent 0.8485(0.8870) | Loss 9.6059(10.0543) | Error 0.3089(0.3171) Steps 604(595.59) | Grad Norm 5.3707(7.9800) | Total Time 0.00(0.00)\n",
      "Iter 2196 | Time 55.4218(59.1070) | Bit/dim 3.6636(3.6588) | Xent 0.8616(0.8863) | Loss 9.7106(10.0440) | Error 0.3086(0.3169) Steps 592(595.48) | Grad Norm 6.1041(7.9237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 23.0935, Epoch Time 394.9772(389.5519), Bit/dim 3.6641(best: 3.6534), Xent 0.8989, Loss 4.1135, Error 0.3223(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 61.0425(59.1651) | Bit/dim 3.6655(3.6590) | Xent 0.8922(0.8865) | Loss 12.9065(10.1299) | Error 0.3204(0.3170) Steps 592(595.37) | Grad Norm 9.3298(7.9659) | Total Time 0.00(0.00)\n",
      "Iter 2198 | Time 56.4571(59.0839) | Bit/dim 3.6604(3.6590) | Xent 0.8743(0.8861) | Loss 9.6172(10.1145) | Error 0.3107(0.3168) Steps 586(595.09) | Grad Norm 10.1026(8.0300) | Total Time 0.00(0.00)\n",
      "Iter 2199 | Time 58.9661(59.0803) | Bit/dim 3.6521(3.6588) | Xent 0.8652(0.8855) | Loss 9.5266(10.0969) | Error 0.3047(0.3164) Steps 610(595.54) | Grad Norm 6.2233(7.9758) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 58.4241(59.0607) | Bit/dim 3.6571(3.6587) | Xent 0.8885(0.8856) | Loss 9.6746(10.0842) | Error 0.3164(0.3164) Steps 580(595.07) | Grad Norm 12.4588(8.1103) | Total Time 0.00(0.00)\n",
      "Iter 2201 | Time 58.1328(59.0328) | Bit/dim 3.6538(3.6586) | Xent 0.9241(0.8867) | Loss 9.7742(10.0749) | Error 0.3215(0.3166) Steps 592(594.98) | Grad Norm 10.2609(8.1748) | Total Time 0.00(0.00)\n",
      "Iter 2202 | Time 54.9654(58.9108) | Bit/dim 3.6587(3.6586) | Xent 0.8739(0.8863) | Loss 9.2892(10.0513) | Error 0.3111(0.3164) Steps 580(594.53) | Grad Norm 6.7752(8.1328) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 23.8395, Epoch Time 387.7871(389.4990), Bit/dim 3.6580(best: 3.6534), Xent 0.9014, Loss 4.1087, Error 0.3254(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 58.2668(58.8915) | Bit/dim 3.6658(3.6588) | Xent 0.8758(0.8860) | Loss 12.8176(10.1343) | Error 0.3136(0.3163) Steps 610(595.00) | Grad Norm 8.6193(8.1474) | Total Time 0.00(0.00)\n",
      "Iter 2204 | Time 57.6885(58.8554) | Bit/dim 3.6449(3.6584) | Xent 0.8773(0.8858) | Loss 9.4332(10.1133) | Error 0.3087(0.3161) Steps 598(595.09) | Grad Norm 7.1245(8.1167) | Total Time 0.00(0.00)\n",
      "Iter 2205 | Time 60.8309(58.9146) | Bit/dim 3.6560(3.6583) | Xent 0.8746(0.8854) | Loss 9.5482(10.0963) | Error 0.3101(0.3159) Steps 592(594.99) | Grad Norm 8.4775(8.1276) | Total Time 0.00(0.00)\n",
      "Iter 2206 | Time 56.3034(58.8363) | Bit/dim 3.6588(3.6583) | Xent 0.8952(0.8857) | Loss 9.5218(10.0791) | Error 0.3187(0.3160) Steps 592(594.90) | Grad Norm 7.2839(8.1023) | Total Time 0.00(0.00)\n",
      "Iter 2207 | Time 58.0980(58.8142) | Bit/dim 3.6573(3.6583) | Xent 0.8811(0.8856) | Loss 9.5331(10.0627) | Error 0.3161(0.3160) Steps 610(595.36) | Grad Norm 7.1007(8.0722) | Total Time 0.00(0.00)\n",
      "Iter 2208 | Time 59.9595(58.8485) | Bit/dim 3.6529(3.6581) | Xent 0.8912(0.8857) | Loss 9.4466(10.0442) | Error 0.3153(0.3160) Steps 616(595.98) | Grad Norm 12.0671(8.1921) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 23.5878, Epoch Time 390.5938(389.5318), Bit/dim 3.6602(best: 3.6534), Xent 0.8886, Loss 4.1045, Error 0.3204(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 60.9773(58.9124) | Bit/dim 3.6411(3.6576) | Xent 0.8782(0.8855) | Loss 12.8344(10.1279) | Error 0.3100(0.3158) Steps 628(596.94) | Grad Norm 6.9233(8.1540) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 58.7386(58.9072) | Bit/dim 3.6628(3.6578) | Xent 0.8728(0.8851) | Loss 9.3210(10.1037) | Error 0.3119(0.3157) Steps 616(597.51) | Grad Norm 7.3148(8.1288) | Total Time 0.00(0.00)\n",
      "Iter 2211 | Time 58.4161(58.8924) | Bit/dim 3.6633(3.6580) | Xent 0.8831(0.8851) | Loss 9.5445(10.0870) | Error 0.3140(0.3157) Steps 610(597.88) | Grad Norm 12.7421(8.2672) | Total Time 0.00(0.00)\n",
      "Iter 2212 | Time 58.3682(58.8767) | Bit/dim 3.6598(3.6580) | Xent 0.8568(0.8842) | Loss 9.5773(10.0717) | Error 0.3059(0.3154) Steps 610(598.25) | Grad Norm 6.0205(8.1998) | Total Time 0.00(0.00)\n",
      "Iter 2213 | Time 55.1360(58.7645) | Bit/dim 3.6580(3.6580) | Xent 0.8686(0.8838) | Loss 9.3852(10.0511) | Error 0.3129(0.3153) Steps 604(598.42) | Grad Norm 8.6396(8.2130) | Total Time 0.00(0.00)\n",
      "Iter 2214 | Time 57.1990(58.7175) | Bit/dim 3.6655(3.6582) | Xent 0.8944(0.8841) | Loss 9.5363(10.0356) | Error 0.3200(0.3154) Steps 604(598.59) | Grad Norm 9.9082(8.2639) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 23.1890, Epoch Time 387.7107(389.4772), Bit/dim 3.6538(best: 3.6534), Xent 0.8709, Loss 4.0893, Error 0.3123(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 59.2189(58.7326) | Bit/dim 3.6527(3.6581) | Xent 0.8563(0.8832) | Loss 13.2381(10.1317) | Error 0.3071(0.3152) Steps 604(598.75) | Grad Norm 4.6155(8.1544) | Total Time 0.00(0.00)\n",
      "Iter 2216 | Time 61.8045(58.8247) | Bit/dim 3.6575(3.6581) | Xent 0.8648(0.8827) | Loss 9.4630(10.1116) | Error 0.3139(0.3151) Steps 610(599.09) | Grad Norm 7.0611(8.1216) | Total Time 0.00(0.00)\n",
      "Iter 2217 | Time 59.4218(58.8426) | Bit/dim 3.6586(3.6581) | Xent 0.8606(0.8820) | Loss 9.4762(10.0926) | Error 0.3131(0.3151) Steps 586(598.69) | Grad Norm 4.2877(8.0066) | Total Time 0.00(0.00)\n",
      "Iter 2218 | Time 60.5561(58.8940) | Bit/dim 3.6606(3.6581) | Xent 0.8709(0.8817) | Loss 9.5985(10.0778) | Error 0.3157(0.3151) Steps 610(599.03) | Grad Norm 5.7766(7.9397) | Total Time 0.00(0.00)\n",
      "Iter 2219 | Time 61.5752(58.9745) | Bit/dim 3.6380(3.6575) | Xent 0.8602(0.8811) | Loss 9.6576(10.0652) | Error 0.3116(0.3150) Steps 598(599.00) | Grad Norm 6.1486(7.8860) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 55.3101(58.8645) | Bit/dim 3.6671(3.6578) | Xent 0.8596(0.8804) | Loss 9.2598(10.0410) | Error 0.3011(0.3146) Steps 586(598.61) | Grad Norm 6.5039(7.8445) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 23.4969, Epoch Time 397.1253(389.7066), Bit/dim 3.6553(best: 3.6534), Xent 0.8732, Loss 4.0919, Error 0.3146(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 59.6264(58.8874) | Bit/dim 3.6555(3.6578) | Xent 0.8684(0.8800) | Loss 13.1015(10.1328) | Error 0.3136(0.3145) Steps 586(598.23) | Grad Norm 7.5327(7.8351) | Total Time 0.00(0.00)\n",
      "Iter 2222 | Time 56.6650(58.8207) | Bit/dim 3.6634(3.6579) | Xent 0.8696(0.8797) | Loss 9.4663(10.1128) | Error 0.3175(0.3146) Steps 586(597.87) | Grad Norm 8.7754(7.8634) | Total Time 0.00(0.00)\n",
      "Iter 2223 | Time 54.9628(58.7050) | Bit/dim 3.6489(3.6577) | Xent 0.8688(0.8794) | Loss 9.5054(10.0946) | Error 0.3101(0.3145) Steps 586(597.51) | Grad Norm 8.8191(7.8920) | Total Time 0.00(0.00)\n",
      "Iter 2224 | Time 59.7022(58.7349) | Bit/dim 3.6416(3.6572) | Xent 0.8540(0.8786) | Loss 9.4524(10.0753) | Error 0.3000(0.3141) Steps 586(597.17) | Grad Norm 8.4652(7.9092) | Total Time 0.00(0.00)\n",
      "Iter 2225 | Time 57.6952(58.7037) | Bit/dim 3.6613(3.6573) | Xent 0.8550(0.8779) | Loss 9.3530(10.0537) | Error 0.3086(0.3139) Steps 598(597.19) | Grad Norm 6.1845(7.8575) | Total Time 0.00(0.00)\n",
      "Iter 2226 | Time 58.5926(58.7004) | Bit/dim 3.6549(3.6572) | Xent 0.8402(0.8768) | Loss 9.3097(10.0313) | Error 0.2964(0.3134) Steps 598(597.22) | Grad Norm 4.7020(7.7628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 23.0947, Epoch Time 386.5264(389.6112), Bit/dim 3.6554(best: 3.6534), Xent 0.8708, Loss 4.0908, Error 0.3101(best: 0.3116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 59.6862(58.7300) | Bit/dim 3.6506(3.6570) | Xent 0.8446(0.8758) | Loss 12.4770(10.1047) | Error 0.3006(0.3130) Steps 604(597.42) | Grad Norm 5.9305(7.7078) | Total Time 0.00(0.00)\n",
      "Iter 2228 | Time 58.4756(58.7223) | Bit/dim 3.6648(3.6573) | Xent 0.8711(0.8757) | Loss 9.5493(10.0880) | Error 0.3089(0.3129) Steps 598(597.44) | Grad Norm 9.5611(7.7634) | Total Time 0.00(0.00)\n",
      "Iter 2229 | Time 56.4716(58.6548) | Bit/dim 3.6514(3.6571) | Xent 0.8624(0.8753) | Loss 9.4343(10.0684) | Error 0.3129(0.3129) Steps 586(597.09) | Grad Norm 10.1212(7.8342) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 60.4637(58.7091) | Bit/dim 3.6418(3.6566) | Xent 0.8654(0.8750) | Loss 9.5283(10.0522) | Error 0.3101(0.3128) Steps 586(596.76) | Grad Norm 6.2744(7.7874) | Total Time 0.00(0.00)\n",
      "Iter 2231 | Time 57.5100(58.6731) | Bit/dim 3.6557(3.6566) | Xent 0.8787(0.8751) | Loss 9.5587(10.0374) | Error 0.3207(0.3130) Steps 592(596.62) | Grad Norm 3.8055(7.6679) | Total Time 0.00(0.00)\n",
      "Iter 2232 | Time 59.9151(58.7104) | Bit/dim 3.6539(3.6565) | Xent 0.8195(0.8734) | Loss 9.3944(10.0181) | Error 0.2930(0.3124) Steps 610(597.02) | Grad Norm 3.7847(7.5514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 23.5031, Epoch Time 392.0133(389.6833), Bit/dim 3.6589(best: 3.6534), Xent 0.8715, Loss 4.0947, Error 0.3108(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2233 | Time 58.6729(58.7092) | Bit/dim 3.6577(3.6566) | Xent 0.8536(0.8728) | Loss 12.6092(10.0959) | Error 0.3073(0.3123) Steps 592(596.87) | Grad Norm 7.2617(7.5427) | Total Time 0.00(0.00)\n",
      "Iter 2234 | Time 60.8017(58.7720) | Bit/dim 3.6657(3.6568) | Xent 0.8738(0.8729) | Loss 9.6256(10.0818) | Error 0.3121(0.3123) Steps 592(596.72) | Grad Norm 10.0985(7.6194) | Total Time 0.00(0.00)\n",
      "Iter 2235 | Time 60.0906(58.8116) | Bit/dim 3.6472(3.6565) | Xent 0.8928(0.8735) | Loss 9.4572(10.0630) | Error 0.3214(0.3125) Steps 604(596.94) | Grad Norm 13.6632(7.8007) | Total Time 0.00(0.00)\n",
      "Iter 2236 | Time 58.3401(58.7974) | Bit/dim 3.6641(3.6568) | Xent 0.8967(0.8742) | Loss 9.4112(10.0435) | Error 0.3240(0.3129) Steps 568(596.07) | Grad Norm 13.4506(7.9702) | Total Time 0.00(0.00)\n",
      "Iter 2237 | Time 60.9018(58.8606) | Bit/dim 3.6522(3.6566) | Xent 0.8722(0.8741) | Loss 9.3270(10.0220) | Error 0.3187(0.3131) Steps 622(596.85) | Grad Norm 7.2354(7.9482) | Total Time 0.00(0.00)\n",
      "Iter 2238 | Time 59.7602(58.8875) | Bit/dim 3.6415(3.6562) | Xent 0.8511(0.8734) | Loss 9.4236(10.0040) | Error 0.3021(0.3127) Steps 592(596.70) | Grad Norm 5.8373(7.8849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 23.0095, Epoch Time 397.2970(389.9117), Bit/dim 3.6550(best: 3.6534), Xent 0.8928, Loss 4.1014, Error 0.3197(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2239 | Time 58.9239(58.8886) | Bit/dim 3.6607(3.6563) | Xent 0.8835(0.8737) | Loss 13.1023(10.0970) | Error 0.3134(0.3128) Steps 586(596.38) | Grad Norm 8.4839(7.9028) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 63.8114(59.0363) | Bit/dim 3.6455(3.6560) | Xent 0.8579(0.8733) | Loss 9.6148(10.0825) | Error 0.3061(0.3126) Steps 586(596.07) | Grad Norm 11.1875(8.0014) | Total Time 0.00(0.00)\n",
      "Iter 2241 | Time 61.0263(59.0960) | Bit/dim 3.6622(3.6562) | Xent 0.8942(0.8739) | Loss 9.5772(10.0673) | Error 0.3175(0.3127) Steps 616(596.67) | Grad Norm 15.5278(8.2272) | Total Time 0.00(0.00)\n",
      "Iter 2242 | Time 56.8775(59.0295) | Bit/dim 3.6477(3.6559) | Xent 0.8812(0.8741) | Loss 9.4972(10.0502) | Error 0.3210(0.3130) Steps 568(595.81) | Grad Norm 10.1671(8.2854) | Total Time 0.00(0.00)\n",
      "Iter 2243 | Time 59.3479(59.0390) | Bit/dim 3.6594(3.6560) | Xent 0.8776(0.8742) | Loss 9.3732(10.0299) | Error 0.3074(0.3128) Steps 586(595.52) | Grad Norm 7.2145(8.2532) | Total Time 0.00(0.00)\n",
      "Iter 2244 | Time 58.7682(59.0309) | Bit/dim 3.6540(3.6560) | Xent 0.9007(0.8750) | Loss 9.2932(10.0078) | Error 0.3219(0.3131) Steps 598(595.59) | Grad Norm 12.6146(8.3841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 23.3078, Epoch Time 398.3723(390.1655), Bit/dim 3.6544(best: 3.6534), Xent 0.9090, Loss 4.1089, Error 0.3304(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2245 | Time 60.6089(59.0782) | Bit/dim 3.6704(3.6564) | Xent 0.9035(0.8759) | Loss 12.6777(10.0879) | Error 0.3183(0.3132) Steps 616(596.20) | Grad Norm 11.6894(8.4832) | Total Time 0.00(0.00)\n",
      "Iter 2246 | Time 61.0188(59.1365) | Bit/dim 3.6511(3.6562) | Xent 0.9024(0.8767) | Loss 9.4892(10.0700) | Error 0.3166(0.3133) Steps 610(596.62) | Grad Norm 9.0977(8.5017) | Total Time 0.00(0.00)\n",
      "Iter 2247 | Time 65.0661(59.3143) | Bit/dim 3.6502(3.6561) | Xent 0.9100(0.8777) | Loss 9.6671(10.0579) | Error 0.3247(0.3137) Steps 592(596.48) | Grad Norm 10.1285(8.5505) | Total Time 0.00(0.00)\n",
      "Iter 2248 | Time 61.3548(59.3756) | Bit/dim 3.6597(3.6562) | Xent 0.8761(0.8776) | Loss 9.4127(10.0385) | Error 0.3130(0.3136) Steps 592(596.34) | Grad Norm 8.5567(8.5507) | Total Time 0.00(0.00)\n",
      "Iter 2249 | Time 57.1205(59.3079) | Bit/dim 3.6487(3.6559) | Xent 0.9303(0.8792) | Loss 9.4786(10.0217) | Error 0.3294(0.3141) Steps 568(595.49) | Grad Norm 10.2610(8.6020) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 58.8677(59.2947) | Bit/dim 3.6541(3.6559) | Xent 0.8674(0.8788) | Loss 9.4353(10.0041) | Error 0.3075(0.3139) Steps 610(595.93) | Grad Norm 4.2494(8.4714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 23.5453, Epoch Time 403.5399(390.5667), Bit/dim 3.6562(best: 3.6534), Xent 0.9312, Loss 4.1218, Error 0.3320(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2251 | Time 58.5440(59.2722) | Bit/dim 3.6645(3.6561) | Xent 0.9274(0.8803) | Loss 13.0423(10.0953) | Error 0.3289(0.3144) Steps 622(596.71) | Grad Norm 10.4112(8.5296) | Total Time 0.00(0.00)\n",
      "Iter 2252 | Time 60.7199(59.3156) | Bit/dim 3.6518(3.6560) | Xent 0.8852(0.8804) | Loss 9.4796(10.0768) | Error 0.3230(0.3146) Steps 598(596.75) | Grad Norm 9.1048(8.5468) | Total Time 0.00(0.00)\n",
      "Iter 2253 | Time 59.9454(59.3345) | Bit/dim 3.6539(3.6560) | Xent 0.8958(0.8809) | Loss 9.1826(10.0500) | Error 0.3214(0.3148) Steps 610(597.15) | Grad Norm 10.1066(8.5936) | Total Time 0.00(0.00)\n",
      "Iter 2254 | Time 58.3363(59.3046) | Bit/dim 3.6522(3.6558) | Xent 0.8636(0.8804) | Loss 9.5791(10.0359) | Error 0.3133(0.3148) Steps 598(597.17) | Grad Norm 5.0473(8.4872) | Total Time 0.00(0.00)\n",
      "Iter 2255 | Time 58.7105(59.2867) | Bit/dim 3.6660(3.6561) | Xent 0.9094(0.8813) | Loss 9.6156(10.0232) | Error 0.3255(0.3151) Steps 598(597.20) | Grad Norm 13.1526(8.6272) | Total Time 0.00(0.00)\n",
      "Iter 2256 | Time 61.1457(59.3425) | Bit/dim 3.6431(3.6558) | Xent 0.8744(0.8810) | Loss 9.5147(10.0080) | Error 0.3087(0.3149) Steps 622(597.94) | Grad Norm 7.0431(8.5797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 23.3157, Epoch Time 398.3536(390.8003), Bit/dim 3.6559(best: 3.6534), Xent 0.9013, Loss 4.1065, Error 0.3214(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2257 | Time 52.5874(59.1398) | Bit/dim 3.6522(3.6556) | Xent 0.8731(0.8808) | Loss 12.6238(10.0865) | Error 0.3173(0.3150) Steps 586(597.58) | Grad Norm 6.5168(8.5178) | Total Time 0.00(0.00)\n",
      "Iter 2258 | Time 56.1618(59.0505) | Bit/dim 3.6559(3.6557) | Xent 0.8809(0.8808) | Loss 9.5499(10.0704) | Error 0.3126(0.3149) Steps 580(597.06) | Grad Norm 5.2662(8.4202) | Total Time 0.00(0.00)\n",
      "Iter 2259 | Time 56.6589(58.9788) | Bit/dim 3.6630(3.6559) | Xent 0.8641(0.8803) | Loss 9.4415(10.0515) | Error 0.3085(0.3147) Steps 586(596.72) | Grad Norm 8.1882(8.4133) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 61.5657(59.0564) | Bit/dim 3.6384(3.6554) | Xent 0.8787(0.8803) | Loss 9.6322(10.0389) | Error 0.3131(0.3147) Steps 592(596.58) | Grad Norm 7.0705(8.3730) | Total Time 0.00(0.00)\n",
      "Iter 2261 | Time 58.5704(59.0418) | Bit/dim 3.6692(3.6558) | Xent 0.8795(0.8802) | Loss 9.4979(10.0227) | Error 0.3176(0.3148) Steps 598(596.62) | Grad Norm 8.1962(8.3677) | Total Time 0.00(0.00)\n",
      "Iter 2262 | Time 59.5883(59.0582) | Bit/dim 3.6542(3.6557) | Xent 0.8541(0.8795) | Loss 9.6494(10.0115) | Error 0.3063(0.3145) Steps 610(597.03) | Grad Norm 10.4194(8.4293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 23.1932, Epoch Time 384.1479(390.6008), Bit/dim 3.6549(best: 3.6534), Xent 0.8732, Loss 4.0916, Error 0.3141(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2263 | Time 60.8694(59.1125) | Bit/dim 3.6596(3.6558) | Xent 0.8420(0.8783) | Loss 13.1993(10.1071) | Error 0.2984(0.3140) Steps 610(597.42) | Grad Norm 7.5536(8.4030) | Total Time 0.00(0.00)\n",
      "Iter 2264 | Time 61.1827(59.1746) | Bit/dim 3.6478(3.6556) | Xent 0.8529(0.8776) | Loss 9.4781(10.0883) | Error 0.3020(0.3137) Steps 610(597.79) | Grad Norm 4.6272(8.2897) | Total Time 0.00(0.00)\n",
      "Iter 2265 | Time 57.8061(59.1336) | Bit/dim 3.6557(3.6556) | Xent 0.8472(0.8767) | Loss 9.4876(10.0702) | Error 0.3037(0.3134) Steps 574(597.08) | Grad Norm 6.3814(8.2325) | Total Time 0.00(0.00)\n",
      "Iter 2266 | Time 62.7630(59.2425) | Bit/dim 3.6512(3.6555) | Xent 0.8564(0.8761) | Loss 9.5631(10.0550) | Error 0.3043(0.3131) Steps 610(597.47) | Grad Norm 5.2139(8.1419) | Total Time 0.00(0.00)\n",
      "Iter 2267 | Time 60.6954(59.2860) | Bit/dim 3.6560(3.6555) | Xent 0.8430(0.8751) | Loss 9.4693(10.0374) | Error 0.3035(0.3128) Steps 592(597.30) | Grad Norm 4.8882(8.0443) | Total Time 0.00(0.00)\n",
      "Iter 2268 | Time 57.6775(59.2378) | Bit/dim 3.6562(3.6555) | Xent 0.8571(0.8745) | Loss 9.3855(10.0179) | Error 0.3036(0.3125) Steps 592(597.14) | Grad Norm 3.4282(7.9058) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 23.5296, Epoch Time 400.3595(390.8935), Bit/dim 3.6619(best: 3.6534), Xent 0.8631, Loss 4.0935, Error 0.3078(best: 0.3101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2269 | Time 62.4775(59.3350) | Bit/dim 3.6473(3.6553) | Xent 0.8618(0.8741) | Loss 12.7467(10.0998) | Error 0.3089(0.3124) Steps 634(598.25) | Grad Norm 3.7917(7.7824) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 56.3864(59.2465) | Bit/dim 3.6537(3.6552) | Xent 0.8451(0.8733) | Loss 9.5532(10.0834) | Error 0.3051(0.3122) Steps 610(598.60) | Grad Norm 4.2443(7.6762) | Total Time 0.00(0.00)\n",
      "Iter 2271 | Time 61.8364(59.3242) | Bit/dim 3.6644(3.6555) | Xent 0.8470(0.8725) | Loss 9.4751(10.0651) | Error 0.3083(0.3121) Steps 598(598.58) | Grad Norm 3.6140(7.5544) | Total Time 0.00(0.00)\n",
      "Iter 2272 | Time 58.9859(59.3141) | Bit/dim 3.6489(3.6553) | Xent 0.8541(0.8719) | Loss 9.3965(10.0450) | Error 0.3009(0.3117) Steps 610(598.93) | Grad Norm 7.1632(7.5426) | Total Time 0.00(0.00)\n",
      "Iter 2273 | Time 60.4978(59.3496) | Bit/dim 3.6421(3.6549) | Xent 0.8624(0.8716) | Loss 9.4976(10.0286) | Error 0.3130(0.3118) Steps 616(599.44) | Grad Norm 11.7623(7.6692) | Total Time 0.00(0.00)\n",
      "Iter 2274 | Time 66.1383(59.5532) | Bit/dim 3.6587(3.6550) | Xent 0.8882(0.8721) | Loss 9.6107(10.0161) | Error 0.3143(0.3119) Steps 634(600.48) | Grad Norm 14.8415(7.8844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 23.4554, Epoch Time 406.0856(391.3493), Bit/dim 3.6648(best: 3.6534), Xent 0.9229, Loss 4.1262, Error 0.3316(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2275 | Time 57.2845(59.4852) | Bit/dim 3.6636(3.6553) | Xent 0.9036(0.8731) | Loss 13.0623(10.1075) | Error 0.3216(0.3121) Steps 616(600.94) | Grad Norm 14.7566(8.0906) | Total Time 0.00(0.00)\n",
      "Iter 2276 | Time 60.5454(59.5170) | Bit/dim 3.6597(3.6554) | Xent 0.8685(0.8729) | Loss 9.6001(10.0923) | Error 0.3117(0.3121) Steps 592(600.67) | Grad Norm 8.6337(8.1069) | Total Time 0.00(0.00)\n",
      "Iter 2277 | Time 59.4746(59.5157) | Bit/dim 3.6435(3.6550) | Xent 0.8539(0.8724) | Loss 9.3285(10.0693) | Error 0.3086(0.3120) Steps 604(600.77) | Grad Norm 3.9338(7.9817) | Total Time 0.00(0.00)\n",
      "Iter 2278 | Time 56.0099(59.4105) | Bit/dim 3.6462(3.6548) | Xent 0.8577(0.8719) | Loss 9.5092(10.0525) | Error 0.3059(0.3118) Steps 604(600.87) | Grad Norm 10.3454(8.0526) | Total Time 0.00(0.00)\n",
      "Iter 2279 | Time 60.2874(59.4368) | Bit/dim 3.6584(3.6549) | Xent 0.9135(0.8732) | Loss 9.4990(10.0359) | Error 0.3249(0.3122) Steps 598(600.78) | Grad Norm 12.0082(8.1712) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 57.6952(59.3846) | Bit/dim 3.6584(3.6550) | Xent 0.8566(0.8727) | Loss 9.5250(10.0206) | Error 0.3059(0.3120) Steps 610(601.06) | Grad Norm 6.6655(8.1261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 23.0323, Epoch Time 390.2207(391.3154), Bit/dim 3.6543(best: 3.6534), Xent 0.8614, Loss 4.0850, Error 0.3084(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2281 | Time 60.4390(59.4162) | Bit/dim 3.6498(3.6548) | Xent 0.8425(0.8718) | Loss 12.7378(10.1021) | Error 0.3023(0.3117) Steps 610(601.33) | Grad Norm 5.1570(8.0370) | Total Time 0.00(0.00)\n",
      "Iter 2282 | Time 61.6609(59.4836) | Bit/dim 3.6519(3.6547) | Xent 0.8590(0.8714) | Loss 9.5572(10.0858) | Error 0.3110(0.3117) Steps 586(600.87) | Grad Norm 10.5452(8.1122) | Total Time 0.00(0.00)\n",
      "Iter 2283 | Time 57.1045(59.4122) | Bit/dim 3.6604(3.6549) | Xent 0.8646(0.8712) | Loss 9.5322(10.0692) | Error 0.3129(0.3118) Steps 574(600.06) | Grad Norm 9.9604(8.1677) | Total Time 0.00(0.00)\n",
      "Iter 2284 | Time 61.9374(59.4880) | Bit/dim 3.6707(3.6554) | Xent 0.8631(0.8709) | Loss 9.4103(10.0494) | Error 0.3099(0.3117) Steps 598(600.00) | Grad Norm 5.5217(8.0883) | Total Time 0.00(0.00)\n",
      "Iter 2285 | Time 61.0854(59.5359) | Bit/dim 3.6566(3.6554) | Xent 0.8694(0.8709) | Loss 9.4670(10.0319) | Error 0.3076(0.3116) Steps 586(599.58) | Grad Norm 7.8914(8.0824) | Total Time 0.00(0.00)\n",
      "Iter 2286 | Time 61.9968(59.6097) | Bit/dim 3.6567(3.6555) | Xent 0.8606(0.8706) | Loss 9.4617(10.0148) | Error 0.3044(0.3114) Steps 634(600.61) | Grad Norm 7.5105(8.0652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 23.1574, Epoch Time 403.3612(391.6768), Bit/dim 3.6589(best: 3.6534), Xent 0.8744, Loss 4.0961, Error 0.3146(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2287 | Time 59.8637(59.6173) | Bit/dim 3.6427(3.6551) | Xent 0.8822(0.8709) | Loss 12.7079(10.0956) | Error 0.3111(0.3114) Steps 616(601.07) | Grad Norm 6.8963(8.0302) | Total Time 0.00(0.00)\n",
      "Iter 2288 | Time 60.1616(59.6337) | Bit/dim 3.6580(3.6552) | Xent 0.8586(0.8706) | Loss 9.5156(10.0782) | Error 0.3096(0.3113) Steps 622(601.70) | Grad Norm 5.1977(7.9452) | Total Time 0.00(0.00)\n",
      "Iter 2289 | Time 58.3323(59.5946) | Bit/dim 3.6538(3.6551) | Xent 0.8580(0.8702) | Loss 9.3025(10.0549) | Error 0.3083(0.3112) Steps 586(601.23) | Grad Norm 8.4436(7.9602) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 64.2943(59.7356) | Bit/dim 3.6536(3.6551) | Xent 0.8323(0.8691) | Loss 9.7272(10.0451) | Error 0.2980(0.3108) Steps 598(601.13) | Grad Norm 6.2515(7.9089) | Total Time 0.00(0.00)\n",
      "Iter 2291 | Time 62.6133(59.8219) | Bit/dim 3.6481(3.6549) | Xent 0.8571(0.8687) | Loss 9.4885(10.0284) | Error 0.3054(0.3107) Steps 616(601.58) | Grad Norm 11.7411(8.0239) | Total Time 0.00(0.00)\n",
      "Iter 2292 | Time 56.5324(59.7233) | Bit/dim 3.6652(3.6552) | Xent 0.9027(0.8697) | Loss 9.6001(10.0156) | Error 0.3199(0.3109) Steps 586(601.11) | Grad Norm 15.5435(8.2495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 23.6212, Epoch Time 401.5407(391.9727), Bit/dim 3.6501(best: 3.6534), Xent 0.8742, Loss 4.0872, Error 0.3143(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2293 | Time 64.3030(59.8606) | Bit/dim 3.6561(3.6552) | Xent 0.8577(0.8694) | Loss 12.7603(10.0979) | Error 0.3067(0.3108) Steps 634(602.10) | Grad Norm 6.2565(8.1897) | Total Time 0.00(0.00)\n",
      "Iter 2294 | Time 59.1522(59.8394) | Bit/dim 3.6553(3.6552) | Xent 0.8728(0.8695) | Loss 9.6327(10.0839) | Error 0.3130(0.3109) Steps 604(602.16) | Grad Norm 9.4613(8.2278) | Total Time 0.00(0.00)\n",
      "Iter 2295 | Time 58.3959(59.7961) | Bit/dim 3.6460(3.6549) | Xent 0.8720(0.8695) | Loss 9.5225(10.0671) | Error 0.3147(0.3110) Steps 616(602.57) | Grad Norm 6.0926(8.1638) | Total Time 0.00(0.00)\n",
      "Iter 2296 | Time 60.8372(59.8273) | Bit/dim 3.6477(3.6547) | Xent 0.8586(0.8692) | Loss 9.4827(10.0496) | Error 0.3047(0.3108) Steps 622(603.15) | Grad Norm 6.1286(8.1027) | Total Time 0.00(0.00)\n",
      "Iter 2297 | Time 58.4901(59.7872) | Bit/dim 3.6539(3.6547) | Xent 0.8405(0.8683) | Loss 9.3851(10.0296) | Error 0.3020(0.3105) Steps 610(603.36) | Grad Norm 2.6653(7.9396) | Total Time 0.00(0.00)\n",
      "Iter 2298 | Time 61.6828(59.8441) | Bit/dim 3.6520(3.6546) | Xent 0.8505(0.8678) | Loss 9.6099(10.0170) | Error 0.2995(0.3102) Steps 634(604.28) | Grad Norm 6.3223(7.8911) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 23.6579, Epoch Time 402.4741(392.2878), Bit/dim 3.6545(best: 3.6501), Xent 0.8678, Loss 4.0884, Error 0.3086(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2299 | Time 62.6769(59.9291) | Bit/dim 3.6478(3.6544) | Xent 0.8366(0.8669) | Loss 12.8759(10.1028) | Error 0.2963(0.3098) Steps 598(604.09) | Grad Norm 5.6699(7.8244) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 60.7010(59.9522) | Bit/dim 3.6388(3.6539) | Xent 0.8473(0.8663) | Loss 9.2868(10.0783) | Error 0.3076(0.3097) Steps 580(603.37) | Grad Norm 5.4093(7.7520) | Total Time 0.00(0.00)\n",
      "Iter 2301 | Time 57.6663(59.8836) | Bit/dim 3.6569(3.6540) | Xent 0.8469(0.8657) | Loss 9.4288(10.0588) | Error 0.3026(0.3095) Steps 622(603.93) | Grad Norm 6.5614(7.7163) | Total Time 0.00(0.00)\n",
      "Iter 2302 | Time 58.2882(59.8358) | Bit/dim 3.6704(3.6545) | Xent 0.8684(0.8658) | Loss 9.3919(10.0388) | Error 0.3083(0.3095) Steps 598(603.75) | Grad Norm 11.8419(7.8400) | Total Time 0.00(0.00)\n",
      "Iter 2303 | Time 58.7930(59.8045) | Bit/dim 3.6563(3.6546) | Xent 0.8733(0.8660) | Loss 9.6605(10.0275) | Error 0.3134(0.3096) Steps 610(603.94) | Grad Norm 12.7947(7.9887) | Total Time 0.00(0.00)\n",
      "Iter 2304 | Time 56.3079(59.6996) | Bit/dim 3.6522(3.6545) | Xent 0.9106(0.8673) | Loss 9.5373(10.0128) | Error 0.3265(0.3101) Steps 604(603.94) | Grad Norm 15.0199(8.1996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 23.4887, Epoch Time 393.8438(392.3344), Bit/dim 3.6622(best: 3.6501), Xent 0.9679, Loss 4.1462, Error 0.3412(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2305 | Time 56.0094(59.5889) | Bit/dim 3.6533(3.6545) | Xent 0.9333(0.8693) | Loss 12.7899(10.0961) | Error 0.3260(0.3106) Steps 568(602.86) | Grad Norm 17.6698(8.4837) | Total Time 0.00(0.00)\n",
      "Iter 2306 | Time 58.2454(59.5486) | Bit/dim 3.6584(3.6546) | Xent 0.8782(0.8696) | Loss 9.4537(10.0768) | Error 0.3179(0.3108) Steps 592(602.53) | Grad Norm 8.3904(8.4809) | Total Time 0.00(0.00)\n",
      "Iter 2307 | Time 63.0054(59.6523) | Bit/dim 3.6509(3.6545) | Xent 0.8849(0.8701) | Loss 9.5422(10.0608) | Error 0.3204(0.3111) Steps 592(602.22) | Grad Norm 8.7619(8.4893) | Total Time 0.00(0.00)\n",
      "Iter 2308 | Time 60.7823(59.6862) | Bit/dim 3.6420(3.6541) | Xent 0.8938(0.8708) | Loss 9.5239(10.0447) | Error 0.3144(0.3112) Steps 592(601.91) | Grad Norm 12.6315(8.6136) | Total Time 0.00(0.00)\n",
      "Iter 2309 | Time 62.2176(59.7621) | Bit/dim 3.6690(3.6545) | Xent 0.8672(0.8707) | Loss 9.6249(10.0321) | Error 0.3113(0.3112) Steps 616(602.33) | Grad Norm 7.3253(8.5750) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 60.2518(59.7768) | Bit/dim 3.6514(3.6545) | Xent 0.8690(0.8706) | Loss 9.4295(10.0140) | Error 0.3127(0.3112) Steps 622(602.92) | Grad Norm 8.6261(8.5765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 23.6805, Epoch Time 399.8677(392.5604), Bit/dim 3.6547(best: 3.6501), Xent 0.8826, Loss 4.0961, Error 0.3133(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2311 | Time 63.2065(59.8797) | Bit/dim 3.6570(3.6545) | Xent 0.8625(0.8704) | Loss 12.8955(10.1004) | Error 0.3054(0.3111) Steps 598(602.78) | Grad Norm 9.5097(8.6045) | Total Time 0.00(0.00)\n",
      "Iter 2312 | Time 59.7049(59.8745) | Bit/dim 3.6517(3.6544) | Xent 0.8585(0.8700) | Loss 9.3320(10.0774) | Error 0.3109(0.3110) Steps 592(602.45) | Grad Norm 5.2260(8.5031) | Total Time 0.00(0.00)\n",
      "Iter 2313 | Time 62.0060(59.9384) | Bit/dim 3.6547(3.6544) | Xent 0.8347(0.8690) | Loss 9.4185(10.0576) | Error 0.3047(0.3109) Steps 598(602.32) | Grad Norm 5.6624(8.4179) | Total Time 0.00(0.00)\n",
      "Iter 2314 | Time 59.8463(59.9357) | Bit/dim 3.6640(3.6547) | Xent 0.8524(0.8685) | Loss 9.5659(10.0429) | Error 0.3037(0.3106) Steps 616(602.73) | Grad Norm 6.7879(8.3690) | Total Time 0.00(0.00)\n",
      "Iter 2315 | Time 60.2114(59.9439) | Bit/dim 3.6600(3.6549) | Xent 0.8314(0.8673) | Loss 9.3706(10.0227) | Error 0.2976(0.3103) Steps 604(602.77) | Grad Norm 5.8787(8.2943) | Total Time 0.00(0.00)\n",
      "Iter 2316 | Time 58.6036(59.9037) | Bit/dim 3.6602(3.6551) | Xent 0.8287(0.8662) | Loss 9.5604(10.0088) | Error 0.2907(0.3097) Steps 586(602.27) | Grad Norm 5.8677(8.2215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 23.1731, Epoch Time 402.8926(392.8704), Bit/dim 3.6574(best: 3.6501), Xent 0.8635, Loss 4.0892, Error 0.3116(best: 0.3078)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run2/current_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
