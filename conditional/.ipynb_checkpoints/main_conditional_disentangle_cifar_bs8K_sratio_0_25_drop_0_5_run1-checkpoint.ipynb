{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1/epoch_125_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=392, bias=True)\n",
      "  (project_class): LinearZeros(in_features=196, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 807722\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0876 | Time 70.6739(32.1940) | Bit/dim 1.2701(1.2428) | Xent 0.1337(0.1232) | Loss 1.3370(1.3044) | Error 0.0426(0.0382) Steps 428(420.35) | Grad Norm 5.4356(3.8485) | Total Time 10.00(10.00)\n",
      "Iter 0877 | Time 31.2095(32.1645) | Bit/dim 1.2479(1.2429) | Xent 0.1401(0.1237) | Loss 1.3179(1.3048) | Error 0.0424(0.0383) Steps 422(420.40) | Grad Norm 4.5923(3.8708) | Total Time 10.00(10.00)\n",
      "Iter 0878 | Time 30.3023(32.1086) | Bit/dim 1.2276(1.2424) | Xent 0.1232(0.1237) | Loss 1.2893(1.3043) | Error 0.0365(0.0383) Steps 422(420.45) | Grad Norm 2.8490(3.8401) | Total Time 10.00(10.00)\n",
      "Iter 0879 | Time 29.5015(32.0304) | Bit/dim 1.2166(1.2417) | Xent 0.1083(0.1233) | Loss 1.2708(1.3033) | Error 0.0359(0.0382) Steps 410(420.14) | Grad Norm 1.1983(3.7609) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 29.6002(31.9575) | Bit/dim 1.2107(1.2407) | Xent 0.1148(0.1230) | Loss 1.2681(1.3023) | Error 0.0333(0.0380) Steps 410(419.83) | Grad Norm 1.5775(3.6954) | Total Time 10.00(10.00)\n",
      "Iter 0881 | Time 30.1860(31.9044) | Bit/dim 1.2146(1.2400) | Xent 0.0911(0.1221) | Loss 1.2601(1.3010) | Error 0.0279(0.0377) Steps 410(419.54) | Grad Norm 2.4114(3.6569) | Total Time 10.00(10.00)\n",
      "Iter 0882 | Time 29.0467(31.8186) | Bit/dim 1.2308(1.2397) | Xent 0.0996(0.1214) | Loss 1.2806(1.3004) | Error 0.0301(0.0375) Steps 410(419.25) | Grad Norm 2.7149(3.6286) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 26.5699, Epoch Time 289.5395(236.6342), Bit/dim 1.2296(best: inf), Xent 0.0477, Loss 1.2535, Error 0.0154(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0883 | Time 30.8526(31.7897) | Bit/dim 1.2365(1.2396) | Xent 0.0941(0.1206) | Loss 1.2836(1.2999) | Error 0.0276(0.0372) Steps 410(418.97) | Grad Norm 2.7424(3.6020) | Total Time 10.00(10.00)\n",
      "Iter 0884 | Time 29.2230(31.7127) | Bit/dim 1.2342(1.2394) | Xent 0.0940(0.1198) | Loss 1.2812(1.2993) | Error 0.0290(0.0370) Steps 410(418.70) | Grad Norm 2.5710(3.5711) | Total Time 10.00(10.00)\n",
      "Iter 0885 | Time 29.4936(31.6461) | Bit/dim 1.2215(1.2389) | Xent 0.0947(0.1190) | Loss 1.2689(1.2984) | Error 0.0280(0.0367) Steps 410(418.44) | Grad Norm 2.1933(3.5298) | Total Time 10.00(10.00)\n",
      "Iter 0886 | Time 29.1103(31.5700) | Bit/dim 1.2199(1.2383) | Xent 0.0900(0.1181) | Loss 1.2649(1.2974) | Error 0.0298(0.0365) Steps 410(418.19) | Grad Norm 1.5157(3.4693) | Total Time 10.00(10.00)\n",
      "Iter 0887 | Time 29.7998(31.5169) | Bit/dim 1.2048(1.2373) | Xent 0.1071(0.1178) | Loss 1.2583(1.2962) | Error 0.0323(0.0364) Steps 410(417.94) | Grad Norm 1.0650(3.3972) | Total Time 10.00(10.00)\n",
      "Iter 0888 | Time 29.6805(31.4618) | Bit/dim 1.2093(1.2365) | Xent 0.0985(0.1172) | Loss 1.2586(1.2951) | Error 0.0300(0.0362) Steps 410(417.71) | Grad Norm 1.4823(3.3398) | Total Time 10.00(10.00)\n",
      "Iter 0889 | Time 29.6817(31.4084) | Bit/dim 1.2048(1.2355) | Xent 0.1082(0.1170) | Loss 1.2589(1.2940) | Error 0.0325(0.0361) Steps 422(417.83) | Grad Norm 2.2206(3.3062) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 15.9142, Epoch Time 235.9810(236.6146), Bit/dim 1.1996(best: 1.2296), Xent 0.0543, Loss 1.2267, Error 0.0170(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0890 | Time 31.8324(31.4211) | Bit/dim 1.2101(1.2348) | Xent 0.0995(0.1164) | Loss 1.2598(1.2930) | Error 0.0314(0.0359) Steps 422(417.96) | Grad Norm 2.4468(3.2804) | Total Time 10.00(10.00)\n",
      "Iter 0891 | Time 30.2964(31.3874) | Bit/dim 1.2014(1.2338) | Xent 0.1261(0.1167) | Loss 1.2644(1.2921) | Error 0.0396(0.0360) Steps 422(418.08) | Grad Norm 2.2466(3.2494) | Total Time 10.00(10.00)\n",
      "Iter 0892 | Time 31.6433(31.3951) | Bit/dim 1.2000(1.2327) | Xent 0.1116(0.1166) | Loss 1.2558(1.2910) | Error 0.0336(0.0360) Steps 422(418.20) | Grad Norm 1.5796(3.1993) | Total Time 10.00(10.00)\n",
      "Iter 0893 | Time 30.4963(31.3681) | Bit/dim 1.1969(1.2317) | Xent 0.1098(0.1164) | Loss 1.2518(1.2899) | Error 0.0357(0.0359) Steps 422(418.31) | Grad Norm 1.1611(3.1381) | Total Time 10.00(10.00)\n",
      "Iter 0894 | Time 30.0871(31.3297) | Bit/dim 1.2002(1.2307) | Xent 0.1086(0.1161) | Loss 1.2545(1.2888) | Error 0.0329(0.0359) Steps 410(418.06) | Grad Norm 1.2991(3.0830) | Total Time 10.00(10.00)\n",
      "Iter 0895 | Time 29.3402(31.2700) | Bit/dim 1.1933(1.2296) | Xent 0.1079(0.1159) | Loss 1.2472(1.2876) | Error 0.0321(0.0357) Steps 410(417.82) | Grad Norm 1.4654(3.0344) | Total Time 10.00(10.00)\n",
      "Iter 0896 | Time 29.2046(31.2080) | Bit/dim 1.2038(1.2288) | Xent 0.0968(0.1153) | Loss 1.2522(1.2865) | Error 0.0292(0.0355) Steps 410(417.59) | Grad Norm 1.4731(2.9876) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 15.2397, Epoch Time 240.2580(236.7239), Bit/dim 1.1942(best: 1.1996), Xent 0.0486, Loss 1.2185, Error 0.0169(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0897 | Time 28.9475(31.1402) | Bit/dim 1.2059(1.2281) | Xent 0.0987(0.1148) | Loss 1.2553(1.2856) | Error 0.0291(0.0354) Steps 410(417.36) | Grad Norm 1.3626(2.9389) | Total Time 10.00(10.00)\n",
      "Iter 0898 | Time 29.2849(31.0845) | Bit/dim 1.1978(1.2272) | Xent 0.0938(0.1142) | Loss 1.2447(1.2843) | Error 0.0270(0.0351) Steps 410(417.14) | Grad Norm 1.2202(2.8873) | Total Time 10.00(10.00)\n",
      "Iter 0899 | Time 30.2185(31.0586) | Bit/dim 1.2016(1.2265) | Xent 0.0950(0.1136) | Loss 1.2491(1.2833) | Error 0.0300(0.0350) Steps 422(417.28) | Grad Norm 1.0756(2.8329) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 30.8976(31.0537) | Bit/dim 1.1992(1.2256) | Xent 0.0927(0.1130) | Loss 1.2455(1.2821) | Error 0.0291(0.0348) Steps 410(417.06) | Grad Norm 1.0437(2.7793) | Total Time 10.00(10.00)\n",
      "Iter 0901 | Time 30.2200(31.0287) | Bit/dim 1.1944(1.2247) | Xent 0.1023(0.1127) | Loss 1.2455(1.2810) | Error 0.0317(0.0347) Steps 422(417.21) | Grad Norm 1.2414(2.7331) | Total Time 10.00(10.00)\n",
      "Iter 0902 | Time 30.1548(31.0025) | Bit/dim 1.1941(1.2238) | Xent 0.1063(0.1125) | Loss 1.2472(1.2800) | Error 0.0341(0.0347) Steps 422(417.36) | Grad Norm 1.0584(2.6829) | Total Time 10.00(10.00)\n",
      "Iter 0903 | Time 30.2411(30.9797) | Bit/dim 1.1927(1.2229) | Xent 0.1072(0.1123) | Loss 1.2463(1.2790) | Error 0.0335(0.0346) Steps 422(417.50) | Grad Norm 0.7815(2.6259) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 16.1211, Epoch Time 238.3422(236.7725), Bit/dim 1.1851(best: 1.1942), Xent 0.0501, Loss 1.2101, Error 0.0159(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0904 | Time 29.9177(30.9478) | Bit/dim 1.1901(1.2219) | Xent 0.0901(0.1116) | Loss 1.2351(1.2777) | Error 0.0271(0.0344) Steps 422(417.63) | Grad Norm 0.7145(2.5685) | Total Time 10.00(10.00)\n",
      "Iter 0905 | Time 29.7950(30.9132) | Bit/dim 1.1954(1.2211) | Xent 0.1055(0.1115) | Loss 1.2481(1.2768) | Error 0.0337(0.0344) Steps 422(417.76) | Grad Norm 0.9728(2.5206) | Total Time 10.00(10.00)\n",
      "Iter 0906 | Time 30.2860(30.8944) | Bit/dim 1.1898(1.2201) | Xent 0.1006(0.1111) | Loss 1.2401(1.2757) | Error 0.0294(0.0342) Steps 422(417.89) | Grad Norm 0.9059(2.4722) | Total Time 10.00(10.00)\n",
      "Iter 0907 | Time 30.2905(30.8763) | Bit/dim 1.1878(1.2192) | Xent 0.0994(0.1108) | Loss 1.2375(1.2746) | Error 0.0310(0.0341) Steps 422(418.01) | Grad Norm 0.9562(2.4267) | Total Time 10.00(10.00)\n",
      "Iter 0908 | Time 30.1025(30.8531) | Bit/dim 1.1902(1.2183) | Xent 0.1095(0.1107) | Loss 1.2450(1.2737) | Error 0.0325(0.0341) Steps 422(418.13) | Grad Norm 0.6466(2.3733) | Total Time 10.00(10.00)\n",
      "Iter 0909 | Time 30.0535(30.8291) | Bit/dim 1.1877(1.2174) | Xent 0.0992(0.1104) | Loss 1.2373(1.2726) | Error 0.0295(0.0340) Steps 422(418.25) | Grad Norm 0.3939(2.3139) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 29.9917(30.8040) | Bit/dim 1.1918(1.2166) | Xent 0.1047(0.1102) | Loss 1.2441(1.2717) | Error 0.0351(0.0340) Steps 422(418.36) | Grad Norm 0.2957(2.2534) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 16.0163, Epoch Time 238.5921(236.8271), Bit/dim 1.1847(best: 1.1851), Xent 0.0482, Loss 1.2089, Error 0.0158(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0911 | Time 29.7900(30.7735) | Bit/dim 1.1950(1.2160) | Xent 0.1012(0.1100) | Loss 1.2456(1.2709) | Error 0.0311(0.0339) Steps 422(418.47) | Grad Norm 0.7923(2.2095) | Total Time 10.00(10.00)\n",
      "Iter 0912 | Time 31.2183(30.7869) | Bit/dim 1.1909(1.2152) | Xent 0.0964(0.1096) | Loss 1.2391(1.2700) | Error 0.0284(0.0337) Steps 422(418.58) | Grad Norm 0.8402(2.1685) | Total Time 10.00(10.00)\n",
      "Iter 0913 | Time 30.6980(30.7842) | Bit/dim 1.1912(1.2145) | Xent 0.1058(0.1094) | Loss 1.2441(1.2692) | Error 0.0300(0.0336) Steps 422(418.68) | Grad Norm 0.7749(2.1267) | Total Time 10.00(10.00)\n",
      "Iter 0914 | Time 30.6556(30.7804) | Bit/dim 1.1938(1.2139) | Xent 0.0937(0.1090) | Loss 1.2406(1.2684) | Error 0.0299(0.0335) Steps 422(418.78) | Grad Norm 0.5002(2.0779) | Total Time 10.00(10.00)\n",
      "Iter 0915 | Time 30.6825(30.7774) | Bit/dim 1.1838(1.2130) | Xent 0.1016(0.1087) | Loss 1.2346(1.2673) | Error 0.0306(0.0334) Steps 422(418.87) | Grad Norm 0.4008(2.0276) | Total Time 10.00(10.00)\n",
      "Iter 0916 | Time 30.1285(30.7580) | Bit/dim 1.1889(1.2122) | Xent 0.0898(0.1082) | Loss 1.2338(1.2663) | Error 0.0286(0.0333) Steps 422(418.97) | Grad Norm 0.5023(1.9818) | Total Time 10.00(10.00)\n",
      "Iter 0917 | Time 29.5722(30.7224) | Bit/dim 1.1856(1.2114) | Xent 0.0942(0.1078) | Loss 1.2327(1.2653) | Error 0.0306(0.0332) Steps 422(419.06) | Grad Norm 0.6109(1.9407) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 15.8111, Epoch Time 241.1461(236.9566), Bit/dim 1.1811(best: 1.1847), Xent 0.0520, Loss 1.2071, Error 0.0177(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0918 | Time 30.8486(30.7262) | Bit/dim 1.1874(1.2107) | Xent 0.1142(0.1079) | Loss 1.2445(1.2647) | Error 0.0344(0.0332) Steps 422(419.15) | Grad Norm 0.5915(1.9002) | Total Time 10.00(10.00)\n",
      "Iter 0919 | Time 29.8533(30.7000) | Bit/dim 1.1887(1.2101) | Xent 0.1086(0.1080) | Loss 1.2430(1.2641) | Error 0.0357(0.0333) Steps 422(419.23) | Grad Norm 0.5472(1.8596) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 30.8789(30.7053) | Bit/dim 1.1815(1.2092) | Xent 0.0944(0.1076) | Loss 1.2287(1.2630) | Error 0.0289(0.0332) Steps 422(419.32) | Grad Norm 0.4523(1.8174) | Total Time 10.00(10.00)\n",
      "Iter 0921 | Time 30.2109(30.6905) | Bit/dim 1.1854(1.2085) | Xent 0.1085(0.1076) | Loss 1.2397(1.2623) | Error 0.0324(0.0332) Steps 422(419.40) | Grad Norm 0.4963(1.7778) | Total Time 10.00(10.00)\n",
      "Iter 0922 | Time 30.2119(30.6762) | Bit/dim 1.1870(1.2079) | Xent 0.0951(0.1072) | Loss 1.2345(1.2615) | Error 0.0300(0.0331) Steps 422(419.47) | Grad Norm 0.4788(1.7388) | Total Time 10.00(10.00)\n",
      "Iter 0923 | Time 30.0787(30.6582) | Bit/dim 1.1881(1.2073) | Xent 0.1011(0.1070) | Loss 1.2386(1.2608) | Error 0.0308(0.0330) Steps 422(419.55) | Grad Norm 0.3740(1.6978) | Total Time 10.00(10.00)\n",
      "Iter 0924 | Time 30.4260(30.6513) | Bit/dim 1.1901(1.2067) | Xent 0.0990(0.1068) | Loss 1.2396(1.2601) | Error 0.0325(0.0330) Steps 422(419.62) | Grad Norm 0.3105(1.6562) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 16.1133, Epoch Time 240.9571(237.0766), Bit/dim 1.1814(best: 1.1811), Xent 0.0485, Loss 1.2057, Error 0.0166(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0925 | Time 31.7533(30.6843) | Bit/dim 1.1854(1.2061) | Xent 0.0911(0.1063) | Loss 1.2310(1.2593) | Error 0.0299(0.0329) Steps 422(419.70) | Grad Norm 0.4096(1.6188) | Total Time 10.00(10.00)\n",
      "Iter 0926 | Time 31.2117(30.7001) | Bit/dim 1.1873(1.2055) | Xent 0.1001(0.1061) | Loss 1.2373(1.2586) | Error 0.0320(0.0329) Steps 422(419.76) | Grad Norm 0.5037(1.5854) | Total Time 10.00(10.00)\n",
      "Iter 0927 | Time 30.5489(30.6956) | Bit/dim 1.1895(1.2051) | Xent 0.0930(0.1057) | Loss 1.2360(1.2579) | Error 0.0308(0.0328) Steps 422(419.83) | Grad Norm 0.4700(1.5519) | Total Time 10.00(10.00)\n",
      "Iter 0928 | Time 31.7560(30.7274) | Bit/dim 1.1825(1.2044) | Xent 0.1073(0.1058) | Loss 1.2362(1.2573) | Error 0.0359(0.0329) Steps 422(419.90) | Grad Norm 0.2827(1.5138) | Total Time 10.00(10.00)\n",
      "Iter 0929 | Time 30.5645(30.7225) | Bit/dim 1.1888(1.2039) | Xent 0.0972(0.1055) | Loss 1.2374(1.2567) | Error 0.0276(0.0327) Steps 422(419.96) | Grad Norm 0.2125(1.4748) | Total Time 10.00(10.00)\n",
      "Iter 0930 | Time 30.0644(30.7028) | Bit/dim 1.1857(1.2034) | Xent 0.0978(0.1053) | Loss 1.2346(1.2560) | Error 0.0295(0.0326) Steps 422(420.02) | Grad Norm 0.3926(1.4423) | Total Time 10.00(10.00)\n",
      "Iter 0931 | Time 30.1281(30.6856) | Bit/dim 1.1820(1.2027) | Xent 0.1019(0.1052) | Loss 1.2330(1.2553) | Error 0.0301(0.0326) Steps 422(420.08) | Grad Norm 0.3742(1.4103) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 15.8496, Epoch Time 244.5190(237.2999), Bit/dim 1.1781(best: 1.1811), Xent 0.0490, Loss 1.2026, Error 0.0163(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0932 | Time 29.6755(30.6552) | Bit/dim 1.1836(1.2022) | Xent 0.1031(0.1051) | Loss 1.2352(1.2547) | Error 0.0326(0.0326) Steps 422(420.14) | Grad Norm 0.3432(1.3783) | Total Time 10.00(10.00)\n",
      "Iter 0933 | Time 30.0566(30.6373) | Bit/dim 1.1862(1.2017) | Xent 0.0947(0.1048) | Loss 1.2335(1.2541) | Error 0.0284(0.0324) Steps 422(420.19) | Grad Norm 0.2282(1.3438) | Total Time 10.00(10.00)\n",
      "Iter 0934 | Time 30.5152(30.6336) | Bit/dim 1.1800(1.2010) | Xent 0.0909(0.1044) | Loss 1.2254(1.2532) | Error 0.0282(0.0323) Steps 422(420.25) | Grad Norm 0.3901(1.3152) | Total Time 10.00(10.00)\n",
      "Iter 0935 | Time 29.7251(30.6064) | Bit/dim 1.1812(1.2004) | Xent 0.0988(0.1042) | Loss 1.2306(1.2525) | Error 0.0310(0.0323) Steps 422(420.30) | Grad Norm 0.3561(1.2864) | Total Time 10.00(10.00)\n",
      "Iter 0936 | Time 30.6110(30.6065) | Bit/dim 1.1856(1.2000) | Xent 0.1021(0.1042) | Loss 1.2367(1.2521) | Error 0.0317(0.0323) Steps 422(420.35) | Grad Norm 0.3224(1.2575) | Total Time 10.00(10.00)\n",
      "Iter 0937 | Time 30.3015(30.5974) | Bit/dim 1.1840(1.1995) | Xent 0.1061(0.1042) | Loss 1.2370(1.2516) | Error 0.0316(0.0322) Steps 422(420.40) | Grad Norm 0.3021(1.2288) | Total Time 10.00(10.00)\n",
      "Iter 0938 | Time 30.7116(30.6008) | Bit/dim 1.1858(1.1991) | Xent 0.0907(0.1038) | Loss 1.2311(1.2510) | Error 0.0266(0.0321) Steps 422(420.45) | Grad Norm 0.2607(1.1998) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 15.9108, Epoch Time 240.0572(237.3826), Bit/dim 1.1780(best: 1.1781), Xent 0.0482, Loss 1.2021, Error 0.0149(best: 0.0154)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0939 | Time 30.6941(30.6036) | Bit/dim 1.1790(1.1985) | Xent 0.0990(0.1037) | Loss 1.2285(1.2503) | Error 0.0310(0.0320) Steps 422(420.50) | Grad Norm 0.2802(1.1722) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 30.0324(30.5864) | Bit/dim 1.1864(1.1981) | Xent 0.0968(0.1035) | Loss 1.2348(1.2499) | Error 0.0286(0.0319) Steps 422(420.54) | Grad Norm 0.1863(1.1426) | Total Time 10.00(10.00)\n",
      "Iter 0941 | Time 30.0971(30.5718) | Bit/dim 1.1799(1.1976) | Xent 0.1086(0.1036) | Loss 1.2342(1.2494) | Error 0.0333(0.0320) Steps 422(420.58) | Grad Norm 0.2055(1.1145) | Total Time 10.00(10.00)\n",
      "Iter 0942 | Time 30.1294(30.5585) | Bit/dim 1.1831(1.1971) | Xent 0.0959(0.1034) | Loss 1.2310(1.2488) | Error 0.0298(0.0319) Steps 422(420.63) | Grad Norm 0.2738(1.0893) | Total Time 10.00(10.00)\n",
      "Iter 0943 | Time 30.2194(30.5483) | Bit/dim 1.1866(1.1968) | Xent 0.0947(0.1031) | Loss 1.2339(1.2484) | Error 0.0281(0.0318) Steps 422(420.67) | Grad Norm 0.2087(1.0628) | Total Time 10.00(10.00)\n",
      "Iter 0944 | Time 30.0630(30.5338) | Bit/dim 1.1813(1.1964) | Xent 0.1081(0.1033) | Loss 1.2353(1.2480) | Error 0.0340(0.0319) Steps 422(420.71) | Grad Norm 0.3779(1.0423) | Total Time 10.00(10.00)\n",
      "Iter 0945 | Time 30.2604(30.5256) | Bit/dim 1.1827(1.1960) | Xent 0.0923(0.1029) | Loss 1.2288(1.2474) | Error 0.0292(0.0318) Steps 422(420.75) | Grad Norm 0.2875(1.0197) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 15.8751, Epoch Time 239.6015(237.4492), Bit/dim 1.1764(best: 1.1780), Xent 0.0503, Loss 1.2016, Error 0.0173(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0946 | Time 30.3187(30.5194) | Bit/dim 1.1786(1.1954) | Xent 0.1078(0.1031) | Loss 1.2325(1.2470) | Error 0.0331(0.0318) Steps 422(420.78) | Grad Norm 0.2431(0.9964) | Total Time 10.00(10.00)\n",
      "Iter 0947 | Time 29.7795(30.4972) | Bit/dim 1.1801(1.1950) | Xent 0.0985(0.1030) | Loss 1.2294(1.2465) | Error 0.0305(0.0318) Steps 422(420.82) | Grad Norm 0.2945(0.9753) | Total Time 10.00(10.00)\n",
      "Iter 0948 | Time 29.8415(30.4775) | Bit/dim 1.1851(1.1947) | Xent 0.0945(0.1027) | Loss 1.2323(1.2460) | Error 0.0292(0.0317) Steps 422(420.86) | Grad Norm 0.2472(0.9535) | Total Time 10.00(10.00)\n",
      "Iter 0949 | Time 31.1180(30.4967) | Bit/dim 1.1797(1.1942) | Xent 0.1131(0.1030) | Loss 1.2362(1.2457) | Error 0.0331(0.0317) Steps 422(420.89) | Grad Norm 0.2903(0.9336) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 30.0794(30.4842) | Bit/dim 1.1859(1.1940) | Xent 0.0926(0.1027) | Loss 1.2322(1.2453) | Error 0.0291(0.0317) Steps 422(420.92) | Grad Norm 0.2189(0.9121) | Total Time 10.00(10.00)\n",
      "Iter 0951 | Time 29.9334(30.4677) | Bit/dim 1.1856(1.1937) | Xent 0.1001(0.1026) | Loss 1.2356(1.2450) | Error 0.0310(0.0316) Steps 422(420.96) | Grad Norm 0.2862(0.8933) | Total Time 10.00(10.00)\n",
      "Iter 0952 | Time 30.2231(30.4603) | Bit/dim 1.1770(1.1932) | Xent 0.0940(0.1024) | Loss 1.2240(1.2444) | Error 0.0299(0.0316) Steps 422(420.99) | Grad Norm 0.2475(0.8740) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 15.7547, Epoch Time 239.4483(237.5092), Bit/dim 1.1755(best: 1.1764), Xent 0.0462, Loss 1.1986, Error 0.0151(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0953 | Time 31.1135(30.4799) | Bit/dim 1.1801(1.1928) | Xent 0.0955(0.1022) | Loss 1.2278(1.2439) | Error 0.0296(0.0315) Steps 422(421.02) | Grad Norm 0.1615(0.8526) | Total Time 10.00(10.00)\n",
      "Iter 0954 | Time 30.7850(30.4891) | Bit/dim 1.1824(1.1925) | Xent 0.1023(0.1022) | Loss 1.2335(1.2436) | Error 0.0291(0.0315) Steps 422(421.05) | Grad Norm 0.2835(0.8355) | Total Time 10.00(10.00)\n",
      "Iter 0955 | Time 30.0590(30.4762) | Bit/dim 1.1826(1.1922) | Xent 0.1061(0.1023) | Loss 1.2357(1.2434) | Error 0.0327(0.0315) Steps 422(421.08) | Grad Norm 0.2373(0.8176) | Total Time 10.00(10.00)\n",
      "Iter 0956 | Time 29.8421(30.4572) | Bit/dim 1.1841(1.1920) | Xent 0.1011(0.1022) | Loss 1.2347(1.2431) | Error 0.0320(0.0315) Steps 422(421.10) | Grad Norm 0.2469(0.8005) | Total Time 10.00(10.00)\n",
      "Iter 0957 | Time 30.1627(30.4483) | Bit/dim 1.1791(1.1916) | Xent 0.1005(0.1022) | Loss 1.2293(1.2427) | Error 0.0323(0.0315) Steps 422(421.13) | Grad Norm 0.2095(0.7827) | Total Time 10.00(10.00)\n",
      "Iter 0958 | Time 30.0946(30.4377) | Bit/dim 1.1816(1.1913) | Xent 0.0903(0.1018) | Loss 1.2267(1.2422) | Error 0.0280(0.0314) Steps 422(421.16) | Grad Norm 0.1959(0.7651) | Total Time 10.00(10.00)\n",
      "Iter 0959 | Time 32.8592(30.5104) | Bit/dim 1.1810(1.1910) | Xent 0.0978(0.1017) | Loss 1.2299(1.2418) | Error 0.0334(0.0315) Steps 422(421.18) | Grad Norm 0.1862(0.7478) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 15.9373, Epoch Time 243.1351(237.6780), Bit/dim 1.1746(best: 1.1755), Xent 0.0474, Loss 1.1983, Error 0.0150(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0960 | Time 30.6768(30.5153) | Bit/dim 1.1836(1.1908) | Xent 0.1003(0.1017) | Loss 1.2338(1.2416) | Error 0.0340(0.0316) Steps 422(421.21) | Grad Norm 0.2929(0.7341) | Total Time 10.00(10.00)\n",
      "Iter 0961 | Time 30.0750(30.5021) | Bit/dim 1.1787(1.1904) | Xent 0.0937(0.1014) | Loss 1.2256(1.2411) | Error 0.0299(0.0315) Steps 422(421.23) | Grad Norm 0.1970(0.7180) | Total Time 10.00(10.00)\n",
      "Iter 0962 | Time 30.1695(30.4922) | Bit/dim 1.1782(1.1900) | Xent 0.0921(0.1012) | Loss 1.2243(1.2406) | Error 0.0294(0.0315) Steps 422(421.25) | Grad Norm 0.1812(0.7019) | Total Time 10.00(10.00)\n",
      "Iter 0963 | Time 30.1232(30.4811) | Bit/dim 1.1840(1.1898) | Xent 0.1044(0.1012) | Loss 1.2362(1.2405) | Error 0.0302(0.0314) Steps 422(421.28) | Grad Norm 0.2911(0.6896) | Total Time 10.00(10.00)\n",
      "Iter 0964 | Time 30.4367(30.4798) | Bit/dim 1.1798(1.1895) | Xent 0.0998(0.1012) | Loss 1.2297(1.2402) | Error 0.0286(0.0313) Steps 422(421.30) | Grad Norm 0.1879(0.6745) | Total Time 10.00(10.00)\n",
      "Iter 0965 | Time 30.4901(30.4801) | Bit/dim 1.1805(1.1893) | Xent 0.0953(0.1010) | Loss 1.2282(1.2398) | Error 0.0290(0.0313) Steps 422(421.32) | Grad Norm 0.2785(0.6626) | Total Time 10.00(10.00)\n",
      "Iter 0966 | Time 30.4157(30.4781) | Bit/dim 1.1816(1.1890) | Xent 0.1002(0.1010) | Loss 1.2318(1.2396) | Error 0.0306(0.0312) Steps 422(421.34) | Grad Norm 0.2076(0.6490) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 15.9249, Epoch Time 240.9656(237.7766), Bit/dim 1.1739(best: 1.1746), Xent 0.0499, Loss 1.1988, Error 0.0164(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0967 | Time 30.3273(30.4736) | Bit/dim 1.1790(1.1887) | Xent 0.1034(0.1011) | Loss 1.2308(1.2393) | Error 0.0320(0.0313) Steps 422(421.36) | Grad Norm 0.1632(0.6344) | Total Time 10.00(10.00)\n",
      "Iter 0968 | Time 30.0154(30.4599) | Bit/dim 1.1809(1.1885) | Xent 0.0938(0.1009) | Loss 1.2278(1.2389) | Error 0.0276(0.0312) Steps 422(421.38) | Grad Norm 0.2566(0.6231) | Total Time 10.00(10.00)\n",
      "Iter 0969 | Time 31.1711(30.4812) | Bit/dim 1.1804(1.1883) | Xent 0.0867(0.1004) | Loss 1.2237(1.2385) | Error 0.0261(0.0310) Steps 422(421.40) | Grad Norm 0.2064(0.6106) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 29.8350(30.4618) | Bit/dim 1.1815(1.1881) | Xent 0.0886(0.1001) | Loss 1.2258(1.2381) | Error 0.0278(0.0309) Steps 422(421.41) | Grad Norm 0.2130(0.5986) | Total Time 10.00(10.00)\n",
      "Iter 0971 | Time 30.4570(30.4617) | Bit/dim 1.1758(1.1877) | Xent 0.1058(0.1002) | Loss 1.2287(1.2378) | Error 0.0324(0.0310) Steps 422(421.43) | Grad Norm 0.2009(0.5867) | Total Time 10.00(10.00)\n",
      "Iter 0972 | Time 29.8965(30.4447) | Bit/dim 1.1832(1.1876) | Xent 0.0946(0.1001) | Loss 1.2305(1.2376) | Error 0.0296(0.0309) Steps 422(421.45) | Grad Norm 0.2303(0.5760) | Total Time 10.00(10.00)\n",
      "Iter 0973 | Time 29.7751(30.4246) | Bit/dim 1.1797(1.1873) | Xent 0.0928(0.0999) | Loss 1.2260(1.2373) | Error 0.0288(0.0308) Steps 422(421.47) | Grad Norm 0.2637(0.5667) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 15.8618, Epoch Time 239.4764(237.8276), Bit/dim 1.1736(best: 1.1739), Xent 0.0497, Loss 1.1985, Error 0.0160(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0974 | Time 30.6309(30.4308) | Bit/dim 1.1821(1.1872) | Xent 0.0873(0.0995) | Loss 1.2257(1.2369) | Error 0.0284(0.0308) Steps 422(421.48) | Grad Norm 0.2061(0.5558) | Total Time 10.00(10.00)\n",
      "Iter 0975 | Time 30.5618(30.4347) | Bit/dim 1.1770(1.1869) | Xent 0.0877(0.0991) | Loss 1.2209(1.2364) | Error 0.0279(0.0307) Steps 422(421.50) | Grad Norm 0.2888(0.5478) | Total Time 10.00(10.00)\n",
      "Iter 0976 | Time 29.8999(30.4187) | Bit/dim 1.1792(1.1866) | Xent 0.0925(0.0989) | Loss 1.2254(1.2361) | Error 0.0300(0.0307) Steps 422(421.51) | Grad Norm 0.2402(0.5386) | Total Time 10.00(10.00)\n",
      "Iter 0977 | Time 30.3296(30.4160) | Bit/dim 1.1815(1.1865) | Xent 0.1014(0.0990) | Loss 1.2322(1.2360) | Error 0.0291(0.0306) Steps 422(421.53) | Grad Norm 0.1745(0.5277) | Total Time 10.00(10.00)\n",
      "Iter 0978 | Time 30.1702(30.4086) | Bit/dim 1.1828(1.1864) | Xent 0.0942(0.0989) | Loss 1.2299(1.2358) | Error 0.0292(0.0306) Steps 422(421.54) | Grad Norm 0.2540(0.5195) | Total Time 10.00(10.00)\n",
      "Iter 0979 | Time 30.4173(30.4089) | Bit/dim 1.1754(1.1860) | Xent 0.1010(0.0989) | Loss 1.2259(1.2355) | Error 0.0312(0.0306) Steps 422(421.56) | Grad Norm 0.2270(0.5107) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 30.0408(30.3979) | Bit/dim 1.1815(1.1859) | Xent 0.1098(0.0992) | Loss 1.2364(1.2355) | Error 0.0344(0.0307) Steps 422(421.57) | Grad Norm 0.2246(0.5021) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 16.0796, Epoch Time 240.2288(237.8996), Bit/dim 1.1732(best: 1.1736), Xent 0.0492, Loss 1.1978, Error 0.0169(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0981 | Time 30.2103(30.3922) | Bit/dim 1.1800(1.1857) | Xent 0.0993(0.0992) | Loss 1.2297(1.2354) | Error 0.0309(0.0307) Steps 422(421.58) | Grad Norm 0.2249(0.4938) | Total Time 10.00(10.00)\n",
      "Iter 0982 | Time 30.9109(30.4078) | Bit/dim 1.1792(1.1855) | Xent 0.0929(0.0991) | Loss 1.2257(1.2351) | Error 0.0288(0.0307) Steps 422(421.59) | Grad Norm 0.1771(0.4843) | Total Time 10.00(10.00)\n",
      "Iter 0983 | Time 30.3337(30.4056) | Bit/dim 1.1750(1.1852) | Xent 0.0981(0.0990) | Loss 1.2240(1.2347) | Error 0.0309(0.0307) Steps 422(421.61) | Grad Norm 0.2517(0.4773) | Total Time 10.00(10.00)\n",
      "Iter 0984 | Time 30.0745(30.3956) | Bit/dim 1.1829(1.1851) | Xent 0.0875(0.0987) | Loss 1.2267(1.2345) | Error 0.0265(0.0305) Steps 422(421.62) | Grad Norm 0.1866(0.4686) | Total Time 10.00(10.00)\n",
      "Iter 0985 | Time 29.9375(30.3819) | Bit/dim 1.1795(1.1850) | Xent 0.0936(0.0985) | Loss 1.2263(1.2342) | Error 0.0300(0.0305) Steps 422(421.63) | Grad Norm 0.2214(0.4612) | Total Time 10.00(10.00)\n",
      "Iter 0986 | Time 30.4177(30.3830) | Bit/dim 1.1767(1.1847) | Xent 0.1138(0.0990) | Loss 1.2336(1.2342) | Error 0.0333(0.0306) Steps 422(421.64) | Grad Norm 0.3244(0.4571) | Total Time 10.00(10.00)\n",
      "Iter 0987 | Time 30.1779(30.3768) | Bit/dim 1.1811(1.1846) | Xent 0.1041(0.0991) | Loss 1.2332(1.2342) | Error 0.0327(0.0307) Steps 422(421.65) | Grad Norm 0.2093(0.4496) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 15.6415, Epoch Time 240.1560(237.9673), Bit/dim 1.1729(best: 1.1732), Xent 0.0477, Loss 1.1967, Error 0.0157(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0988 | Time 29.7908(30.3592) | Bit/dim 1.1782(1.1844) | Xent 0.1075(0.0994) | Loss 1.2320(1.2341) | Error 0.0334(0.0308) Steps 422(421.66) | Grad Norm 0.2297(0.4430) | Total Time 10.00(10.00)\n",
      "Iter 0989 | Time 29.8070(30.3427) | Bit/dim 1.1775(1.1842) | Xent 0.1053(0.0996) | Loss 1.2302(1.2340) | Error 0.0295(0.0307) Steps 422(421.67) | Grad Norm 0.2334(0.4368) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 30.2081(30.3386) | Bit/dim 1.1801(1.1841) | Xent 0.1032(0.0997) | Loss 1.2318(1.2339) | Error 0.0321(0.0308) Steps 422(421.68) | Grad Norm 0.2815(0.4321) | Total Time 10.00(10.00)\n",
      "Iter 0991 | Time 30.0942(30.3313) | Bit/dim 1.1750(1.1838) | Xent 0.0909(0.0994) | Loss 1.2204(1.2335) | Error 0.0285(0.0307) Steps 422(421.69) | Grad Norm 0.3048(0.4283) | Total Time 10.00(10.00)\n",
      "Iter 0992 | Time 30.4665(30.3354) | Bit/dim 1.1847(1.1839) | Xent 0.0975(0.0994) | Loss 1.2335(1.2335) | Error 0.0305(0.0307) Steps 422(421.70) | Grad Norm 0.2674(0.4235) | Total Time 10.00(10.00)\n",
      "Iter 0993 | Time 30.4393(30.3385) | Bit/dim 1.1766(1.1836) | Xent 0.0960(0.0993) | Loss 1.2246(1.2333) | Error 0.0282(0.0306) Steps 422(421.71) | Grad Norm 0.4552(0.4244) | Total Time 10.00(10.00)\n",
      "Iter 0994 | Time 30.1009(30.3313) | Bit/dim 1.1789(1.1835) | Xent 0.1030(0.0994) | Loss 1.2304(1.2332) | Error 0.0324(0.0307) Steps 422(421.72) | Grad Norm 0.1847(0.4172) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 15.9172, Epoch Time 238.9651(237.9972), Bit/dim 1.1731(best: 1.1729), Xent 0.0487, Loss 1.1975, Error 0.0166(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0995 | Time 29.7867(30.3150) | Bit/dim 1.1815(1.1834) | Xent 0.0967(0.0993) | Loss 1.2299(1.2331) | Error 0.0295(0.0306) Steps 422(421.73) | Grad Norm 0.1903(0.4104) | Total Time 10.00(10.00)\n",
      "Iter 0996 | Time 30.2130(30.3119) | Bit/dim 1.1818(1.1834) | Xent 0.1042(0.0994) | Loss 1.2340(1.2331) | Error 0.0315(0.0307) Steps 422(421.73) | Grad Norm 0.2267(0.4049) | Total Time 10.00(10.00)\n",
      "Iter 0997 | Time 29.6566(30.2923) | Bit/dim 1.1775(1.1832) | Xent 0.0935(0.0993) | Loss 1.2242(1.2328) | Error 0.0281(0.0306) Steps 422(421.74) | Grad Norm 0.3639(0.4037) | Total Time 10.00(10.00)\n",
      "Iter 0998 | Time 30.1683(30.2886) | Bit/dim 1.1792(1.1831) | Xent 0.0998(0.0993) | Loss 1.2291(1.2327) | Error 0.0315(0.0306) Steps 422(421.75) | Grad Norm 0.2465(0.3989) | Total Time 10.00(10.00)\n",
      "Iter 0999 | Time 30.1608(30.2847) | Bit/dim 1.1790(1.1830) | Xent 0.1062(0.0995) | Loss 1.2321(1.2327) | Error 0.0335(0.0307) Steps 422(421.76) | Grad Norm 0.2463(0.3944) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 30.9633(30.3051) | Bit/dim 1.1777(1.1828) | Xent 0.1072(0.0997) | Loss 1.2313(1.2327) | Error 0.0351(0.0308) Steps 428(421.95) | Grad Norm 0.2395(0.3897) | Total Time 10.00(10.00)\n",
      "Iter 1001 | Time 32.2408(30.3632) | Bit/dim 1.1783(1.1827) | Xent 0.1003(0.0997) | Loss 1.2285(1.2325) | Error 0.0337(0.0309) Steps 428(422.13) | Grad Norm 0.2724(0.3862) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 16.2134, Epoch Time 241.6830(238.1078), Bit/dim 1.1734(best: 1.1729), Xent 0.0474, Loss 1.1971, Error 0.0156(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1002 | Time 31.5683(30.3993) | Bit/dim 1.1801(1.1826) | Xent 0.1007(0.0998) | Loss 1.2305(1.2325) | Error 0.0319(0.0309) Steps 428(422.30) | Grad Norm 0.2451(0.3820) | Total Time 10.00(10.00)\n",
      "Iter 1003 | Time 32.0118(30.4477) | Bit/dim 1.1798(1.1825) | Xent 0.1067(0.1000) | Loss 1.2332(1.2325) | Error 0.0305(0.0309) Steps 428(422.47) | Grad Norm 0.2737(0.3787) | Total Time 10.00(10.00)\n",
      "Iter 1004 | Time 31.3246(30.4740) | Bit/dim 1.1817(1.1825) | Xent 0.0943(0.0998) | Loss 1.2288(1.2324) | Error 0.0278(0.0308) Steps 428(422.64) | Grad Norm 0.2384(0.3745) | Total Time 10.00(10.00)\n",
      "Iter 1005 | Time 30.9721(30.4889) | Bit/dim 1.1738(1.1822) | Xent 0.0966(0.0997) | Loss 1.2221(1.2321) | Error 0.0314(0.0308) Steps 428(422.80) | Grad Norm 0.2949(0.3721) | Total Time 10.00(10.00)\n",
      "Iter 1006 | Time 31.0053(30.5044) | Bit/dim 1.1785(1.1821) | Xent 0.0901(0.0994) | Loss 1.2236(1.2318) | Error 0.0295(0.0308) Steps 428(422.96) | Grad Norm 0.2139(0.3674) | Total Time 10.00(10.00)\n",
      "Iter 1007 | Time 32.1095(30.5526) | Bit/dim 1.1808(1.1821) | Xent 0.0982(0.0994) | Loss 1.2299(1.2318) | Error 0.0299(0.0308) Steps 428(423.11) | Grad Norm 0.2861(0.3649) | Total Time 10.00(10.00)\n",
      "Iter 1008 | Time 30.8768(30.5623) | Bit/dim 1.1773(1.1819) | Xent 0.0980(0.0993) | Loss 1.2263(1.2316) | Error 0.0296(0.0307) Steps 428(423.25) | Grad Norm 0.3366(0.3641) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 16.2272, Epoch Time 248.6039(238.4227), Bit/dim 1.1722(best: 1.1729), Xent 0.0461, Loss 1.1952, Error 0.0151(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1009 | Time 32.0270(30.6063) | Bit/dim 1.1795(1.1819) | Xent 0.1014(0.0994) | Loss 1.2302(1.2316) | Error 0.0320(0.0308) Steps 428(423.40) | Grad Norm 0.2077(0.3594) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 31.2028(30.6241) | Bit/dim 1.1843(1.1819) | Xent 0.1055(0.0996) | Loss 1.2371(1.2317) | Error 0.0320(0.0308) Steps 428(423.54) | Grad Norm 0.3167(0.3581) | Total Time 10.00(10.00)\n",
      "Iter 1011 | Time 31.6497(30.6549) | Bit/dim 1.1811(1.1819) | Xent 0.1038(0.0997) | Loss 1.2330(1.2318) | Error 0.0319(0.0309) Steps 428(423.67) | Grad Norm 0.2879(0.3560) | Total Time 10.00(10.00)\n",
      "Iter 1012 | Time 31.4933(30.6801) | Bit/dim 1.1760(1.1817) | Xent 0.1010(0.0998) | Loss 1.2265(1.2316) | Error 0.0312(0.0309) Steps 428(423.80) | Grad Norm 0.2594(0.3531) | Total Time 10.00(10.00)\n",
      "Iter 1013 | Time 31.0698(30.6918) | Bit/dim 1.1751(1.1815) | Xent 0.0941(0.0996) | Loss 1.2221(1.2313) | Error 0.0279(0.0308) Steps 428(423.93) | Grad Norm 0.2354(0.3496) | Total Time 10.00(10.00)\n",
      "Iter 1014 | Time 31.1630(30.7059) | Bit/dim 1.1777(1.1814) | Xent 0.1001(0.0996) | Loss 1.2278(1.2312) | Error 0.0300(0.0308) Steps 428(424.05) | Grad Norm 0.2964(0.3480) | Total Time 10.00(10.00)\n",
      "Iter 1015 | Time 30.8525(30.7103) | Bit/dim 1.1821(1.1814) | Xent 0.0872(0.0992) | Loss 1.2257(1.2311) | Error 0.0260(0.0306) Steps 428(424.17) | Grad Norm 0.3991(0.3495) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 16.1309, Epoch Time 247.8641(238.7059), Bit/dim 1.1721(best: 1.1722), Xent 0.0484, Loss 1.1963, Error 0.0153(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1016 | Time 31.3477(30.7294) | Bit/dim 1.1753(1.1813) | Xent 0.0959(0.0991) | Loss 1.2233(1.2308) | Error 0.0309(0.0306) Steps 428(424.28) | Grad Norm 0.2111(0.3454) | Total Time 10.00(10.00)\n",
      "Iter 1017 | Time 30.8444(30.7329) | Bit/dim 1.1798(1.1812) | Xent 0.1010(0.0992) | Loss 1.2303(1.2308) | Error 0.0312(0.0306) Steps 428(424.39) | Grad Norm 0.3664(0.3460) | Total Time 10.00(10.00)\n",
      "Iter 1018 | Time 31.4836(30.7554) | Bit/dim 1.1760(1.1811) | Xent 0.1002(0.0992) | Loss 1.2261(1.2307) | Error 0.0309(0.0306) Steps 428(424.50) | Grad Norm 0.2675(0.3436) | Total Time 10.00(10.00)\n",
      "Iter 1019 | Time 30.7298(30.7546) | Bit/dim 1.1850(1.1812) | Xent 0.1053(0.0994) | Loss 1.2376(1.2309) | Error 0.0333(0.0307) Steps 428(424.61) | Grad Norm 0.2592(0.3411) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 32.2519(30.7995) | Bit/dim 1.1814(1.1812) | Xent 0.0882(0.0991) | Loss 1.2255(1.2307) | Error 0.0271(0.0306) Steps 428(424.71) | Grad Norm 0.3154(0.3403) | Total Time 10.00(10.00)\n",
      "Iter 1021 | Time 32.4111(30.8479) | Bit/dim 1.1786(1.1811) | Xent 0.1055(0.0993) | Loss 1.2314(1.2307) | Error 0.0319(0.0307) Steps 428(424.81) | Grad Norm 0.3300(0.3400) | Total Time 10.00(10.00)\n",
      "Iter 1022 | Time 31.1774(30.8578) | Bit/dim 1.1766(1.1810) | Xent 0.0973(0.0992) | Loss 1.2252(1.2306) | Error 0.0294(0.0306) Steps 428(424.90) | Grad Norm 0.2280(0.3367) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 15.9624, Epoch Time 248.5912(239.0025), Bit/dim 1.1738(best: 1.1721), Xent 0.0497, Loss 1.1987, Error 0.0167(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1023 | Time 30.0225(30.8327) | Bit/dim 1.1859(1.1811) | Xent 0.0947(0.0991) | Loss 1.2332(1.2306) | Error 0.0301(0.0306) Steps 428(425.00) | Grad Norm 0.2049(0.3327) | Total Time 10.00(10.00)\n",
      "Iter 1024 | Time 31.0654(30.8397) | Bit/dim 1.1775(1.1810) | Xent 0.0968(0.0990) | Loss 1.2259(1.2305) | Error 0.0296(0.0306) Steps 428(425.09) | Grad Norm 0.3153(0.3322) | Total Time 10.00(10.00)\n",
      "Iter 1025 | Time 31.7416(30.8668) | Bit/dim 1.1799(1.1810) | Xent 0.0967(0.0989) | Loss 1.2282(1.2304) | Error 0.0282(0.0305) Steps 428(425.17) | Grad Norm 0.2850(0.3308) | Total Time 10.00(10.00)\n",
      "Iter 1026 | Time 31.4083(30.8830) | Bit/dim 1.1740(1.1808) | Xent 0.0958(0.0988) | Loss 1.2219(1.2302) | Error 0.0294(0.0305) Steps 428(425.26) | Grad Norm 0.1900(0.3265) | Total Time 10.00(10.00)\n",
      "Iter 1027 | Time 31.6271(30.9053) | Bit/dim 1.1850(1.1809) | Xent 0.0901(0.0986) | Loss 1.2300(1.2302) | Error 0.0274(0.0304) Steps 428(425.34) | Grad Norm 0.2386(0.3239) | Total Time 10.00(10.00)\n",
      "Iter 1028 | Time 31.8943(30.9350) | Bit/dim 1.1815(1.1809) | Xent 0.0944(0.0984) | Loss 1.2287(1.2301) | Error 0.0276(0.0303) Steps 428(425.42) | Grad Norm 0.2020(0.3203) | Total Time 10.00(10.00)\n",
      "Iter 1029 | Time 31.6421(30.9562) | Bit/dim 1.1750(1.1807) | Xent 0.0986(0.0984) | Loss 1.2243(1.2300) | Error 0.0320(0.0303) Steps 440(425.86) | Grad Norm 0.2553(0.3183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 16.0225, Epoch Time 247.6619(239.2623), Bit/dim 1.1737(best: 1.1721), Xent 0.0503, Loss 1.1989, Error 0.0164(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1030 | Time 31.1329(30.9615) | Bit/dim 1.1833(1.1808) | Xent 0.1045(0.0986) | Loss 1.2356(1.2301) | Error 0.0306(0.0304) Steps 422(425.74) | Grad Norm 0.2885(0.3174) | Total Time 10.00(10.00)\n",
      "Iter 1031 | Time 31.6839(30.9832) | Bit/dim 1.1776(1.1807) | Xent 0.0935(0.0985) | Loss 1.2243(1.2299) | Error 0.0290(0.0303) Steps 422(425.63) | Grad Norm 0.3533(0.3185) | Total Time 10.00(10.00)\n",
      "Iter 1032 | Time 30.9457(30.9821) | Bit/dim 1.1812(1.1807) | Xent 0.0977(0.0984) | Loss 1.2300(1.2300) | Error 0.0299(0.0303) Steps 422(425.52) | Grad Norm 0.2884(0.3176) | Total Time 10.00(10.00)\n",
      "Iter 1033 | Time 30.7954(30.9765) | Bit/dim 1.1809(1.1807) | Xent 0.1017(0.0985) | Loss 1.2317(1.2300) | Error 0.0294(0.0303) Steps 422(425.41) | Grad Norm 0.2188(0.3146) | Total Time 10.00(10.00)\n",
      "Iter 1034 | Time 31.7751(31.0004) | Bit/dim 1.1780(1.1807) | Xent 0.0924(0.0984) | Loss 1.2242(1.2298) | Error 0.0276(0.0302) Steps 422(425.31) | Grad Norm 0.2489(0.3126) | Total Time 10.00(10.00)\n",
      "Iter 1035 | Time 32.3196(31.0400) | Bit/dim 1.1784(1.1806) | Xent 0.0923(0.0982) | Loss 1.2246(1.2297) | Error 0.0282(0.0301) Steps 434(425.57) | Grad Norm 0.3250(0.3130) | Total Time 10.00(10.00)\n",
      "Iter 1036 | Time 31.7914(31.0625) | Bit/dim 1.1837(1.1807) | Xent 0.0973(0.0982) | Loss 1.2323(1.2298) | Error 0.0299(0.0301) Steps 434(425.83) | Grad Norm 0.3073(0.3128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 16.1849, Epoch Time 248.8011(239.5484), Bit/dim 1.1754(best: 1.1721), Xent 0.0483, Loss 1.1995, Error 0.0156(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1037 | Time 31.1561(31.0653) | Bit/dim 1.1853(1.1808) | Xent 0.0961(0.0981) | Loss 1.2334(1.2299) | Error 0.0294(0.0301) Steps 434(426.07) | Grad Norm 0.3051(0.3126) | Total Time 10.00(10.00)\n",
      "Iter 1038 | Time 31.2008(31.0694) | Bit/dim 1.1821(1.1809) | Xent 0.0932(0.0979) | Loss 1.2287(1.2298) | Error 0.0302(0.0301) Steps 434(426.31) | Grad Norm 0.2500(0.3107) | Total Time 10.00(10.00)\n",
      "Iter 1039 | Time 31.9744(31.0966) | Bit/dim 1.1822(1.1809) | Xent 0.1012(0.0980) | Loss 1.2327(1.2299) | Error 0.0305(0.0301) Steps 434(426.54) | Grad Norm 0.2066(0.3076) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 32.3107(31.1330) | Bit/dim 1.1797(1.1809) | Xent 0.1022(0.0982) | Loss 1.2308(1.2299) | Error 0.0330(0.0302) Steps 434(426.76) | Grad Norm 0.3593(0.3092) | Total Time 10.00(10.00)\n",
      "Iter 1041 | Time 31.5459(31.1454) | Bit/dim 1.1832(1.1809) | Xent 0.0984(0.0982) | Loss 1.2325(1.2300) | Error 0.0324(0.0303) Steps 434(426.98) | Grad Norm 0.2320(0.3068) | Total Time 10.00(10.00)\n",
      "Iter 1042 | Time 32.6512(31.1905) | Bit/dim 1.1784(1.1809) | Xent 0.1074(0.0984) | Loss 1.2321(1.2301) | Error 0.0343(0.0304) Steps 434(427.19) | Grad Norm 0.3126(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 1043 | Time 31.1843(31.1904) | Bit/dim 1.1810(1.1809) | Xent 0.0878(0.0981) | Loss 1.2249(1.2299) | Error 0.0271(0.0303) Steps 434(427.40) | Grad Norm 0.2134(0.3042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 16.2357, Epoch Time 250.3928(239.8738), Bit/dim 1.1754(best: 1.1721), Xent 0.0479, Loss 1.1994, Error 0.0156(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1044 | Time 32.8332(31.2396) | Bit/dim 1.1742(1.1807) | Xent 0.0975(0.0981) | Loss 1.2229(1.2297) | Error 0.0296(0.0303) Steps 434(427.59) | Grad Norm 0.2165(0.3016) | Total Time 10.00(10.00)\n",
      "Iter 1045 | Time 31.5307(31.2484) | Bit/dim 1.1847(1.1808) | Xent 0.1043(0.0983) | Loss 1.2368(1.2299) | Error 0.0340(0.0304) Steps 434(427.79) | Grad Norm 0.2492(0.3000) | Total Time 10.00(10.00)\n",
      "Iter 1046 | Time 31.6104(31.2592) | Bit/dim 1.1865(1.1809) | Xent 0.0903(0.0981) | Loss 1.2317(1.2300) | Error 0.0272(0.0303) Steps 434(427.97) | Grad Norm 0.1831(0.2965) | Total Time 10.00(10.00)\n",
      "Iter 1047 | Time 31.7632(31.2744) | Bit/dim 1.1801(1.1809) | Xent 0.1067(0.0983) | Loss 1.2334(1.2301) | Error 0.0323(0.0303) Steps 434(428.15) | Grad Norm 0.3397(0.2978) | Total Time 10.00(10.00)\n",
      "Iter 1048 | Time 33.4638(31.3400) | Bit/dim 1.1854(1.1811) | Xent 0.0923(0.0981) | Loss 1.2316(1.2301) | Error 0.0268(0.0302) Steps 434(428.33) | Grad Norm 0.2308(0.2958) | Total Time 10.00(10.00)\n",
      "Iter 1049 | Time 31.9609(31.3587) | Bit/dim 1.1824(1.1811) | Xent 0.0988(0.0982) | Loss 1.2318(1.2302) | Error 0.0327(0.0303) Steps 434(428.50) | Grad Norm 0.3371(0.2970) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 32.9189(31.4055) | Bit/dim 1.1838(1.1812) | Xent 0.0997(0.0982) | Loss 1.2336(1.2303) | Error 0.0333(0.0304) Steps 440(428.84) | Grad Norm 0.3640(0.2990) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 16.4948, Epoch Time 254.8814(240.3240), Bit/dim 1.1769(best: 1.1721), Xent 0.0473, Loss 1.2006, Error 0.0162(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1051 | Time 33.2299(31.4602) | Bit/dim 1.1797(1.1811) | Xent 0.0993(0.0982) | Loss 1.2293(1.2303) | Error 0.0285(0.0303) Steps 440(429.18) | Grad Norm 0.3421(0.3003) | Total Time 10.00(10.00)\n",
      "Iter 1052 | Time 32.4188(31.4890) | Bit/dim 1.1816(1.1811) | Xent 0.0933(0.0981) | Loss 1.2282(1.2302) | Error 0.0279(0.0303) Steps 440(429.50) | Grad Norm 0.2109(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 1053 | Time 33.4467(31.5477) | Bit/dim 1.1859(1.1813) | Xent 0.0964(0.0980) | Loss 1.2341(1.2303) | Error 0.0295(0.0302) Steps 434(429.64) | Grad Norm 0.3769(0.3000) | Total Time 10.00(10.00)\n",
      "Iter 1054 | Time 33.8277(31.6161) | Bit/dim 1.1838(1.1814) | Xent 0.1024(0.0982) | Loss 1.2350(1.2305) | Error 0.0299(0.0302) Steps 440(429.95) | Grad Norm 0.3928(0.3028) | Total Time 10.00(10.00)\n",
      "Iter 1055 | Time 32.9210(31.6552) | Bit/dim 1.1877(1.1816) | Xent 0.0981(0.0982) | Loss 1.2368(1.2306) | Error 0.0299(0.0302) Steps 440(430.25) | Grad Norm 0.2776(0.3020) | Total Time 10.00(10.00)\n",
      "Iter 1056 | Time 33.1181(31.6991) | Bit/dim 1.1851(1.1817) | Xent 0.1013(0.0983) | Loss 1.2358(1.2308) | Error 0.0312(0.0303) Steps 440(430.54) | Grad Norm 0.2784(0.3013) | Total Time 10.00(10.00)\n",
      "Iter 1057 | Time 34.0188(31.7687) | Bit/dim 1.1777(1.1815) | Xent 0.0974(0.0982) | Loss 1.2264(1.2307) | Error 0.0305(0.0303) Steps 440(430.83) | Grad Norm 0.2483(0.2997) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 15.9873, Epoch Time 261.2422(240.9515), Bit/dim 1.1778(best: 1.1721), Xent 0.0472, Loss 1.2014, Error 0.0148(best: 0.0149)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1058 | Time 33.8629(31.8315) | Bit/dim 1.1834(1.1816) | Xent 0.1073(0.0985) | Loss 1.2370(1.2309) | Error 0.0333(0.0304) Steps 440(431.10) | Grad Norm 0.2278(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 1059 | Time 32.3166(31.8461) | Bit/dim 1.1848(1.1817) | Xent 0.1046(0.0987) | Loss 1.2371(1.2310) | Error 0.0300(0.0303) Steps 440(431.37) | Grad Norm 0.2756(0.2969) | Total Time 10.00(10.00)\n",
      "Iter 1060 | Time 32.8439(31.8760) | Bit/dim 1.1845(1.1818) | Xent 0.0959(0.0986) | Loss 1.2324(1.2311) | Error 0.0305(0.0303) Steps 440(431.63) | Grad Norm 0.2467(0.2954) | Total Time 10.00(10.00)\n",
      "Iter 1061 | Time 33.2669(31.9177) | Bit/dim 1.1836(1.1818) | Xent 0.1002(0.0987) | Loss 1.2337(1.2312) | Error 0.0295(0.0303) Steps 440(431.88) | Grad Norm 0.2923(0.2953) | Total Time 10.00(10.00)\n",
      "Iter 1062 | Time 32.7950(31.9441) | Bit/dim 1.1835(1.1819) | Xent 0.0942(0.0985) | Loss 1.2306(1.2311) | Error 0.0295(0.0303) Steps 440(432.12) | Grad Norm 0.2484(0.2939) | Total Time 10.00(10.00)\n",
      "Iter 1063 | Time 33.8518(32.0013) | Bit/dim 1.1865(1.1820) | Xent 0.0952(0.0984) | Loss 1.2340(1.2312) | Error 0.0308(0.0303) Steps 440(432.36) | Grad Norm 0.2153(0.2916) | Total Time 10.00(10.00)\n",
      "Iter 1064 | Time 32.6975(32.0222) | Bit/dim 1.1782(1.1819) | Xent 0.0937(0.0983) | Loss 1.2251(1.2310) | Error 0.0306(0.0303) Steps 440(432.59) | Grad Norm 0.4054(0.2950) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 16.4175, Epoch Time 260.4547(241.5366), Bit/dim 1.1773(best: 1.1721), Xent 0.0499, Loss 1.2023, Error 0.0170(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1065 | Time 34.1506(32.0860) | Bit/dim 1.1807(1.1819) | Xent 0.0909(0.0981) | Loss 1.2262(1.2309) | Error 0.0284(0.0303) Steps 440(432.81) | Grad Norm 0.3017(0.2952) | Total Time 10.00(10.00)\n",
      "Iter 1066 | Time 33.7495(32.1359) | Bit/dim 1.1784(1.1818) | Xent 0.1025(0.0982) | Loss 1.2297(1.2309) | Error 0.0310(0.0303) Steps 434(432.85) | Grad Norm 0.3041(0.2954) | Total Time 10.00(10.00)\n",
      "Iter 1067 | Time 32.6097(32.1502) | Bit/dim 1.1859(1.1819) | Xent 0.0996(0.0982) | Loss 1.2357(1.2310) | Error 0.0285(0.0302) Steps 440(433.06) | Grad Norm 0.3086(0.2958) | Total Time 10.00(10.00)\n",
      "Iter 1068 | Time 32.6279(32.1645) | Bit/dim 1.1836(1.1819) | Xent 0.1039(0.0984) | Loss 1.2355(1.2311) | Error 0.0330(0.0303) Steps 440(433.27) | Grad Norm 0.3157(0.2964) | Total Time 10.00(10.00)\n",
      "Iter 1069 | Time 32.5011(32.1746) | Bit/dim 1.1840(1.1820) | Xent 0.0862(0.0980) | Loss 1.2271(1.2310) | Error 0.0264(0.0302) Steps 440(433.47) | Grad Norm 0.2209(0.2942) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 33.9766(32.2286) | Bit/dim 1.1817(1.1820) | Xent 0.0949(0.0979) | Loss 1.2291(1.2310) | Error 0.0282(0.0301) Steps 440(433.67) | Grad Norm 0.2831(0.2938) | Total Time 10.00(10.00)\n",
      "Iter 1071 | Time 32.0675(32.2238) | Bit/dim 1.1883(1.1822) | Xent 0.1003(0.0980) | Loss 1.2384(1.2312) | Error 0.0305(0.0301) Steps 440(433.86) | Grad Norm 0.3535(0.2956) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 16.3022, Epoch Time 260.3013(242.0996), Bit/dim 1.1770(best: 1.1721), Xent 0.0483, Loss 1.2012, Error 0.0152(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1072 | Time 33.3197(32.2567) | Bit/dim 1.1833(1.1822) | Xent 0.0848(0.0976) | Loss 1.2257(1.2310) | Error 0.0269(0.0301) Steps 440(434.04) | Grad Norm 0.1899(0.2925) | Total Time 10.00(10.00)\n",
      "Iter 1073 | Time 33.5895(32.2967) | Bit/dim 1.1874(1.1824) | Xent 0.0940(0.0975) | Loss 1.2344(1.2311) | Error 0.0276(0.0300) Steps 440(434.22) | Grad Norm 0.3451(0.2940) | Total Time 10.00(10.00)\n",
      "Iter 1074 | Time 32.7285(32.3096) | Bit/dim 1.1815(1.1823) | Xent 0.0939(0.0974) | Loss 1.2285(1.2310) | Error 0.0302(0.0300) Steps 440(434.39) | Grad Norm 0.2606(0.2930) | Total Time 10.00(10.00)\n",
      "Iter 1075 | Time 32.2827(32.3088) | Bit/dim 1.1771(1.1822) | Xent 0.0945(0.0973) | Loss 1.2244(1.2308) | Error 0.0278(0.0299) Steps 440(434.56) | Grad Norm 0.2372(0.2914) | Total Time 10.00(10.00)\n",
      "Iter 1076 | Time 32.7073(32.3208) | Bit/dim 1.1829(1.1822) | Xent 0.1016(0.0974) | Loss 1.2338(1.2309) | Error 0.0316(0.0300) Steps 434(434.54) | Grad Norm 0.2668(0.2906) | Total Time 10.00(10.00)\n",
      "Iter 1077 | Time 32.0659(32.3131) | Bit/dim 1.1870(1.1824) | Xent 0.1110(0.0978) | Loss 1.2424(1.2313) | Error 0.0320(0.0300) Steps 440(434.71) | Grad Norm 0.3813(0.2933) | Total Time 10.00(10.00)\n",
      "Iter 1078 | Time 32.6831(32.3242) | Bit/dim 1.1794(1.1823) | Xent 0.0963(0.0978) | Loss 1.2275(1.2312) | Error 0.0296(0.0300) Steps 440(434.87) | Grad Norm 0.2742(0.2928) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 16.3460, Epoch Time 257.8353(242.5717), Bit/dim 1.1753(best: 1.1721), Xent 0.0512, Loss 1.2009, Error 0.0154(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1079 | Time 33.6710(32.3646) | Bit/dim 1.1829(1.1823) | Xent 0.1113(0.0982) | Loss 1.2385(1.2314) | Error 0.0339(0.0301) Steps 440(435.02) | Grad Norm 0.2048(0.2901) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 32.7023(32.3748) | Bit/dim 1.1776(1.1821) | Xent 0.0974(0.0982) | Loss 1.2263(1.2312) | Error 0.0300(0.0301) Steps 440(435.17) | Grad Norm 0.2827(0.2899) | Total Time 10.00(10.00)\n",
      "Iter 1081 | Time 33.4492(32.4070) | Bit/dim 1.1837(1.1822) | Xent 0.0970(0.0981) | Loss 1.2322(1.2313) | Error 0.0298(0.0301) Steps 440(435.32) | Grad Norm 0.2705(0.2893) | Total Time 10.00(10.00)\n",
      "Iter 1082 | Time 31.9446(32.3931) | Bit/dim 1.1817(1.1822) | Xent 0.0997(0.0982) | Loss 1.2316(1.2313) | Error 0.0311(0.0301) Steps 440(435.46) | Grad Norm 0.3610(0.2915) | Total Time 10.00(10.00)\n",
      "Iter 1083 | Time 32.9227(32.4090) | Bit/dim 1.1856(1.1823) | Xent 0.0935(0.0981) | Loss 1.2323(1.2313) | Error 0.0270(0.0301) Steps 440(435.59) | Grad Norm 0.2031(0.2888) | Total Time 10.00(10.00)\n",
      "Iter 1084 | Time 32.6715(32.4169) | Bit/dim 1.1810(1.1822) | Xent 0.0912(0.0978) | Loss 1.2266(1.2312) | Error 0.0284(0.0300) Steps 440(435.72) | Grad Norm 0.3179(0.2897) | Total Time 10.00(10.00)\n",
      "Iter 1085 | Time 33.2825(32.4429) | Bit/dim 1.1778(1.1821) | Xent 0.0897(0.0976) | Loss 1.2226(1.2309) | Error 0.0256(0.0299) Steps 440(435.85) | Grad Norm 0.3208(0.2906) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 16.3709, Epoch Time 259.2099(243.0708), Bit/dim 1.1749(best: 1.1721), Xent 0.0493, Loss 1.1995, Error 0.0165(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1086 | Time 35.0905(32.5223) | Bit/dim 1.1839(1.1822) | Xent 0.0912(0.0974) | Loss 1.2295(1.2309) | Error 0.0295(0.0299) Steps 440(435.98) | Grad Norm 0.2603(0.2897) | Total Time 10.00(10.00)\n",
      "Iter 1087 | Time 33.5713(32.5538) | Bit/dim 1.1796(1.1821) | Xent 0.1004(0.0975) | Loss 1.2298(1.2308) | Error 0.0296(0.0299) Steps 440(436.10) | Grad Norm 0.2765(0.2893) | Total Time 10.00(10.00)\n",
      "Iter 1088 | Time 32.8754(32.5634) | Bit/dim 1.1791(1.1820) | Xent 0.0967(0.0975) | Loss 1.2275(1.2307) | Error 0.0270(0.0298) Steps 440(436.21) | Grad Norm 0.2126(0.2870) | Total Time 10.00(10.00)\n",
      "Iter 1089 | Time 32.7159(32.5680) | Bit/dim 1.1771(1.1818) | Xent 0.0853(0.0971) | Loss 1.2198(1.2304) | Error 0.0275(0.0297) Steps 440(436.33) | Grad Norm 0.1867(0.2840) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 33.1848(32.5865) | Bit/dim 1.1796(1.1818) | Xent 0.0983(0.0971) | Loss 1.2288(1.2304) | Error 0.0302(0.0297) Steps 434(436.26) | Grad Norm 0.2620(0.2834) | Total Time 10.00(10.00)\n",
      "Iter 1091 | Time 32.9644(32.5978) | Bit/dim 1.1794(1.1817) | Xent 0.1048(0.0974) | Loss 1.2318(1.2304) | Error 0.0321(0.0298) Steps 440(436.37) | Grad Norm 0.3497(0.2853) | Total Time 10.00(10.00)\n",
      "Iter 1092 | Time 34.0166(32.6404) | Bit/dim 1.1854(1.1818) | Xent 0.1079(0.0977) | Loss 1.2393(1.2307) | Error 0.0344(0.0299) Steps 440(436.48) | Grad Norm 0.2606(0.2846) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 16.2986, Epoch Time 263.0592(243.6705), Bit/dim 1.1740(best: 1.1721), Xent 0.0499, Loss 1.1989, Error 0.0174(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1093 | Time 32.3932(32.6330) | Bit/dim 1.1782(1.1817) | Xent 0.0961(0.0976) | Loss 1.2263(1.2305) | Error 0.0301(0.0299) Steps 440(436.59) | Grad Norm 0.2525(0.2836) | Total Time 10.00(10.00)\n",
      "Iter 1094 | Time 32.8656(32.6399) | Bit/dim 1.1799(1.1817) | Xent 0.1047(0.0979) | Loss 1.2323(1.2306) | Error 0.0326(0.0300) Steps 440(436.69) | Grad Norm 0.2724(0.2833) | Total Time 10.00(10.00)\n",
      "Iter 1095 | Time 33.2957(32.6596) | Bit/dim 1.1809(1.1816) | Xent 0.1002(0.0979) | Loss 1.2310(1.2306) | Error 0.0320(0.0301) Steps 440(436.79) | Grad Norm 0.4535(0.2884) | Total Time 10.00(10.00)\n",
      "Iter 1096 | Time 33.6289(32.6887) | Bit/dim 1.1847(1.1817) | Xent 0.1071(0.0982) | Loss 1.2383(1.2308) | Error 0.0326(0.0301) Steps 440(436.88) | Grad Norm 0.2831(0.2882) | Total Time 10.00(10.00)\n",
      "Iter 1097 | Time 34.1660(32.7330) | Bit/dim 1.1767(1.1816) | Xent 0.0960(0.0981) | Loss 1.2247(1.2306) | Error 0.0325(0.0302) Steps 440(436.98) | Grad Norm 0.3420(0.2899) | Total Time 10.00(10.00)\n",
      "Iter 1098 | Time 32.6712(32.7312) | Bit/dim 1.1840(1.1817) | Xent 0.1004(0.0982) | Loss 1.2342(1.2308) | Error 0.0294(0.0302) Steps 440(437.07) | Grad Norm 0.4191(0.2937) | Total Time 10.00(10.00)\n",
      "Iter 1099 | Time 32.5864(32.7268) | Bit/dim 1.1811(1.1816) | Xent 0.0961(0.0981) | Loss 1.2291(1.2307) | Error 0.0288(0.0302) Steps 440(437.16) | Grad Norm 0.2485(0.2924) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 16.0879, Epoch Time 259.8133(244.1547), Bit/dim 1.1740(best: 1.1721), Xent 0.0469, Loss 1.1974, Error 0.0164(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1100 | Time 34.9099(32.7923) | Bit/dim 1.1799(1.1816) | Xent 0.0986(0.0982) | Loss 1.2292(1.2307) | Error 0.0299(0.0301) Steps 440(437.24) | Grad Norm 0.4649(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 1101 | Time 32.4897(32.7832) | Bit/dim 1.1798(1.1815) | Xent 0.0927(0.0980) | Loss 1.2261(1.2305) | Error 0.0289(0.0301) Steps 440(437.32) | Grad Norm 0.2559(0.2963) | Total Time 10.00(10.00)\n",
      "Iter 1102 | Time 32.3440(32.7701) | Bit/dim 1.1807(1.1815) | Xent 0.0981(0.0980) | Loss 1.2297(1.2305) | Error 0.0281(0.0300) Steps 440(437.40) | Grad Norm 0.3390(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 1103 | Time 32.0872(32.7496) | Bit/dim 1.1830(1.1816) | Xent 0.1061(0.0982) | Loss 1.2361(1.2307) | Error 0.0319(0.0301) Steps 440(437.48) | Grad Norm 0.3377(0.2988) | Total Time 10.00(10.00)\n",
      "Iter 1104 | Time 32.9888(32.7567) | Bit/dim 1.1813(1.1815) | Xent 0.0967(0.0982) | Loss 1.2296(1.2306) | Error 0.0280(0.0300) Steps 440(437.56) | Grad Norm 0.2477(0.2973) | Total Time 10.00(10.00)\n",
      "Iter 1105 | Time 33.5534(32.7806) | Bit/dim 1.1816(1.1815) | Xent 0.1038(0.0984) | Loss 1.2335(1.2307) | Error 0.0295(0.0300) Steps 440(437.63) | Grad Norm 0.3579(0.2991) | Total Time 10.00(10.00)\n",
      "Iter 1106 | Time 34.2317(32.8242) | Bit/dim 1.1800(1.1815) | Xent 0.0872(0.0980) | Loss 1.2237(1.2305) | Error 0.0260(0.0299) Steps 440(437.70) | Grad Norm 0.4093(0.3024) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 16.1927, Epoch Time 261.0612(244.6619), Bit/dim 1.1747(best: 1.1721), Xent 0.0450, Loss 1.1972, Error 0.0156(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1107 | Time 32.7549(32.8221) | Bit/dim 1.1769(1.1814) | Xent 0.0954(0.0979) | Loss 1.2246(1.2303) | Error 0.0290(0.0299) Steps 440(437.77) | Grad Norm 0.1706(0.2984) | Total Time 10.00(10.00)\n",
      "Iter 1108 | Time 33.8376(32.8526) | Bit/dim 1.1770(1.1812) | Xent 0.0979(0.0979) | Loss 1.2260(1.2302) | Error 0.0302(0.0299) Steps 434(437.66) | Grad Norm 0.4064(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 1109 | Time 32.6562(32.8467) | Bit/dim 1.1771(1.1811) | Xent 0.0941(0.0978) | Loss 1.2241(1.2300) | Error 0.0290(0.0299) Steps 440(437.73) | Grad Norm 0.4601(0.3064) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 33.1768(32.8566) | Bit/dim 1.1842(1.1812) | Xent 0.0943(0.0977) | Loss 1.2314(1.2301) | Error 0.0302(0.0299) Steps 440(437.80) | Grad Norm 0.3051(0.3064) | Total Time 10.00(10.00)\n",
      "Iter 1111 | Time 32.8265(32.8557) | Bit/dim 1.1875(1.1814) | Xent 0.0997(0.0978) | Loss 1.2373(1.2303) | Error 0.0308(0.0299) Steps 434(437.68) | Grad Norm 0.4820(0.3117) | Total Time 10.00(10.00)\n",
      "Iter 1112 | Time 33.6259(32.8788) | Bit/dim 1.1832(1.1814) | Xent 0.0986(0.0978) | Loss 1.2325(1.2303) | Error 0.0320(0.0300) Steps 434(437.57) | Grad Norm 0.3781(0.3136) | Total Time 10.00(10.00)\n",
      "Iter 1113 | Time 33.0815(32.8849) | Bit/dim 1.1803(1.1814) | Xent 0.0937(0.0977) | Loss 1.2272(1.2303) | Error 0.0304(0.0300) Steps 440(437.64) | Grad Norm 0.3777(0.3156) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 16.2691, Epoch Time 260.6702(245.1422), Bit/dim 1.1757(best: 1.1721), Xent 0.0479, Loss 1.1996, Error 0.0162(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1114 | Time 33.5169(32.9038) | Bit/dim 1.1864(1.1816) | Xent 0.1010(0.0978) | Loss 1.2369(1.2305) | Error 0.0308(0.0300) Steps 440(437.72) | Grad Norm 0.4775(0.3204) | Total Time 10.00(10.00)\n",
      "Iter 1115 | Time 32.3728(32.8879) | Bit/dim 1.1765(1.1814) | Xent 0.0984(0.0978) | Loss 1.2257(1.2303) | Error 0.0292(0.0300) Steps 440(437.78) | Grad Norm 0.3561(0.3215) | Total Time 10.00(10.00)\n",
      "Iter 1116 | Time 33.3351(32.9013) | Bit/dim 1.1810(1.1814) | Xent 0.0912(0.0976) | Loss 1.2266(1.2302) | Error 0.0285(0.0299) Steps 440(437.85) | Grad Norm 0.3858(0.3234) | Total Time 10.00(10.00)\n",
      "Iter 1117 | Time 33.4196(32.9169) | Bit/dim 1.1859(1.1815) | Xent 0.0910(0.0974) | Loss 1.2315(1.2302) | Error 0.0269(0.0298) Steps 434(437.73) | Grad Norm 0.4219(0.3264) | Total Time 10.00(10.00)\n",
      "Iter 1118 | Time 33.0867(32.9220) | Bit/dim 1.1877(1.1817) | Xent 0.1000(0.0975) | Loss 1.2377(1.2305) | Error 0.0305(0.0299) Steps 440(437.80) | Grad Norm 0.3027(0.3257) | Total Time 10.00(10.00)\n",
      "Iter 1119 | Time 33.6918(32.9451) | Bit/dim 1.1798(1.1817) | Xent 0.1001(0.0976) | Loss 1.2299(1.2304) | Error 0.0309(0.0299) Steps 440(437.87) | Grad Norm 0.2787(0.3243) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 33.2950(32.9555) | Bit/dim 1.1825(1.1817) | Xent 0.0988(0.0976) | Loss 1.2319(1.2305) | Error 0.0290(0.0299) Steps 440(437.93) | Grad Norm 0.2423(0.3218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 16.1990, Epoch Time 261.0333(245.6189), Bit/dim 1.1754(best: 1.1721), Xent 0.0492, Loss 1.2000, Error 0.0163(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1121 | Time 33.0216(32.9575) | Bit/dim 1.1851(1.1818) | Xent 0.0916(0.0974) | Loss 1.2309(1.2305) | Error 0.0288(0.0298) Steps 440(437.99) | Grad Norm 0.2670(0.3202) | Total Time 10.00(10.00)\n",
      "Iter 1122 | Time 33.3304(32.9687) | Bit/dim 1.1815(1.1818) | Xent 0.0965(0.0974) | Loss 1.2297(1.2305) | Error 0.0285(0.0298) Steps 440(438.05) | Grad Norm 0.2414(0.3178) | Total Time 10.00(10.00)\n",
      "Iter 1123 | Time 33.0986(32.9726) | Bit/dim 1.1824(1.1818) | Xent 0.0896(0.0972) | Loss 1.2272(1.2304) | Error 0.0282(0.0297) Steps 440(438.11) | Grad Norm 0.3946(0.3201) | Total Time 10.00(10.00)\n",
      "Iter 1124 | Time 33.7766(32.9967) | Bit/dim 1.1813(1.1818) | Xent 0.1003(0.0973) | Loss 1.2315(1.2304) | Error 0.0302(0.0298) Steps 440(438.17) | Grad Norm 0.2592(0.3183) | Total Time 10.00(10.00)\n",
      "Iter 1125 | Time 34.0897(33.0295) | Bit/dim 1.1776(1.1817) | Xent 0.0929(0.0971) | Loss 1.2240(1.2302) | Error 0.0268(0.0297) Steps 440(438.22) | Grad Norm 0.3289(0.3186) | Total Time 10.00(10.00)\n",
      "Iter 1126 | Time 34.7003(33.0796) | Bit/dim 1.1831(1.1817) | Xent 0.0968(0.0971) | Loss 1.2315(1.2303) | Error 0.0286(0.0296) Steps 434(438.10) | Grad Norm 0.2578(0.3168) | Total Time 10.00(10.00)\n",
      "Iter 1127 | Time 33.8142(33.1017) | Bit/dim 1.1864(1.1818) | Xent 0.0944(0.0970) | Loss 1.2336(1.2304) | Error 0.0288(0.0296) Steps 434(437.97) | Grad Norm 0.2956(0.3161) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 16.2294, Epoch Time 264.2858(246.1789), Bit/dim 1.1751(best: 1.1721), Xent 0.0486, Loss 1.1994, Error 0.0170(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1128 | Time 34.6168(33.1471) | Bit/dim 1.1822(1.1818) | Xent 0.1012(0.0972) | Loss 1.2328(1.2304) | Error 0.0306(0.0296) Steps 434(437.86) | Grad Norm 0.3869(0.3183) | Total Time 10.00(10.00)\n",
      "Iter 1129 | Time 33.1168(33.1462) | Bit/dim 1.1823(1.1819) | Xent 0.1081(0.0975) | Loss 1.2364(1.2306) | Error 0.0314(0.0297) Steps 440(437.92) | Grad Norm 0.2496(0.3162) | Total Time 10.00(10.00)\n",
      "Iter 1130 | Time 33.6787(33.1622) | Bit/dim 1.1830(1.1819) | Xent 0.0973(0.0975) | Loss 1.2316(1.2306) | Error 0.0312(0.0297) Steps 440(437.98) | Grad Norm 0.4096(0.3190) | Total Time 10.00(10.00)\n",
      "Iter 1131 | Time 33.5753(33.1746) | Bit/dim 1.1851(1.1820) | Xent 0.0975(0.0975) | Loss 1.2338(1.2307) | Error 0.0314(0.0298) Steps 440(438.04) | Grad Norm 0.2281(0.3163) | Total Time 10.00(10.00)\n",
      "Iter 1132 | Time 34.0141(33.1998) | Bit/dim 1.1815(1.1820) | Xent 0.0951(0.0974) | Loss 1.2291(1.2307) | Error 0.0302(0.0298) Steps 440(438.10) | Grad Norm 0.3418(0.3170) | Total Time 10.00(10.00)\n",
      "Iter 1133 | Time 33.8956(33.2207) | Bit/dim 1.1820(1.1820) | Xent 0.0965(0.0974) | Loss 1.2303(1.2307) | Error 0.0291(0.0298) Steps 440(438.16) | Grad Norm 0.2656(0.3155) | Total Time 10.00(10.00)\n",
      "Iter 1134 | Time 34.4388(33.2572) | Bit/dim 1.1759(1.1818) | Xent 0.0961(0.0973) | Loss 1.2239(1.2305) | Error 0.0308(0.0298) Steps 440(438.21) | Grad Norm 0.4412(0.3193) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 16.1952, Epoch Time 265.7521(246.7661), Bit/dim 1.1750(best: 1.1721), Xent 0.0479, Loss 1.1989, Error 0.0159(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1135 | Time 34.9765(33.3088) | Bit/dim 1.1798(1.1817) | Xent 0.0961(0.0973) | Loss 1.2278(1.2304) | Error 0.0304(0.0298) Steps 440(438.27) | Grad Norm 0.2085(0.3159) | Total Time 10.00(10.00)\n",
      "Iter 1136 | Time 34.9143(33.3569) | Bit/dim 1.1777(1.1816) | Xent 0.1028(0.0975) | Loss 1.2291(1.2304) | Error 0.0339(0.0299) Steps 440(438.32) | Grad Norm 0.2369(0.3136) | Total Time 10.00(10.00)\n",
      "Iter 1137 | Time 33.4936(33.3610) | Bit/dim 1.1798(1.1816) | Xent 0.0966(0.0974) | Loss 1.2281(1.2303) | Error 0.0300(0.0300) Steps 440(438.37) | Grad Norm 0.3502(0.3147) | Total Time 10.00(10.00)\n",
      "Iter 1138 | Time 33.5615(33.3671) | Bit/dim 1.1846(1.1816) | Xent 0.0917(0.0973) | Loss 1.2304(1.2303) | Error 0.0296(0.0299) Steps 440(438.42) | Grad Norm 0.2245(0.3120) | Total Time 10.00(10.00)\n",
      "Iter 1139 | Time 33.5472(33.3725) | Bit/dim 1.1874(1.1818) | Xent 0.1037(0.0975) | Loss 1.2392(1.2306) | Error 0.0294(0.0299) Steps 446(438.65) | Grad Norm 0.4279(0.3154) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 34.4369(33.4044) | Bit/dim 1.1817(1.1818) | Xent 0.0948(0.0974) | Loss 1.2291(1.2305) | Error 0.0288(0.0299) Steps 440(438.69) | Grad Norm 0.1855(0.3115) | Total Time 10.00(10.00)\n",
      "Iter 1141 | Time 33.7897(33.4159) | Bit/dim 1.1810(1.1818) | Xent 0.0910(0.0972) | Loss 1.2265(1.2304) | Error 0.0284(0.0298) Steps 440(438.73) | Grad Norm 0.4286(0.3151) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 15.9720, Epoch Time 267.0256(247.3739), Bit/dim 1.1747(best: 1.1721), Xent 0.0467, Loss 1.1981, Error 0.0156(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1142 | Time 33.5619(33.4203) | Bit/dim 1.1803(1.1817) | Xent 0.0933(0.0971) | Loss 1.2270(1.2303) | Error 0.0272(0.0298) Steps 452(439.12) | Grad Norm 0.2269(0.3124) | Total Time 10.00(10.00)\n",
      "Iter 1143 | Time 34.9082(33.4650) | Bit/dim 1.1805(1.1817) | Xent 0.0822(0.0966) | Loss 1.2216(1.2300) | Error 0.0268(0.0297) Steps 440(439.15) | Grad Norm 0.3413(0.3133) | Total Time 10.00(10.00)\n",
      "Iter 1144 | Time 33.3337(33.4610) | Bit/dim 1.1822(1.1817) | Xent 0.0978(0.0967) | Loss 1.2311(1.2301) | Error 0.0290(0.0297) Steps 446(439.36) | Grad Norm 0.2507(0.3114) | Total Time 10.00(10.00)\n",
      "Iter 1145 | Time 34.3341(33.4872) | Bit/dim 1.1824(1.1817) | Xent 0.0994(0.0967) | Loss 1.2321(1.2301) | Error 0.0300(0.0297) Steps 446(439.56) | Grad Norm 0.2569(0.3098) | Total Time 10.00(10.00)\n",
      "Iter 1146 | Time 34.7087(33.5239) | Bit/dim 1.1814(1.1817) | Xent 0.0939(0.0967) | Loss 1.2284(1.2301) | Error 0.0274(0.0296) Steps 452(439.93) | Grad Norm 0.3966(0.3124) | Total Time 10.00(10.00)\n",
      "Iter 1147 | Time 35.4149(33.5806) | Bit/dim 1.1809(1.1817) | Xent 0.1006(0.0968) | Loss 1.2312(1.2301) | Error 0.0314(0.0296) Steps 446(440.11) | Grad Norm 0.2792(0.3114) | Total Time 10.00(10.00)\n",
      "Iter 1148 | Time 34.8525(33.6187) | Bit/dim 1.1809(1.1817) | Xent 0.1016(0.0969) | Loss 1.2317(1.2301) | Error 0.0319(0.0297) Steps 452(440.47) | Grad Norm 0.3772(0.3133) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 16.7658, Epoch Time 270.2016(248.0587), Bit/dim 1.1764(best: 1.1721), Xent 0.0473, Loss 1.2001, Error 0.0163(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1149 | Time 34.8821(33.6566) | Bit/dim 1.1784(1.1816) | Xent 0.0966(0.0969) | Loss 1.2266(1.2300) | Error 0.0291(0.0297) Steps 446(440.63) | Grad Norm 0.2421(0.3112) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 34.7116(33.6883) | Bit/dim 1.1753(1.1814) | Xent 0.0848(0.0966) | Loss 1.2177(1.2297) | Error 0.0275(0.0296) Steps 452(440.97) | Grad Norm 0.3414(0.3121) | Total Time 10.00(10.00)\n",
      "Iter 1151 | Time 34.6646(33.7176) | Bit/dim 1.1796(1.1813) | Xent 0.0972(0.0966) | Loss 1.2282(1.2296) | Error 0.0317(0.0297) Steps 452(441.31) | Grad Norm 0.2361(0.3098) | Total Time 10.00(10.00)\n",
      "Iter 1152 | Time 35.6471(33.7755) | Bit/dim 1.1812(1.1813) | Xent 0.0980(0.0966) | Loss 1.2302(1.2296) | Error 0.0316(0.0298) Steps 452(441.63) | Grad Norm 0.3281(0.3104) | Total Time 10.00(10.00)\n",
      "Iter 1153 | Time 34.3488(33.7927) | Bit/dim 1.1840(1.1814) | Xent 0.0971(0.0966) | Loss 1.2326(1.2297) | Error 0.0298(0.0298) Steps 446(441.76) | Grad Norm 0.2210(0.3077) | Total Time 10.00(10.00)\n",
      "Iter 1154 | Time 35.3419(33.8391) | Bit/dim 1.1854(1.1815) | Xent 0.0958(0.0966) | Loss 1.2333(1.2298) | Error 0.0300(0.0298) Steps 446(441.88) | Grad Norm 0.2464(0.3059) | Total Time 10.00(10.00)\n",
      "Iter 1155 | Time 35.9065(33.9012) | Bit/dim 1.1832(1.1816) | Xent 0.0920(0.0965) | Loss 1.2292(1.2298) | Error 0.0305(0.0298) Steps 452(442.19) | Grad Norm 0.2632(0.3046) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 16.7165, Epoch Time 274.1756(248.8422), Bit/dim 1.1757(best: 1.1721), Xent 0.0475, Loss 1.1995, Error 0.0153(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1156 | Time 35.3822(33.9456) | Bit/dim 1.1834(1.1816) | Xent 0.0924(0.0963) | Loss 1.2297(1.2298) | Error 0.0299(0.0298) Steps 446(442.30) | Grad Norm 0.2084(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 1157 | Time 35.0110(33.9776) | Bit/dim 1.1824(1.1817) | Xent 0.0956(0.0963) | Loss 1.2302(1.2298) | Error 0.0290(0.0298) Steps 446(442.41) | Grad Norm 0.2291(0.2995) | Total Time 10.00(10.00)\n",
      "Iter 1158 | Time 35.6640(34.0282) | Bit/dim 1.1866(1.1818) | Xent 0.0879(0.0961) | Loss 1.2306(1.2299) | Error 0.0269(0.0297) Steps 446(442.52) | Grad Norm 0.3723(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 1159 | Time 34.6595(34.0471) | Bit/dim 1.1845(1.1819) | Xent 0.0989(0.0962) | Loss 1.2340(1.2300) | Error 0.0302(0.0297) Steps 446(442.63) | Grad Norm 0.2214(0.2993) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 34.2432(34.0530) | Bit/dim 1.1791(1.1818) | Xent 0.0915(0.0960) | Loss 1.2248(1.2298) | Error 0.0304(0.0297) Steps 446(442.73) | Grad Norm 0.2548(0.2980) | Total Time 10.00(10.00)\n",
      "Iter 1161 | Time 33.2439(34.0287) | Bit/dim 1.1770(1.1817) | Xent 0.1054(0.0963) | Loss 1.2296(1.2298) | Error 0.0329(0.0298) Steps 452(443.00) | Grad Norm 0.3265(0.2988) | Total Time 10.00(10.00)\n",
      "Iter 1162 | Time 35.7542(34.0805) | Bit/dim 1.1794(1.1816) | Xent 0.0929(0.0962) | Loss 1.2259(1.2297) | Error 0.0288(0.0298) Steps 452(443.27) | Grad Norm 0.2435(0.2972) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 16.4269, Epoch Time 272.6082(249.5552), Bit/dim 1.1745(best: 1.1721), Xent 0.0464, Loss 1.1978, Error 0.0154(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1163 | Time 35.1670(34.1131) | Bit/dim 1.1758(1.1814) | Xent 0.0987(0.0963) | Loss 1.2252(1.2296) | Error 0.0289(0.0297) Steps 452(443.54) | Grad Norm 0.2490(0.2957) | Total Time 10.00(10.00)\n",
      "Iter 1164 | Time 34.4223(34.1223) | Bit/dim 1.1841(1.1815) | Xent 0.1014(0.0964) | Loss 1.2348(1.2297) | Error 0.0309(0.0298) Steps 446(443.61) | Grad Norm 0.2505(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 1165 | Time 34.6544(34.1383) | Bit/dim 1.1795(1.1814) | Xent 0.1003(0.0965) | Loss 1.2296(1.2297) | Error 0.0300(0.0298) Steps 446(443.68) | Grad Norm 0.3888(0.2972) | Total Time 10.00(10.00)\n",
      "Iter 1166 | Time 35.5551(34.1808) | Bit/dim 1.1785(1.1814) | Xent 0.1027(0.0967) | Loss 1.2299(1.2297) | Error 0.0316(0.0298) Steps 446(443.75) | Grad Norm 0.3187(0.2978) | Total Time 10.00(10.00)\n",
      "Iter 1167 | Time 34.5250(34.1911) | Bit/dim 1.1835(1.1814) | Xent 0.0980(0.0968) | Loss 1.2325(1.2298) | Error 0.0286(0.0298) Steps 446(443.82) | Grad Norm 0.3385(0.2991) | Total Time 10.00(10.00)\n",
      "Iter 1168 | Time 34.5749(34.2027) | Bit/dim 1.1780(1.1813) | Xent 0.0976(0.0968) | Loss 1.2268(1.2297) | Error 0.0291(0.0298) Steps 446(443.88) | Grad Norm 0.2341(0.2971) | Total Time 10.00(10.00)\n",
      "Iter 1169 | Time 34.7106(34.2179) | Bit/dim 1.1836(1.1814) | Xent 0.0897(0.0966) | Loss 1.2284(1.2297) | Error 0.0286(0.0298) Steps 452(444.13) | Grad Norm 0.2847(0.2967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 16.6283, Epoch Time 272.7055(250.2497), Bit/dim 1.1747(best: 1.1721), Xent 0.0472, Loss 1.1983, Error 0.0147(best: 0.0148)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1170 | Time 34.0467(34.2128) | Bit/dim 1.1812(1.1814) | Xent 0.0923(0.0964) | Loss 1.2274(1.2296) | Error 0.0291(0.0297) Steps 452(444.36) | Grad Norm 0.2724(0.2960) | Total Time 10.00(10.00)\n",
      "Iter 1171 | Time 35.2874(34.2450) | Bit/dim 1.1824(1.1814) | Xent 0.0851(0.0961) | Loss 1.2250(1.2295) | Error 0.0252(0.0296) Steps 446(444.41) | Grad Norm 0.2968(0.2960) | Total Time 10.00(10.00)\n",
      "Iter 1172 | Time 34.1503(34.2422) | Bit/dim 1.1797(1.1814) | Xent 0.0826(0.0957) | Loss 1.2210(1.2292) | Error 0.0264(0.0295) Steps 452(444.64) | Grad Norm 0.3013(0.2962) | Total Time 10.00(10.00)\n",
      "Iter 1173 | Time 34.7384(34.2570) | Bit/dim 1.1802(1.1813) | Xent 0.0977(0.0958) | Loss 1.2290(1.2292) | Error 0.0298(0.0295) Steps 446(444.68) | Grad Norm 0.2921(0.2961) | Total Time 10.00(10.00)\n",
      "Iter 1174 | Time 34.5754(34.2666) | Bit/dim 1.1776(1.1812) | Xent 0.0945(0.0957) | Loss 1.2249(1.2291) | Error 0.0285(0.0295) Steps 452(444.90) | Grad Norm 0.3544(0.2978) | Total Time 10.00(10.00)\n",
      "Iter 1175 | Time 34.3625(34.2695) | Bit/dim 1.1709(1.1809) | Xent 0.0995(0.0958) | Loss 1.2206(1.2288) | Error 0.0302(0.0295) Steps 452(445.11) | Grad Norm 0.1720(0.2940) | Total Time 10.00(10.00)\n",
      "Iter 1176 | Time 34.3566(34.2721) | Bit/dim 1.1854(1.1810) | Xent 0.0923(0.0957) | Loss 1.2316(1.2289) | Error 0.0269(0.0294) Steps 446(445.14) | Grad Norm 0.3594(0.2960) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 16.8445, Epoch Time 270.7082(250.8635), Bit/dim 1.1741(best: 1.1721), Xent 0.0473, Loss 1.1977, Error 0.0153(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1177 | Time 34.5274(34.2797) | Bit/dim 1.1801(1.1810) | Xent 0.0935(0.0957) | Loss 1.2268(1.2288) | Error 0.0284(0.0294) Steps 446(445.17) | Grad Norm 0.4208(0.2997) | Total Time 10.00(10.00)\n",
      "Iter 1178 | Time 34.0550(34.2730) | Bit/dim 1.1814(1.1810) | Xent 0.0850(0.0953) | Loss 1.2239(1.2287) | Error 0.0249(0.0293) Steps 446(445.19) | Grad Norm 0.4461(0.3041) | Total Time 10.00(10.00)\n",
      "Iter 1179 | Time 33.9316(34.2628) | Bit/dim 1.1789(1.1810) | Xent 0.1011(0.0955) | Loss 1.2294(1.2287) | Error 0.0330(0.0294) Steps 446(445.22) | Grad Norm 0.5629(0.3119) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 34.7048(34.2760) | Bit/dim 1.1783(1.1809) | Xent 0.0970(0.0956) | Loss 1.2268(1.2287) | Error 0.0296(0.0294) Steps 446(445.24) | Grad Norm 0.5210(0.3182) | Total Time 10.00(10.00)\n",
      "Iter 1181 | Time 34.3013(34.2768) | Bit/dim 1.1818(1.1809) | Xent 0.0849(0.0952) | Loss 1.2243(1.2285) | Error 0.0284(0.0293) Steps 452(445.44) | Grad Norm 0.4634(0.3225) | Total Time 10.00(10.00)\n",
      "Iter 1182 | Time 34.9770(34.2978) | Bit/dim 1.1798(1.1809) | Xent 0.0941(0.0952) | Loss 1.2269(1.2285) | Error 0.0282(0.0293) Steps 452(445.64) | Grad Norm 0.3627(0.3237) | Total Time 10.00(10.00)\n",
      "Iter 1183 | Time 35.8771(34.3452) | Bit/dim 1.1808(1.1809) | Xent 0.1072(0.0956) | Loss 1.2344(1.2287) | Error 0.0337(0.0294) Steps 446(445.65) | Grad Norm 0.5122(0.3294) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 16.5458, Epoch Time 271.1505(251.4721), Bit/dim 1.1746(best: 1.1721), Xent 0.0454, Loss 1.1973, Error 0.0161(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1184 | Time 33.5221(34.3205) | Bit/dim 1.1796(1.1808) | Xent 0.0980(0.0956) | Loss 1.2286(1.2287) | Error 0.0320(0.0295) Steps 452(445.84) | Grad Norm 0.4892(0.3342) | Total Time 10.00(10.00)\n",
      "Iter 1185 | Time 35.2877(34.3495) | Bit/dim 1.1810(1.1808) | Xent 0.1056(0.0959) | Loss 1.2338(1.2288) | Error 0.0321(0.0296) Steps 452(446.02) | Grad Norm 0.6398(0.3433) | Total Time 10.00(10.00)\n",
      "Iter 1186 | Time 34.3723(34.3502) | Bit/dim 1.1810(1.1808) | Xent 0.0933(0.0959) | Loss 1.2276(1.2288) | Error 0.0285(0.0296) Steps 452(446.20) | Grad Norm 0.2839(0.3416) | Total Time 10.00(10.00)\n",
      "Iter 1187 | Time 33.6700(34.3298) | Bit/dim 1.1756(1.1807) | Xent 0.0832(0.0955) | Loss 1.2172(1.2284) | Error 0.0254(0.0294) Steps 446(446.20) | Grad Norm 0.4955(0.3462) | Total Time 10.00(10.00)\n",
      "Iter 1188 | Time 34.0006(34.3199) | Bit/dim 1.1849(1.1808) | Xent 0.0840(0.0951) | Loss 1.2269(1.2284) | Error 0.0275(0.0294) Steps 446(446.19) | Grad Norm 0.3328(0.3458) | Total Time 10.00(10.00)\n",
      "Iter 1189 | Time 34.1420(34.3145) | Bit/dim 1.1808(1.1808) | Xent 0.1041(0.0954) | Loss 1.2328(1.2285) | Error 0.0321(0.0295) Steps 446(446.19) | Grad Norm 0.3531(0.3460) | Total Time 10.00(10.00)\n",
      "Iter 1190 | Time 33.7035(34.2962) | Bit/dim 1.1797(1.1808) | Xent 0.1022(0.0956) | Loss 1.2308(1.2286) | Error 0.0311(0.0295) Steps 446(446.18) | Grad Norm 0.2720(0.3438) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 16.6346, Epoch Time 267.5534(251.9545), Bit/dim 1.1747(best: 1.1721), Xent 0.0482, Loss 1.1988, Error 0.0161(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1191 | Time 35.0268(34.3181) | Bit/dim 1.1760(1.1806) | Xent 0.1013(0.0958) | Loss 1.2266(1.2285) | Error 0.0298(0.0295) Steps 446(446.18) | Grad Norm 0.2710(0.3416) | Total Time 10.00(10.00)\n",
      "Iter 1192 | Time 34.2491(34.3161) | Bit/dim 1.1816(1.1807) | Xent 0.1038(0.0960) | Loss 1.2335(1.2287) | Error 0.0321(0.0296) Steps 446(446.17) | Grad Norm 0.3497(0.3418) | Total Time 10.00(10.00)\n",
      "Iter 1193 | Time 34.5780(34.3239) | Bit/dim 1.1832(1.1807) | Xent 0.0972(0.0961) | Loss 1.2318(1.2288) | Error 0.0316(0.0297) Steps 452(446.34) | Grad Norm 0.2273(0.3384) | Total Time 10.00(10.00)\n",
      "Iter 1194 | Time 34.3307(34.3241) | Bit/dim 1.1842(1.1808) | Xent 0.1014(0.0962) | Loss 1.2349(1.2290) | Error 0.0300(0.0297) Steps 452(446.51) | Grad Norm 0.2402(0.3355) | Total Time 10.00(10.00)\n",
      "Iter 1195 | Time 34.4651(34.3284) | Bit/dim 1.1828(1.1809) | Xent 0.0872(0.0959) | Loss 1.2264(1.2289) | Error 0.0280(0.0296) Steps 446(446.50) | Grad Norm 0.3250(0.3351) | Total Time 10.00(10.00)\n",
      "Iter 1196 | Time 33.2967(34.2974) | Bit/dim 1.1765(1.1808) | Xent 0.0908(0.0958) | Loss 1.2219(1.2287) | Error 0.0296(0.0296) Steps 446(446.48) | Grad Norm 0.2939(0.3339) | Total Time 10.00(10.00)\n",
      "Iter 1197 | Time 34.5503(34.3050) | Bit/dim 1.1838(1.1809) | Xent 0.0956(0.0958) | Loss 1.2316(1.2288) | Error 0.0304(0.0296) Steps 446(446.47) | Grad Norm 0.3169(0.3334) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 16.6172, Epoch Time 269.3531(252.4765), Bit/dim 1.1750(best: 1.1721), Xent 0.0468, Loss 1.1984, Error 0.0164(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1198 | Time 34.6323(34.3148) | Bit/dim 1.1852(1.1810) | Xent 0.1024(0.0960) | Loss 1.2364(1.2290) | Error 0.0327(0.0297) Steps 446(446.46) | Grad Norm 0.3145(0.3328) | Total Time 10.00(10.00)\n",
      "Iter 1199 | Time 36.0500(34.3669) | Bit/dim 1.1780(1.1809) | Xent 0.1019(0.0962) | Loss 1.2290(1.2290) | Error 0.0317(0.0298) Steps 452(446.62) | Grad Norm 0.4699(0.3369) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 33.9310(34.3538) | Bit/dim 1.1831(1.1810) | Xent 0.0895(0.0960) | Loss 1.2279(1.2289) | Error 0.0269(0.0297) Steps 446(446.60) | Grad Norm 0.2426(0.3341) | Total Time 10.00(10.00)\n",
      "Iter 1201 | Time 33.5801(34.3306) | Bit/dim 1.1859(1.1811) | Xent 0.0973(0.0960) | Loss 1.2345(1.2291) | Error 0.0286(0.0297) Steps 446(446.59) | Grad Norm 0.3270(0.3339) | Total Time 10.00(10.00)\n",
      "Iter 1202 | Time 33.5073(34.3059) | Bit/dim 1.1769(1.1810) | Xent 0.0984(0.0961) | Loss 1.2261(1.2290) | Error 0.0298(0.0297) Steps 446(446.57) | Grad Norm 0.3005(0.3329) | Total Time 10.00(10.00)\n",
      "Iter 1203 | Time 33.7154(34.2882) | Bit/dim 1.1833(1.1811) | Xent 0.0960(0.0961) | Loss 1.2313(1.2291) | Error 0.0281(0.0296) Steps 446(446.55) | Grad Norm 0.4000(0.3349) | Total Time 10.00(10.00)\n",
      "Iter 1204 | Time 34.1419(34.2838) | Bit/dim 1.1771(1.1809) | Xent 0.0949(0.0960) | Loss 1.2245(1.2290) | Error 0.0295(0.0296) Steps 440(446.35) | Grad Norm 0.2937(0.3337) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 16.5643, Epoch Time 268.3809(252.9536), Bit/dim 1.1762(best: 1.1721), Xent 0.0486, Loss 1.2004, Error 0.0160(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1205 | Time 33.3775(34.2566) | Bit/dim 1.1815(1.1810) | Xent 0.0878(0.0958) | Loss 1.2254(1.2288) | Error 0.0252(0.0295) Steps 440(446.16) | Grad Norm 0.3817(0.3351) | Total Time 10.00(10.00)\n",
      "Iter 1206 | Time 33.5079(34.2341) | Bit/dim 1.1804(1.1809) | Xent 0.0932(0.0957) | Loss 1.2270(1.2288) | Error 0.0292(0.0295) Steps 446(446.16) | Grad Norm 0.4477(0.3385) | Total Time 10.00(10.00)\n",
      "Iter 1207 | Time 34.4665(34.2411) | Bit/dim 1.1846(1.1811) | Xent 0.0995(0.0958) | Loss 1.2344(1.2290) | Error 0.0308(0.0295) Steps 440(445.97) | Grad Norm 0.3702(0.3394) | Total Time 10.00(10.00)\n",
      "Iter 1208 | Time 34.1852(34.2394) | Bit/dim 1.1832(1.1811) | Xent 0.0873(0.0956) | Loss 1.2269(1.2289) | Error 0.0289(0.0295) Steps 440(445.79) | Grad Norm 0.2309(0.3362) | Total Time 10.00(10.00)\n",
      "Iter 1209 | Time 33.8601(34.2280) | Bit/dim 1.1847(1.1812) | Xent 0.1071(0.0959) | Loss 1.2382(1.2292) | Error 0.0305(0.0295) Steps 440(445.62) | Grad Norm 0.5391(0.3423) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 34.1309(34.2251) | Bit/dim 1.1815(1.1812) | Xent 0.0883(0.0957) | Loss 1.2257(1.2291) | Error 0.0271(0.0295) Steps 446(445.63) | Grad Norm 0.2748(0.3403) | Total Time 10.00(10.00)\n",
      "Iter 1211 | Time 36.9754(34.3076) | Bit/dim 1.1824(1.1813) | Xent 0.0977(0.0957) | Loss 1.2313(1.2291) | Error 0.0290(0.0295) Steps 446(445.64) | Grad Norm 0.6972(0.3510) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 16.2604, Epoch Time 269.0531(253.4366), Bit/dim 1.1760(best: 1.1721), Xent 0.0460, Loss 1.1990, Error 0.0158(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1212 | Time 34.6722(34.3186) | Bit/dim 1.1839(1.1813) | Xent 0.0926(0.0957) | Loss 1.2302(1.2292) | Error 0.0272(0.0294) Steps 446(445.65) | Grad Norm 0.4508(0.3540) | Total Time 10.00(10.00)\n",
      "Iter 1213 | Time 34.6032(34.3271) | Bit/dim 1.1786(1.1813) | Xent 0.1017(0.0958) | Loss 1.2295(1.2292) | Error 0.0290(0.0294) Steps 446(445.66) | Grad Norm 0.4311(0.3563) | Total Time 10.00(10.00)\n",
      "Iter 1214 | Time 34.7300(34.3392) | Bit/dim 1.1809(1.1813) | Xent 0.0930(0.0957) | Loss 1.2275(1.2291) | Error 0.0280(0.0293) Steps 446(445.67) | Grad Norm 0.4551(0.3592) | Total Time 10.00(10.00)\n",
      "Iter 1215 | Time 34.3343(34.3391) | Bit/dim 1.1825(1.1813) | Xent 0.0964(0.0958) | Loss 1.2307(1.2292) | Error 0.0317(0.0294) Steps 446(445.68) | Grad Norm 0.3148(0.3579) | Total Time 10.00(10.00)\n",
      "Iter 1216 | Time 34.8625(34.3548) | Bit/dim 1.1820(1.1813) | Xent 0.0940(0.0957) | Loss 1.2290(1.2292) | Error 0.0280(0.0294) Steps 446(445.69) | Grad Norm 0.4277(0.3600) | Total Time 10.00(10.00)\n",
      "Iter 1217 | Time 35.3967(34.3860) | Bit/dim 1.1847(1.1814) | Xent 0.0957(0.0957) | Loss 1.2325(1.2293) | Error 0.0288(0.0293) Steps 446(445.70) | Grad Norm 0.3305(0.3591) | Total Time 10.00(10.00)\n",
      "Iter 1218 | Time 34.3304(34.3843) | Bit/dim 1.1796(1.1814) | Xent 0.0971(0.0958) | Loss 1.2281(1.2292) | Error 0.0281(0.0293) Steps 446(445.71) | Grad Norm 0.6016(0.3664) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 16.6830, Epoch Time 271.7981(253.9875), Bit/dim 1.1743(best: 1.1721), Xent 0.0465, Loss 1.1976, Error 0.0153(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1219 | Time 35.5875(34.4204) | Bit/dim 1.1854(1.1815) | Xent 0.0939(0.0957) | Loss 1.2323(1.2293) | Error 0.0282(0.0293) Steps 446(445.72) | Grad Norm 0.5419(0.3717) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 34.8416(34.4331) | Bit/dim 1.1812(1.1815) | Xent 0.1050(0.0960) | Loss 1.2337(1.2295) | Error 0.0329(0.0294) Steps 446(445.73) | Grad Norm 0.6481(0.3799) | Total Time 10.00(10.00)\n",
      "Iter 1221 | Time 34.5535(34.4367) | Bit/dim 1.1801(1.1814) | Xent 0.0953(0.0960) | Loss 1.2278(1.2294) | Error 0.0295(0.0294) Steps 446(445.74) | Grad Norm 0.7319(0.3905) | Total Time 10.00(10.00)\n",
      "Iter 1222 | Time 34.3240(34.4333) | Bit/dim 1.1785(1.1813) | Xent 0.0874(0.0957) | Loss 1.2222(1.2292) | Error 0.0265(0.0293) Steps 440(445.56) | Grad Norm 0.4469(0.3922) | Total Time 10.00(10.00)\n",
      "Iter 1223 | Time 34.3327(34.4303) | Bit/dim 1.1765(1.1812) | Xent 0.0883(0.0955) | Loss 1.2206(1.2289) | Error 0.0289(0.0293) Steps 446(445.58) | Grad Norm 0.7019(0.4015) | Total Time 10.00(10.00)\n",
      "Iter 1224 | Time 34.9371(34.4455) | Bit/dim 1.1775(1.1811) | Xent 0.0872(0.0952) | Loss 1.2211(1.2287) | Error 0.0272(0.0292) Steps 446(445.59) | Grad Norm 0.5398(0.4056) | Total Time 10.00(10.00)\n",
      "Iter 1225 | Time 35.2129(34.4685) | Bit/dim 1.1821(1.1811) | Xent 0.0888(0.0950) | Loss 1.2266(1.2286) | Error 0.0286(0.0292) Steps 440(445.42) | Grad Norm 0.6454(0.4128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 16.5498, Epoch Time 272.7649(254.5508), Bit/dim 1.1738(best: 1.1721), Xent 0.0437, Loss 1.1957, Error 0.0149(best: 0.0147)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1226 | Time 34.0812(34.4569) | Bit/dim 1.1812(1.1811) | Xent 0.0857(0.0948) | Loss 1.2240(1.2285) | Error 0.0235(0.0290) Steps 440(445.26) | Grad Norm 0.5566(0.4171) | Total Time 10.00(10.00)\n",
      "Iter 1227 | Time 34.5917(34.4609) | Bit/dim 1.1798(1.1811) | Xent 0.0926(0.0947) | Loss 1.2261(1.2284) | Error 0.0312(0.0291) Steps 440(445.10) | Grad Norm 0.6743(0.4249) | Total Time 10.00(10.00)\n",
      "Iter 1228 | Time 34.2980(34.4561) | Bit/dim 1.1790(1.1810) | Xent 0.0965(0.0947) | Loss 1.2273(1.2284) | Error 0.0298(0.0291) Steps 446(445.13) | Grad Norm 0.2837(0.4206) | Total Time 10.00(10.00)\n",
      "Iter 1229 | Time 34.0633(34.4443) | Bit/dim 1.1814(1.1810) | Xent 0.0983(0.0949) | Loss 1.2306(1.2285) | Error 0.0305(0.0292) Steps 446(445.16) | Grad Norm 0.6523(0.4276) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 33.7950(34.4248) | Bit/dim 1.1765(1.1809) | Xent 0.1025(0.0951) | Loss 1.2278(1.2284) | Error 0.0333(0.0293) Steps 446(445.18) | Grad Norm 0.3819(0.4262) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1/epoch_125_checkpt.pth --seed 0 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
