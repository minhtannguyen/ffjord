{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 15080 | Time 25.5967(26.5441) | Bit/dim 3.5285(3.5432) | Xent 0.1060(0.1071) | Loss 3.5815(3.5967) | Error 0.0411(0.0370) Steps 1066(1079.03) | Grad Norm 1.2714(1.4821) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 26.6380(26.3611) | Bit/dim 3.5844(3.5437) | Xent 0.1176(0.1068) | Loss 3.6432(3.5971) | Error 0.0389(0.0364) Steps 1072(1077.19) | Grad Norm 1.7668(1.5063) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 25.7547(26.1366) | Bit/dim 3.5307(3.5411) | Xent 0.1189(0.1061) | Loss 3.5901(3.5941) | Error 0.0489(0.0368) Steps 1084(1074.29) | Grad Norm 1.5015(1.5416) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 25.7698(26.0054) | Bit/dim 3.5567(3.5429) | Xent 0.1185(0.1080) | Loss 3.6160(3.5969) | Error 0.0456(0.0378) Steps 1066(1071.40) | Grad Norm 1.7070(1.5488) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 25.5494(25.9300) | Bit/dim 3.5391(3.5428) | Xent 0.1178(0.1087) | Loss 3.5980(3.5972) | Error 0.0389(0.0380) Steps 1066(1070.88) | Grad Norm 1.5463(1.5433) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 133.9772, Epoch Time 1581.6275(1549.7358), Bit/dim 3.5537(best: inf), Xent 1.0663, Loss 4.0868, Error 0.2394(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 25.8563(25.9617) | Bit/dim 3.5608(3.5425) | Xent 0.1243(0.1083) | Loss 3.6230(3.5967) | Error 0.0411(0.0379) Steps 1048(1069.52) | Grad Norm 2.4083(1.5637) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 25.1152(25.8903) | Bit/dim 3.5373(3.5411) | Xent 0.0887(0.1053) | Loss 3.5816(3.5937) | Error 0.0289(0.0367) Steps 1060(1068.57) | Grad Norm 1.5207(1.5760) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 26.9887(25.9070) | Bit/dim 3.5662(3.5414) | Xent 0.0970(0.1035) | Loss 3.6146(3.5931) | Error 0.0400(0.0363) Steps 1060(1067.71) | Grad Norm 2.2571(1.5890) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 25.2121(25.8508) | Bit/dim 3.5001(3.5405) | Xent 0.1210(0.1054) | Loss 3.5606(3.5932) | Error 0.0433(0.0369) Steps 1060(1067.16) | Grad Norm 2.1742(1.5934) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 25.4876(25.8486) | Bit/dim 3.5429(3.5390) | Xent 0.1097(0.1064) | Loss 3.5977(3.5922) | Error 0.0367(0.0372) Steps 1054(1065.86) | Grad Norm 1.3414(1.5878) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 25.3723(25.7928) | Bit/dim 3.5501(3.5420) | Xent 0.1071(0.1056) | Loss 3.6036(3.5948) | Error 0.0378(0.0371) Steps 1066(1065.57) | Grad Norm 1.1879(1.6188) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 125.3831, Epoch Time 1562.7280(1550.1256), Bit/dim 3.5516(best: 3.5537), Xent 1.0539, Loss 4.0785, Error 0.2355(best: 0.2394)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 25.3619(25.8604) | Bit/dim 3.5288(3.5411) | Xent 0.0914(0.1035) | Loss 3.5745(3.5929) | Error 0.0278(0.0362) Steps 1060(1065.24) | Grad Norm 1.1670(1.5767) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 26.0925(25.8355) | Bit/dim 3.5210(3.5385) | Xent 0.1214(0.1047) | Loss 3.5817(3.5909) | Error 0.0411(0.0368) Steps 1078(1065.42) | Grad Norm 1.6862(1.5987) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 25.8911(25.8214) | Bit/dim 3.5524(3.5393) | Xent 0.1177(0.1042) | Loss 3.6112(3.5914) | Error 0.0367(0.0363) Steps 1060(1064.50) | Grad Norm 1.5992(1.5690) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 25.5791(25.7990) | Bit/dim 3.5401(3.5422) | Xent 0.1007(0.1024) | Loss 3.5905(3.5934) | Error 0.0300(0.0354) Steps 1066(1064.56) | Grad Norm 1.4613(1.5283) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 26.3120(25.8371) | Bit/dim 3.5681(3.5434) | Xent 0.1099(0.1038) | Loss 3.6231(3.5953) | Error 0.0344(0.0362) Steps 1054(1064.78) | Grad Norm 1.8931(1.5633) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 125.1939, Epoch Time 1565.1282(1550.5756), Bit/dim 3.5510(best: 3.5516), Xent 1.0608, Loss 4.0814, Error 0.2338(best: 0.2355)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 25.4025(25.8104) | Bit/dim 3.5499(3.5437) | Xent 0.1080(0.1064) | Loss 3.6039(3.5969) | Error 0.0411(0.0375) Steps 1072(1064.38) | Grad Norm 1.5821(1.5542) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 25.3968(25.6960) | Bit/dim 3.5255(3.5437) | Xent 0.0981(0.1056) | Loss 3.5746(3.5965) | Error 0.0344(0.0368) Steps 1060(1065.27) | Grad Norm 1.2063(1.4998) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 25.5676(25.7563) | Bit/dim 3.5535(3.5426) | Xent 0.1123(0.1047) | Loss 3.6096(3.5949) | Error 0.0344(0.0361) Steps 1060(1064.96) | Grad Norm 1.5680(1.4572) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 26.1058(25.7464) | Bit/dim 3.5140(3.5417) | Xent 0.1043(0.1034) | Loss 3.5661(3.5934) | Error 0.0311(0.0359) Steps 1090(1066.68) | Grad Norm 1.7528(1.4594) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 24.7611(25.7813) | Bit/dim 3.5170(3.5409) | Xent 0.1184(0.1023) | Loss 3.5762(3.5921) | Error 0.0433(0.0355) Steps 1090(1069.05) | Grad Norm 1.6288(1.4595) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 26.1213(25.8776) | Bit/dim 3.5459(3.5393) | Xent 0.1038(0.1035) | Loss 3.5978(3.5910) | Error 0.0311(0.0358) Steps 1060(1070.45) | Grad Norm 1.8303(1.5066) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 125.0925, Epoch Time 1560.9606(1550.8872), Bit/dim 3.5529(best: 3.5510), Xent 1.0987, Loss 4.1023, Error 0.2332(best: 0.2338)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 25.9775(25.8570) | Bit/dim 3.5757(3.5406) | Xent 0.1100(0.1009) | Loss 3.6307(3.5910) | Error 0.0356(0.0345) Steps 1072(1070.78) | Grad Norm 1.4992(1.4889) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 26.2982(25.8816) | Bit/dim 3.5465(3.5409) | Xent 0.0814(0.1020) | Loss 3.5872(3.5919) | Error 0.0322(0.0354) Steps 1066(1070.67) | Grad Norm 1.7079(1.4901) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 24.8435(25.7996) | Bit/dim 3.5674(3.5412) | Xent 0.1129(0.1038) | Loss 3.6239(3.5931) | Error 0.0322(0.0362) Steps 1072(1070.25) | Grad Norm 1.4050(1.6074) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 25.6492(25.7939) | Bit/dim 3.5339(3.5394) | Xent 0.1166(0.1046) | Loss 3.5922(3.5917) | Error 0.0411(0.0365) Steps 1060(1070.56) | Grad Norm 2.2464(1.6947) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 26.2428(25.8906) | Bit/dim 3.5714(3.5419) | Xent 0.1009(0.1048) | Loss 3.6218(3.5943) | Error 0.0333(0.0361) Steps 1090(1070.73) | Grad Norm 2.1115(1.7519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 125.9605, Epoch Time 1568.5680(1551.4176), Bit/dim 3.5526(best: 3.5510), Xent 1.0593, Loss 4.0822, Error 0.2357(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 26.6296(26.0315) | Bit/dim 3.5583(3.5406) | Xent 0.1047(0.1047) | Loss 3.6107(3.5930) | Error 0.0356(0.0363) Steps 1072(1069.87) | Grad Norm 1.8248(1.7192) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 25.7971(26.0312) | Bit/dim 3.5657(3.5403) | Xent 0.1013(0.1044) | Loss 3.6163(3.5925) | Error 0.0311(0.0363) Steps 1072(1070.28) | Grad Norm 1.4103(1.7328) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 25.0983(25.9334) | Bit/dim 3.5565(3.5399) | Xent 0.1065(0.1034) | Loss 3.6097(3.5916) | Error 0.0367(0.0360) Steps 1066(1070.38) | Grad Norm 1.2964(1.7365) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 26.1714(25.8796) | Bit/dim 3.5265(3.5383) | Xent 0.0996(0.1034) | Loss 3.5763(3.5900) | Error 0.0333(0.0364) Steps 1090(1068.60) | Grad Norm 1.7428(1.7850) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 25.5850(25.8635) | Bit/dim 3.5021(3.5385) | Xent 0.0942(0.1042) | Loss 3.5492(3.5905) | Error 0.0311(0.0367) Steps 1078(1066.51) | Grad Norm 1.3945(1.7936) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 25.8852(25.9035) | Bit/dim 3.5538(3.5412) | Xent 0.1276(0.1051) | Loss 3.6176(3.5938) | Error 0.0433(0.0373) Steps 1078(1067.89) | Grad Norm 2.4272(1.8316) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 126.1901, Epoch Time 1568.3057(1551.9242), Bit/dim 3.5522(best: 3.5510), Xent 1.0897, Loss 4.0971, Error 0.2390(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 26.1871(25.8922) | Bit/dim 3.5492(3.5406) | Xent 0.1064(0.1045) | Loss 3.6024(3.5929) | Error 0.0356(0.0368) Steps 1030(1065.37) | Grad Norm 1.2949(1.8545) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 25.5693(25.9229) | Bit/dim 3.5284(3.5413) | Xent 0.0873(0.1037) | Loss 3.5721(3.5931) | Error 0.0311(0.0367) Steps 1096(1068.19) | Grad Norm 1.7042(1.8895) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 26.4273(25.9640) | Bit/dim 3.5344(3.5414) | Xent 0.1166(0.1039) | Loss 3.5926(3.5934) | Error 0.0422(0.0364) Steps 1078(1067.77) | Grad Norm 2.3346(1.9078) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 25.8551(25.9451) | Bit/dim 3.5420(3.5423) | Xent 0.1053(0.1038) | Loss 3.5947(3.5942) | Error 0.0411(0.0367) Steps 1048(1067.03) | Grad Norm 1.5745(1.8792) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 26.5013(26.0273) | Bit/dim 3.5343(3.5405) | Xent 0.1058(0.1053) | Loss 3.5872(3.5931) | Error 0.0422(0.0375) Steps 1072(1069.35) | Grad Norm 2.0616(1.8769) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 126.1205, Epoch Time 1574.3827(1552.5980), Bit/dim 3.5511(best: 3.5510), Xent 1.0948, Loss 4.0985, Error 0.2356(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 25.5868(26.0385) | Bit/dim 3.5178(3.5434) | Xent 0.0974(0.1026) | Loss 3.5665(3.5947) | Error 0.0300(0.0365) Steps 1048(1069.88) | Grad Norm 1.8390(1.7982) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 27.2645(26.0811) | Bit/dim 3.5440(3.5431) | Xent 0.0743(0.1040) | Loss 3.5811(3.5951) | Error 0.0244(0.0373) Steps 1078(1070.23) | Grad Norm 1.5992(1.8066) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 25.2889(26.0328) | Bit/dim 3.5513(3.5455) | Xent 0.1014(0.1033) | Loss 3.6020(3.5971) | Error 0.0333(0.0370) Steps 1066(1070.77) | Grad Norm 1.7011(1.7780) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 25.4549(25.9685) | Bit/dim 3.5090(3.5413) | Xent 0.0871(0.1027) | Loss 3.5525(3.5927) | Error 0.0289(0.0366) Steps 1072(1070.78) | Grad Norm 1.7452(1.7835) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 25.1831(25.9247) | Bit/dim 3.5418(3.5387) | Xent 0.1048(0.1021) | Loss 3.5942(3.5898) | Error 0.0356(0.0361) Steps 1066(1071.29) | Grad Norm 1.9872(1.7120) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 25.6466(25.9620) | Bit/dim 3.5396(3.5404) | Xent 0.1202(0.1032) | Loss 3.5997(3.5920) | Error 0.0367(0.0364) Steps 1054(1070.43) | Grad Norm 1.4936(1.6762) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 127.4265, Epoch Time 1573.6104(1553.2284), Bit/dim 3.5496(best: 3.5510), Xent 1.0694, Loss 4.0843, Error 0.2356(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 25.9582(25.9416) | Bit/dim 3.5654(3.5449) | Xent 0.0942(0.1031) | Loss 3.6125(3.5965) | Error 0.0289(0.0361) Steps 1078(1070.35) | Grad Norm 1.8766(1.6753) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 25.2798(25.9202) | Bit/dim 3.5430(3.5427) | Xent 0.1023(0.1018) | Loss 3.5942(3.5936) | Error 0.0389(0.0356) Steps 1078(1069.81) | Grad Norm 2.2780(1.6572) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 25.9780(25.9112) | Bit/dim 3.5288(3.5420) | Xent 0.1047(0.1015) | Loss 3.5812(3.5927) | Error 0.0422(0.0359) Steps 1072(1070.28) | Grad Norm 2.2136(1.7040) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 26.2464(26.0310) | Bit/dim 3.5265(3.5401) | Xent 0.0965(0.1008) | Loss 3.5748(3.5905) | Error 0.0356(0.0353) Steps 1066(1072.03) | Grad Norm 1.8481(1.7422) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 26.5982(26.0929) | Bit/dim 3.5572(3.5378) | Xent 0.0867(0.1015) | Loss 3.6006(3.5886) | Error 0.0344(0.0352) Steps 1090(1072.08) | Grad Norm 1.2539(1.6454) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 128.3017, Epoch Time 1577.7832(1553.9650), Bit/dim 3.5496(best: 3.5496), Xent 1.0675, Loss 4.0834, Error 0.2332(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 26.4916(26.0534) | Bit/dim 3.5398(3.5396) | Xent 0.1032(0.1001) | Loss 3.5914(3.5897) | Error 0.0367(0.0346) Steps 1084(1072.55) | Grad Norm 1.6193(1.6052) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 25.7859(26.0545) | Bit/dim 3.5644(3.5428) | Xent 0.0925(0.1001) | Loss 3.6106(3.5929) | Error 0.0322(0.0341) Steps 1090(1071.46) | Grad Norm 1.8992(1.6784) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 26.4183(26.0742) | Bit/dim 3.5175(3.5392) | Xent 0.1102(0.1011) | Loss 3.5726(3.5898) | Error 0.0356(0.0340) Steps 1036(1070.57) | Grad Norm 1.9191(1.7434) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 26.3528(26.0850) | Bit/dim 3.5438(3.5379) | Xent 0.1026(0.1007) | Loss 3.5951(3.5882) | Error 0.0333(0.0341) Steps 1060(1070.27) | Grad Norm 2.2129(1.7180) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 25.4880(26.1404) | Bit/dim 3.5569(3.5388) | Xent 0.1497(0.0997) | Loss 3.6318(3.5887) | Error 0.0556(0.0339) Steps 1060(1068.63) | Grad Norm 1.5847(1.6615) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 26.2954(26.2070) | Bit/dim 3.5667(3.5406) | Xent 0.0935(0.1002) | Loss 3.6134(3.5907) | Error 0.0278(0.0342) Steps 1066(1069.78) | Grad Norm 1.4718(1.6025) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 126.1752, Epoch Time 1584.5684(1554.8831), Bit/dim 3.5515(best: 3.5496), Xent 1.0913, Loss 4.0972, Error 0.2375(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 26.2013(26.1300) | Bit/dim 3.5732(3.5407) | Xent 0.0775(0.0993) | Loss 3.6120(3.5904) | Error 0.0244(0.0339) Steps 1066(1068.68) | Grad Norm 1.2756(1.5562) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 25.7757(26.0927) | Bit/dim 3.5452(3.5416) | Xent 0.0848(0.1004) | Loss 3.5876(3.5918) | Error 0.0333(0.0349) Steps 1066(1069.14) | Grad Norm 1.8688(1.6118) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 26.0653(26.0403) | Bit/dim 3.5086(3.5408) | Xent 0.0963(0.1008) | Loss 3.5568(3.5912) | Error 0.0378(0.0353) Steps 1090(1069.78) | Grad Norm 1.0606(1.6317) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 25.4499(26.0768) | Bit/dim 3.5337(3.5419) | Xent 0.1163(0.0999) | Loss 3.5919(3.5918) | Error 0.0467(0.0350) Steps 1060(1070.48) | Grad Norm 1.9011(1.6339) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 25.7315(25.9633) | Bit/dim 3.5438(3.5387) | Xent 0.0923(0.1007) | Loss 3.5899(3.5890) | Error 0.0344(0.0350) Steps 1078(1068.26) | Grad Norm 2.1050(1.7203) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 127.5253, Epoch Time 1574.0748(1555.4589), Bit/dim 3.5495(best: 3.5496), Xent 1.0779, Loss 4.0884, Error 0.2345(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 26.4828(26.0073) | Bit/dim 3.5418(3.5379) | Xent 0.0812(0.1005) | Loss 3.5824(3.5882) | Error 0.0289(0.0345) Steps 1078(1066.86) | Grad Norm 1.4919(1.6974) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 25.5866(25.9976) | Bit/dim 3.5047(3.5384) | Xent 0.1099(0.0992) | Loss 3.5596(3.5880) | Error 0.0400(0.0346) Steps 1078(1069.56) | Grad Norm 2.7345(1.7190) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 25.9319(26.0005) | Bit/dim 3.5508(3.5386) | Xent 0.1143(0.1001) | Loss 3.6080(3.5886) | Error 0.0456(0.0351) Steps 1060(1068.83) | Grad Norm 1.9511(1.7912) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 26.4616(26.0287) | Bit/dim 3.5385(3.5412) | Xent 0.0859(0.0997) | Loss 3.5815(3.5911) | Error 0.0256(0.0354) Steps 1084(1069.30) | Grad Norm 1.3931(1.7898) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 25.5446(25.9971) | Bit/dim 3.5433(3.5418) | Xent 0.1102(0.0998) | Loss 3.5984(3.5917) | Error 0.0422(0.0351) Steps 1060(1068.10) | Grad Norm 2.8983(1.8312) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 25.6361(25.9618) | Bit/dim 3.5187(3.5401) | Xent 0.0925(0.1008) | Loss 3.5649(3.5905) | Error 0.0344(0.0359) Steps 1066(1069.54) | Grad Norm 1.9502(1.8155) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 127.0241, Epoch Time 1573.4593(1555.9989), Bit/dim 3.5512(best: 3.5495), Xent 1.1141, Loss 4.1083, Error 0.2378(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 25.2730(25.9430) | Bit/dim 3.5442(3.5408) | Xent 0.1019(0.0988) | Loss 3.5952(3.5902) | Error 0.0356(0.0345) Steps 1072(1070.07) | Grad Norm 1.9897(1.8194) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 25.9896(25.9715) | Bit/dim 3.5617(3.5424) | Xent 0.0898(0.0990) | Loss 3.6066(3.5918) | Error 0.0322(0.0346) Steps 1084(1070.00) | Grad Norm 1.7992(1.7854) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 25.4622(25.8972) | Bit/dim 3.5353(3.5400) | Xent 0.1074(0.1001) | Loss 3.5889(3.5900) | Error 0.0378(0.0351) Steps 1060(1068.09) | Grad Norm 1.7934(1.7627) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 26.0304(25.8706) | Bit/dim 3.5061(3.5405) | Xent 0.1175(0.0983) | Loss 3.5648(3.5897) | Error 0.0411(0.0348) Steps 1084(1068.18) | Grad Norm 2.5726(1.7670) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 25.9672(25.8613) | Bit/dim 3.5501(3.5404) | Xent 0.0891(0.0981) | Loss 3.5947(3.5895) | Error 0.0367(0.0348) Steps 1072(1068.98) | Grad Norm 1.6796(1.7958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 126.3305, Epoch Time 1565.7695(1556.2920), Bit/dim 3.5526(best: 3.5495), Xent 1.1212, Loss 4.1132, Error 0.2355(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.5018(25.9177) | Bit/dim 3.4931(3.5414) | Xent 0.1032(0.0978) | Loss 3.5447(3.5903) | Error 0.0378(0.0340) Steps 1096(1072.98) | Grad Norm 2.1562(1.7614) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 26.2579(25.9929) | Bit/dim 3.5505(3.5419) | Xent 0.0959(0.0977) | Loss 3.5984(3.5907) | Error 0.0344(0.0341) Steps 1072(1072.11) | Grad Norm 1.8060(1.7518) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 25.6502(26.0389) | Bit/dim 3.5388(3.5406) | Xent 0.1193(0.0998) | Loss 3.5984(3.5905) | Error 0.0422(0.0351) Steps 1072(1070.96) | Grad Norm 1.5940(1.7694) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 25.8122(26.0437) | Bit/dim 3.5359(3.5411) | Xent 0.0979(0.0992) | Loss 3.5849(3.5907) | Error 0.0389(0.0349) Steps 1078(1071.97) | Grad Norm 1.4205(1.7851) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 25.5454(26.1476) | Bit/dim 3.5043(3.5366) | Xent 0.0955(0.1001) | Loss 3.5520(3.5866) | Error 0.0378(0.0353) Steps 1072(1072.45) | Grad Norm 1.4300(1.7635) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 26.4094(26.1380) | Bit/dim 3.5483(3.5390) | Xent 0.1045(0.0996) | Loss 3.6005(3.5888) | Error 0.0333(0.0350) Steps 1060(1071.96) | Grad Norm 1.2626(1.7570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 127.5187, Epoch Time 1587.3902(1557.2249), Bit/dim 3.5505(best: 3.5495), Xent 1.1102, Loss 4.1057, Error 0.2319(best: 0.2332)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 25.4700(25.9881) | Bit/dim 3.5091(3.5378) | Xent 0.1041(0.0995) | Loss 3.5612(3.5876) | Error 0.0322(0.0349) Steps 1072(1070.54) | Grad Norm 1.3842(1.7173) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 25.6316(25.9509) | Bit/dim 3.5249(3.5402) | Xent 0.0915(0.0990) | Loss 3.5707(3.5897) | Error 0.0311(0.0348) Steps 1066(1072.93) | Grad Norm 1.5497(1.6735) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 26.5600(26.0147) | Bit/dim 3.5429(3.5402) | Xent 0.1114(0.0988) | Loss 3.5986(3.5896) | Error 0.0400(0.0344) Steps 1090(1071.01) | Grad Norm 1.3372(1.6177) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 26.4472(26.0743) | Bit/dim 3.5578(3.5388) | Xent 0.1063(0.1000) | Loss 3.6109(3.5888) | Error 0.0411(0.0353) Steps 1078(1072.11) | Grad Norm 1.3182(1.6237) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 26.1188(26.0385) | Bit/dim 3.5328(3.5380) | Xent 0.0825(0.1008) | Loss 3.5741(3.5885) | Error 0.0289(0.0356) Steps 1072(1072.18) | Grad Norm 1.8614(1.6723) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 127.1948, Epoch Time 1570.6955(1557.6291), Bit/dim 3.5479(best: 3.5495), Xent 1.0802, Loss 4.0880, Error 0.2394(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 25.5008(25.9368) | Bit/dim 3.5416(3.5389) | Xent 0.0696(0.1016) | Loss 3.5763(3.5897) | Error 0.0222(0.0356) Steps 1066(1071.50) | Grad Norm 1.6059(1.7092) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 26.1144(25.9370) | Bit/dim 3.4947(3.5351) | Xent 0.1265(0.1023) | Loss 3.5579(3.5862) | Error 0.0356(0.0357) Steps 1054(1070.18) | Grad Norm 1.8206(1.7176) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 26.3152(25.9129) | Bit/dim 3.5305(3.5376) | Xent 0.0842(0.1018) | Loss 3.5726(3.5885) | Error 0.0267(0.0355) Steps 1078(1069.73) | Grad Norm 1.3610(1.6962) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 26.1107(25.8783) | Bit/dim 3.5385(3.5363) | Xent 0.1130(0.1014) | Loss 3.5950(3.5870) | Error 0.0344(0.0353) Steps 1072(1069.22) | Grad Norm 1.7174(1.7099) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 26.1981(25.7796) | Bit/dim 3.5675(3.5392) | Xent 0.0902(0.1017) | Loss 3.6126(3.5900) | Error 0.0311(0.0354) Steps 1084(1070.38) | Grad Norm 2.0994(1.7219) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 25.6953(25.7459) | Bit/dim 3.5709(3.5422) | Xent 0.1030(0.1008) | Loss 3.6224(3.5926) | Error 0.0389(0.0349) Steps 1072(1069.23) | Grad Norm 2.0049(1.7826) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 126.6101, Epoch Time 1560.3443(1557.7105), Bit/dim 3.5493(best: 3.5479), Xent 1.0976, Loss 4.0981, Error 0.2379(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 26.4655(25.8568) | Bit/dim 3.5163(3.5395) | Xent 0.0887(0.1042) | Loss 3.5606(3.5916) | Error 0.0322(0.0356) Steps 1060(1069.84) | Grad Norm 2.3186(1.8185) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 26.4498(25.9431) | Bit/dim 3.5573(3.5401) | Xent 0.1121(0.1036) | Loss 3.6134(3.5919) | Error 0.0411(0.0354) Steps 1066(1071.61) | Grad Norm 1.9090(1.8139) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 26.3498(26.0184) | Bit/dim 3.5037(3.5405) | Xent 0.1102(0.1033) | Loss 3.5588(3.5921) | Error 0.0389(0.0360) Steps 1048(1071.76) | Grad Norm 1.7225(1.8410) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 25.7389(26.0630) | Bit/dim 3.5506(3.5424) | Xent 0.1145(0.1026) | Loss 3.6079(3.5937) | Error 0.0389(0.0355) Steps 1060(1072.58) | Grad Norm 1.7729(1.9333) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 27.2479(26.1971) | Bit/dim 3.5341(3.5413) | Xent 0.1136(0.1021) | Loss 3.5908(3.5924) | Error 0.0411(0.0355) Steps 1072(1072.56) | Grad Norm 2.2508(1.9956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 126.2299, Epoch Time 1591.2052(1558.7154), Bit/dim 3.5512(best: 3.5479), Xent 1.1405, Loss 4.1214, Error 0.2377(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 26.0759(26.2948) | Bit/dim 3.5193(3.5408) | Xent 0.0846(0.1014) | Loss 3.5616(3.5915) | Error 0.0278(0.0351) Steps 1072(1075.79) | Grad Norm 1.4178(1.9340) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 26.0725(26.2884) | Bit/dim 3.5216(3.5401) | Xent 0.0716(0.0993) | Loss 3.5574(3.5898) | Error 0.0233(0.0347) Steps 1084(1076.59) | Grad Norm 1.1486(1.8949) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 25.5876(26.1963) | Bit/dim 3.5313(3.5398) | Xent 0.1119(0.1001) | Loss 3.5873(3.5899) | Error 0.0422(0.0350) Steps 1066(1074.15) | Grad Norm 2.3959(1.9401) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 25.8301(26.1693) | Bit/dim 3.5294(3.5370) | Xent 0.1203(0.1006) | Loss 3.5896(3.5873) | Error 0.0489(0.0351) Steps 1078(1073.13) | Grad Norm 1.5657(1.9371) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 25.6627(26.2178) | Bit/dim 3.5589(3.5372) | Xent 0.1137(0.1005) | Loss 3.6157(3.5875) | Error 0.0356(0.0352) Steps 1078(1074.58) | Grad Norm 2.0262(1.9143) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 26.2851(26.2717) | Bit/dim 3.5667(3.5414) | Xent 0.1085(0.0994) | Loss 3.6209(3.5911) | Error 0.0433(0.0349) Steps 1078(1074.83) | Grad Norm 1.8157(1.8577) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 127.6860, Epoch Time 1589.4033(1559.6360), Bit/dim 3.5505(best: 3.5479), Xent 1.0985, Loss 4.0997, Error 0.2376(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 25.5009(26.1542) | Bit/dim 3.5464(3.5411) | Xent 0.1067(0.0986) | Loss 3.5997(3.5904) | Error 0.0389(0.0346) Steps 1060(1073.75) | Grad Norm 1.5528(1.7592) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 25.8458(26.0243) | Bit/dim 3.5323(3.5419) | Xent 0.1019(0.0986) | Loss 3.5832(3.5912) | Error 0.0322(0.0344) Steps 1060(1070.81) | Grad Norm 2.0247(1.7598) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 25.4745(25.9990) | Bit/dim 3.5329(3.5407) | Xent 0.0805(0.0994) | Loss 3.5732(3.5904) | Error 0.0311(0.0343) Steps 1072(1068.62) | Grad Norm 1.1356(1.7634) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 26.0779(26.0426) | Bit/dim 3.5167(3.5397) | Xent 0.1151(0.0990) | Loss 3.5742(3.5892) | Error 0.0478(0.0345) Steps 1060(1070.23) | Grad Norm 1.4449(1.7335) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 26.0931(26.0910) | Bit/dim 3.5114(3.5366) | Xent 0.0990(0.0983) | Loss 3.5609(3.5858) | Error 0.0344(0.0344) Steps 1054(1072.74) | Grad Norm 2.5067(1.7396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 127.3529, Epoch Time 1574.3959(1560.0788), Bit/dim 3.5499(best: 3.5479), Xent 1.1447, Loss 4.1223, Error 0.2388(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 26.2871(26.0779) | Bit/dim 3.5031(3.5369) | Xent 0.0874(0.0978) | Loss 3.5468(3.5858) | Error 0.0300(0.0343) Steps 1090(1075.23) | Grad Norm 1.9778(1.7319) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 26.1603(26.0607) | Bit/dim 3.5804(3.5361) | Xent 0.1029(0.0986) | Loss 3.6318(3.5854) | Error 0.0356(0.0347) Steps 1084(1073.59) | Grad Norm 1.7042(1.7159) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 26.4290(26.0900) | Bit/dim 3.5715(3.5395) | Xent 0.0892(0.0976) | Loss 3.6162(3.5883) | Error 0.0344(0.0340) Steps 1066(1070.98) | Grad Norm 1.9720(1.7304) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 26.1254(26.0504) | Bit/dim 3.5686(3.5396) | Xent 0.1180(0.0998) | Loss 3.6276(3.5895) | Error 0.0378(0.0346) Steps 1048(1069.32) | Grad Norm 3.6354(1.9630) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 25.6265(25.9280) | Bit/dim 3.5146(3.5393) | Xent 0.0710(0.0990) | Loss 3.5501(3.5889) | Error 0.0222(0.0341) Steps 1078(1068.42) | Grad Norm 1.3588(1.9792) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 26.6021(26.0169) | Bit/dim 3.5429(3.5385) | Xent 0.0875(0.0986) | Loss 3.5867(3.5878) | Error 0.0289(0.0344) Steps 1078(1070.51) | Grad Norm 1.6237(2.0358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 127.4918, Epoch Time 1574.8115(1560.5208), Bit/dim 3.5483(best: 3.5479), Xent 1.0894, Loss 4.0931, Error 0.2355(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 26.0039(26.0698) | Bit/dim 3.5461(3.5392) | Xent 0.0978(0.0963) | Loss 3.5950(3.5873) | Error 0.0344(0.0333) Steps 1072(1070.53) | Grad Norm 1.6913(1.9307) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 25.5345(26.0310) | Bit/dim 3.5526(3.5393) | Xent 0.1016(0.0976) | Loss 3.6034(3.5881) | Error 0.0378(0.0346) Steps 1054(1070.65) | Grad Norm 2.1227(1.9504) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 26.0309(26.0538) | Bit/dim 3.5585(3.5387) | Xent 0.1065(0.0987) | Loss 3.6117(3.5881) | Error 0.0344(0.0349) Steps 1066(1071.30) | Grad Norm 2.0947(1.9661) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 25.4946(26.0763) | Bit/dim 3.5304(3.5379) | Xent 0.1024(0.0985) | Loss 3.5816(3.5872) | Error 0.0356(0.0351) Steps 1078(1072.53) | Grad Norm 1.8215(1.9395) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 25.1504(26.1043) | Bit/dim 3.5597(3.5378) | Xent 0.0826(0.0998) | Loss 3.6010(3.5877) | Error 0.0244(0.0354) Steps 1066(1072.40) | Grad Norm 1.3507(1.9119) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 126.2335, Epoch Time 1581.4455(1561.1485), Bit/dim 3.5502(best: 3.5479), Xent 1.1340, Loss 4.1172, Error 0.2350(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 26.8368(26.0831) | Bit/dim 3.5478(3.5391) | Xent 0.0984(0.0991) | Loss 3.5970(3.5887) | Error 0.0344(0.0352) Steps 1084(1073.42) | Grad Norm 2.4247(1.9396) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 26.0641(26.0826) | Bit/dim 3.5082(3.5385) | Xent 0.0917(0.0987) | Loss 3.5540(3.5879) | Error 0.0267(0.0350) Steps 1078(1070.77) | Grad Norm 1.2707(1.8658) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 26.4769(26.1253) | Bit/dim 3.5380(3.5385) | Xent 0.0757(0.0967) | Loss 3.5759(3.5868) | Error 0.0222(0.0339) Steps 1072(1069.22) | Grad Norm 1.9102(1.8399) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 26.2081(26.1996) | Bit/dim 3.5177(3.5375) | Xent 0.0824(0.0978) | Loss 3.5589(3.5864) | Error 0.0311(0.0345) Steps 1066(1069.43) | Grad Norm 1.2457(1.8377) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 25.5809(26.2094) | Bit/dim 3.5339(3.5397) | Xent 0.1033(0.0974) | Loss 3.5855(3.5884) | Error 0.0389(0.0336) Steps 1096(1070.61) | Grad Norm 1.6132(1.7881) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 26.0964(26.2983) | Bit/dim 3.5031(3.5377) | Xent 0.0762(0.0988) | Loss 3.5412(3.5871) | Error 0.0200(0.0342) Steps 1078(1072.09) | Grad Norm 1.5213(1.7834) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 126.7699, Epoch Time 1590.2333(1562.0211), Bit/dim 3.5497(best: 3.5479), Xent 1.1107, Loss 4.1050, Error 0.2347(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 26.4442(26.2930) | Bit/dim 3.5163(3.5369) | Xent 0.0972(0.0984) | Loss 3.5649(3.5860) | Error 0.0322(0.0341) Steps 1078(1073.52) | Grad Norm 2.2660(1.8499) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 26.6683(26.3598) | Bit/dim 3.5108(3.5367) | Xent 0.0881(0.0978) | Loss 3.5548(3.5856) | Error 0.0289(0.0341) Steps 1096(1072.92) | Grad Norm 1.6777(1.8977) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 26.5501(26.4184) | Bit/dim 3.5571(3.5383) | Xent 0.1123(0.0976) | Loss 3.6133(3.5871) | Error 0.0422(0.0343) Steps 1096(1074.95) | Grad Norm 2.2376(1.9183) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 26.3172(26.4197) | Bit/dim 3.5634(3.5389) | Xent 0.0869(0.0993) | Loss 3.6068(3.5885) | Error 0.0300(0.0350) Steps 1066(1074.90) | Grad Norm 1.9814(2.0434) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 26.4656(26.3280) | Bit/dim 3.5536(3.5392) | Xent 0.1023(0.0995) | Loss 3.6048(3.5890) | Error 0.0378(0.0352) Steps 1066(1074.15) | Grad Norm 2.7061(2.0397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 126.8182, Epoch Time 1595.2652(1563.0184), Bit/dim 3.5495(best: 3.5479), Xent 1.1554, Loss 4.1272, Error 0.2354(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 25.3259(26.2669) | Bit/dim 3.5328(3.5401) | Xent 0.0801(0.0979) | Loss 3.5729(3.5891) | Error 0.0300(0.0340) Steps 1054(1075.41) | Grad Norm 1.9900(2.0174) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 25.9144(26.2093) | Bit/dim 3.5061(3.5371) | Xent 0.1010(0.0976) | Loss 3.5566(3.5859) | Error 0.0356(0.0337) Steps 1084(1074.39) | Grad Norm 1.6424(1.9398) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 26.5166(26.2587) | Bit/dim 3.5281(3.5385) | Xent 0.0952(0.0958) | Loss 3.5757(3.5864) | Error 0.0344(0.0333) Steps 1084(1075.40) | Grad Norm 2.1452(1.8975) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 25.8308(26.2112) | Bit/dim 3.5677(3.5367) | Xent 0.0785(0.0952) | Loss 3.6070(3.5843) | Error 0.0233(0.0325) Steps 1072(1073.96) | Grad Norm 1.6277(1.8927) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 26.6633(26.3107) | Bit/dim 3.5624(3.5383) | Xent 0.1008(0.0951) | Loss 3.6128(3.5859) | Error 0.0322(0.0328) Steps 1090(1075.46) | Grad Norm 2.9284(1.8820) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 25.2775(26.3352) | Bit/dim 3.5467(3.5383) | Xent 0.1030(0.0971) | Loss 3.5982(3.5868) | Error 0.0344(0.0338) Steps 1066(1074.37) | Grad Norm 2.6359(1.9190) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 126.0800, Epoch Time 1588.9947(1563.7977), Bit/dim 3.5510(best: 3.5479), Xent 1.1665, Loss 4.1342, Error 0.2390(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 26.3252(26.3831) | Bit/dim 3.5362(3.5352) | Xent 0.1152(0.0988) | Loss 3.5938(3.5846) | Error 0.0444(0.0343) Steps 1072(1074.71) | Grad Norm 1.7680(1.9476) | Total Time 14.00(14.00)\n",
      "Iter 16410 | Time 27.0574(26.4272) | Bit/dim 3.5617(3.5376) | Xent 0.0897(0.0989) | Loss 3.6065(3.5871) | Error 0.0322(0.0348) Steps 1084(1076.23) | Grad Norm 2.0461(1.9532) | Total Time 14.00(14.00)\n",
      "Iter 16420 | Time 25.6546(26.3833) | Bit/dim 3.5625(3.5386) | Xent 0.0814(0.0982) | Loss 3.6032(3.5877) | Error 0.0244(0.0344) Steps 1078(1076.62) | Grad Norm 1.3437(1.9247) | Total Time 14.00(14.00)\n",
      "Iter 16430 | Time 27.0416(26.3968) | Bit/dim 3.5607(3.5382) | Xent 0.0971(0.0981) | Loss 3.6093(3.5873) | Error 0.0378(0.0349) Steps 1090(1078.16) | Grad Norm 1.3278(1.9514) | Total Time 14.00(14.00)\n",
      "Iter 16440 | Time 26.9680(26.4705) | Bit/dim 3.5236(3.5392) | Xent 0.1161(0.1002) | Loss 3.5817(3.5893) | Error 0.0467(0.0358) Steps 1060(1078.32) | Grad Norm 3.7100(2.0742) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 126.6480, Epoch Time 1600.0507(1564.8853), Bit/dim 3.5499(best: 3.5479), Xent 1.1383, Loss 4.1190, Error 0.2348(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 26.0050(26.4179) | Bit/dim 3.5602(3.5405) | Xent 0.0749(0.0980) | Loss 3.5976(3.5895) | Error 0.0256(0.0347) Steps 1084(1079.35) | Grad Norm 2.0389(2.0356) | Total Time 14.00(14.00)\n",
      "Iter 16460 | Time 26.1439(26.4062) | Bit/dim 3.5724(3.5414) | Xent 0.0704(0.0969) | Loss 3.6076(3.5899) | Error 0.0233(0.0345) Steps 1072(1077.14) | Grad Norm 1.6852(2.0097) | Total Time 14.00(14.00)\n",
      "Iter 16470 | Time 26.4544(26.3537) | Bit/dim 3.5479(3.5426) | Xent 0.0827(0.0957) | Loss 3.5893(3.5905) | Error 0.0289(0.0342) Steps 1072(1075.65) | Grad Norm 1.3278(1.9509) | Total Time 14.00(14.00)\n",
      "Iter 16480 | Time 25.7992(26.2236) | Bit/dim 3.5273(3.5396) | Xent 0.0872(0.0966) | Loss 3.5709(3.5879) | Error 0.0311(0.0339) Steps 1072(1072.94) | Grad Norm 1.6540(1.9625) | Total Time 14.00(14.00)\n",
      "Iter 16490 | Time 25.1869(26.1627) | Bit/dim 3.4939(3.5364) | Xent 0.0914(0.0971) | Loss 3.5396(3.5850) | Error 0.0344(0.0344) Steps 1072(1073.47) | Grad Norm 1.4495(1.8928) | Total Time 14.00(14.00)\n",
      "Iter 16500 | Time 24.8925(26.0217) | Bit/dim 3.5145(3.5365) | Xent 0.1282(0.0986) | Loss 3.5786(3.5858) | Error 0.0567(0.0354) Steps 1066(1072.61) | Grad Norm 2.6356(1.9116) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 126.6010, Epoch Time 1577.1582(1565.2535), Bit/dim 3.5509(best: 3.5479), Xent 1.1342, Loss 4.1179, Error 0.2378(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 26.8400(26.0443) | Bit/dim 3.5516(3.5357) | Xent 0.1001(0.0979) | Loss 3.6017(3.5847) | Error 0.0411(0.0351) Steps 1084(1072.13) | Grad Norm 2.5712(1.9344) | Total Time 14.00(14.00)\n",
      "Iter 16520 | Time 26.7469(26.1588) | Bit/dim 3.5533(3.5372) | Xent 0.0882(0.0990) | Loss 3.5974(3.5867) | Error 0.0300(0.0354) Steps 1096(1073.68) | Grad Norm 1.9079(1.9693) | Total Time 14.00(14.00)\n",
      "Iter 16530 | Time 26.7764(26.2354) | Bit/dim 3.5218(3.5356) | Xent 0.1020(0.0998) | Loss 3.5728(3.5855) | Error 0.0411(0.0359) Steps 1084(1074.27) | Grad Norm 2.3337(2.0186) | Total Time 14.00(14.00)\n",
      "Iter 16540 | Time 25.7465(26.3479) | Bit/dim 3.5492(3.5379) | Xent 0.1083(0.1003) | Loss 3.6033(3.5881) | Error 0.0389(0.0363) Steps 1066(1075.44) | Grad Norm 1.9543(1.9691) | Total Time 14.00(14.00)\n",
      "Iter 16550 | Time 26.1123(26.2190) | Bit/dim 3.5463(3.5384) | Xent 0.1047(0.1001) | Loss 3.5986(3.5884) | Error 0.0333(0.0358) Steps 1078(1074.15) | Grad Norm 1.9182(1.9223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 126.3560, Epoch Time 1591.2187(1566.0324), Bit/dim 3.5485(best: 3.5479), Xent 1.1281, Loss 4.1125, Error 0.2359(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 25.9327(26.1952) | Bit/dim 3.5519(3.5401) | Xent 0.1063(0.1000) | Loss 3.6050(3.5900) | Error 0.0367(0.0360) Steps 1096(1074.37) | Grad Norm 2.3516(1.9833) | Total Time 14.00(14.00)\n",
      "Iter 16570 | Time 26.6580(26.2968) | Bit/dim 3.5543(3.5418) | Xent 0.1025(0.0982) | Loss 3.6055(3.5910) | Error 0.0344(0.0352) Steps 1066(1073.95) | Grad Norm 1.5439(1.9398) | Total Time 14.00(14.00)\n",
      "Iter 16580 | Time 27.0450(26.2872) | Bit/dim 3.5463(3.5385) | Xent 0.0881(0.0978) | Loss 3.5903(3.5874) | Error 0.0289(0.0344) Steps 1066(1072.44) | Grad Norm 1.4725(1.9020) | Total Time 14.00(14.00)\n",
      "Iter 16590 | Time 25.7500(26.2737) | Bit/dim 3.5345(3.5412) | Xent 0.1340(0.0980) | Loss 3.6015(3.5902) | Error 0.0500(0.0346) Steps 1060(1071.62) | Grad Norm 4.4618(1.9955) | Total Time 14.00(14.00)\n",
      "Iter 16600 | Time 26.2754(26.2093) | Bit/dim 3.5403(3.5397) | Xent 0.1147(0.0978) | Loss 3.5976(3.5887) | Error 0.0400(0.0340) Steps 1078(1071.23) | Grad Norm 1.6730(2.0185) | Total Time 14.00(14.00)\n",
      "Iter 16610 | Time 26.3545(26.1479) | Bit/dim 3.5182(3.5368) | Xent 0.0994(0.1005) | Loss 3.5678(3.5870) | Error 0.0378(0.0355) Steps 1090(1070.80) | Grad Norm 2.1471(2.2002) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 126.0453, Epoch Time 1584.8367(1566.5965), Bit/dim 3.5470(best: 3.5479), Xent 1.1120, Loss 4.1030, Error 0.2378(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 26.0876(26.2034) | Bit/dim 3.5757(3.5383) | Xent 0.1072(0.0974) | Loss 3.6293(3.5870) | Error 0.0322(0.0343) Steps 1096(1071.32) | Grad Norm 1.7857(2.0258) | Total Time 14.00(14.00)\n",
      "Iter 16630 | Time 26.2697(26.2502) | Bit/dim 3.5452(3.5384) | Xent 0.0690(0.0977) | Loss 3.5797(3.5872) | Error 0.0267(0.0345) Steps 1084(1072.24) | Grad Norm 1.4298(1.9889) | Total Time 14.00(14.00)\n",
      "Iter 16640 | Time 26.0495(26.2736) | Bit/dim 3.5277(3.5375) | Xent 0.0995(0.0981) | Loss 3.5774(3.5865) | Error 0.0367(0.0345) Steps 1060(1072.29) | Grad Norm 1.9207(2.0117) | Total Time 14.00(14.00)\n",
      "Iter 16650 | Time 26.6278(26.2919) | Bit/dim 3.5483(3.5371) | Xent 0.0890(0.0977) | Loss 3.5928(3.5860) | Error 0.0311(0.0346) Steps 1090(1074.30) | Grad Norm 1.3507(1.9158) | Total Time 14.00(14.00)\n",
      "Iter 16660 | Time 26.3382(26.2572) | Bit/dim 3.5211(3.5368) | Xent 0.0948(0.0975) | Loss 3.5685(3.5856) | Error 0.0356(0.0341) Steps 1078(1073.12) | Grad Norm 2.0913(1.8988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 126.4142, Epoch Time 1591.6246(1567.3474), Bit/dim 3.5467(best: 3.5470), Xent 1.1190, Loss 4.1062, Error 0.2358(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 26.2398(26.2512) | Bit/dim 3.5436(3.5361) | Xent 0.1025(0.0976) | Loss 3.5948(3.5849) | Error 0.0278(0.0338) Steps 1060(1073.75) | Grad Norm 1.2065(1.8632) | Total Time 14.00(14.00)\n",
      "Iter 16680 | Time 25.2655(26.2509) | Bit/dim 3.5700(3.5377) | Xent 0.0961(0.0973) | Loss 3.6181(3.5863) | Error 0.0378(0.0340) Steps 1036(1072.25) | Grad Norm 1.7602(1.8175) | Total Time 14.00(14.00)\n",
      "Iter 16690 | Time 26.5115(26.2505) | Bit/dim 3.5621(3.5375) | Xent 0.0965(0.0966) | Loss 3.6104(3.5858) | Error 0.0422(0.0337) Steps 1066(1071.98) | Grad Norm 1.6382(1.7891) | Total Time 14.00(14.00)\n",
      "Iter 16700 | Time 25.9863(26.2203) | Bit/dim 3.5373(3.5365) | Xent 0.0907(0.0961) | Loss 3.5826(3.5846) | Error 0.0311(0.0334) Steps 1072(1072.83) | Grad Norm 1.7128(1.7484) | Total Time 14.00(14.00)\n",
      "Iter 16710 | Time 25.5617(26.1962) | Bit/dim 3.5664(3.5384) | Xent 0.0841(0.0956) | Loss 3.6084(3.5862) | Error 0.0278(0.0334) Steps 1060(1073.24) | Grad Norm 1.8166(1.8671) | Total Time 14.00(14.00)\n",
      "Iter 16720 | Time 27.1289(26.2111) | Bit/dim 3.5148(3.5377) | Xent 0.0827(0.0955) | Loss 3.5561(3.5855) | Error 0.0289(0.0332) Steps 1090(1074.03) | Grad Norm 1.5006(1.9016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 126.7972, Epoch Time 1586.0831(1567.9095), Bit/dim 3.5489(best: 3.5467), Xent 1.1528, Loss 4.1253, Error 0.2366(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 27.2987(26.3213) | Bit/dim 3.5463(3.5373) | Xent 0.0821(0.0955) | Loss 3.5874(3.5850) | Error 0.0300(0.0336) Steps 1102(1075.31) | Grad Norm 1.9928(1.9443) | Total Time 14.00(14.00)\n",
      "Iter 16740 | Time 26.3369(26.3071) | Bit/dim 3.5505(3.5413) | Xent 0.0934(0.0957) | Loss 3.5972(3.5892) | Error 0.0344(0.0339) Steps 1072(1076.07) | Grad Norm 1.7424(1.9731) | Total Time 14.00(14.00)\n",
      "Iter 16750 | Time 26.0347(26.2814) | Bit/dim 3.5330(3.5412) | Xent 0.1149(0.0955) | Loss 3.5905(3.5890) | Error 0.0422(0.0346) Steps 1054(1075.77) | Grad Norm 2.1739(1.9516) | Total Time 14.00(14.00)\n",
      "Iter 16760 | Time 26.5274(26.3878) | Bit/dim 3.5003(3.5385) | Xent 0.0914(0.0956) | Loss 3.5460(3.5863) | Error 0.0278(0.0342) Steps 1084(1077.20) | Grad Norm 1.8746(1.9487) | Total Time 14.00(14.00)\n",
      "Iter 16770 | Time 25.8706(26.3316) | Bit/dim 3.5372(3.5349) | Xent 0.0763(0.0941) | Loss 3.5753(3.5819) | Error 0.0244(0.0336) Steps 1072(1076.08) | Grad Norm 1.4541(1.8670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 127.7673, Epoch Time 1595.9974(1568.7521), Bit/dim 3.5479(best: 3.5467), Xent 1.1745, Loss 4.1352, Error 0.2419(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 26.2213(26.2663) | Bit/dim 3.5578(3.5376) | Xent 0.1088(0.0937) | Loss 3.6122(3.5844) | Error 0.0444(0.0339) Steps 1072(1077.87) | Grad Norm 1.8605(1.8736) | Total Time 14.00(14.00)\n",
      "Iter 16790 | Time 25.9866(26.2966) | Bit/dim 3.5256(3.5358) | Xent 0.0993(0.0951) | Loss 3.5752(3.5834) | Error 0.0322(0.0343) Steps 1084(1077.56) | Grad Norm 1.8955(1.8966) | Total Time 14.00(14.00)\n",
      "Iter 16800 | Time 25.8633(26.3263) | Bit/dim 3.5776(3.5379) | Xent 0.0832(0.0941) | Loss 3.6192(3.5850) | Error 0.0311(0.0335) Steps 1078(1075.60) | Grad Norm 1.6624(1.9305) | Total Time 14.00(14.00)\n",
      "Iter 16810 | Time 25.8174(26.3310) | Bit/dim 3.5262(3.5372) | Xent 0.0939(0.0951) | Loss 3.5731(3.5848) | Error 0.0300(0.0337) Steps 1066(1076.11) | Grad Norm 1.9277(2.0004) | Total Time 14.00(14.00)\n",
      "Iter 16820 | Time 26.5250(26.3354) | Bit/dim 3.5532(3.5387) | Xent 0.1240(0.0980) | Loss 3.6152(3.5877) | Error 0.0422(0.0345) Steps 1084(1076.77) | Grad Norm 2.4630(1.9993) | Total Time 14.00(14.00)\n",
      "Iter 16830 | Time 26.4647(26.3018) | Bit/dim 3.5112(3.5360) | Xent 0.0793(0.0984) | Loss 3.5509(3.5853) | Error 0.0289(0.0350) Steps 1096(1076.81) | Grad Norm 1.8362(2.0509) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 127.9123, Epoch Time 1593.4062(1569.4917), Bit/dim 3.5482(best: 3.5467), Xent 1.1698, Loss 4.1331, Error 0.2359(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 26.1441(26.2725) | Bit/dim 3.5395(3.5362) | Xent 0.1126(0.0964) | Loss 3.5957(3.5844) | Error 0.0433(0.0344) Steps 1084(1074.61) | Grad Norm 1.7893(2.0455) | Total Time 14.00(14.00)\n",
      "Iter 16850 | Time 26.5818(26.3539) | Bit/dim 3.5249(3.5348) | Xent 0.0988(0.0941) | Loss 3.5743(3.5818) | Error 0.0256(0.0331) Steps 1072(1073.07) | Grad Norm 1.7152(1.9325) | Total Time 14.00(14.00)\n",
      "Iter 16860 | Time 26.1183(26.3585) | Bit/dim 3.5367(3.5334) | Xent 0.1016(0.0941) | Loss 3.5875(3.5804) | Error 0.0356(0.0336) Steps 1078(1071.84) | Grad Norm 1.9740(1.8715) | Total Time 14.00(14.00)\n",
      "Iter 16870 | Time 26.6095(26.3125) | Bit/dim 3.5171(3.5335) | Xent 0.0629(0.0924) | Loss 3.5485(3.5797) | Error 0.0178(0.0327) Steps 1078(1070.54) | Grad Norm 1.9173(1.8323) | Total Time 14.00(14.00)\n",
      "Iter 16880 | Time 26.3425(26.2787) | Bit/dim 3.5573(3.5369) | Xent 0.0811(0.0928) | Loss 3.5979(3.5832) | Error 0.0267(0.0330) Steps 1090(1071.50) | Grad Norm 1.5525(1.8663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 127.0279, Epoch Time 1591.1650(1570.1419), Bit/dim 3.5466(best: 3.5467), Xent 1.1498, Loss 4.1215, Error 0.2382(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 26.4964(26.2649) | Bit/dim 3.5574(3.5369) | Xent 0.0832(0.0925) | Loss 3.5990(3.5832) | Error 0.0300(0.0327) Steps 1078(1072.59) | Grad Norm 2.1711(1.9120) | Total Time 14.00(14.00)\n",
      "Iter 16900 | Time 27.3415(26.3169) | Bit/dim 3.5419(3.5371) | Xent 0.1108(0.0932) | Loss 3.5973(3.5836) | Error 0.0311(0.0328) Steps 1054(1070.54) | Grad Norm 1.9791(1.8883) | Total Time 14.00(14.00)\n",
      "Iter 16910 | Time 25.4661(26.3096) | Bit/dim 3.5340(3.5375) | Xent 0.0843(0.0939) | Loss 3.5761(3.5845) | Error 0.0300(0.0329) Steps 1066(1071.86) | Grad Norm 2.1033(1.8438) | Total Time 14.00(14.00)\n",
      "Iter 16920 | Time 25.6874(26.3443) | Bit/dim 3.5323(3.5385) | Xent 0.0956(0.0956) | Loss 3.5801(3.5863) | Error 0.0333(0.0330) Steps 1072(1073.15) | Grad Norm 1.3868(1.9031) | Total Time 14.00(14.00)\n",
      "Iter 16930 | Time 26.5016(26.3475) | Bit/dim 3.5390(3.5383) | Xent 0.0834(0.0957) | Loss 3.5807(3.5862) | Error 0.0300(0.0330) Steps 1090(1075.51) | Grad Norm 1.9713(1.9297) | Total Time 14.00(14.00)\n",
      "Iter 16940 | Time 26.0416(26.3322) | Bit/dim 3.5193(3.5374) | Xent 0.1093(0.0951) | Loss 3.5739(3.5849) | Error 0.0378(0.0327) Steps 1066(1076.39) | Grad Norm 1.3813(1.8407) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 127.6172, Epoch Time 1595.1848(1570.8932), Bit/dim 3.5467(best: 3.5466), Xent 1.1325, Loss 4.1130, Error 0.2381(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 26.7549(26.3414) | Bit/dim 3.5666(3.5361) | Xent 0.1167(0.0966) | Loss 3.6249(3.5845) | Error 0.0367(0.0329) Steps 1060(1076.01) | Grad Norm 2.9236(1.8412) | Total Time 14.00(14.00)\n",
      "Iter 16960 | Time 26.5318(26.3663) | Bit/dim 3.5412(3.5375) | Xent 0.0905(0.0960) | Loss 3.5865(3.5855) | Error 0.0333(0.0333) Steps 1072(1075.10) | Grad Norm 1.5166(1.8629) | Total Time 14.00(14.00)\n",
      "Iter 16970 | Time 25.9800(26.3570) | Bit/dim 3.5351(3.5366) | Xent 0.0801(0.0954) | Loss 3.5751(3.5843) | Error 0.0267(0.0335) Steps 1096(1076.90) | Grad Norm 1.2429(1.8323) | Total Time 14.00(14.00)\n",
      "Iter 16980 | Time 25.6626(26.2984) | Bit/dim 3.5192(3.5353) | Xent 0.0901(0.0952) | Loss 3.5643(3.5829) | Error 0.0389(0.0338) Steps 1078(1075.67) | Grad Norm 1.4533(1.9687) | Total Time 14.00(14.00)\n",
      "Iter 16990 | Time 25.6817(26.2399) | Bit/dim 3.5518(3.5380) | Xent 0.0972(0.0951) | Loss 3.6004(3.5856) | Error 0.0389(0.0341) Steps 1066(1073.95) | Grad Norm 1.6782(1.9513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 126.7031, Epoch Time 1589.6995(1571.4574), Bit/dim 3.5470(best: 3.5466), Xent 1.1636, Loss 4.1289, Error 0.2379(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 26.9621(26.2712) | Bit/dim 3.5549(3.5371) | Xent 0.0999(0.0940) | Loss 3.6048(3.5841) | Error 0.0400(0.0335) Steps 1078(1074.64) | Grad Norm 2.5400(1.9587) | Total Time 14.00(14.00)\n",
      "Iter 17010 | Time 26.4353(26.2651) | Bit/dim 3.5505(3.5350) | Xent 0.0846(0.0952) | Loss 3.5928(3.5826) | Error 0.0289(0.0338) Steps 1078(1075.01) | Grad Norm 1.8964(1.9806) | Total Time 14.00(14.00)\n",
      "Iter 17020 | Time 25.8924(26.2492) | Bit/dim 3.5594(3.5372) | Xent 0.0960(0.0966) | Loss 3.6074(3.5856) | Error 0.0322(0.0339) Steps 1078(1075.11) | Grad Norm 1.9544(2.0099) | Total Time 14.00(14.00)\n",
      "Iter 17030 | Time 25.6978(26.2114) | Bit/dim 3.5502(3.5388) | Xent 0.0872(0.0982) | Loss 3.5938(3.5879) | Error 0.0289(0.0348) Steps 1054(1075.05) | Grad Norm 1.7569(2.0942) | Total Time 14.00(14.00)\n",
      "Iter 17040 | Time 25.8336(26.1708) | Bit/dim 3.5234(3.5371) | Xent 0.1238(0.0988) | Loss 3.5853(3.5865) | Error 0.0422(0.0350) Steps 1066(1073.17) | Grad Norm 2.2890(2.0786) | Total Time 14.00(14.00)\n",
      "Iter 17050 | Time 26.4853(26.1227) | Bit/dim 3.5773(3.5375) | Xent 0.1100(0.0987) | Loss 3.6323(3.5869) | Error 0.0444(0.0353) Steps 1078(1073.87) | Grad Norm 2.2290(2.0389) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 127.0317, Epoch Time 1583.0832(1571.8062), Bit/dim 3.5496(best: 3.5466), Xent 1.1508, Loss 4.1249, Error 0.2391(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 26.5833(26.1505) | Bit/dim 3.5147(3.5372) | Xent 0.0822(0.0979) | Loss 3.5558(3.5862) | Error 0.0211(0.0344) Steps 1072(1073.30) | Grad Norm 1.7334(1.9585) | Total Time 14.00(14.00)\n",
      "Iter 17070 | Time 26.0993(26.1707) | Bit/dim 3.5440(3.5368) | Xent 0.0850(0.0965) | Loss 3.5865(3.5850) | Error 0.0278(0.0337) Steps 1066(1071.11) | Grad Norm 1.3249(1.9039) | Total Time 14.00(14.00)\n",
      "Iter 17080 | Time 26.1833(26.2800) | Bit/dim 3.5505(3.5344) | Xent 0.0932(0.0963) | Loss 3.5971(3.5826) | Error 0.0389(0.0339) Steps 1060(1072.73) | Grad Norm 2.0771(1.8124) | Total Time 14.00(14.00)\n",
      "Iter 17090 | Time 26.6370(26.2578) | Bit/dim 3.4944(3.5344) | Xent 0.0854(0.0959) | Loss 3.5371(3.5823) | Error 0.0256(0.0334) Steps 1066(1072.75) | Grad Norm 1.8058(1.8924) | Total Time 14.00(14.00)\n",
      "Iter 17100 | Time 26.9755(26.3291) | Bit/dim 3.5254(3.5354) | Xent 0.1105(0.0934) | Loss 3.5807(3.5822) | Error 0.0400(0.0326) Steps 1072(1073.83) | Grad Norm 2.5063(1.9215) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 127.5417, Epoch Time 1596.8461(1572.5574), Bit/dim 3.5480(best: 3.5466), Xent 1.1760, Loss 4.1360, Error 0.2396(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 26.4181(26.3737) | Bit/dim 3.5442(3.5375) | Xent 0.0955(0.0931) | Loss 3.5919(3.5840) | Error 0.0289(0.0331) Steps 1066(1074.67) | Grad Norm 2.1370(1.9176) | Total Time 14.00(14.00)\n",
      "Iter 17120 | Time 26.6120(26.4109) | Bit/dim 3.5675(3.5387) | Xent 0.0855(0.0926) | Loss 3.6103(3.5850) | Error 0.0311(0.0325) Steps 1066(1073.79) | Grad Norm 2.0366(1.9582) | Total Time 14.00(14.00)\n",
      "Iter 17130 | Time 25.3783(26.3848) | Bit/dim 3.5670(3.5398) | Xent 0.1304(0.0955) | Loss 3.6322(3.5875) | Error 0.0478(0.0334) Steps 1078(1073.94) | Grad Norm 2.1092(1.9420) | Total Time 14.00(14.00)\n",
      "Iter 17140 | Time 26.4135(26.4246) | Bit/dim 3.5081(3.5375) | Xent 0.1092(0.0969) | Loss 3.5627(3.5860) | Error 0.0433(0.0345) Steps 1072(1073.62) | Grad Norm 2.7354(1.9815) | Total Time 14.00(14.00)\n",
      "Iter 17150 | Time 25.3995(26.4116) | Bit/dim 3.5295(3.5372) | Xent 0.1018(0.0960) | Loss 3.5804(3.5852) | Error 0.0367(0.0337) Steps 1048(1072.06) | Grad Norm 2.5260(1.9843) | Total Time 14.00(14.00)\n",
      "Iter 17160 | Time 26.3243(26.4298) | Bit/dim 3.5345(3.5357) | Xent 0.0989(0.0961) | Loss 3.5839(3.5838) | Error 0.0322(0.0341) Steps 1072(1072.31) | Grad Norm 1.5393(2.0103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 126.7208, Epoch Time 1599.0446(1573.3520), Bit/dim 3.5484(best: 3.5466), Xent 1.1805, Loss 4.1387, Error 0.2351(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 26.4996(26.4617) | Bit/dim 3.5636(3.5376) | Xent 0.0803(0.0943) | Loss 3.6037(3.5847) | Error 0.0311(0.0338) Steps 1066(1074.11) | Grad Norm 2.2893(2.0079) | Total Time 14.00(14.00)\n",
      "Iter 17180 | Time 26.6492(26.3871) | Bit/dim 3.5606(3.5397) | Xent 0.0906(0.0934) | Loss 3.6059(3.5864) | Error 0.0267(0.0334) Steps 1078(1073.39) | Grad Norm 1.9635(2.0288) | Total Time 14.00(14.00)\n",
      "Iter 17190 | Time 25.8773(26.4135) | Bit/dim 3.5753(3.5421) | Xent 0.0962(0.0940) | Loss 3.6234(3.5891) | Error 0.0311(0.0331) Steps 1078(1074.74) | Grad Norm 2.0744(2.0458) | Total Time 14.00(14.00)\n",
      "Iter 17200 | Time 26.3935(26.4438) | Bit/dim 3.5323(3.5384) | Xent 0.1106(0.0946) | Loss 3.5876(3.5857) | Error 0.0378(0.0333) Steps 1096(1076.55) | Grad Norm 2.2243(2.0518) | Total Time 14.00(14.00)\n",
      "Iter 17210 | Time 26.6721(26.4573) | Bit/dim 3.5358(3.5356) | Xent 0.0844(0.0955) | Loss 3.5780(3.5833) | Error 0.0300(0.0338) Steps 1066(1075.97) | Grad Norm 1.8982(2.2091) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 127.8704, Epoch Time 1600.3255(1574.1612), Bit/dim 3.5473(best: 3.5466), Xent 1.1988, Loss 4.1467, Error 0.2392(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 26.0748(26.4155) | Bit/dim 3.5391(3.5345) | Xent 0.0919(0.0945) | Loss 3.5851(3.5818) | Error 0.0367(0.0340) Steps 1084(1074.63) | Grad Norm 1.8316(2.1554) | Total Time 14.00(14.00)\n",
      "Iter 17230 | Time 26.7858(26.3912) | Bit/dim 3.5503(3.5357) | Xent 0.1004(0.0949) | Loss 3.6005(3.5831) | Error 0.0367(0.0342) Steps 1084(1075.13) | Grad Norm 1.8187(2.1024) | Total Time 14.00(14.00)\n",
      "Iter 17240 | Time 26.1203(26.3974) | Bit/dim 3.5273(3.5369) | Xent 0.1306(0.0957) | Loss 3.5926(3.5847) | Error 0.0422(0.0344) Steps 1072(1076.77) | Grad Norm 2.4215(2.1985) | Total Time 14.00(14.00)\n",
      "Iter 17250 | Time 25.5787(26.3488) | Bit/dim 3.5360(3.5348) | Xent 0.0890(0.0968) | Loss 3.5805(3.5832) | Error 0.0289(0.0347) Steps 1072(1076.66) | Grad Norm 1.2932(2.2276) | Total Time 14.00(14.00)\n",
      "Iter 17260 | Time 26.9906(26.3689) | Bit/dim 3.5163(3.5359) | Xent 0.0986(0.0968) | Loss 3.5656(3.5843) | Error 0.0344(0.0344) Steps 1042(1073.88) | Grad Norm 2.4689(2.2287) | Total Time 14.00(14.00)\n",
      "Iter 17270 | Time 26.2627(26.3749) | Bit/dim 3.5228(3.5354) | Xent 0.0922(0.0977) | Loss 3.5689(3.5843) | Error 0.0333(0.0344) Steps 1066(1073.41) | Grad Norm 2.4403(2.1695) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 127.0235, Epoch Time 1593.6116(1574.7447), Bit/dim 3.5484(best: 3.5466), Xent 1.1873, Loss 4.1420, Error 0.2361(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 26.4755(26.3534) | Bit/dim 3.5154(3.5351) | Xent 0.0876(0.0961) | Loss 3.5592(3.5831) | Error 0.0256(0.0337) Steps 1066(1072.76) | Grad Norm 1.7040(2.1944) | Total Time 14.00(14.00)\n",
      "Iter 17290 | Time 26.2625(26.3390) | Bit/dim 3.5357(3.5395) | Xent 0.0805(0.0941) | Loss 3.5759(3.5865) | Error 0.0311(0.0335) Steps 1066(1073.07) | Grad Norm 1.8650(2.0919) | Total Time 14.00(14.00)\n",
      "Iter 17300 | Time 26.1278(26.2714) | Bit/dim 3.5179(3.5376) | Xent 0.0869(0.0947) | Loss 3.5613(3.5849) | Error 0.0267(0.0336) Steps 1090(1072.20) | Grad Norm 1.9526(2.0749) | Total Time 14.00(14.00)\n",
      "Iter 17310 | Time 25.9930(26.2678) | Bit/dim 3.5375(3.5361) | Xent 0.0973(0.0929) | Loss 3.5862(3.5825) | Error 0.0344(0.0327) Steps 1054(1072.04) | Grad Norm 1.8921(2.0300) | Total Time 14.00(14.00)\n",
      "Iter 17320 | Time 25.8721(26.2132) | Bit/dim 3.5616(3.5358) | Xent 0.1061(0.0931) | Loss 3.6147(3.5824) | Error 0.0467(0.0330) Steps 1072(1072.51) | Grad Norm 2.4919(2.0200) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 127.3448, Epoch Time 1586.6817(1575.1028), Bit/dim 3.5459(best: 3.5466), Xent 1.2006, Loss 4.1463, Error 0.2401(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 26.2624(26.2048) | Bit/dim 3.5113(3.5340) | Xent 0.1015(0.0919) | Loss 3.5621(3.5799) | Error 0.0378(0.0327) Steps 1084(1074.82) | Grad Norm 1.3683(1.9451) | Total Time 14.00(14.00)\n",
      "Iter 17340 | Time 26.3717(26.2417) | Bit/dim 3.5659(3.5348) | Xent 0.0841(0.0926) | Loss 3.6080(3.5811) | Error 0.0311(0.0332) Steps 1072(1073.46) | Grad Norm 1.7039(1.9452) | Total Time 14.00(14.00)\n",
      "Iter 17350 | Time 27.0929(26.2795) | Bit/dim 3.5417(3.5351) | Xent 0.0844(0.0919) | Loss 3.5839(3.5811) | Error 0.0278(0.0324) Steps 1090(1073.72) | Grad Norm 2.0252(1.9151) | Total Time 14.00(14.00)\n",
      "Iter 17360 | Time 26.5870(26.3331) | Bit/dim 3.5291(3.5334) | Xent 0.0707(0.0920) | Loss 3.5644(3.5794) | Error 0.0233(0.0324) Steps 1042(1072.93) | Grad Norm 1.4213(1.9198) | Total Time 14.00(14.00)\n",
      "Iter 17370 | Time 26.4980(26.3198) | Bit/dim 3.5444(3.5346) | Xent 0.0885(0.0929) | Loss 3.5887(3.5811) | Error 0.0289(0.0328) Steps 1078(1072.21) | Grad Norm 1.3137(1.9110) | Total Time 14.00(14.00)\n",
      "Iter 17380 | Time 26.5221(26.3629) | Bit/dim 3.5654(3.5365) | Xent 0.0886(0.0936) | Loss 3.6097(3.5833) | Error 0.0267(0.0331) Steps 1084(1073.70) | Grad Norm 1.6527(1.9453) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 127.4666, Epoch Time 1595.6914(1575.7205), Bit/dim 3.5465(best: 3.5459), Xent 1.1722, Loss 4.1326, Error 0.2396(best: 0.2319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 26.3998(26.4039) | Bit/dim 3.5172(3.5353) | Xent 0.0954(0.0930) | Loss 3.5650(3.5818) | Error 0.0367(0.0325) Steps 1084(1074.01) | Grad Norm 1.7309(1.8980) | Total Time 14.00(14.00)\n",
      "Iter 17400 | Time 26.0099(26.3794) | Bit/dim 3.4997(3.5341) | Xent 0.0993(0.0920) | Loss 3.5493(3.5801) | Error 0.0333(0.0324) Steps 1072(1074.71) | Grad Norm 2.2097(1.8948) | Total Time 14.00(14.00)\n",
      "Iter 17410 | Time 26.6509(26.3518) | Bit/dim 3.5379(3.5344) | Xent 0.0977(0.0918) | Loss 3.5868(3.5803) | Error 0.0389(0.0325) Steps 1072(1073.30) | Grad Norm 1.6095(1.8811) | Total Time 14.00(14.00)\n",
      "Iter 17420 | Time 25.8723(26.3747) | Bit/dim 3.5267(3.5365) | Xent 0.1130(0.0927) | Loss 3.5831(3.5829) | Error 0.0300(0.0322) Steps 1066(1071.98) | Grad Norm 3.3865(1.9191) | Total Time 14.00(14.00)\n",
      "Iter 17430 | Time 26.5502(26.3716) | Bit/dim 3.5447(3.5382) | Xent 0.1206(0.0948) | Loss 3.6050(3.5856) | Error 0.0422(0.0335) Steps 1078(1073.92) | Grad Norm 3.0277(2.0283) | Total Time 14.00(14.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
