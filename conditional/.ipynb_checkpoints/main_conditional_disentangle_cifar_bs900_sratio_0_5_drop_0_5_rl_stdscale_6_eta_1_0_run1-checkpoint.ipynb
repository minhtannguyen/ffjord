{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=1.0, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.5875(33.6745) | Bit/dim 8.6893(8.9521) | Xent 2.2804(2.3001) | Loss 30.0711(30.5735) | Error 0.7956(0.8601) Steps 0(0.00) | Grad Norm 22.1317(28.2137) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.5498(28.1913) | Bit/dim 8.4871(8.8630) | Xent 2.2263(2.2873) | Loss 29.5221(30.3136) | Error 0.7300(0.8325) Steps 0(0.00) | Grad Norm 9.2050(24.4204) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 13.5750(24.2428) | Bit/dim 8.3930(8.7514) | Xent 2.1747(2.2634) | Loss 30.3287(30.0826) | Error 0.7567(0.8093) Steps 0(0.00) | Grad Norm 9.2289(20.0609) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.8945(21.2351) | Bit/dim 8.1916(8.6233) | Xent 2.1139(2.2346) | Loss 29.2665(29.8366) | Error 0.7256(0.7912) Steps 0(0.00) | Grad Norm 5.6850(16.4686) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.5041(19.0808) | Bit/dim 7.9736(8.4738) | Xent 2.1029(2.2021) | Loss 28.0125(29.5035) | Error 0.7067(0.7754) Steps 0(0.00) | Grad Norm 5.4840(13.6788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 82.4929, Epoch Time 843.6272(843.6272), Bit/dim 7.7698(best: inf), Xent 2.0785, Loss 8.8091, Error 0.7001(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.8994(17.5767) | Bit/dim 7.6771(8.2914) | Xent 2.0785(2.1721) | Loss 27.7623(34.7939) | Error 0.7033(0.7588) Steps 0(0.00) | Grad Norm 5.7181(11.5566) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.6611(16.4653) | Bit/dim 7.3717(8.0805) | Xent 2.0567(2.1462) | Loss 27.6434(32.9097) | Error 0.6733(0.7420) Steps 0(0.00) | Grad Norm 4.1307(9.7680) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 13.2704(15.6306) | Bit/dim 7.1861(7.8637) | Xent 2.0718(2.1258) | Loss 27.3356(31.3536) | Error 0.6689(0.7269) Steps 0(0.00) | Grad Norm 3.3737(8.1630) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.6181(15.0324) | Bit/dim 7.0821(7.6689) | Xent 2.0847(2.1129) | Loss 27.1166(30.1848) | Error 0.7278(0.7183) Steps 0(0.00) | Grad Norm 2.1317(6.7216) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 14.1479(14.6768) | Bit/dim 7.0013(7.5061) | Xent 2.0634(2.1033) | Loss 27.2623(29.2770) | Error 0.6944(0.7128) Steps 0(0.00) | Grad Norm 3.4369(5.7585) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 14.6190(14.5082) | Bit/dim 7.0033(7.3753) | Xent 2.0597(2.0946) | Loss 27.6843(28.6227) | Error 0.7322(0.7121) Steps 0(0.00) | Grad Norm 2.3314(4.8608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 78.8444, Epoch Time 844.9779(843.6677), Bit/dim 6.9921(best: 7.7698), Xent 2.0585, Loss 8.0214, Error 0.6945(best: 0.7001)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.7206(14.6093) | Bit/dim 6.9409(7.2695) | Xent 2.0553(2.0853) | Loss 26.5716(33.4248) | Error 0.7089(0.7101) Steps 0(0.00) | Grad Norm 1.7610(4.2667) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.6531(14.7008) | Bit/dim 6.9199(7.1828) | Xent 2.0416(2.0745) | Loss 26.8223(31.6105) | Error 0.6867(0.7055) Steps 0(0.00) | Grad Norm 2.9476(3.7821) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 16.6386(14.7927) | Bit/dim 6.8950(7.1070) | Xent 2.0474(2.0646) | Loss 26.6761(30.2377) | Error 0.6867(0.7023) Steps 0(0.00) | Grad Norm 8.0744(3.9091) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 15.5574(14.8709) | Bit/dim 6.7860(7.0357) | Xent 2.0172(2.0566) | Loss 25.6046(29.1695) | Error 0.7100(0.7002) Steps 0(0.00) | Grad Norm 4.1132(4.9577) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 15.0718(14.9225) | Bit/dim 6.7047(6.9610) | Xent 2.0360(2.0516) | Loss 26.8020(28.4668) | Error 0.7244(0.6992) Steps 0(0.00) | Grad Norm 22.2163(7.5919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 81.9880, Epoch Time 928.4460(846.2111), Bit/dim 6.6372(best: 6.9921), Xent 2.0291, Loss 7.6517, Error 0.6957(best: 0.6945)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 15.0452(15.0211) | Bit/dim 6.6037(6.8783) | Xent 2.0397(2.0522) | Loss 26.5857(34.0104) | Error 0.7044(0.7029) Steps 0(0.00) | Grad Norm 35.4942(14.5152) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 16.5161(15.1456) | Bit/dim 6.4337(6.7775) | Xent 2.1616(2.0553) | Loss 25.8004(31.9085) | Error 0.8067(0.7136) Steps 0(0.00) | Grad Norm 92.9596(23.4793) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.6611(15.0375) | Bit/dim 6.2298(6.6586) | Xent 2.0440(2.0550) | Loss 25.0884(30.2754) | Error 0.7044(0.7163) Steps 0(0.00) | Grad Norm 49.0343(31.6851) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 15.0331(15.0488) | Bit/dim 6.1299(6.5348) | Xent 2.1122(2.0749) | Loss 25.5786(29.0409) | Error 0.7611(0.7290) Steps 0(0.00) | Grad Norm 69.7908(49.2931) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 14.1810(15.1285) | Bit/dim 5.9546(6.3992) | Xent 2.0935(2.0787) | Loss 24.8824(27.9833) | Error 0.7622(0.7327) Steps 0(0.00) | Grad Norm 27.5489(47.4720) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 14.1408(14.9993) | Bit/dim 5.8111(6.2593) | Xent 2.0449(2.0706) | Loss 25.0553(27.1242) | Error 0.7200(0.7246) Steps 0(0.00) | Grad Norm 39.5773(42.4226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 81.9724, Epoch Time 930.5518(848.7413), Bit/dim 5.8294(best: 6.6372), Xent 2.0427, Loss 6.8508, Error 0.6987(best: 0.6945)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 14.9132(15.1537) | Bit/dim 5.8005(6.1400) | Xent 2.0937(2.0613) | Loss 24.4551(31.5514) | Error 0.7433(0.7174) Steps 0(0.00) | Grad Norm 57.4468(43.2920) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 14.8088(14.9736) | Bit/dim 5.7420(6.0359) | Xent 2.0134(2.0519) | Loss 25.0429(29.5796) | Error 0.6900(0.7144) Steps 0(0.00) | Grad Norm 25.0453(43.2604) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.5505(14.8128) | Bit/dim 5.6795(5.9479) | Xent 1.9793(2.0387) | Loss 23.7165(28.1640) | Error 0.6511(0.7028) Steps 0(0.00) | Grad Norm 24.5210(37.9235) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 14.2565(14.6829) | Bit/dim 5.6271(5.8690) | Xent 1.9573(2.0208) | Loss 23.5656(27.1183) | Error 0.6856(0.6930) Steps 0(0.00) | Grad Norm 35.5167(33.4740) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 14.2878(14.6973) | Bit/dim 5.7855(5.8310) | Xent 2.1210(2.0410) | Loss 22.8949(26.4156) | Error 0.7733(0.6971) Steps 0(0.00) | Grad Norm 101.6268(48.6704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 82.0888, Epoch Time 909.2528(850.5566), Bit/dim 5.7209(best: 5.8294), Xent 2.0149, Loss 6.7284, Error 0.6953(best: 0.6945)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 14.4384(14.6567) | Bit/dim 5.6386(5.8012) | Xent 2.0307(2.0387) | Loss 24.8897(31.7294) | Error 0.7033(0.7000) Steps 0(0.00) | Grad Norm 21.5392(43.8774) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.7259(14.4946) | Bit/dim 5.6237(5.7556) | Xent 1.9522(2.0314) | Loss 24.2140(29.6838) | Error 0.6567(0.6994) Steps 0(0.00) | Grad Norm 12.0673(37.4997) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.9039(14.3281) | Bit/dim 5.5429(5.7117) | Xent 2.0065(2.0210) | Loss 23.4422(28.1966) | Error 0.7311(0.6969) Steps 0(0.00) | Grad Norm 35.8717(33.3131) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.1161(14.1936) | Bit/dim 5.5058(5.6661) | Xent 1.9980(2.0115) | Loss 22.0446(27.0394) | Error 0.6700(0.6919) Steps 0(0.00) | Grad Norm 21.1788(29.7698) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.7650(14.1940) | Bit/dim 5.5011(5.6243) | Xent 1.9754(2.0024) | Loss 23.7455(26.1742) | Error 0.6889(0.6901) Steps 0(0.00) | Grad Norm 6.4651(25.1914) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 14.1906(14.2199) | Bit/dim 5.5341(5.5957) | Xent 1.9943(2.0045) | Loss 23.7200(25.5089) | Error 0.7033(0.6925) Steps 0(0.00) | Grad Norm 38.6344(27.9124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 81.2209, Epoch Time 874.9132(851.2873), Bit/dim 5.4979(best: 5.7209), Xent 2.1143, Loss 6.5551, Error 0.7789(best: 0.6945)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 15.0118(14.2520) | Bit/dim 5.4588(5.5641) | Xent 2.0063(2.0062) | Loss 24.4020(30.3119) | Error 0.6833(0.6965) Steps 0(0.00) | Grad Norm 9.0217(26.0346) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 14.8251(14.3389) | Bit/dim 5.3760(5.5207) | Xent 1.9186(1.9910) | Loss 24.1439(28.5970) | Error 0.6411(0.6880) Steps 0(0.00) | Grad Norm 12.0384(21.6479) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 14.6959(14.3817) | Bit/dim 5.3352(5.4832) | Xent 1.9407(1.9718) | Loss 22.9621(27.2145) | Error 0.6756(0.6814) Steps 0(0.00) | Grad Norm 5.1436(17.4308) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.4283(14.5157) | Bit/dim 5.3397(5.4483) | Xent 1.8987(1.9573) | Loss 22.9586(26.2543) | Error 0.6733(0.6756) Steps 0(0.00) | Grad Norm 13.5093(17.3310) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 14.2651(14.7056) | Bit/dim 5.3226(5.4128) | Xent 1.9375(1.9511) | Loss 23.1897(25.5707) | Error 0.6922(0.6772) Steps 0(0.00) | Grad Norm 25.3543(18.8723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 82.9202, Epoch Time 912.6081(853.1270), Bit/dim 5.2818(best: 5.4979), Xent 1.8737, Loss 6.2186, Error 0.6362(best: 0.6945)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 16.0967(14.7881) | Bit/dim 5.2409(5.3844) | Xent 1.8918(1.9435) | Loss 23.8382(30.8795) | Error 0.6633(0.6742) Steps 0(0.00) | Grad Norm 31.3231(19.7819) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 15.4428(14.8140) | Bit/dim 5.2804(5.3490) | Xent 1.8889(1.9274) | Loss 24.4946(28.9246) | Error 0.6633(0.6695) Steps 0(0.00) | Grad Norm 18.9857(21.1345) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 15.1351(14.9168) | Bit/dim 5.3370(5.3184) | Xent 2.0767(1.9303) | Loss 23.5489(27.5566) | Error 0.7267(0.6716) Steps 0(0.00) | Grad Norm 83.5883(26.3817) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 19.0514(15.0685) | Bit/dim 5.2361(5.2933) | Xent 1.9251(1.9438) | Loss 24.0429(26.4894) | Error 0.6989(0.6776) Steps 0(0.00) | Grad Norm 31.5513(29.5503) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 15.4919(15.1089) | Bit/dim 5.2086(5.2618) | Xent 1.9283(1.9445) | Loss 23.5447(25.5760) | Error 0.6722(0.6799) Steps 0(0.00) | Grad Norm 26.6348(27.4293) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.6269(15.1618) | Bit/dim 5.2056(5.2304) | Xent 1.9423(1.9390) | Loss 22.8496(24.9259) | Error 0.6844(0.6769) Steps 0(0.00) | Grad Norm 19.1220(24.0971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 83.6163, Epoch Time 938.2665(855.6811), Bit/dim 5.1153(best: 5.2818), Xent 1.8757, Loss 6.0532, Error 0.6382(best: 0.6362)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 15.8402(15.0798) | Bit/dim 5.1210(5.2020) | Xent 1.9158(1.9307) | Loss 22.3486(29.7760) | Error 0.6733(0.6743) Steps 0(0.00) | Grad Norm 9.2379(20.3769) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 17.9456(15.3364) | Bit/dim 5.0876(5.1691) | Xent 1.9260(1.9184) | Loss 23.3231(28.0442) | Error 0.6800(0.6708) Steps 0(0.00) | Grad Norm 25.7401(20.2587) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 15.6254(15.3165) | Bit/dim 5.0624(5.1427) | Xent 1.8831(1.9092) | Loss 22.6469(26.7185) | Error 0.6667(0.6676) Steps 0(0.00) | Grad Norm 20.7133(22.1943) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 16.0799(15.3577) | Bit/dim 5.0253(5.1145) | Xent 1.8723(1.9063) | Loss 23.3502(25.7047) | Error 0.6300(0.6683) Steps 0(0.00) | Grad Norm 29.5393(24.6390) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.7228(15.4196) | Bit/dim 5.0295(5.0891) | Xent 1.8920(1.9091) | Loss 23.1490(24.9687) | Error 0.6711(0.6710) Steps 0(0.00) | Grad Norm 11.5618(27.0535) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 85.1488, Epoch Time 953.3608(858.6115), Bit/dim 4.9597(best: 5.1153), Xent 1.8289, Loss 5.8742, Error 0.6197(best: 0.6362)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.6702(15.4251) | Bit/dim 4.9385(5.0535) | Xent 1.8451(1.8943) | Loss 22.0527(30.0570) | Error 0.6400(0.6654) Steps 0(0.00) | Grad Norm 6.8646(23.9536) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 15.7615(15.4516) | Bit/dim 5.1153(5.0407) | Xent 2.0054(1.8930) | Loss 23.6380(28.1863) | Error 0.6956(0.6649) Steps 0(0.00) | Grad Norm 100.7878(30.9111) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 16.6837(15.7988) | Bit/dim 4.9337(5.0226) | Xent 1.8969(1.8992) | Loss 23.4574(26.8749) | Error 0.6733(0.6684) Steps 0(0.00) | Grad Norm 16.2366(30.5439) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.3221(16.0393) | Bit/dim 4.9000(4.9933) | Xent 1.9642(1.9056) | Loss 21.8422(25.8456) | Error 0.7022(0.6708) Steps 0(0.00) | Grad Norm 19.3440(26.6979) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 16.8298(16.1434) | Bit/dim 4.8717(4.9691) | Xent 1.8627(1.9026) | Loss 23.3968(24.9958) | Error 0.6711(0.6708) Steps 0(0.00) | Grad Norm 21.0534(25.1102) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 17.1404(16.2280) | Bit/dim 4.8033(4.9412) | Xent 1.9229(1.8923) | Loss 22.9375(24.4058) | Error 0.6756(0.6655) Steps 0(0.00) | Grad Norm 14.4574(23.6480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 85.4127, Epoch Time 1001.1237(862.8869), Bit/dim 4.8629(best: 4.9597), Xent 1.7975, Loss 5.7616, Error 0.6119(best: 0.6197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 16.0933(16.1999) | Bit/dim 4.8237(4.9167) | Xent 1.8010(1.8735) | Loss 22.3568(29.3833) | Error 0.6478(0.6582) Steps 0(0.00) | Grad Norm 31.5195(25.6135) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.3325(16.1712) | Bit/dim 4.9602(4.9033) | Xent 1.8838(1.8734) | Loss 22.8804(27.5880) | Error 0.6600(0.6605) Steps 0(0.00) | Grad Norm 50.8105(30.3297) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 16.4473(16.1777) | Bit/dim 4.8458(4.8856) | Xent 1.8881(1.8828) | Loss 22.8458(26.4074) | Error 0.6556(0.6623) Steps 0(0.00) | Grad Norm 11.7085(31.1058) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 16.2346(16.3093) | Bit/dim 4.7917(4.8642) | Xent 1.8229(1.8742) | Loss 21.3758(25.4131) | Error 0.6367(0.6599) Steps 0(0.00) | Grad Norm 8.6866(28.2219) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 18.0830(16.3993) | Bit/dim 4.7869(4.8413) | Xent 1.7741(1.8574) | Loss 22.3772(24.6685) | Error 0.6089(0.6533) Steps 0(0.00) | Grad Norm 18.2371(25.6727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 90.2036, Epoch Time 1006.7197(867.2019), Bit/dim 4.7452(best: 4.8629), Xent 1.7296, Loss 5.6100, Error 0.6022(best: 0.6119)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.8063(16.2997) | Bit/dim 4.7536(4.8180) | Xent 1.8076(1.8412) | Loss 21.1793(30.6029) | Error 0.6111(0.6484) Steps 0(0.00) | Grad Norm 20.7209(24.7550) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 16.8610(16.4298) | Bit/dim 4.7003(4.7953) | Xent 1.7822(1.8280) | Loss 23.2179(28.5121) | Error 0.6122(0.6464) Steps 0(0.00) | Grad Norm 45.3456(26.7041) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 17.0854(16.4968) | Bit/dim 4.7260(4.7907) | Xent 1.7525(1.8381) | Loss 21.9937(26.9229) | Error 0.6233(0.6502) Steps 0(0.00) | Grad Norm 15.7707(30.5435) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.4971(16.6193) | Bit/dim 4.7243(4.7703) | Xent 1.8265(1.8326) | Loss 23.3192(25.8360) | Error 0.6400(0.6464) Steps 0(0.00) | Grad Norm 34.7901(27.6419) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 16.7194(16.5929) | Bit/dim 4.7220(4.7638) | Xent 1.8864(1.8297) | Loss 22.2875(24.9719) | Error 0.6822(0.6462) Steps 0(0.00) | Grad Norm 32.0527(29.0986) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 16.6320(16.6537) | Bit/dim 4.7011(4.7463) | Xent 1.8345(1.8249) | Loss 22.9597(24.3671) | Error 0.6444(0.6435) Steps 0(0.00) | Grad Norm 29.8773(27.0452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 91.3724, Epoch Time 1028.8164(872.0503), Bit/dim 4.6701(best: 4.7452), Xent 1.7091, Loss 5.5247, Error 0.6004(best: 0.6022)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.2979(16.6214) | Bit/dim 4.6575(4.7295) | Xent 1.6765(1.8013) | Loss 22.2961(29.0835) | Error 0.6089(0.6370) Steps 0(0.00) | Grad Norm 4.4361(25.1318) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.9208(16.7412) | Bit/dim 4.6276(4.7178) | Xent 1.7141(1.7808) | Loss 22.2055(27.3323) | Error 0.6078(0.6304) Steps 0(0.00) | Grad Norm 19.1320(24.8312) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 16.7591(16.8607) | Bit/dim 4.6564(4.7014) | Xent 1.7689(1.7672) | Loss 21.4737(26.0426) | Error 0.6333(0.6247) Steps 0(0.00) | Grad Norm 21.8232(24.4152) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 17.3200(16.7988) | Bit/dim 4.7547(4.6832) | Xent 1.8365(1.7550) | Loss 23.1754(25.0850) | Error 0.6489(0.6217) Steps 0(0.00) | Grad Norm 66.8876(23.4306) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 16.5232(16.7664) | Bit/dim 4.6961(4.7054) | Xent 1.7217(1.7787) | Loss 22.8719(24.5461) | Error 0.5844(0.6306) Steps 0(0.00) | Grad Norm 19.1194(29.1022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 89.4273, Epoch Time 1032.2485(876.8563), Bit/dim 4.6666(best: 4.6701), Xent 1.6868, Loss 5.5100, Error 0.5835(best: 0.6004)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.9720(16.7063) | Bit/dim 4.6185(4.6941) | Xent 1.7015(1.7777) | Loss 22.5684(30.1975) | Error 0.5956(0.6305) Steps 0(0.00) | Grad Norm 12.2692(26.2454) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 16.2024(16.6278) | Bit/dim 4.5764(4.6723) | Xent 1.6725(1.7556) | Loss 20.9992(28.0011) | Error 0.5767(0.6226) Steps 0(0.00) | Grad Norm 15.2225(24.3067) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 16.2111(16.9077) | Bit/dim 4.5457(4.6525) | Xent 1.6954(1.7428) | Loss 22.4731(26.5520) | Error 0.6111(0.6193) Steps 0(0.00) | Grad Norm 32.7849(24.4364) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.0152(16.9880) | Bit/dim 4.5926(4.6356) | Xent 1.6778(1.7431) | Loss 21.9367(25.4260) | Error 0.6278(0.6213) Steps 0(0.00) | Grad Norm 21.3086(23.8645) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 16.9447(17.0221) | Bit/dim 4.5659(4.6195) | Xent 1.6745(1.7308) | Loss 23.3229(24.6728) | Error 0.5833(0.6178) Steps 0(0.00) | Grad Norm 11.7961(21.9139) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 17.0960(17.1503) | Bit/dim 4.5635(4.6022) | Xent 1.6806(1.7171) | Loss 22.6915(24.0619) | Error 0.5978(0.6120) Steps 0(0.00) | Grad Norm 30.3188(21.1396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 93.7656, Epoch Time 1056.8259(882.2554), Bit/dim 4.5462(best: 4.6666), Xent 1.5924, Loss 5.3424, Error 0.5668(best: 0.5835)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 17.5832(17.3543) | Bit/dim 4.5288(4.5859) | Xent 1.6152(1.7043) | Loss 21.3158(29.3435) | Error 0.5667(0.6065) Steps 0(0.00) | Grad Norm 16.3608(22.1825) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 16.9888(17.3474) | Bit/dim 4.5586(4.5733) | Xent 1.7362(1.6925) | Loss 22.8631(27.5669) | Error 0.6356(0.6034) Steps 0(0.00) | Grad Norm 45.3520(22.9054) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 17.9480(17.4576) | Bit/dim 4.5751(4.5754) | Xent 1.9133(1.7174) | Loss 22.4168(26.2922) | Error 0.6611(0.6123) Steps 0(0.00) | Grad Norm 53.3950(27.0353) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 17.4852(17.5045) | Bit/dim 4.5070(4.5651) | Xent 1.7651(1.7310) | Loss 22.0036(25.2962) | Error 0.6356(0.6189) Steps 0(0.00) | Grad Norm 17.9282(27.4144) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 22.4977(17.5476) | Bit/dim 4.5185(4.5516) | Xent 1.7556(1.7347) | Loss 22.0181(24.4516) | Error 0.6111(0.6200) Steps 0(0.00) | Grad Norm 27.3049(25.4073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 92.4949, Epoch Time 1085.2657(888.3457), Bit/dim 4.5388(best: 4.5462), Xent 1.6290, Loss 5.3533, Error 0.5707(best: 0.5668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.7457(17.5514) | Bit/dim 4.4702(4.5404) | Xent 1.6724(1.7150) | Loss 21.5891(30.4890) | Error 0.6000(0.6132) Steps 0(0.00) | Grad Norm 16.3294(23.9504) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 17.2234(17.5606) | Bit/dim 4.4805(4.5267) | Xent 1.7026(1.7036) | Loss 22.1864(28.3272) | Error 0.6022(0.6095) Steps 0(0.00) | Grad Norm 7.0057(21.9017) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 17.3115(17.4569) | Bit/dim 4.4544(4.5118) | Xent 1.6152(1.6838) | Loss 22.0725(26.7110) | Error 0.5956(0.6041) Steps 0(0.00) | Grad Norm 14.0172(20.0680) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.2660(17.4579) | Bit/dim 4.4402(4.4957) | Xent 1.6381(1.6638) | Loss 20.9862(25.4758) | Error 0.5733(0.5975) Steps 0(0.00) | Grad Norm 16.4705(18.7705) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 18.0550(17.6367) | Bit/dim 4.4658(4.4884) | Xent 1.6941(1.6681) | Loss 22.5478(24.7913) | Error 0.6067(0.5986) Steps 0(0.00) | Grad Norm 19.2538(22.0825) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 18.2254(17.7992) | Bit/dim 4.4617(4.4766) | Xent 1.6572(1.6619) | Loss 22.6166(24.1784) | Error 0.5856(0.5951) Steps 0(0.00) | Grad Norm 28.4193(21.8732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 96.2968, Epoch Time 1086.4985(894.2902), Bit/dim 4.4400(best: 4.5388), Xent 1.5785, Loss 5.2292, Error 0.5703(best: 0.5668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.9462(17.8323) | Bit/dim 4.4557(4.4703) | Xent 1.6072(1.6555) | Loss 22.2060(29.4554) | Error 0.5700(0.5934) Steps 0(0.00) | Grad Norm 11.9557(21.4976) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 17.8357(17.8165) | Bit/dim 4.3918(4.4523) | Xent 1.5474(1.6387) | Loss 21.0107(27.5133) | Error 0.5489(0.5890) Steps 0(0.00) | Grad Norm 6.4502(18.9980) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 20.1111(17.9741) | Bit/dim 4.3762(4.4379) | Xent 1.6005(1.6295) | Loss 22.9657(26.2590) | Error 0.5744(0.5854) Steps 0(0.00) | Grad Norm 6.4104(17.6893) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 17.6893(17.9486) | Bit/dim 4.3419(4.4210) | Xent 1.6347(1.6218) | Loss 20.9353(25.1256) | Error 0.5933(0.5833) Steps 0(0.00) | Grad Norm 13.5420(16.3454) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 17.1539(17.9363) | Bit/dim 4.3734(4.4066) | Xent 1.5976(1.6113) | Loss 22.6101(24.3937) | Error 0.5489(0.5796) Steps 0(0.00) | Grad Norm 24.0474(15.8063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 96.1234, Epoch Time 1102.1920(900.5273), Bit/dim 4.3601(best: 4.4400), Xent 1.5543, Loss 5.1373, Error 0.5582(best: 0.5668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 19.5883(17.9092) | Bit/dim 4.3873(4.3959) | Xent 1.5399(1.6078) | Loss 22.2719(30.2961) | Error 0.5544(0.5788) Steps 0(0.00) | Grad Norm 24.7309(17.6021) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 20.1861(17.9431) | Bit/dim 4.3839(4.3884) | Xent 1.5419(1.6009) | Loss 22.4884(28.1000) | Error 0.5622(0.5768) Steps 0(0.00) | Grad Norm 28.6530(19.3330) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 17.1813(17.8702) | Bit/dim 4.3969(4.3784) | Xent 1.5881(1.5985) | Loss 21.0718(26.3878) | Error 0.5811(0.5777) Steps 0(0.00) | Grad Norm 17.0540(19.4251) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 17.7683(18.0112) | Bit/dim 4.2990(4.3694) | Xent 1.5902(1.5863) | Loss 21.7674(25.3087) | Error 0.5600(0.5746) Steps 0(0.00) | Grad Norm 6.6399(18.5728) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 17.2519(17.9365) | Bit/dim 4.3478(4.3613) | Xent 1.6005(1.5791) | Loss 21.1843(24.3936) | Error 0.5656(0.5720) Steps 0(0.00) | Grad Norm 20.6134(18.5529) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 26.9436(18.1105) | Bit/dim 4.3034(4.3489) | Xent 1.5936(1.5846) | Loss 23.0393(23.7447) | Error 0.5889(0.5741) Steps 0(0.00) | Grad Norm 22.0621(19.4200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 97.5032, Epoch Time 1108.6322(906.7704), Bit/dim 4.3384(best: 4.3601), Xent 1.5255, Loss 5.1012, Error 0.5449(best: 0.5582)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 18.0607(18.0646) | Bit/dim 4.2787(4.3356) | Xent 1.4749(1.5714) | Loss 22.8451(29.0516) | Error 0.5456(0.5712) Steps 0(0.00) | Grad Norm 12.7210(19.1370) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.2861(17.9386) | Bit/dim 4.3082(4.3218) | Xent 1.5599(1.5606) | Loss 21.5535(27.1149) | Error 0.5633(0.5670) Steps 0(0.00) | Grad Norm 14.3762(16.9159) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 18.2841(17.9454) | Bit/dim 4.2868(4.3184) | Xent 1.5542(1.5663) | Loss 22.1863(25.8289) | Error 0.5667(0.5672) Steps 0(0.00) | Grad Norm 23.5274(18.3644) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.3152(17.9217) | Bit/dim 4.3086(4.3102) | Xent 1.5390(1.5558) | Loss 21.6520(24.8091) | Error 0.5489(0.5640) Steps 0(0.00) | Grad Norm 20.7341(18.3963) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.7048(17.8071) | Bit/dim 4.2947(4.3015) | Xent 1.5559(1.5503) | Loss 20.8231(23.9439) | Error 0.5744(0.5626) Steps 0(0.00) | Grad Norm 18.6528(18.2520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 94.9695, Epoch Time 1097.0990(912.4803), Bit/dim 4.2454(best: 4.3384), Xent 1.4187, Loss 4.9547, Error 0.5133(best: 0.5449)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 18.9079(18.0695) | Bit/dim 4.2329(4.2918) | Xent 1.5390(1.5457) | Loss 21.6904(29.9305) | Error 0.5900(0.5616) Steps 0(0.00) | Grad Norm 16.9140(19.4473) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 18.6973(18.1902) | Bit/dim 4.2357(4.2786) | Xent 1.5069(1.5375) | Loss 22.8700(27.8956) | Error 0.5389(0.5576) Steps 0(0.00) | Grad Norm 26.5307(19.1807) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 17.9921(18.2346) | Bit/dim 4.2037(4.2622) | Xent 1.4513(1.5242) | Loss 21.5163(26.3261) | Error 0.5256(0.5520) Steps 0(0.00) | Grad Norm 7.1407(17.6890) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 19.0402(18.0956) | Bit/dim 4.2740(4.2718) | Xent 1.6097(1.5445) | Loss 22.0562(25.1806) | Error 0.5800(0.5581) Steps 0(0.00) | Grad Norm 13.9078(19.2871) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 18.7668(18.0120) | Bit/dim 4.2248(4.2668) | Xent 1.5699(1.5482) | Loss 22.2801(24.2277) | Error 0.5656(0.5607) Steps 0(0.00) | Grad Norm 16.6414(18.5151) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.1191(18.0719) | Bit/dim 4.2130(4.2565) | Xent 1.5410(1.5406) | Loss 21.3482(23.5346) | Error 0.5489(0.5584) Steps 0(0.00) | Grad Norm 16.1956(17.0867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 93.5648, Epoch Time 1112.4870(918.4805), Bit/dim 4.2045(best: 4.2454), Xent 1.4096, Loss 4.9093, Error 0.5056(best: 0.5133)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 18.3986(18.1534) | Bit/dim 4.2181(4.2418) | Xent 1.4639(1.5218) | Loss 21.4227(28.6123) | Error 0.5289(0.5511) Steps 0(0.00) | Grad Norm 13.7248(15.5128) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 19.2062(18.0455) | Bit/dim 4.1792(4.2250) | Xent 1.3382(1.5018) | Loss 21.4476(26.7585) | Error 0.4833(0.5435) Steps 0(0.00) | Grad Norm 8.0609(14.4158) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 21.7423(18.1231) | Bit/dim 4.1680(4.2139) | Xent 1.4307(1.4824) | Loss 21.5293(25.4100) | Error 0.5078(0.5367) Steps 0(0.00) | Grad Norm 8.9624(13.4631) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 18.4144(18.0361) | Bit/dim 4.1903(4.2045) | Xent 1.4653(1.4711) | Loss 21.0936(24.4501) | Error 0.5278(0.5325) Steps 0(0.00) | Grad Norm 23.4252(12.9289) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.5801(17.9152) | Bit/dim 4.1272(4.1940) | Xent 1.4384(1.4693) | Loss 21.0904(23.5922) | Error 0.5189(0.5314) Steps 0(0.00) | Grad Norm 9.9465(14.2558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 94.0911, Epoch Time 1101.4060(923.9683), Bit/dim 4.2160(best: 4.2045), Xent 1.3937, Loss 4.9129, Error 0.5069(best: 0.5056)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 17.6151(18.0198) | Bit/dim 4.1736(4.1939) | Xent 1.4808(1.4720) | Loss 21.3746(29.8715) | Error 0.5344(0.5318) Steps 0(0.00) | Grad Norm 20.6809(16.1663) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 17.5570(17.9137) | Bit/dim 4.1451(4.1898) | Xent 1.5185(1.4728) | Loss 21.8461(27.6930) | Error 0.5333(0.5307) Steps 0(0.00) | Grad Norm 31.4757(17.1129) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.6249(17.9660) | Bit/dim 4.1558(4.1830) | Xent 1.4080(1.4602) | Loss 20.1408(26.0052) | Error 0.5289(0.5285) Steps 0(0.00) | Grad Norm 9.8640(16.2637) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 19.3934(17.9524) | Bit/dim 4.1379(4.1678) | Xent 1.5419(1.4482) | Loss 21.8810(24.7763) | Error 0.5533(0.5233) Steps 0(0.00) | Grad Norm 22.1125(15.4441) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 17.0844(17.8587) | Bit/dim 4.1418(4.1577) | Xent 1.4292(1.4535) | Loss 20.8125(23.8928) | Error 0.5244(0.5252) Steps 0(0.00) | Grad Norm 8.4442(16.3725) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 17.0799(17.7472) | Bit/dim 4.1140(4.1472) | Xent 1.3600(1.4454) | Loss 21.1288(23.0944) | Error 0.4900(0.5232) Steps 0(0.00) | Grad Norm 7.1772(14.5683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 92.9067, Epoch Time 1091.3379(928.9894), Bit/dim 4.1197(best: 4.2045), Xent 1.3244, Loss 4.7819, Error 0.4848(best: 0.5056)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 17.1972(17.7836) | Bit/dim 4.0894(4.1353) | Xent 1.3948(1.4261) | Loss 20.5535(28.1316) | Error 0.5000(0.5168) Steps 0(0.00) | Grad Norm 12.0124(13.7151) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 17.3237(17.6139) | Bit/dim 4.1055(4.1309) | Xent 1.4302(1.4196) | Loss 20.8836(26.2539) | Error 0.5122(0.5143) Steps 0(0.00) | Grad Norm 9.8422(14.0013) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 18.4868(17.5669) | Bit/dim 4.1100(4.1244) | Xent 1.3514(1.4162) | Loss 21.2604(24.8742) | Error 0.4933(0.5125) Steps 0(0.00) | Grad Norm 14.4678(15.0584) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 21.4268(17.7032) | Bit/dim 4.1098(4.1221) | Xent 1.3689(1.4137) | Loss 21.6647(23.9879) | Error 0.4900(0.5120) Steps 0(0.00) | Grad Norm 20.2935(15.8282) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 16.3982(17.5919) | Bit/dim 4.1343(4.1208) | Xent 1.3902(1.4123) | Loss 20.2683(23.2126) | Error 0.5033(0.5110) Steps 0(0.00) | Grad Norm 15.3943(16.5558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 91.5661, Epoch Time 1076.5046(933.4148), Bit/dim 4.0989(best: 4.1197), Xent 1.2705, Loss 4.7341, Error 0.4609(best: 0.4848)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 20.0322(17.6444) | Bit/dim 4.0561(4.1137) | Xent 1.3513(1.3981) | Loss 22.6295(29.2686) | Error 0.4989(0.5066) Steps 0(0.00) | Grad Norm 6.5073(14.9298) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.9303(17.5321) | Bit/dim 4.1078(4.1056) | Xent 1.3785(1.3877) | Loss 20.2096(27.1292) | Error 0.5156(0.5036) Steps 0(0.00) | Grad Norm 16.6804(14.1736) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 18.4120(17.4751) | Bit/dim 4.0732(4.0993) | Xent 1.3639(1.3782) | Loss 21.8953(25.6321) | Error 0.5033(0.5002) Steps 0(0.00) | Grad Norm 9.7705(14.0435) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 16.5056(17.3718) | Bit/dim 4.0696(4.0884) | Xent 1.3678(1.3722) | Loss 20.2422(24.3965) | Error 0.5011(0.4986) Steps 0(0.00) | Grad Norm 7.2627(12.7681) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 17.4177(17.3438) | Bit/dim 4.0898(4.0833) | Xent 1.3766(1.3621) | Loss 22.1915(23.5508) | Error 0.4700(0.4958) Steps 0(0.00) | Grad Norm 23.6869(13.3977) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.8047(17.1851) | Bit/dim 4.0605(4.0770) | Xent 1.3405(1.3602) | Loss 21.3221(22.8052) | Error 0.4711(0.4934) Steps 0(0.00) | Grad Norm 12.4609(13.5911) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 92.1853, Epoch Time 1058.4981(937.1673), Bit/dim 4.0575(best: 4.0989), Xent 1.3019, Loss 4.7084, Error 0.4766(best: 0.4609)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 17.3908(17.2970) | Bit/dim 4.0754(4.0722) | Xent 1.2656(1.3476) | Loss 21.8594(28.1513) | Error 0.4644(0.4889) Steps 0(0.00) | Grad Norm 17.6429(13.1276) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.1409(17.3331) | Bit/dim 4.0531(4.0672) | Xent 1.3058(1.3412) | Loss 19.4996(26.2187) | Error 0.4756(0.4858) Steps 0(0.00) | Grad Norm 8.4695(13.1881) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 17.8782(17.3341) | Bit/dim 4.0601(4.0643) | Xent 1.3055(1.3357) | Loss 20.5809(24.8482) | Error 0.4633(0.4841) Steps 0(0.00) | Grad Norm 7.3199(12.9280) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.8490(17.3803) | Bit/dim 4.0677(4.0594) | Xent 1.3587(1.3317) | Loss 20.9411(23.8277) | Error 0.4833(0.4827) Steps 0(0.00) | Grad Norm 13.3204(11.8776) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 18.6034(17.3432) | Bit/dim 4.0052(4.0496) | Xent 1.2719(1.3299) | Loss 21.4812(23.0759) | Error 0.4644(0.4815) Steps 0(0.00) | Grad Norm 5.0796(11.4234) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 91.2455, Epoch Time 1069.7809(941.1457), Bit/dim 4.0212(best: 4.0575), Xent 1.2267, Loss 4.6346, Error 0.4475(best: 0.4609)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 17.4674(17.2943) | Bit/dim 3.9934(4.0412) | Xent 1.2871(1.3183) | Loss 21.5768(28.8519) | Error 0.4700(0.4762) Steps 0(0.00) | Grad Norm 9.1164(10.3553) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 17.8205(17.3379) | Bit/dim 4.0352(4.0363) | Xent 1.3405(1.3238) | Loss 21.4676(26.7391) | Error 0.4967(0.4782) Steps 0(0.00) | Grad Norm 15.3817(12.8302) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 16.0215(17.1761) | Bit/dim 4.0363(4.0337) | Xent 1.2885(1.3220) | Loss 20.7836(25.1595) | Error 0.4667(0.4778) Steps 0(0.00) | Grad Norm 13.6882(13.4503) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 17.1898(17.2484) | Bit/dim 4.0473(4.0318) | Xent 1.2778(1.3224) | Loss 20.5616(23.9978) | Error 0.4722(0.4788) Steps 0(0.00) | Grad Norm 19.8689(13.7886) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 16.9004(17.3879) | Bit/dim 4.0111(4.0277) | Xent 1.3238(1.3141) | Loss 21.0376(23.2179) | Error 0.4389(0.4754) Steps 0(0.00) | Grad Norm 16.4351(13.5489) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 18.8281(17.4983) | Bit/dim 3.9831(4.0247) | Xent 1.2820(1.3086) | Loss 21.1096(22.5949) | Error 0.4678(0.4737) Steps 0(0.00) | Grad Norm 6.1938(12.2309) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 92.5527, Epoch Time 1068.3783(944.9627), Bit/dim 4.0069(best: 4.0212), Xent 1.2136, Loss 4.6137, Error 0.4386(best: 0.4475)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.5161(17.5133) | Bit/dim 4.0227(4.0184) | Xent 1.2212(1.3004) | Loss 19.7702(27.6597) | Error 0.4322(0.4688) Steps 0(0.00) | Grad Norm 11.4534(11.6229) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.4713(17.4087) | Bit/dim 3.9863(4.0151) | Xent 1.2619(1.2889) | Loss 19.8437(25.7520) | Error 0.4689(0.4662) Steps 0(0.00) | Grad Norm 14.5762(11.7561) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 17.8648(17.5453) | Bit/dim 3.9403(4.0064) | Xent 1.2748(1.2837) | Loss 20.1602(24.4199) | Error 0.4556(0.4637) Steps 0(0.00) | Grad Norm 12.0496(11.5436) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 17.4468(17.5039) | Bit/dim 3.9905(4.0045) | Xent 1.2590(1.2815) | Loss 22.0843(23.5169) | Error 0.4644(0.4661) Steps 0(0.00) | Grad Norm 9.7453(11.8657) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 16.2377(17.3842) | Bit/dim 3.9996(4.0023) | Xent 1.3796(1.2874) | Loss 19.2950(22.6685) | Error 0.4867(0.4687) Steps 0(0.00) | Grad Norm 20.1498(12.3972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 91.5498, Epoch Time 1068.2268(948.6606), Bit/dim 3.9933(best: 4.0069), Xent 1.2047, Loss 4.5957, Error 0.4342(best: 0.4386)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 16.8430(17.3755) | Bit/dim 3.9637(3.9991) | Xent 1.3263(1.2857) | Loss 20.9545(28.7010) | Error 0.4544(0.4653) Steps 0(0.00) | Grad Norm 9.5602(13.1752) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 17.3308(17.3420) | Bit/dim 3.9724(3.9955) | Xent 1.2414(1.2820) | Loss 20.6867(26.5797) | Error 0.4556(0.4634) Steps 0(0.00) | Grad Norm 7.0653(12.5616) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 17.5851(17.5637) | Bit/dim 3.9577(3.9882) | Xent 1.2066(1.2635) | Loss 20.5305(25.0221) | Error 0.4289(0.4550) Steps 0(0.00) | Grad Norm 8.0405(11.1350) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 18.6919(17.5510) | Bit/dim 3.9890(3.9850) | Xent 1.3093(1.2696) | Loss 19.8368(23.7777) | Error 0.4700(0.4587) Steps 0(0.00) | Grad Norm 17.3533(12.4683) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 18.5234(17.6435) | Bit/dim 3.9943(3.9825) | Xent 1.2076(1.2642) | Loss 21.2535(22.9420) | Error 0.4067(0.4562) Steps 0(0.00) | Grad Norm 10.7489(12.6680) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 16.5677(17.5578) | Bit/dim 3.9848(3.9801) | Xent 1.2403(1.2628) | Loss 18.9590(22.2174) | Error 0.4300(0.4538) Steps 0(0.00) | Grad Norm 8.9515(11.9147) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 90.8972, Epoch Time 1077.9782(952.5401), Bit/dim 3.9612(best: 3.9933), Xent 1.1659, Loss 4.5442, Error 0.4238(best: 0.4342)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 18.9363(17.5951) | Bit/dim 3.9658(3.9766) | Xent 1.1818(1.2501) | Loss 21.0527(27.4787) | Error 0.4267(0.4489) Steps 0(0.00) | Grad Norm 5.1344(11.5720) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 17.1741(17.5865) | Bit/dim 3.9368(3.9698) | Xent 1.1997(1.2444) | Loss 19.4931(25.5541) | Error 0.4278(0.4477) Steps 0(0.00) | Grad Norm 12.0487(10.7453) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 17.0744(17.4611) | Bit/dim 3.9441(3.9656) | Xent 1.1616(1.2326) | Loss 20.6615(24.2134) | Error 0.4078(0.4434) Steps 0(0.00) | Grad Norm 12.8669(10.4318) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 16.9929(17.4277) | Bit/dim 3.9540(3.9621) | Xent 1.2761(1.2329) | Loss 20.4719(23.2102) | Error 0.4578(0.4435) Steps 0(0.00) | Grad Norm 23.1291(11.5534) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 16.9371(17.3828) | Bit/dim 3.9772(3.9594) | Xent 1.1857(1.2326) | Loss 19.4719(22.4013) | Error 0.4111(0.4441) Steps 0(0.00) | Grad Norm 4.3701(11.4273) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 90.7399, Epoch Time 1066.3267(955.9537), Bit/dim 3.9425(best: 3.9612), Xent 1.1618, Loss 4.5234, Error 0.4269(best: 0.4238)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 16.4847(17.3490) | Bit/dim 3.9538(3.9569) | Xent 1.2057(1.2266) | Loss 19.5409(28.2369) | Error 0.4178(0.4400) Steps 0(0.00) | Grad Norm 12.8782(11.1087) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 16.9254(17.3338) | Bit/dim 3.9370(3.9517) | Xent 1.1381(1.2142) | Loss 18.8622(26.0030) | Error 0.4111(0.4363) Steps 0(0.00) | Grad Norm 10.6143(11.3555) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 17.7801(17.3788) | Bit/dim 3.9440(3.9510) | Xent 1.2029(1.2164) | Loss 20.1653(24.5251) | Error 0.4333(0.4377) Steps 0(0.00) | Grad Norm 8.2440(12.4138) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 16.3291(17.3456) | Bit/dim 3.9399(3.9502) | Xent 1.2616(1.2157) | Loss 19.3854(23.4210) | Error 0.4367(0.4361) Steps 0(0.00) | Grad Norm 26.5507(12.4052) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 17.2338(17.2070) | Bit/dim 3.9563(3.9496) | Xent 1.2254(1.2157) | Loss 19.7699(22.5659) | Error 0.4656(0.4376) Steps 0(0.00) | Grad Norm 8.9465(12.7401) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 18.2070(17.3595) | Bit/dim 3.9169(3.9439) | Xent 1.1619(1.2169) | Loss 20.4863(21.9779) | Error 0.4156(0.4365) Steps 0(0.00) | Grad Norm 6.0996(12.2530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 90.4492, Epoch Time 1061.8438(959.1304), Bit/dim 3.9404(best: 3.9425), Xent 1.2006, Loss 4.5407, Error 0.4287(best: 0.4238)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 17.1104(17.3080) | Bit/dim 3.9311(3.9399) | Xent 1.1879(1.2163) | Loss 19.6515(26.8247) | Error 0.4289(0.4357) Steps 0(0.00) | Grad Norm 10.0764(11.8284) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 17.0268(17.2180) | Bit/dim 3.9093(3.9387) | Xent 1.0997(1.2074) | Loss 19.9265(24.9638) | Error 0.4000(0.4316) Steps 0(0.00) | Grad Norm 6.8990(11.7418) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 23.4385(17.4354) | Bit/dim 3.9163(3.9356) | Xent 1.1861(1.1942) | Loss 20.2800(23.7343) | Error 0.4367(0.4277) Steps 0(0.00) | Grad Norm 13.7813(11.1238) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 16.5803(17.2912) | Bit/dim 3.9695(3.9359) | Xent 1.1667(1.1897) | Loss 20.5299(22.7621) | Error 0.4222(0.4259) Steps 0(0.00) | Grad Norm 14.5025(11.4439) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 17.1477(17.2131) | Bit/dim 3.9037(3.9306) | Xent 1.1701(1.1903) | Loss 19.6425(21.9710) | Error 0.4089(0.4264) Steps 0(0.00) | Grad Norm 7.4928(11.3245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 90.1678, Epoch Time 1056.8101(962.0608), Bit/dim 3.9251(best: 3.9404), Xent 1.1008, Loss 4.4755, Error 0.3914(best: 0.4238)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 18.0498(17.1617) | Bit/dim 3.9201(3.9253) | Xent 1.1149(1.1771) | Loss 20.3414(27.6718) | Error 0.4078(0.4228) Steps 0(0.00) | Grad Norm 11.8596(11.2707) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.3117(17.1929) | Bit/dim 3.9311(3.9238) | Xent 1.1128(1.1674) | Loss 19.9537(25.6788) | Error 0.3900(0.4188) Steps 0(0.00) | Grad Norm 8.4751(11.1394) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 17.9327(17.1591) | Bit/dim 3.9257(3.9212) | Xent 1.1602(1.1705) | Loss 21.0193(24.1980) | Error 0.4078(0.4188) Steps 0(0.00) | Grad Norm 14.6746(12.1434) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 17.5205(17.2496) | Bit/dim 3.9822(3.9218) | Xent 1.2032(1.1744) | Loss 19.5212(23.1745) | Error 0.4478(0.4203) Steps 0(0.00) | Grad Norm 20.8673(13.2846) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 16.8247(17.3902) | Bit/dim 3.9771(3.9227) | Xent 1.1128(1.1737) | Loss 20.4031(22.4634) | Error 0.3933(0.4204) Steps 0(0.00) | Grad Norm 19.9858(13.4186) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 16.5410(17.5111) | Bit/dim 3.8984(3.9187) | Xent 1.1034(1.1731) | Loss 19.8005(21.9695) | Error 0.3922(0.4206) Steps 0(0.00) | Grad Norm 14.2604(13.4468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 93.0296, Epoch Time 1069.4984(965.2840), Bit/dim 3.9070(best: 3.9251), Xent 1.0697, Loss 4.4418, Error 0.3828(best: 0.3914)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 17.3770(17.5177) | Bit/dim 3.8965(3.9167) | Xent 1.0873(1.1596) | Loss 20.3589(27.1648) | Error 0.4178(0.4157) Steps 0(0.00) | Grad Norm 12.7215(12.5510) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 16.5556(17.4739) | Bit/dim 3.9227(3.9156) | Xent 1.1552(1.1547) | Loss 19.6743(25.3224) | Error 0.4111(0.4133) Steps 0(0.00) | Grad Norm 7.1573(12.0315) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 17.0492(17.4523) | Bit/dim 3.9052(3.9102) | Xent 1.1640(1.1492) | Loss 19.4813(23.8928) | Error 0.4344(0.4123) Steps 0(0.00) | Grad Norm 7.1728(11.7237) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 16.2884(17.3801) | Bit/dim 3.9132(3.9103) | Xent 1.1566(1.1542) | Loss 19.4269(22.8640) | Error 0.4178(0.4138) Steps 0(0.00) | Grad Norm 14.2024(12.8793) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 17.6748(17.2737) | Bit/dim 3.9035(3.9081) | Xent 1.1307(1.1461) | Loss 20.8514(21.9959) | Error 0.3967(0.4121) Steps 0(0.00) | Grad Norm 10.2366(12.5098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 89.2327, Epoch Time 1060.3510(968.1360), Bit/dim 3.8971(best: 3.9070), Xent 1.0492, Loss 4.4216, Error 0.3792(best: 0.3828)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 16.7438(17.3332) | Bit/dim 3.9142(3.9062) | Xent 1.1424(1.1387) | Loss 21.1060(27.7057) | Error 0.4233(0.4086) Steps 0(0.00) | Grad Norm 10.0111(11.7562) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 17.5275(17.2536) | Bit/dim 3.8579(3.9015) | Xent 1.1236(1.1308) | Loss 20.0626(25.6544) | Error 0.4156(0.4061) Steps 0(0.00) | Grad Norm 18.9803(11.1969) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 17.2621(17.3364) | Bit/dim 3.8504(3.8968) | Xent 1.1693(1.1256) | Loss 18.2672(24.1653) | Error 0.4256(0.4039) Steps 0(0.00) | Grad Norm 14.1705(11.2167) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 17.3043(17.2523) | Bit/dim 3.8823(3.8979) | Xent 1.1788(1.1254) | Loss 21.3176(23.0721) | Error 0.4022(0.4043) Steps 0(0.00) | Grad Norm 24.1671(11.4720) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 16.7887(17.1328) | Bit/dim 3.8704(3.8930) | Xent 1.1007(1.1250) | Loss 19.4285(22.1981) | Error 0.3844(0.4021) Steps 0(0.00) | Grad Norm 11.3194(11.9077) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 16.7355(17.1507) | Bit/dim 3.8975(3.8928) | Xent 1.0760(1.1257) | Loss 19.1182(21.6353) | Error 0.3789(0.4021) Steps 0(0.00) | Grad Norm 9.8179(11.5753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 91.3104, Epoch Time 1053.8725(970.7081), Bit/dim 3.8890(best: 3.8971), Xent 1.0261, Loss 4.4020, Error 0.3671(best: 0.3792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 17.4840(17.1651) | Bit/dim 3.9008(3.8917) | Xent 1.0317(1.1158) | Loss 20.4676(26.7236) | Error 0.3633(0.3978) Steps 0(0.00) | Grad Norm 6.6315(11.2569) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 16.0757(17.1884) | Bit/dim 3.8791(3.8902) | Xent 1.0288(1.1050) | Loss 19.2631(24.9746) | Error 0.3511(0.3913) Steps 0(0.00) | Grad Norm 12.4888(11.4080) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.4893(17.2431) | Bit/dim 3.8584(3.8879) | Xent 1.0804(1.1048) | Loss 19.3287(23.6834) | Error 0.3933(0.3924) Steps 0(0.00) | Grad Norm 10.3243(11.5220) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 16.9659(17.1343) | Bit/dim 3.8699(3.8855) | Xent 1.1675(1.1135) | Loss 20.3899(22.7546) | Error 0.4000(0.3968) Steps 0(0.00) | Grad Norm 10.0146(12.0065) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 18.2902(17.2065) | Bit/dim 3.8825(3.8808) | Xent 1.1455(1.1078) | Loss 20.5947(22.1015) | Error 0.4289(0.3970) Steps 0(0.00) | Grad Norm 8.0947(11.5294) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 89.4347, Epoch Time 1057.1210(973.3005), Bit/dim 3.8744(best: 3.8890), Xent 1.0545, Loss 4.4017, Error 0.3791(best: 0.3671)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 17.1672(17.2920) | Bit/dim 3.8679(3.8812) | Xent 1.2075(1.1125) | Loss 21.0527(28.0538) | Error 0.4456(0.3986) Steps 0(0.00) | Grad Norm 9.0188(11.2584) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 17.2802(17.2862) | Bit/dim 3.8645(3.8757) | Xent 1.0983(1.1062) | Loss 19.1005(25.9408) | Error 0.3922(0.3969) Steps 0(0.00) | Grad Norm 21.3430(11.5477) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 19.1217(17.4358) | Bit/dim 3.9098(3.8772) | Xent 1.0930(1.0990) | Loss 21.3070(24.3574) | Error 0.3989(0.3946) Steps 0(0.00) | Grad Norm 16.5843(12.2714) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 17.6334(17.5095) | Bit/dim 3.8565(3.8737) | Xent 1.0757(1.0954) | Loss 19.8995(23.1753) | Error 0.3778(0.3917) Steps 0(0.00) | Grad Norm 8.4653(11.6461) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 18.5352(17.4839) | Bit/dim 3.8630(3.8683) | Xent 1.0947(1.0922) | Loss 19.3543(22.2833) | Error 0.3956(0.3911) Steps 0(0.00) | Grad Norm 14.5970(11.2652) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 17.0287(17.4379) | Bit/dim 3.8740(3.8705) | Xent 1.0937(1.0876) | Loss 20.4711(21.6254) | Error 0.3978(0.3908) Steps 0(0.00) | Grad Norm 9.5194(11.2260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 88.8249, Epoch Time 1071.0014(976.2315), Bit/dim 3.8584(best: 3.8744), Xent 1.0307, Loss 4.3738, Error 0.3644(best: 0.3671)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 17.7878(17.4290) | Bit/dim 3.8752(3.8684) | Xent 1.0594(1.0808) | Loss 21.4410(26.6069) | Error 0.3722(0.3867) Steps 0(0.00) | Grad Norm 10.8556(10.7787) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 20.1788(17.5448) | Bit/dim 3.8484(3.8677) | Xent 1.0605(1.0703) | Loss 21.0655(24.9293) | Error 0.3878(0.3839) Steps 0(0.00) | Grad Norm 7.7053(10.9785) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 16.5718(17.4733) | Bit/dim 3.8823(3.8669) | Xent 1.0263(1.0680) | Loss 20.2990(23.6305) | Error 0.3800(0.3845) Steps 0(0.00) | Grad Norm 17.1069(11.0876) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 16.6790(17.5345) | Bit/dim 3.8696(3.8630) | Xent 1.1713(1.0755) | Loss 19.8813(22.6725) | Error 0.4089(0.3861) Steps 0(0.00) | Grad Norm 10.4494(11.6973) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 18.0202(17.7312) | Bit/dim 3.9216(3.8635) | Xent 1.0829(1.0840) | Loss 19.4575(22.0000) | Error 0.3789(0.3880) Steps 0(0.00) | Grad Norm 8.6612(12.3397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 90.0117, Epoch Time 1086.1332(979.5285), Bit/dim 3.8619(best: 3.8584), Xent 1.0262, Loss 4.3750, Error 0.3692(best: 0.3644)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 17.4324(17.8110) | Bit/dim 3.8759(3.8627) | Xent 1.0351(1.0780) | Loss 19.7047(28.0213) | Error 0.3911(0.3857) Steps 0(0.00) | Grad Norm 7.3412(11.5890) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 17.1446(17.6943) | Bit/dim 3.8631(3.8617) | Xent 1.0023(1.0674) | Loss 20.4036(25.8543) | Error 0.3578(0.3826) Steps 0(0.00) | Grad Norm 9.0030(11.4398) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 17.1972(17.7590) | Bit/dim 3.8501(3.8594) | Xent 1.0872(1.0684) | Loss 19.6282(24.4150) | Error 0.4078(0.3824) Steps 0(0.00) | Grad Norm 7.2504(11.0800) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 17.6158(17.7742) | Bit/dim 3.8323(3.8557) | Xent 1.0342(1.0596) | Loss 19.4398(23.1952) | Error 0.3633(0.3791) Steps 0(0.00) | Grad Norm 5.9257(10.3445) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 17.8997(17.8993) | Bit/dim 3.8565(3.8559) | Xent 1.0414(1.0544) | Loss 20.3821(22.4936) | Error 0.3611(0.3760) Steps 0(0.00) | Grad Norm 8.6926(10.0958) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 17.6072(18.0281) | Bit/dim 3.7953(3.8493) | Xent 1.0818(1.0556) | Loss 19.8036(21.8855) | Error 0.3833(0.3762) Steps 0(0.00) | Grad Norm 11.2892(9.9829) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 90.1555, Epoch Time 1097.1229(983.0564), Bit/dim 3.8542(best: 3.8584), Xent 1.0228, Loss 4.3656, Error 0.3634(best: 0.3644)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 17.8844(17.9022) | Bit/dim 3.8548(3.8487) | Xent 1.0953(1.0545) | Loss 19.9508(26.9953) | Error 0.3756(0.3752) Steps 0(0.00) | Grad Norm 18.1695(11.4808) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 18.4045(17.8813) | Bit/dim 3.8465(3.8518) | Xent 1.1140(1.0567) | Loss 19.7545(25.1311) | Error 0.4067(0.3769) Steps 0(0.00) | Grad Norm 7.1954(11.6435) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 18.2156(17.7736) | Bit/dim 3.8282(3.8485) | Xent 1.0699(1.0483) | Loss 20.4517(23.8093) | Error 0.3822(0.3749) Steps 0(0.00) | Grad Norm 9.5782(10.8460) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 17.1687(17.6043) | Bit/dim 3.8217(3.8472) | Xent 1.0199(1.0545) | Loss 19.8257(22.8477) | Error 0.3878(0.3779) Steps 0(0.00) | Grad Norm 9.5131(11.2572) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 17.7314(17.5637) | Bit/dim 3.8661(3.8455) | Xent 1.0217(1.0565) | Loss 19.1467(22.0712) | Error 0.3778(0.3772) Steps 0(0.00) | Grad Norm 8.3459(11.1431) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 89.9539, Epoch Time 1072.1754(985.7299), Bit/dim 3.8383(best: 3.8542), Xent 0.9771, Loss 4.3268, Error 0.3459(best: 0.3634)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 17.3478(17.5411) | Bit/dim 3.8312(3.8431) | Xent 1.0388(1.0492) | Loss 20.0504(27.9927) | Error 0.3733(0.3755) Steps 0(0.00) | Grad Norm 8.2193(10.6947) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 17.4785(17.5568) | Bit/dim 3.8508(3.8438) | Xent 1.0037(1.0396) | Loss 20.5902(26.0213) | Error 0.3611(0.3708) Steps 0(0.00) | Grad Norm 11.9195(10.6641) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 17.4124(17.5203) | Bit/dim 3.8306(3.8464) | Xent 1.0357(1.0280) | Loss 20.4371(24.4581) | Error 0.3678(0.3662) Steps 0(0.00) | Grad Norm 7.2914(10.6265) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 17.7194(17.5301) | Bit/dim 3.8053(3.8434) | Xent 1.0031(1.0263) | Loss 20.3767(23.2679) | Error 0.3544(0.3669) Steps 0(0.00) | Grad Norm 6.2731(10.0854) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 18.8074(17.8226) | Bit/dim 3.8087(3.8394) | Xent 1.0150(1.0240) | Loss 21.3016(22.5671) | Error 0.3633(0.3661) Steps 0(0.00) | Grad Norm 8.2169(10.1056) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 17.4668(17.8330) | Bit/dim 3.8216(3.8366) | Xent 0.9923(1.0213) | Loss 19.9390(21.9020) | Error 0.3478(0.3652) Steps 0(0.00) | Grad Norm 10.7224(9.6326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 91.5574, Epoch Time 1087.1891(988.7737), Bit/dim 3.8322(best: 3.8383), Xent 1.0013, Loss 4.3329, Error 0.3557(best: 0.3459)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 17.0525(17.7218) | Bit/dim 3.8387(3.8342) | Xent 0.9764(1.0159) | Loss 19.7057(26.6329) | Error 0.3633(0.3645) Steps 0(0.00) | Grad Norm 11.7973(10.4792) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.9866(17.7276) | Bit/dim 3.8226(3.8318) | Xent 1.0848(1.0159) | Loss 18.8472(24.8736) | Error 0.3833(0.3642) Steps 0(0.00) | Grad Norm 10.8377(10.7991) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 17.8783(17.6511) | Bit/dim 3.8357(3.8320) | Xent 1.0519(1.0149) | Loss 20.2316(23.6299) | Error 0.3856(0.3627) Steps 0(0.00) | Grad Norm 7.8686(10.7446) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 16.8893(17.5418) | Bit/dim 3.8185(3.8310) | Xent 0.9843(1.0153) | Loss 18.9062(22.5261) | Error 0.3644(0.3623) Steps 0(0.00) | Grad Norm 14.2236(10.5518) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 17.2421(17.4707) | Bit/dim 3.8262(3.8310) | Xent 1.0289(1.0168) | Loss 20.1750(21.9417) | Error 0.3700(0.3637) Steps 0(0.00) | Grad Norm 13.6608(10.6813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 90.9245, Epoch Time 1072.5403(991.2867), Bit/dim 3.8250(best: 3.8322), Xent 0.9681, Loss 4.3091, Error 0.3431(best: 0.3459)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 16.6986(17.4829) | Bit/dim 3.8223(3.8314) | Xent 1.0013(1.0089) | Loss 19.2495(27.8085) | Error 0.3467(0.3601) Steps 0(0.00) | Grad Norm 11.9412(11.0330) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 16.8887(17.5497) | Bit/dim 3.8194(3.8294) | Xent 0.9931(1.0061) | Loss 19.1342(25.7714) | Error 0.3622(0.3595) Steps 0(0.00) | Grad Norm 6.0105(10.7534) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 18.0417(17.5839) | Bit/dim 3.8300(3.8282) | Xent 1.0131(1.0026) | Loss 20.4825(24.2633) | Error 0.3656(0.3583) Steps 0(0.00) | Grad Norm 22.6817(10.4994) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 17.7149(17.8601) | Bit/dim 3.7890(3.8252) | Xent 0.9928(1.0048) | Loss 20.5747(23.2695) | Error 0.3422(0.3589) Steps 0(0.00) | Grad Norm 11.2483(11.3543) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 19.2680(17.8948) | Bit/dim 3.8255(3.8225) | Xent 0.9869(1.0126) | Loss 19.4364(22.4166) | Error 0.3589(0.3640) Steps 0(0.00) | Grad Norm 5.7861(11.0046) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 19.0191(18.0889) | Bit/dim 3.8124(3.8209) | Xent 0.9759(1.0033) | Loss 20.0841(21.9242) | Error 0.3589(0.3604) Steps 0(0.00) | Grad Norm 9.5243(9.9300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 90.2986, Epoch Time 1101.1962(994.5840), Bit/dim 3.8193(best: 3.8250), Xent 0.9762, Loss 4.3075, Error 0.3460(best: 0.3431)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 17.9454(17.9983) | Bit/dim 3.8255(3.8208) | Xent 0.9143(1.0027) | Loss 20.0235(27.0393) | Error 0.3378(0.3607) Steps 0(0.00) | Grad Norm 5.8815(10.0000) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 19.2914(17.8523) | Bit/dim 3.8151(3.8210) | Xent 0.9903(0.9979) | Loss 22.3532(25.1930) | Error 0.3378(0.3580) Steps 0(0.00) | Grad Norm 12.4090(10.3393) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 17.7354(17.8224) | Bit/dim 3.8226(3.8209) | Xent 1.0043(1.0001) | Loss 20.6460(23.8856) | Error 0.3644(0.3590) Steps 0(0.00) | Grad Norm 14.5181(10.8551) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 18.0598(17.7499) | Bit/dim 3.8235(3.8197) | Xent 0.9834(1.0002) | Loss 21.3183(22.9429) | Error 0.3556(0.3576) Steps 0(0.00) | Grad Norm 15.1318(10.8379) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.7009(17.7200) | Bit/dim 3.7817(3.8183) | Xent 0.9922(0.9917) | Loss 19.9410(22.2388) | Error 0.3444(0.3544) Steps 0(0.00) | Grad Norm 12.3664(10.4798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 90.4221, Epoch Time 1078.0219(997.0871), Bit/dim 3.8122(best: 3.8193), Xent 0.9473, Loss 4.2858, Error 0.3356(best: 0.3431)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 17.5685(17.6909) | Bit/dim 3.8158(3.8164) | Xent 0.8950(0.9862) | Loss 20.3416(27.9042) | Error 0.3378(0.3533) Steps 0(0.00) | Grad Norm 8.9686(10.0139) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 17.4349(17.5990) | Bit/dim 3.8465(3.8183) | Xent 0.9777(0.9823) | Loss 20.8046(25.8486) | Error 0.3489(0.3524) Steps 0(0.00) | Grad Norm 7.3726(10.3143) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 18.2301(17.6278) | Bit/dim 3.7795(3.8147) | Xent 0.9892(0.9873) | Loss 19.9484(24.3820) | Error 0.3544(0.3529) Steps 0(0.00) | Grad Norm 8.6787(10.2187) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 17.0030(17.7075) | Bit/dim 3.8010(3.8123) | Xent 1.0143(0.9897) | Loss 20.5906(23.2755) | Error 0.3611(0.3538) Steps 0(0.00) | Grad Norm 7.9540(10.2968) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 18.4019(17.6625) | Bit/dim 3.7897(3.8085) | Xent 1.0087(0.9945) | Loss 20.5056(22.3757) | Error 0.3622(0.3551) Steps 0(0.00) | Grad Norm 10.1965(11.0116) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 17.3076(17.6611) | Bit/dim 3.7906(3.8078) | Xent 0.8814(0.9874) | Loss 20.6211(21.8862) | Error 0.3156(0.3519) Steps 0(0.00) | Grad Norm 4.2082(10.7073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 89.8631, Epoch Time 1078.0522(999.5161), Bit/dim 3.8138(best: 3.8122), Xent 0.9265, Loss 4.2770, Error 0.3294(best: 0.3356)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 19.8875(17.8946) | Bit/dim 3.7988(3.8078) | Xent 0.9359(0.9824) | Loss 20.7074(27.0182) | Error 0.3533(0.3502) Steps 0(0.00) | Grad Norm 7.8658(10.2754) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 19.7969(17.7930) | Bit/dim 3.8154(3.8055) | Xent 1.0119(0.9823) | Loss 20.7046(25.2355) | Error 0.3633(0.3505) Steps 0(0.00) | Grad Norm 14.4026(10.8829) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 16.6808(17.6323) | Bit/dim 3.8073(3.8053) | Xent 0.9459(0.9842) | Loss 19.3048(23.8643) | Error 0.3178(0.3506) Steps 0(0.00) | Grad Norm 11.9822(11.3044) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 16.8345(17.5493) | Bit/dim 3.8335(3.8085) | Xent 0.9330(0.9874) | Loss 20.5036(22.9325) | Error 0.3444(0.3530) Steps 0(0.00) | Grad Norm 11.3996(11.7839) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 20.3018(17.5481) | Bit/dim 3.8109(3.8081) | Xent 1.0119(0.9906) | Loss 21.1388(22.2049) | Error 0.3633(0.3541) Steps 0(0.00) | Grad Norm 18.8344(11.7872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 90.0070, Epoch Time 1073.7953(1001.7445), Bit/dim 3.8173(best: 3.8122), Xent 1.0138, Loss 4.3242, Error 0.3573(best: 0.3294)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 16.4413(17.5409) | Bit/dim 3.7678(3.8069) | Xent 1.0168(0.9963) | Loss 19.3917(28.1253) | Error 0.3822(0.3564) Steps 0(0.00) | Grad Norm 8.7335(11.5169) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 17.3973(17.4779) | Bit/dim 3.8109(3.8060) | Xent 0.9898(0.9883) | Loss 19.6841(25.9109) | Error 0.3622(0.3536) Steps 0(0.00) | Grad Norm 5.8159(10.9594) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 17.1434(17.4818) | Bit/dim 3.7884(3.8059) | Xent 1.0141(0.9809) | Loss 20.0403(24.3266) | Error 0.3444(0.3508) Steps 0(0.00) | Grad Norm 14.2711(10.7033) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 19.8605(17.7519) | Bit/dim 3.7962(3.8065) | Xent 0.9701(0.9768) | Loss 21.7978(23.3303) | Error 0.3500(0.3483) Steps 0(0.00) | Grad Norm 7.1526(10.4526) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 17.7224(17.6587) | Bit/dim 3.7976(3.8031) | Xent 0.9854(0.9708) | Loss 18.7274(22.3338) | Error 0.3400(0.3467) Steps 0(0.00) | Grad Norm 16.2745(10.2129) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 17.6966(17.5667) | Bit/dim 3.7778(3.8022) | Xent 0.9251(0.9694) | Loss 20.4500(21.7623) | Error 0.3344(0.3460) Steps 0(0.00) | Grad Norm 12.6454(10.7886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 89.6031, Epoch Time 1079.1567(1004.0668), Bit/dim 3.8014(best: 3.8122), Xent 0.9208, Loss 4.2618, Error 0.3247(best: 0.3294)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 17.0102(17.4863) | Bit/dim 3.7999(3.8012) | Xent 0.9366(0.9639) | Loss 19.0536(26.8742) | Error 0.3378(0.3454) Steps 0(0.00) | Grad Norm 5.5348(10.4494) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 20.8039(17.8690) | Bit/dim 3.7859(3.8019) | Xent 0.9777(0.9594) | Loss 21.4997(25.0964) | Error 0.3311(0.3426) Steps 0(0.00) | Grad Norm 13.3892(10.2643) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 17.3727(17.9003) | Bit/dim 3.8121(3.7971) | Xent 0.9005(0.9497) | Loss 19.6345(23.7184) | Error 0.3178(0.3378) Steps 0(0.00) | Grad Norm 9.0979(9.6336) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 17.5040(17.8790) | Bit/dim 3.8143(3.7961) | Xent 0.9708(0.9456) | Loss 19.7702(22.7569) | Error 0.3478(0.3379) Steps 0(0.00) | Grad Norm 4.8998(9.6225) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 19.3693(17.8364) | Bit/dim 3.7871(3.7939) | Xent 0.9545(0.9474) | Loss 21.0509(22.0880) | Error 0.3422(0.3391) Steps 0(0.00) | Grad Norm 7.7914(9.7784) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 89.1441, Epoch Time 1101.5358(1006.9909), Bit/dim 3.7927(best: 3.8014), Xent 0.9119, Loss 4.2486, Error 0.3254(best: 0.3247)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 19.1437(17.8794) | Bit/dim 3.7513(3.7882) | Xent 0.8278(0.9507) | Loss 20.4629(28.2599) | Error 0.2878(0.3398) Steps 0(0.00) | Grad Norm 13.1783(10.0781) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 17.0481(17.8247) | Bit/dim 3.7836(3.7893) | Xent 0.9009(0.9438) | Loss 19.4595(26.0288) | Error 0.3267(0.3370) Steps 0(0.00) | Grad Norm 15.7764(10.3604) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 17.4884(17.8312) | Bit/dim 3.8041(3.7901) | Xent 0.9524(0.9518) | Loss 20.3571(24.4421) | Error 0.3189(0.3395) Steps 0(0.00) | Grad Norm 13.6828(10.7346) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 17.3445(17.7958) | Bit/dim 3.7747(3.7898) | Xent 0.9717(0.9555) | Loss 19.7074(23.2539) | Error 0.3544(0.3417) Steps 0(0.00) | Grad Norm 11.4490(10.7606) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 17.5641(17.7223) | Bit/dim 3.7735(3.7911) | Xent 1.0067(0.9546) | Loss 19.9048(22.3112) | Error 0.3567(0.3405) Steps 0(0.00) | Grad Norm 13.0560(10.8869) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 17.4513(17.8185) | Bit/dim 3.8164(3.7902) | Xent 0.9971(0.9558) | Loss 19.5776(21.6450) | Error 0.3578(0.3409) Steps 0(0.00) | Grad Norm 8.6631(10.4436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 90.0189, Epoch Time 1083.7797(1009.2946), Bit/dim 3.7863(best: 3.7927), Xent 0.9566, Loss 4.2647, Error 0.3405(best: 0.3247)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 17.8548(17.5959) | Bit/dim 3.7466(3.7872) | Xent 0.9313(0.9515) | Loss 20.8370(26.6673) | Error 0.3367(0.3396) Steps 0(0.00) | Grad Norm 6.6632(10.0562) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 19.0688(17.6315) | Bit/dim 3.7821(3.7874) | Xent 0.9597(0.9494) | Loss 20.9148(24.9295) | Error 0.3400(0.3386) Steps 0(0.00) | Grad Norm 11.1888(10.2413) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 16.8922(17.5095) | Bit/dim 3.8027(3.7888) | Xent 1.0060(0.9468) | Loss 20.2598(23.6176) | Error 0.3511(0.3380) Steps 0(0.00) | Grad Norm 13.5924(10.0669) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 17.6366(17.5875) | Bit/dim 3.7924(3.7902) | Xent 0.8747(0.9381) | Loss 20.5503(22.7053) | Error 0.3178(0.3349) Steps 0(0.00) | Grad Norm 7.1727(9.5547) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 17.0205(17.4991) | Bit/dim 3.7360(3.7866) | Xent 0.9961(0.9391) | Loss 20.8966(21.9889) | Error 0.3689(0.3352) Steps 0(0.00) | Grad Norm 10.3386(9.3812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 91.5384, Epoch Time 1071.0912(1011.1485), Bit/dim 3.7773(best: 3.7863), Xent 0.9008, Loss 4.2277, Error 0.3233(best: 0.3247)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 18.1561(17.6598) | Bit/dim 3.7852(3.7851) | Xent 0.9203(0.9341) | Loss 20.3308(28.2085) | Error 0.3311(0.3341) Steps 0(0.00) | Grad Norm 15.2612(9.5173) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 17.1869(17.6620) | Bit/dim 3.7728(3.7861) | Xent 0.8380(0.9372) | Loss 20.2276(26.0378) | Error 0.2933(0.3353) Steps 0(0.00) | Grad Norm 9.2569(10.3181) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 17.9661(17.6947) | Bit/dim 3.7686(3.7868) | Xent 0.8640(0.9336) | Loss 20.2650(24.5065) | Error 0.3200(0.3335) Steps 0(0.00) | Grad Norm 12.2980(10.0266) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 16.4528(17.5712) | Bit/dim 3.7600(3.7821) | Xent 0.8892(0.9262) | Loss 19.8351(23.2376) | Error 0.3256(0.3311) Steps 0(0.00) | Grad Norm 8.3532(9.4636) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 17.0334(17.6607) | Bit/dim 3.7884(3.7782) | Xent 0.9921(0.9361) | Loss 19.0892(22.4674) | Error 0.3600(0.3336) Steps 0(0.00) | Grad Norm 12.5139(9.8773) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.8996(17.6270) | Bit/dim 3.8116(3.7790) | Xent 1.0513(0.9406) | Loss 19.2810(21.7717) | Error 0.3878(0.3366) Steps 0(0.00) | Grad Norm 17.1665(10.5881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 93.1123, Epoch Time 1082.7396(1013.2962), Bit/dim 3.7797(best: 3.7773), Xent 0.9333, Loss 4.2464, Error 0.3341(best: 0.3233)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 17.2749(17.7183) | Bit/dim 3.7906(3.7787) | Xent 0.9051(0.9366) | Loss 20.4969(27.0348) | Error 0.3022(0.3353) Steps 0(0.00) | Grad Norm 10.9719(10.5080) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 17.3898(17.6851) | Bit/dim 3.7764(3.7780) | Xent 0.9184(0.9341) | Loss 19.5106(25.0896) | Error 0.3322(0.3346) Steps 0(0.00) | Grad Norm 7.2178(9.8180) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.9342(17.6275) | Bit/dim 3.7734(3.7770) | Xent 0.9463(0.9229) | Loss 19.6043(23.7724) | Error 0.3278(0.3296) Steps 0(0.00) | Grad Norm 12.5391(9.3526) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 17.4761(17.5113) | Bit/dim 3.7646(3.7769) | Xent 0.8997(0.9170) | Loss 20.5067(22.8087) | Error 0.3078(0.3270) Steps 0(0.00) | Grad Norm 9.6804(9.6335) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 17.7941(17.5719) | Bit/dim 3.7682(3.7763) | Xent 0.8620(0.9169) | Loss 19.9474(22.1851) | Error 0.3122(0.3278) Steps 0(0.00) | Grad Norm 10.8993(10.0927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 90.4122, Epoch Time 1075.6927(1015.1681), Bit/dim 3.7703(best: 3.7773), Xent 0.8892, Loss 4.2149, Error 0.3139(best: 0.3233)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 16.9676(17.5636) | Bit/dim 3.7904(3.7771) | Xent 0.9331(0.9142) | Loss 20.1154(28.2285) | Error 0.3344(0.3259) Steps 0(0.00) | Grad Norm 9.3145(9.7073) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 16.8233(17.4911) | Bit/dim 3.7620(3.7768) | Xent 0.8767(0.9085) | Loss 20.1759(25.9514) | Error 0.3167(0.3239) Steps 0(0.00) | Grad Norm 11.7653(9.8774) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 16.4746(17.4551) | Bit/dim 3.7514(3.7755) | Xent 0.8208(0.9136) | Loss 19.2740(24.3326) | Error 0.2789(0.3257) Steps 0(0.00) | Grad Norm 8.6768(10.3170) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 16.9678(17.4387) | Bit/dim 3.7567(3.7734) | Xent 0.8583(0.9082) | Loss 20.0056(23.1679) | Error 0.3144(0.3252) Steps 0(0.00) | Grad Norm 6.4155(9.8705) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 16.3854(17.3233) | Bit/dim 3.7543(3.7730) | Xent 0.9548(0.9029) | Loss 19.3506(22.2383) | Error 0.3389(0.3231) Steps 0(0.00) | Grad Norm 8.0905(9.2240) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 18.3788(17.4920) | Bit/dim 3.7884(3.7711) | Xent 0.9542(0.9170) | Loss 21.4998(21.6953) | Error 0.3300(0.3267) Steps 0(0.00) | Grad Norm 14.9989(10.1080) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 92.0232, Epoch Time 1071.7217(1016.8647), Bit/dim 3.7738(best: 3.7703), Xent 0.9829, Loss 4.2653, Error 0.3530(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 17.9681(17.5015) | Bit/dim 3.7619(3.7719) | Xent 0.9084(0.9160) | Loss 20.6245(26.9683) | Error 0.3189(0.3283) Steps 0(0.00) | Grad Norm 24.7018(11.0036) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 18.4325(17.5090) | Bit/dim 3.7456(3.7732) | Xent 0.8594(0.9119) | Loss 21.0941(25.1413) | Error 0.3078(0.3265) Steps 0(0.00) | Grad Norm 6.4380(10.6129) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 16.6931(17.5310) | Bit/dim 3.7456(3.7724) | Xent 0.8337(0.9057) | Loss 20.3319(23.8258) | Error 0.2889(0.3245) Steps 0(0.00) | Grad Norm 5.8784(9.8418) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.9323(17.5304) | Bit/dim 3.7658(3.7702) | Xent 0.9809(0.9078) | Loss 20.1689(22.7989) | Error 0.3511(0.3240) Steps 0(0.00) | Grad Norm 11.8156(9.8195) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.4780(17.6396) | Bit/dim 3.7529(3.7678) | Xent 0.9278(0.9051) | Loss 20.0441(21.9720) | Error 0.3200(0.3231) Steps 0(0.00) | Grad Norm 10.5277(9.7469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 91.1902, Epoch Time 1077.9752(1018.6980), Bit/dim 3.7633(best: 3.7703), Xent 0.8699, Loss 4.1983, Error 0.3097(best: 0.3139)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 19.8825(17.6007) | Bit/dim 3.7699(3.7682) | Xent 0.9140(0.9006) | Loss 20.1000(27.9439) | Error 0.3211(0.3227) Steps 0(0.00) | Grad Norm 12.3623(9.9246) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 17.8676(17.6058) | Bit/dim 3.7647(3.7672) | Xent 0.8316(0.8975) | Loss 19.8642(25.8949) | Error 0.2900(0.3200) Steps 0(0.00) | Grad Norm 13.0515(10.0343) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 19.2678(17.6154) | Bit/dim 3.7717(3.7663) | Xent 0.9372(0.8911) | Loss 20.4288(24.3524) | Error 0.3367(0.3175) Steps 0(0.00) | Grad Norm 10.6305(10.1468) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 17.1353(17.6203) | Bit/dim 3.7454(3.7646) | Xent 0.8984(0.8928) | Loss 18.6413(23.1764) | Error 0.3233(0.3178) Steps 0(0.00) | Grad Norm 6.3666(9.4589) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 19.8370(17.8135) | Bit/dim 3.7365(3.7599) | Xent 0.8191(0.8867) | Loss 21.2346(22.3940) | Error 0.3011(0.3168) Steps 0(0.00) | Grad Norm 8.2776(9.0865) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 19.0690(17.8648) | Bit/dim 3.7469(3.7602) | Xent 0.9303(0.8858) | Loss 21.0030(21.8132) | Error 0.3244(0.3166) Steps 0(0.00) | Grad Norm 11.5118(9.1210) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 93.2128, Epoch Time 1094.0629(1020.9590), Bit/dim 3.7611(best: 3.7633), Xent 0.8854, Loss 4.2037, Error 0.3137(best: 0.3097)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.6075(17.7664) | Bit/dim 3.7348(3.7616) | Xent 0.9153(0.8830) | Loss 19.1205(27.0344) | Error 0.3133(0.3150) Steps 0(0.00) | Grad Norm 11.4563(9.3995) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 18.0870(17.7274) | Bit/dim 3.7628(3.7625) | Xent 0.8606(0.8832) | Loss 20.2658(25.1061) | Error 0.3011(0.3149) Steps 0(0.00) | Grad Norm 11.3476(10.4554) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 16.5572(17.7686) | Bit/dim 3.8162(3.7676) | Xent 0.8799(0.8989) | Loss 19.2480(23.7496) | Error 0.3144(0.3199) Steps 0(0.00) | Grad Norm 7.8916(11.3065) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 18.3536(17.6569) | Bit/dim 3.7666(3.7640) | Xent 0.8614(0.8962) | Loss 20.8425(22.7888) | Error 0.2978(0.3201) Steps 0(0.00) | Grad Norm 7.4309(10.4415) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 19.5259(17.6273) | Bit/dim 3.7589(3.7628) | Xent 0.9684(0.8941) | Loss 20.3913(22.0814) | Error 0.3289(0.3188) Steps 0(0.00) | Grad Norm 11.2079(9.8761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 93.2117, Epoch Time 1078.6007(1022.6882), Bit/dim 3.7636(best: 3.7611), Xent 0.8672, Loss 4.1973, Error 0.3058(best: 0.3097)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.9658(17.6238) | Bit/dim 3.7642(3.7596) | Xent 0.8895(0.8946) | Loss 21.0626(27.9887) | Error 0.3133(0.3182) Steps 0(0.00) | Grad Norm 6.0443(9.9334) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 18.2153(17.5833) | Bit/dim 3.7664(3.7596) | Xent 0.8628(0.8840) | Loss 19.6311(25.8430) | Error 0.3311(0.3141) Steps 0(0.00) | Grad Norm 10.2811(9.9536) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 17.8891(17.7075) | Bit/dim 3.7626(3.7576) | Xent 0.8157(0.8766) | Loss 19.2914(24.2412) | Error 0.2833(0.3112) Steps 0(0.00) | Grad Norm 12.6546(9.6600) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 18.7317(17.8732) | Bit/dim 3.7692(3.7578) | Xent 0.9447(0.8717) | Loss 21.6671(23.1866) | Error 0.3378(0.3107) Steps 0(0.00) | Grad Norm 9.7292(9.5972) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 17.4912(17.8086) | Bit/dim 3.7793(3.7553) | Xent 0.9052(0.8716) | Loss 19.9121(22.2732) | Error 0.3267(0.3105) Steps 0(0.00) | Grad Norm 9.1854(9.5088) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 19.6906(17.8845) | Bit/dim 3.7572(3.7543) | Xent 0.8753(0.8683) | Loss 20.7294(21.6749) | Error 0.3011(0.3084) Steps 0(0.00) | Grad Norm 6.7272(9.1495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 94.3906, Epoch Time 1097.0741(1024.9198), Bit/dim 3.7486(best: 3.7611), Xent 0.8350, Loss 4.1661, Error 0.2963(best: 0.3058)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 17.3198(17.8665) | Bit/dim 3.7524(3.7549) | Xent 0.8904(0.8688) | Loss 20.7307(26.8947) | Error 0.3189(0.3089) Steps 0(0.00) | Grad Norm 7.2271(9.2171) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 17.7529(17.8166) | Bit/dim 3.7233(3.7526) | Xent 0.8731(0.8659) | Loss 19.2038(25.0314) | Error 0.3122(0.3083) Steps 0(0.00) | Grad Norm 8.6858(9.3262) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 16.5589(17.7965) | Bit/dim 3.7832(3.7501) | Xent 0.9255(0.8601) | Loss 20.1135(23.7051) | Error 0.3344(0.3067) Steps 0(0.00) | Grad Norm 13.2761(9.8818) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 16.1182(17.7901) | Bit/dim 3.7253(3.7498) | Xent 0.9179(0.8661) | Loss 20.0798(22.7257) | Error 0.3456(0.3092) Steps 0(0.00) | Grad Norm 10.3144(10.3093) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 18.3312(17.7114) | Bit/dim 3.7520(3.7492) | Xent 0.8365(0.8716) | Loss 19.9938(21.9498) | Error 0.2978(0.3121) Steps 0(0.00) | Grad Norm 9.5376(10.4550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 90.7780, Epoch Time 1087.9301(1026.8101), Bit/dim 3.7587(best: 3.7486), Xent 0.8755, Loss 4.1964, Error 0.3080(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 18.6733(17.8128) | Bit/dim 3.7404(3.7477) | Xent 0.8706(0.8668) | Loss 21.2520(27.5561) | Error 0.2989(0.3090) Steps 0(0.00) | Grad Norm 10.2051(10.3773) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 16.9477(17.8130) | Bit/dim 3.7528(3.7463) | Xent 0.8101(0.8624) | Loss 18.9438(25.6167) | Error 0.2822(0.3066) Steps 0(0.00) | Grad Norm 10.4187(9.9942) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 16.1862(17.6709) | Bit/dim 3.7840(3.7496) | Xent 0.8060(0.8602) | Loss 20.3155(24.1433) | Error 0.2789(0.3059) Steps 0(0.00) | Grad Norm 9.5418(10.0053) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 19.2824(17.7475) | Bit/dim 3.7321(3.7480) | Xent 0.8547(0.8510) | Loss 20.6336(23.0965) | Error 0.3200(0.3049) Steps 0(0.00) | Grad Norm 6.3915(9.0609) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 17.2168(17.7043) | Bit/dim 3.7422(3.7476) | Xent 0.9524(0.8533) | Loss 20.8675(22.3373) | Error 0.3367(0.3054) Steps 0(0.00) | Grad Norm 15.9385(9.2038) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 18.1208(17.6243) | Bit/dim 3.7629(3.7478) | Xent 0.8460(0.8516) | Loss 20.6879(21.6407) | Error 0.3100(0.3047) Steps 0(0.00) | Grad Norm 8.1823(9.1179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 92.0092, Epoch Time 1079.9946(1028.4056), Bit/dim 3.7503(best: 3.7486), Xent 0.8649, Loss 4.1828, Error 0.3083(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 17.4499(17.5355) | Bit/dim 3.7483(3.7486) | Xent 0.8404(0.8494) | Loss 19.5822(26.9137) | Error 0.2933(0.3039) Steps 0(0.00) | Grad Norm 9.0761(9.3664) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 18.8054(17.6658) | Bit/dim 3.7319(3.7444) | Xent 0.8059(0.8480) | Loss 19.8861(25.0770) | Error 0.2967(0.3037) Steps 0(0.00) | Grad Norm 11.9714(9.3847) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 18.7944(18.0036) | Bit/dim 3.7497(3.7450) | Xent 0.8566(0.8439) | Loss 19.5464(23.7804) | Error 0.3100(0.3012) Steps 0(0.00) | Grad Norm 13.1688(9.7726) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 18.8806(18.0396) | Bit/dim 3.7440(3.7456) | Xent 0.8938(0.8502) | Loss 20.5341(22.8123) | Error 0.3278(0.3037) Steps 0(0.00) | Grad Norm 7.2394(9.4931) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 17.2673(17.8427) | Bit/dim 3.7472(3.7465) | Xent 0.8774(0.8538) | Loss 20.0053(22.0370) | Error 0.3211(0.3045) Steps 0(0.00) | Grad Norm 8.9821(9.2219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 93.7703, Epoch Time 1096.4002(1030.4455), Bit/dim 3.7521(best: 3.7486), Xent 0.8589, Loss 4.1815, Error 0.3022(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 16.0785(17.6271) | Bit/dim 3.7704(3.7472) | Xent 0.8329(0.8506) | Loss 19.4097(27.8583) | Error 0.3100(0.3041) Steps 0(0.00) | Grad Norm 11.9774(9.6445) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 17.8041(17.7947) | Bit/dim 3.7287(3.7455) | Xent 0.7797(0.8473) | Loss 19.3676(25.7442) | Error 0.2567(0.3010) Steps 0(0.00) | Grad Norm 11.3901(10.2098) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 18.0704(17.8182) | Bit/dim 3.7405(3.7452) | Xent 0.8583(0.8506) | Loss 19.2965(24.2653) | Error 0.3278(0.3037) Steps 0(0.00) | Grad Norm 8.1096(10.4040) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 18.3808(17.8609) | Bit/dim 3.7622(3.7488) | Xent 0.7587(0.8410) | Loss 19.9356(23.1902) | Error 0.2767(0.2987) Steps 0(0.00) | Grad Norm 7.5898(9.8923) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.8953(17.7736) | Bit/dim 3.7216(3.7436) | Xent 0.8891(0.8464) | Loss 19.6863(22.3102) | Error 0.3178(0.3011) Steps 0(0.00) | Grad Norm 12.3168(10.3559) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 18.1663(17.7718) | Bit/dim 3.7691(3.7466) | Xent 0.8418(0.8481) | Loss 19.3596(21.6560) | Error 0.3122(0.3029) Steps 0(0.00) | Grad Norm 8.3507(10.0779) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 92.0154, Epoch Time 1090.3637(1032.2430), Bit/dim 3.7449(best: 3.7486), Xent 0.8498, Loss 4.1697, Error 0.3004(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 16.0282(17.7648) | Bit/dim 3.7156(3.7442) | Xent 0.8267(0.8461) | Loss 18.9070(26.7571) | Error 0.2878(0.3025) Steps 0(0.00) | Grad Norm 5.7704(10.1774) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 17.2822(17.8411) | Bit/dim 3.7456(3.7421) | Xent 0.8708(0.8401) | Loss 20.1248(24.9703) | Error 0.2956(0.3010) Steps 0(0.00) | Grad Norm 10.6259(9.9578) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 17.3251(17.7518) | Bit/dim 3.7195(3.7382) | Xent 0.7320(0.8352) | Loss 20.8324(23.6414) | Error 0.2533(0.2984) Steps 0(0.00) | Grad Norm 9.9727(9.7620) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 16.2244(17.9207) | Bit/dim 3.7552(3.7395) | Xent 0.8343(0.8390) | Loss 19.6735(22.6877) | Error 0.2900(0.2987) Steps 0(0.00) | Grad Norm 9.6889(10.1641) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 18.1075(17.8139) | Bit/dim 3.7160(3.7424) | Xent 0.8453(0.8458) | Loss 20.2474(21.9524) | Error 0.2978(0.3007) Steps 0(0.00) | Grad Norm 10.3610(10.3822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 93.2613, Epoch Time 1090.7541(1033.9983), Bit/dim 3.7380(best: 3.7449), Xent 0.8082, Loss 4.1420, Error 0.2858(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 17.2938(17.7252) | Bit/dim 3.7553(3.7406) | Xent 0.8185(0.8338) | Loss 19.9077(27.8771) | Error 0.2756(0.2950) Steps 0(0.00) | Grad Norm 8.1675(9.8059) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 16.1440(17.6812) | Bit/dim 3.7352(3.7383) | Xent 0.7893(0.8186) | Loss 19.3420(25.7382) | Error 0.2767(0.2903) Steps 0(0.00) | Grad Norm 5.6766(9.0174) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 17.0715(17.7397) | Bit/dim 3.7057(3.7321) | Xent 0.8372(0.8187) | Loss 19.2521(24.2142) | Error 0.2956(0.2903) Steps 0(0.00) | Grad Norm 6.3187(8.9125) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 17.2637(17.6075) | Bit/dim 3.7454(3.7326) | Xent 0.8264(0.8169) | Loss 20.8000(23.1206) | Error 0.2944(0.2909) Steps 0(0.00) | Grad Norm 8.4182(8.5794) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 17.1299(17.5503) | Bit/dim 3.7310(3.7352) | Xent 0.8274(0.8145) | Loss 19.6989(22.1819) | Error 0.2878(0.2922) Steps 0(0.00) | Grad Norm 11.2905(9.3486) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 17.9639(17.3958) | Bit/dim 3.7408(3.7393) | Xent 0.8508(0.8233) | Loss 19.6470(21.4685) | Error 0.3011(0.2959) Steps 0(0.00) | Grad Norm 10.2196(9.8877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 93.9479, Epoch Time 1072.6232(1035.1571), Bit/dim 3.7468(best: 3.7380), Xent 0.8333, Loss 4.1635, Error 0.2899(best: 0.2858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 17.2957(17.5649) | Bit/dim 3.7409(3.7377) | Xent 0.7780(0.8197) | Loss 20.1475(26.9516) | Error 0.2911(0.2947) Steps 0(0.00) | Grad Norm 7.7120(9.6689) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.3319(17.3787) | Bit/dim 3.7054(3.7373) | Xent 0.7643(0.8088) | Loss 18.8251(25.0661) | Error 0.2633(0.2906) Steps 0(0.00) | Grad Norm 5.4131(8.8876) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 17.0488(17.3670) | Bit/dim 3.7402(3.7355) | Xent 0.8424(0.8134) | Loss 19.9746(23.6920) | Error 0.3011(0.2916) Steps 0(0.00) | Grad Norm 10.9351(9.4022) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.6148(17.4380) | Bit/dim 3.7236(3.7336) | Xent 0.7910(0.8156) | Loss 20.3505(22.6680) | Error 0.2733(0.2928) Steps 0(0.00) | Grad Norm 5.6449(9.8606) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 19.8753(17.6633) | Bit/dim 3.7601(3.7351) | Xent 0.7747(0.8200) | Loss 20.6026(21.9909) | Error 0.2656(0.2936) Steps 0(0.00) | Grad Norm 6.8410(9.6961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 94.1825, Epoch Time 1083.7202(1036.6140), Bit/dim 3.7361(best: 3.7380), Xent 0.8082, Loss 4.1402, Error 0.2861(best: 0.2858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 18.4368(17.6298) | Bit/dim 3.7209(3.7341) | Xent 0.7971(0.8106) | Loss 20.2366(28.0291) | Error 0.2844(0.2909) Steps 0(0.00) | Grad Norm 7.2704(9.1010) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 18.7772(17.6357) | Bit/dim 3.6744(3.7335) | Xent 0.8915(0.8102) | Loss 19.9135(25.9712) | Error 0.3200(0.2924) Steps 0(0.00) | Grad Norm 13.5940(9.2703) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 16.9149(17.5957) | Bit/dim 3.7250(3.7298) | Xent 0.7754(0.8117) | Loss 19.4433(24.3184) | Error 0.2856(0.2924) Steps 0(0.00) | Grad Norm 6.2839(9.6927) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 17.6621(17.7868) | Bit/dim 3.7012(3.7286) | Xent 0.8018(0.8056) | Loss 20.2324(23.2250) | Error 0.2867(0.2881) Steps 0(0.00) | Grad Norm 7.1051(9.2217) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 17.7208(17.8526) | Bit/dim 3.7501(3.7278) | Xent 0.8244(0.8059) | Loss 19.2215(22.3972) | Error 0.2900(0.2884) Steps 0(0.00) | Grad Norm 15.0447(9.1161) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 16.9247(17.7416) | Bit/dim 3.7234(3.7283) | Xent 0.8624(0.8116) | Loss 20.1779(21.8621) | Error 0.3100(0.2894) Steps 0(0.00) | Grad Norm 12.8327(9.4756) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 93.2706, Epoch Time 1089.5221(1038.2012), Bit/dim 3.7293(best: 3.7361), Xent 0.8306, Loss 4.1446, Error 0.2912(best: 0.2858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 18.1334(17.7117) | Bit/dim 3.7507(3.7259) | Xent 0.8234(0.8126) | Loss 20.2719(27.1323) | Error 0.2767(0.2892) Steps 0(0.00) | Grad Norm 13.3712(9.8295) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 17.9665(17.7273) | Bit/dim 3.7732(3.7318) | Xent 0.8001(0.8096) | Loss 19.6408(25.3257) | Error 0.2878(0.2884) Steps 0(0.00) | Grad Norm 9.2889(10.4905) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 17.3312(17.7654) | Bit/dim 3.7440(3.7309) | Xent 0.7502(0.8068) | Loss 19.8373(23.8726) | Error 0.2744(0.2874) Steps 0(0.00) | Grad Norm 5.7442(9.7962) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.9053(17.7683) | Bit/dim 3.7160(3.7285) | Xent 0.8629(0.8103) | Loss 19.9545(22.8462) | Error 0.3356(0.2889) Steps 0(0.00) | Grad Norm 8.3499(10.1146) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 19.2937(17.7366) | Bit/dim 3.7174(3.7257) | Xent 0.8267(0.8105) | Loss 20.7852(22.1105) | Error 0.2944(0.2886) Steps 0(0.00) | Grad Norm 8.7785(9.3778) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 92.8991, Epoch Time 1086.0552(1039.6368), Bit/dim 3.7186(best: 3.7293), Xent 0.8126, Loss 4.1249, Error 0.2886(best: 0.2858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 17.0720(17.6369) | Bit/dim 3.7116(3.7240) | Xent 0.7982(0.8039) | Loss 19.6235(27.5915) | Error 0.2744(0.2857) Steps 0(0.00) | Grad Norm 8.4092(8.8496) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 18.3239(17.6355) | Bit/dim 3.7405(3.7232) | Xent 0.7116(0.7954) | Loss 19.9458(25.5993) | Error 0.2556(0.2820) Steps 0(0.00) | Grad Norm 9.9306(8.8483) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 18.7817(17.6500) | Bit/dim 3.7154(3.7224) | Xent 0.8017(0.7902) | Loss 20.4070(24.1287) | Error 0.3011(0.2809) Steps 0(0.00) | Grad Norm 11.3627(8.7537) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 18.1674(17.7011) | Bit/dim 3.7667(3.7223) | Xent 0.7602(0.7902) | Loss 20.0632(23.0686) | Error 0.2644(0.2810) Steps 0(0.00) | Grad Norm 13.9928(8.8044) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 18.1774(17.6967) | Bit/dim 3.7279(3.7224) | Xent 0.8000(0.7872) | Loss 19.7704(22.2650) | Error 0.2989(0.2807) Steps 0(0.00) | Grad Norm 5.7479(8.8631) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.6545(17.5029) | Bit/dim 3.7377(3.7239) | Xent 0.8022(0.7889) | Loss 19.1574(21.5524) | Error 0.2633(0.2801) Steps 0(0.00) | Grad Norm 9.0660(8.7058) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 93.1369, Epoch Time 1075.9312(1040.7257), Bit/dim 3.7245(best: 3.7186), Xent 0.8033, Loss 4.1262, Error 0.2808(best: 0.2858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 18.3743(17.4587) | Bit/dim 3.7734(3.7275) | Xent 0.8270(0.7990) | Loss 20.3396(26.7217) | Error 0.2978(0.2837) Steps 0(0.00) | Grad Norm 9.5114(9.8502) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 17.2495(17.5454) | Bit/dim 3.7190(3.7241) | Xent 0.8989(0.8046) | Loss 20.6250(25.0077) | Error 0.3122(0.2863) Steps 0(0.00) | Grad Norm 12.3833(10.0215) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 17.8580(17.5687) | Bit/dim 3.7064(3.7276) | Xent 0.7900(0.8038) | Loss 20.5888(23.6412) | Error 0.2911(0.2869) Steps 0(0.00) | Grad Norm 7.8700(9.8011) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 15.8603(17.4860) | Bit/dim 3.7013(3.7276) | Xent 0.7359(0.7983) | Loss 19.6836(22.6203) | Error 0.2800(0.2867) Steps 0(0.00) | Grad Norm 6.3266(9.7119) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 19.3252(17.5826) | Bit/dim 3.7096(3.7218) | Xent 0.7947(0.7962) | Loss 19.8602(21.9482) | Error 0.2867(0.2862) Steps 0(0.00) | Grad Norm 5.8833(9.2347) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 96.2912, Epoch Time 1084.9322(1042.0519), Bit/dim 3.7188(best: 3.7186), Xent 0.7821, Loss 4.1099, Error 0.2755(best: 0.2808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.5600(17.6630) | Bit/dim 3.7358(3.7230) | Xent 0.7726(0.7848) | Loss 20.0306(27.9255) | Error 0.2811(0.2825) Steps 0(0.00) | Grad Norm 7.6491(8.7697) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 17.6511(17.5683) | Bit/dim 3.7049(3.7212) | Xent 0.7790(0.7738) | Loss 20.1921(25.7954) | Error 0.2811(0.2771) Steps 0(0.00) | Grad Norm 6.6859(8.2618) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 16.7135(17.5303) | Bit/dim 3.7143(3.7182) | Xent 0.7381(0.7708) | Loss 18.7292(24.2424) | Error 0.2522(0.2747) Steps 0(0.00) | Grad Norm 7.5852(7.9518) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 19.5149(17.6113) | Bit/dim 3.7439(3.7166) | Xent 0.8077(0.7703) | Loss 19.4342(23.1136) | Error 0.3011(0.2760) Steps 0(0.00) | Grad Norm 10.6034(7.8637) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 17.6767(17.7835) | Bit/dim 3.6854(3.7152) | Xent 0.8296(0.7743) | Loss 20.1220(22.2893) | Error 0.3167(0.2777) Steps 0(0.00) | Grad Norm 6.2547(8.3103) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 18.4218(17.8020) | Bit/dim 3.7058(3.7159) | Xent 0.7698(0.7721) | Loss 20.3754(21.7067) | Error 0.2578(0.2763) Steps 0(0.00) | Grad Norm 7.9065(8.0059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 93.7655, Epoch Time 1087.5474(1043.4167), Bit/dim 3.7106(best: 3.7186), Xent 0.7656, Loss 4.0934, Error 0.2702(best: 0.2755)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 18.3991(17.6675) | Bit/dim 3.7311(3.7137) | Xent 0.7309(0.7732) | Loss 20.9170(26.7923) | Error 0.2578(0.2763) Steps 0(0.00) | Grad Norm 13.1322(8.6560) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.7761(17.6607) | Bit/dim 3.7174(3.7124) | Xent 0.6704(0.7699) | Loss 21.1471(25.0226) | Error 0.2522(0.2757) Steps 0(0.00) | Grad Norm 8.7625(8.7402) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 18.0834(17.5748) | Bit/dim 3.7356(3.7113) | Xent 0.7925(0.7617) | Loss 20.5868(23.6175) | Error 0.2656(0.2721) Steps 0(0.00) | Grad Norm 6.8110(8.6446) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.3575(17.5640) | Bit/dim 3.7379(3.7151) | Xent 0.7392(0.7590) | Loss 19.6986(22.6651) | Error 0.2656(0.2713) Steps 0(0.00) | Grad Norm 8.9030(8.1725) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 17.8282(17.5114) | Bit/dim 3.7154(3.7155) | Xent 0.9359(0.7710) | Loss 20.8741(21.9389) | Error 0.3211(0.2755) Steps 0(0.00) | Grad Norm 13.8572(9.3554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 92.8518, Epoch Time 1069.6967(1044.2051), Bit/dim 3.7129(best: 3.7106), Xent 0.8079, Loss 4.1168, Error 0.2832(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 17.4943(17.4991) | Bit/dim 3.7207(3.7168) | Xent 0.7138(0.7708) | Loss 19.7314(28.0916) | Error 0.2633(0.2754) Steps 0(0.00) | Grad Norm 7.5815(9.1009) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 17.1826(17.4927) | Bit/dim 3.7101(3.7154) | Xent 0.7274(0.7728) | Loss 18.8799(25.8521) | Error 0.2533(0.2759) Steps 0(0.00) | Grad Norm 7.6583(9.8237) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 17.1411(17.6007) | Bit/dim 3.7360(3.7172) | Xent 0.7901(0.7732) | Loss 20.2102(24.3719) | Error 0.2922(0.2770) Steps 0(0.00) | Grad Norm 6.9528(9.6404) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.7494(17.6004) | Bit/dim 3.7205(3.7186) | Xent 0.7640(0.7692) | Loss 18.8962(23.2238) | Error 0.2589(0.2752) Steps 0(0.00) | Grad Norm 6.7758(9.0666) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 17.9466(17.5826) | Bit/dim 3.6966(3.7138) | Xent 0.8025(0.7700) | Loss 20.5753(22.4328) | Error 0.2967(0.2757) Steps 0(0.00) | Grad Norm 10.3151(8.8978) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 17.1517(17.7109) | Bit/dim 3.7045(3.7110) | Xent 0.7404(0.7627) | Loss 18.8225(21.6454) | Error 0.2611(0.2737) Steps 0(0.00) | Grad Norm 11.4671(9.0639) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 93.1666, Epoch Time 1086.2722(1045.4672), Bit/dim 3.7168(best: 3.7106), Xent 0.8166, Loss 4.1251, Error 0.2842(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 18.2173(17.5986) | Bit/dim 3.7475(3.7129) | Xent 0.8047(0.7669) | Loss 19.1439(26.8465) | Error 0.2856(0.2744) Steps 0(0.00) | Grad Norm 12.4178(9.3522) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 17.1145(17.6135) | Bit/dim 3.7200(3.7138) | Xent 0.7480(0.7636) | Loss 20.1607(25.0235) | Error 0.2711(0.2733) Steps 0(0.00) | Grad Norm 8.2950(9.3841) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.2212(17.7347) | Bit/dim 3.6902(3.7091) | Xent 0.7484(0.7659) | Loss 19.4730(23.7695) | Error 0.2644(0.2730) Steps 0(0.00) | Grad Norm 10.5822(9.7994) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 17.5727(17.7849) | Bit/dim 3.6995(3.7122) | Xent 0.7631(0.7616) | Loss 20.2453(22.7758) | Error 0.2678(0.2705) Steps 0(0.00) | Grad Norm 9.3910(9.5359) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.7202(17.7602) | Bit/dim 3.7017(3.7101) | Xent 0.7183(0.7601) | Loss 19.9557(22.0805) | Error 0.2556(0.2702) Steps 0(0.00) | Grad Norm 6.1046(8.9105) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 93.8492, Epoch Time 1086.9334(1046.7111), Bit/dim 3.7188(best: 3.7106), Xent 0.8152, Loss 4.1264, Error 0.2814(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 17.4166(17.6353) | Bit/dim 3.7051(3.7096) | Xent 0.7835(0.7615) | Loss 19.7867(28.1529) | Error 0.2689(0.2707) Steps 0(0.00) | Grad Norm 10.8238(9.5130) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 17.9535(17.6544) | Bit/dim 3.7295(3.7067) | Xent 0.7386(0.7545) | Loss 20.1111(25.9166) | Error 0.2500(0.2687) Steps 0(0.00) | Grad Norm 5.9043(9.2258) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.9190(17.6913) | Bit/dim 3.7175(3.7073) | Xent 0.6952(0.7506) | Loss 19.5619(24.3245) | Error 0.2611(0.2685) Steps 0(0.00) | Grad Norm 9.7630(9.4469) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 17.9432(17.7612) | Bit/dim 3.6759(3.7021) | Xent 0.7950(0.7575) | Loss 19.5572(23.1265) | Error 0.2656(0.2708) Steps 0(0.00) | Grad Norm 6.8214(9.5324) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.5685(17.7138) | Bit/dim 3.7471(3.7067) | Xent 0.7289(0.7554) | Loss 19.6902(22.1850) | Error 0.2711(0.2701) Steps 0(0.00) | Grad Norm 12.0556(9.5638) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.8876(17.7004) | Bit/dim 3.7306(3.7090) | Xent 0.7427(0.7539) | Loss 20.3573(21.6182) | Error 0.2467(0.2689) Steps 0(0.00) | Grad Norm 8.1276(9.1136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 93.4439, Epoch Time 1085.1191(1047.8634), Bit/dim 3.7087(best: 3.7106), Xent 0.7446, Loss 4.0809, Error 0.2587(best: 0.2702)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 17.4833(17.5835) | Bit/dim 3.7084(3.7086) | Xent 0.7637(0.7532) | Loss 20.1297(26.8990) | Error 0.2844(0.2682) Steps 0(0.00) | Grad Norm 12.0597(9.1297) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.8186(17.4968) | Bit/dim 3.7532(3.7120) | Xent 0.7231(0.7562) | Loss 18.3683(25.0108) | Error 0.2600(0.2701) Steps 0(0.00) | Grad Norm 9.3337(9.5432) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 19.8090(17.5931) | Bit/dim 3.7124(3.7107) | Xent 0.7688(0.7516) | Loss 19.7117(23.6662) | Error 0.2611(0.2678) Steps 0(0.00) | Grad Norm 6.8593(9.2450) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 18.8683(17.7619) | Bit/dim 3.7030(3.7078) | Xent 0.7257(0.7467) | Loss 21.0399(22.7158) | Error 0.2767(0.2672) Steps 0(0.00) | Grad Norm 7.1168(8.9053) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 17.7050(17.7819) | Bit/dim 3.7194(3.7053) | Xent 0.7966(0.7398) | Loss 20.4634(21.9338) | Error 0.2667(0.2639) Steps 0(0.00) | Grad Norm 5.3429(8.3498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 93.8708, Epoch Time 1087.6183(1049.0560), Bit/dim 3.7141(best: 3.7087), Xent 0.8256, Loss 4.1269, Error 0.2864(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 17.6467(17.7704) | Bit/dim 3.7102(3.7047) | Xent 0.7780(0.7487) | Loss 20.5075(27.8560) | Error 0.2733(0.2658) Steps 0(0.00) | Grad Norm 12.4347(9.3239) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 18.8281(17.8102) | Bit/dim 3.7452(3.7075) | Xent 0.7473(0.7534) | Loss 21.5092(25.8810) | Error 0.2667(0.2678) Steps 0(0.00) | Grad Norm 8.5190(9.5505) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 18.9147(17.7441) | Bit/dim 3.7236(3.7064) | Xent 0.7194(0.7484) | Loss 20.5350(24.2926) | Error 0.2611(0.2671) Steps 0(0.00) | Grad Norm 8.2218(9.4162) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 16.9151(17.7324) | Bit/dim 3.6734(3.7063) | Xent 0.7050(0.7370) | Loss 18.7347(23.0634) | Error 0.2478(0.2627) Steps 0(0.00) | Grad Norm 6.3304(8.8584) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.7534(17.8735) | Bit/dim 3.7016(3.7044) | Xent 0.7722(0.7389) | Loss 19.5989(22.2725) | Error 0.2822(0.2654) Steps 0(0.00) | Grad Norm 14.4220(8.6285) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 20.8513(17.9948) | Bit/dim 3.7054(3.7041) | Xent 0.7169(0.7349) | Loss 21.1774(21.6824) | Error 0.2600(0.2629) Steps 0(0.00) | Grad Norm 8.0830(8.4534) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 95.0457, Epoch Time 1099.0321(1050.5553), Bit/dim 3.7004(best: 3.7087), Xent 0.7577, Loss 4.0792, Error 0.2673(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.8147(17.9663) | Bit/dim 3.6796(3.7024) | Xent 0.7388(0.7350) | Loss 18.7719(26.8867) | Error 0.2656(0.2616) Steps 0(0.00) | Grad Norm 9.0700(8.6608) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.9117(17.9970) | Bit/dim 3.6864(3.7021) | Xent 0.8581(0.7461) | Loss 20.3487(25.1096) | Error 0.3167(0.2656) Steps 0(0.00) | Grad Norm 25.2550(9.8706) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.3658(18.0293) | Bit/dim 3.7223(3.7052) | Xent 0.7007(0.7500) | Loss 18.6030(23.7600) | Error 0.2444(0.2676) Steps 0(0.00) | Grad Norm 7.4882(10.2625) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 18.6864(18.1738) | Bit/dim 3.7123(3.7057) | Xent 0.7740(0.7480) | Loss 19.6522(22.7565) | Error 0.2733(0.2665) Steps 0(0.00) | Grad Norm 8.2265(9.7145) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.2510(18.1402) | Bit/dim 3.6786(3.7066) | Xent 0.7274(0.7504) | Loss 18.8539(21.9713) | Error 0.2789(0.2683) Steps 0(0.00) | Grad Norm 9.0605(9.6191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 95.2197, Epoch Time 1115.0310(1052.4896), Bit/dim 3.7046(best: 3.7004), Xent 0.7569, Loss 4.0830, Error 0.2614(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 18.7323(18.2908) | Bit/dim 3.7174(3.7071) | Xent 0.7614(0.7488) | Loss 21.4844(28.1183) | Error 0.2689(0.2665) Steps 0(0.00) | Grad Norm 7.5559(9.2794) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 19.7307(18.1802) | Bit/dim 3.6835(3.7065) | Xent 0.7464(0.7408) | Loss 20.9972(25.9356) | Error 0.2678(0.2645) Steps 0(0.00) | Grad Norm 12.2518(9.4820) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 17.6837(17.9774) | Bit/dim 3.7188(3.7056) | Xent 0.7082(0.7296) | Loss 19.0497(24.3174) | Error 0.2589(0.2614) Steps 0(0.00) | Grad Norm 8.6636(8.6698) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 17.6450(17.8422) | Bit/dim 3.7046(3.7037) | Xent 0.8230(0.7270) | Loss 20.2982(23.1770) | Error 0.2944(0.2595) Steps 0(0.00) | Grad Norm 8.3390(8.4205) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 17.9758(17.7946) | Bit/dim 3.7115(3.7027) | Xent 0.7803(0.7257) | Loss 20.5321(22.4060) | Error 0.2822(0.2596) Steps 0(0.00) | Grad Norm 14.9666(8.7669) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 17.6930(17.7101) | Bit/dim 3.6897(3.6994) | Xent 0.6709(0.7372) | Loss 18.9186(21.6947) | Error 0.2333(0.2634) Steps 0(0.00) | Grad Norm 9.1167(9.2480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 95.1558, Epoch Time 1085.9780(1053.4942), Bit/dim 3.7184(best: 3.7004), Xent 0.7627, Loss 4.0998, Error 0.2685(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 17.1166(17.8761) | Bit/dim 3.6944(3.6996) | Xent 0.7563(0.7304) | Loss 18.6492(26.7585) | Error 0.2722(0.2598) Steps 0(0.00) | Grad Norm 11.5744(9.1918) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 17.8868(17.8174) | Bit/dim 3.6843(3.6990) | Xent 0.7121(0.7222) | Loss 19.5788(24.9383) | Error 0.2456(0.2571) Steps 0(0.00) | Grad Norm 8.6643(8.9924) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 18.2800(17.7496) | Bit/dim 3.6972(3.6989) | Xent 0.6882(0.7283) | Loss 20.5882(23.6474) | Error 0.2500(0.2594) Steps 0(0.00) | Grad Norm 6.0418(8.9917) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.4993(17.6907) | Bit/dim 3.6771(3.6992) | Xent 0.7451(0.7241) | Loss 18.8777(22.5595) | Error 0.2711(0.2579) Steps 0(0.00) | Grad Norm 11.9007(8.9591) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 17.3455(17.7817) | Bit/dim 3.6929(3.7023) | Xent 0.7656(0.7260) | Loss 20.0075(21.8385) | Error 0.2600(0.2578) Steps 0(0.00) | Grad Norm 9.3848(9.2357) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 94.8695, Epoch Time 1092.8628(1054.6753), Bit/dim 3.7092(best: 3.7004), Xent 0.9326, Loss 4.1756, Error 0.3336(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 17.2021(17.7336) | Bit/dim 3.7254(3.6986) | Xent 0.7794(0.7365) | Loss 19.5110(27.7328) | Error 0.2811(0.2618) Steps 0(0.00) | Grad Norm 12.4676(10.3008) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.7749(17.7131) | Bit/dim 3.7268(3.7034) | Xent 0.6701(0.7270) | Loss 20.1803(25.6769) | Error 0.2456(0.2591) Steps 0(0.00) | Grad Norm 9.2552(9.7625) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 17.5144(17.4817) | Bit/dim 3.6850(3.7028) | Xent 0.7128(0.7219) | Loss 19.8297(24.0873) | Error 0.2778(0.2581) Steps 0(0.00) | Grad Norm 6.0097(8.8591) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.3805(17.5072) | Bit/dim 3.7127(3.7022) | Xent 0.7274(0.7172) | Loss 19.6090(22.9414) | Error 0.2533(0.2560) Steps 0(0.00) | Grad Norm 5.3140(8.5285) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 18.0887(17.6163) | Bit/dim 3.6849(3.7003) | Xent 0.7633(0.7092) | Loss 20.4558(22.1786) | Error 0.2889(0.2535) Steps 0(0.00) | Grad Norm 7.8214(8.0051) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.9713(17.6661) | Bit/dim 3.6904(3.6976) | Xent 0.7066(0.7115) | Loss 19.3221(21.5816) | Error 0.2544(0.2550) Steps 0(0.00) | Grad Norm 8.8126(8.2509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 94.6809, Epoch Time 1079.0377(1055.4062), Bit/dim 3.6936(best: 3.7004), Xent 0.7428, Loss 4.0650, Error 0.2643(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.3879(17.6576) | Bit/dim 3.7050(3.6957) | Xent 0.6187(0.7055) | Loss 17.8623(26.9160) | Error 0.2256(0.2525) Steps 0(0.00) | Grad Norm 10.3268(8.2743) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 17.4764(17.6185) | Bit/dim 3.6681(3.6958) | Xent 0.6659(0.7062) | Loss 19.4497(25.0690) | Error 0.2433(0.2530) Steps 0(0.00) | Grad Norm 9.2707(8.7038) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 18.5159(17.7648) | Bit/dim 3.7005(3.6980) | Xent 0.7323(0.7074) | Loss 20.1770(23.6901) | Error 0.2467(0.2527) Steps 0(0.00) | Grad Norm 6.5682(8.3118) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 19.6870(17.7326) | Bit/dim 3.6769(3.6963) | Xent 0.7504(0.7141) | Loss 21.1509(22.7217) | Error 0.2622(0.2544) Steps 0(0.00) | Grad Norm 11.4549(8.7766) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_eta_1_0_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --eta 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
