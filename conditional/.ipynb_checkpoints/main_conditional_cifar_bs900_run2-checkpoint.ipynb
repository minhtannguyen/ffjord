{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_run1/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 4630 | Time 21.6568(31.4662) | Bit/dim 3.6701(3.6794) | Xent 0.0855(0.0765) | Loss 3.7128(3.7176) | Error 0.0256(0.0265) Steps 898(877.18) | Grad Norm 3.1503(3.4754) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 21.3246(28.8199) | Bit/dim 3.6677(3.6763) | Xent 0.0453(0.0733) | Loss 3.6903(3.7130) | Error 0.0167(0.0254) Steps 874(875.32) | Grad Norm 2.7688(3.4163) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 21.8083(26.8733) | Bit/dim 3.6783(3.6761) | Xent 0.0505(0.0675) | Loss 3.7035(3.7098) | Error 0.0222(0.0234) Steps 868(874.91) | Grad Norm 1.8967(3.1607) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 21.5328(25.4592) | Bit/dim 3.6595(3.6715) | Xent 0.0640(0.0639) | Loss 3.6915(3.7035) | Error 0.0200(0.0222) Steps 868(874.07) | Grad Norm 2.5255(2.9726) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 21.6206(24.4636) | Bit/dim 3.6944(3.6716) | Xent 0.0419(0.0626) | Loss 3.7154(3.7029) | Error 0.0178(0.0216) Steps 892(875.20) | Grad Norm 2.1613(2.8131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 113.6007, Epoch Time 1342.1426(1989.0876), Bit/dim 3.6690(best: inf), Xent 2.7007, Loss 5.0193, Error 0.3866(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 21.4906(23.7582) | Bit/dim 3.7062(3.6710) | Xent 0.0359(0.0610) | Loss 3.7242(3.7016) | Error 0.0133(0.0213) Steps 874(874.40) | Grad Norm 1.9734(2.7512) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 21.3344(23.1223) | Bit/dim 3.6861(3.6684) | Xent 0.0584(0.0586) | Loss 3.7153(3.6977) | Error 0.0189(0.0207) Steps 886(873.43) | Grad Norm 2.3583(2.5836) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 22.1782(22.6779) | Bit/dim 3.6245(3.6639) | Xent 0.0304(0.0549) | Loss 3.6397(3.6914) | Error 0.0122(0.0194) Steps 886(872.14) | Grad Norm 2.3881(2.4159) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 21.1254(22.3212) | Bit/dim 3.6595(3.6607) | Xent 0.0368(0.0514) | Loss 3.6779(3.6864) | Error 0.0111(0.0183) Steps 856(870.96) | Grad Norm 2.5839(2.3797) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 21.5812(22.0757) | Bit/dim 3.6639(3.6608) | Xent 0.0634(0.0527) | Loss 3.6956(3.6872) | Error 0.0222(0.0184) Steps 880(870.69) | Grad Norm 3.2547(2.5293) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 21.3785(21.9109) | Bit/dim 3.7129(3.6648) | Xent 0.0577(0.0577) | Loss 3.7417(3.6937) | Error 0.0244(0.0207) Steps 886(870.08) | Grad Norm 5.6566(2.9922) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 105.9317, Epoch Time 1301.6944(1968.4658), Bit/dim 3.6784(best: 3.6690), Xent 2.7370, Loss 5.0469, Error 0.3831(best: 0.3866)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.6676(21.6957) | Bit/dim 3.6638(3.6666) | Xent 0.0560(0.0593) | Loss 3.6918(3.6962) | Error 0.0222(0.0214) Steps 856(870.91) | Grad Norm 3.0702(3.3881) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 21.6554(21.7169) | Bit/dim 3.6986(3.6693) | Xent 0.0752(0.0622) | Loss 3.7362(3.7004) | Error 0.0256(0.0222) Steps 874(871.85) | Grad Norm 5.2096(3.6407) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 21.2420(21.7115) | Bit/dim 3.6749(3.6716) | Xent 0.0916(0.0639) | Loss 3.7207(3.7035) | Error 0.0367(0.0225) Steps 874(873.60) | Grad Norm 5.8752(3.7382) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 21.4283(21.7212) | Bit/dim 3.6377(3.6721) | Xent 0.1045(0.0669) | Loss 3.6900(3.7055) | Error 0.0389(0.0234) Steps 892(875.63) | Grad Norm 3.7906(3.7234) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 21.5430(21.6663) | Bit/dim 3.6700(3.6725) | Xent 0.0838(0.0696) | Loss 3.7119(3.7073) | Error 0.0311(0.0247) Steps 886(874.36) | Grad Norm 5.1680(3.8980) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 105.6273, Epoch Time 1308.6950(1948.6726), Bit/dim 3.6777(best: 3.6690), Xent 2.7866, Loss 5.0710, Error 0.3753(best: 0.3831)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 21.4321(21.6702) | Bit/dim 3.6607(3.6702) | Xent 0.0408(0.0668) | Loss 3.6811(3.7036) | Error 0.0133(0.0241) Steps 892(877.18) | Grad Norm 2.8640(3.6735) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 21.2139(21.6193) | Bit/dim 3.6747(3.6684) | Xent 0.0718(0.0649) | Loss 3.7106(3.7008) | Error 0.0222(0.0230) Steps 880(877.46) | Grad Norm 3.1022(3.3907) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 21.7199(21.6507) | Bit/dim 3.6665(3.6634) | Xent 0.0588(0.0622) | Loss 3.6959(3.6945) | Error 0.0244(0.0221) Steps 874(876.25) | Grad Norm 2.9270(3.1099) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 21.4677(21.6473) | Bit/dim 3.6532(3.6646) | Xent 0.0569(0.0598) | Loss 3.6817(3.6945) | Error 0.0200(0.0208) Steps 880(877.18) | Grad Norm 2.9218(2.9280) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 21.3965(21.6135) | Bit/dim 3.6674(3.6640) | Xent 0.0445(0.0568) | Loss 3.6897(3.6924) | Error 0.0156(0.0197) Steps 850(876.84) | Grad Norm 3.4513(2.8474) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 21.7021(21.6938) | Bit/dim 3.6731(3.6635) | Xent 0.0866(0.0605) | Loss 3.7164(3.6937) | Error 0.0311(0.0211) Steps 868(876.37) | Grad Norm 6.2034(3.1337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 106.8428, Epoch Time 1316.9530(1929.7211), Bit/dim 3.6699(best: 3.6690), Xent 2.7985, Loss 5.0691, Error 0.3849(best: 0.3753)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 20.8905(21.6586) | Bit/dim 3.7119(3.6650) | Xent 0.0810(0.0656) | Loss 3.7524(3.6978) | Error 0.0244(0.0226) Steps 862(879.57) | Grad Norm 4.8394(3.4568) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 22.2907(21.6171) | Bit/dim 3.6488(3.6656) | Xent 0.0766(0.0653) | Loss 3.6871(3.6983) | Error 0.0278(0.0228) Steps 880(879.22) | Grad Norm 3.0126(3.4943) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 20.5097(21.5926) | Bit/dim 3.6397(3.6629) | Xent 0.0490(0.0652) | Loss 3.6642(3.6955) | Error 0.0211(0.0225) Steps 874(879.08) | Grad Norm 2.3424(3.5031) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 21.4146(21.4930) | Bit/dim 3.7035(3.6663) | Xent 0.0695(0.0635) | Loss 3.7382(3.6980) | Error 0.0256(0.0219) Steps 862(875.03) | Grad Norm 4.9379(3.5259) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 21.4311(21.5741) | Bit/dim 3.6492(3.6646) | Xent 0.0535(0.0625) | Loss 3.6760(3.6959) | Error 0.0167(0.0216) Steps 880(877.06) | Grad Norm 3.5041(3.5077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 106.7520, Epoch Time 1307.0655(1911.0414), Bit/dim 3.6666(best: 3.6690), Xent 2.6479, Loss 4.9906, Error 0.3654(best: 0.3753)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 22.0363(21.6030) | Bit/dim 3.6845(3.6649) | Xent 0.0446(0.0601) | Loss 3.7068(3.6949) | Error 0.0133(0.0204) Steps 868(876.14) | Grad Norm 1.9829(3.2952) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 20.8716(21.5163) | Bit/dim 3.6355(3.6626) | Xent 0.0409(0.0578) | Loss 3.6559(3.6915) | Error 0.0144(0.0200) Steps 862(874.98) | Grad Norm 2.1676(3.1186) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 21.1423(21.4176) | Bit/dim 3.6582(3.6615) | Xent 0.0499(0.0569) | Loss 3.6831(3.6900) | Error 0.0200(0.0196) Steps 880(874.82) | Grad Norm 2.6484(3.1716) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 21.6877(21.3590) | Bit/dim 3.6471(3.6614) | Xent 0.0569(0.0565) | Loss 3.6755(3.6896) | Error 0.0178(0.0192) Steps 892(876.82) | Grad Norm 2.4713(3.0751) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 20.9824(21.4249) | Bit/dim 3.6473(3.6594) | Xent 0.0600(0.0571) | Loss 3.6773(3.6879) | Error 0.0233(0.0196) Steps 862(875.52) | Grad Norm 4.5834(3.0921) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 20.8647(21.4269) | Bit/dim 3.6627(3.6603) | Xent 0.0759(0.0615) | Loss 3.7007(3.6911) | Error 0.0278(0.0213) Steps 874(873.62) | Grad Norm 3.6707(3.1439) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 104.1107, Epoch Time 1298.2694(1892.6582), Bit/dim 3.6668(best: 3.6666), Xent 2.6174, Loss 4.9755, Error 0.3717(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 20.8746(21.4512) | Bit/dim 3.6554(3.6615) | Xent 0.0411(0.0587) | Loss 3.6760(3.6908) | Error 0.0111(0.0201) Steps 868(874.46) | Grad Norm 2.4708(3.1607) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 21.4825(21.4161) | Bit/dim 3.6399(3.6571) | Xent 0.0404(0.0564) | Loss 3.6601(3.6854) | Error 0.0133(0.0196) Steps 892(873.13) | Grad Norm 2.4856(3.0300) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 21.2956(21.3775) | Bit/dim 3.6688(3.6577) | Xent 0.0550(0.0547) | Loss 3.6963(3.6850) | Error 0.0256(0.0191) Steps 880(872.66) | Grad Norm 4.0415(2.9329) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 21.0489(21.3267) | Bit/dim 3.6830(3.6586) | Xent 0.0704(0.0573) | Loss 3.7182(3.6873) | Error 0.0233(0.0199) Steps 850(871.02) | Grad Norm 5.0921(3.1775) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 21.6111(21.3667) | Bit/dim 3.6591(3.6599) | Xent 0.0441(0.0590) | Loss 3.6811(3.6894) | Error 0.0167(0.0202) Steps 880(871.74) | Grad Norm 2.5079(3.2990) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 106.1563, Epoch Time 1299.1803(1874.8539), Bit/dim 3.6601(best: 3.6666), Xent 2.6653, Loss 4.9927, Error 0.3761(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 21.3230(21.3735) | Bit/dim 3.6267(3.6597) | Xent 0.0449(0.0573) | Loss 3.6491(3.6884) | Error 0.0156(0.0196) Steps 874(871.69) | Grad Norm 3.1139(3.2865) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 21.2142(21.3802) | Bit/dim 3.6406(3.6575) | Xent 0.0408(0.0553) | Loss 3.6610(3.6851) | Error 0.0144(0.0191) Steps 868(870.06) | Grad Norm 1.8238(3.0544) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 20.9766(21.3139) | Bit/dim 3.6500(3.6563) | Xent 0.0560(0.0531) | Loss 3.6780(3.6829) | Error 0.0189(0.0181) Steps 868(869.86) | Grad Norm 3.5319(2.9963) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 20.8431(21.3270) | Bit/dim 3.6715(3.6544) | Xent 0.0563(0.0514) | Loss 3.6996(3.6801) | Error 0.0178(0.0177) Steps 874(870.78) | Grad Norm 1.9013(2.7978) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 21.8861(21.2835) | Bit/dim 3.6518(3.6522) | Xent 0.0591(0.0507) | Loss 3.6814(3.6776) | Error 0.0178(0.0173) Steps 874(869.38) | Grad Norm 3.5754(2.7412) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 21.4899(21.2481) | Bit/dim 3.6430(3.6523) | Xent 0.0533(0.0498) | Loss 3.6696(3.6772) | Error 0.0200(0.0174) Steps 868(869.37) | Grad Norm 3.2395(2.7644) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 102.8656, Epoch Time 1289.2842(1857.2868), Bit/dim 3.6509(best: 3.6601), Xent 2.7177, Loss 5.0098, Error 0.3691(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.8981(21.2254) | Bit/dim 3.6559(3.6517) | Xent 0.0625(0.0513) | Loss 3.6871(3.6773) | Error 0.0244(0.0176) Steps 862(867.83) | Grad Norm 3.9704(3.0503) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 21.1684(21.1437) | Bit/dim 3.6107(3.6526) | Xent 0.0585(0.0526) | Loss 3.6400(3.6789) | Error 0.0244(0.0179) Steps 862(868.11) | Grad Norm 3.6118(3.1417) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 20.7988(21.0863) | Bit/dim 3.6855(3.6526) | Xent 0.0784(0.0545) | Loss 3.7248(3.6798) | Error 0.0289(0.0188) Steps 844(866.77) | Grad Norm 4.6879(3.2076) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 21.5718(21.1540) | Bit/dim 3.6622(3.6533) | Xent 0.0858(0.0558) | Loss 3.7051(3.6812) | Error 0.0322(0.0194) Steps 874(868.93) | Grad Norm 3.3834(3.1906) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 20.8713(21.1507) | Bit/dim 3.6424(3.6572) | Xent 0.1412(0.0675) | Loss 3.7131(3.6909) | Error 0.0433(0.0238) Steps 862(871.92) | Grad Norm 4.7066(3.7800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 104.2738, Epoch Time 1285.2062(1840.1244), Bit/dim 3.6809(best: 3.6509), Xent 2.7714, Loss 5.0666, Error 0.3882(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 21.3392(21.2655) | Bit/dim 3.6724(3.6636) | Xent 0.0418(0.0698) | Loss 3.6933(3.6985) | Error 0.0156(0.0249) Steps 880(874.43) | Grad Norm 3.9521(3.7153) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 21.9398(21.3356) | Bit/dim 3.6559(3.6633) | Xent 0.0404(0.0672) | Loss 3.6761(3.6970) | Error 0.0167(0.0235) Steps 880(873.06) | Grad Norm 2.6943(3.4836) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 20.6178(21.3306) | Bit/dim 3.6559(3.6645) | Xent 0.0869(0.0662) | Loss 3.6993(3.6976) | Error 0.0244(0.0230) Steps 868(873.18) | Grad Norm 4.9298(3.4719) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 22.0833(21.3220) | Bit/dim 3.6439(3.6611) | Xent 0.1191(0.0734) | Loss 3.7035(3.6978) | Error 0.0444(0.0256) Steps 868(874.43) | Grad Norm 4.8385(3.8945) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 21.6311(21.3388) | Bit/dim 3.6579(3.6646) | Xent 0.1506(0.0858) | Loss 3.7332(3.7075) | Error 0.0422(0.0298) Steps 898(877.47) | Grad Norm 7.2159(4.4666) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 20.7191(21.3886) | Bit/dim 3.6747(3.6678) | Xent 0.0650(0.0884) | Loss 3.7072(3.7120) | Error 0.0211(0.0307) Steps 862(878.79) | Grad Norm 3.0195(4.2742) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 104.7766, Epoch Time 1300.9380(1823.9488), Bit/dim 3.6675(best: 3.6509), Xent 2.5895, Loss 4.9622, Error 0.3700(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 21.4848(21.3262) | Bit/dim 3.6505(3.6662) | Xent 0.0477(0.0833) | Loss 3.6744(3.7078) | Error 0.0156(0.0286) Steps 868(876.98) | Grad Norm 2.4993(3.8499) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 21.2106(21.2899) | Bit/dim 3.6931(3.6645) | Xent 0.0470(0.0785) | Loss 3.7166(3.7038) | Error 0.0156(0.0271) Steps 862(876.13) | Grad Norm 2.4359(3.6324) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 21.5052(21.3346) | Bit/dim 3.6588(3.6631) | Xent 0.0907(0.0763) | Loss 3.7042(3.7013) | Error 0.0322(0.0262) Steps 874(876.89) | Grad Norm 5.0626(3.6514) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 21.2062(21.2964) | Bit/dim 3.6124(3.6573) | Xent 0.0640(0.0743) | Loss 3.6444(3.6944) | Error 0.0211(0.0255) Steps 850(874.05) | Grad Norm 4.5997(3.7721) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 21.6228(21.2854) | Bit/dim 3.6273(3.6583) | Xent 0.0558(0.0716) | Loss 3.6552(3.6941) | Error 0.0200(0.0248) Steps 874(874.79) | Grad Norm 2.8107(3.6606) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 106.1924, Epoch Time 1292.9796(1808.0197), Bit/dim 3.6537(best: 3.6509), Xent 2.6719, Loss 4.9897, Error 0.3775(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 21.9543(21.3692) | Bit/dim 3.6224(3.6591) | Xent 0.0564(0.0708) | Loss 3.6506(3.6945) | Error 0.0200(0.0248) Steps 898(877.42) | Grad Norm 2.4994(3.4341) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 21.1128(21.3350) | Bit/dim 3.6310(3.6558) | Xent 0.0533(0.0679) | Loss 3.6577(3.6898) | Error 0.0211(0.0239) Steps 898(876.77) | Grad Norm 2.1729(3.2725) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 21.6400(21.4287) | Bit/dim 3.6502(3.6547) | Xent 0.0443(0.0656) | Loss 3.6723(3.6875) | Error 0.0133(0.0228) Steps 862(877.07) | Grad Norm 2.3096(3.0674) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 21.4906(21.3410) | Bit/dim 3.6742(3.6508) | Xent 0.0648(0.0646) | Loss 3.7066(3.6831) | Error 0.0256(0.0226) Steps 874(875.08) | Grad Norm 3.0259(2.9693) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 21.3843(21.3862) | Bit/dim 3.6391(3.6496) | Xent 0.0345(0.0632) | Loss 3.6564(3.6812) | Error 0.0122(0.0222) Steps 850(874.83) | Grad Norm 1.9816(2.9268) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 21.1985(21.3384) | Bit/dim 3.6656(3.6496) | Xent 0.0492(0.0593) | Loss 3.6902(3.6793) | Error 0.0167(0.0212) Steps 880(874.08) | Grad Norm 2.4031(2.7420) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 104.8140, Epoch Time 1299.4883(1792.7638), Bit/dim 3.6484(best: 3.6509), Xent 2.8488, Loss 5.0728, Error 0.3808(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 21.5188(21.2931) | Bit/dim 3.6318(3.6481) | Xent 0.0421(0.0578) | Loss 3.6528(3.6770) | Error 0.0156(0.0204) Steps 862(871.96) | Grad Norm 2.4374(2.6770) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 21.1866(21.2689) | Bit/dim 3.6396(3.6469) | Xent 0.0267(0.0551) | Loss 3.6529(3.6744) | Error 0.0089(0.0195) Steps 856(870.99) | Grad Norm 2.1250(2.6699) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 21.2108(21.2180) | Bit/dim 3.6953(3.6492) | Xent 0.0628(0.0539) | Loss 3.7267(3.6761) | Error 0.0267(0.0192) Steps 862(870.17) | Grad Norm 2.7813(2.6162) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 20.8061(21.2146) | Bit/dim 3.6527(3.6451) | Xent 0.0271(0.0509) | Loss 3.6662(3.6705) | Error 0.0089(0.0184) Steps 874(870.20) | Grad Norm 1.9064(2.5736) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 21.2398(21.2382) | Bit/dim 3.6507(3.6443) | Xent 0.0666(0.0518) | Loss 3.6840(3.6702) | Error 0.0222(0.0186) Steps 874(869.83) | Grad Norm 2.5247(2.5830) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 103.7923, Epoch Time 1290.6494(1777.7003), Bit/dim 3.6525(best: 3.6484), Xent 2.7489, Loss 5.0269, Error 0.3775(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 21.0758(21.2837) | Bit/dim 3.6364(3.6456) | Xent 0.0521(0.0536) | Loss 3.6624(3.6724) | Error 0.0200(0.0190) Steps 874(871.48) | Grad Norm 3.3974(2.9369) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 20.7418(21.3240) | Bit/dim 3.6851(3.6478) | Xent 0.0289(0.0539) | Loss 3.6995(3.6747) | Error 0.0089(0.0189) Steps 862(871.55) | Grad Norm 2.1052(2.9203) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 21.5391(21.3732) | Bit/dim 3.6152(3.6446) | Xent 0.0530(0.0533) | Loss 3.6417(3.6712) | Error 0.0178(0.0187) Steps 892(872.98) | Grad Norm 3.2001(2.9142) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 20.9739(21.3361) | Bit/dim 3.5980(3.6430) | Xent 0.0421(0.0529) | Loss 3.6191(3.6695) | Error 0.0133(0.0185) Steps 856(872.06) | Grad Norm 2.3782(2.8240) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 20.4012(21.2880) | Bit/dim 3.6645(3.6439) | Xent 0.0402(0.0512) | Loss 3.6846(3.6695) | Error 0.0133(0.0179) Steps 862(869.59) | Grad Norm 3.0437(2.7746) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 20.8833(21.2515) | Bit/dim 3.6516(3.6428) | Xent 0.0784(0.0505) | Loss 3.6908(3.6680) | Error 0.0256(0.0178) Steps 886(870.51) | Grad Norm 3.9165(2.7761) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 105.1874, Epoch Time 1293.0218(1763.1600), Bit/dim 3.6415(best: 3.6484), Xent 2.6808, Loss 4.9818, Error 0.3730(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 21.6411(21.2907) | Bit/dim 3.6310(3.6393) | Xent 0.0527(0.0465) | Loss 3.6573(3.6626) | Error 0.0189(0.0163) Steps 880(871.90) | Grad Norm 2.1987(2.5528) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 21.6640(21.2101) | Bit/dim 3.6216(3.6375) | Xent 0.0303(0.0432) | Loss 3.6367(3.6591) | Error 0.0111(0.0151) Steps 862(871.55) | Grad Norm 2.3614(2.3706) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 20.6236(21.1929) | Bit/dim 3.5951(3.6330) | Xent 0.0427(0.0444) | Loss 3.6165(3.6551) | Error 0.0178(0.0154) Steps 874(873.00) | Grad Norm 3.1628(2.4307) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 21.0797(21.2527) | Bit/dim 3.6604(3.6381) | Xent 0.0231(0.0431) | Loss 3.6720(3.6596) | Error 0.0089(0.0149) Steps 862(872.72) | Grad Norm 2.0419(2.4773) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 20.6737(21.1496) | Bit/dim 3.6443(3.6390) | Xent 0.0548(0.0441) | Loss 3.6717(3.6611) | Error 0.0178(0.0151) Steps 850(870.75) | Grad Norm 3.2009(2.5790) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 104.5293, Epoch Time 1287.5717(1748.8923), Bit/dim 3.6433(best: 3.6415), Xent 2.6501, Loss 4.9684, Error 0.3759(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 21.0360(21.2136) | Bit/dim 3.6342(3.6392) | Xent 0.0451(0.0462) | Loss 3.6567(3.6623) | Error 0.0133(0.0158) Steps 880(872.97) | Grad Norm 2.3853(2.6953) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 21.1142(21.1996) | Bit/dim 3.6372(3.6386) | Xent 0.0462(0.0468) | Loss 3.6603(3.6620) | Error 0.0167(0.0161) Steps 886(875.03) | Grad Norm 3.2165(2.8598) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 21.3420(21.2034) | Bit/dim 3.6185(3.6386) | Xent 0.0622(0.0474) | Loss 3.6497(3.6622) | Error 0.0233(0.0164) Steps 856(873.57) | Grad Norm 3.4618(2.7755) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 21.5827(21.2791) | Bit/dim 3.6759(3.6390) | Xent 0.0301(0.0461) | Loss 3.6909(3.6620) | Error 0.0111(0.0161) Steps 880(876.51) | Grad Norm 3.5052(2.7883) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 21.2880(21.2718) | Bit/dim 3.6284(3.6391) | Xent 0.0626(0.0473) | Loss 3.6597(3.6628) | Error 0.0200(0.0170) Steps 862(874.44) | Grad Norm 3.3339(2.9064) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 21.3346(21.2493) | Bit/dim 3.6431(3.6383) | Xent 0.1166(0.0617) | Loss 3.7014(3.6692) | Error 0.0411(0.0219) Steps 886(875.68) | Grad Norm 5.3230(3.7057) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 104.7306, Epoch Time 1292.7800(1735.2090), Bit/dim 3.6534(best: 3.6415), Xent 2.8491, Loss 5.0779, Error 0.3792(best: 0.3654)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 20.7102(21.2584) | Bit/dim 3.6342(3.6390) | Xent 0.0814(0.0613) | Loss 3.6749(3.6696) | Error 0.0300(0.0219) Steps 856(876.05) | Grad Norm 3.2894(3.7426) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 22.0937(21.2211) | Bit/dim 3.6774(3.6456) | Xent 0.2774(0.0798) | Loss 3.8161(3.6855) | Error 0.0789(0.0276) Steps 886(875.33) | Grad Norm 10.6381(4.6693) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 25.4516(21.6767) | Bit/dim 7.2919(4.0642) | Xent 8.0698(2.6446) | Loss 11.3268(5.3865) | Error 0.8089(0.1812) Steps 1138(906.62) | Grad Norm 36.5454(19.3853) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 46.4631(25.1422) | Bit/dim 6.2360(4.7897) | Xent 7.6875(3.4356) | Loss 10.0798(6.5074) | Error 0.8733(0.3531) Steps 1972(1071.85) | Grad Norm 5719391101678248.0000(2887141844755017.0000) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_run1 --resume ../experiments_published/cnf_conditional_cifar10_bs900_run1/current_checkpt.pth --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
